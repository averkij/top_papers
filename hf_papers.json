{
    "date": {
        "ru": "13 марта",
        "en": "March 13",
        "zh": "3月13日"
    },
    "time_utc": "2025-03-13 07:10",
    "weekday": 3,
    "issue_id": 2681,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.09566",
            "title": "TPDiff: Temporal Pyramid Video Diffusion Model",
            "url": "https://huggingface.co/papers/2503.09566",
            "abstract": "The development of video diffusion models unveils a significant challenge: the substantial computational demands. To mitigate this challenge, we note that the reverse process of diffusion exhibits an inherent entropy-reducing nature. Given the inter-frame redundancy in video modality, maintaining full frame rates in high-entropy stages is unnecessary. Based on this insight, we propose TPDiff, a unified framework to enhance training and inference efficiency. By dividing diffusion into several stages, our framework progressively increases frame rate along the diffusion process with only the last stage operating on full frame rate, thereby optimizing computational efficiency. To train the multi-stage diffusion model, we introduce a dedicated training framework: stage-wise diffusion. By solving the partitioned probability flow ordinary differential equations (ODE) of diffusion under aligned data and noise, our training strategy is applicable to various diffusion forms and further enhances training efficiency. Comprehensive experimental evaluations validate the generality of our method, demonstrating 50% reduction in training cost and 1.5x improvement in inference efficiency.",
            "score": 18,
            "issue_id": 2681,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "6952e94de20936ce",
            "authors": [
                "Lingmin Ran",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09566.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#diffusion",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Ускорение видео-диффузии: эффективность через поэтапность",
                    "desc": "Статья представляет TPDiff - унифицированную систему для повышения эффективности обучения и вывода видео-диффузионных моделей. Авторы предлагают разделить процесс диффузии на несколько этапов, постепенно увеличивая частоту кадров, что оптимизирует вычислительные ресурсы. Для обучения многоэтапной диффузионной модели вводится специальная структура обучения: поэтапная диффузия. Экспериментальные оценки подтверждают универсальность метода, демонстрируя сокращение затрат на обучение на 50% и повышение эффективности вывода в 1,5 раза."
                },
                "en": {
                    "title": "Optimizing Video Diffusion with TPDiff: Efficiency Unleashed!",
                    "desc": "This paper addresses the high computational costs associated with video diffusion models by introducing TPDiff, a framework that optimizes training and inference efficiency. The authors leverage the entropy-reducing nature of the diffusion process and the redundancy between video frames to reduce the need for full frame rates during high-entropy stages. TPDiff operates in multiple stages, gradually increasing the frame rate, with only the final stage using the full frame rate, thus enhancing computational efficiency. The proposed stage-wise diffusion training framework further improves efficiency by solving partitioned probability flow ordinary differential equations, leading to significant reductions in training costs and improvements in inference speed."
                },
                "zh": {
                    "title": "优化视频扩散模型的计算效率",
                    "desc": "视频扩散模型的发展面临着巨大的计算需求。为了缓解这一挑战，我们注意到扩散的反向过程具有固有的减少熵的特性。考虑到视频模态中的帧间冗余，在高熵阶段保持全帧率是没有必要的。基于这一见解，我们提出了TPDiff框架，通过将扩散过程分为多个阶段，逐步提高帧率，从而优化计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09151",
            "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
            "url": "https://huggingface.co/papers/2503.09151",
            "abstract": "We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/",
            "score": 15,
            "issue_id": 2681,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "e1463c182b0fe8e9",
            "authors": [
                "Hyeonho Jeong",
                "Suhyeon Lee",
                "Jong Chul Ye"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09151.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Революция в многоракурсной видеогенерации без 4D-датасетов",
                    "desc": "Reangle-A-Video - это новый подход к генерации синхронизированных многоракурсных видео из одного входного видео. Метод использует двухэтапный процесс: обучение многоракурсному движению и согласованный перевод изображения в изображения с разных ракурсов. В отличие от традиционных методов, Reangle-A-Video не требует больших 4D-датасетов, а использует существующие модели диффузии для изображений и видео. Эксперименты показывают, что этот метод превосходит существующие подходы в задачах статического переноса ракурса и динамического управления камерой."
                },
                "en": {
                    "title": "Transforming Single Videos into Multi-View Masterpieces!",
                    "desc": "Reangle-A-Video is a novel framework designed to create synchronized multi-view videos from a single input video. It innovatively approaches the multi-view video generation task by treating it as a video-to-video translation problem, utilizing existing image and video diffusion models. The process consists of two main stages: first, it learns motion patterns from warped videos in a self-supervised manner, and second, it generates consistent multi-view images by warping and inpainting the initial frame under specific guidance. This method outperforms current techniques in both static view transport and dynamic camera control, marking a significant advancement in multi-view video generation."
                },
                "zh": {
                    "title": "Reangle-A-Video：单视频生成多视角同步视频的新方法",
                    "desc": "我们提出了Reangle-A-Video，这是一个统一框架，用于从单个输入视频生成同步的多视角视频。与主流方法不同，我们的方法将多视角视频生成任务重新定义为视频到视频的转换，利用公开可用的图像和视频扩散先验。Reangle-A-Video的操作分为两个阶段：首先，通过自监督方式对图像到视频的扩散变换器进行同步微调，以提取视角不变的运动；其次，在推理时使用DUSt3R进行跨视角一致性指导，将输入视频的第一帧变形并修复为不同的摄像机视角，生成多视角一致的起始图像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09573",
            "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
            "url": "https://huggingface.co/papers/2503.09573",
            "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/",
            "score": 9,
            "issue_id": 2678,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "32f097e93cbf5f3a",
            "authors": [
                "Marianne Arriola",
                "Aaron Gokaslan",
                "Justin T Chiu",
                "Zhihan Yang",
                "Zhixuan Qi",
                "Jiaqi Han",
                "Subham Sekhar Sahoo",
                "Volodymyr Kuleshov"
            ],
            "affiliations": [
                "Cohere, NY, USA",
                "Cornell Tech, NY, USA",
                "Stanford University, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09573.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Блочные диффузионные модели: лучшее из двух миров в языковом моделировании",
                    "desc": "Статья представляет новый класс языковых моделей - блочные диффузионные модели. Они объединяют преимущества диффузионных и авторегрессивных моделей, позволяя генерировать тексты произвольной длины и повышая эффективность вывода. Авторы предлагают эффективный алгоритм обучения, оценки дисперсии градиента и расписания шума для минимизации дисперсии. Блочные диффузионные модели достигают нового уровня производительности среди диффузионных моделей в задачах языкового моделирования."
                },
                "en": {
                    "title": "Block Diffusion: The Future of Flexible Language Generation",
                    "desc": "This paper presents block diffusion language models, which combine the strengths of diffusion models and autoregressive models. These models allow for flexible-length text generation and improve efficiency during inference by using techniques like KV caching and parallel token sampling. The authors introduce a comprehensive approach for training these models, including methods to reduce gradient variance and optimize noise schedules. As a result, block diffusion models achieve state-of-the-art performance in language modeling tasks and can generate sequences of varying lengths."
                },
                "zh": {
                    "title": "块扩散模型：灵活生成与高效推理的结合",
                    "desc": "扩散语言模型相比自回归模型具有独特的优势，如并行生成和可控性，但在似然建模方面表现较差，并且生成长度固定。本文提出了一类块扩散语言模型，结合了离散去噪扩散和自回归模型的优点。块扩散克服了这两种方法的关键限制，支持灵活长度的生成，并通过KV缓存和并行令牌采样提高推理效率。我们提出了一种构建有效块扩散模型的方案，包括高效的训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06955",
            "title": "Motion Anything: Any to Motion Generation",
            "url": "https://huggingface.co/papers/2503.06955",
            "abstract": "Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. See our project website https://steve-zeyu-zhang.github.io/MotionAnything",
            "score": 2,
            "issue_id": 2680,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 марта",
                "en": "March 10",
                "zh": "3月10日"
            },
            "hash": "9199d2d99b75d862",
            "authors": [
                "Zeyu Zhang",
                "Yiran Wang",
                "Wei Mao",
                "Danning Li",
                "Rui Zhao",
                "Biao Wu",
                "Zirui Song",
                "Bohan Zhuang",
                "Ian Reid",
                "Richard Hartley"
            ],
            "affiliations": [
                "ANU",
                "Google",
                "JD.com",
                "MBZUAI",
                "McGill",
                "Tencent",
                "USYD",
                "UTS",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06955.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#synthetic",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🕺",
                "ru": {
                    "title": "Универсальная генерация движений с мультимодальным контролем",
                    "desc": "Статья представляет Motion Anything - новую мультимодальную систему генерации движений, использующую маскирование на основе внимания для точного контроля над ключевыми кадрами и действиями. Модель адаптивно кодирует различные условия, включая текст и музыку, что улучшает управляемость генерируемых движений. Авторы также представляют новый датасет Text-Music-Dance (TMD), содержащий 2153 пары текста, музыки и танца. Эксперименты показывают, что Motion Anything превосходит современные методы на нескольких бенчмарках, достигая 15% улучшения FID на HumanML3D."
                },
                "en": {
                    "title": "Revolutionizing Motion Generation with Multimodal Control",
                    "desc": "This paper presents Motion Anything, a new framework for generating motion that effectively combines multiple input types like text and music. It addresses two main challenges in motion generation: the need for prioritizing dynamic elements and the integration of different conditioning modalities. The proposed Attention-based Mask Modeling allows for better control over key frames and actions, enhancing the quality of generated motions. Additionally, the introduction of the Text-Music-Dance dataset provides a larger resource for training, leading to significant improvements in performance compared to existing methods."
                },
                "zh": {
                    "title": "多模态运动生成的新突破",
                    "desc": "本文提出了一种名为Motion Anything的多模态运动生成框架，旨在解决现有方法在动态帧和身体部位优先级方面的不足。我们引入了一种基于注意力的掩模建模方法，使得对关键帧和动作的空间和时间控制更加精细。该模型能够自适应编码文本和音乐等多模态条件，从而提高生成运动的可控性。此外，我们还创建了一个新的运动数据集Text-Music-Dance (TMD)，包含2153对文本、音乐和舞蹈，填补了社区中的重要空白。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09427",
            "title": "Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation",
            "url": "https://huggingface.co/papers/2503.09427",
            "abstract": "Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in k-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
            "score": 1,
            "issue_id": 2677,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "491beb48064068d2",
            "authors": [
                "Yaorui Shi",
                "Jiaqi Yang",
                "Sihang Li",
                "Junfeng Fang",
                "Xiang Wang",
                "Zhiyuan Liu",
                "Yang Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09427.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#transfer_learning",
                    "#science",
                    "#multimodal",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Единая модель для анализа клеток и текста",
                    "desc": "scMMGPT - это новая языковая модель, объединяющая анализ одиночных клеток и текста. Она решает проблему ограниченности существующих моделей, которые специализируются только на одной из этих модальностей. scMMGPT использует специальные проекторы для преодоления разрыва между клеточными и текстовыми данными. Модель была предобучена на 27 миллионах клеток, что является крупнейшим датасетом для мультимодальных клеточно-текстовых моделей на сегодняшний день."
                },
                "en": {
                    "title": "Bridging Cells and Text: The Power of scMMGPT",
                    "desc": "This paper introduces the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a novel model designed to integrate single-cell RNA sequencing data with textual information. Traditional pre-trained language models struggle with this integration due to their inability to process both modalities effectively. scMMGPT addresses these limitations by utilizing cross-modal projectors and extensive pre-training on a large dataset of 27 million cells, enhancing its performance in joint tasks. The results demonstrate significant improvements in cell description generation, cell type annotation accuracy, and text-conditioned pseudo-cell generation compared to existing models."
                },
                "zh": {
                    "title": "单细胞多模态生成预训练变换器的创新应用",
                    "desc": "预训练语言模型（PLMs）在科学研究中带来了革命性的变化，但在单细胞分析中的应用仍然有限。现有的文本PLMs无法处理单细胞RNA测序数据，而细胞PLMs又无法处理自由文本，这限制了它们在多模态任务中的使用。为了解决这些问题，我们提出了单细胞多模态生成预训练变换器（scMMGPT），这是一个用于细胞和文本联合建模的统一PLM。scMMGPT通过专门的跨模态投影器和在2700万个细胞上进行的大规模预训练，显著提高了细胞描述生成和细胞类型注释的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09419",
            "title": "Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space",
            "url": "https://huggingface.co/papers/2503.09419",
            "abstract": "Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/SingleZombie/AFLDM",
            "score": 1,
            "issue_id": 2680,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "1c497a991b18da6a",
            "authors": [
                "Yifan Zhou",
                "Zeqi Xiao",
                "Shuai Yang",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09419.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Стабильная генерация изображений с помощью эквивариантных латентных диффузионных моделей",
                    "desc": "Эта статья представляет новый подход к улучшению стабильности латентных диффузионных моделей (LDM). Авторы предлагают модифицированную архитектуру AF-LDM, которая обладает свойством эквивариантности к сдвигу, что повышает согласованность результатов генерации. Ключевые изменения включают переработку модулей внимания и введение специальной функции потерь для подавления частотной полосы признаков. Эксперименты показывают, что AF-LDM значительно превосходит стандартные LDM по стабильности результатов в различных задачах, таких как редактирование видео и перевод изображений."
                },
                "en": {
                    "title": "Achieving Consistency in Latent Diffusion Models with Shift-Equivariance",
                    "desc": "Latent Diffusion Models (LDMs) often produce inconsistent outputs due to their sensitivity to input noise variations. This paper presents a redesign of LDMs to improve their consistency by implementing shift-equivariance. The authors address challenges such as aliasing during VAE training and the limitations of self-attention modules by introducing new shift-equivariant attention mechanisms and an equivariance loss. The resulting alias-free LDM (AF-LDM) shows enhanced robustness and consistency in applications like video editing and image translation compared to traditional LDMs."
                },
                "zh": {
                    "title": "提升潜在扩散模型的一致性",
                    "desc": "潜在扩散模型（LDMs）在生成过程中存在不稳定性，输入噪声的微小变化可能导致输出结果显著不同，这限制了其在需要一致性结果的应用中的适用性。本文通过重新设计LDMs，使其具备平移等变性，从而增强一致性。我们提出了一种新的注意力模块，使其具备平移等变性，并引入了一种等变损失，有效抑制特征在连续域中的频率带宽。最终，得到的无别名LDM（AF-LDM）在多个应用中表现出更强的一致性，尤其是在视频编辑和图像到图像转换任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08525",
            "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
            "url": "https://huggingface.co/papers/2503.08525",
            "abstract": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes.",
            "score": 1,
            "issue_id": 2681,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 марта",
                "en": "March 11",
                "zh": "3月11日"
            },
            "hash": "e82260bba3e835b4",
            "authors": [
                "Tong Wei",
                "Yijun Yang",
                "Junliang Xing",
                "Yuanchun Shi",
                "Zongqing Lu",
                "Deheng Ye"
            ],
            "affiliations": [
                "Peking University",
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08525.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#reasoning",
                    "#video",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Управляемое обучение рассуждениям для визуально-языковых моделей",
                    "desc": "Исследование посвящено применению обучения с подкреплением для улучшения рассуждений в визуально-языковых моделях. Авторы обнаружили феномен 'коллапса мыслей' при использовании наград, основанных только на результатах действий. Для решения этой проблемы предложен метод GTR (Guided Thought Reinforcement), который автоматически оценивает и уточняет рассуждения агента на каждом шаге обучения. Эксперименты показали, что GTR значительно улучшает производительность и обобщающую способность модели LLaVA-7b в различных визуальных средах."
                },
                "en": {
                    "title": "Enhancing VLMs with Guided Thought Reinforcement",
                    "desc": "This paper explores the challenges of using reinforcement learning (RL) to train vision-language models (VLMs) for reasoning in visual tasks. It identifies a problem called 'thought collapse', where the model's reasoning becomes less diverse and leads to incorrect actions when rewards are based only on outcomes. To address this, the authors propose a Guided Thought Reinforcement (GTR) framework that provides process guidance to improve the reasoning of VLMs during training. Their experiments show that GTR significantly boosts the performance of the LLaVA-7b model, achieving much higher success rates in complex tasks compared to state-of-the-art models."
                },
                "zh": {
                    "title": "引导思维强化：提升视觉语言模型的推理能力",
                    "desc": "本研究探讨了可验证结果奖励的强化学习（RLVR）在视觉语言模型（VLM）中的应用，尤其是在复杂的视觉环境中进行目标导向的推理。我们发现，当奖励仅基于行动结果时，RL无法有效激励VLM的思维链推理，导致思维崩溃现象，表现为代理的思维多样性迅速下降和推理不完整。为了解决这一问题，我们提出了一种自动纠正器，能够在每个RL步骤中评估和改进代理的推理过程。通过实验，我们的引导思维强化（GTR）框架显著提高了LLaVA-7b模型在各种视觉环境中的表现和泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09402",
            "title": "VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary",
            "url": "https://huggingface.co/papers/2503.09402",
            "abstract": "Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) A generative retrieval model, marrying language model's complex reasoning capabilities with contrastive retrieval's efficient similarity search. (ii) A hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting a tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, a development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering a novel perspective on video understanding. Codes are released at https://github.com/showlab/VLog.",
            "score": 0,
            "issue_id": 2681,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "6a25ed0e2c069e4f",
            "authors": [
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09402.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#games",
                    "#reasoning",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VLog: Пересказ видео через словарь событий",
                    "desc": "VLog - это новая система понимания видео, которая использует словарь событий для описания повседневной деятельности человека. Она основана на языковой модели GPT-2 и включает в себя генеративную модель поиска, иерархический словарь и стратегию обновления словаря. VLog способна генерировать краткие и точные описания видео, учитывая контекст и логические связи между событиями. Эффективность системы была продемонстрирована на нескольких наборах данных, включая специально созданный набор VidCap-Eval."
                },
                "en": {
                    "title": "VLog: Revolutionizing Video Narration with Hierarchical Vocabulary",
                    "desc": "The paper presents VLog, a new framework for understanding videos by narrating daily activities as sequences of events. It introduces a hierarchical vocabulary that allows for efficient indexing of specific actions within broader contexts, enhancing the way video content is interpreted. VLog combines a generative retrieval model with a lightweight language model, enabling complex reasoning and efficient similarity searches. Additionally, it features a vocabulary update strategy that adapts to new events during inference, demonstrating its effectiveness through experiments on various datasets."
                },
                "zh": {
                    "title": "VLog：视频理解的新视角",
                    "desc": "本论文介绍了一种名为VLog的视频理解框架，旨在将视频叙述定义为词汇，超越现有生成视频语言模型中的子词词汇。VLog基于轻量级语言模型GPT-2，具有三项关键创新：生成检索模型、层次词汇和词汇更新策略。生成检索模型结合了语言模型的复杂推理能力和对比检索的高效相似性搜索。通过在EgoSchema、COIN和HiREST数据集上的实验，验证了VLog在生成简洁、上下文准确的叙述方面的有效性。"
                }
            }
        }
    ],
    "link_prev": "2025-03-12.html",
    "link_next": "2025-03-14.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "12.03",
        "en": "03/12",
        "zh": "3月12日"
    },
    "short_date_next": {
        "ru": "14.03",
        "en": "03/14",
        "zh": "3月14日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了YuE，一种基于LLaMA2架构的开放基础模型。它能生成长达五分钟的音乐，保持歌词对齐、连贯的音乐结构和引人入胜的歌唱旋律。YuE通过多任务、多阶段的预训练方法实现这一点。它还能进行风格转换和双向生成。实验显示，YuE在音乐性和声乐灵活性上匹敌或超越一些专有系统。此外，YuE在音乐理解任务中也表现出色。",
        "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
        "pinyin": "这篇文章介绍了YuE，一种基于LLaMA2架构的开放基础模型。它能生成长达五分钟的音乐，保持歌词对齐、连贯的音乐结构和引人入胜的歌唱旋律。YuE通过多任务、多阶段的预训练方法实现这一点。它还能进行风格转换和双向生成。实验显示，YuE在音乐性和声乐灵活性上匹敌或超越一些专有系统。此外，YuE在音乐理解任务中也表现出色。\n\nZhè piān wénzhāng jièshào le YuE, yī zhǒng jīyú LLaMA2 jiàgòu de kāifàng jīchǔ móxíng. Tā néng shēngchéng cháng dá wǔ fēnzhōng de yīnyuè, bǎochí gēcí duìqí, liánhé de yīnyuè jiégòu hé yǐnrénrùshèng de gēchàng xuánlǜ. YuE tōngguò duō rènwù, duō jiēduàn de yùxùnliàn fāngfǎ shíxiàn zhè yīdiǎn. Tā hái néng jìnxíng fēnggé zhuǎnhuàn hé shuāngxiàng shēngchéng. Shíyàn xiǎnshì, YuE zài yīnyuèxìng hé shēngyuè línghuóxìng shàng pǐdí huò chāoyuè yīxiē zhuānyǒu xìtǒng. Cǐwài,YuE zài yīnyuè lǐjiě rènwù zhōng yě biǎoxiàn chūsè.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"架构\", \"pinyin\": \"jià gòu\", \"trans\": \"architecture\"},\n    {\"word\": \"开放\", \"pinyin\": \"kāi fàng\", \"trans\": \"open\"},\n    {\"word\": \"基础\", \"pinyin\": \"jī chǔ\", \"trans\": \"foundation\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"保持\", \"pinyin\": \"bǎo chí\", \"trans\": \"maintain\"},\n    {\"word\": \"对齐\", \"pinyin\": \"duì qí\", \"trans\": \"alignment\"},\n    {\"word\": \"连贯\", \"pinyin\": \"lián guàn\", \"trans\": \"coherent\"},\n    {\"word\": \"结构\", \"pinyin\": \"jié gòu\", \"trans\": \"structure\"},\n    {\"word\": \"引人入胜\", \"pinyin\": \"yǐn rén rù shèng\", \"trans\": \"fascinating\"},\n    {\"word\": \"旋律\", \"pinyin\": \"xuán lǜ\", \"trans\": \"melody\"},\n    {\"word\": \"多任务\", \"pinyin\": \"duō rèn wù\", \"trans\": \"multi-task\"},\n    {\"word\": \"多阶段\", \"pinyin\": \"duō jiē duàn\", \"trans\": \"multi-stage\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"风格\", \"pinyin\": \"fēng gé\", \"trans\": \"style\"},\n    {\"word\": \"转换\", \"pinyin\": \"zhuǎn huàn\", \"trans\": \"conversion\"},\n    {\"word\": \"双向\", \"pinyin\": \"shuāng xiàng\", \"trans\": \"bidirectional\"},\n    {\"word\": \"匹敌\", \"pinyin\": \"pǐ dí\", \"trans\": \"match\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"专有\", \"pinyin\": \"zhuān yǒu\", \"trans\": \"proprietary\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"此外\", \"pinyin\": \"cǐ wài\", \"trans\": \"moreover\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"}\n]",
        "trans": "This article introduces YuE, an open-source foundational model based on the LLaMA2 architecture. It can generate music up to five minutes long, maintaining lyric alignment, coherent musical structure, and captivating vocal melodies. YuE achieves this through a multi-task, multi-stage pre-training method. It is also capable of style transfer and bidirectional generation. Experiments show that YuE matches or surpasses some proprietary systems in terms of musicality and vocal flexibility. Additionally, YuE performs excellently in music understanding tasks.",
        "update_ts": "2025-03-12 09:11"
    }
}