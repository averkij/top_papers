{
    "date": {
        "ru": "9 апреля",
        "en": "April 9",
        "zh": "4月9日"
    },
    "time_utc": "2025-04-09 02:22",
    "weekday": 2,
    "issue_id": 3137,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.05979",
            "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
            "url": "https://huggingface.co/papers/2504.05979",
            "abstract": "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.",
            "score": 8,
            "issue_id": 3137,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "f1195a87ec5b86f1",
            "authors": [
                "Sixiang Chen",
                "Jinbin Bai",
                "Zhuoran Zhao",
                "Tian Ye",
                "Qingyu Shi",
                "Donghao Zhou",
                "Wenhao Chai",
                "Xin Lin",
                "Jianzong Wu",
                "Chao Tang",
                "Shilin Xu",
                "Tao Zhang",
                "Haobo Yuan",
                "Yikang Zhou",
                "Wei Chow",
                "Linfeng Li",
                "Xiangtai Li",
                "Lei Zhu",
                "Lu Qi"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology (GZ)",
                "University of Washington",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05979.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#architecture",
                    "#multimodal",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "GPT-4o: Новый рубеж в унифицированной генерации изображений",
                    "desc": "Статья посвящена исследованию возможностей генерации изображений моделью GPT-4o. Авторы проводят эмпирическое сравнение GPT-4o с ведущими открытыми и коммерческими моделями в более чем 20 задачах генерации. Оценка охватывает четыре основные категории: текст-в-изображение, изображение-в-изображение, изображение-в-3D и изображение-в-X. На основе анализа выявляются сильные и слабые стороны GPT-4o в различных условиях, а также определяются перспективные направления для будущих унифицированных генеративных моделей."
                },
                "en": {
                    "title": "Unifying Image and Text Generation with GPT-4o",
                    "desc": "This paper explores the advancements in image generation, focusing on the capabilities of the GPT-4o model. It conducts a thorough evaluation of GPT-4o's performance in various generative tasks, including text-to-image and image-to-3D generation. The study benchmarks GPT-4o against other leading models, revealing its strengths and weaknesses in multimodal generation. The findings suggest future directions for improving unified generative architectures, particularly in terms of design and data utilization."
                },
                "zh": {
                    "title": "探索统一生成模型的未来方向",
                    "desc": "本文探讨了图像生成领域的最新进展，特别是GPT-4o模型在图像生成方面的能力。我们对其进行了实证研究，并与领先的开源和商业模型进行了基准测试，涵盖了文本到图像、图像到图像、图像到3D和图像到X生成等四个主要类别。分析结果揭示了GPT-4o在不同设置下的优缺点，并将其置于生成建模的更广泛演变中。通过这项研究，我们识别出未来统一生成模型的有希望的方向，强调了架构设计和数据扩展的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00043",
            "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation",
            "url": "https://huggingface.co/papers/2504.00043",
            "abstract": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.",
            "score": 2,
            "issue_id": 3137,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 марта",
                "en": "March 30",
                "zh": "3月30日"
            },
            "hash": "2b2bfdd590c5394d",
            "authors": [
                "Jixuan Leng",
                "Chengsong Huang",
                "Langlin Huang",
                "Bill Yuchen Lin",
                "William W. Cohen",
                "Haohan Wang",
                "Jiaxin Huang"
            ],
            "affiliations": [
                "CMU",
                "UIUC",
                "UW",
                "WUSTL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00043.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Кроссворды как инструмент оценки искусственного интеллекта",
                    "desc": "CrossWordBench - это новый метод оценки способностей больших языковых моделей (LLM) и больших визуально-языковых моделей (LVLM) к рассуждению. Он использует кроссворды как задачу, требующую мультимодального соблюдения семантических ограничений из текстовых подсказок и пересекающихся ограничений из визуальных структур сетки. Оценка более 20 моделей показала, что LLM с возможностями рассуждения значительно превосходят модели без таких возможностей. Результаты также выявили трудности LVLM с этой задачей, демонстрируя сильную корреляцию между их способностью решать головоломки и точностью разбора сетки."
                },
                "en": {
                    "title": "CrossWordBench: Evaluating Reasoning in LLMs and LVLMs with Crossword Puzzles",
                    "desc": "This paper presents CrossWordBench, a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) using crossword puzzles. The benchmark focuses on the interaction between text-based clues and visual grid structures, requiring models to adhere to both semantic and intersectional constraints. The study shows that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints, while LVLMs face challenges linked to their grid-parsing accuracy. Overall, the findings highlight the limitations of current models in reasoning tasks and suggest a novel approach for multimodal evaluation."
                },
                "zh": {
                    "title": "跨模态推理的新基准：CrossWordBench",
                    "desc": "现有的大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的推理评估框架主要集中在文本推理或视觉语言理解能力上，缺乏文本与视觉之间的动态互动。为了解决这个问题，我们提出了CrossWordBench，这是一个通过填字游戏评估LLMs和LVLMs推理能力的基准，要求在文本线索和视觉网格结构的语义约束下进行多模态推理。CrossWordBench利用可控的拼图生成框架，生成多种格式的拼图，并提供从直接解谜到互动模式的不同评估策略。我们的评估结果显示，推理能力强的LLMs在利用交叉字母约束方面显著优于非推理模型，而LVLMs在此任务中表现不佳，其解谜表现与网格解析准确性之间存在强相关性。"
                }
            }
        }
    ],
    "link_prev": "2025-04-08.html",
    "link_next": "2025-04-10.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4月8日"
    },
    "short_date_next": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4月10日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了变压器在生成一分钟视频时的挑战，因为自注意力层在长上下文中效率低下。作者尝试使用测试时训练（TTT）层，其隐藏状态本身可以是神经网络，从而更具表现力。将TTT层加入预训练的变压器后，可以从文字故事板生成一分钟视频。为了验证这一概念，作者根据《猫和老鼠》动画片编制了一个数据集。与其他基线方法相比，TTT层生成的视频更连贯，能够讲述复杂的故事。",
        "title": "One-Minute Video Generation with Test-Time Training",
        "pinyin": "这篇文章讨论了变压器在生成一分钟视频时的挑战，因为自注意力层在长上下文中效率低下。作者尝试使用测试时训练（TTT）层，其隐藏状态本身可以是神经网络，从而更具表现力。将TTT层加入预训练的变压器后，可以从文字故事板生成一分钟视频。为了验证这一概念，作者根据《猫和老鼠》动画片编制了一个数据集。与其他基线方法相比，TTT层生成的视频更连贯，能够讲述复杂的故事。\n\nzhè piān wén zhāng tǎo lùn le biàn yā qì zài shēng chéng yī fēn zhōng shì pǐn de cháo zhàn, yīn wèi zì zhù yì lì céng zài cháng shàng xià wén zhōng xiào lǜ dī xià. zuò zhě cháng shì shǐ yòng cè shì shí xùn liàn (TTT) céng, qí yǐn cáng zhuàng tài běn shēn kě yǐ shì shén jīng wǎng luò, cóng ér gèng jù biǎo xiàn lì. jiāng TTT céng jiā rù yù xùn liàn de biàn yā qì hòu, kě yǐ cóng wén zì gù shì bǎn shēng chéng yī fēn zhōng shì pǐn. wèi le yàn zhèng zhè yī gài niàn, zuò zhě gēn jù 《māo hé lǎo shǔ》 dòng huà piān biān zhì le yī gè shù jù jí. yǔ qí tā jī xiàn fāng fǎ biǎo bǐ, TTT céng shēng chéng de shì pǐn gèng lián gǔ, néng gòu jiǎng shù fú zà de gù shì.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"变压器\", \"pinyin\": \"biàn yā qì\", \"trans\": \"transformer\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"自注意力\", \"pinyin\": \"zì zhù yì lì\", \"trans\": \"self-attention\"},\n    {\"word\": \"层\", \"pinyin\": \"céng\", \"trans\": \"layer\"},\n    {\"word\": \"长\", \"pinyin\": \"cháng\", \"trans\": \"long\"},\n    {\"word\": \"上下文\", \"pinyin\": \"shàng xià wén\", \"trans\": \"context\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiào lǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"低下\", \"pinyin\": \"dī xià\", \"trans\": \"low\"},\n    {\"word\": \"尝试\", \"pinyin\": \"cháng shì\", \"trans\": \"attempt\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"测试时训练\", \"pinyin\": \"cè shì shí xùn liàn\", \"trans\": \"test-time training\"},\n    {\"word\": \"隐藏\", \"pinyin\": \"yǐn cáng\", \"trans\": \"hidden\"},\n    {\"word\": \"状态\", \"pinyin\": \"zhuàng tài\", \"trans\": \"state\"},\n    {\"word\": \"本身\", \"pinyin\": \"běn shēn\", \"trans\": \"itself\"},\n    {\"word\": \"神经网络\", \"pinyin\": \"shén jīng wǎng luò\", \"trans\": \"neural network\"},\n    {\"word\": \"表现力\", \"pinyin\": \"biǎo xiàn lì\", \"trans\": \"expressiveness\"},\n    {\"word\": \"加入\", \"pinyin\": \"jiā rù\", \"trans\": \"add\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-trained\"},\n    {\"word\": \"文字\", \"pinyin\": \"wén zì\", \"trans\": \"text\"},\n    {\"word\": \"故事板\", \"pinyin\": \"gù shì bǎn\", \"trans\": \"storyboard\"},\n    {\"word\": \"验证\", \"pinyin\": \"yàn zhèng\", \"trans\": \"validate\"},\n    {\"word\": \"概念\", \"pinyin\": \"gài niàn\", \"trans\": \"concept\"},\n    {\"word\": \"根据\", \"pinyin\": \"gēn jù\", \"trans\": \"based on\"},\n    {\"word\": \"《猫和老鼠》\", \"pinyin\": \"《māo hé lǎo shǔ》\", \"trans\": \"Tom and Jerry\"},\n    {\"word\": \"动画片\", \"pinyin\": \"dòng huà piàn\", \"trans\": \"cartoon\"},\n    {\"word\": \"编制\", \"pinyin\": \"biān zhì\", \"trans\": \"compile\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"基线\", \"pinyin\": \"jī xiàn\", \"trans\": \"baseline\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"相比\", \"pinyin\": \"xiāng bǐ\", \"trans\": \"compared to\"},\n    {\"word\": \"连贯\", \"pinyin\": \"lián guàn\", \"trans\": \"coherent\"},\n    {\"word\": \"讲述\", \"pinyin\": \"jiǎng shù\", \"trans\": \"narrate\"},\n    {\"word\": \"复杂\", \"pinyin\": \"fù zá\", \"trans\": \"complex\"}\n]",
        "trans": "This article discusses the challenges faced by transformers in generating a one-minute video due to the inefficiency of self-attention layers in long contexts. The authors attempt to use Test-Time Training (TTT) layers, whose hidden states can themselves be neural networks, making them more expressive. By incorporating TTT layers into pre-trained transformers, it is possible to generate a one-minute video from a textual storyboard. To validate this concept, the authors created a dataset based on the \"Tom and Jerry\" cartoon. Compared to other baseline methods, the videos generated by TTT layers are more coherent and capable of telling complex stories.",
        "update_ts": "2025-04-08 09:12"
    }
}