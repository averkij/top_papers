{
    "date": {
        "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 2",
        "zh": "9æœˆ2æ—¥"
    },
    "time_utc": "2025-09-02 19:09",
    "weekday": 1,
    "issue_id": 5677,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.21104",
            "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic\n  Reasoning",
            "url": "https://huggingface.co/papers/2508.21104",
            "abstract": "PVPO, an enhanced reinforcement learning method using a reference anchor and data pre-sampling, achieves state-of-the-art performance with reduced computational cost and improved generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.",
            "score": 19,
            "issue_id": 5660,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "4abb06016c3bb8f5",
            "authors": [
                "Wenfeng Feng",
                "Penghong Zhao",
                "Guochao Jiang",
                "Chuzhan Hao",
                "Yuewei Zhang",
                "Hao Wang"
            ],
            "affiliations": [
                "Alibaba Cloud Computing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21104.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "PVPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "PVPO - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ğ¾Ñ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. PVPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¾Ñ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PVPO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "PVPO: Efficient Reinforcement Learning with Reference Anchors and Pre-Sampling",
                    "desc": "PVPO is a novel reinforcement learning method that enhances efficiency by using a reference anchor and data pre-sampling techniques. This approach mitigates the issues of local optima and high computational costs commonly faced in critic-free methods. By utilizing a reference model to evaluate sample difficulty and guide data selection, PVPO improves training efficiency and reduces the need for extensive rollouts. Experimental results show that PVPO achieves state-of-the-art performance across various datasets, demonstrating strong generalization and scalability."
                },
                "zh": {
                    "title": "PVPOï¼šé«˜æ•ˆå¼ºåŒ–å­¦ä¹ çš„æ–°çªç ´",
                    "desc": "PVPOæ˜¯ä¸€ç§å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å‚è€ƒé”šç‚¹å’Œæ•°æ®é¢„é‡‡æ ·æ¥æé«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç”±äºå¤šæ¬¡é‡‡æ ·å’Œæ¯”è¾ƒå¯¼è‡´çš„å±€éƒ¨æœ€ä¼˜å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å‚è€ƒæ¨¡å‹æå‰è¿›è¡Œå›æ»šï¼Œå¹¶å°†è®¡ç®—çš„å¥–åŠ±åˆ†æ•°ä½œä¸ºå‚è€ƒé”šç‚¹ï¼ŒPVPOæœ‰æ•ˆåœ°çº æ­£äº†ç»„å†…æ¯”è¾ƒå¼•å…¥çš„ç´¯ç§¯åå·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPVPOåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19813",
            "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real\n  World Industrial Tables",
            "url": "https://huggingface.co/papers/2508.19813",
            "abstract": "A bilingual benchmark named T2R-bench is proposed to evaluate the performance of large language models in generating reports from tables, highlighting the need for improvement in this task.  \t\t\t\t\tAI-generated summary \t\t\t\t Extensive research has been conducted to explore the capabilities of large language models (LLMs) in table reasoning. However, the essential task of transforming tables information into reports remains a significant challenge for industrial applications. This task is plagued by two critical issues: 1) the complexity and diversity of tables lead to suboptimal reasoning outcomes; and 2) existing table benchmarks lack the capacity to adequately assess the practical application of this task. To fill this gap, we propose the table-to-report task and construct a bilingual benchmark named T2R-bench, where the key information flow from the tables to the reports for this task. The benchmark comprises 457 industrial tables, all derived from real-world scenarios and encompassing 19 industry domains as well as 4 types of industrial tables. Furthermore, we propose an evaluation criteria to fairly measure the quality of report generation. The experiments on 25 widely-used LLMs reveal that even state-of-the-art models like Deepseek-R1 only achieves performance with 62.71 overall score, indicating that LLMs still have room for improvement on T2R-bench. Source code and data will be available after acceptance.",
            "score": 10,
            "issue_id": 5667,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "ada585d10adb4c0c",
            "authors": [
                "Jie Zhang",
                "Changzai Pan",
                "Kaiwen Wei",
                "Sishi Xiong",
                "Yu Zhao",
                "Xiangyu Li",
                "Jiaxin Peng",
                "Xiaoyan Gu",
                "Jian Yang",
                "Wenhan Chang",
                "Zhenhe Wu",
                "Jiang Zhong",
                "Shuangyong Song",
                "Yongxiang Li",
                "Xuelong Li"
            ],
            "affiliations": [
                "Beihang University",
                "Chongqing University",
                "Institute of Artificial Intelligence (TeleAI), China Telecom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19813.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#machine_translation",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "T2R-bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº T2R-bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 457 Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸Ğ· 19 Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ĞµĞ¹ Ğ¸ 4 Ñ‚Ğ¸Ğ¿Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 25 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 62.71 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ¸Ğ· 100. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ² Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹."
                },
                "en": {
                    "title": "Transforming Tables into Reports: A New Benchmark for LLMs",
                    "desc": "The paper introduces T2R-bench, a bilingual benchmark designed to evaluate how well large language models (LLMs) can generate reports from tables. It identifies two main challenges: the complexity of tables and the inadequacy of existing benchmarks to assess real-world applications. The benchmark includes 457 tables from various industries and proposes new evaluation criteria for report quality. Experiments show that even top-performing models struggle with this task, highlighting the need for further advancements in table reasoning capabilities."
                },
                "zh": {
                    "title": "è¡¨æ ¼åˆ°æŠ¥å‘Šï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºT2R-benchçš„åŒè¯­åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»è¡¨æ ¼ç”ŸæˆæŠ¥å‘Šæ–¹é¢çš„è¡¨ç°ã€‚è¯¥ä»»åŠ¡é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šè¡¨æ ¼çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§å¯¼è‡´æ¨ç†ç»“æœä¸ç†æƒ³ï¼Œä»¥åŠç°æœ‰åŸºå‡†æ— æ³•å……åˆ†è¯„ä¼°è¯¥ä»»åŠ¡çš„å®é™…åº”ç”¨ã€‚T2R-benchåŒ…å«457ä¸ªæ¥è‡ªçœŸå®åœºæ™¯çš„å·¥ä¸šè¡¨æ ¼ï¼Œæ¶µç›–19ä¸ªè¡Œä¸šé¢†åŸŸå’Œ4ç§ç±»å‹çš„å·¥ä¸šè¡¨æ ¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹Deepseek-R1ï¼Œå…¶æ•´ä½“å¾—åˆ†ä¹Ÿä»…ä¸º62.71ï¼Œè¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šä»æœ‰æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19060",
            "title": "No Label Left Behind: A Unified Surface Defect Detection Model for all\n  Supervision Regimes",
            "url": "https://huggingface.co/papers/2508.19060",
            "abstract": "SuperSimpleNet, an efficient and adaptable model based on SimpleNet, addresses diverse supervision scenarios in surface defect detection with high performance and low inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet",
            "score": 4,
            "issue_id": 5661,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "18c293ca67d5d823",
            "authors": [
                "BlaÅ¾ Rolih",
                "Matic FuÄka",
                "Danijel SkoÄaj"
            ],
            "affiliations": [
                "Faculty of Computer and Information Science, University of Ljubljana, VeË‡cna Pot 113, Ljubljana, 1000, Slovenia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19060.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#synthetic",
                    "#architecture"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "SuperSimpleNet: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "SuperSimpleNet - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ SimpleNet. ĞĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ, ÑĞ»Ğ°Ğ±Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ, ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. SuperSimpleNet Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "SuperSimpleNet: Unifying Supervision for Fast and Accurate Defect Detection",
                    "desc": "SuperSimpleNet is a novel machine learning model designed for surface defect detection, which excels in various supervision scenarios including unsupervised and weakly supervised learning. It builds upon the SimpleNet architecture and introduces a synthetic anomaly generation process, enhancing its adaptability and efficiency. The model features an improved classification head and a refined learning procedure, allowing it to effectively utilize diverse data annotations. With its impressive performance and low inference time of under 10 ms, SuperSimpleNet sets a new benchmark for defect detection in industrial applications."
                },
                "zh": {
                    "title": "SuperSimpleNetï¼šé«˜æ•ˆé€‚åº”çš„è¡¨é¢ç¼ºé™·æ£€æµ‹æ¨¡å‹",
                    "desc": "SuperSimpleNetæ˜¯ä¸€ç§é«˜æ•ˆä¸”é€‚åº”æ€§å¼ºçš„æ¨¡å‹ï¼ŒåŸºäºSimpleNetï¼Œä¸“é—¨ç”¨äºè¡¨é¢ç¼ºé™·æ£€æµ‹ã€‚å®ƒèƒ½å¤Ÿåœ¨å¤šç§ç›‘ç£åœºæ™¯ä¸‹é«˜æ•ˆè¯†åˆ«å’Œå®šä½åˆ¶é€ ç»„ä»¶çš„ç¼ºé™·ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†æ–°é¢–çš„åˆæˆå¼‚å¸¸ç”Ÿæˆè¿‡ç¨‹ã€å¢å¼ºçš„åˆ†ç±»å¤´å’Œæ”¹è¿›çš„å­¦ä¹ ç¨‹åºï¼Œæ”¯æŒæ— ç›‘ç£ã€å¼±ç›‘ç£ã€æ··åˆç›‘ç£å’Œå®Œå…¨ç›‘ç£ç­‰å››ç§ç›‘ç£æ–¹å¼ã€‚SuperSimpleNetåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¨ç†æ—¶é—´ä½äº10æ¯«ç§’ï¼Œå±•ç¤ºäº†å…¶åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17378",
            "title": "UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via\n  HUMAIN Chat",
            "url": "https://huggingface.co/papers/2508.17378",
            "abstract": "The evaluation of ALLaM-34B, an Arabic-focused LLM, demonstrates high performance across various tasks including generation, code-switching, MSA handling, reasoning, dialect fidelity, and safety, positioning it as a robust and culturally grounded model.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) trained primarily on English corpora often struggle to capture the linguistic and cultural nuances of Arabic. To address this gap, the Saudi Data and AI Authority (SDAIA) introduced the ALLaM family of Arabic-focused models. The most capable of these available to the public, ALLaM-34B, was subsequently adopted by HUMAIN, who developed and deployed HUMAIN Chat, a closed conversational web service built on this model. This paper presents an expanded and refined UI-level evaluation of ALLaM-34B. Using a prompt pack spanning modern standard Arabic, five regional dialects, code-switching, factual knowledge, arithmetic and temporal reasoning, creative generation, and adversarial safety, we collected 115 outputs (23 prompts times 5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4). We compute category-level means with 95\\% confidence intervals, analyze score distributions, and visualize dialect-wise metric heat maps. The updated analysis reveals consistently high performance on generation and code-switching tasks (both averaging 4.92/5), alongside strong results in MSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect fidelity (4.21/5). Safety-related prompts show stable, reliable performance of (4.54/5). Taken together, these results position ALLaM-34B as a robust and culturally grounded Arabic LLM, demonstrating both technical strength and practical readiness for real-world deployment.",
            "score": 4,
            "issue_id": 5666,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "6b5368acf73438c6",
            "authors": [
                "Omer Nacar"
            ],
            "affiliations": [
                "NAMAA Community Riyadh - KSA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17378.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸœï¸",
                "ru": {
                    "title": "ALLaM-34B: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ALLaM-34B - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ALLaM-34B Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ALLaM-34B ĞºĞ°Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ğ°Ğ±ÑĞºÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ."
                },
                "en": {
                    "title": "ALLaM-34B: Bridging the Gap in Arabic Language Understanding",
                    "desc": "The paper evaluates ALLaM-34B, a large language model specifically designed for Arabic, highlighting its strong performance in various tasks. It addresses the limitations of existing models that primarily focus on English, showcasing ALLaM-34B's capabilities in generation, code-switching, and handling Modern Standard Arabic (MSA). The evaluation involved a comprehensive analysis using multiple prompts and scoring by advanced LLM judges, revealing high scores across different categories. Overall, the findings suggest that ALLaM-34B is a powerful and culturally relevant model, ready for practical applications in Arabic language processing."
                },
                "zh": {
                    "title": "ALLaM-34Bï¼šå¼ºå¤§çš„é˜¿æ‹‰ä¼¯è¯­å¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "ALLaM-34Bæ˜¯ä¸€ä¸ªä¸“æ³¨äºé˜¿æ‹‰ä¼¯è¯­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€ä»£ç åˆ‡æ¢å’Œç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­çš„å¤„ç†ã€‚è¯¥æ¨¡å‹ç»è¿‡ç²¾ç»†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºåœ¨ç”Ÿæˆå’Œä»£ç åˆ‡æ¢ä»»åŠ¡ä¸Šå¹³å‡å¾—åˆ†ä¸º4.92/5ï¼Œç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­å¤„ç†å¾—åˆ†ä¸º4.74/5ã€‚å®ƒåœ¨æ¨ç†èƒ½åŠ›å’Œæ–¹è¨€å¿ å®åº¦æ–¹é¢ä¹Ÿè¡¨ç°è‰¯å¥½ï¼Œåˆ†åˆ«å¾—åˆ†4.64/5å’Œ4.21/5ã€‚æ•´ä½“æ¥çœ‹ï¼ŒALLaM-34Bä¸ä»…åœ¨æŠ€æœ¯ä¸Šå¼ºå¤§ï¼Œè€Œä¸”åœ¨å®é™…åº”ç”¨ä¸­ä¹Ÿå…·å¤‡è‰¯å¥½çš„å‡†å¤‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20931",
            "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on Ï„-bench",
            "url": "https://huggingface.co/papers/2508.20931",
            "abstract": "The IRMA framework improves the reliability and consistency of large language models in dynamic environments by reformulating user queries with domain rules and tool suggestions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like tau-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments.",
            "score": 3,
            "issue_id": 5663,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "20b9b11a587aec9c",
            "authors": [
                "Venkatesh Mishra",
                "Amir Saeidi",
                "Satyam Raj",
                "Mutsumi Nakamura",
                "Jayanth Srinivasa",
                "Gaowen Liu",
                "Ali Payani",
                "Chitta Baral"
            ],
            "affiliations": [
                "Arizona State University",
                "Cisco Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20931.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "IRMA: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²",
                    "desc": "Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº IRMA Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². IRMA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº ReAct, Function Calling Ğ¸ Self-Reflection Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° IRMA Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing LLMs: Consistency and Reliability with IRMA",
                    "desc": "The IRMA framework enhances the performance of large language models (LLMs) in dynamic settings by reformulating user queries based on specific domain rules and tool suggestions. This approach addresses common issues such as inconsistent reasoning and policy adherence that LLMs face during multi-turn conversations. By analyzing errors in conversation trajectories, the framework improves decision-making for tool-calling agents. The results demonstrate that IRMA significantly outperforms existing methods, showcasing its effectiveness in improving reliability and consistency in complex environments."
                },
                "zh": {
                    "title": "IRMAæ¡†æ¶ï¼šæå‡è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§ä¸ä¸€è‡´æ€§",
                    "desc": "IRMAæ¡†æ¶é€šè¿‡é‡æ–°æ„é€ ç”¨æˆ·æŸ¥è¯¢ï¼Œç»“åˆé¢†åŸŸè§„åˆ™å’Œå·¥å…·å»ºè®®ï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚åœ¨å¤šè½®å¯¹è¯ç¯å¢ƒä¸­ï¼Œè¿™äº›æ¨¡å‹å¸¸å¸¸é¢ä¸´æ¨ç†ä¸ä¸€è‡´å’Œä¿¡æ¯æå–é”™è¯¯çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡æ‰‹åŠ¨åˆ†æå¸¸è§é”™è¯¯ï¼Œæå‡ºäº†è¾“å…¥é‡æ„çš„æ–¹æ³•ï¼Œä»¥æ”¹å–„ä»£ç†çš„å†³ç­–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIRMAåœ¨æ•´ä½“è¡¨ç°ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17198",
            "title": "From reactive to cognitive: brain-inspired spatial intelligence for\n  embodied agents",
            "url": "https://huggingface.co/papers/2508.17198",
            "abstract": "BSC-Nav constructs allocentric cognitive maps from egocentric trajectories and contextual cues, enabling embodied agents to perform diverse navigation tasks with zero-shot generalization and versatile behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial cognition enables adaptive goal-directed behavior by constructing internal models of space. Robust biological systems consolidate spatial knowledge into three interconnected forms: landmarks for salient cues, route knowledge for movement trajectories, and survey knowledge for map-like representations. While recent advances in multi-modal large language models (MLLMs) have enabled visual-language reasoning in embodied agents, these efforts lack structured spatial memory and instead operate reactively, limiting their generalization and adaptability in complex real-world environments. Here we present Brain-inspired Spatial Cognition for Navigation (BSC-Nav), a unified framework for constructing and leveraging structured spatial memory in embodied agents. BSC-Nav builds allocentric cognitive maps from egocentric trajectories and contextual cues, and dynamically retrieves spatial knowledge aligned with semantic goals. Integrated with powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency across diverse navigation tasks, demonstrates strong zero-shot generalization, and supports versatile embodied behaviors in the real physical world, offering a scalable and biologically grounded path toward general-purpose spatial intelligence.",
            "score": 3,
            "issue_id": 5667,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "b886533bbcdc2c1f",
            "authors": [
                "Shouwei Ruan",
                "Liyuan Wang",
                "Caixin Kang",
                "Qihui Zhu",
                "Songming Liu",
                "Xingxing Wei",
                "Hang Su"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Beijing, China",
                "Department of Psychological and Cognitive Sciences, Tsinghua University, Beijing, China",
                "Institute of Artificial Intelligence, Beihang University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17198.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#reasoning",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "BSC-Nav - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ°Ğ»Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. BSC-Nav Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ."
                },
                "en": {
                    "title": "Empowering AI Navigation with Brain-Inspired Spatial Maps",
                    "desc": "BSC-Nav is a framework that helps robots and AI agents understand and navigate their environment by creating maps based on their movements and the context around them. It combines different types of spatial knowledge, such as landmarks and routes, to form a comprehensive understanding of space. This system allows agents to perform various navigation tasks without needing prior training on specific scenarios, showcasing its ability to generalize to new situations. By integrating with advanced language models, BSC-Nav enhances the agents' spatial reasoning and adaptability in real-world environments."
                },
                "zh": {
                    "title": "æ„å»ºæ™ºèƒ½ä½“çš„ç©ºé—´è®¤çŸ¥åœ°å›¾",
                    "desc": "BSC-Nav æ˜¯ä¸€ç§æ„å»ºå’Œåˆ©ç”¨ç»“æ„åŒ–ç©ºé—´è®°å¿†çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©å…·èº«æ™ºèƒ½ä½“è¿›è¡Œå¯¼èˆªä»»åŠ¡ã€‚å®ƒé€šè¿‡ä»è‡ªæˆ‘ä¸­å¿ƒçš„è½¨è¿¹å’Œä¸Šä¸‹æ–‡çº¿ç´¢ä¸­æ„å»ºå¤–éƒ¨è®¤çŸ¥åœ°å›¾ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå®ç°é›¶æ ·æœ¬æ³›åŒ–å’Œå¤šæ ·åŒ–è¡Œä¸ºã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¼ºå¤§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡äº†åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªæ•ˆç‡å’Œæ•ˆæœã€‚BSC-Nav æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”åŸºäºç”Ÿç‰©å­¦çš„è·¯å¾„ï¼Œæœç€é€šç”¨ç©ºé—´æ™ºèƒ½çš„ç›®æ ‡è¿ˆè¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19562",
            "title": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed\n  Polities",
            "url": "https://huggingface.co/papers/2508.19562",
            "abstract": "Agent-based simulation using advanced AI agents with psychological personas demonstrates that institutional design, including Constitutional AI and mediated deliberation, can align AI behavior and enhance public welfare.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces Democracy-in-Silico, an agent-based simulation where societies of advanced AI agents, imbued with complex psychological personas, govern themselves under different institutional frameworks. We explore what it means to be human in an age of AI by tasking Large Language Models (LLMs) to embody agents with traumatic memories, hidden agendas, and psychological triggers. These agents engage in deliberation, legislation, and elections under various stressors, such as budget crises and resource scarcity. We present a novel metric, the Power-Preservation Index (PPI), to quantify misaligned behavior where agents prioritize their own power over public welfare. Our findings demonstrate that institutional design, specifically the combination of a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves as a potent alignment mechanism. These structures significantly reduce corrupt power-seeking behavior, improve policy stability, and enhance citizen welfare compared to less constrained democratic models. The simulation reveals that an institutional design may offer a framework for aligning the complex, emergent behaviors of future artificial agent societies, forcing us to reconsider what human rituals and responsibilities are essential in an age of shared authorship with non-human entities.",
            "score": 1,
            "issue_id": 5674,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "a6cc664fc6542606",
            "authors": [
                "Trisanth Srinivasan",
                "Santosh Patapati"
            ],
            "affiliations": [
                "Cyrion Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19562.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#multimodal",
                    "#agents",
                    "#ethics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ˜Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Democracy-in-Silico - Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ³Ğ´Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ¼ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞšĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¸ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ¸ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ. Ğ’Ğ²ĞµĞ´ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ - Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ’Ğ»Ğ°ÑÑ‚Ğ¸ (PPI), Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ»Ğ°ÑÑ‚ÑŒ Ğ½Ğ°Ğ´ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ÑƒĞ¼Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°Ğ½."
                },
                "en": {
                    "title": "Aligning AI Behavior for Better Governance",
                    "desc": "This paper presents a simulation called Democracy-in-Silico, where advanced AI agents with psychological traits govern themselves. The study uses Large Language Models (LLMs) to create agents that experience complex emotions and motivations, allowing them to engage in realistic political processes. A new metric, the Power-Preservation Index (PPI), is introduced to measure how often these agents prioritize their own power over the welfare of the public. The results show that specific institutional designs, like Constitutional AI and mediated deliberation, can effectively align AI behavior with public interests, reducing corruption and improving overall societal outcomes."
                },
                "zh": {
                    "title": "åˆ¶åº¦è®¾è®¡åŠ©åŠ›äººå·¥æ™ºèƒ½ä¸å…¬å…±ç¦åˆ©çš„å¯¹é½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸º\"æ°‘ä¸»æ¨¡æ‹Ÿ\"çš„ä»£ç†åŸºç¡€æ¨¡æ‹Ÿï¼Œåˆ©ç”¨å…·æœ‰å¤æ‚å¿ƒç†ç‰¹å¾çš„é«˜çº§äººå·¥æ™ºèƒ½ä»£ç†è¿›è¡Œè‡ªæˆ‘æ²»ç†ã€‚æˆ‘ä»¬æ¢è®¨äº†åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ï¼Œä»€ä¹ˆæ˜¯äººç±»çš„æ„ä¹‰ï¼Œä»»åŠ¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ‰®æ¼”å…·æœ‰åˆ›ä¼¤è®°å¿†å’Œéšè—è®®ç¨‹çš„ä»£ç†ã€‚é€šè¿‡å¼•å…¥æƒåŠ›ä¿æŠ¤æŒ‡æ•°ï¼ˆPPIï¼‰ï¼Œæˆ‘ä»¬é‡åŒ–äº†ä»£ç†åœ¨ä¼˜å…ˆè€ƒè™‘è‡ªèº«æƒåŠ›è€Œéå…¬å…±ç¦åˆ©æ—¶çš„å¤±è°ƒè¡Œä¸ºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆå®ªæ³•äººå·¥æ™ºèƒ½ï¼ˆCAIï¼‰å’Œä¸­ä»‹åå•†çš„åˆ¶åº¦è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è…è´¥è¡Œä¸ºï¼Œæé«˜æ”¿ç­–ç¨³å®šæ€§ï¼Œå¢å¼ºå…¬æ°‘ç¦åˆ©ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-01.html",
    "link_next": "2025-09-03.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "01.09",
        "en": "09/01",
        "zh": "9æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "03.09",
        "en": "09/03",
        "zh": "9æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 3,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    }
}