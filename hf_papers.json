{
    "date": {
        "ru": "10 января",
        "en": "January 10",
        "zh": "1月10日"
    },
    "time_utc": "2025-01-10 10:11",
    "weekday": 4,
    "issue_id": 1602,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.05453",
            "title": "An Empirical Study of Autoregressive Pre-training from Videos",
            "url": "https://huggingface.co/papers/2501.05453",
            "abstract": "We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at https://brjathu.github.io/toto/",
            "score": 13,
            "issue_id": 1596,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 января",
                "en": "January 9",
                "zh": "1月9日"
            },
            "hash": "3846ea8507d046be",
            "authors": [
                "Jathushan Rajasegaran",
                "Ilija Radosavovic",
                "Rahul Ravishankar",
                "Yossi Gandelsman",
                "Christoph Feichtenhofer",
                "Jitendra Malik"
            ],
            "affiliations": [
                "Meta FAIR",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05453.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#architecture",
                    "#robotics",
                    "#video",
                    "#cv"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Авторегрессионное предобучение видео: путь к универсальному компьютерному зрению",
                    "desc": "В статье исследуется авторегрессионное предобучение на видеоданных с использованием модели Toto. Авторы рассматривают видео как последовательности визуальных токенов и обучают трансформеры предсказывать будущие токены. Модели предобучаются на разнообразном наборе данных из более чем триллиона визуальных токенов. Результаты показывают, что такой подход дает конкурентоспособную производительность на различных задачах компьютерного зрения."
                },
                "en": {
                    "title": "Unlocking Video Understanding with Autoregressive Models",
                    "desc": "This paper investigates the use of autoregressive pre-training for video data through a series of models named Toto. The authors treat videos as sequences of visual tokens and employ transformer architectures to predict future tokens in these sequences. They pre-train their models on a massive dataset containing over 1 trillion visual tokens, exploring various design choices in architecture and training. The results show that these autoregressive models achieve strong performance on tasks like image recognition and video classification, indicating that scaling video models can yield similar benefits as seen in language models."
                },
                "zh": {
                    "title": "自回归预训练：视频模型的新突破",
                    "desc": "本文研究了视频的自回归预训练。我们构建了一系列名为Toto的自回归视频模型，将视频视为视觉标记的序列，并训练变换器模型以自回归方式预测未来的标记。我们的模型在一个包含超过1万亿视觉标记的多样化视频和图像数据集上进行预训练，并在多个下游任务上评估学习到的视觉表示。结果表明，尽管诱导偏差较小，自回归预训练在所有基准测试中表现出竞争力的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05441",
            "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
            "url": "https://huggingface.co/papers/2501.05441",
            "abstract": "There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, our new loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline -- R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.",
            "score": 13,
            "issue_id": 1596,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 января",
                "en": "January 9",
                "zh": "1月9日"
            },
            "hash": "eb1cd90c4d5cb0ef",
            "authors": [
                "Yiwen Huang",
                "Aaron Gokaslan",
                "Volodymyr Kuleshov",
                "James Tompkin"
            ],
            "affiliations": [
                "Brown University",
                "Cornell University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05441.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Упрощение и модернизация GAN: новый взгляд на обучение генеративных моделей",
                    "desc": "Исследователи опровергают распространенное мнение о сложности обучения генеративно-состязательных сетей (GAN). Они разработали новый регуляризованный релятивистский GAN-лосс, который решает проблемы потери мод и отсутствия сходимости. Авторы математически доказывают, что их лосс обеспечивает локальную сходимость, в отличие от большинства существующих релятивистских лоссов. На основе этого подхода они создали минималистичную базовую модель R3GAN, которая превосходит StyleGAN2 и другие современные GAN на нескольких наборах данных."
                },
                "en": {
                    "title": "Simplifying GAN Training with R3GAN: A New Era of Efficiency",
                    "desc": "This paper challenges the common belief that Generative Adversarial Networks (GANs) are inherently difficult to train. It introduces a new GAN loss function called the regularized relativistic GAN loss, which effectively addresses issues like mode dropping and non-convergence without relying on numerous empirical tricks. The authors provide mathematical analysis showing that their loss function guarantees local convergence, which is a significant improvement over existing methods. By applying this new loss to modern architectures like StyleGAN2, they create a simplified and efficient GAN model named R3GAN, which outperforms previous models on several benchmark datasets."
                },
                "zh": {
                    "title": "简化GAN训练，超越传统架构",
                    "desc": "这篇论文探讨了生成对抗网络（GAN）训练的难点，并提出了一种新的方法来简化这一过程。作者提出了一种正则化的相对GAN损失函数，解决了模式丢失和非收敛的问题。通过数学分析，证明了这种损失函数具有局部收敛的保证，优于现有的相对损失函数。最终，作者展示了一个新的简约基线R3GAN，其在多个数据集上的表现超过了StyleGAN2，并与最先进的GAN和扩散模型相媲美。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04003",
            "title": "Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives",
            "url": "https://huggingface.co/papers/2501.04003",
            "abstract": "Recent advancements in Vision-Language Models (VLMs) have sparked interest in their use for autonomous driving, particularly in generating interpretable driving decisions through natural language. However, the assumption that VLMs inherently provide visually grounded, reliable, and interpretable explanations for driving remains largely unexamined. To address this gap, we introduce DriveBench, a benchmark dataset designed to evaluate VLM reliability across 17 settings (clean, corrupted, and text-only inputs), encompassing 19,200 frames, 20,498 question-answer pairs, three question types, four mainstream driving tasks, and a total of 12 popular VLMs. Our findings reveal that VLMs often generate plausible responses derived from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs. This behavior, concealed by dataset imbalances and insufficient evaluation metrics, poses significant risks in safety-critical scenarios like autonomous driving. We further observe that VLMs struggle with multi-modal reasoning and display heightened sensitivity to input corruptions, leading to inconsistencies in performance. To address these challenges, we propose refined evaluation metrics that prioritize robust visual grounding and multi-modal understanding. Additionally, we highlight the potential of leveraging VLMs' awareness of corruptions to enhance their reliability, offering a roadmap for developing more trustworthy and interpretable decision-making systems in real-world autonomous driving contexts. The benchmark toolkit is publicly accessible.",
            "score": 6,
            "issue_id": 1599,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 января",
                "en": "January 7",
                "zh": "1月7日"
            },
            "hash": "720b493a608f478a",
            "authors": [
                "Shaoyuan Xie",
                "Lingdong Kong",
                "Yuhao Dong",
                "Chonghao Sima",
                "Wenwei Zhang",
                "Qi Alfred Chen",
                "Ziwei Liu",
                "Liang Pan"
            ],
            "affiliations": [
                "National University of Singapore",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory",
                "The University of Hong Kong",
                "University of California, Irvine"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04003.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#interpretability",
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Проверка надёжности VLM для безопасного автономного вождения",
                    "desc": "Статья представляет DriveBench - набор данных для оценки надёжности мультимодальных языковых моделей (VLM) в контексте автономного вождения. Исследование выявило, что VLM часто генерируют правдоподобные ответы на основе общих знаний, а не визуальной информации, что опасно в критически важных сценариях. Авторы предлагают усовершенствованные метрики оценки, ориентированные на надёжную визуальную привязку и мультимодальное понимание. Также отмечается потенциал использования осведомленности VLM о искажениях для повышения их надёжности."
                },
                "en": {
                    "title": "Enhancing Trust in Vision-Language Models for Safer Autonomous Driving",
                    "desc": "This paper discusses the limitations of Vision-Language Models (VLMs) in the context of autonomous driving, particularly their ability to provide reliable and interpretable driving decisions. The authors introduce DriveBench, a comprehensive benchmark dataset that tests VLM performance across various conditions, including clean and corrupted inputs. Their research shows that VLMs often rely on general knowledge rather than true visual understanding, especially when visual data is compromised. To improve VLM reliability, the paper suggests new evaluation metrics focused on visual grounding and multi-modal reasoning, aiming to enhance the safety of autonomous driving systems."
                },
                "zh": {
                    "title": "提升自动驾驶决策的可靠性与可解释性",
                    "desc": "本文介绍了DriveBench，一个用于评估视觉语言模型（VLMs）在自动驾驶中可靠性的基准数据集。该数据集包含19200帧图像和20498个问答对，涵盖了多种驾驶任务和输入类型。研究发现，VLMs在处理受损或缺失的视觉输入时，往往依赖于一般知识而非真实的视觉信息，导致安全隐患。为了解决这些问题，本文提出了改进的评估指标，强调视觉基础和多模态理解的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04377",
            "title": "On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis",
            "url": "https://huggingface.co/papers/2501.04377",
            "abstract": "Recently, Visual Autoregressive (VAR) Models introduced a groundbreaking advancement in the field of image generation, offering a scalable approach through a coarse-to-fine \"next-scale prediction\" paradigm. However, the state-of-the-art algorithm of VAR models in [Tian, Jiang, Yuan, Peng and Wang, NeurIPS 2024] takes O(n^4) time, which is computationally inefficient. In this work, we analyze the computational limits and efficiency criteria of VAR Models through a fine-grained complexity lens. Our key contribution is identifying the conditions under which VAR computations can achieve sub-quadratic time complexity. Specifically, we establish a critical threshold for the norm of input matrices used in VAR attention mechanisms. Above this threshold, assuming the Strong Exponential Time Hypothesis (SETH) from fine-grained complexity theory, a sub-quartic time algorithm for VAR models is impossible. To substantiate our theoretical findings, we present efficient constructions leveraging low-rank approximations that align with the derived criteria. This work initiates the study of the computational efficiency of the VAR model from a theoretical perspective. Our technique will shed light on advancing scalable and efficient image generation in VAR frameworks.",
            "score": 5,
            "issue_id": 1597,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 января",
                "en": "January 8",
                "zh": "1月8日"
            },
            "hash": "be8a0f20db676680",
            "authors": [
                "Yekun Ke",
                "Xiaoyu Li",
                "Yingyu Liang",
                "Zhizhou Sha",
                "Zhenmei Shi",
                "Zhao Song"
            ],
            "affiliations": [
                "The Simons Institute for the Theory of Computing at UC Berkeley",
                "The University of Hong Kong",
                "Tsinghua University",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04377.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Преодоление вычислительных барьеров в VAR моделях",
                    "desc": "Статья исследует вычислительные ограничения и критерии эффективности Визуальных Авторегрессионных (VAR) моделей с точки зрения тонкой теории сложности. Авторы определяют условия, при которых вычисления VAR могут достичь субквадратичной временной сложности. Они устанавливают критический порог для нормы входных матриц, используемых в механизмах внимания VAR, выше которого невозможен субкварцевый алгоритм времени для моделей VAR. Представлены эффективные конструкции, использующие аппроксимации низкого ранга, которые соответствуют выведенным критериям."
                },
                "en": {
                    "title": "Unlocking Efficiency in Image Generation with VAR Models",
                    "desc": "This paper explores the computational efficiency of Visual Autoregressive (VAR) Models, which are used for generating images. The authors identify that the current state-of-the-art VAR algorithm is computationally expensive, operating in O(n^4) time complexity. They establish conditions under which VAR computations can be optimized to achieve sub-quadratic time complexity, particularly focusing on the input matrix norms in the attention mechanisms. By applying low-rank approximations, the authors provide practical constructions that meet their theoretical criteria, paving the way for more efficient image generation techniques in VAR frameworks."
                },
                "zh": {
                    "title": "提升VAR模型的计算效率",
                    "desc": "最近，视觉自回归（VAR）模型在图像生成领域取得了突破性进展，采用粗到细的“下一个尺度预测”范式。然而，VAR模型的最新算法在计算上效率低下，时间复杂度为O(n^4)。本文通过细粒度复杂性分析，探讨了VAR模型的计算限制和效率标准。我们确定了VAR计算可以实现亚二次时间复杂度的条件，并提出了利用低秩近似的高效构造，以支持我们的理论发现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03489",
            "title": "Entropy-Guided Attention for Private LLMs",
            "url": "https://huggingface.co/papers/2501.03489",
            "abstract": "The pervasiveness of proprietary language models has raised critical privacy concerns, necessitating advancements in private inference (PI), where computations are performed directly on encrypted data without revealing users' sensitive information. While PI offers a promising solution, its practical deployment is hindered by substantial communication and latency overheads, primarily stemming from nonlinear operations. To address this, we introduce an information-theoretic framework to characterize the role of nonlinearities in decoder-only language models, laying a principled foundation for optimizing transformer-architectures tailored to the demands of PI.   By leveraging Shannon's entropy as a quantitative measure, we uncover the previously unexplored dual significance of nonlinearities: beyond ensuring training stability, they are crucial for maintaining attention head diversity. Specifically, we find that their removal triggers two critical failure modes: {\\em entropy collapse} in deeper layers that destabilizes training, and {\\em entropic overload} in earlier layers that leads to under-utilization of Multi-Head Attention's (MHA) representational capacity.   We propose an entropy-guided attention mechanism paired with a novel entropy regularization technique to mitigate entropic overload. Additionally, we explore PI-friendly alternatives to layer normalization for preventing entropy collapse and stabilizing the training of LLMs with reduced-nonlinearities. Our study bridges the gap between information theory and architectural design, establishing entropy dynamics as a principled guide for developing efficient PI architectures. The code and implementation are available at https://github.com/Nandan91/entropy-guided-attention-llm{entropy-guided-llm}.",
            "score": 3,
            "issue_id": 1597,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 января",
                "en": "January 7",
                "zh": "1月7日"
            },
            "hash": "18abcfb3fe1b209b",
            "authors": [
                "Nandan Kumar Jha",
                "Brandon Reagen"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03489.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#inference",
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "Энтропия как ключ к конфиденциальным языковым моделям",
                    "desc": "Статья рассматривает проблему конфиденциальности при использовании языковых моделей и предлагает решение через частное вычисление (PI). Авторы представляют информационно-теоретическую основу для оптимизации архитектур трансформеров под задачи PI, используя энтропию Шеннона как количественную меру. Исследование выявляет двойную роль нелинейностей в моделях: обеспечение стабильности обучения и поддержание разнообразия в механизме внимания. Предложен энтропийно-управляемый механизм внимания и новая техника регуляризации энтропии для улучшения эффективности PI-архитектур."
                },
                "en": {
                    "title": "Optimizing Language Models for Privacy with Entropy Dynamics",
                    "desc": "This paper addresses privacy concerns related to proprietary language models by focusing on private inference (PI), which allows computations on encrypted data. The authors introduce an information-theoretic framework to analyze the impact of nonlinearities in decoder-only language models, which are essential for optimizing transformer architectures for PI. They identify two critical issues caused by the removal of nonlinearities: entropy collapse in deeper layers and entropic overload in earlier layers, both of which affect training stability and attention mechanisms. To resolve these issues, the paper proposes an entropy-guided attention mechanism and explores alternatives to layer normalization, aiming to enhance the efficiency of PI architectures while maintaining model performance."
                },
                "zh": {
                    "title": "优化私密推理架构的熵动态",
                    "desc": "本论文探讨了在加密数据上进行私密推理（PI）时，非线性操作对解码器语言模型的影响。我们提出了一种信息论框架，帮助优化适合PI需求的变换器架构。研究发现，非线性不仅确保了训练的稳定性，还对注意力头的多样性至关重要。为了解决熵崩溃和熵过载问题，我们提出了一种基于熵的注意力机制和新的熵正则化技术。"
                }
            }
        }
    ],
    "link_prev": "2025-01-09.html",
    "link_next": "2025-01-13.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "09.01",
        "en": "01/09",
        "zh": "1月9日"
    },
    "short_date_next": {
        "ru": "13.01",
        "en": "01/13",
        "zh": "1月13日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们通过实验研究了从视频中进行自回归预训练的方法。为了进行研究，我们构建了一系列自回归视频模型，称为Toto。我们将视频视为视觉标记的序列，并训练变压器模型自回归地预测未来的标记。我们的模型在包含超过1万亿视觉标记的多样化视频和图像数据集上进行了预训练。我们探索了不同的架构、训练和推理设计选择。我们在包括图像识别、视频分类、目标跟踪和机器人技术在内的多个下游任务上评估了学习到的视觉表示。我们的结果表明，尽管存在最小的归纳偏差，自回归预训练在所有基准测试中都表现出竞争力。最后，我们发现扩展我们的视频模型会产生类似于语言模型的扩展曲线，只是速率不同。更多细节请访问 https://brjathu.github.io/toto/。",
        "title": "An Empirical Study of Autoregressive Pre-training from Videos",
        "pinyin": "Wǒmen tōngguò shíyàn yánjiūle cóng shìpín zhōng jìnxíng zìhuíguī yùxùnliàn de fāngfǎ. Wèile jìnxíng yánjiū, wǒmen gòujiànle yī xìliè zìhuíguī shìpín móxíng, chēngwéi Toto. Wǒmen jiāng shìpín shìwéi shìjué biāojì de xùliè, bìng xùnliàn biànshùqì móxíng zìhuíguī de yùcè wèilái de biāojì. Wǒmen de móxíng zài bāohán chāoguò 1 wàn yì shìjué biāojì de duōyànghuà shìpín hé túxiàng shùjùjí shàng jìnxíngle yùxùnliàn. Wǒmen tuànsuǒle bùtóng de jiàgòu, xùnliàn hé tuīlǐ shèjì xuǎnzé. Wǒmen zài bāokuò túxiàng shíbié, shìpín fēnlèi, mùbiāo gēnzōng hé jīqìrén jìshù zài nèi de duōgè xiàyóu rènwù shàng pínggūle xuéxí dào de shìjué biǎoshì. Wǒmen de jiéguǒ biǎomíng, jīnshǐ yǒu zuìshǎo de guīnà piānchá, zìhuíguī yùxùnliàn zài suǒyǒu jīzhǔn cèshì zhōng dōu biǎoxiàn chū jìngzhēnglì. Zuìhòu, wǒmen fāxiàn kuòzhǎn wǒmen de shìpín móxíng huì chǎnshēng xiàng yǔyán móxíng de kuòzhǎn qǔxiàn, zhǐshì sùlǜ bùtóng. Gèng duō xìjiè qǐng fǎngwèn https://brjathu.github.io/toto/。",
        "vocab": "[{'word': '自回归', 'pinyin': 'zì huí guī', 'trans': 'autoregressive'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pretraining'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'token'}, {'word': '变压器', 'pinyin': 'biàn yā qì', 'trans': 'transformer'}, {'word': '多样化', 'pinyin': 'duō yàng huà', 'trans': 'diversified'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '归纳', 'pinyin': 'guī nà', 'trans': 'inductive'}, {'word': '偏差', 'pinyin': 'piān chā', 'trans': 'bias'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '竞争力', 'pinyin': 'jìng zhēng lì', 'trans': 'competitive'}, {'word': '扩展', 'pinyin': 'kuò zhǎn', 'trans': 'scaling'}, {'word': '曲线', 'pinyin': 'qǔ xiàn', 'trans': 'curve'}, {'word': '速率', 'pinyin': 'sù lǜ', 'trans': 'rate'}]",
        "trans": "We investigated methods for autoregressive pretraining from videos through experimental research. To conduct the study, we constructed a series of autoregressive video models called Toto. We treated videos as sequences of visual tokens and trained transformer models to autoregressively predict future tokens. Our models were pretrained on diverse video and image datasets containing over 1 trillion visual tokens. We explored various architectural, training, and inference design choices. We evaluated the learned visual representations on multiple downstream tasks, including image recognition, video classification, object tracking, and robotics. Our results indicate that, despite minimal inductive bias, autoregressive pretraining performs competitively on all benchmarks. Finally, we found that scaling our video models produces scaling curves similar to those of language models, albeit at different rates. For more details, please visit https://brjathu.github.io/toto/.",
        "update_ts": "2025-01-10 09:11"
    }
}