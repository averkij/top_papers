{
    "date": {
        "ru": "7 июля",
        "en": "July 7",
        "zh": "7月7日"
    },
    "time_utc": "2025-07-07 11:10",
    "weekday": 0,
    "issue_id": 4678,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.01853",
            "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
            "url": "https://huggingface.co/papers/2507.01853",
            "abstract": "EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA initiative (https://eka.soket.ai), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
            "score": 2,
            "issue_id": 4672,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "a397a0d71f721623",
            "authors": [
                "Samridhi Raj Sinha",
                "Rajvee Sheth",
                "Abhishek Upperwal",
                "Mayank Singh"
            ],
            "affiliations": [
                "Indian Institute of Technology Gandhinagar",
                "LINGO Research Group",
                "NMIMS",
                "Soket AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01853.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🌏",
                "ru": {
                    "title": "EKA-EVAL: Универсальная платформа для оценки многоязычных языковых моделей",
                    "desc": "EKA-EVAL - это комплексная многоязычная система оценки больших языковых моделей (LLM), поддерживающая разнообразные бенчмарки. Она включает более 35 тестов, в том числе 10 наборов данных для индийских языков, охватывающих такие категории, как рассуждение, математика, использование инструментов и понимание длинного контекста. EKA-EVAL предлагает встроенную поддержку распределенного вывода, квантизации и использования нескольких GPU. Эта система позиционируется как первый комплексный инструмент оценки, адаптированный как для глобальных, так и для индийских LLM."
                },
                "en": {
                    "title": "Empowering Multilingual Evaluation for Large Language Models",
                    "desc": "EKA-EVAL is a multilingual evaluation framework designed for large language models (LLMs), focusing on diverse linguistic needs, particularly in regions like India. It includes over 35 benchmarks, with specific datasets for Indic languages, covering various tasks such as reasoning and reading comprehension. The framework supports efficient distributed inference and multi-GPU usage, making it suitable for production environments. EKA-EVAL aims to lower the barriers for multilingual benchmarking and is part of a larger initiative to expand its benchmark offerings."
                },
                "zh": {
                    "title": "EKA-EVAL：多语言模型评估的新标准",
                    "desc": "EKA-EVAL是一个全面的多语言评估框架，专为大型语言模型设计。它支持多种基准测试和功能，能够高效地进行分布式推理和GPU使用。该框架整合了超过35个基准，包括10个特定于印度的数据库，涵盖推理、数学、工具使用、长文本理解和阅读理解等类别。EKA-EVAL是首个为全球和印度语言模型量身定制的端到端可扩展评估套件，显著降低了多语言基准测试的门槛。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01955",
            "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks",
            "url": "https://huggingface.co/papers/2507.01955",
            "abstract": "Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments.",
            "score": 0,
            "issue_id": 4676,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "650bb4b7601b21e0",
            "authors": [
                "Rahul Ramachandran",
                "Ali Garjani",
                "Roman Bachmann",
                "Andrei Atanov",
                "Oğuzhan Fatih Kar",
                "Amir Zamir"
            ],
            "affiliations": [
                "Swiss Federal Institute of Technology Lausanne (EPFL)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01955.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#multimodal",
                    "#games",
                    "#cv",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Универсальность мультимодальных моделей в компьютерном зрении: потенциал и ограничения",
                    "desc": "Статья исследует эффективность мультимодальных фундаментальных моделей в различных задачах компьютерного зрения. Авторы разработали стандартизированную систему оценки, используя цепочки промптов для адаптации моделей к задачам, которые обычно требуют специализированных выходных данных. Результаты показывают, что хотя эти модели уступают специализированным решениям, они демонстрируют достойную производительность в качестве универсальных инструментов. Исследование также выявило, что модели лучше справляются с семантическими задачами, чем с геометрическими, а GPT-4o показала наилучшие результаты среди нерассуждающих моделей."
                },
                "en": {
                    "title": "Benchmarking Multimodal Models: Generalists in Vision Tasks",
                    "desc": "This paper evaluates the performance of multimodal foundation models on various computer vision tasks, such as semantic segmentation and object detection. Despite their training primarily on image-text tasks, these models show respectable generalist capabilities but do not match the performance of specialized models. The authors introduce a benchmarking framework that translates vision tasks into text-promptable formats to address challenges related to model accessibility and output limitations. Findings indicate that while these models excel in semantic tasks, they struggle with geometric tasks, and the best-performing model, GPT-4o, leads in several categories despite some quirks in image generation."
                },
                "zh": {
                    "title": "多模态模型的视觉任务表现评估",
                    "desc": "多模态基础模型主要在图像-文本任务上训练，但在适应性提示链的帮助下，在各种视觉任务上表现出色。本文对多种流行的多模态基础模型在标准计算机视觉任务上的表现进行了基准测试，发现这些模型在语义任务上表现优于几何任务。尽管它们在任何任务上都未能接近专业模型的水平，但作为通用模型，它们的表现仍然令人瞩目。我们提出了一种标准化的基准框架，通过将标准视觉任务转换为可通过提示链和API兼容的任务来解决模型适应性的问题。"
                }
            }
        }
    ],
    "link_prev": "2025-07-04.html",
    "link_next": "2025-07-08.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "04.07",
        "en": "07/04",
        "zh": "7月4日"
    },
    "short_date_next": {
        "ru": "08.07",
        "en": "07/08",
        "zh": "7月8日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}