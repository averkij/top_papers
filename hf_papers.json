{
    "date": {
        "ru": "13 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 13",
        "zh": "11æœˆ13æ—¥"
    },
    "time_utc": "2024-11-13 07:09",
    "weekday": 2,
    "issue_id": 544,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07184",
            "title": "SAMPart3D: Segment Any Part in 3D Objects",
            "url": "https://huggingface.co/papers/2411.07184",
            "abstract": "3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.",
            "score": 10,
            "issue_id": 541,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "b4e58a99e4a7e86c",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "SAMPart3D: Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SAMPart3D - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±ĞµĞ·Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SAMPart3D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with SAMPart3D",
                    "desc": "This paper presents SAMPart3D, a novel framework for zero-shot 3D part segmentation that does not depend on predefined text prompts. It utilizes text-agnostic vision foundation models to extract 3D features, enabling it to scale effectively to large unlabeled datasets. The framework also incorporates scale-conditioned part-aware features, allowing for segmentation at various levels of detail. SAMPart3D outperforms existing methods and introduces a new benchmark to enhance the diversity and complexity of 3D part segmentation tasks."
                },
                "zh": {
                    "title": "SAMPart3Dï¼šæ— æ–‡æœ¬æç¤ºçš„3Déƒ¨ä»¶åˆ†å‰²æ–°æ¡†æ¶",
                    "desc": "3Déƒ¨ä»¶åˆ†å‰²æ˜¯3Dæ„ŸçŸ¥ä¸­çš„ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºæœºå™¨äººæŠ€æœ¯ã€3Dç”Ÿæˆå’Œ3Dç¼–è¾‘ç­‰é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†SAMPart3Dæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ä¾èµ–é¢„å®šä¹‰æ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹ï¼Œå¯¹ä»»æ„3Då¯¹è±¡è¿›è¡Œå¤šç²’åº¦çš„è¯­ä¹‰éƒ¨ä»¶åˆ†å‰²ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ— æ–‡æœ¬ä¾èµ–çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œä»å¤§è§„æ¨¡æœªæ ‡è®°çš„3Dæ•°æ®é›†ä¸­æå–ä¸°å¯Œçš„3Dç‰¹å¾ï¼Œå¹¶é€šè¿‡æ¡ä»¶åŒ–çš„éƒ¨ä»¶æ„ŸçŸ¥ç‰¹å¾å®ç°çµæ´»çš„åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAMPart3Dåœ¨å¤„ç†å¤æ‚å¯¹è±¡æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²æ–¹æ³•ï¼Œå¹¶èƒ½æ”¯æŒå¤šç§åº”ç”¨ï¼Œå¦‚éƒ¨ä»¶çº§ç¼–è¾‘å’Œäº¤äº’å¼åˆ†å‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07975",
            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2411.07975",
            "abstract": "We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.",
            "score": 2,
            "issue_id": 544,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 12",
                "zh": "11æœˆ12æ—¥"
            },
            "hash": "294dc65a01cd1218",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#architecture",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "JanusFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ rectified flow Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ rectified flow Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "JanusFlow: Unifying Image Understanding and Generation Efficiently",
                    "desc": "JanusFlow is a novel framework that combines image understanding and generation into one cohesive model. It utilizes a simple architecture that merges autoregressive language models with rectified flow, enhancing generative modeling capabilities. The study reveals that rectified flow can be effectively trained within the large language model context without needing complex changes to the architecture. By decoupling the encoders for understanding and generation and aligning their representations during training, JanusFlow demonstrates superior performance compared to specialized models and existing unified approaches."
                },
                "zh": {
                    "title": "JanusFlowï¼šå›¾åƒç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "JanusFlowæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œå°†å›¾åƒç†è§£å’Œç”Ÿæˆç»Ÿä¸€åœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ã€‚å®ƒé‡‡ç”¨äº†ç®€çº¦çš„æ¶æ„ï¼Œå°†è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸ä¿®æ­£æµç»“åˆï¼Œè¿™æ˜¯ç”Ÿæˆå»ºæ¨¡ä¸­çš„ä¸€ç§å…ˆè¿›æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œä¿®æ­£æµå¯ä»¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶å†…è½»æ¾è®­ç»ƒï¼Œæ— éœ€å¤æ‚çš„æ¶æ„ä¿®æ”¹ã€‚é€šè¿‡è§£è€¦ç†è§£å’Œç”Ÿæˆç¼–ç å™¨ä»¥åŠåœ¨ç»Ÿä¸€è®­ç»ƒä¸­å¯¹é½å®ƒä»¬çš„è¡¨ç¤ºï¼ŒJanusFlowåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç»Ÿä¸€æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-12.html",
    "link_next": "2024-11-14.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "14.11",
        "en": "11/14",
        "zh": "11æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºAdd-itçš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨å›¾åƒä¸­æ ¹æ®æ–‡æœ¬æŒ‡ä»¤æ·»åŠ ç‰©ä½“ã€‚Add-itåˆ©ç”¨æ‰©å±•çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆåœºæ™¯å›¾åƒã€æ–‡æœ¬æç¤ºå’Œç”Ÿæˆå›¾åƒæœ¬èº«çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸éœ€è¦ä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒæƒ…å†µä¸‹ï¼Œä¿æŒç»“æ„ä¸€è‡´æ€§å’Œç»†èŠ‚ï¼Œå¹¶ç¡®ä¿ç‰©ä½“è‡ªç„¶æ”¾ç½®ã€‚Add-itåœ¨çœŸå®å’Œç”Ÿæˆå›¾åƒæ’å…¥åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ä¸­èƒœå‡ºè¶…è¿‡80%çš„æ¡ˆä¾‹ã€‚",
        "title": "Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models",
        "pinyin": "zhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ¨i Add-it de xÄ«n fÄng fÇ, yÃ²ng yÃº zÃ i tÃº xiÃ ng zhÅng gÄ“n jÃ¹ wÃ©n bÄ›n zhÇ lÇng tiÄn jiÄ wÃ¹ tÇ. Add-it lÃ¬ yÃ²ng kuÃ² zhÇn de zhÃ¹ yÃ¬ jÄ« zhÃ¬, jiÃ© hÃ© chÇng jÇng tÃº xiÃ ng, wÃ©n bÄ›n tÃ­ shÃ¬ hÃ© shÄ“ng chÃ©ng tÃº xiÃ ng bÄ›n shÄ“n de xÃ¬n xÄ«. zhÃ¨ zhÇ’ng fÄng fÇ zÃ i bÃ¹ xÅ« yÃ o rÃ¨n wÃ¹ tÃ¨ dÃ¬ng de wÄ“i tiÃ¡o qÃ­ng kuÃ ng xiÃ , bÇo chÃ­ jiÄ“ gÃ²u yÄ« zhÃ¬ xÃ¬ng hÃ© xÃ¬ jiÃ©, bÃ¬ng quÃ¨ shÃ­ Add-it zÃ i zhÄ“n shÃ­ hÃ© shÄ“ng chÃ©ng tÃº xiÃ ng chÄ rÃ¹ jÄ« zhÇ”n shÃ ng quÇn dÃ© le zuÃ¬ xiÄn jÃ¬n de jiÃ© guÇ’, bÃ¬ng zÃ i rÃ©n lÃ¨i pÃ­ng guÄ zhÅng shÃ¨ng chÅ« chÄo guÃ² 80% de Ã n lÃ¬.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"åä¸º\", \"pinyin\": \"mÃ­ngwÃ©i\", \"trans\": \"named\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"æ ¹æ®\", \"pinyin\": \"gÄ“njÃ¹\", \"trans\": \"according to\"},\n    {\"word\": \"æŒ‡ä»¤\", \"pinyin\": \"zhÇlÃ¬ng\", \"trans\": \"instruction\"},\n    {\"word\": \"æ·»åŠ \", \"pinyin\": \"tiÄnjiÄ\", \"trans\": \"add\"},\n    {\"word\": \"ç‰©ä½“\", \"pinyin\": \"wÃ¹tÇ\", \"trans\": \"object\"},\n    {\"word\": \"åˆ©ç”¨\", \"pinyin\": \"lÃ¬yÃ²ng\", \"trans\": \"utilize\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ²zhÇn\", \"trans\": \"extend\"},\n    {\"word\": \"æ³¨æ„åŠ›\", \"pinyin\": \"zhÃ¹yÃ¬lÃ¬\", \"trans\": \"attention\"},\n    {\"word\": \"æœºåˆ¶\", \"pinyin\": \"jÄ«zhÃ¬\", \"trans\": \"mechanism\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ©hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"åœºæ™¯\", \"pinyin\": \"chÇngjÇng\", \"trans\": \"scene\"},\n    {\"word\": \"æç¤º\", \"pinyin\": \"tÃ­shÃ¬\", \"trans\": \"prompt\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"æœ¬èº«\", \"pinyin\": \"bÄ›nshÄ“n\", \"trans\": \"itself\"},\n    {\"word\": \"ä¿¡æ¯\", \"pinyin\": \"xÃ¬nxÄ«\", \"trans\": \"information\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨nwÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"ç‰¹å®š\", \"pinyin\": \"tÃ¨dÃ¬ng\", \"trans\": \"specific\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“itiÃ¡o\", \"trans\": \"fine-tune\"},\n    {\"word\": \"æƒ…å†µ\", \"pinyin\": \"qÃ­ngkuÃ ng\", \"trans\": \"situation\"},\n    {\"word\": \"ä¿æŒ\", \"pinyin\": \"bÇochÃ­\", \"trans\": \"maintain\"},\n    {\"word\": \"ä¸€è‡´æ€§\", \"pinyin\": \"yÄ«zhÃ¬xÃ¬ng\", \"trans\": \"consistency\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬jiÃ©\", \"trans\": \"detail\"},\n    {\"word\": \"ç¡®ä¿\", \"pinyin\": \"quÃ¨bÇo\", \"trans\": \"ensure\"},\n    {\"word\": \"è‡ªç„¶\", \"pinyin\": \"zÃ¬rÃ¡n\", \"trans\": \"natural\"},\n    {\"word\": \"æ”¾ç½®\", \"pinyin\": \"fÃ ngzhÃ¬\", \"trans\": \"place\"},\n    {\"word\": \"çœŸå®\", \"pinyin\": \"zhÄ“nshÃ­\", \"trans\": \"real\"},\n    {\"word\": \"æ’å…¥\", \"pinyin\": \"chÄrÃ¹\", \"trans\": \"insert\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ«zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"å–å¾—\", \"pinyin\": \"qÇ”dÃ©\", \"trans\": \"achieve\"},\n    {\"word\": \"æœ€å…ˆè¿›\", \"pinyin\": \"zuÃ¬xiÄnjÃ¬n\", \"trans\": \"state-of-the-art\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ©guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"äººç±»\", \"pinyin\": \"rÃ©nlÃ¨i\", \"trans\": \"human\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"èƒœå‡º\", \"pinyin\": \"shÃ¨ngchÅ«\", \"trans\": \"win\"},\n    {\"word\": \"è¶…è¿‡\", \"pinyin\": \"chÄoguÃ²\", \"trans\": \"exceed\"},\n    {\"word\": \"æ¡ˆä¾‹\", \"pinyin\": \"Ã nlÃ¬\", \"trans\": \"case\"}\n]",
        "trans": "This article introduces a new method called Add-it for adding objects to images based on textual instructions. Add-it utilizes an extended attention mechanism, combining information from the scene image, textual prompts, and the generated image itself. This method maintains structural consistency and detail without requiring task-specific fine-tuning, ensuring that objects are naturally placed. Add-it achieves state-of-the-art results in both real and generated image insertion benchmarks and outperforms in over 80% of cases in human evaluations.",
        "update_ts": "2024-11-12 09:51"
    }
}