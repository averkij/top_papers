{
    "date": {
        "ru": "17 июня",
        "en": "June 17",
        "zh": "6月17日"
    },
    "time_utc": "2025-06-17 02:43",
    "weekday": 1,
    "issue_id": 4324,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.13585",
            "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention",
            "url": "https://huggingface.co/papers/2506.13585",
            "abstract": "A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.",
            "score": 55,
            "issue_id": 4324,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "05163c188bd37051",
            "authors": [
                "MiniMax",
                ":",
                "Aili Chen",
                "Aonian Li",
                "Bangwei Gong",
                "Binyang Jiang",
                "Bo Fei",
                "Bo Yang",
                "Boji Shan",
                "Changqing Yu",
                "Chao Wang",
                "Cheng Zhu",
                "Chengjun Xiao",
                "Chengyu Du",
                "Chi Zhang",
                "Chu Qiao",
                "Chunhao Zhang",
                "Chunhui Du",
                "Congchao Guo",
                "Da Chen",
                "Deming Ding",
                "Dianjun Sun",
                "Dong Li",
                "Enwei Jiao",
                "Haigang Zhou",
                "Haimo Zhang",
                "Han Ding",
                "Haohai Sun",
                "Haoyu Feng",
                "Huaiguang Cai",
                "Haichao Zhu",
                "Jian Sun",
                "Jiaqi Zhuang",
                "Jiaren Cai",
                "Jiayuan Song",
                "Jin Zhu",
                "Jingyang Li",
                "Jinhao Tian",
                "Jinli Liu",
                "Junhao Xu",
                "Junjie Yan",
                "Junteng Liu",
                "Junxian He",
                "Kaiyi Feng",
                "Ke Yang",
                "Kecheng Xiao",
                "Le Han",
                "Leyang Wang",
                "Lianfei Yu",
                "Liheng Feng",
                "Lin Li",
                "Lin Zheng",
                "Linge Du",
                "Lingyu Yang",
                "Lunbin Zeng",
                "Minghui Yu",
                "Mingliang Tao",
                "Mingyuan Chi",
                "Mozhi Zhang",
                "Mujie Lin",
                "Nan Hu",
                "Nongyu Di",
                "Peng Gao",
                "Pengfei Li",
                "Pengyu Zhao",
                "Qibing Ren",
                "Qidi Xu",
                "Qile Li",
                "Qin Wang",
                "Rong Tian",
                "Ruitao Leng",
                "Shaoxiang Chen",
                "Shaoyu Chen",
                "Shengmin Shi",
                "Shitong Weng",
                "Shuchang Guan",
                "Shuqi Yu",
                "Sichen Li",
                "Songquan Zhu",
                "Tengfei Li",
                "Tianchi Cai",
                "Tianrun Liang",
                "Weiyu Cheng",
                "Weize Kong",
                "Wenkai Li",
                "Xiancai Chen",
                "Xiangjun Song",
                "Xiao Luo",
                "Xiao Su",
                "Xiaobo Li",
                "Xiaodong Han",
                "Xinzhu Hou",
                "Xuan Lu",
                "Xun Zou",
                "Xuyang Shen",
                "Yan Gong",
                "Yan Ma",
                "Yang Wang",
                "Yiqi Shi",
                "Yiran Zhong",
                "Yonghong Duan",
                "Yongxiang Fu",
                "Yongyi Hu",
                "Yu Gao",
                "Yuanxiang Fan",
                "Yufeng Yang",
                "Yuhao Li",
                "Yulin Hu",
                "Yunan Huang",
                "Yunji Li",
                "Yunzhi Xu",
                "Yuxin Mao",
                "Yuxuan Shi",
                "Yuze Wenren",
                "Zehan Li",
                "Zelin Li",
                "Zhanxu Tian",
                "Zhengmao Zhu",
                "Zhenhua Fan",
                "Zhenzhen Wu",
                "Zhichao Xu",
                "Zhihang Yu",
                "Zhiheng Lyu",
                "Zhuo Jiang",
                "Zibo Gao",
                "Zijia Wu",
                "Zijian Song",
                "Zijun Sun"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.13585.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#open_source",
                    "#architecture",
                    "#reasoning",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MiniMax-M1: Гибридный ИИ для эффективной обработки сложных задач",
                    "desc": "MiniMax-M1 - это гибридная модель рассуждений с архитектурой Mixture-of-Experts и механизмом молниеносного внимания. Модель поддерживает контекст длиной 1 миллион токенов и эффективно обрабатывает длинные входные данные. MiniMax-M1 обучена с помощью масштабного обучения с подкреплением на разнообразных задачах, включая реальные среды разработки программного обеспечения. Эксперименты показывают, что модель превосходит аналоги в сложных задачах программирования, использовании инструментов и работе с длинным контекстом."
                },
                "en": {
                    "title": "Revolutionizing Long-Input Processing with MiniMax-M1",
                    "desc": "MiniMax-M1 is a groundbreaking hybrid-attention reasoning model that utilizes a Mixture-of-Experts architecture and a lightning attention mechanism to efficiently handle long input sequences. With 456 billion parameters and the ability to process up to 1 million tokens, it significantly outperforms previous models in context length. The model is trained using a novel reinforcement learning algorithm called CISPO, which enhances training efficiency by clipping importance sampling weights. MiniMax-M1 demonstrates superior performance in complex tasks, particularly in software engineering and long-context applications, making it a valuable tool for various AI challenges."
                },
                "zh": {
                    "title": "高效长输入处理的混合注意力模型",
                    "desc": "MiniMax-M1是一种混合注意力推理模型，采用混合专家架构和闪电注意力机制，旨在高效处理长输入和强化学习任务。该模型基于之前的MiniMax-Text-01模型，具有4560亿个参数，并支持高达100万个token的上下文长度。MiniMax-M1在复杂任务中表现出色，尤其是在软件工程和工具利用方面。我们还提出了一种新颖的强化学习算法CISPO，进一步提高了训练效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11763",
            "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
            "url": "https://huggingface.co/papers/2506.11763",
            "abstract": "DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.",
            "score": 6,
            "issue_id": 4324,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "197213635094ee83",
            "authors": [
                "Mingxuan Du",
                "Benfeng Xu",
                "Chiwei Zhu",
                "Xiaorui Wang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "MetastoneTechnology, Beijing, China",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11763.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#science",
                    "#agents",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Комплексная оценка ИИ-агентов для глубоких исследований",
                    "desc": "DeepResearch Bench - это система оценки возможностей агентов глубоких исследований в области качества исследований и точности поиска информации. Бенчмарк включает 100 исследовательских задач уровня PhD по 22 различным областям, разработанных экспертами. Предложены две новые методологии оценки, хорошо согласующиеся с человеческим суждением: метод на основе эталонов и оценка возможностей поиска информации. DeepResearch Bench доступен в открытом доступе для ускорения разработки практических агентов на основе языковых моделей."
                },
                "en": {
                    "title": "Benchmarking Deep Research Agents for Superior Research Quality",
                    "desc": "DeepResearch Bench is a new framework designed to evaluate the performance of Deep Research Agents (DRAs) in generating high-quality research outputs. It includes 100 PhD-level tasks across 22 fields, allowing for a comprehensive assessment of DRAs' capabilities in information retrieval and report synthesis. The framework introduces two innovative evaluation methods: one that uses reference-based criteria to judge report quality, and another that measures citation effectiveness and accuracy. By providing these tools, DeepResearch Bench aims to enhance the development of LLM-based agents and improve their research efficiency."
                },
                "zh": {
                    "title": "评估深度研究代理的新基准",
                    "desc": "DeepResearch Bench是一个基准框架，用于评估深度研究代理在研究质量和信息检索准确性方面的能力。该框架包含100个由领域专家精心设计的博士级研究任务，涵盖22个不同领域。我们提出了两种新方法来评估这些代理的能力，一种是基于参考的评估方法，另一种是评估信息检索和引用准确性的框架。通过开源DeepResearch Bench及其关键组件，我们希望加速基于大型语言模型的代理的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13759",
            "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
            "url": "https://huggingface.co/papers/2506.13759",
            "abstract": "Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.   The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.   In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.   Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
            "score": 4,
            "issue_id": 4324,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "0a523ab9b7563360",
            "authors": [
                "Runpeng Yu",
                "Qi Li",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13759.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#diffusion",
                    "#inference",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Революция в языковом моделировании: дискретные диффузионные модели",
                    "desc": "Статья представляет собой систематический обзор дискретных диффузионных языковых моделей (dLLMs) и мультимодальных языковых моделей (dMLLMs). В отличие от авторегрессионных моделей, dLLMs и dMLLMs используют параллельное декодирование и стратегию генерации на основе шумоподавления. Эти модели обеспечивают более быстрый вывод и лучшую контролируемость по сравнению с авторегрессионными аналогами. В работе рассматриваются математические основы, ключевые техники обучения и вывода, а также применения dLLMs и dMLLMs в различных областях."
                },
                "en": {
                    "title": "Accelerating Language Generation with Discrete Diffusion Models",
                    "desc": "This paper surveys Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs), highlighting their advantages over traditional autoregressive models. dLLMs and dMLLMs utilize a parallel decoding approach with full attention and denoising strategies, allowing for faster generation and improved output control. The paper reviews the historical development, mathematical foundations, and key techniques for training these models, as well as their applications in various domains. It also discusses the future potential of dLLMs and dMLLMs in advancing machine learning research and deployment."
                },
                "zh": {
                    "title": "离散扩散模型：加速生成与控制的未来",
                    "desc": "本文系统性地调查了离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs）。与自回归模型不同，dLLMs和dMLLMs采用多标记并行解码的范式，利用全注意力机制和去噪生成策略，从而实现并行生成和更快的推理速度。这种新方法使得细粒度的输出控制和动态响应感知成为可能，这在自回归模型中是难以实现的。研究表明，dLLMs和dMLLMs在推理速度上可实现高达10倍的加速，且在多个领域的应用中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08343",
            "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves\n  Reasoning Efficiency",
            "url": "https://huggingface.co/papers/2506.08343",
            "abstract": "NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.",
            "score": 4,
            "issue_id": 4324,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "bc2b3e7cb2a8d002",
            "authors": [
                "Chenlong Wang",
                "Yuanning Feng",
                "Dongping Chen",
                "Zhaoyang Chu",
                "Ranjay Krishna",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University College London",
                "University of Maryland",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08343.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное рассуждение без лишних слов",
                    "desc": "Исследование представляет метод NoWait, который подавляет токены явной саморефлексии в больших языковых моделях во время вывода. Это позволяет сократить длину цепочки рассуждений на 27-51% без ущерба для полезности модели. Эксперименты проводились на десяти бенчмарках, охватывающих задачи рассуждения с текстом, изображениями и видео. NoWait предлагает простое решение для повышения эффективности мультимодального рассуждения в ИИ-системах."
                },
                "en": {
                    "title": "NoWait: Streamlining Multimodal Reasoning for Efficiency",
                    "desc": "The paper introduces NoWait, a method that improves the efficiency of multimodal reasoning models by suppressing explicit self-reflection tokens during inference. These tokens, like 'Wait' and 'Hmm', often lead to unnecessary verbosity and can slow down the reasoning process. By removing these tokens, NoWait significantly shortens the reasoning paths while maintaining the effectiveness of the model. The results from various benchmarks demonstrate that this approach can reduce the length of reasoning trajectories by up to 51%, making it a valuable enhancement for large reasoning models."
                },
                "zh": {
                    "title": "NoWait：提升多模态推理效率的创新方法",
                    "desc": "NoWait是一种新方法，通过在推理过程中抑制显式自我反思的标记（如“等一下”和“嗯”），来提高多模态推理的效率，而不降低模型的实用性。研究表明，传统的推理模型在复杂推理时常常会出现过度思考，导致输出冗长且重复，影响效率。通过在十个基准测试中进行广泛实验，NoWait能够将思维链的长度减少27%到51%。因此，NoWait为高效且保持实用性的多模态推理提供了一种简单有效的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03968",
            "title": "From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding",
            "url": "https://huggingface.co/papers/2506.03968",
            "abstract": "The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.",
            "score": 4,
            "issue_id": 4324,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "991991c3f686afa8",
            "authors": [
                "Chiwei Zhu",
                "Benfeng Xu",
                "Xiaorui Wang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Metastone Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03968.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#alignment",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Синтез сложных инструкций для эффективного обучения языковых моделей",
                    "desc": "Статья представляет метод генерации разнообразных и сложных инструкций для обучения больших языковых моделей (LLM) с использованием атрибутивного заземления. Авторы создали набор данных SynthQuestions, содержащий 1 миллион синтетических инструкций. Метод включает в себя нисходящий процесс атрибуции, который связывает реальные инструкции с конкретными пользователями, и восходящий процесс синтеза, использующий веб-документы для создания ситуаций и соответствующих инструкций. Модели, обученные на этом наборе данных, показали ведущие результаты на нескольких стандартных тестах."
                },
                "en": {
                    "title": "Harnessing Attributed Grounding for Diverse Instruction Generation",
                    "desc": "This paper introduces a novel method for generating diverse and complex instruction data for large language models (LLMs) through a technique called attributed grounding. The approach combines a top-down attribution process, which connects real-world instructions to specific user contexts, with a bottom-up synthesis process that creates meaningful instructions from web documents. By leveraging this framework, the authors successfully produce a large dataset of 1 million synthesized instructions, named SynthQuestions, which significantly enhances the performance of LLMs on various benchmarks. The results indicate that as more web data is utilized, the effectiveness of the models continues to improve, showcasing the importance of rich and varied instruction data for model alignment."
                },
                "zh": {
                    "title": "通过属性基础生成复杂指令数据，提升语言模型性能",
                    "desc": "本文提出了一种通过属性基础生成多样化和复杂的指令数据的方法，以提高大型语言模型的性能。该方法结合了自上而下的归因过程和自下而上的合成过程，能够有效地从真实指令和网络文档中生成有意义的指令。通过这种框架，我们构建了一个包含100万个指令的数据集SynthQuestions，并在多个基准测试中取得了领先的表现。该研究表明，利用丰富的网络文档可以大规模收集复杂的指令，从而提升模型的对齐能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07961",
            "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models",
            "url": "https://huggingface.co/papers/2506.07961",
            "abstract": "BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/",
            "score": 3,
            "issue_id": 4324,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "3fcf8d6329af3962",
            "authors": [
                "Peiyan Li",
                "Yixiang Chen",
                "Hongtao Wu",
                "Xiao Ma",
                "Xiangnan Wu",
                "Yan Huang",
                "Liang Wang",
                "Tao Kong",
                "Tieniu Tan"
            ],
            "affiliations": [
                "ByteDance Seed",
                "CASIA",
                "FiveAges",
                "NJU",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07961.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#3d",
                    "#games",
                    "#optimization",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "BridgeVLA: Эффективное обучение роботов через проекцию 3D в 2D",
                    "desc": "BridgeVLA - это новая модель машинного обучения для роботизированных манипуляций, объединяющая 3D-зрение, язык и действия. Она проецирует 3D-входы на 2D-изображения и использует 2D-тепловые карты для эффективного прогнозирования действий. Модель превосходит существующие методы в различных тестах, включая симуляции и эксперименты с реальными роботами. BridgeVLA демонстрирует высокую эффективность обучения и способность к обобщению в нестандартных ситуациях."
                },
                "en": {
                    "title": "BridgeVLA: Bridging 3D Vision and Action with 2D Heatmaps",
                    "desc": "BridgeVLA is a novel model that integrates 3D vision with language and action prediction by projecting 3D inputs into 2D images. This approach allows it to utilize 2D heatmaps for more efficient action prediction, enhancing the model's performance in robot manipulation tasks. The model is pre-trained to predict these heatmaps, which helps it learn effectively from fewer samples. Extensive testing shows that BridgeVLA significantly outperforms existing methods in various benchmarks, demonstrating its robustness and efficiency in real-world applications."
                },
                "zh": {
                    "title": "BridgeVLA：高效的3D视觉-语言-动作模型",
                    "desc": "BridgeVLA是一种3D视觉-语言-动作模型，它将3D输入投影到2D图像，并利用2D热图进行高效的动作预测。该模型通过将3D信号整合到视觉-语言模型中，充分利用了3D数据的空间结构，从而提高了样本效率。我们提出了一种可扩展的预训练方法，使得视觉-语言模型能够在下游策略学习之前预测2D热图。实验结果表明，BridgeVLA在多个基准测试中超越了现有的最先进方法，展现了卓越的学习效率和效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13750",
            "title": "Test3R: Learning to Reconstruct 3D at Test Time",
            "url": "https://huggingface.co/papers/2506.13750",
            "abstract": "Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, the reliance on pairwise prediction and the limited generalization capability inherently restrict the global geometric consistency. In this work, we introduce Test3R, a surprisingly simple test-time learning technique that significantly boosts geometric accuracy. Using image triplets (I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and (I_1,I_3). The core idea is to optimize the network at test time via a self-supervised objective: maximizing the geometric consistency between these two reconstructions relative to the common image I_1. This ensures the model produces cross-pair consistent outputs, regardless of the inputs. Extensive experiments demonstrate that our technique significantly outperforms previous state-of-the-art methods on the 3D reconstruction and multi-view depth estimation tasks. Moreover, it is universally applicable and nearly cost-free, making it easily applied to other models and implemented with minimal test-time training overhead and parameter footprint. Code is available at https://github.com/nopQAQ/Test3R.",
            "score": 2,
            "issue_id": 4324,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "68d521856e78273a",
            "authors": [
                "Yuheng Yuan",
                "Qiuhong Shen",
                "Shizun Wang",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13750.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🏛️",
                "ru": {
                    "title": "Test3R: Повышение точности 3D-реконструкции через самообучение на тестовых данных",
                    "desc": "Test3R - это новая техника обучения во время тестирования для 3D-реконструкции, которая улучшает геометрическую точность. Метод оптимизирует согласованность нейронной сети, используя самоконтролируемое обучение на триплетах изображений. Test3R генерирует реконструкции из пар изображений и максимизирует их геометрическую согласованность относительно общего изображения. Эксперименты показывают, что техника значительно превосходит предыдущие методы в задачах 3D-реконструкции и многоракурсной оценки глубины."
                },
                "en": {
                    "title": "Boosting 3D Reconstruction Accuracy with Test3R",
                    "desc": "Test3R is a novel test-time learning approach designed to improve the accuracy of 3D reconstruction by leveraging self-supervised learning on image triplets. It addresses the limitations of traditional dense matching methods that rely on pairwise predictions, which can lead to inconsistencies in global geometry. By optimizing the network's output during testing, Test3R ensures that reconstructions from different image pairs maintain geometric consistency relative to a common reference image. This technique not only enhances performance on 3D reconstruction tasks but is also easy to implement and applicable to various models with minimal additional training requirements."
                },
                "zh": {
                    "title": "Test3R：提升3D重建精度的简单方法",
                    "desc": "Test3R是一种用于3D重建的测试时学习技术，通过自监督学习优化网络一致性，从而提高几何精度。该方法利用图像三元组生成重建，确保不同图像对之间的一致性。Test3R的核心思想是在测试时最大化重建之间的几何一致性，确保模型输出在不同输入下保持一致。实验结果表明，Test3R在3D重建和多视角深度估计任务中显著优于现有的最先进方法，且适用性广泛，几乎不增加成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12953",
            "title": "Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition",
            "url": "https://huggingface.co/papers/2506.12953",
            "abstract": "PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.",
            "score": 1,
            "issue_id": 4324,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 июня",
                "en": "June 15",
                "zh": "6月15日"
            },
            "hash": "fb2789e38592ff5a",
            "authors": [
                "Mayank Bumb",
                "Anshul Vemulapalli",
                "Sri Harsha Vardhan Prasad Jella",
                "Anish Gupta",
                "An La",
                "Ryan A. Rossi",
                "Hongjie Chen",
                "Franck Dernoncourt",
                "Nesreen K. Ahmed",
                "Yu Wang"
            ],
            "affiliations": [
                "Adobe",
                "Dolby Labs",
                "Intel",
                "University of Massachusetts Amherst",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12953.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#optimization"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "Точное прогнозирование временных рядов с помощью языковых моделей",
                    "desc": "Статья представляет метод PatchInstruct для улучшения качества прогнозирования временных рядов с помощью больших языковых моделей (LLM). Метод использует специализированные техники промптинга, включая декомпозицию временных рядов, токенизацию на основе патчей и расширение данных с помощью похожих соседей. PatchInstruct позволяет LLM делать точные прогнозы без сложной архитектуры или масштабного дообучения. Это простой и гибкий подход, требующий минимальной предобработки данных."
                },
                "en": {
                    "title": "Enhancing LLM Forecasting with Simple Prompting Techniques",
                    "desc": "PatchInstruct is a method that improves the forecasting abilities of Large Language Models (LLMs) by using innovative prompting techniques. It incorporates time series decomposition to break down data into manageable parts, patch-based tokenization to efficiently handle input, and similarity-based neighbor augmentation to enhance predictions by considering related data points. This approach allows LLMs to perform time series forecasting without the need for extensive fine-tuning or complex architectures. Overall, PatchInstruct simplifies the process while boosting the accuracy of predictions in time series analysis."
                },
                "zh": {
                    "title": "PatchInstruct：简化时间序列预测的有效方法",
                    "desc": "PatchInstruct是一种增强大型语言模型（LLM）时间序列预测质量的方法。它通过专门的提示策略，如时间序列分解、基于补丁的标记化和相似性邻居增强，来实现这一目标。与以往需要大量微调的方法不同，PatchInstruct能够在不复杂重训练的情况下，灵活地进行时间序列预测。该方法保持了简单性，并且对数据的预处理要求最低。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12623",
            "title": "MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos",
            "url": "https://huggingface.co/papers/2506.12623",
            "abstract": "A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization.",
            "score": 1,
            "issue_id": 4324,
            "pub_date": "2025-06-14",
            "pub_date_card": {
                "ru": "14 июня",
                "en": "June 14",
                "zh": "6月14日"
            },
            "hash": "ef83eb4ade9dc4bf",
            "authors": [
                "Yuan Zang",
                "Hao Tan",
                "Seunghyun Yoon",
                "Franck Dernoncourt",
                "Jiuxiang Gu",
                "Kushal Kafle",
                "Chen Sun",
                "Trung Bui"
            ],
            "affiliations": [
                "Adobe Research",
                "Brown University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12623.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Новый подход к сумматизации обучающих видео по UI",
                    "desc": "Предложен новый бенчмарк и набор данных для мультимодальной сумматизации обучающих видео по пользовательским интерфейсам. Исследование направлено на создание пошаговых исполняемых инструкций и выделение ключевых кадров видео. Собран датасет из 2413 обучающих видео общей продолжительностью более 167 часов. Эксперименты показали, что современные методы мультимодальной сумматизации испытывают трудности с обработкой таких видео, что подчеркивает важность разработки новых подходов."
                },
                "en": {
                    "title": "Enhancing Learning with UI Video Summarization",
                    "desc": "This paper introduces a new benchmark and dataset specifically designed for multi-modal summarization of user interface (UI) instructional videos. The goal is to create efficient summaries that include step-by-step text instructions and key video frames, which are essential for effective learning. The authors highlight that existing benchmarks are inadequate for this purpose, as they focus on general video summarization rather than instructional content. Through extensive experiments on their dataset of 2,413 annotated UI instructional videos, they demonstrate that current multi-modal summarization techniques are not effective for this specific type of video, indicating a need for improved methods."
                },
                "zh": {
                    "title": "提升UI教学视频的多模态总结能力",
                    "desc": "本文提出了一种新的基准和数据集，用于多模态总结用户界面（UI）教学视频，旨在提供逐步可执行的指令和关键视频帧。现有的基准主要关注一般的语义级视频总结，无法满足教学视频中对逐步指令和插图的需求。我们收集了2413个UI教学视频的数据集，手动标注了视频分割、文本总结和视频总结，以便进行全面评估。实验结果表明，现有的多模态总结方法在UI视频总结上表现不佳，强调了开发新方法的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12189",
            "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis",
            "url": "https://huggingface.co/papers/2506.12189",
            "abstract": "The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.",
            "score": 0,
            "issue_id": 4324,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "952c0d68aa23cbda",
            "authors": [
                "Pranav Agarwal",
                "Ioana Ciucă"
            ],
            "affiliations": [
                "Google Deep Research",
                "Institute",
                "Mila",
                "Quebec AI",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12189.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#small_models",
                    "#reasoning",
                    "#long_context",
                    "#interpretability",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрывая личность искусственного интеллекта: новый подход к интерпретации языковых моделей",
                    "desc": "Исследование оценивает различные большие языковые модели (LLM) на разнообразных текстовых задачах с использованием нового набора данных, выявляя их отличительные личностные черты и улучшая интерпретируемость моделей. Авторы предлагают набор данных Supernova Event Dataset, содержащий разнообразные статьи, и используют его для оценки способности LLM извлекать и ранжировать ключевые события из текста. Анализ показывает, что разные модели демонстрируют различные подходы к рассуждению и анализу информации. Это исследование улучшает понимание 'личности' языковых моделей, делая их более удобными для широкого спектра приложений."
                },
                "en": {
                    "title": "Unveiling LLM Personalities for Better Interpretability",
                    "desc": "This study investigates how different Large Language Models (LLMs) perform on various text tasks using a new dataset called the Supernova Event Dataset. The dataset includes a wide range of articles, allowing for the evaluation of LLMs in extracting and ranking key events, which requires complex reasoning and understanding of context. The research reveals distinct personality traits among the models, such as emotional reasoning in Orca 2 and strategic thinking in Qwen 2.5, enhancing our understanding of their decision-making processes. By using another LLM as a judge to assess these traits, the study improves the interpretability of models, making them more accessible for diverse applications."
                },
                "zh": {
                    "title": "揭示大型语言模型的个性特征",
                    "desc": "本研究评估了多种大型语言模型（LLMs）在不同文本任务上的表现，使用了一个新的数据集。通过分析模型的选择和分类事件，我们揭示了模型的个性特征，并提高了模型的可解释性。我们提出的超新星事件数据集包含多样的文章，帮助我们基准测试模型在提取和排序关键事件方面的能力。研究结果显示，不同模型在处理情感推理、战略分析和因果推理等方面表现出明显的个性差异。"
                }
            }
        }
    ],
    "link_prev": "2025-06-16.html",
    "link_next": "2025-06-18.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "16.06",
        "en": "06/16",
        "zh": "6月16日"
    },
    "short_date_next": {
        "ru": "18.06",
        "en": "06/18",
        "zh": "6月18日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}