{
    "date": {
        "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 27",
        "zh": "12æœˆ27æ—¥"
    },
    "time_utc": "2024-12-27 08:13",
    "weekday": 4,
    "issue_id": 1355,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17743",
            "title": "YuLan-Mini: An Open Data-efficient Language Model",
            "url": "https://huggingface.co/papers/2412.17743",
            "abstract": "Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
            "score": 16,
            "issue_id": 1353,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "8d62bad0fb313231",
            "authors": [
                "Yiwen Hu",
                "Huatong Song",
                "Jia Deng",
                "Jiapeng Wang",
                "Jie Chen",
                "Kun Zhou",
                "Yutao Zhu",
                "Jinhao Jiang",
                "Zican Dong",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17743.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#optimization",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ YuLan-Mini - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2.42 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, robust-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. YuLan-Mini, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 1.08 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "YuLan-Mini: Efficient Pre-Training for Powerful Language Models",
                    "desc": "This paper introduces YuLan-Mini, a large language model with 2.42 billion parameters that performs exceptionally well compared to other models of similar size. The authors focus on improving the pre-training process by implementing a sophisticated data pipeline, a strong optimization technique to reduce instability, and a targeted data selection strategy during training. By utilizing 1.08 trillion tokens, YuLan-Mini achieves results that rival those of larger models that require more data. The paper also provides comprehensive details on the training data to support reproducibility in future research."
                },
                "zh": {
                    "title": "é«˜æ•ˆé¢„è®­ç»ƒï¼ŒYuLan-Miniå¼•é¢†è¯­è¨€æ¨¡å‹æ–°æ½®æµ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†YuLan-Miniï¼Œä¸€ä¸ªå…·æœ‰2.42äº¿å‚æ•°çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨åŒç±»æ¨¡å‹ä¸­å®ç°é¡¶å°–æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œé‡ç‚¹åœ¨äºé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®æ¥æé«˜è®­ç»ƒæ•ˆç‡ï¼šç²¾ç»†çš„æ•°æ®å¤„ç†æµç¨‹ç»“åˆäº†æ•°æ®æ¸…æ´—å’Œè°ƒåº¦ç­–ç•¥ï¼Œå¼ºå¤§çš„ä¼˜åŒ–æ–¹æ³•å‡å°‘äº†è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œä»¥åŠæœ‰æ•ˆçš„é€€ç«æ–¹æ³•ç»“åˆäº†ç›®æ ‡æ•°æ®é€‰æ‹©å’Œé•¿ä¸Šä¸‹æ–‡è®­ç»ƒã€‚YuLan-Miniåœ¨1.08Tçš„è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½å¯ä¸éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17483",
            "title": "A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression",
            "url": "https://huggingface.co/papers/2412.17483",
            "abstract": "In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gist-based compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segment-wise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities.",
            "score": 7,
            "issue_id": 1355,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "e390119f893ae13b",
            "authors": [
                "Chenlong Deng",
                "Zhisong Zhang",
                "Kelong Mao",
                "Shuaiyi Li",
                "Xinting Huang",
                "Dong Yu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17483.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rag",
                    "#optimization",
                    "#long_context",
                    "#data"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¸ÑÑ‚-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾ ÑĞ¶Ğ°Ñ‚Ğ¸ĞµĞ¼. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ: Ñ‚Ğ¾Ğ½ĞºĞ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¸ÑÑ‚-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Long-Context Processing with Gist-Based Compression",
                    "desc": "This paper explores gist-based context compression methods to enhance the ability of large language models to process long contexts. It investigates how these methods can serve as alternatives to full attention models and identifies potential failure patterns that may occur during compression. The authors demonstrate that while gist-based compression performs well in certain tasks, it struggles with others, revealing specific issues like information loss at boundaries and unexpected surprises. To address these challenges, they propose strategies such as fine-grained autoencoding and segment-wise token importance estimation to improve the effectiveness of context compression."
                },
                "zh": {
                    "title": "æå‡é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„è¦æ—¨å‹ç¼©ç­–ç•¥",
                    "desc": "æœ¬æ–‡æ·±å…¥ç ”ç©¶äº†åŸºäºè¦æ—¨çš„ä¸Šä¸‹æ–‡å‹ç¼©æ–¹æ³•ï¼Œä»¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€‚æˆ‘ä»¬å…³æ³¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šè¿™äº›æ–¹æ³•èƒ½å¤šå¥½åœ°æ›¿ä»£å…¨æ³¨æ„åŠ›æ¨¡å‹ï¼Œä»¥åŠå‹ç¼©å¯èƒ½å‡ºç°çš„å¤±è´¥æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºè¦æ—¨çš„å‹ç¼©åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œé•¿æ–‡æ¡£é—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨ç°æ¥è¿‘æ— æŸï¼Œä½†åœ¨åˆæˆå›å¿†ç­‰ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æœ‰æ•ˆç­–ç•¥ï¼šç»†ç²’åº¦è‡ªç¼–ç å’ŒåŸºäºæ®µçš„æ ‡è®°é‡è¦æ€§ä¼°è®¡ï¼Œä»¥æé«˜å‹ç¼©èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-26.html",
    "link_next": "2024-12-30.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "30.12",
        "en": "12/30",
        "zh": "12æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 2,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†çš„é‡è¦æ€§ã€‚è™½ç„¶Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨çš„å¼€é”€ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰LLMsçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä»¤ç‰Œé¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œæ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°ç®—ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°ç®—çš„ä»¤ç‰Œé¢„ç®—æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨CoTæ¨ç†ä¸­æœ‰æ•ˆåœ°å‡å°‘äº†ä»¤ç‰Œæˆæœ¬ï¼Œä»…ç•¥å¾®é™ä½äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§åœ¨LLMæ¨ç†ä¸­å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç ï¼šhttps://github.com/GeniusHTX/TALEã€‚",
        "title": "Token-Budget-Aware LLM Reasoning",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†çš„é‡è¦æ€§ã€‚è™½ç„¶Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨çš„å¼€é”€ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰LLMsçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä»¤ç‰Œé¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œæ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°ç®—ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°ç®—çš„ä»¤ç‰Œé¢„ç®—æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨CoTæ¨ç†ä¸­æœ‰æ•ˆåœ°å‡å°‘äº†ä»¤ç‰Œæˆæœ¬ï¼Œä»…ç•¥å¾®é™ä½äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§åœ¨LLMæ¨ç†ä¸­å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç ï¼šhttps://github.com/GeniusHTX/TALEã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs) zhÅng tuÄ« lÇ de zhÃ²ng yÃ o xÃ¬ng. suÄ« rÃ¡n Chain-of-Thought (CoT) tuÄ« lÇ fÄng fÇ tÅng guÃ² jiÄng wÃ¨n tÃ­ fÄ“n jiÄ› wÃ©i zhÅng jiÄn bÃ¹ zhÃ²u lÃ¡i tÃ­ gÄo LLM xÃ¬ng nÃ©ng, dÃ n yÄ› zÄ“ng jiÄ le lÃ¬ng pÃ¡i shÇ yÃ²ng de kÄi xiÇo, dÇo zhÃ¬ chÃ©ng bÄ›n zÄ“ng jiÄ. yÃ¡n jiÅ« fÄ xiÃ n, dÄng qiÃ¡n LLMs de tuÄ« lÇ guÃ² chÃ©ng guÃ² yÃº rÇ’ng chÃ¡ng, kÄ› yÇ tÅng guÃ² zÃ i tÃ­ shÃ¬ zhÅng bÄo hÃ¡n hÃ© lÇ de lÃ¬ng pÃ¡i yÃ¹ suÃ n lÃ¡i yÄ suÅ, dÃ n lÃ¬ng pÃ¡i yÃ¹ suÃ n de xuÇn zÃ© duÃ¬ shÃ­ jÃ¬ yÄ suÅ xiÃ o guÇ’ zhÃ¬ guÄn zhÃ²ng yÃ o. zuÃ² zhÄ› tÃ­ chÅ« le yÄ« gÃ¨ lÃ¬ng pÃ¡i yÃ¹ suÃ n gÇn zhÄ« de LLM tuÄ« lÇ kuÃ ng jiÃ , gÄ“n jÃ¹ tuÄ« lÇ fÃº zÃ  xÃ¬ng dÃ²ng tÃ i gÅ« sÇ”an bÃ¹ tÃ³ng wÃ¨n tÃ­ de lÃ¬ng pÃ¡i yÃ¹ suÃ n, bÃ¬ng shÇ yÃ²ng gÅ« sÇ”an de lÃ¬ng pÃ¡i yÃ¹ suÃ n zhÇ dÇo tuÄ« lÇ guÃ² chÃ©ng. shÃ­ yÃ n biÇo mÃ­ng, zhÃ¨ zhÇ’ng fÄng fÇ zÃ i CoT tuÄ« lÇ zhÅng yÇ’u xiÃ o de jiÇn shÇo le lÃ¬ng pÃ¡i chÃ©ng bÄ›n, jÇn lÃ¼Ã¨ wÄ“i jÄ«ng le xÃ¬ng nÃ©ng, tÃ­ gÅng le yÄ« zhÇ’ng zÃ i LLM tuÄ« lÇ zhÅng pÃ­ng hÃ©ng xiÃ o yÃ¬ hÃ© zhÇ”n quÃ¨ xÃ¬ng de shÃ­ yÃ²ng jiÄ› juÃ© fÄng Ã n. dÃ i mÇ: https://github.com/GeniusHTX/TALE.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'}, {'word': 'Chain-of-Thought', 'pinyin': 'Chain-of-Thought', 'trans': 'Chain-of-Thought'}, {'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'}, {'word': 'ä»¤ç‰Œ', 'pinyin': 'lÃ¬ng pÃ¡i', 'trans': 'token'}, {'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'}, {'word': 'å¼€é”€', 'pinyin': 'kÄi xiÄo', 'trans': 'cost'}, {'word': 'å¯¼è‡´', 'pinyin': 'dÇo zhÃ¬', 'trans': 'lead to'}, {'word': 'æˆæœ¬', 'pinyin': 'chÃ©ng bÄ›n', 'trans': 'cost'}, {'word': 'å¢åŠ ', 'pinyin': 'zÄ“ng jiÄ', 'trans': 'increase'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'å‘ç°', 'pinyin': 'fÄ xiÃ n', 'trans': 'discover'}, {'word': 'å½“å‰', 'pinyin': 'dÄng qiÃ¡n', 'trans': 'current'}, {'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ² chÃ©ng', 'trans': 'process'}, {'word': 'å†—é•¿', 'pinyin': 'rÇ’ng chÃ¡ng', 'trans': 'tedious'}, {'word': 'å‹ç¼©', 'pinyin': 'yÄ suÅ', 'trans': 'compress'}, {'word': 'æç¤º', 'pinyin': 'tÃ­ shÃ¬', 'trans': 'prompt'}, {'word': 'åŒ…å«', 'pinyin': 'bÄo hÃ¡n', 'trans': 'include'}, {'word': 'åˆç†', 'pinyin': 'hÃ© lÇ', 'trans': 'reasonable'}, {'word': 'é¢„ç®—', 'pinyin': 'yÃ¹ suÃ n', 'trans': 'budget'}, {'word': 'è‡³å…³é‡è¦', 'pinyin': 'zhÃ¬ guÄn zhÃ²ng yÃ o', 'trans': 'crucial'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇn zhÄ«', 'trans': 'perceive'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'}, {'word': 'æ ¹æ®', 'pinyin': 'gÄ“n jÃ¹', 'trans': 'based on'}, {'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'}, {'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'}, {'word': 'ä¼°ç®—', 'pinyin': 'gÅ« suÃ n', 'trans': 'estimate'}, {'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇ dÇo', 'trans': 'guide'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'è¡¨æ˜', 'pinyin': 'biÇo mÃ­ng', 'trans': 'indicate'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'}, {'word': 'å‡å°‘', 'pinyin': 'jiÇn shÇo', 'trans': 'reduce'}, {'word': 'ç•¥å¾®', 'pinyin': 'lÃ¼Ã¨ wÄ“i', 'trans': 'slightly'}, {'word': 'é™ä½', 'pinyin': 'jiÃ ng dÄ«', 'trans': 'lower'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'æä¾›', 'pinyin': 'tÃ­ gÅng', 'trans': 'provide'}, {'word': 'å¹³è¡¡', 'pinyin': 'pÃ­ng hÃ©ng', 'trans': 'balance'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ o lÇœ', 'trans': 'efficiency'}, {'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”n quÃ¨ xÃ¬ng', 'trans': 'accuracy'}, {'word': 'è§£å†³æ–¹æ¡ˆ', 'pinyin': 'jiÄ› juÃ© fÄng Ã n', 'trans': 'solution'}, {'word': 'å®ç”¨', 'pinyin': 'shÃ­ yÃ²ng', 'trans': 'practical'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}]",
        "trans": "This article discusses the importance of reasoning in large language models (LLMs). Although the Chain-of-Thought (CoT) reasoning method improves LLM performance by breaking down problems into intermediate steps, it also increases the overhead of token usage, leading to higher costs. Research has found that the current reasoning process in LLMs is overly lengthy and can be compressed by including a reasonable token budget in the prompts. However, the choice of token budget is crucial for the actual compression effect. The authors propose a token budget-aware LLM reasoning framework that dynamically estimates the token budget for different problems based on reasoning complexity and uses the estimated token budget to guide the reasoning process. Experiments show that this method effectively reduces token costs in CoT reasoning with only a slight decrease in performance, providing a practical solution for balancing efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
        "update_ts": "2024-12-26 09:10"
    }
}