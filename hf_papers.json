{
    "date": {
        "ru": "24 января",
        "en": "January 24",
        "zh": "1月24日"
    },
    "time_utc": "2025-01-24 07:09",
    "weekday": 4,
    "issue_id": 1845,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.13629",
            "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models",
            "url": "https://huggingface.co/papers/2501.13629",
            "abstract": "We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of Sigma by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the model's varying sensitivity to the compression of K and V components, leading to the development of differentially compressed KV, and (2) propose augmented Q to expand the Q head dimension, which enhances the model's representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to a 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, Sigma achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%.",
            "score": 18,
            "issue_id": 1842,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "d036f75a81877ded",
            "authors": [
                "Zhenghao Lin",
                "Zihao Tang",
                "Xiao Liu",
                "Yeyun Gong",
                "Yi Cheng",
                "Qi Chen",
                "Hang Li",
                "Ying Xin",
                "Ziyue Yang",
                "Kailai Yang",
                "Yu Yan",
                "Xiao Liang",
                "Shuai Lu",
                "Yiming Huang",
                "Zheheng Luo",
                "Lei Qu",
                "Xuan Feng",
                "Yaoxiang Wang",
                "Yuqing Xia",
                "Feiyang Chen",
                "Yuting Jiang",
                "Yasen Hu",
                "Hao Ni",
                "Binyang Li",
                "Guoshuai Zhao",
                "Jui-Hao Chiang",
                "Zhongxin Guo",
                "Chen Lin",
                "Kun Kuang",
                "Wenjie Li",
                "Yelong Shen",
                "Jian Jiao",
                "Peng Cheng",
                "Mao Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.13629.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#dataset",
                    "#benchmark",
                    "#long_context",
                    "#training",
                    "#synthetic",
                    "#data",
                    "#inference"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Sigma: эффективная ЯМ для системной области с инновационным механизмом внимания",
                    "desc": "Исследователи представили Sigma - эффективную большую языковую модель, специализированную для системной области. Модель использует новую архитектуру с DiffQKV-вниманием, которая оптимизирует компоненты Q, K и V механизма внимания для повышения эффективности. Sigma предобучена на тщательно собранных данных системной области объемом 6T токенов. На общих задачах модель показывает результаты на уровне современных аналогов, а в системной области значительно превосходит GPT-4 на новом бенчмарке AIMicius."
                },
                "en": {
                    "title": "Sigma: Revolutionizing System Domain Language Models with DiffQKV Attention",
                    "desc": "The paper presents Sigma, a specialized large language model designed for the system domain, utilizing a new architecture called DiffQKV attention. This innovative attention mechanism optimizes the Query, Key, and Value components to improve inference efficiency, particularly in long-context scenarios. Through extensive experiments, the authors show that Sigma achieves significant speed improvements, outperforming traditional models like GPT-4 in various tasks. The model is pre-trained on a vast dataset, including 19.5 billion tokens from the system domain, establishing a new benchmark for performance in this area."
                },
                "zh": {
                    "title": "Sigma：系统领域的高效语言模型",
                    "desc": "我们介绍了Sigma，这是一个高效的大型语言模型，专门针对系统领域。它采用了一种新颖的架构，包括DiffQKV注意力机制，并在我们精心收集的系统领域数据上进行了预训练。DiffQKV注意力通过优化注意力机制中的查询（Q）、键（K）和值（V）组件，显著提高了推理效率。实验结果表明，Sigma在系统领域的表现优于GPT-4，绝对提升幅度可达52.5%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13926",
            "title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step",
            "url": "https://huggingface.co/papers/2501.13926",
            "abstract": "Chain-of-Thought (CoT) reasoning has been extensively explored in large models to tackle complex understanding tasks. However, it still remains an open question whether such strategies can be applied to verifying and reinforcing image generation scenarios. In this paper, we provide the first comprehensive investigation of the potential of CoT reasoning to enhance autoregressive image generation. We focus on three techniques: scaling test-time computation for verification, aligning model preferences with Direct Preference Optimization (DPO), and integrating these techniques for complementary effects. Our results demonstrate that these approaches can be effectively adapted and combined to significantly improve image generation performance. Furthermore, given the pivotal role of reward models in our findings, we propose the Potential Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image generation. PARM adaptively assesses each generation step through a potential assessment approach, merging the strengths of existing reward models, and PARM++ further introduces a reflection mechanism to self-correct the generated unsatisfactory image. Using our investigated reasoning strategies, we enhance a baseline model, Show-o, to achieve superior results, with a significant +24% improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We hope our study provides unique insights and paves a new path for integrating CoT reasoning with autoregressive image generation. Code and models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT",
            "score": 3,
            "issue_id": 1841,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "61611cbe661736ff",
            "authors": [
                "Ziyu Guo",
                "Renrui Zhang",
                "Chengzhuo Tong",
                "Zhizheng Zhao",
                "Peng Gao",
                "Hongsheng Li",
                "Pheng-Ann Heng"
            ],
            "affiliations": [
                "CUHK",
                "MMLab",
                "MiuLar Lab",
                "Peking University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13926.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#games",
                    "#dataset",
                    "#cv",
                    "#reasoning",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Рассуждения по цепочке мыслей открывают новые горизонты в генерации изображений",
                    "desc": "Статья исследует применение рассуждений по цепочке мыслей (Chain-of-Thought) для улучшения автореграссивной генерации изображений. Авторы предлагают три метода: масштабирование вычислений во время тестирования, оптимизацию предпочтений модели и интеграцию этих техник. Они также представляют новые модели вознаграждения PARM и PARM++, специально разработанные для генерации изображений. Результаты показывают значительное улучшение производительности базовой модели Show-o на 24% по сравнению с эталоном GenEval."
                },
                "en": {
                    "title": "Enhancing Image Generation with Chain-of-Thought Reasoning",
                    "desc": "This paper explores the use of Chain-of-Thought (CoT) reasoning to improve autoregressive image generation models. It investigates three main techniques: enhancing verification through increased computation, aligning model preferences using Direct Preference Optimization (DPO), and combining these methods for better outcomes. The authors introduce the Potential Assessment Reward Model (PARM) and its enhanced version PARM++, which help assess and correct image generation steps. The results show a significant performance boost, achieving a 24% improvement on the GenEval benchmark compared to previous models."
                },
                "zh": {
                    "title": "链式思维提升图像生成性能",
                    "desc": "本文探讨了链式思维（CoT）推理在自回归图像生成中的应用潜力。我们提出了三种技术：测试时计算的扩展、与直接偏好优化（DPO）对齐模型偏好，以及这些技术的整合。研究结果表明，这些方法可以有效结合，显著提升图像生成性能。此外，我们提出了潜力评估奖励模型（PARM）和PARM++，专门用于自回归图像生成，进一步提高了生成质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13124",
            "title": "Debate Helps Weak-to-Strong Generalization",
            "url": "https://huggingface.co/papers/2501.13124",
            "abstract": "Common methods for aligning already-capable models with desired behavior rely on the ability of humans to provide supervision. However, future superhuman models will surpass the capability of humans. Therefore, humans will only be able to weakly supervise superhuman models. This expected deficiency of human evaluation would weaken the safety of future AI systems. Scalable oversight and weak-to-strong generalization are two complementary approaches to tackle this issue. In this paper, we attempt to combine the strengths of these two approaches to further improve alignment. Specifically, we investigate ways of improving human supervision with a strong pretrained model and then supervise the strong model with enhanced weak human supervision. To make iterative empirical progress, we consider an analogy: can we use a strong model to improve weak model supervision and then use it to supervise the strong model? We empirically test it by finetuning a small weak model on ground truth labels with the additional help from a large strong model, and then finetuning the strong model on labels generated by the weak model. We find that debate can assist a weak model in extracting trustworthy information from an untrustworthy strong model, which provides leverage as context on samples when training a weak model. We also show that an ensemble of weak models helps exploit long arguments generated by strong model debaters and obtain a more robust supervision estimate. Extensive experiments on the OpenAI weak-to-strong NLP benchmarks show that the combination approach leads to better alignment, which indicates that debate has the potential to help weak-to-strong generalization.",
            "score": 2,
            "issue_id": 1843,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "cacd0d01e3d119ee",
            "authors": [
                "Hao Lang",
                "Fei Huang",
                "Yongbin Li"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13124.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Улучшение контроля над ИИ: от слабого к сильному",
                    "desc": "Эта статья исследует методы улучшения контроля над сверхчеловеческими моделями искусственного интеллекта. Авторы предлагают комбинированный подход, используя сильную предобученную модель для улучшения слабого человеческого надзора, а затем применяя этот улучшенный надзор для обучения сильной модели. Эксперименты показывают, что метод дебатов помогает слабой модели извлекать достоверную информацию из ненадежной сильной модели. Результаты на бенчмарках OpenAI демонстрируют, что комбинированный подход приводит к лучшему выравниванию моделей с желаемым поведением."
                },
                "en": {
                    "title": "Enhancing AI Alignment through Model Debate and Supervision",
                    "desc": "This paper addresses the challenge of aligning superhuman AI models with desired behaviors, given that human supervision may be insufficient. It proposes a novel approach that combines scalable oversight with weak-to-strong generalization to enhance model alignment. The authors explore how a strong pretrained model can improve the supervision of a weak model, and in turn, how the weak model can provide valuable feedback to the strong model. Their experiments demonstrate that using debate between models can help extract reliable information, leading to improved alignment and performance on NLP tasks."
                },
                "zh": {
                    "title": "利用辩论提升AI模型的监督能力",
                    "desc": "本文探讨了如何在未来超人类模型的监督下改善人类的监督能力。由于人类的监督能力有限，未来的AI系统可能会面临安全性问题。我们提出了一种结合可扩展监督和弱到强泛化的方法，通过强大的预训练模型来增强人类的监督。实验结果表明，辩论可以帮助弱模型从强模型中提取可靠信息，从而提高监督的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13920",
            "title": "IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models",
            "url": "https://huggingface.co/papers/2501.13920",
            "abstract": "With the rapid development of diffusion models, text-to-image(T2I) models have made significant progress, showcasing impressive abilities in prompt following and image generation. Recently launched models such as FLUX.1 and Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have demonstrated exceptional performance across various complex tasks, raising questions about whether T2I models are moving towards general-purpose applicability. Beyond traditional image generation, these models exhibit capabilities across a range of fields, including controllable generation, image editing, video, audio, 3D, and motion generation, as well as computer vision tasks like semantic segmentation and depth estimation. However, current evaluation frameworks are insufficient to comprehensively assess these models' performance across expanding domains. To thoroughly evaluate these models, we developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0, Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided into five key domains: structured output generation, realism, and physical consistency, specific domain generation, challenging scenario generation, and multi-style creation tasks. This comprehensive assessment highlights each model's strengths and limitations, particularly the outstanding performance of FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring the expanding applications and potential of T2I models as foundational AI tools. This study provides valuable insights into the current state and future trajectory of T2I models as they evolve towards general-purpose usability. Evaluation scripts will be released at https://github.com/jylei16/Imagine-e.",
            "score": 2,
            "issue_id": 1843,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "837193826ae51376",
            "authors": [
                "Jiayi Lei",
                "Renrui Zhang",
                "Xiangfei Hu",
                "Weifeng Lin",
                "Zhen Li",
                "Wenjian Sun",
                "Ruoyi Du",
                "Le Zhuo",
                "Zhongyu Li",
                "Xinyue Li",
                "Shitian Zhao",
                "Ziyu Guo",
                "Yiting Lu",
                "Peng Gao",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13920.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#cv",
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Новый рубеж в оценке моделей текст-изображение: путь к универсальному ИИ",
                    "desc": "Эта статья посвящена оценке современных моделей преобразования текста в изображение (T2I). Авторы разработали новую систему оценки IMAGINE-E для тестирования шести ведущих моделей в пяти ключевых областях. Исследование выявило выдающиеся способности моделей FLUX.1 и Ideogram2.0 в структурированных задачах и задачах специфических доменов. Результаты подчеркивают растущий потенциал моделей T2I как универсальных инструментов искусственного интеллекта."
                },
                "en": {
                    "title": "Evaluating the Future of Text-to-Image Models",
                    "desc": "This paper discusses the advancements in text-to-image (T2I) models, particularly focusing on recent models like FLUX.1 and Ideogram2.0. These models not only excel in generating images from text prompts but also show versatility in various tasks such as image editing and video generation. The authors introduce a new evaluation framework called IMAGINE-E to assess the performance of six leading T2I models across multiple domains. The findings reveal that while some models perform exceptionally well in specific tasks, there is a need for better evaluation methods to fully understand their capabilities and limitations."
                },
                "zh": {
                    "title": "文本到图像模型的未来：通用性与评估的挑战",
                    "desc": "随着扩散模型的快速发展，文本到图像（T2I）模型在提示跟随和图像生成方面取得了显著进展。新推出的模型如FLUX.1和Ideogram2.0，以及Dall-E3和Stable Diffusion 3等，展示了在各种复杂任务中的卓越表现，提出了T2I模型是否朝着通用适用性发展的疑问。除了传统的图像生成，这些模型在可控生成、图像编辑、视频、音频、3D和运动生成等多个领域也展现了能力。为了全面评估这些模型的性能，我们开发了IMAGINE-E，并对六个主要模型进行了测试，强调了它们在不同领域的优势和局限性，特别是FLUX.1和Ideogram2.0在结构化和特定领域任务中的出色表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10799",
            "title": "Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback",
            "url": "https://huggingface.co/papers/2501.10799",
            "abstract": "Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces Step-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, Step-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that Step-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.",
            "score": 1,
            "issue_id": 1842,
            "pub_date": "2025-01-18",
            "pub_date_card": {
                "ru": "18 января",
                "en": "January 18",
                "zh": "1月18日"
            },
            "hash": "d43b005a69156930",
            "authors": [
                "Yen-Ting Lin",
                "Di Jin",
                "Tengyu Xu",
                "Tianhao Wu",
                "Sainbayar Sukhbaatar",
                "Chen Zhu",
                "Yun He",
                "Yun-Nung Chen",
                "Jason Weston",
                "Yuandong Tian",
                "Arash Rahnama",
                "Sinong Wang",
                "Hao Ma",
                "Han Fang"
            ],
            "affiliations": [
                "Meta FAIR",
                "Meta GenAI",
                "National Taiwan University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10799.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Шаг за шагом к надежным математическим рассуждениям ИИ",
                    "desc": "Статья представляет новый подход к обучению больших языковых моделей (LLM) для математических рассуждений. Метод Step-KTO использует бинарную обратную связь как для промежуточных шагов рассуждения, так и для конечного результата. Это позволяет модели следовать логичному ходу мыслей, а не полагаться на поверхностные шаблоны. Эксперименты на сложных математических тестах показали значительное улучшение как точности конечного ответа, так и качества промежуточных шагов рассуждения."
                },
                "en": {
                    "title": "Enhancing Trustworthy Reasoning in LLMs with Step-KTO",
                    "desc": "This paper presents Step-KTO, a new training framework for large language models (LLMs) that enhances their mathematical reasoning abilities. Unlike previous methods that focus solely on the final answer, Step-KTO provides feedback on both the reasoning process and the outcome, promoting logical coherence. By evaluating intermediate reasoning steps alongside the final result, the framework helps LLMs avoid shortcuts and develop more reliable reasoning paths. Experiments show that Step-KTO significantly boosts accuracy and improves the quality of reasoning in challenging mathematical tasks, indicating its potential for creating more interpretable AI systems."
                },
                "zh": {
                    "title": "提升推理可信度的Step-KTO框架",
                    "desc": "大型语言模型（LLMs）在数学推理方面取得了显著成功。尽管链式思维提示和自一致性采样等方法有所进展，但这些方法往往只关注最终结果的正确性，而未能确保推理过程的连贯性和可靠性。本文提出了Step-KTO，这是一种结合过程级和结果级二元反馈的训练框架，旨在引导LLMs朝着更可信的推理轨迹发展。实验结果表明，Step-KTO显著提高了最终答案的准确性和中间推理步骤的质量，展示了逐步过程反馈在LLM训练中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13075",
            "title": "Evolution and The Knightian Blindspot of Machine Learning",
            "url": "https://huggingface.co/papers/2501.13075",
            "abstract": "This paper claims that machine learning (ML) largely overlooks an important facet of general intelligence: robustness to a qualitatively unknown future in an open world. Such robustness relates to Knightian uncertainty (KU) in economics, i.e. uncertainty that cannot be quantified, which is excluded from consideration in ML's key formalisms. This paper aims to identify this blind spot, argue its importance, and catalyze research into addressing it, which we believe is necessary to create truly robust open-world AI. To help illuminate the blind spot, we contrast one area of ML, reinforcement learning (RL), with the process of biological evolution. Despite staggering ongoing progress, RL still struggles in open-world situations, often failing under unforeseen situations. For example, the idea of zero-shot transferring a self-driving car policy trained only in the US to the UK currently seems exceedingly ambitious. In dramatic contrast, biological evolution routinely produces agents that thrive within an open world, sometimes even to situations that are remarkably out-of-distribution (e.g. invasive species; or humans, who do undertake such zero-shot international driving). Interestingly, evolution achieves such robustness without explicit theory, formalisms, or mathematical gradients. We explore the assumptions underlying RL's typical formalisms, showing how they limit RL's engagement with the unknown unknowns characteristic of an ever-changing complex world. Further, we identify mechanisms through which evolutionary processes foster robustness to novel and unpredictable challenges, and discuss potential pathways to algorithmically embody them. The conclusion is that the intriguing remaining fragility of ML may result from blind spots in its formalisms, and that significant gains may result from direct confrontation with the challenge of KU.",
            "score": 0,
            "issue_id": 1845,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 января",
                "en": "January 22",
                "zh": "1月22日"
            },
            "hash": "5be12844b33bd729",
            "authors": [
                "Joel Lehman",
                "Elliot Meyerson",
                "Tarek El-Gaaly",
                "Kenneth O. Stanley",
                "Tarin Ziyaee"
            ],
            "affiliations": [
                "Cognizant AI Labs",
                "Second Nature AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13075.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agi",
                    "#agents",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Преодоление неизвестного: уроки эволюции для машинного обучения",
                    "desc": "Статья утверждает, что машинное обучение упускает важный аспект общего интеллекта: устойчивость к качественно неизвестному будущему в открытом мире. Авторы сравнивают обучение с подкреплением (RL) и биологическую эволюцию, показывая, что RL часто не справляется с непредвиденными ситуациями. В статье исследуются предположения, лежащие в основе формализмов RL, и выявляются механизмы, с помощью которых эволюционные процессы способствуют устойчивости к новым и непредсказуемым вызовам. Авторы приходят к выводу, что хрупкость машинного обучения может быть результатом слепых пятен в его формализмах, и значительные улучшения могут быть достигнуты путем прямого противостояния проблеме неопределенности Найта."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing ML Robustness through Evolutionary Insights",
                    "desc": "This paper highlights a critical gap in machine learning (ML) regarding its ability to handle unknown future scenarios, which is essential for general intelligence. It draws parallels between reinforcement learning (RL) and biological evolution, emphasizing that while RL struggles with unforeseen situations, evolution naturally adapts to them. The authors argue that current ML formalisms overlook Knightian uncertainty, which limits the robustness of AI systems in open-world environments. They propose that by understanding and integrating evolutionary mechanisms, ML can improve its resilience to unpredictable challenges."
                },
                "zh": {
                    "title": "机器学习需面对未知不确定性挑战",
                    "desc": "这篇论文指出，机器学习（ML）在处理开放世界中的未知未来时，忽视了一个重要方面：对未知不确定性的鲁棒性。作者将这种鲁棒性与经济学中的奈特不确定性（Knightian Uncertainty）相联系，认为这是机器学习关键形式化中被排除的因素。通过对比强化学习（RL）与生物进化过程，论文强调了RL在开放世界情境中的局限性，并探讨了生物进化如何在没有明确理论的情况下，培养出适应复杂环境的能力。最后，作者认为，机器学习的脆弱性可能源于其形式化中的盲点，直接面对奈特不确定性挑战可能会带来显著的进步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13919",
            "title": "Temporal Preference Optimization for Long-Form Video Understanding",
            "url": "https://huggingface.co/papers/2501.13919",
            "abstract": "Despite significant advancements in video large multimodal models (video-LMMs), achieving effective temporal grounding in long-form videos remains a challenge for existing models. To address this limitation, we propose Temporal Preference Optimization (TPO), a novel post-training framework designed to enhance the temporal grounding capabilities of video-LMMs through preference learning. TPO adopts a self-training approach that enables models to differentiate between well-grounded and less accurate temporal responses by leveraging curated preference datasets at two granularities: localized temporal grounding, which focuses on specific video segments, and comprehensive temporal grounding, which captures extended temporal dependencies across entire video sequences. By optimizing on these preference datasets, TPO significantly enhances temporal understanding while reducing reliance on manually annotated data. Extensive experiments on three long-form video understanding benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO establishes itself as the leading 7B model on the Video-MME benchmark, underscoring the potential of TPO as a scalable and efficient solution for advancing temporal reasoning in long-form video understanding. Project page: https://ruili33.github.io/tpo_website.",
            "score": 0,
            "issue_id": 1843,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 января",
                "en": "January 23",
                "zh": "1月23日"
            },
            "hash": "6e08b56893fb98a9",
            "authors": [
                "Rui Li",
                "Xiaohan Wang",
                "Yuhui Zhang",
                "Zeyu Wang",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Stanford University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13919.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#long_context",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "⏳",
                "ru": {
                    "title": "TPO: Улучшение временного понимания в видео-LMM без ручной разметки",
                    "desc": "Статья представляет новый метод под названием Temporal Preference Optimization (TPO) для улучшения временной привязки в видео-LMM моделях. TPO использует самообучение на основе предпочтений для различения хорошо и плохо привязанных во времени ответов. Метод работает на двух уровнях: локальная временная привязка для конкретных сегментов видео и комплексная для всей последовательности. Эксперименты на трех бенчмарках для длинных видео показали эффективность TPO для улучшения временного понимания в видео-LMM."
                },
                "en": {
                    "title": "Enhancing Temporal Understanding in Long Videos with TPO",
                    "desc": "This paper introduces Temporal Preference Optimization (TPO), a new framework aimed at improving how video large multimodal models (video-LMMs) understand time in long videos. TPO uses a self-training method that helps models learn to tell the difference between accurate and inaccurate timing responses by using specially curated preference datasets. These datasets focus on both specific video segments and the overall flow of the entire video, enhancing the model's ability to grasp temporal relationships. The results show that TPO significantly boosts performance on various benchmarks, making it a promising approach for better temporal reasoning in video analysis."
                },
                "zh": {
                    "title": "时间偏好优化：提升视频理解的关键",
                    "desc": "尽管视频大型多模态模型（video-LMMs）取得了显著进展，但在长视频中实现有效的时间定位仍然是一个挑战。为了解决这个问题，我们提出了一种新的后训练框架——时间偏好优化（TPO），旨在通过偏好学习增强视频-LMMs的时间定位能力。TPO采用自我训练的方法，利用精心策划的偏好数据集，使模型能够区分准确的时间响应和不太准确的时间响应。通过在这两个层次上优化偏好数据集，TPO显著提高了时间理解能力，同时减少了对手动标注数据的依赖。"
                }
            }
        }
    ],
    "link_prev": "2025-01-23.html",
    "link_next": "2025-01-27.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1月23日"
    },
    "short_date_next": {
        "ru": "27.01",
        "en": "01/27",
        "zh": "1月27日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了两种推理模型：DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 通过大规模强化学习训练，展示了出色的推理能力，但存在可读性差和语言混合的问题。为了解决这些问题，我们开发了 DeepSeek-R1，它在强化学习之前进行多阶段训练和冷启动数据处理。DeepSeek-R1 在推理任务上的表现与 OpenAI-o1-1217 相当。我们开源了这些模型和六个基于 Qwen 和 Llama 的压缩模型。",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "pinyin": "我们介绍了两种推理模型：DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 通过大规模强化学习训练，展示了出色的推理能力，但存在可读性差和语言混合的问题。为了解决这些问题，我们开发了 DeepSeek-R1，它在强化学习之前进行多阶段训练和冷启动数据处理。DeepSeek-R1 在推理任务上的表现与 OpenAI-o1-1217 相当。我们开源了这些模型和六个基于 Qwen 和 Llama 的压缩模型。\n\nWǒmen jièshào le liǎng zhǒng tuīlǐ móxíng: DeepSeek-R1-Zero hé DeepSeek-R1. DeepSeek-R1-Zero tōngguò dà guīmó qiángzhù xuéxí xùnliàn, zhǎnshì le chūsè de tuīlǐ nénglì, dàn cúnzài kědúxìng chà hé yǔyán hùnhé de wèntí. Wèile jiějué zhèxiē wèntí, wǒmen kāifā le DeepSeek-R1, tā zài qiángzhù xuéxí zhīqián jìnxíng duō jiēduàn xùnliàn hé lěng qǐdòng shùjù chǔlǐ. DeepSeek-R1 zài tuīlǐ rènwù shàng de biǎoxiàn yǔ OpenAI-o1-1217 xiāngdāng. Wǒmen kāiyuán le zhèxiē móxíng hé liù gè jīyú Qwen hé Llama de yāsuō móxíng.",
        "vocab": "[{'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'reasoning'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '强化学习', 'pinyin': 'qiáng​huà​xué​xí', 'trans': 'reinforcement learning'}, {'word': '训练', 'pinyin': 'xùnliàn', 'trans': 'training'}, {'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'demonstrate'}, {'word': '可读性', 'pinyin': 'kě​dú​xìng', 'trans': 'readability'}, {'word': '语言混合', 'pinyin': 'yǔ​yán​hùn​hé', 'trans': 'language mixing'}, {'word': '开发', 'pinyin': 'kāifā', 'trans': 'develop'}, {'word': '多阶段', 'pinyin': 'duō​jiē​duàn', 'trans': 'multi-stage'}, {'word': '冷启动', 'pinyin': 'lěng​qǐ​dòng', 'trans': 'cold start'}, {'word': '数据处理', 'pinyin': 'shù​jù​chǔ​lǐ', 'trans': 'data processing'}, {'word': '表现', 'pinyin': 'biǎo​xiàn', 'trans': 'performance'}, {'word': '相当', 'pinyin': 'xiāng​dāng', 'trans': 'equivalent'}, {'word': '开源', 'pinyin': 'kāi​yuán', 'trans': 'open source'}, {'word': '基于', 'pinyin': 'jī​yú', 'trans': 'based on'}, {'word': '压缩', 'pinyin': 'yā​suō', 'trans': 'compression'}]",
        "trans": "We introduced two reasoning models: DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, trained through large-scale reinforcement learning, demonstrated excellent reasoning capabilities but suffered from poor readability and language mixing issues. To address these problems, we developed DeepSeek-R1, which undergoes multi-stage training and cold start data processing before reinforcement learning. DeepSeek-R1 performs comparably to OpenAI-o1-1217 in reasoning tasks. We have open-sourced these models along with six compressed models based on Qwen and Llama.",
        "update_ts": "2025-01-23 09:10"
    }
}