{
    "date": {
        "ru": "26 августа",
        "en": "August 26",
        "zh": "8月26日"
    },
    "time_utc": "2025-08-26 03:37",
    "weekday": 1,
    "issue_id": 5540,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.18265",
            "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
            "url": "https://huggingface.co/papers/2508.18265",
            "abstract": "InternVL 3.5 introduces Cascade RL, ViR, and DvD to enhance reasoning, efficiency, and performance in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05times inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.",
            "score": 11,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "f9e73f2c4171c881",
            "authors": [
                "Weiyun Wang",
                "Zhangwei Gao",
                "Lixin Gu",
                "Hengjun Pu",
                "Long Cui",
                "Xingguang Wei",
                "Zhaoyang Liu",
                "Linglin Jing",
                "Shenglong Ye",
                "Jie Shao",
                "Zhaokai Wang",
                "Zhe Chen",
                "Hongjie Zhang",
                "Ganlin Yang",
                "Haomin Wang",
                "Qi Wei",
                "Jinhui Yin",
                "Wenhao Li",
                "Erfei Cui",
                "Guanzhou Chen",
                "Zichen Ding",
                "Changyao Tian",
                "Zhenyu Wu",
                "Jingjing Xie",
                "Zehao Li",
                "Bowen Yang",
                "Yuchen Duan",
                "Xuehui Wang",
                "Songze Li",
                "Xiangyu Zhao",
                "Haodong Duan",
                "Nianchen Deng",
                "Bin Fu",
                "Yinan He",
                "Yi Wang",
                "Conghui He",
                "Botian Shi",
                "Junjun He",
                "Yingtong Xiong",
                "Han Lv",
                "Lijun Wu",
                "Wenqi Shao",
                "Kaipeng Zhang",
                "Huipeng Deng",
                "Biqing Qi",
                "Jiaye Ge",
                "Qipeng Guo",
                "Wenwei Zhang",
                "Wanli Ouyang",
                "Limin Wang",
                "Min Dou",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Jifeng Dai",
                "Bowen Zhou",
                "Weijie Su",
                "Kai Chen",
                "Yu Qiao",
                "Wenhai Wang",
                "Gen Luo"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18265.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#reasoning",
                    "#architecture",
                    "#inference",
                    "#open_source",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "InternVL 3.5: Революция в мультимодальном ИИ через каскадное обучение с подкреплением",
                    "desc": "InternVL 3.5 представляет собой новое семейство открытых мультимодальных моделей, значительно улучшающих универсальность, способность к рассуждениям и эффективность вывода. Ключевым нововведением является фреймворк Cascade Reinforcement Learning (Cascade RL), который улучшает рассуждения через двухэтапный процесс: офлайн-обучение с подкреплением для стабильной сходимости и онлайн-обучение для уточненного выравнивания. Для оптимизации эффективности предложен Visual Resolution Router (ViR), динамически регулирующий разрешение визуальных токенов. Модель достигает значительного улучшения производительности в задачах рассуждений и ускорения вывода по сравнению с предшественником."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Reasoning with InternVL 3.5",
                    "desc": "InternVL 3.5 is a new set of open-source multimodal models that improve reasoning, efficiency, and performance. It introduces Cascade Reinforcement Learning (Cascade RL), which uses a two-stage training process to enhance reasoning capabilities. The Visual Resolution Router (ViR) optimizes the resolution of visual inputs dynamically, while the Decoupled Vision-Language Deployment (DvD) strategy balances the computational load across GPUs. These innovations lead to significant performance gains in reasoning tasks and faster inference speeds compared to previous models."
                },
                "zh": {
                    "title": "提升推理与效率的多模态模型",
                    "desc": "InternVL 3.5 是一款新型的开源多模态模型，显著提升了其多样性、推理能力和推理效率。其核心创新是级联强化学习（Cascade RL）框架，通过离线和在线强化学习的两阶段过程来增强推理能力。为了优化效率，提出了视觉分辨率路由器（ViR），动态调整视觉标记的分辨率，同时不影响性能。结合解耦视觉-语言部署（DvD）策略，InternVL 3.5 在推理性能上提升了 16.0%，并实现了 4.05 倍的推理速度提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17580",
            "title": "UQ: Assessing Language Models on Unsolved Questions",
            "url": "https://huggingface.co/papers/2508.17580",
            "abstract": "UQ is a benchmark for evaluating AI models on unsolved questions, combining difficulty and realism to assess capabilities like reasoning, factuality, and browsing.  \t\t\t\t\tAI-generated summary \t\t\t\t Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. We release UQ at https://uq.stanford.edu.",
            "score": 3,
            "issue_id": 5539,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "9f8a8ba45f2a8eca",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#survey",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "UQ: Оценка ИИ на грани человеческих знаний",
                    "desc": "UQ - это новый бенчмарк для оценки моделей искусственного интеллекта на нерешенных вопросах, сочетающий сложность и реалистичность. Он включает 500 разнообразных вопросов из Stack Exchange, охватывающих темы от теории информатики до научной фантастики. UQ использует валидаторы для предварительной проверки решений и открытую платформу для экспертной верификации. Этот подход позволяет оценивать передовые возможности моделей ИИ в решении реальных открытых задач."
                },
                "en": {
                    "title": "UQ: Evaluating AI on Real-World Unsolved Questions",
                    "desc": "The paper introduces UQ, a new benchmark for evaluating AI models on unsolved questions, which combines difficulty and realism to better assess capabilities like reasoning and factuality. Unlike traditional benchmarks that often present artificially difficult questions, UQ focuses on real-world challenges that arise from genuine human inquiries. The UQ framework includes a dataset of 500 diverse questions, validation strategies, and a collaborative platform for community verification. This innovative approach aims to push the boundaries of AI performance by addressing open-ended challenges that reflect actual knowledge gaps."
                },
                "zh": {
                    "title": "UQ：评估AI模型的新标准",
                    "desc": "UQ是一个用于评估人工智能模型在未解决问题上的基准，结合了难度和现实性，以评估推理、事实性和浏览等能力。当前的基准往往面临难度与现实性之间的矛盾，而UQ通过评估未解决的问题，提供了一种新的评估范式。我们构建了一个包含500个来自Stack Exchange的挑战性问题的数据集，并引入了验证者辅助筛选和社区验证的方法。UQ为评估前沿模型在真实世界中的表现提供了新的路径，推动人类知识的前沿。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18032",
            "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2508.18032",
            "abstract": "The Visual-Chain of Guidance (Visual-CoG) paradigm enhances text-to-image generation by providing stage-aware rewards, improving performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.",
            "score": 1,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "19b061daf44e7f58",
            "authors": [
                "Yaqi Li",
                "Peng Chen",
                "Mingyang Han",
                "Bu Pi",
                "Haoxiang Shi",
                "Runzhou Zhao",
                "Yang Yao",
                "Xuan Zhang",
                "Jun Song"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18032.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Поэтапное обучение для улучшенной генерации изображений по тексту",
                    "desc": "Статья представляет новую парадигму Visual-Chain of Guidance (Visual-CoG) для улучшения генерации изображений по текстовому описанию. Авторы предлагают трехэтапный процесс с поэтапным обучением с подкреплением, что позволяет лучше обрабатывать сложные и неоднозначные запросы. Метод включает семантические рассуждения, уточнение процесса и оценку результата, с вознаграждениями на каждом этапе. Эксперименты на нескольких бенчмарках показывают значительное улучшение производительности по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Stage-Aware Rewards for Enhanced Image Generation",
                    "desc": "The Visual-Chain of Guidance (Visual-CoG) paradigm improves text-to-image generation by introducing stage-aware rewards that enhance the model's performance. Traditional models often provide feedback only at the end of the generation process, making it hard to understand which parts of the process are effective. Visual-CoG breaks down the generation into three stages: semantic reasoning, process refining, and outcome evaluation, allowing for immediate feedback at each stage. This approach leads to significant performance improvements across various benchmarks, demonstrating its effectiveness in handling complex prompts."
                },
                "zh": {
                    "title": "视觉引导链：提升文本到图像生成的革命性方法",
                    "desc": "本文提出了一种名为视觉引导链（Visual-CoG）的新范式，旨在提升文本到图像生成的效果。该方法通过提供阶段性奖励，改善了模型在处理多属性和模糊提示时的表现。与传统的仅在生成结束时提供奖励的方式不同，Visual-CoG在生成过程中每个阶段都给予即时指导，从而优化生成策略。通过在多个基准测试上的评估，Visual-CoG显示出显著的性能提升，证明了其有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17188",
            "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs",
            "url": "https://huggingface.co/papers/2508.17188",
            "abstract": "PosterGen, a multi-agent framework using large language models, automates paper-to-poster generation with high design quality and minimal manual refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems built upon large language models (LLMs) have demonstrated remarkable capabilities in tackling complex compositional tasks. In this work, we apply this paradigm to the paper-to-poster generation problem, a practical yet time-consuming process faced by researchers preparing for conferences. While recent approaches have attempted to automate this task, most neglect core design and aesthetic principles, resulting in posters that require substantial manual refinement. To address these design limitations, we propose PosterGen, a multi-agent framework that mirrors the workflow of professional poster designers. It consists of four collaborative specialized agents: (1) Parser and Curator agents extract content from the paper and organize storyboard; (2) Layout agent maps the content into a coherent spatial layout; (3) Stylist agents apply visual design elements such as color and typography; and (4) Renderer composes the final poster. Together, these agents produce posters that are both semantically grounded and visually appealing. To evaluate design quality, we introduce a vision-language model (VLM)-based rubric that measures layout balance, readability, and aesthetic coherence. Experimental results show that PosterGen consistently matches in content fidelity, and significantly outperforms existing methods in visual designs, generating posters that are presentation-ready with minimal human refinements.",
            "score": 1,
            "issue_id": 5539,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 августа",
                "en": "August 24",
                "zh": "8月24日"
            },
            "hash": "fb29b368892da03a",
            "authors": [
                "Zhilin Zhang",
                "Xiang Zhang",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Chenyu You"
            ],
            "affiliations": [
                "New York University",
                "Stony Brook University",
                "University of British Columbia",
                "University of California, Los Angeles",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17188.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#agi",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "PosterGen: ИИ-дизайнер научных постеров",
                    "desc": "PosterGen - это мультиагентная система на основе больших языковых моделей (LLM) для автоматического создания постеров из научных статей. Система состоит из четырех специализированных агентов, которые извлекают содержание, создают макет, применяют визуальный дизайн и формируют итоговый постер. PosterGen превосходит существующие методы по качеству визуального дизайна, создавая постеры, готовые к презентации с минимальными доработками. Для оценки качества дизайна авторы предложили рубрику на основе мультимодальной языковой модели (VLM)."
                },
                "en": {
                    "title": "Automating Academic Posters with PosterGen: Design Meets Efficiency",
                    "desc": "PosterGen is a multi-agent framework that automates the process of creating academic posters from research papers using large language models. It consists of four specialized agents that work together: Parser and Curator agents extract and organize content, the Layout agent arranges this content spatially, the Stylist agents enhance the visual design, and the Renderer composes the final poster. This system not only ensures that the posters are semantically accurate but also visually appealing, addressing the shortcomings of previous automation methods. The framework is evaluated using a vision-language model to ensure high design quality, demonstrating superior performance in generating ready-to-present posters with minimal manual adjustments."
                },
                "zh": {
                    "title": "PosterGen：自动生成高质量学术海报的智能框架",
                    "desc": "PosterGen是一个基于大型语言模型的多智能体框架，旨在自动生成高质量的学术海报。该系统通过四个协作的专门代理，分别负责内容提取、布局设计、视觉风格应用和最终海报合成。与传统方法相比，PosterGen在设计美学和内容准确性上表现更佳，能够生成几乎无需人工修改的海报。通过引入视觉-语言模型评估设计质量，PosterGen确保了海报的可读性和视觉一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18190",
            "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
            "url": "https://huggingface.co/papers/2508.18190",
            "abstract": "ST-Raptor, a tree-based framework using large language models, addresses challenges in answering questions from semi-structured tables by introducing a Hierarchical Orthogonal Tree and a two-stage verification mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor.",
            "score": 0,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "e7ede69164051787",
            "authors": [
                "Zirui Tang",
                "Boyu Niu",
                "Xuanhe Zhou",
                "Boxiu Li",
                "Wei Zhou",
                "Jiannan Wang",
                "Guoliang Li",
                "Xinyi Zhang",
                "Fan Wu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai Jiao Tong University",
                "Simon Fraser University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18190.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#data",
                    "#interpretability"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "ST-Raptor: Древовидный подход к анализу сложных таблиц с помощью ИИ",
                    "desc": "ST-Raptor - это древовидная система для ответов на вопросы по полуструктурированным таблицам с использованием больших языковых моделей. Она вводит Иерархическое Ортогональное Дерево (HO-Tree) для захвата сложных макетов таблиц и набор базовых древесных операций для выполнения задач вопросно-ответной системы. ST-Raptor применяет двухэтапный механизм проверки: прямую валидацию для проверки правильности шагов выполнения и обратную валидацию для оценки надежности ответов. Эксперименты показывают, что ST-Raptor превосходит базовые методы до 20% по точности ответов."
                },
                "en": {
                    "title": "ST-Raptor: Revolutionizing Table Question Answering with Hierarchical Trees",
                    "desc": "ST-Raptor is a novel framework designed to enhance question answering from semi-structured tables using large language models (LLMs). It introduces a Hierarchical Orthogonal Tree (HO-Tree) to effectively represent complex table layouts, allowing for better interpretation of the data. The framework employs a two-stage verification mechanism to ensure the accuracy of answers by validating execution steps and reconstructing queries. Experimental results demonstrate that ST-Raptor significantly improves answer accuracy compared to existing methods, making it a valuable tool for automating table-based question answering."
                },
                "zh": {
                    "title": "ST-Raptor：半结构化表格问答的新突破",
                    "desc": "ST-Raptor是一个基于树的框架，旨在解决从半结构化表格中回答问题的挑战。它引入了层次正交树（HO-Tree）来捕捉复杂的表格布局，并通过基本树操作指导大型语言模型（LLM）执行常见的问答任务。该框架还采用了两阶段验证机制，确保执行步骤的正确性和答案的可靠性。实验结果表明，ST-Raptor在答案准确性上比九个基线方法提高了多达20%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18159",
            "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
            "url": "https://huggingface.co/papers/2508.18159",
            "abstract": "SpotEdit is a benchmark for evaluating visually-guided image editing methods, revealing performance disparities and hallucination issues across diffusion, autoregressive, and hybrid generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Visually-guided image editing, where edits are conditioned on both visual cues and textual prompts, has emerged as a powerful paradigm for fine-grained, controllable content generation. Although recent generative models have shown remarkable capabilities, existing evaluations remain simple and insufficiently representative of real-world editing challenges. We present SpotEdit, a comprehensive benchmark designed to systematically assess visually-guided image editing methods across diverse diffusion, autoregressive, and hybrid generative models, uncovering substantial performance disparities. To address a critical yet underexplored challenge, our benchmark includes a dedicated component on hallucination, highlighting how leading models, such as GPT-4o, often hallucinate the existence of a visual cue and erroneously perform the editing task. Our code and benchmark are publicly released at https://github.com/SaraGhazanfari/SpotEdit.",
            "score": 0,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "7e9e7873459fbab7",
            "authors": [
                "Sara Ghazanfari",
                "Wei-An Lin",
                "Haitong Tian",
                "Ersin Yumer"
            ],
            "affiliations": [
                "Adobe Inc.",
                "New York University, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18159.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#open_source",
                    "#hallucinations",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "SpotEdit: новый стандарт в редактировании изображений",
                    "desc": "SpotEdit — это новый бенчмарк для оценки методов редактирования изображений, управляемых визуальными подсказками. Он выявляет различия в производительности и проблемы галлюцинаций в диффузионных, авторегрессионных и гибридных генеративных моделях. Исследование показывает, что современные модели, такие как GPT-4o, часто ошибочно воспринимают визуальные подсказки, что приводит к неверному редактированию. SpotEdit предоставляет более полное представление о реальных вызовах редактирования изображений."
                },
                "en": {
                    "title": "SpotEdit: A New Standard for Evaluating Image Editing Models",
                    "desc": "SpotEdit is a new benchmark created to evaluate how well different image editing methods work when guided by visual cues and text prompts. It highlights the differences in performance among various generative models, including diffusion, autoregressive, and hybrid types. The benchmark also focuses on a significant issue called hallucination, where models mistakenly believe a visual cue exists and make incorrect edits. By providing a more thorough evaluation framework, SpotEdit aims to improve the understanding and development of visually-guided image editing techniques."
                },
                "zh": {
                    "title": "SpotEdit：评估视觉引导图像编辑的基准",
                    "desc": "SpotEdit是一个用于评估视觉引导图像编辑方法的基准，揭示了扩散、自动回归和混合生成模型之间的性能差异和幻觉问题。视觉引导图像编辑结合了视觉线索和文本提示，成为一种强大的细粒度可控内容生成方式。尽管最近的生成模型表现出色，但现有的评估方法过于简单，无法充分代表现实世界的编辑挑战。SpotEdit基准系统地评估不同生成模型的表现，并特别关注幻觉问题，指出一些领先模型在编辑任务中常常错误地假设存在视觉线索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18076",
            "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges",
            "url": "https://huggingface.co/papers/2508.18076",
            "abstract": "The paper critiques the use of large language models as judges for evaluating natural language generation systems, questioning their reliability, capabilities, scalability, and cost-effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating natural language generation (NLG) systems remains a core challenge of natural language processing (NLP), further complicated by the rise of large language models (LLMs) that aims to be general-purpose. Recently, large language models as judges (LLJs) have emerged as a promising alternative to traditional metrics, but their validity remains underexplored. This position paper argues that the current enthusiasm around LLJs may be premature, as their adoption has outpaced rigorous scrutiny of their reliability and validity as evaluators. Drawing on measurement theory from the social sciences, we identify and critically assess four core assumptions underlying the use of LLJs: their ability to act as proxies for human judgment, their capabilities as evaluators, their scalability, and their cost-effectiveness. We examine how each of these assumptions may be challenged by the inherent limitations of LLMs, LLJs, or current practices in NLG evaluation. To ground our analysis, we explore three applications of LLJs: text summarization, data annotation, and safety alignment. Finally, we highlight the need for more responsible evaluation practices in LLJs evaluation, to ensure that their growing role in the field supports, rather than undermines, progress in NLG.",
            "score": 0,
            "issue_id": 5540,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "0e751250907ca130",
            "authors": [
                "Khaoula Chehbouni",
                "Mohammed Haddou",
                "Jackie Chi Kit Cheung",
                "Golnoosh Farnadi"
            ],
            "affiliations": [
                "McGill University",
                "Mila - Quebec AI Institute",
                "Statistics Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18076.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#alignment",
                    "#multimodal",
                    "#interpretability"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Большие языковые модели как судьи: преждевременный энтузиазм?",
                    "desc": "Статья критикует использование больших языковых моделей (LLM) в качестве судей для оценки систем генерации естественного языка. Авторы ставят под сомнение надежность, возможности, масштабируемость и экономическую эффективность такого подхода. Они анализируют четыре ключевых предположения, лежащих в основе использования LLM как судей: способность заменять человеческие оценки, возможности в качестве оценщиков, масштабируемость и экономичность. Исследователи призывают к более ответственным практикам оценки с использованием LLM для обеспечения прогресса в области генерации естественного языка."
                },
                "en": {
                    "title": "Rethinking Large Language Models as Evaluators in NLG",
                    "desc": "This paper critiques the use of large language models (LLMs) as judges for evaluating natural language generation (NLG) systems. It questions their reliability, capabilities, scalability, and cost-effectiveness, suggesting that the excitement around using LLMs as evaluators may be premature. The authors analyze four key assumptions about LLMs acting as proxies for human judgment and their effectiveness in evaluation tasks. They call for more responsible evaluation practices to ensure that the integration of LLMs in NLG evaluation supports meaningful progress in the field."
                },
                "zh": {
                    "title": "审慎使用大型语言模型评估生成系统",
                    "desc": "这篇论文批评了将大型语言模型（LLMs）作为评估自然语言生成系统的评判者的做法，质疑其可靠性、能力、可扩展性和成本效益。作者认为，尽管大型语言模型作为评判者（LLJs）被视为传统评估指标的有希望替代方案，但其有效性尚未得到充分探讨。论文分析了LLJs使用的四个核心假设，包括其作为人类判断代理的能力、评估能力、可扩展性和成本效益，并指出这些假设可能受到LLMs的固有限制的挑战。最后，作者强调需要更负责任的评估实践，以确保LLJs在自然语言生成领域的作用能够促进而不是阻碍进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16790",
            "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language\n  Modeling",
            "url": "https://huggingface.co/papers/2508.16790",
            "abstract": "TaDiCodec, a Text-aware Diffusion Transformer Speech Codec, achieves low frame rates and bitrates with superior speech generation performance using end-to-end optimization and text guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Speech tokenizers serve as foundational components for speech language models, yet current designs exhibit several limitations, including: 1) dependence on multi-layer residual vector quantization structures or high frame rates, 2) reliance on auxiliary pre-trained models for semantic distillation, and 3) requirements for complex two-stage training processes. In this work, we introduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a novel approach designed to overcome these challenges. TaDiCodec employs end-to-end optimization for quantization and reconstruction through a diffusion autoencoder, while integrating text guidance into the diffusion decoder to enhance reconstruction quality and achieve optimal compression. TaDiCodec achieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of 0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining superior performance on critical speech generation evaluation metrics such as Word Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS). Notably, TaDiCodec employs a single-stage, end-to-end training paradigm, and obviating the need for auxiliary pre-trained models. We also validate the compatibility of TaDiCodec in language model based zero-shot text-to-speech with both autoregressive modeling and masked generative modeling, demonstrating its effectiveness and efficiency for speech language modeling, as well as a significantly small reconstruction-generation gap. We will open source our code and model checkpoints. Audio samples are are available at https:/tadicodec.github.io/. We release code and model checkpoints at https:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.",
            "score": 0,
            "issue_id": 5540,
            "pub_date": "2025-08-22",
            "pub_date_card": {
                "ru": "22 августа",
                "en": "August 22",
                "zh": "8月22日"
            },
            "hash": "566e852623f03412",
            "authors": [
                "Yuancheng Wang",
                "Dekun Chen",
                "Xueyao Zhang",
                "Junan Zhang",
                "Jiaqi Li",
                "Zhizheng Wu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.16790.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#audio",
                    "#open_source"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "TaDiCodec: эффективное кодирование речи с помощью диффузии и текста",
                    "desc": "TaDiCodec - это новый подход к кодированию речи, использующий диффузионный трансформер и текстовое руководство. Он достигает низкой частоты кадров (6,25 Гц) и битрейта (0,0875 кбит/с) при сохранении высокого качества генерации речи. TaDiCodec применяет сквозную оптимизацию для квантования и реконструкции, интегрируя текстовое руководство в диффузионный декодер. Модель показывает превосходные результаты по таким метрикам, как Word Error Rate, сходство голоса и качество речи."
                },
                "en": {
                    "title": "Revolutionizing Speech Generation with TaDiCodec",
                    "desc": "TaDiCodec is a new speech codec that uses a Text-aware Diffusion Transformer to improve speech generation while keeping low frame rates and bitrates. It addresses limitations of existing speech tokenizers by using end-to-end optimization for quantization and reconstruction, eliminating the need for complex training processes and auxiliary models. The codec achieves a low frame rate of 6.25 Hz and a bitrate of 0.0875 kbps, while still performing well on important metrics like Word Error Rate and speech quality. Additionally, TaDiCodec supports zero-shot text-to-speech applications, showcasing its versatility in speech language modeling."
                },
                "zh": {
                    "title": "文本感知的高效语音编解码器",
                    "desc": "TaDiCodec是一种文本感知的扩散变换器语音编解码器，旨在通过端到端优化和文本指导来实现低帧率和低比特率的优越语音生成性能。该模型克服了现有语音模型在多层残差向量量化、依赖预训练模型和复杂的两阶段训练过程等方面的局限性。TaDiCodec采用扩散自编码器进行量化和重建，并通过扩散解码器集成文本指导，以提高重建质量和压缩效率。最终，TaDiCodec在语音生成评估指标上表现出色，同时实现了极低的帧率和比特率。"
                }
            }
        }
    ],
    "link_prev": "2025-08-25.html",
    "link_next": "2025-08-27.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "25.08",
        "en": "08/25",
        "zh": "8月25日"
    },
    "short_date_next": {
        "ru": "27.08",
        "en": "08/27",
        "zh": "8月27日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}