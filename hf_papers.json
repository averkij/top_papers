{
    "date": {
        "ru": "5 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 5",
        "zh": "11æœˆ5æ—¥"
    },
    "time_utc": "2024-11-05 02:42",
    "weekday": 1,
    "issue_id": 420,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.00836",
            "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
            "url": "https://huggingface.co/papers/2411.00836",
            "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.",
            "score": 4,
            "issue_id": 420,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "e73ae00a5621a2b9",
            "data": {
                "categories": [
                    "#benchmark",
                    "#math",
                    "#cv"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "DynaMath: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DynaMath - Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Vision-Language Models (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4V, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑˆĞ°Ğ³Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğº Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸. DynaMath Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 501 Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Python-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 14 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ…ÑƒĞ´ÑˆĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing VLMs: Unveiling Mathematical Reasoning Limitations with DynaMath",
                    "desc": "This paper explores the limitations of Vision-Language Models (VLMs) in performing mathematical reasoning tasks that involve visual elements. It highlights that while humans can adapt their problem-solving strategies to slight changes in problems, current state-of-the-art VLMs like GPT-4o struggle with this adaptability. To address this issue, the authors introduce DynaMath, a dynamic benchmark that generates a wide variety of mathematical questions to rigorously test VLMs' reasoning capabilities. The findings reveal that VLMs exhibit significantly lower accuracy in worst-case scenarios compared to average cases, underscoring the importance of evaluating their robustness in mathematical reasoning."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰ä¸Šä¸‹æ–‡çš„å½±å“ä¸‹ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡äººç±»èƒ½å¤Ÿçµæ´»åº”å¯¹ç›¸ä¼¼é—®é¢˜çš„å˜åŒ–ï¼Œå½“å‰çš„æœ€å…ˆè¿›æ¨¡å‹å¦‚GPT-4oåœ¨é¢å¯¹è¿™äº›å˜åŒ–æ—¶å´è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºå…¶æ•°å­¦æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DynaMathï¼Œä¸€ä¸ªåŠ¨æ€è§†è§‰æ•°å­¦åŸºå‡†ï¼Œæ—¨åœ¨æ·±å…¥è¯„ä¼°VLMsçš„æ¨ç†ç¨³å¥æ€§ã€‚é€šè¿‡å¯¹501ä¸ªé«˜è´¨é‡ç§å­é—®é¢˜çš„è‡ªåŠ¨ç”Ÿæˆï¼ŒDynaMathèƒ½å¤Ÿè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¾“å…¥æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨æœ€åæƒ…å†µä¸‹çš„å‡†ç¡®ç‡æ˜¾è‘—ä½äºå¹³å‡æƒ…å†µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00918",
            "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
            "url": "https://huggingface.co/papers/2411.00918",
            "abstract": "Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.",
            "score": 2,
            "issue_id": 420,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 1",
                "zh": "11æœˆ1æ—¥"
            },
            "hash": "a406640433a3de34",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LibMoE: Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Mixture of Experts Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LibMoE - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Mixture of Experts (MoE) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). LibMoE Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ…: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ LibMoE, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² MoE Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¸ 11 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ²ÑĞµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ MoE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ñƒ ÑĞ¿ĞµĞºÑ‚Ñ€Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "LibMoE: Streamlining Mixture of Experts for Large Language Models",
                    "desc": "This paper introduces LibMoE, a framework designed to facilitate research on Mixture of Experts (MoE) algorithms for large language models (LLMs). It emphasizes a modular design, efficient training processes, and comprehensive evaluation methods to make MoE more accessible to researchers. The authors benchmark five leading MoE algorithms across three LLMs and 11 datasets, revealing that their performance is generally similar across various tasks. LibMoE aims to standardize the training and evaluation of MoE, helping researchers advance the development of future LLMs."
                },
                "zh": {
                    "title": "LibMoEï¼šè®©æ··åˆä¸“å®¶ç®—æ³•æ›´æ˜“äºç ”ç©¶å’Œåº”ç”¨",
                    "desc": "æ··åˆä¸“å®¶ï¼ˆMoEï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é«˜æ•ˆå’Œæœ‰æ•ˆå‘å±•ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç”±äºèµ„æºéœ€æ±‚å·¨å¤§ï¼Œè®¸å¤šç ”ç©¶è€…éš¾ä»¥ç ”ç©¶å¤§è§„æ¨¡çš„MoEç®—æ³•ã€‚æœ¬æ–‡å¼€å‘äº†LibMoEï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ä¸”æ¨¡å—åŒ–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–MoEç®—æ³•çš„ç ”ç©¶ã€è®­ç»ƒå’Œè¯„ä¼°ã€‚é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ã€é«˜æ•ˆè®­ç»ƒå’Œå…¨é¢è¯„ä¼°ï¼ŒLibMoEä½¿å¾—MoEåœ¨LLMsä¸­çš„åº”ç”¨å¯¹æ›´å¤šç ”ç©¶è€…å˜å¾—å¯åŠã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-04.html",
    "link_next": "2024-11-06.html",
    "short_date_prev": {
        "ru": "04.11",
        "en": "11/04",
        "zh": "11æœˆ4æ—¥"
    },
    "short_date_next": {
        "ru": "06.11",
        "en": "11/06",
        "zh": "11æœˆ6æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#medicine": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„ç°æœ‰åŠªåŠ›ã€‚ç›®å‰ä¸»è¦ä¾èµ–äºå¼ºå¤§çš„å•†ä¸šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚GPT-4oå’ŒGeminiProVisionã€‚å¼€æºVLMsåœ¨GUIç†è§£å’Œæœªè§åˆ†å¸ƒï¼ˆOODï¼‰åœºæ™¯ä¸­è¡¨ç°è¾ƒå·®ï¼Œå› æ­¤å®è·µè€…ä¸å¤ªæ„¿æ„ä½¿ç”¨ã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ï¼Œä½œè€…å¼€å‘äº†OS-Atlasï¼Œè¿™æ˜¯ä¸€ä¸ªæ“…é•¿GUIç†è§£å’ŒOODä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ã€‚ä»–ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«1300å¤šä¸‡GUIå…ƒç´ çš„è·¨å¹³å°æ•°æ®é›†ï¼Œå¹¶è¯æ˜äº†OS-Atlasåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚",
        "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le gÃ²ujiÃ n tÃºxÃ­ng yÃ²nghÃ¹ jiÄ“miÃ n (GUI) dÃ ilÇ de xiÃ n yÇ’u nÇ”lÃ¬. DÄngqiÃ¡n zhÇ”yÃ o yÄ«lÃ i zÃ i qiÃ¡ngdÃ  de shÄngyÃ¨ shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng (VLMs) shÃ ng, rÃº GPT-4o hÃ© GeminiProVision. KÄiyuÃ¡n VLMs zÃ i GUI lÇjiÄ› hÃ© wÃ¨i jiÃ n fÄ“nbÃ¹ (OOD) chÇngjÇng zhÅng biÇoxiÇn jiÃ ochÇ, yÄ«ncÇ shÃ­jiÃ nzhÄ› bÃ¹ tÃ i yuÃ nyÃ¬ shÇyÃ²ng. WÃ¨ile tuÄ«dÃ²ng zhÃ¨ yÄ« lÇngyÃ¹ de yÃ¡njiÅ«, zuÃ²zhÄ› kÄifÄle OS-Atlas, zhÃ¨ shÃ¬ yÄ«gÃ¨ shÃ nchÃ¡ng GUI lÇjiÄ› hÃ© OOD rÃ¨nwÃ¹ de jÄ«chÇ” mÃ³xÃ­ng. TÄmen hÃ¡i fÄbÃ¹le yÄ«gÃ¨ bÄohÃ¡n 1300 duÅ wÃ n GUI yuÃ¡nsÃ¹ de kuÃ  pÃ­ngtÃ¡i shÃ¹jÃ¹jÃ­, bÃ¬ng zhÃ¨ngmÃ­ngle OS-Atlas zÃ i duÅgÃ¨ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng de xiÇnzhÃ¹ xiÃ onÃ©ng tÇshÄ“ng.",
        "vocab": "[\n    {\"word\": \"æ„å»º\", \"pinyin\": \"gÃ²u jiÃ n\", \"trans\": \"construct\"},\n    {\"word\": \"å›¾å½¢ç”¨æˆ·ç•Œé¢\", \"pinyin\": \"tÃº xÃ­ng yÃ²ng hÃ¹ jiÃ¨ miÃ n\", \"trans\": \"graphical user interface\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ i lÇ\", \"trans\": \"agent\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ n yÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"åŠªåŠ›\", \"pinyin\": \"nÇ” lÃ¬\", \"trans\": \"efforts\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ« lÃ i\", \"trans\": \"rely on\"},\n    {\"word\": \"å¼ºå¤§\", \"pinyin\": \"qiÃ¡ng dÃ \", \"trans\": \"powerful\"},\n    {\"word\": \"å•†ä¸š\", \"pinyin\": \"shÄng yÃ¨\", \"trans\": \"commercial\"},\n    {\"word\": \"è§†è§‰è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"visual language model\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"æœªè§åˆ†å¸ƒ\", \"pinyin\": \"wÃ¨i jiÃ n fÄ“n bÃ¹\", \"trans\": \"out-of-distribution\"},\n    {\"word\": \"åœºæ™¯\", \"pinyin\": \"chÇng jÇng\", \"trans\": \"scenario\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"è¾ƒå·®\", \"pinyin\": \"jiÃ o chÃ \", \"trans\": \"poor\"},\n    {\"word\": \"å®è·µè€…\", \"pinyin\": \"shÃ­ jiÃ n zhÄ›\", \"trans\": \"practitioner\"},\n    {\"word\": \"ä¸å¤ªæ„¿æ„\", \"pinyin\": \"bÃ¹ tÃ i yuÃ n yÃ¬\", \"trans\": \"not very willing\"},\n    {\"word\": \"æ¨åŠ¨\", \"pinyin\": \"tuÄ« dÃ²ng\", \"trans\": \"promote\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇng yÃ¹\", \"trans\": \"field\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"å¼€å‘\", \"pinyin\": \"kÄi fÄ\", \"trans\": \"develop\"},\n    {\"word\": \"åŸºç¡€æ¨¡å‹\", \"pinyin\": \"jÄ« chÇ” mÃ³ xÃ­ng\", \"trans\": \"foundation model\"},\n    {\"word\": \"æ“…é•¿\", \"pinyin\": \"shÃ n chÃ¡ng\", \"trans\": \"proficient\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"å‘å¸ƒ\", \"pinyin\": \"fÄ bÃ¹\", \"trans\": \"release\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"è·¨å¹³å°\", \"pinyin\": \"kuÃ  pÃ­ng tÃ¡i\", \"trans\": \"cross-platform\"},\n    {\"word\": \"å…ƒç´ \", \"pinyin\": \"yuÃ¡n sÃ¹\", \"trans\": \"element\"},\n    {\"word\": \"è¯æ˜\", \"pinyin\": \"zhÃ¨ng mÃ­ng\", \"trans\": \"demonstrate\"},\n    {\"word\": \"åŸºå‡†æµ‹è¯•\", \"pinyin\": \"jÄ« zhÇ”n cÃ¨ shÃ¬\", \"trans\": \"benchmark test\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­ shÄ“ng\", \"trans\": \"improvement\"}\n]",
        "trans": "This article discusses current efforts in building graphical user interface (GUI) agents. Currently, these efforts primarily rely on powerful commercial vision language models (VLMs) such as GPT-4o and GeminiProVision. Open-source VLMs perform poorly in GUI understanding and out-of-distribution (OOD) scenarios, making practitioners reluctant to use them. To advance research in this field, the authors developed OS-Atlas, a foundational model adept at GUI understanding and OOD tasks. They also released a cross-platform dataset containing over 13 million GUI elements and demonstrated significant performance improvements of OS-Atlas across multiple benchmark tests.",
        "update_ts": "2024-11-04 10:14"
    }
}