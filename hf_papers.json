{
    "date": {
        "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 17",
        "zh": "12æœˆ17æ—¥"
    },
    "time_utc": "2024-12-17 07:11",
    "weekday": 1,
    "issue_id": 1162,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.11919",
            "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
            "url": "https://huggingface.co/papers/2412.11919",
            "abstract": "Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks. The code is available at https://github.com/sunnynexus/RetroLLM.",
            "score": 18,
            "issue_id": 1158,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "35265a6474f53410",
            "authors": [
                "Xiaoxi Li",
                "Jiajie Jin",
                "Yujia Zhou",
                "Yongkang Wu",
                "Zhonghua Li",
                "Qi Ye",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Huawei Poisson Lab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11919.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#hallucinations",
                    "#rag"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "RetroLLM: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "RetroLLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ FM-Ğ¸Ğ½Ğ´ĞµĞºÑĞ° Ğ¸ Ğ¾Ğ¿ĞµÑ€ĞµĞ¶Ğ°ÑÑ‰ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ RetroLLM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ĞºĞ°Ğº Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ½Ğµ ĞµĞµ."
                },
                "en": {
                    "title": "RetroLLM: Unifying Retrieval and Generation for Accurate Evidence Generation",
                    "desc": "This paper introduces RetroLLM, a novel framework that combines retrieval and generation in large language models (LLMs) to enhance their performance and reduce hallucinations. By integrating these processes, RetroLLM allows LLMs to generate precise evidence directly from a knowledge corpus while minimizing unnecessary input tokens. The framework employs hierarchical FM-Index constraints to filter relevant documents and a forward-looking constrained decoding strategy to ensure the accuracy of generated evidence. Experimental results show that RetroLLM outperforms existing methods on various open-domain question-answering datasets, demonstrating its effectiveness in both in-domain and out-of-domain scenarios."
                },
                "zh": {
                    "title": "æ•´åˆæ£€ç´¢ä¸ç”Ÿæˆï¼Œæå‡è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆèƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å‡ºç°å¹»è§‰ç°è±¡ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå¦‚é¢å¤–çš„æ£€ç´¢å™¨éƒ¨ç½²æˆæœ¬ã€ä»æ£€ç´¢æ–‡æœ¬å—ä¸­äº§ç”Ÿçš„å†—ä½™è¾“å…¥æ ‡è®°ï¼Œä»¥åŠæ£€ç´¢ä¸ç”Ÿæˆç¼ºä¹è”åˆä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RetroLLMï¼Œä¸€ä¸ªå°†æ£€ç´¢ä¸ç”Ÿæˆæ•´åˆä¸ºä¸€ä¸ªç»Ÿä¸€è¿‡ç¨‹çš„æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤Ÿç›´æ¥ä»è¯­æ–™åº“ä¸­ç”Ÿæˆç»†ç²’åº¦è¯æ®ï¼Œå¹¶è¿›è¡Œå—é™è§£ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±‚æ¬¡FM-Indexçº¦æŸå’Œå‰ç»æ€§å—é™è§£ç ç­–ç•¥ï¼Œä»¥æé«˜è¯æ®ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09645",
            "title": "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models",
            "url": "https://huggingface.co/papers/2412.09645",
            "abstract": "Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.",
            "score": 11,
            "issue_id": 1161,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "69f7aa2abe9671ed",
            "authors": [
                "Fan Zhang",
                "Shulin Tian",
                "Ziqi Huang",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09645.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#cv",
                    "#open_source",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Evaluation Agent. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Evaluation Agent Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´ Ğ½ÑƒĞ¶Ğ´Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ 10% Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Efficient and Tailored Evaluation for Visual Generative Models",
                    "desc": "This paper introduces the Evaluation Agent framework, designed to improve the evaluation of visual generative models like image and video generators. Traditional evaluation methods are slow and often fail to meet specific user needs, requiring extensive sampling that is computationally expensive. The Evaluation Agent mimics human evaluation by using fewer samples and providing detailed, tailored analyses, making the process more efficient and explainable. Experiments demonstrate that this framework can reduce evaluation time significantly while maintaining comparable results to existing methods."
                },
                "zh": {
                    "title": "é«˜æ•ˆè¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘ï¼Œè§†è§‰ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ä½¿å¾—é«˜è´¨é‡çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæˆä¸ºå¯èƒ½ï¼Œåº”ç”¨èŒƒå›´å¹¿æ³›ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦é‡‡æ ·æ•°ç™¾æˆ–æ•°åƒå¼ å›¾åƒæˆ–è§†é¢‘ï¼Œè¿™ä½¿å¾—è®¡ç®—è¿‡ç¨‹éå¸¸è€—æ—¶ï¼Œå°¤å…¶æ˜¯å¯¹äºåŸºäºæ‰©æ•£çš„æ¨¡å‹ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¾èµ–äºå›ºå®šçš„æµç¨‹ï¼Œå¿½è§†äº†ç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚ï¼Œå¹¶ä¸”æä¾›çš„æ•°å€¼ç»“æœç¼ºä¹æ¸…æ™°çš„è§£é‡Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è¯„ä¼°ä»£ç†æ¡†æ¶ï¼Œé‡‡ç”¨ç±»äººç­–ç•¥è¿›è¡Œé«˜æ•ˆã€åŠ¨æ€çš„å¤šè½®è¯„ä¼°ï¼Œä»…éœ€å°‘é‡æ ·æœ¬ï¼Œå¹¶æä¾›è¯¦ç»†çš„ç”¨æˆ·å®šåˆ¶åˆ†æã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11815",
            "title": "ColorFlow: Retrieval-Augmented Image Sequence Colorization",
            "url": "https://huggingface.co/papers/2412.11815",
            "abstract": "Automatic black-and-white image sequence colorization while preserving character and object identity (ID) is a complex task with significant market demand, such as in cartoon or comic series colorization. Despite advancements in visual colorization using large-scale generative models like diffusion models, challenges with controllability and identity consistency persist, making current solutions unsuitable for industrial application.To address this, we propose ColorFlow, a three-stage diffusion-based framework tailored for image sequence colorization in industrial applications. Unlike existing methods that require per-ID finetuning or explicit ID embedding extraction, we propose a novel robust and generalizable Retrieval Augmented Colorization pipeline for colorizing images with relevant color references. Our pipeline also features a dual-branch design: one branch for color identity extraction and the other for colorization, leveraging the strengths of diffusion models. We utilize the self-attention mechanism in diffusion models for strong in-context learning and color identity matching. To evaluate our model, we introduce ColorFlow-Bench, a comprehensive benchmark for reference-based colorization. Results show that ColorFlow outperforms existing models across multiple metrics, setting a new standard in sequential image colorization and potentially benefiting the art industry. We release our codes and models on our project page: https://zhuang2002.github.io/ColorFlow/.",
            "score": 10,
            "issue_id": 1161,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "de381dc70d0db48f",
            "authors": [
                "Junhao Zhuang",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Yong Liu",
                "Shiyi Zhang",
                "Chun Yuan",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11815.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#rag",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ColorFlow: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "ColorFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡Ñ‘Ñ€Ğ½Ğ¾-Ğ±ĞµĞ»Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞµÑ‘ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ColorFlow Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½ĞµÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Image Sequence Colorization with ColorFlow",
                    "desc": "This paper presents ColorFlow, a novel framework designed for colorizing black-and-white image sequences while maintaining the identity of characters and objects. It utilizes a three-stage diffusion model that enhances controllability and consistency, addressing limitations found in previous methods. The framework features a dual-branch architecture that separates color identity extraction from the colorization process, allowing for effective use of color references. The authors also introduce ColorFlow-Bench, a benchmark for evaluating colorization performance, demonstrating that their approach significantly outperforms existing techniques in the field."
                },
                "zh": {
                    "title": "ColorFlowï¼šå›¾åƒåºåˆ—ä¸Šè‰²çš„æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºColorFlowçš„ä¸‰é˜¶æ®µæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨ä¸ºé»‘ç™½å›¾åƒåºåˆ—ä¸Šè‰²ï¼ŒåŒæ—¶ä¿æŒè§’è‰²å’Œç‰©ä½“çš„èº«ä»½ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€ç´¢å¢å¼ºçš„ä¸Šè‰²ç®¡é“ï¼Œåˆ©ç”¨ç›¸å…³çš„é¢œè‰²å‚è€ƒè¿›è¡Œå›¾åƒä¸Šè‰²ï¼Œé¿å…äº†ç°æœ‰æ–¹æ³•ä¸­éœ€è¦é€ä¸ªèº«ä»½å¾®è°ƒçš„å¤æ‚æ€§ã€‚ColorFlowé‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡ï¼Œä¸€æ–¹é¢æå–é¢œè‰²èº«ä»½ï¼Œå¦ä¸€æ–¹é¢è¿›è¡Œä¸Šè‰²ï¼Œå……åˆ†åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚é€šè¿‡ColorFlow-BenchåŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸ºå›¾åƒåºåˆ—ä¸Šè‰²è®¾å®šäº†æ–°æ ‡å‡†ï¼Œå¯èƒ½å¯¹è‰ºæœ¯è¡Œä¸šå¸¦æ¥ç§¯æå½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12083",
            "title": "IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations",
            "url": "https://huggingface.co/papers/2412.12083",
            "abstract": "Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view inputs, while still struggling with inherent ambiguities between lighting and material. On the other hand, learning-based approaches leverage rich material priors from existing 3D object datasets but face challenges with maintaining multi-view consistency. In this paper, we introduce IDArb, a diffusion-based model designed to perform intrinsic decomposition on an arbitrary number of images under varying illuminations. Our method achieves accurate and multi-view consistent estimation on surface normals and material properties. This is made possible through a novel cross-view, cross-domain attention module and an illumination-augmented, view-adaptive training strategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides large-scale multi-view intrinsic data and renderings under diverse lighting conditions, supporting robust training. Extensive experiments demonstrate that IDArb outperforms state-of-the-art methods both qualitatively and quantitatively. Moreover, our approach facilitates a range of downstream tasks, including single-image relighting, photometric stereo, and 3D reconstruction, highlighting its broad applications in realistic 3D content creation.",
            "score": 10,
            "issue_id": 1158,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "333271d63ddd2102",
            "authors": [
                "Zhibing Li",
                "Tong Wu",
                "Jing Tan",
                "Mengchen Zhang",
                "Jiaqi Wang",
                "Dahua Lin"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12083.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#cv",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "IDArb - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¼ĞµĞ¶Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼ĞµĞ¶Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ARB-Objaverse Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Content Creation with IDArb",
                    "desc": "This paper presents IDArb, a diffusion-based model that addresses the challenge of intrinsic decomposition from multiple images with varying lighting conditions. Traditional methods are slow and struggle with ambiguities, while learning-based approaches often lack multi-view consistency. IDArb utilizes a novel cross-view, cross-domain attention mechanism and an illumination-augmented training strategy to achieve accurate estimations of surface normals and material properties. The introduction of the ARB-Objaverse dataset further enhances the model's training, leading to superior performance in various applications such as relighting and 3D reconstruction."
                },
                "zh": {
                    "title": "IDArbï¼šå¤šè§†è§’ä¸€è‡´çš„å†…åœ¨åˆ†è§£æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºIDArbçš„æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨ä»å¤šè§†è§’å›¾åƒä¸­è¿›è¡Œå†…åœ¨åˆ†è§£ï¼Œæ•æ‰å‡ ä½•å’Œææ–™ä¿¡æ¯ã€‚ä¸ä¼ ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒIDArbèƒ½å¤Ÿåœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹å®ç°å‡†ç¡®ä¸”å¤šè§†è§’ä¸€è‡´çš„è¡¨é¢æ³•çº¿å’Œææ–™å±æ€§ä¼°è®¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ARB-Objaverseï¼Œæä¾›äº†å¤§è§„æ¨¡çš„å¤šè§†è§’å†…åœ¨æ•°æ®ï¼Œæ”¯æŒæ¨¡å‹çš„ç¨³å¥è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDArbåœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12095",
            "title": "Causal Diffusion Transformers for Generative Modeling",
            "url": "https://huggingface.co/papers/2412.12095",
            "abstract": "We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data.",
            "score": 9,
            "issue_id": 1161,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "e5107a05a397194f",
            "authors": [
                "Chaorui Deng",
                "Deyao Zh",
                "Kunchang Li",
                "Shi Guan",
                "Haoqi Fan"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12095.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "CausalFusion: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Causal Diffusion - Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ CausalFusion - Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼ ÑˆÑƒĞ¼Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ImageNet, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸. CausalFusion Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº zero-shot Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "CausalFusion: Bridging Autoregressive and Diffusion Models for Enhanced Multimodal Generation",
                    "desc": "Causal Diffusion is a new approach that combines autoregressive (AR) models with diffusion models for predicting the next token in sequences. This method allows for better performance by introducing sequential factorization, which helps in transitioning smoothly between AR and diffusion generation modes. The proposed model, CausalFusion, is a transformer that effectively handles both discrete and continuous data, achieving top results in image generation tasks. Additionally, it demonstrates the ability to generate images and captions together, as well as perform image manipulations without prior training on specific tasks."
                },
                "zh": {
                    "title": "å› æœæ‰©æ•£ï¼šè‡ªå›å½’ä¸æ‰©æ•£æ¨¡å‹çš„å®Œç¾ç»“åˆ",
                    "desc": "æˆ‘ä»¬æå‡ºäº†å› æœæ‰©æ•£ï¼ˆCausal Diffusionï¼‰ï¼Œä½œä¸ºæ‰©æ•£æ¨¡å‹çš„è‡ªå›å½’ï¼ˆARï¼‰å¯¹åº”ç‰©ã€‚å®ƒæ˜¯ä¸€ç§å‹å¥½äºç¦»æ•£å’Œè¿ç»­æ¨¡å¼çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¡†æ¶ï¼Œå¹¶ä¸ç°æœ‰çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¨¡å‹ï¼ˆå¦‚LLaMAå’ŒGPTï¼‰å…¼å®¹ã€‚é€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥åºåˆ—å› å­åŒ–ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå¹¶å®ç°äº†è‡ªå›å½’å’Œæ‰©æ•£ç”Ÿæˆæ¨¡å¼ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å› æœèåˆï¼ˆCausalFusionï¼‰åœ¨å¤šæ¨¡æ€èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ï¼ŒåŒ…æ‹¬è”åˆå›¾åƒç”Ÿæˆå’Œæ ‡é¢˜ç”Ÿæˆï¼Œä»¥åŠé›¶-shotä¸Šä¸‹æ–‡å›¾åƒæ“ä½œçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11605",
            "title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models",
            "url": "https://huggingface.co/papers/2412.11605",
            "abstract": "Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often optimized by preference learning. However, existing methods often directly sample multiple independent responses from the model when creating preference pairs. Such practice can introduce content variations irrelevant to whether the instruction is precisely followed (e.g., different expressions about the same semantic), interfering with the goal of teaching models to recognize the key differences that lead to improved instruction following. In light of this, we introduce SPaR, a self-play framework integrating tree-search self-refinement to yield valid and comparable preference pairs free from distractions. By playing against itself, an LLM employs a tree-search strategy to refine its previous responses with respect to the instruction while minimizing unnecessary variations. Our experiments show that a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses GPT-4-Turbo on the IFEval benchmark without losing general capabilities. Furthermore, SPaR demonstrates promising scalability and transferability, greatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how inference scaling in tree search would impact model performance. Our code and data are publicly available at https://github.com/thu-coai/SPaR.",
            "score": 9,
            "issue_id": 1159,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "6e4d876c9f198e44",
            "authors": [
                "Jiale Cheng",
                "Xiao Liu",
                "Cunxiang Wang",
                "Xiaotao Gu",
                "Yida Lu",
                "Dan Zhang",
                "Yuxiao Dong",
                "Jie Tang",
                "Hongning Wang",
                "Minlie Huang"
            ],
            "affiliations": [
                "The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University",
                "The Knowledge Engineering Group (KEG), Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11605.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#rlhf",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "SPaR: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SPaR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², SPaR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ñƒ Ğ¸ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaMA3-8B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SPaR, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4-Turbo Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ IFEval. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Instruction Following with SPaR: A Self-Play Approach",
                    "desc": "This paper presents SPaR, a self-play framework designed to improve instruction-following capabilities in language models. It utilizes a tree-search strategy to refine responses, ensuring that preference pairs are valid and comparable without introducing irrelevant content variations. By training a LLaMA3-8B model with SPaR, the authors demonstrate that it outperforms GPT-4-Turbo on the IFEval benchmark while maintaining general performance. The framework also shows potential for scalability and transferability to larger models like GLM-4-9B and LLaMA3-70B, with insights on how tree search impacts inference performance."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å¯¹å¼ˆæå‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSPaRçš„è‡ªæˆ‘å¯¹å¼ˆæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹å¯¹æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›ã€‚é€šè¿‡æ ‘æœç´¢è‡ªæˆ‘ç²¾ç‚¼ï¼ŒSPaRèƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆä¸”å¯æ¯”è¾ƒçš„åå¥½å¯¹ï¼Œé¿å…äº†æ— å…³å†…å®¹çš„å¹²æ‰°ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡SPaRè®­ç»ƒçš„LLaMA3-8Bæ¨¡å‹åœ¨IFEvalåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†GPT-4-Turboï¼ŒåŒæ—¶ä¿æŒäº†å…¶é€šç”¨èƒ½åŠ›ã€‚SPaRè¿˜å±•ç¤ºäº†è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œè¿ç§»æ€§ï¼Œæ˜¾è‘—æå‡äº†GLM-4-9Bå’ŒLLaMA3-70Bç­‰æ¨¡å‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11231",
            "title": "Smaller Language Models Are Better Instruction Evolvers",
            "url": "https://huggingface.co/papers/2412.11231",
            "abstract": "Instruction tuning has been widely used to unleash the complete potential of large language models. Notably, complex and diverse instructions are of significant importance as they can effectively align models with various downstream tasks. However, current approaches to constructing large-scale instructions predominantly favour powerful models such as GPT-4 or those with over 70 billion parameters, under the empirical presumption that such larger language models (LLMs) inherently possess enhanced capabilities. In this study, we question this prevalent assumption and conduct an in-depth exploration into the potential of smaller language models (SLMs) in the context of instruction evolution. Extensive experiments across three scenarios of instruction evolution reveal that smaller language models (SLMs) can synthesize more effective instructions than LLMs. Further analysis demonstrates that SLMs possess a broader output space during instruction evolution, resulting in more complex and diverse variants. We also observe that the existing metrics fail to focus on the impact of the instructions. Thus, we propose Instruction Complex-Aware IFD (IC-IFD), which introduces instruction complexity in the original IFD score to evaluate the effectiveness of instruction data more accurately. Our source code is available at: https://github.com/HypherX/Evolution-Analysis{https://github.com/HypherX/Evolution-Analysis}",
            "score": 9,
            "issue_id": 1159,
            "pub_date": "2024-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "0fd693d18eb484a1",
            "authors": [
                "Tingfeng Hui",
                "Lulu Zhao",
                "Guanting Dong",
                "Yaqi Zhang",
                "Hua Zhou",
                "Sen Su"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence, BAAI, Beijing, China",
                "Beijing University of Posts and Telecommunications, Beijing, China",
                "Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11231.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#open_source",
                    "#alignment",
                    "#optimization"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM) Ğ² ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‡ĞµĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM), Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ IC-IFD Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ LLM Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ."
                },
                "en": {
                    "title": "Unlocking the Power of Smaller Models in Instruction Tuning",
                    "desc": "This paper investigates the effectiveness of smaller language models (SLMs) in instruction tuning, challenging the belief that larger models like GPT-4 are always superior. The authors conduct experiments showing that SLMs can generate more effective and diverse instructions compared to their larger counterparts. They highlight that SLMs have a wider output space, allowing for richer instruction variants during the evolution process. Additionally, the paper introduces a new metric, Instruction Complex-Aware IFD (IC-IFD), to better assess the impact of instruction complexity on model performance."
                },
                "zh": {
                    "title": "å°å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜æ½œåŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æŒ‡ä»¤è°ƒä¼˜åœ¨å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä¸­çš„æ½œåŠ›ï¼ŒæŒ‘æˆ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŒ‡ä»¤æ¼”å˜ä¸­çš„ä¸»å¯¼åœ°ä½ã€‚å®éªŒè¡¨æ˜ï¼ŒSLMsèƒ½å¤Ÿç”Ÿæˆæ¯”LLMsæ›´æœ‰æ•ˆçš„æŒ‡ä»¤ï¼Œä¸”åœ¨æŒ‡ä»¤æ¼”å˜è¿‡ç¨‹ä¸­å…·æœ‰æ›´å¹¿æ³›çš„è¾“å‡ºç©ºé—´ã€‚æˆ‘ä»¬è¿˜å‘ç°ç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡æœªèƒ½å……åˆ†è€ƒè™‘æŒ‡ä»¤çš„å½±å“ï¼Œå› æ­¤æå‡ºäº†æŒ‡ä»¤å¤æ‚åº¦æ„ŸçŸ¥çš„IFDï¼ˆIC-IFDï¼‰æ–¹æ³•ï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æŒ‡ä»¤æ•°æ®çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨å¯¹å°å‹è¯­è¨€æ¨¡å‹çš„ç†è§£å’Œåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12091",
            "title": "Wonderland: Navigating 3D Scenes from a Single Image",
            "url": "https://huggingface.co/papers/2412.12091",
            "abstract": "This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas. We propose a novel pipeline to overcome these limitations. Specifically, we introduce a large-scale reconstruction model that uses latents from a video diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images. For the first time, we demonstrate that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation.",
            "score": 7,
            "issue_id": 1161,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "56eac228e6d5c48b",
            "authors": [
                "Hanwen Liang",
                "Junli Cao",
                "Vidit Goel",
                "Guocheng Qian",
                "Sergei Korolev",
                "Demetri Terzopoulos",
                "Konstantinos N. Plataniotis",
                "Sergey Tulyakov",
                "Jian Ren"
            ],
            "affiliations": [
                "Snap Inc.",
                "University of California, Los Angeles",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12091.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞÑ‚ 2D Ğº 3D: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ 3D Gaussian Splatting. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Transforming Single Images into Rich 3D Worlds Efficiently!",
                    "desc": "This paper presents a new approach to generating high-quality 3D scenes from a single image, addressing limitations of existing methods that often require multiple views and extensive optimization. The authors introduce a large-scale reconstruction model that leverages latents from a video diffusion model, enabling the prediction of 3D Gaussian Splattings in a fast, feed-forward manner. By utilizing a video diffusion model that generates videos along specific camera paths, the method captures multi-view information while ensuring 3D consistency. The proposed model shows significant improvements in generating 3D scenes, especially for images not seen during training, marking a breakthrough in single-view 3D reconstruction."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡3Dåœºæ™¯çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•ä»å•å¼ ä»»æ„å›¾åƒé«˜æ•ˆåˆ›å»ºé«˜è´¨é‡ã€å¹¿èŒƒå›´çš„3Dåœºæ™¯ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´å¤šè§†å›¾æ•°æ®éœ€æ±‚ã€æ¯ä¸ªåœºæ™¯ä¼˜åŒ–è€—æ—¶ã€èƒŒæ™¯è§†è§‰è´¨é‡ä½ä»¥åŠæœªè§åŒºåŸŸé‡å»ºå¤±çœŸç­‰é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾é¢„æµ‹3Dé«˜æ–¯ç‚¹äº‘ï¼Œä»è€Œå…‹æœè¿™äº›é™åˆ¶ã€‚é€šè¿‡åœ¨è§†é¢‘æ½œåœ¨ç©ºé—´ä¸Šè®­ç»ƒ3Dé‡å»ºæ¨¡å‹ï¼Œæˆ‘ä»¬å®ç°äº†é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¹¿èŒƒå›´çš„é€šç”¨3Dåœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11258",
            "title": "GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs",
            "url": "https://huggingface.co/papers/2412.11258",
            "abstract": "Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. To address these challenges, we introduce GaussianProperty, a training-free framework that assigns physical properties of materials to 3D Gaussians. Specifically, we integrate the segmentation capability of SAM with the recognition capability of GPT-4V(ision) to formulate a global-local physical property reasoning module for 2D images. Then we project the physical properties from multi-view 2D images to 3D Gaussians using a voting strategy. We demonstrate that 3D Gaussians with physical property annotations enable applications in physics-based dynamic simulation and robotic grasping. For physics-based dynamic simulation, we leverage the Material Point Method (MPM) for realistic dynamic simulation. For robot grasping, we develop a grasping force prediction strategy that estimates a safe force range required for object grasping based on the estimated physical properties. Extensive experiments on material segmentation, physics-based dynamic simulation, and robotic grasping validate the effectiveness of our proposed method, highlighting its crucial role in understanding physical properties from visual data. Online demo, code, more cases and annotated datasets are available on https://Gaussian-Property.github.io{this https URL}.",
            "score": 6,
            "issue_id": 1158,
            "pub_date": "2024-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "b8115a0ffb05a0df",
            "authors": [
                "Xinli Xu",
                "Wenhang Ge",
                "Dicong Qiu",
                "ZhiFei Chen",
                "Dongyu Yan",
                "Zhuoyun Liu",
                "Haoyu Zhao",
                "Hanfeng Zhao",
                "Shunsi Zhang",
                "Junwei Liang",
                "Ying-Cong Chen"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11258.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#robotics"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "GaussianProperty: Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ² 3D Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "GaussianProperty - ÑÑ‚Ğ¾ Ğ±ĞµĞ·Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ SAM Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ GPT-4V Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ° 2D Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ½Ğ° 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ (MPM) Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ»Ñ‹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Physical Property Estimation with GaussianProperty",
                    "desc": "This paper presents GaussianProperty, a novel framework designed to estimate physical properties of materials from visual data without the need for extensive training. It combines the segmentation capabilities of SAM with the recognition abilities of GPT-4V(ision) to create a reasoning module that works on 2D images. The framework projects these properties into 3D Gaussians using a voting strategy, facilitating applications in dynamic simulation and robotic grasping. The authors demonstrate the effectiveness of their approach through experiments, showcasing its potential in enhancing physics-based simulations and improving robotic interactions with objects."
                },
                "zh": {
                    "title": "ä»è§†è§‰æ•°æ®ä¸­æå–ç‰©ç†å±æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGaussianPropertyçš„æ¡†æ¶ï¼Œç”¨äºä»è§†è§‰æ•°æ®ä¸­ä¼°è®¡ç‰©ç†å±æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†SAMçš„åˆ†å‰²èƒ½åŠ›å’ŒGPT-4V(ision)çš„è¯†åˆ«èƒ½åŠ›ï¼Œå½¢æˆäº†ä¸€ä¸ªé’ˆå¯¹2Då›¾åƒçš„å…¨å±€-å±€éƒ¨ç‰©ç†å±æ€§æ¨ç†æ¨¡å—ã€‚é€šè¿‡æŠ•ç¥¨ç­–ç•¥ï¼Œæˆ‘ä»¬å°†å¤šè§†è§’2Då›¾åƒçš„ç‰©ç†å±æ€§æŠ•å½±åˆ°3Dé«˜æ–¯åˆ†å¸ƒä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‰©ç†åŸºç¡€åŠ¨æ€ä»¿çœŸå’Œæœºå™¨äººæŠ“å–ç­‰åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11834",
            "title": "Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture",
            "url": "https://huggingface.co/papers/2412.11834",
            "abstract": "In order to make the foundation model more efficient and effective, our idea is combining sequence transformation and state transformation. First, we prove the availability of rotary position embedding in the state space duality algorithm, which reduces the perplexity of the hybrid quadratic causal self-attention and state space duality by more than 4%, to ensure that the combining sequence transformation unifies position encoding. Second, we propose dynamic mask attention, which maintains 100% accuracy in the more challenging multi-query associative recall task, improving by more than 150% compared to quadratic causal self-attention and state space duality, to ensure that the combining sequence transformation selectively filters relevant information. Third, we design cross domain mixture of experts, which makes the computational speed of expert retrieval with more than 1024 experts 8 to 10 times faster than the mixture of experts, to ensure that the combining state transformation quickly retrieval mixture. Finally, we summarize these matrix algorithms that can form the foundation model: Wonderful Matrices, which can be a competitor to popular model architectures.",
            "score": 4,
            "issue_id": 1159,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "7090d4cb4588f236",
            "authors": [
                "Jingze Shi",
                "Bingheng Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.11834.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ§ÑƒĞ´ĞµÑĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğµ Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ÑƒÑĞºĞ¾Ñ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Foundation Models with Efficient Transformations",
                    "desc": "This paper introduces a novel approach to enhance foundation models by integrating sequence transformation and state transformation techniques. It demonstrates the effectiveness of rotary position embedding in reducing perplexity in hybrid attention mechanisms, leading to improved performance. The authors also present dynamic mask attention, which significantly boosts accuracy in complex recall tasks while filtering relevant information efficiently. Additionally, they propose a cross-domain mixture of experts that accelerates expert retrieval, making it a competitive alternative to existing model architectures."
                },
                "zh": {
                    "title": "ç»“åˆåºåˆ—ä¸çŠ¶æ€å˜æ¢ï¼Œæå‡åŸºç¡€æ¨¡å‹æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆåºåˆ—å˜æ¢å’ŒçŠ¶æ€å˜æ¢çš„æ–¹æ³•ï¼Œä»¥æé«˜åŸºç¡€æ¨¡å‹çš„æ•ˆç‡å’Œæ•ˆæœã€‚æˆ‘ä»¬è¯æ˜äº†æ—‹è½¬ä½ç½®åµŒå…¥åœ¨çŠ¶æ€ç©ºé—´å¯¹å¶ç®—æ³•ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—é™ä½äº†æ··åˆäºŒæ¬¡å› æœè‡ªæ³¨æ„åŠ›å’ŒçŠ¶æ€ç©ºé—´å¯¹å¶çš„å›°æƒ‘åº¦ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åŠ¨æ€æ©ç æ³¨æ„åŠ›ï¼Œåœ¨å¤šæŸ¥è¯¢å…³è”å›å¿†ä»»åŠ¡ä¸­ä¿æŒ100%çš„å‡†ç¡®ç‡ï¼Œæå‡äº†150%ä»¥ä¸Šã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†è·¨åŸŸä¸“å®¶æ··åˆï¼Œä½¿å¾—ä¸“å®¶æ£€ç´¢çš„è®¡ç®—é€Ÿåº¦æ¯”ä¼ ç»Ÿæ–¹æ³•å¿«8åˆ°10å€ï¼Œå½¢æˆäº†å…·æœ‰ç«äº‰åŠ›çš„åŸºç¡€æ¨¡å‹ï¼šå¥‡å¦™çŸ©é˜µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11586",
            "title": "StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors",
            "url": "https://huggingface.co/papers/2412.11586",
            "abstract": "While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the general or entangled representation. We propose StrandHead, a novel text to 3D head avatar generation method capable of generating disentangled 3D hair with strand representation. Without using 3D data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative diffusion models. To this end, we propose a series of reliable priors on shape initialization, geometric primitives, and statistical haircut features, leading to a stable optimization and text-aligned performance. Extensive experiments show that StrandHead achieves the state-of-the-art reality and diversity of generated 3D head and hair. The generated 3D hair can also be easily implemented in the Unreal Engine for physical simulation and other applications. The code will be available at https://xiaokunsun.github.io/StrandHead.github.io.",
            "score": 4,
            "issue_id": 1159,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "9787d6a0befef45d",
            "authors": [
                "Xiaokun Sun",
                "Zeyu Cai",
                "Zhenyu Zhang",
                "Ying Tai",
                "Jian Yang"
            ],
            "affiliations": [
                "Nanjing University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11586.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ’‡",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-Ğ¿Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ StrandHead - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ»Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ 2D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ÑĞ´ĞµĞ¹ Ğ²Ğ¾Ğ»Ğ¾Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¿Ñ€Ğ¸Ñ‡ĞµÑĞ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ StrandHead Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡ĞµÑĞ¾Ğº."
                },
                "en": {
                    "title": "StrandHead: Realistic 3D Hair Generation from Text Prompts",
                    "desc": "This paper introduces StrandHead, a new method for creating 3D head avatars from text descriptions, focusing on generating realistic hair. Unlike previous methods that struggle with hair representation, StrandHead uses a unique approach to model hair as individual strands, allowing for more detailed and diverse outputs. The method leverages 2D generative diffusion models without needing 3D data, employing reliable priors to ensure stable optimization and alignment with text prompts. The results show that StrandHead outperforms existing techniques in generating lifelike 3D heads and hair, making it suitable for applications like physical simulation in gaming engines."
                },
                "zh": {
                    "title": "StrandHeadï¼šç”Ÿæˆç‹¬ç‰¹3Då‘å‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„3Då¤´åƒç”Ÿæˆæ–¹æ³•StrandHeadï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ç‹¬ç«‹å‘ä¸è¡¨ç¤ºçš„3Då¤´å‘ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äº3Dæ•°æ®è¿›è¡Œç›‘ç£ï¼Œè€Œæ˜¯é€šè¿‡æç‚¼2Dç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆé€¼çœŸçš„å‘ä¸ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—å¯é çš„å…ˆéªŒçŸ¥è¯†ï¼ŒåŒ…æ‹¬å½¢çŠ¶åˆå§‹åŒ–ã€å‡ ä½•åŸè¯­å’Œç»Ÿè®¡å‘å‹ç‰¹å¾ï¼Œä»è€Œå®ç°ç¨³å®šçš„ä¼˜åŒ–å’Œä¸æ–‡æœ¬å¯¹é½çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStrandHeadåœ¨ç”Ÿæˆ3Då¤´éƒ¨å’Œå¤´å‘çš„çœŸå®æ„Ÿå’Œå¤šæ ·æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10316",
            "title": "BrushEdit: All-In-One Image Inpainting and Editing",
            "url": "https://huggingface.co/papers/2412.10316",
            "abstract": "Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.",
            "score": 2,
            "issue_id": 1162,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 13",
                "zh": "12æœˆ13æ—¥"
            },
            "hash": "d8789f3e683b7c6b",
            "authors": [
                "Yaowei Li",
                "Yuxuan Bian",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Ying Shan",
                "Yuexian Zou",
                "Qiang Xu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Peking University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10316.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#cv",
                    "#diffusion",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ–Œï¸",
                "ru": {
                    "title": "BrushEdit: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BrushEdit - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑĞ¾Ğº Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑĞµĞ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Image Editing with Interactive Instruction and Inpainting",
                    "desc": "This paper introduces BrushEdit, a new method for image editing that combines instruction-based and inpainting techniques. It addresses the limitations of current methods by allowing users to interactively specify editing regions and intensity without being constrained to black-box operations. The system uses multimodal large language models (MLLMs) alongside a dual-branch image inpainting model to classify editing categories, identify main objects, and create masks for editing. Experimental results demonstrate that BrushEdit outperforms existing methods in preserving mask regions and maintaining coherent editing effects."
                },
                "zh": {
                    "title": "è‡ªç”±å½¢å¼çš„æ™ºèƒ½å›¾åƒç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œç§°ä¸ºBrushEditï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒç¼–è¾‘æŠ€æœ¯çš„å±€é™æ€§ã€‚é€šè¿‡ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œå›¾åƒä¿®å¤æ¨¡å‹ï¼Œè¯¥æ–¹æ³•å®ç°äº†ç”¨æˆ·å‹å¥½çš„è‡ªç”±å½¢å¼æŒ‡ä»¤ç¼–è¾‘ã€‚BrushEditèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«ç¼–è¾‘ç±»åˆ«ã€ä¸»è¦å¯¹è±¡ï¼Œå¹¶è·å–ç¼–è¾‘åŒºåŸŸçš„æ©ç ï¼Œä»è€Œæ”¯æŒæ›´å¤§å¹…åº¦çš„å›¾åƒä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿æŒæ©ç åŒºåŸŸå’Œç¼–è¾‘æ•ˆæœçš„ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11457",
            "title": "MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes",
            "url": "https://huggingface.co/papers/2412.11457",
            "abstract": "Repurposing pre-trained diffusion models has been proven to be effective for NVS. However, these methods are mostly limited to a single object; directly applying such methods to compositional multi-object scenarios yields inferior results, especially incorrect object placement and inconsistent shape and appearance under novel views. How to enhance and systematically evaluate the cross-view consistency of such models remains under-explored. To address this issue, we propose MOVIS to enhance the structural awareness of the view-conditioned diffusion model for multi-object NVS in terms of model inputs, auxiliary tasks, and training strategy. First, we inject structure-aware features, including depth and object mask, into the denoising U-Net to enhance the model's comprehension of object instances and their spatial relationships. Second, we introduce an auxiliary task requiring the model to simultaneously predict novel view object masks, further improving the model's capability in differentiating and placing objects. Finally, we conduct an in-depth analysis of the diffusion sampling process and carefully devise a structure-guided timestep sampling scheduler during training, which balances the learning of global object placement and fine-grained detail recovery. To systematically evaluate the plausibility of synthesized images, we propose to assess cross-view consistency and novel view object placement alongside existing image-level NVS metrics. Extensive experiments on challenging synthetic and realistic datasets demonstrate that our method exhibits strong generalization capabilities and produces consistent novel view synthesis, highlighting its potential to guide future 3D-aware multi-object NVS tasks.",
            "score": 1,
            "issue_id": 1158,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "415b98ca8c3ed003",
            "authors": [
                "Ruijie Lu",
                "Yixin Chen",
                "Junfeng Ni",
                "Baoxiong Jia",
                "Yu Liu",
                "Diwen Wan",
                "Gang Zeng",
                "Siyuan Huang"
            ],
            "affiliations": [
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "State Key Laboratory of General Artificial Intelligence, Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11457.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#training",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² ÑÑ†ĞµĞ½ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ MOVIS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² ÑÑ†ĞµĞ½ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ğ½ĞµĞ´Ñ€ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ² U-Net Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Multi-Object Novel View Synthesis with Structural Awareness",
                    "desc": "This paper introduces MOVIS, a method designed to improve multi-object novel view synthesis (NVS) using pre-trained diffusion models. The approach enhances the model's understanding of object structures by incorporating depth and object masks into the denoising U-Net architecture. Additionally, it introduces an auxiliary task that helps the model predict object masks for novel views, improving object differentiation and placement. The authors also propose a new sampling scheduler that balances global object placement with detailed recovery, and they evaluate the model's performance using cross-view consistency metrics alongside traditional NVS measures."
                },
                "zh": {
                    "title": "æå‡å¤šç‰©ä½“æ–°è§†è§’åˆæˆçš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMOVISçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šç‰©ä½“æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰ä¸­çš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å°†æ·±åº¦ä¿¡æ¯å’Œç‰©ä½“æ©ç ç­‰ç»“æ„æ„ŸçŸ¥ç‰¹å¾æ³¨å…¥å»å™ªU-Netï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç‰©ä½“å®ä¾‹åŠå…¶ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜è¢«è¦æ±‚åŒæ—¶é¢„æµ‹æ–°è§†è§’çš„ç‰©ä½“æ©ç ï¼Œä»è€Œå¢å¼ºå…¶åŒºåˆ†å’Œæ”¾ç½®ç‰©ä½“çš„èƒ½åŠ›ã€‚æœ€åï¼Œæœ¬æ–‡é€šè¿‡åˆ†ææ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œè®¾è®¡äº†ä¸€ç§ç»“æ„å¼•å¯¼çš„æ—¶é—´æ­¥é‡‡æ ·è°ƒåº¦å™¨ï¼Œä»¥å¹³è¡¡å…¨å±€ç‰©ä½“æ”¾ç½®å’Œç»†èŠ‚æ¢å¤çš„å­¦ä¹ ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-16.html",
    "link_next": "2024-12-18.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "16.12",
        "en": "12/16",
        "zh": "12æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.12",
        "en": "12/18",
        "zh": "12æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 5,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 0,
        "#3d": 5,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 8,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†GenExç³»ç»Ÿï¼Œå®ƒèƒ½é€šè¿‡å•å¼ RGBå›¾åƒç”Ÿæˆ3Dä¸€è‡´çš„æƒ³è±¡ç¯å¢ƒã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è™šå¹»å¼•æ“çš„3Dæ•°æ®ï¼Œå¸®åŠ©AIä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­è¿›è¡Œæ¢ç´¢å’Œå¯¼èˆªã€‚GenExå±•ç¤ºäº†é«˜è´¨é‡çš„ä¸–ç•Œç”Ÿæˆå’Œå¼ºå¤§çš„3Dèƒ½åŠ›ï¼Œå¦‚3Dæ˜ å°„å’Œå¾ªç¯ä¸€è‡´æ€§ã€‚æ–‡ç« æ€»ç»“è¯´ï¼ŒGenExä¸ºæå‡æƒ³è±¡ç©ºé—´ä¸­çš„ embodied AI æä¾›äº†å˜é©æ€§å¹³å°ï¼Œå¹¶æœ‰æ½œåŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œæ¢ç´¢ã€‚",
        "title": "GenEx: Generating an Explorable World",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†GenExç³»ç»Ÿï¼Œå®ƒèƒ½é€šè¿‡å•å¼ RGBå›¾åƒç”Ÿæˆ3Dä¸€è‡´çš„æƒ³è±¡ç¯å¢ƒã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è™šå¹»å¼•æ“çš„3Dæ•°æ®ï¼Œå¸®åŠ©AIä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­è¿›è¡Œæ¢ç´¢å’Œå¯¼èˆªã€‚GenExå±•ç¤ºäº†é«˜è´¨é‡çš„ä¸–ç•Œç”Ÿæˆå’Œå¼ºå¤§çš„3Dèƒ½åŠ›ï¼Œå¦‚3Dæ˜ å°„å’Œå¾ªç¯ä¸€è‡´æ€§ã€‚æ–‡ç« æ€»ç»“è¯´ï¼ŒGenExä¸ºæå‡æƒ³è±¡ç©ºé—´ä¸­çš„ embodied AI æä¾›äº†å˜é©æ€§å¹³å°ï¼Œå¹¶æœ‰æ½œåŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œæ¢ç´¢ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le GenEx xÃ¬ tÇ’ng, tÄ nÃ©ng tÅng guÃ² dÄn zhÄng RGB tÃº xÃ­ng shÄ“ng chÃ©ng 3D yÄ« zhÃ¬ de xiÇng xiÃ ng huÃ¡n jÃ¬ng. gÇi xÃ¬ tÇ’ng lÃ¬ yÃ²ng xÅ« huÃ n yÇn qÃ­ng de 3D shÃ¹ jÃ¹, bÄng zhÃ¹ AI dÃ i lÇ zÃ i fÃº zÃ  rÃ¨n wÃ¹ zhÅng jÃ¬n xÃ­ng tÃ n suÇ’ hÃ© dÇo hÃ¡ng. GenEx zhÇn shÃ¬ le gÄo zhÃ¬ liÃ ng de shÃ¬ jiÃ¨ shÄ“ng chÃ©ng hÃ© qiÃ¡ng dÃ  de 3D nÃ©ng lÃ¬, rÃº 3D yÇng shÃ¨ hÃ© xÃºn huÃ¡n yÄ« zhÃ¬ xÃ¬ng. wÃ©n zhÄng zÇ’ng jiÃ¨ shuÅ, GenEx wÃ¨i tÃ­ shÄ“ng xiÇng xiÃ ng kÅng zhÅng de embodied AI tÃ­ gÅng le biÃ n gÃ© xÃ¬ng pÃ­ng tÃ¡i, bÃ¬ng yÇ’u qiÃ¡n lÃ¬ kuÃ² zhÇn dÃ o xiÃ n shÃ­ shÃ¬ jiÃ¨ tÃ n suÇ’.",
        "vocab": "[{'word': 'GenEx', 'pinyin': '', 'trans': 'GenEx'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬tÇ’ng', 'trans': 'system'}, {'word': 'é€šè¿‡', 'pinyin': 'tÅngguÃ²', 'trans': 'through'}, {'word': 'å•å¼ ', 'pinyin': 'dÄn zhÄng', 'trans': 'single'}, {'word': 'RGB', 'pinyin': '', 'trans': 'RGB'}, {'word': 'å›¾åƒ', 'pinyin': 'tÃºxiÃ ng', 'trans': 'image'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'}, {'word': '3D', 'pinyin': '', 'trans': '3D'}, {'word': 'ä¸€è‡´', 'pinyin': 'yÄ«zhÃ¬', 'trans': 'consistent'}, {'word': 'æƒ³è±¡', 'pinyin': 'xiÇngxiÃ ng', 'trans': 'imaginary'}, {'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡njÃ¬ng', 'trans': 'environment'}, {'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬yÃ²ng', 'trans': 'utilize'}, {'word': 'è™šå¹»', 'pinyin': 'xÅ«huÃ n', 'trans': 'virtual'}, {'word': 'å¼•æ“', 'pinyin': 'yÇnqÃ­ng', 'trans': 'engine'}, {'word': 'æ•°æ®', 'pinyin': 'shÃ¹jÃ¹', 'trans': 'data'}, {'word': 'å¸®åŠ©', 'pinyin': 'bÄngzhÃ¹', 'trans': 'help'}, {'word': 'AI', 'pinyin': '', 'trans': 'AI'}, {'word': 'ä»£ç†', 'pinyin': 'dÃ ilÇ', 'trans': 'agent'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹zÃ¡', 'trans': 'complex'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'}, {'word': 'è¿›è¡Œ', 'pinyin': 'jÃ¬nxÃ­ng', 'trans': 'conduct'}, {'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'explore'}, {'word': 'å¯¼èˆª', 'pinyin': 'dÇohÃ¡ng', 'trans': 'navigate'}, {'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'demonstrate'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬liÃ ng', 'trans': 'high quality'}, {'word': 'ä¸–ç•Œ', 'pinyin': 'shÃ¬jiÃ¨', 'trans': 'world'}, {'word': 'å¼ºå¤§', 'pinyin': 'qiÃ¡ngdÃ ', 'trans': 'powerful'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'ability'}, {'word': 'æ˜ å°„', 'pinyin': 'yÃ¬ngshÃ¨', 'trans': 'mapping'}, {'word': 'å¾ªç¯', 'pinyin': 'xÃºnhuÃ¡n', 'trans': 'cyclic'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ«zhÃ¬xÃ¬ng', 'trans': 'consistency'}, {'word': 'æ€»ç»“', 'pinyin': 'zÇ’ngjiÃ©', 'trans': 'summarize'}, {'word': 'è¯´', 'pinyin': 'shuÅ', 'trans': 'say'}, {'word': 'æå‡', 'pinyin': 'tÃ­shÄ“ng', 'trans': 'enhance'}, {'word': 'ç©ºé—´', 'pinyin': 'kÅngjiÄn', 'trans': 'space'}, {'word': 'embodied', 'pinyin': '', 'trans': 'embodied'}, {'word': 'å˜é©æ€§', 'pinyin': 'biÃ ngÃ©xÃ¬ng', 'trans': 'transformative'}, {'word': 'å¹³å°', 'pinyin': 'pÃ­ngtÃ¡i', 'trans': 'platform'}, {'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡nlÃ¬', 'trans': 'potential'}, {'word': 'æ‰©å±•', 'pinyin': 'kuÃ²zhÇn', 'trans': 'expand'}, {'word': 'ç°å®', 'pinyin': 'xiÃ nshÃ­', 'trans': 'real'}, {'word': 'ä¸–ç•Œ', 'pinyin': 'shÃ¬jiÃ¨', 'trans': 'world'}, {'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'explore'}]",
        "trans": "This article introduces the GenEx system, which can generate a 3D-consistent imagined environment from a single RGB image. The system leverages 3D data from the Unreal Engine to assist AI agents in exploration and navigation in complex tasks. GenEx demonstrates high-quality world generation and powerful 3D capabilities, such as 3D mapping and cyclic consistency. The article concludes that GenEx provides a transformative platform for enhancing embodied AI in imagined spaces and has the potential to extend to real-world exploration.",
        "update_ts": "2024-12-16 09:12"
    }
}