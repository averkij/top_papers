{
    "date": {
        "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 31",
        "zh": "1æœˆ31æ—¥"
    },
    "time_utc": "2025-01-31 04:12",
    "weekday": 4,
    "issue_id": 1964,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.18492",
            "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
            "url": "https://huggingface.co/papers/2501.18492",
            "abstract": "As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/.",
            "score": 3,
            "issue_id": 1964,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "2ea0bf2a655fc703",
            "authors": [
                "Yue Liu",
                "Hongcheng Gao",
                "Shengfang Zhai",
                "Jun Xia",
                "Tianyi Wu",
                "Zhiwei Xue",
                "Yulin Chen",
                "Kenji Kawaguchi",
                "Jiaheng Zhang",
                "Bryan Hooi"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Chinese"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18492.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ Ğ°Ğ·ÑƒĞ¼Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GuardReasoner - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ GuardReasonerTrain Ñ 127 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ 460 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 13 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GuardReasoner Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4 Ğ¸ LLaMA Guard."
                },
                "en": {
                    "title": "GuardReasoner: Enhancing LLM Safety through Advanced Reasoning Techniques",
                    "desc": "This paper introduces GuardReasoner, a novel safeguard designed to enhance the safety of large language models (LLMs) in critical applications. It presents a new dataset, GuardReasonerTrain, containing 127,000 samples with 460,000 reasoning steps to train guard models effectively. The authors implement reasoning Supervised Fine-Tuning (SFT) and hard sample Direct Preference Optimization (DPO) to improve the reasoning capabilities of these models. Experimental results show that GuardReasoner outperforms existing models like GPT-4o+CoT and LLaMA Guard 3 in terms of performance, explainability, and generalizability across multiple benchmarks."
                },
                "zh": {
                    "title": "GuardReasonerï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å½±å“æ—¥ç›Šå¢åŠ ï¼Œç¡®ä¿å…¶å®‰å…¨æ€§æˆä¸ºä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¿æŠ¤æœºåˆ¶GuardReasonerï¼Œé€šè¿‡å¼•å¯¼ä¿æŠ¤æ¨¡å‹å­¦ä¹ æ¨ç†æ¥å¢å¼ºå®‰å…¨æ€§ã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºäº†GuardReasonerTrainæ•°æ®é›†ï¼ŒåŒ…å«127Kæ ·æœ¬å’Œ460Kè¯¦ç»†æ¨ç†æ­¥éª¤ï¼Œå¹¶å¼•å…¥æ¨ç†SFTä»¥è§£é”ä¿æŠ¤æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†å›°éš¾æ ·æœ¬DPOï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-30.html",
    "link_next": "2025-02-03.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "30.01",
        "en": "01/30",
        "zh": "1æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "03.02",
        "en": "02/03",
        "zh": "2æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« æŒ‘æˆ˜äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†æ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ç­–ç•¥ã€‚CFTè®©æ¨¡å‹å­¦ä¹ æ‰¹è¯„æœ‰å™ªéŸ³çš„å“åº”ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿æ­£ç¡®çš„å“åº”ã€‚å—å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´çš„äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼ŒCFTé¼“åŠ±æ›´æ·±å…¥çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†CFTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šä¼˜äºSFTã€‚",
        "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
        "pinyin": "è¿™ç¯‡æ–‡ç« æŒ‘æˆ˜äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†æ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ç­–ç•¥ã€‚CFTè®©æ¨¡å‹å­¦ä¹ æ‰¹è¯„æœ‰å™ªéŸ³çš„å“åº”ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿æ­£ç¡®çš„å“åº”ã€‚å—å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´çš„äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼ŒCFTé¼“åŠ±æ›´æ·±å…¥çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†CFTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šä¼˜äºSFTã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tiÇozhÃ n le jiÃ ndÅ« wÄ“itiÃ¡o (SFT) de fÃ nshÃ¬, tÃ­chÅ« le pÄ«pÃ­ng wÄ“itiÃ¡o (CFT) cÃ¨lÃ¼Ã¨. CFT rÃ ng mÃ³xÃ­ng xuÃ©xÃ­ pÄ«pÃ­ng yÇ’u zÃ oyÄ«n de xiÇngyÃ¬ng, Ã©r bÃ¹shÃ¬ jiÇndÄn de mÃ³fÇng zhÃ¨ngquÃ¨ de xiÇngyÃ¬ng. ShÃ²u qiÃ¡ngdiÃ o pÄ«pÃ n xÃ¬ng sÄ«wÃ©i de rÃ©nlÃ¨i xuÃ©xÃ­ guÃ²chÃ©ng qÇfÄ, CFT gÇ”lÃ¬ gÃ¨ng shÄ“nrÃ¹ de fÄ“nxi hÃ© xÃ¬zhÃ¬ de lÇjiÄ›. ZuÃ²zhÄ› tÅngguÃ² shÃ¬yÃ n yÃ nzhÃ¨ng le CFT de yÇ’uxiÃ oxÃ¬ng, jiÃ©guÇ’ xiÇnshÃ¬ CFT zÃ i duÅgÃ¨ shÃ¹xuÃ© jÄ«zhÇ”n shÃ ng yÅuyÃº SFT.",
        "vocab": "[{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},\n{'word': 'ç›‘ç£', 'pinyin': 'jiÃ n dÅ«', 'trans': 'supervise'},\n{'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tune'},\n{'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'æ‰¹è¯„', 'pinyin': 'pÄ« pÃ­ng', 'trans': 'criticize'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'å™ªéŸ³', 'pinyin': 'zÃ o yÄ«n', 'trans': 'noise'},\n{'word': 'å“åº”', 'pinyin': 'xiÇng yÃ¬ng', 'trans': 'response'},\n{'word': 'æ¨¡ä»¿', 'pinyin': 'mÃ³ fÇng', 'trans': 'imitate'},\n{'word': 'å¯å‘', 'pinyin': 'qÇ fÄ', 'trans': 'inspire'},\n{'word': 'æ‰¹åˆ¤æ€§', 'pinyin': 'pÄ« pÃ n xÃ¬ng', 'trans': 'critical'},\n{'word': 'æ€ç»´', 'pinyin': 'sÄ« wÃ©i', 'trans': 'thinking'},\n{'word': 'é¼“åŠ±', 'pinyin': 'gÇ” lÃ¬', 'trans': 'encourage'},\n{'word': 'æ·±å…¥', 'pinyin': 'shÄ“n rÃ¹', 'trans': 'in-depth'},\n{'word': 'ç»†è‡´', 'pinyin': 'xÃ¬ zhÃ¬', 'trans': 'detailed'},\n{'word': 'éªŒè¯', 'pinyin': 'yÃ n zhÃ¨ng', 'trans': 'verify'},\n{'word': 'æœ‰æ•ˆæ€§', 'pinyin': 'yÇ’u xiÃ o xÃ¬ng', 'trans': 'effectiveness'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}]",
        "trans": "This article challenges the paradigm of Supervised Fine-Tuning (SFT) by proposing the Critical Fine-Tuning (CFT) strategy. CFT allows the model to learn to critique noisy responses rather than simply mimicking correct responses. Inspired by the human learning process that emphasizes critical thinking, CFT encourages deeper analysis and detailed understanding. The authors validated the effectiveness of CFT through experiments, with results showing that CFT outperforms SFT on multiple mathematical benchmarks.",
        "update_ts": "2025-01-30 09:10"
    }
}