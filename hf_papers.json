{
    "date": {
        "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 26",
        "zh": "11æœˆ26æ—¥"
    },
    "time_utc": "2024-11-26 21:09",
    "weekday": 1,
    "issue_id": 798,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.15138",
            "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
            "url": "https://huggingface.co/papers/2411.15138",
            "abstract": "We present Material Anything, a fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as a dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing a progressive material generation strategy guided by these confidence masks, along with a UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across a wide range of object categories and lighting conditions.",
            "score": 32,
            "issue_id": 776,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "34b8f6718115f1e3",
            "authors": [
                "Xin Huang",
                "Tengfei Wang",
                "Ziwei Liu",
                "Qing Wang"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15138.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Material Anything - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ½ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ñ€Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Ğ¼Ğ°ÑĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ĞµĞ»ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ… Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Automating Realistic Material Generation for 3D Objects",
                    "desc": "Material Anything is a novel framework that automates the generation of realistic materials for 3D objects using a unified diffusion approach. It simplifies the material creation process by eliminating the need for complex workflows and optimizations tailored to specific cases. The framework utilizes a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss, to ensure high-quality and stable material outputs. By incorporating confidence masks, it dynamically adapts to different object types and lighting scenarios, resulting in consistent and UV-ready materials across various conditions."
                },
                "zh": {
                    "title": "å…¨è‡ªåŠ¨ææ–™ç”Ÿæˆï¼Œé€‚åº”å¤šç§å…‰ç…§æ¡ä»¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMaterial Anythingçš„å…¨è‡ªåŠ¨ç»Ÿä¸€æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ä¸º3Dç‰©ä½“ç”ŸæˆåŸºäºç‰©ç†çš„ææ–™ã€‚ä¸ç°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚æµç¨‹æˆ–ç‰¹å®šä¼˜åŒ–ä¸åŒï¼ŒMaterial Anythingæä¾›äº†ä¸€ç§ç¨³å¥çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œé€‚åº”ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„ç‰©ä½“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¸‰å¤´æ¶æ„å’Œæ¸²æŸ“æŸå¤±æ¥æé«˜ç¨³å®šæ€§å’Œææ–™è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç½®ä¿¡æ©ç ä½œä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„åŠ¨æ€åˆ‡æ¢å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æœ‰çº¹ç†å’Œæ— çº¹ç†çš„ç‰©ä½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15466",
            "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator",
            "url": "https://huggingface.co/papers/2411.15466",
            "abstract": "Subject-driven text-to-image generation aims to produce images of a new subject within a desired context by accurately capturing both the visual characteristics of the subject and the semantic content of a text prompt. Traditional methods rely on time- and resource-intensive fine-tuning for subject alignment, while recent zero-shot approaches leverage on-the-fly image prompting, often sacrificing subject alignment. In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Diptych Prompting arranges an incomplete diptych with the reference image in the left panel, and performs text-conditioned inpainting on the right panel. We further prevent unwanted content leakage by removing the background in the reference image and improve fine-grained details in the generated subject by enhancing attention weights between the panels during inpainting. Experimental results confirm that our approach significantly outperforms zero-shot image prompting methods, resulting in images that are visually preferred by users. Additionally, our method supports not only subject-driven generation but also stylized image generation and subject-driven image editing, demonstrating versatility across diverse image generation applications. Project page: https://diptychprompting.github.io/",
            "score": 27,
            "issue_id": 777,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 23",
                "zh": "11æœˆ23æ—¥"
            },
            "hash": "288600e8c54930f4",
            "authors": [
                "Chaehun Shin",
                "Jooyoung Choi",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15466.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Diptych Prompting: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Diptych Prompting. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ¿Ñ‚Ğ¸Ñ…Ğ° Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… text-to-image Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ»ĞµĞ²Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ¿Ñ‚Ğ¸Ñ…Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ°Ğ²ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°. Diptych Prompting Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ zero-shot Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Diptych Prompting: Zero-Shot Image Generation with Subject Precision",
                    "desc": "This paper presents Diptych Prompting, a new method for generating images from text prompts while maintaining accurate subject alignment. Unlike traditional methods that require extensive fine-tuning, this zero-shot approach treats the task as inpainting, using a diptych format with a reference image. By focusing on the left panel for the reference and performing text-conditioned inpainting on the right, the method enhances detail and prevents unwanted content from leaking into the generated image. The results show that Diptych Prompting not only improves visual quality but also allows for versatile applications in stylized image generation and editing."
                },
                "zh": {
                    "title": "Diptych Promptingï¼šç²¾å‡†çš„ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é›¶-shotæ–¹æ³•ï¼Œç§°ä¸ºDiptych Promptingï¼Œæ—¨åœ¨å®ç°ä¸»é¢˜é©±åŠ¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ç”Ÿæˆä»»åŠ¡é‡æ–°è§£é‡Šä¸ºå›¾åƒä¿®è¡¥ï¼Œç¡®ä¿äº†ä¸»é¢˜çš„ç²¾ç¡®å¯¹é½ã€‚Diptych Promptingåˆ©ç”¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŒè”ç”Ÿæˆç‰¹æ€§ï¼Œå·¦ä¾§é¢æ¿å±•ç¤ºå‚è€ƒå›¾åƒï¼Œå³ä¾§é¢æ¿è¿›è¡Œæ–‡æœ¬æ¡ä»¶çš„ä¿®è¡¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰æ•ˆæœä¸Šä¼˜äºä¼ ç»Ÿçš„é›¶-shotå›¾åƒæç¤ºæ–¹æ³•ï¼Œä¸”æ”¯æŒå¤šç§å›¾åƒç”Ÿæˆåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16594",
            "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
            "url": "https://huggingface.co/papers/2411.16594",
            "abstract": "Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and https://llm-as-a-judge.github.io.",
            "score": 17,
            "issue_id": 778,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "56883eb77dcb5fa3",
            "authors": [
                "Dawei Li",
                "Bohan Jiang",
                "Liangjie Huang",
                "Alimohammad Beigi",
                "Chengshuai Zhao",
                "Zhen Tan",
                "Amrita Bhattacharjee",
                "Yuxuan Jiang",
                "Canyu Chen",
                "Tianhao Wu",
                "Kai Shu",
                "Lu Cheng",
                "Huan Liu"
            ],
            "affiliations": [
                "Arizona State University",
                "Emory University",
                "Illinois Institute of Technology",
                "University of California, Berkeley",
                "University of Illinois Chicago",
                "University of Maryland, Baltimore County"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16594.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº ÑÑƒĞ´ÑŒÑ: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² AI Ğ¸ NLP",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° 'LLM-as-a-judge', Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ Ñ‡Ñ‚Ğ¾, ĞºĞ°Ğº Ğ¸ Ğ³Ğ´Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Harnessing LLMs for Enhanced AI Evaluation",
                    "desc": "This paper discusses the challenges of assessment and evaluation in artificial intelligence and natural language processing, particularly focusing on the limitations of traditional methods. It introduces the innovative concept of using Large Language Models (LLMs) as judges to score, rank, or select outputs in various tasks. The authors provide a detailed framework that categorizes LLM-based judgment into three key areas: what to judge, how to judge, and where to judge. Additionally, the paper compiles benchmarks for evaluating these models and identifies future research directions to enhance the effectiveness of LLMs in assessment tasks."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼šè¯„åˆ¤çš„æ–°åŠ›é‡",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°å’Œåˆ¤æ–­ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†â€œLLMä½œä¸ºè¯„åˆ¤è€…â€çš„æ–°èŒƒå¼ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ¤æ–­ç»†å¾®çš„å±æ€§ï¼Œè€ŒLLMèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­è¿›è¡Œæ‰“åˆ†ã€æ’åå’Œé€‰æ‹©ã€‚æˆ‘ä»¬ä»è¾“å…¥å’Œè¾“å‡ºçš„è§’åº¦è¯¦ç»†å®šä¹‰äº†è¯„åˆ¤çš„æ¦‚å¿µï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„åˆ†ç±»æ³•ï¼Œæ¢è®¨äº†è¯„åˆ¤çš„å†…å®¹ã€æ–¹å¼å’Œåœºæ‰€ã€‚æœ€åï¼Œæˆ‘ä»¬ç¼–åˆ¶äº†è¯„ä¼°LLMä½œä¸ºè¯„åˆ¤è€…çš„åŸºå‡†ï¼Œå¹¶å¼ºè°ƒäº†å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16205",
            "title": "MH-MoE:Multi-Head Mixture-of-Experts",
            "url": "https://huggingface.co/papers/2411.16205",
            "abstract": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by using the multi-head mechanism to collectively attend to information from various representation spaces within different experts. In this paper, we present a novel implementation of MH-MoE that maintains both FLOPs and parameter parity with sparse Mixture of Experts models. Experimental results on language models show that the new implementation yields quality improvements over both vanilla MoE and fine-grained MoE models. Additionally, our experiments demonstrate that MH-MoE is compatible with 1-bit Large Language Models (LLMs) such as BitNet.",
            "score": 15,
            "issue_id": 778,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "b684b6d745cb66ff",
            "authors": [
                "Shaohan Huang",
                "Xun Wu",
                "Shuming Ma",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16205.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ñ ÑĞ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MH-MoE), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ¿Ğ¾ FLOP Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ MoE Ğ¸ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ñ‹Ğ¼Ğ¸ MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. MH-MoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ MH-MoE Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº BitNet."
                },
                "en": {
                    "title": "Unlocking Performance with Multi-Head Mixture-of-Experts",
                    "desc": "The Multi-Head Mixture-of-Experts (MH-MoE) model enhances performance by allowing multiple heads to focus on different aspects of data from various experts. This paper introduces a new way to implement MH-MoE that keeps the same computational cost and number of parameters as traditional sparse Mixture of Experts models. Experiments conducted on language models reveal that this new approach provides better results compared to standard MoE and fine-grained MoE models. Furthermore, the findings indicate that MH-MoE can effectively work with 1-bit Large Language Models like BitNet."
                },
                "zh": {
                    "title": "å¤šå¤´æ··åˆä¸“å®¶ï¼šæå‡æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "å¤šå¤´æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMH-MoEï¼‰é€šè¿‡å¤šå¤´æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶å…³æ³¨æ¥è‡ªä¸åŒä¸“å®¶çš„å¤šç§è¡¨ç¤ºç©ºé—´çš„ä¿¡æ¯ï¼Œä»è€Œå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„MH-MoEå®ç°ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—é‡ï¼ˆFLOPsï¼‰å’Œå‚æ•°æ•°é‡ä¸Šä¸ç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–°å®ç°ç›¸è¾ƒäºä¼ ç»Ÿçš„MoEå’Œç»†ç²’åº¦MoEæ¨¡å‹åœ¨è¯­è¨€æ¨¡å‹ä¸Šæœ‰æ˜¾è‘—çš„è´¨é‡æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¿˜è¡¨æ˜MH-MoEä¸1ä½å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BitNetï¼‰å…¼å®¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15611",
            "title": "Knowledge Transfer Across Modalities with Natural Language Supervision",
            "url": "https://huggingface.co/papers/2411.15611",
            "abstract": "We present a way to learn novel concepts by only using their textual description. We call this method Knowledge Transfer. Similarly to human perception, we leverage cross-modal interaction to introduce new concepts. We hypothesize that in a pre-trained visual encoder there are enough low-level features already learned (e.g. shape, appearance, color) that can be used to describe previously unknown high-level concepts. Provided with a textual description of the novel concept, our method works by aligning the known low-level features of the visual encoder to its high-level textual description. We show that Knowledge Transfer can successfully introduce novel concepts in multimodal models, in a very efficient manner, by only requiring a single description of the target concept. Our approach is compatible with both separate textual and visual encoders (e.g. CLIP) and shared parameters across modalities. We also show that, following the same principle, Knowledge Transfer can improve concepts already known by the model. Leveraging Knowledge Transfer we improve zero-shot performance across different tasks such as classification, segmentation, image-text retrieval, and captioning.",
            "score": 13,
            "issue_id": 784,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 23",
                "zh": "11æœˆ23æ—¥"
            },
            "hash": "117dd5d3b47d2583",
            "authors": [
                "Carlo Alberto Barbano",
                "Luca Molinaro",
                "Emanuele Aiello",
                "Marco Grangetto"
            ],
            "affiliations": [
                "Politecnico di Torino",
                "University of Turin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15611.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#transfer_learning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹' (Knowledge Transfer), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğµ ÑƒĞ¶Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ÑÑ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ñ Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹."
                },
                "en": {
                    "title": "Learn New Concepts with Just Words!",
                    "desc": "This paper introduces a method called Knowledge Transfer, which allows machine learning models to learn new concepts using only their textual descriptions. The approach utilizes a pre-trained visual encoder that has already learned basic features like shape and color, which can be aligned with high-level textual descriptions of new concepts. By doing this, the model can efficiently incorporate novel concepts into its understanding without needing extensive retraining. Additionally, Knowledge Transfer enhances the model's performance on existing concepts and improves its zero-shot capabilities across various tasks such as classification and image-text retrieval."
                },
                "zh": {
                    "title": "çŸ¥è¯†è¿ç§»ï¼šé€šè¿‡æ–‡æœ¬æè¿°å­¦ä¹ æ–°æ¦‚å¿µ",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡æ–‡æœ¬æè¿°å­¦ä¹ æ–°æ¦‚å¿µçš„æ–¹æ³•ï¼Œç§°ä¸ºçŸ¥è¯†è¿ç§»ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è·¨æ¨¡æ€äº¤äº’ï¼Œç±»ä¼¼äºäººç±»çš„æ„ŸçŸ¥ï¼Œæ¥å¼•å…¥æ–°æ¦‚å¿µã€‚æˆ‘ä»¬å‡è®¾é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸­å·²ç»å­¦ä¹ äº†è¶³å¤Ÿçš„ä½çº§ç‰¹å¾ï¼ˆå¦‚å½¢çŠ¶ã€å¤–è§‚ã€é¢œè‰²ï¼‰ï¼Œå¯ä»¥ç”¨æ¥æè¿°ä¹‹å‰æœªçŸ¥çš„é«˜çº§æ¦‚å¿µã€‚é€šè¿‡å¯¹ç›®æ ‡æ¦‚å¿µçš„å•ä¸€æ–‡æœ¬æè¿°è¿›è¡Œå¯¹é½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­å¼•å…¥æ–°æ¦‚å¿µï¼Œå¹¶æå‡æ¨¡å‹åœ¨åˆ†ç±»ã€åˆ†å‰²ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œæ ‡é¢˜ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šçš„é›¶-shotæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16489",
            "title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?",
            "url": "https://huggingface.co/papers/2411.16489",
            "abstract": "This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount.",
            "score": 13,
            "issue_id": 780,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "9d7613aad6cae404",
            "authors": [
                "Zhen Huang",
                "Haoyang Zou",
                "Xuefeng Li",
                "Yixiu Liu",
                "Yuxiang Zheng",
                "Ethan Chern",
                "Shijie Xia",
                "Yiwei Qin",
                "Weizhe Yuan",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Generative AI Research Lab (GAIR)",
                "NYU",
                "SII",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16489.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#data",
                    "#math",
                    "#benchmark",
                    "#transfer_learning",
                    "#hallucinations",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ O1 Ğ¾Ñ‚ OpenAI, ÑƒĞ´ĞµĞ»ÑÑ Ğ¾ÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ, Ğ½Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½ĞµÑ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ· API O1 Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´ĞµÑÑÑ‚ĞºĞ°Ñ… Ñ‚Ñ‹ÑÑÑ‡ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· O1 Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ O1-preview Ğ½Ğ° Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğµ AIME Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· O1, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Unlocking AI Potential: The Power of Transparent Distillation",
                    "desc": "This paper critically analyzes the methods used to replicate the capabilities of OpenAI's O1 model, emphasizing the often hidden use of knowledge distillation techniques. The authors demonstrate that fine-tuning a base model with data distilled from O1's API can lead to better performance on complex mathematical reasoning tasks compared to the original O1 model. Their experiments reveal that models trained on O1-distilled data not only excel in math but also generalize well to other tasks, showing reduced biases and improved safety. The study advocates for transparency in AI research and highlights the importance of foundational understanding in developing advanced AI systems."
                },
                "zh": {
                    "title": "çŸ¥è¯†è’¸é¦ï¼šæå‡AIæ¨¡å‹æ€§èƒ½çš„å…³é”®",
                    "desc": "æœ¬æ–‡å¯¹å½“å‰å¤åˆ¶OpenAI O1æ¨¡å‹èƒ½åŠ›çš„æ–¹æ³•è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œç‰¹åˆ«å…³æ³¨çŸ¥è¯†è’¸é¦æŠ€æœ¯çš„å¹¿æ³›ä½¿ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç®€å•çš„ä»O1 APIè¿›è¡Œè’¸é¦ï¼Œå¹¶ç»“åˆç›‘ç£å¾®è°ƒï¼Œå¯ä»¥åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°ä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„åŸºç¡€æ¨¡å‹åœ¨ç¾å›½é‚€è¯·æ•°å­¦è€ƒè¯•ï¼ˆAIMEï¼‰ä¸­è¡¨ç°ä¼˜äºO1é¢„è§ˆï¼Œä¸”æŠ€æœ¯å¤æ‚æ€§è¾ƒä½ã€‚æ­¤å¤–ï¼Œå°½ç®¡æ¨¡å‹ä»…åœ¨æ•°å­¦é—®é¢˜è§£å†³æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨å¼€æ”¾å¼é—®ç­”ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16657",
            "title": "DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation",
            "url": "https://huggingface.co/papers/2411.16657",
            "abstract": "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.",
            "score": 13,
            "issue_id": 777,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "02cf3312e8d1f6ca",
            "authors": [
                "Zun Wang",
                "Jialu Li",
                "Han Lin",
                "Jaehong Yoon",
                "Mohit Bansal"
            ],
            "affiliations": [
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16657.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#video",
                    "#story_generation"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "DreamRunner: ĞÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "DreamRunner - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ğµ. DreamRunner Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ñ…."
                },
                "en": {
                    "title": "DreamRunner: Crafting Seamless Storytelling Videos from Text",
                    "desc": "This paper introduces DreamRunner, a new method for generating storytelling videos from text scripts. It addresses challenges in creating complex motions and maintaining object consistency across scenes by using a large language model for scene planning and a retrieval-augmented approach for motion customization. The method incorporates a novel spatial-temporal region-based attention module to ensure precise object-motion binding and semantic control in video frames. DreamRunner outperforms existing models in character consistency and smooth transitions, showcasing its effectiveness in generating multi-object interactions and adhering to compositional text prompts."
                },
                "zh": {
                    "title": "DreamRunnerï¼šåˆ›æ–°çš„æ•…äº‹è§†é¢‘ç”Ÿæˆæ–¹æ³•",
                    "desc": "æ•…äº‹è§†é¢‘ç”Ÿæˆï¼ˆSVGï¼‰æ˜¯ä¸€é¡¹æ–°å…´ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è¾“å…¥æ–‡æœ¬è„šæœ¬åˆ›å»ºé•¿ç¯‡ã€å¤šåŠ¨ä½œã€å¤šåœºæ™¯çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•é¢ä¸´ç€å¤šä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¹è±¡éœ€è¦å±•ç°å¤æ‚çš„ç»†å¾®åŠ¨ä½œï¼Œä»¥åŠå¤šä¸ªå¯¹è±¡åœ¨ä¸åŒåœºæ™¯ä¸­çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DreamRunnerï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ•…äº‹åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œåœºæ™¯è§„åˆ’å’Œå¯¹è±¡å¸ƒå±€ã€‚DreamRunnerè¿˜å¼•å…¥äº†ç©ºé—´-æ—¶é—´åŒºåŸŸåŸºç¡€çš„3Dæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„å¯¹è±¡åŠ¨ä½œç»‘å®šå’Œé€å¸§è¯­ä¹‰æ§åˆ¶ï¼Œå±•ç°å‡ºåœ¨è§’è‰²ä¸€è‡´æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢çš„å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14522",
            "title": "GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI",
            "url": "https://huggingface.co/papers/2411.14522",
            "abstract": "Despite significant advancements in general artificial intelligence, such as GPT-4, their effectiveness in the medical domain (general medical AI, GMAI) remains constrained due to the absence of specialized medical knowledge. To address this challenge, we present GMAI-VL-5.5M, a comprehensive multimodal medical dataset created by converting hundreds of specialized medical datasets into meticulously constructed image-text pairs. This dataset features comprehensive task coverage, diverse modalities, and high-quality image-text data. Building upon this multimodal dataset, we propose GMAI-VL, a general medical vision-language model with a progressively three-stage training strategy. This approach significantly enhances the model's ability by integrating visual and textual information, thereby improving its ability to process multimodal data and support accurate diagnosis and clinical decision-making. Experimental evaluations demonstrate that GMAI-VL achieves state-of-the-art results across a wide range of multimodal medical tasks, such as visual question answering and medical image diagnosis. Our contributions include the development of the GMAI-VL-5.5M dataset, the introduction of the GMAI-VL model, and the establishment of new benchmarks in multiple medical domains. Code and dataset will be released at https://github.com/uni-medical/GMAI-VL.",
            "score": 11,
            "issue_id": 778,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "eb0e262f1661d5c8",
            "authors": [
                "Tianbin Li",
                "Yanzhou Su",
                "Wei Li",
                "Bin Fu",
                "Zhe Chen",
                "Ziyan Huang",
                "Guoan Wang",
                "Chenglong Ma",
                "Ying Chen",
                "Ming Hu",
                "Yanjun Li",
                "Pengcheng Chen",
                "Xiaowei Hu",
                "Zhongying Deng",
                "Yuanfeng Ji",
                "Jin Ye",
                "Yu Qiao",
                "Junjun He"
            ],
            "affiliations": [
                "East China Normal University",
                "Fudan University",
                "Monash University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences",
                "Stanford University",
                "University of Cambridge",
                "University of Washington",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14522.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#multimodal",
                    "#optimization",
                    "#science",
                    "#healthcare",
                    "#dataset"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "GMAI-VL: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ GMAI-VL-5.5M - Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ‚ĞµĞ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¿Ğ°Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GMAI-VL - Ğ¾Ğ±Ñ‰Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ¾ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸. GMAI-VL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Empowering Medical AI with Multimodal Learning",
                    "desc": "This paper introduces GMAI-VL-5.5M, a new multimodal medical dataset designed to enhance general medical AI (GMAI) by combining various specialized medical datasets into image-text pairs. The dataset supports a wide range of medical tasks and includes high-quality data that improves the model's understanding of both visual and textual information. The authors propose a three-stage training strategy for the GMAI-VL model, which significantly boosts its performance in tasks like visual question answering and medical image diagnosis. Experimental results show that GMAI-VL sets new benchmarks in the medical domain, demonstrating its effectiveness in aiding clinical decision-making."
                },
                "zh": {
                    "title": "åŒ»å­¦é¢†åŸŸçš„å¤šæ¨¡æ€æ™ºèƒ½çªç ´",
                    "desc": "å°½ç®¡é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆå¦‚GPT-4ï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸçš„æœ‰æ•ˆæ€§ä»ç„¶å—åˆ°é™åˆ¶ï¼Œå› ä¸ºç¼ºä¹ä¸“ä¸šçš„åŒ»å­¦çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GMAI-VL-5.5Mï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å°†æ•°ç™¾ä¸ªä¸“ä¸šåŒ»å­¦æ•°æ®é›†è½¬æ¢ä¸ºç²¾å¿ƒæ„å»ºçš„å›¾åƒ-æ–‡æœ¬å¯¹è€Œåˆ›å»ºçš„ç»¼åˆå¤šæ¨¡æ€åŒ»å­¦æ•°æ®é›†ã€‚åŸºäºè¿™ä¸ªå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†GMAI-VLï¼Œä¸€ä¸ªå…·æœ‰é€æ­¥ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥çš„é€šç”¨åŒ»å­¦è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒGMAI-VLåœ¨è§†è§‰é—®ç­”å’ŒåŒ»å­¦å›¾åƒè¯Šæ–­ç­‰å¤šç§å¤šæ¨¡æ€åŒ»å­¦ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16318",
            "title": "One Diffusion to Generate Them All",
            "url": "https://huggingface.co/papers/2411.16318",
            "abstract": "We introduce OneDiffusion, a versatile, large-scale diffusion model that seamlessly supports bidirectional image synthesis and understanding across diverse tasks. It enables conditional generation from inputs such as text, depth, pose, layout, and semantic maps, while also handling tasks like image deblurring, upscaling, and reverse processes such as depth estimation and segmentation. Additionally, OneDiffusion allows for multi-view generation, camera pose estimation, and instant personalization using sequential image inputs. Our model takes a straightforward yet effective approach by treating all tasks as frame sequences with varying noise scales during training, allowing any frame to act as a conditioning image at inference time. Our unified training framework removes the need for specialized architectures, supports scalable multi-task training, and adapts smoothly to any resolution, enhancing both generalization and scalability. Experimental results demonstrate competitive performance across tasks in both generation and prediction such as text-to-image, multiview generation, ID preservation, depth estimation and camera pose estimation despite relatively small training dataset. Our code and checkpoint are freely available at https://github.com/lehduong/OneDiffusion",
            "score": 10,
            "issue_id": 782,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "2719da6b8a88249f",
            "authors": [
                "Duong H. Le",
                "Tuan Pham",
                "Sangho Lee",
                "Christopher Clark",
                "Aniruddha Kembhavi",
                "Stephan Mandt",
                "Ranjay Krishna",
                "Jiasen Lu"
            ],
            "affiliations": [
                "AI2",
                "University of California, Irvine",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16318.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "OneDiffusion - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¿Ğ¾Ğ·Ñ‹, Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ°Ñ€Ñ‚, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. OneDiffusion Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ Ğ²ÑĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ ÑˆÑƒĞ¼Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "OneDiffusion: Unifying Image Synthesis and Understanding with Versatile Diffusion",
                    "desc": "OneDiffusion is a powerful diffusion model designed for various image synthesis and understanding tasks. It can generate images based on different inputs like text and depth, and also perform tasks such as image deblurring and segmentation. The model treats all tasks as sequences of frames with different noise levels, allowing flexibility during inference. Its unified training approach enhances scalability and generalization, achieving strong performance across multiple tasks with a relatively small dataset."
                },
                "zh": {
                    "title": "OneDiffusionï¼šå¤šä»»åŠ¡å›¾åƒç”Ÿæˆä¸ç†è§£çš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "OneDiffusionæ˜¯ä¸€ç§å¤šåŠŸèƒ½çš„å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ”¯æŒåŒå‘å›¾åƒåˆæˆå’Œç†è§£ã€‚å®ƒå¯ä»¥æ ¹æ®æ–‡æœ¬ã€æ·±åº¦ã€å§¿æ€ã€å¸ƒå±€å’Œè¯­ä¹‰å›¾ç­‰è¾“å…¥è¿›è¡Œæ¡ä»¶ç”Ÿæˆï¼ŒåŒæ—¶å¤„ç†å›¾åƒå»æ¨¡ç³Šã€æ”¾å¤§å’Œæ·±åº¦ä¼°è®¡ç­‰ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†æ‰€æœ‰ä»»åŠ¡è§†ä¸ºå…·æœ‰ä¸åŒå™ªå£°å°ºåº¦çš„å¸§åºåˆ—è¿›è¡Œè®­ç»ƒï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å…è®¸åœ¨æ¨ç†æ—¶ä½¿ç”¨ä»»æ„å¸§ä½œä¸ºæ¡ä»¶å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOneDiffusionåœ¨ç”Ÿæˆå’Œé¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°½ç®¡è®­ç»ƒæ•°æ®é›†ç›¸å¯¹è¾ƒå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16034",
            "title": "VisualLens: Personalization through Visual History",
            "url": "https://huggingface.co/papers/2411.16034",
            "abstract": "We hypothesize that a user's visual history with images reflecting their daily life, offers valuable insights into their interests and preferences, and can be leveraged for personalization. Among the many challenges to achieve this goal, the foremost is the diversity and noises in the visual history, containing images not necessarily related to a recommendation task, not necessarily reflecting the user's interest, or even not necessarily preference-relevant. Existing recommendation systems either rely on task-specific user interaction logs, such as online shopping history for shopping recommendations, or focus on text signals. We propose a novel approach, VisualLens, that extracts, filters, and refines image representations, and leverages these signals for personalization. We created two new benchmarks with task-agnostic visual histories, and show that our method improves over state-of-the-art recommendations by 5-10% on Hit@3, and improves over GPT-4o by 2-5%. Our approach paves the way for personalized recommendations in scenarios where traditional methods fail.",
            "score": 10,
            "issue_id": 781,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "5eece5978da5fe1d",
            "authors": [
                "Wang Bill Zhu",
                "Deqing Fu",
                "Kai Sun",
                "Yi Lu",
                "Zhaojiang Lin",
                "Seungwhan Moon",
                "Kanika Narang",
                "Mustafa Canim",
                "Yue Liu",
                "Anuj Kumar",
                "Xin Luna Dong"
            ],
            "affiliations": [
                "Meta",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16034.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ VisualLens Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5-10% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Hit@3 Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Personalization Through Visual History",
                    "desc": "This paper introduces VisualLens, a new method for enhancing personalized recommendations by utilizing a user's visual history of images. The authors argue that these images, which reflect daily life, can provide insights into user interests and preferences, despite challenges like noise and irrelevant content. VisualLens processes and refines image representations to filter out unhelpful data, improving the quality of recommendations. The results show that this approach outperforms existing systems, achieving a 5-10% increase in recommendation accuracy on new benchmarks."
                },
                "zh": {
                    "title": "åˆ©ç”¨è§†è§‰å†å²å®ç°ä¸ªæ€§åŒ–æ¨è",
                    "desc": "æˆ‘ä»¬å‡è®¾ç”¨æˆ·çš„è§†è§‰å†å²å¯ä»¥æä¾›å…³äºä»–ä»¬å…´è¶£å’Œåå¥½çš„é‡è¦ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥ç”¨äºä¸ªæ€§åŒ–æ¨èã€‚å½“å‰çš„æ¨èç³»ç»Ÿé€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„ç”¨æˆ·äº¤äº’è®°å½•ï¼Œæˆ–è€…å…³æ³¨æ–‡æœ¬ä¿¡å·ï¼Œè€Œå¿½è§†äº†è§†è§‰ä¿¡æ¯çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼ŒVisualLensï¼Œé€šè¿‡æå–ã€è¿‡æ»¤å’Œä¼˜åŒ–å›¾åƒè¡¨ç¤ºï¼Œåˆ©ç”¨è¿™äº›ä¿¡å·è¿›è¡Œä¸ªæ€§åŒ–æ¨èã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— ä»»åŠ¡è§†è§‰å†å²çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨èæ•ˆæœæ¯”ç°æœ‰æŠ€æœ¯æé«˜äº†5-10%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16681",
            "title": "Factorized Visual Tokenization and Generation",
            "url": "https://huggingface.co/papers/2411.16681",
            "abstract": "Visual tokenizers are fundamental to image generation. They convert visual data into discrete tokens, enabling transformer-based models to excel at image generation. Despite their success, VQ-based tokenizers like VQGAN face significant limitations due to constrained vocabulary sizes. Simply expanding the codebook often leads to training instability and diminishing performance gains, making scalability a critical challenge. In this work, we introduce Factorized Quantization (FQ), a novel approach that revitalizes VQ-based tokenizers by decomposing a large codebook into multiple independent sub-codebooks. This factorization reduces the lookup complexity of large codebooks, enabling more efficient and scalable visual tokenization. To ensure each sub-codebook captures distinct and complementary information, we propose a disentanglement regularization that explicitly reduces redundancy, promoting diversity across the sub-codebooks. Furthermore, we integrate representation learning into the training process, leveraging pretrained vision models like CLIP and DINO to infuse semantic richness into the learned representations. This design ensures our tokenizer captures diverse semantic levels, leading to more expressive and disentangled representations. Experiments show that the proposed FQGAN model substantially improves the reconstruction quality of visual tokenizers, achieving state-of-the-art performance. We further demonstrate that this tokenizer can be effectively adapted into auto-regressive image generation. https://showlab.github.io/FQGAN",
            "score": 10,
            "issue_id": 780,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "966d673404fb7a77",
            "authors": [
                "Zechen Bai",
                "Jianxiong Gao",
                "Ziteng Gao",
                "Pichao Wang",
                "Zheng Zhang",
                "Tong He",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Amazon",
                "Fudan University",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16681.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ¤Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - Ğ¤Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (FQ). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ĞºĞ¾Ğ´Ğ±ÑƒĞº Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ´-ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ´-ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ°Ğ¼Ğ¸. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CLIP Ğ¸ DINO, Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revitalizing Image Generation with Factorized Quantization",
                    "desc": "This paper presents a new method called Factorized Quantization (FQ) to improve visual tokenizers used in image generation. Traditional VQ-based tokenizers struggle with limited vocabulary sizes, which can hinder performance and scalability. FQ addresses this by breaking down a large codebook into smaller, independent sub-codebooks, reducing complexity and enhancing efficiency. Additionally, the method incorporates disentanglement regularization and representation learning to ensure diverse and rich semantic representations, leading to better image generation results."
                },
                "zh": {
                    "title": "å› å­åŒ–é‡åŒ–ï¼šæå‡è§†è§‰æ ‡è®°å™¨çš„æ•ˆç‡ä¸è¡¨ç°",
                    "desc": "è§†è§‰æ ‡è®°å™¨æ˜¯å›¾åƒç”Ÿæˆçš„åŸºç¡€ï¼Œå®ƒå°†è§†è§‰æ•°æ®è½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°ï¼Œä½¿åŸºäºå˜æ¢å™¨çš„æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å°½ç®¡VQGANç­‰åŸºäºVQçš„æ ‡è®°å™¨å–å¾—äº†ä¸€å®šæˆåŠŸï¼Œä½†ç”±äºè¯æ±‡é‡æœ‰é™ï¼Œå®ƒä»¬é¢ä¸´ç€æ˜¾è‘—çš„å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”å› å­åŒ–é‡åŒ–ï¼ˆFQï¼‰ï¼Œé€šè¿‡å°†å¤§å‹ä»£ç æœ¬åˆ†è§£ä¸ºå¤šä¸ªç‹¬ç«‹çš„å­ä»£ç æœ¬ï¼Œæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä»è€Œæé«˜äº†è§†è§‰æ ‡è®°åŒ–çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFQGANæ¨¡å‹æ˜¾è‘—æé«˜äº†è§†è§‰æ ‡è®°å™¨çš„é‡å»ºè´¨é‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14525",
            "title": "SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation",
            "url": "https://huggingface.co/papers/2411.14525",
            "abstract": "Computed Tomography (CT) is one of the most popular modalities for medical imaging. By far, CT images have contributed to the largest publicly available datasets for volumetric medical segmentation tasks, covering full-body anatomical structures. Large amounts of full-body CT images provide the opportunity to pre-train powerful models, e.g., STU-Net pre-trained in a supervised fashion, to segment numerous anatomical structures. However, it remains unclear in which conditions these pre-trained models can be transferred to various downstream medical segmentation tasks, particularly segmenting the other modalities and diverse targets. To address this problem, a large-scale benchmark for comprehensive evaluation is crucial for finding these conditions. Thus, we collected 87 public datasets varying in modality, target, and sample size to evaluate the transfer ability of full-body CT pre-trained models. We then employed a representative model, STU-Net with multiple model scales, to conduct transfer learning across modalities and targets. Our experimental results show that (1) there may be a bottleneck effect concerning the dataset size in fine-tuning, with more improvement on both small- and large-scale datasets than medium-size ones. (2) Models pre-trained on full-body CT demonstrate effective modality transfer, adapting well to other modalities such as MRI. (3) Pre-training on the full-body CT not only supports strong performance in structure detection but also shows efficacy in lesion detection, showcasing adaptability across target tasks. We hope that this large-scale open evaluation of transfer learning can direct future research in volumetric medical image segmentation.",
            "score": 6,
            "issue_id": 778,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "eb68ad946d69ba56",
            "authors": [
                "Jin Ye",
                "Ying Chen",
                "Yanjun Li",
                "Haoyu Wang",
                "Zhongying Deng",
                "Ziyan Huang",
                "Yanzhou Su",
                "Chenglong Ma",
                "Yuanfeng Ji",
                "Junjun He"
            ],
            "affiliations": [
                "East China Normal University",
                "Shanghai AI Laboratory",
                "Stanford University",
                "University of Cambridge",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14525.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#healthcare",
                    "#open_source",
                    "#training",
                    "#transfer_learning",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ĞšĞ¢-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ĞšĞ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 87 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ĞœĞ Ğ¢) Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚ 'Ğ±ÑƒÑ‚Ñ‹Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ñ€Ğ»Ñ‹ÑˆĞºĞ°' Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Transfer Learning in Medical Imaging with CT Datasets",
                    "desc": "This paper investigates the transferability of models pre-trained on full-body CT images for various medical segmentation tasks. It highlights the effectiveness of the STU-Net model in adapting to different imaging modalities, such as MRI, and diverse anatomical targets. The study reveals that dataset size impacts fine-tuning performance, with both small and large datasets yielding better results than medium-sized ones. Overall, the findings emphasize the potential of using large-scale CT datasets to enhance model performance in medical image segmentation tasks."
                },
                "zh": {
                    "title": "å…¨èº«CTé¢„è®­ç»ƒæ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ç ”ç©¶",
                    "desc": "è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯åŒ»å­¦æˆåƒä¸­æœ€å¸¸ç”¨çš„æŠ€æœ¯ä¹‹ä¸€ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨ä¸åŒæ¡ä»¶ä¸‹ï¼ŒåŸºäºå…¨èº«CTé¢„è®­ç»ƒæ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶ä»–æˆåƒæ¨¡æ€å’Œå¤šæ ·åŒ–ç›®æ ‡çš„åˆ†å‰²ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬æ”¶é›†äº†87ä¸ªå…¬å…±æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œæ•°æ®é›†å¤§å°å¯¹å¾®è°ƒæœ‰ç“¶é¢ˆæ•ˆåº”ï¼Œä¸”å…¨èº«CTé¢„è®­ç»ƒæ¨¡å‹åœ¨è¿ç§»åˆ°å…¶ä»–æ¨¡æ€ï¼ˆå¦‚MRIï¼‰æ—¶è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¸Œæœ›ä¸ºæœªæ¥çš„ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ç ”ç©¶æä¾›æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16341",
            "title": "From CISC to RISC: language-model guided assembly transpilation",
            "url": "https://huggingface.co/papers/2411.16341",
            "abstract": "The transition from x86 to ARM architecture is becoming increasingly common across various domains, primarily driven by ARM's energy efficiency and improved performance across traditional sectors. However, this ISA shift poses significant challenges, mainly due to the extensive legacy ecosystem of x86 software and lack of portability across proprietary ecosystems and software stacks. This paper introduces CRT, a lightweight LLM-based transpiler that automatically converts x86 assembly to ARM assembly. Our approach bridges the fundamental architectural gap between x86's CISC-based and ARM's RISC-based computing paradigms while preserving program semantics and optimizing performance. We evaluate CRT on diverse real-world applications, achieving 79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite, and an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2 hardware (ARMv8), our transpiled code achieves 1.73times speedup compared to Apple's Rosetta 2 virtualization engine, while delivering 2.41times memory efficiency and 1.47times better energy consumption. Through testing and analysis, we show that CRT successfully navigates the CISC/RISC divide and generates correctly executable RISC code despite machine ``language'' barriers. We release our code, models, training datasets, and benchmarks at: https://ahmedheakl.github.io/asm2asm/.",
            "score": 6,
            "issue_id": 778,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "1d28eaae89074f91",
            "authors": [
                "Ahmed Heakl",
                "Chaimaa Abi",
                "Rania Hossam",
                "Abdulrahman Mahmoud"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16341.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° CISC/RISC: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ñ x86 Ğ² ARM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CRT - Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°ÑÑĞµĞ¼Ğ±Ğ»ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ x86 Ğ² Ğ°ÑÑĞµĞ¼Ğ±Ğ»ĞµÑ€ ARM. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼Ğ¸ CISC (x86) Ğ¸ RISC (ARM), ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ CRT Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 79.25% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ x86 Ğ½Ğ° ARMv5. Ğ’ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ½Ğ° Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Apple M2 (ARMv8) Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 1.73 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ¼ Apple Rosetta 2."
                },
                "en": {
                    "title": "Bridging the CISC/RISC Divide with CRT Transpiler",
                    "desc": "This paper presents CRT, a lightweight LLM-based transpiler designed to convert x86 assembly code into ARM assembly code. The transition from x86 to ARM architecture is challenging due to the differences in their instruction set architectures (ISAs), specifically x86's Complex Instruction Set Computing (CISC) and ARM's Reduced Instruction Set Computing (RISC). CRT effectively bridges this gap while maintaining the original program's functionality and optimizing performance. The evaluation shows that CRT achieves high translation accuracy and outperforms existing solutions in terms of speed, memory efficiency, and energy consumption on ARM hardware."
                },
                "zh": {
                    "title": "è½»æ¾è·¨è¶Šæ¶æ„é¸¿æ²Ÿï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCRTçš„è½»é‡çº§LLMåŸºç¡€çš„è½¬è¯‘å™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†x86æ±‡ç¼–ä»£ç è½¬æ¢ä¸ºARMæ±‡ç¼–ä»£ç ã€‚è¯¥æ–¹æ³•è§£å†³äº†x86çš„å¤æ‚æŒ‡ä»¤é›†ï¼ˆCISCï¼‰ä¸ARMçš„ç²¾ç®€æŒ‡ä»¤é›†ï¼ˆRISCï¼‰ä¹‹é—´çš„æ¶æ„å·®å¼‚ï¼ŒåŒæ—¶ä¿æŒç¨‹åºè¯­ä¹‰å¹¶ä¼˜åŒ–æ€§èƒ½ã€‚é€šè¿‡åœ¨çœŸå®åº”ç”¨ä¸Šçš„è¯„ä¼°ï¼ŒCRTåœ¨x86åˆ°ARMv5çš„è½¬æ¢å‡†ç¡®ç‡è¾¾åˆ°79.25%ï¼Œåœ¨x86åˆ°RISC-Vçš„è½¬æ¢å‡†ç¡®ç‡è¾¾åˆ°88.68%ã€‚åœ¨Apple M2ç¡¬ä»¶ä¸Šçš„å®é™…éƒ¨ç½²ä¸­ï¼Œè½¬è¯‘åçš„ä»£ç ç›¸æ¯”äºAppleçš„Rosetta 2è™šæ‹ŸåŒ–å¼•æ“å®ç°äº†1.73å€çš„é€Ÿåº¦æå‡å’Œ2.41å€çš„å†…å­˜æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12814",
            "title": "Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline",
            "url": "https://huggingface.co/papers/2411.12814",
            "abstract": "Interactive Medical Image Segmentation (IMIS) has long been constrained by the limited availability of large-scale, diverse, and densely annotated datasets, which hinders model generalization and consistent evaluation across different models. In this paper, we introduce the IMed-361M benchmark dataset, a significant advancement in general IMIS research. First, we collect and standardize over 6.4 million medical images and their corresponding ground truth masks from multiple data sources. Then, leveraging the strong object recognition capabilities of a vision foundational model, we automatically generated dense interactive masks for each image and ensured their quality through rigorous quality control and granularity management. Unlike previous datasets, which are limited by specific modalities or sparse annotations, IMed-361M spans 14 modalities and 204 segmentation targets, totaling 361 million masks-an average of 56 masks per image. Finally, we developed an IMIS baseline network on this dataset that supports high-quality mask generation through interactive inputs, including clicks, bounding boxes, text prompts, and their combinations. We evaluate its performance on medical image segmentation tasks from multiple perspectives, demonstrating superior accuracy and scalability compared to existing interactive segmentation models. To facilitate research on foundational models in medical computer vision, we release the IMed-361M and model at https://github.com/uni-medical/IMIS-Bench.",
            "score": 5,
            "issue_id": 789,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "693c4e9e00a65f7c",
            "authors": [
                "Junlong Cheng",
                "Bin Fu",
                "Jin Ye",
                "Guoan Wang",
                "Tianbin Li",
                "Haoyu Wang",
                "Ruoyu Li",
                "He Yao",
                "Junren Chen",
                "Jingwen Li",
                "Yanzhou Su",
                "Min Zhu",
                "Junjun He"
            ],
            "affiliations": [
                "East China Normal University, School of computer science and technology",
                "Monash University",
                "Shanghai AI Laboratory, General Medical Artificial Intelligence",
                "Shanghai Jiao Tong University, School of biomedical engineering",
                "Sichuan University, School of Computer Science",
                "Xinjiang University, School of Computer Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12814.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#science",
                    "#benchmark",
                    "#healthcare",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: IMed-361M Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IMed-361M Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 6,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 361 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ 14 Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ 204 Ñ†ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ IMIS, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Medical Image Segmentation with IMed-361M",
                    "desc": "This paper presents the IMed-361M benchmark dataset, which addresses the challenges in Interactive Medical Image Segmentation (IMIS) caused by the lack of large and diverse annotated datasets. The dataset includes over 6.4 million medical images and 361 million dense interactive masks, covering 14 different modalities and 204 segmentation targets. By utilizing a vision foundational model, the authors generated high-quality masks and established a baseline network that supports various interactive input methods for improved segmentation accuracy. The results show that this new dataset and model significantly enhance the performance and scalability of medical image segmentation tasks compared to existing methods."
                },
                "zh": {
                    "title": "IMISç ”ç©¶çš„æ–°é‡Œç¨‹ç¢‘ï¼šIMed-361Mæ•°æ®é›†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†IMed-361MåŸºå‡†æ•°æ®é›†ï¼Œè¿™æ˜¯äº’åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆIMISï¼‰ç ”ç©¶çš„é‡è¦è¿›å±•ã€‚æˆ‘ä»¬æ”¶é›†å¹¶æ ‡å‡†åŒ–äº†è¶…è¿‡640ä¸‡å¼ åŒ»å­¦å›¾åƒåŠå…¶å¯¹åº”çš„çœŸå®æ©è†œï¼Œæ¶µç›–14ç§æ¨¡æ€å’Œ204ä¸ªåˆ†å‰²ç›®æ ‡ï¼Œå…±è®¡3.61äº¿ä¸ªæ©è†œã€‚é€šè¿‡åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„å¼ºå¤§ç‰©ä½“è¯†åˆ«èƒ½åŠ›ï¼Œæˆ‘ä»¬è‡ªåŠ¨ç”Ÿæˆäº†å¯†é›†çš„äº’åŠ¨æ©è†œï¼Œå¹¶é€šè¿‡ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ç¡®ä¿å…¶è´¨é‡ã€‚æˆ‘ä»¬è¿˜åœ¨è¯¥æ•°æ®é›†ä¸Šå¼€å‘äº†IMISåŸºçº¿ç½‘ç»œï¼Œæ”¯æŒé€šè¿‡ç‚¹å‡»ã€è¾¹ç•Œæ¡†å’Œæ–‡æœ¬æç¤ºç­‰äº’åŠ¨è¾“å…¥ç”Ÿæˆé«˜è´¨é‡æ©è†œï¼Œå±•ç¤ºäº†ä¼˜äºç°æœ‰æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16443",
            "title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis",
            "url": "https://huggingface.co/papers/2411.16443",
            "abstract": "Text-based generation and editing of 3D scenes hold significant potential for streamlining content creation through intuitive user interactions. While recent advances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time rendering, existing methods are often specialized and task-focused, lacking a unified framework for both generation and editing. In this paper, we introduce SplatFlow, a comprehensive framework that addresses this gap by enabling direct 3DGS generation and editing. SplatFlow comprises two main components: a multi-view rectified flow (RF) model and a Gaussian Splatting Decoder (GSDecoder). The multi-view RF model operates in latent space, generating multi-view images, depths, and camera poses simultaneously, conditioned on text prompts, thus addressing challenges like diverse scene scales and complex camera trajectories in real-world settings. Then, the GSDecoder efficiently translates these latent outputs into 3DGS representations through a feed-forward 3DGS method. Leveraging training-free inversion and inpainting techniques, SplatFlow enables seamless 3DGS editing and supports a broad range of 3D tasks-including object editing, novel view synthesis, and camera pose estimation-within a unified framework without requiring additional complex pipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K datasets, demonstrating its versatility and effectiveness in various 3D generation, editing, and inpainting-based tasks.",
            "score": 5,
            "issue_id": 779,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "8447f5d110a89105",
            "authors": [
                "Hyojun Go",
                "Byeongjun Park",
                "Jiho Jang",
                "Jin-Young Kim",
                "Soonwoo Kwon",
                "Changick Kim"
            ],
            "affiliations": [
                "KAIST",
                "Twelve Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16443.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½",
                    "desc": "SplatFlow - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ² 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. SplatFlow Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "SplatFlow: Unifying 3D Scene Generation and Editing",
                    "desc": "This paper presents SplatFlow, a unified framework for generating and editing 3D scenes using 3D Gaussian Splatting (3DGS). It features a multi-view rectified flow model that generates images, depths, and camera poses from text prompts, addressing challenges in scene diversity and camera movement. The framework also includes a Gaussian Splatting Decoder that converts latent outputs into 3DGS representations efficiently. SplatFlow supports various 3D tasks like object editing and novel view synthesis, demonstrating its effectiveness on multiple datasets without the need for complex pipelines."
                },
                "zh": {
                    "title": "SplatFlowï¼šç»Ÿä¸€çš„3Dç”Ÿæˆä¸ç¼–è¾‘æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSplatFlowçš„ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–3Dåœºæ™¯çš„ç”Ÿæˆå’Œç¼–è¾‘ã€‚SplatFlowç»“åˆäº†å¤šè§†è§’æ•´æµæµæ¨¡å‹å’Œé«˜æ–¯ç‚¹äº‘è§£ç å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºåŒæ—¶ç”Ÿæˆå¤šè§†è§’å›¾åƒã€æ·±åº¦å’Œç›¸æœºå§¿æ€ã€‚è¯¥æ¡†æ¶é€šè¿‡æ— è®­ç»ƒåæ¼”å’Œä¿®è¡¥æŠ€æœ¯ï¼Œå®ç°äº†æ— ç¼çš„3Dé«˜æ–¯ç‚¹äº‘ç¼–è¾‘ï¼Œæ”¯æŒå¤šç§3Dä»»åŠ¡ï¼Œå¦‚ç‰©ä½“ç¼–è¾‘å’Œæ–°è§†è§’åˆæˆã€‚æˆ‘ä»¬åœ¨MVImgNetå’ŒDL3DV-7Kæ•°æ®é›†ä¸ŠéªŒè¯äº†SplatFlowçš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§3Dç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16035",
            "title": "Predicting Emergent Capabilities by Finetuning",
            "url": "https://huggingface.co/papers/2411.16035",
            "abstract": "A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models. In this work, we first pose the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? We then discover a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, we can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., \"emergence laws\"). We validate this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, we find that, in some cases, we can accurately predict whether models trained with up to 4x more compute have emerged. Finally, we present a case study of two realistic uses for emergence prediction.",
            "score": 5,
            "issue_id": 779,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "a3e818d78a8bea58",
            "authors": [
                "Charlie Snell",
                "Eric Wallace",
                "Dan Klein",
                "Sergey Levine"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16035.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#agi",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ˜Ğ˜: Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñƒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ´Ğ²Ğ¸Ğ½ÑƒÑ‚ÑŒ Ñ‚Ğ¾Ñ‡ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¼ĞµĞ½ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¼Ğ¾Ğ³Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Predicting Emergence: Unlocking Future LLM Capabilities",
                    "desc": "This paper addresses the challenge of predicting emergent capabilities in large language models (LLMs) as they scale. While the pretraining loss of LLMs is predictable based on compute resources, their downstream performance can show unexpected jumps in capability. The authors introduce the concept of emergence prediction, which involves fine-tuning LLMs on specific tasks to determine when these emergent capabilities will appear. They validate their approach using standard NLP benchmarks and demonstrate that it is possible to predict the emergence of capabilities in future models based on the performance of smaller models."
                },
                "zh": {
                    "title": "é¢„æµ‹è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„çªç ´",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ‰©å±•æ—¶å‡ºç°çš„èƒ½åŠ›é¢„æµ‹é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡å¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æå‰é¢„æµ‹æœªæ¥æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¾®è°ƒå¯ä»¥å°†èƒ½åŠ›å‡ºç°çš„ä¸´ç•Œç‚¹å‘èƒ½åŠ›è¾ƒä½çš„æ¨¡å‹ç§»åŠ¨ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†ä¸ŠéªŒè¯äº†è¿™ä¸€æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™ä¸€é¢„æµ‹èƒ½åŠ›è¿›è¡Œå®é™…åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16085",
            "title": "Cautious Optimizers: Improving Training with One Line of Code",
            "url": "https://huggingface.co/papers/2411.16085",
            "abstract": "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to 1.47times. Code is available at https://github.com/kyleliang919/C-Optim",
            "score": 5,
            "issue_id": 777,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "48a2e1454fdde298",
            "authors": [
                "Kaizhao Liang",
                "Lizhang Chen",
                "Bo Liu",
                "Qiang Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.16085.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Cautious Optimizer. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ´Ğµ PyTorch, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ“Ğ°Ğ¼Ğ¸Ğ»ÑŒÑ‚Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ Adam Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ›ÑĞ¿ÑƒĞ½Ğ¾Ğ²Ğ°. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Llama Ğ¸ MAE Ğ´Ğ¾ 1.47 Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "Cautious Optimizer: Speeding Up Transformer Pretraining!",
                    "desc": "This paper introduces a new optimizer called the Cautious Optimizer, which is a simple modification of existing momentum-based optimizers like AdamW and Lion. The modification preserves the Hamiltonian function of Adam, ensuring that the convergence properties remain intact according to Lyapunov stability analysis. The authors demonstrate that this new optimizer can significantly speed up the pretraining of models like Llama and MAE, achieving improvements of up to 1.47 times. Additionally, the research opens the door to a new family of optimizers, expanding the options available for machine learning practitioners."
                },
                "zh": {
                    "title": "æå‡å˜æ¢å™¨é¢„è®­ç»ƒé€Ÿåº¦çš„æ–°ä¼˜åŒ–å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–å™¨ï¼Œç§°ä¸ºCautious Optimizerï¼Œæ—¨åœ¨æé«˜å˜æ¢å™¨é¢„è®­ç»ƒçš„é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚é€šè¿‡å¯¹ç°æœ‰çš„åŠ¨é‡ä¼˜åŒ–å™¨è¿›è¡Œç®€å•çš„ä¿®æ”¹ï¼Œæˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿™ç§ä¿®æ”¹ä¿æŒäº†Adamçš„å“ˆå¯†é¡¿å‡½æ•°ï¼Œå¹¶ä¸”åœ¨Lyapunovåˆ†æä¸‹ä¸ç ´åæ”¶æ•›æ€§ä¿è¯ã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†ä¸€ç³»åˆ—æ–°çš„ä¼˜åŒ–å™¨ï¼Œå¹¶é€‰æ‹©äº†å…¶ä¸­æœ€ç®€å•çš„è¿›è¡Œå®è¯å®éªŒï¼Œç»“æœæ˜¾ç¤ºåœ¨Llamaå’ŒMAEé¢„è®­ç»ƒä¸­é€Ÿåº¦æå‡å¯è¾¾1.47å€ã€‚ç›¸å…³ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14486",
            "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
            "url": "https://huggingface.co/papers/2411.14486",
            "abstract": "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to acknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand challenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including both open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect responses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in fields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem difficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more challenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more prone to generate speculative answers when problems appear more tractable. The study also revealed significant variations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and NP-hard problems while performing relatively better on philosophical and psychological challenges. These results contribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the importance of uncertainty recognition as a critical component of future machine intelligence evaluation. This impossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing empirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting new directions for improving model training architectures and evaluation approaches.",
            "score": 5,
            "issue_id": 777,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "c30a94b30cace49a",
            "authors": [
                "David Noever",
                "Forrest McKee"
            ],
            "affiliations": [
                "PeopleTec, Inc., Huntsville, AL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14486.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#interpretability",
                    "#agi",
                    "#dataset"
                ],
                "emoji": "ğŸ¤”",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ·Ğ½Ğ°Ğ½Ğ¸Ñ: ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ°ÑĞ¿ĞµĞºÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 675 Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµÑ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ñ…. Ğ”Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¸Ñ… ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ğ½Ğ¸Ğµ, Ğ° Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 62-68% Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼."
                },
                "en": {
                    "title": "Evaluating Uncertainty: A New Benchmark for Language Models",
                    "desc": "This research presents a new framework to evaluate how well large language models (LLMs) recognize their own uncertainty when faced with unsolvable problems. By testing twelve advanced LLMs on a set of graduate-level questions that have no answers, the study found that the best models could admit ignorance 62-68% of the time. Interestingly, the models were more likely to acknowledge uncertainty on harder problems, with GPT-4 showing a 35.8% acknowledgment rate on difficult questions. The findings emphasize the need for better training and evaluation methods to enhance LLMs' ability to recognize their knowledge limits, which is crucial for advancing artificial general intelligence (AGI)."
                },
                "zh": {
                    "title": "æ‰¿è®¤ä¸ç¡®å®šæ€§ï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°è§†è§’",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨675ä¸ªæ ¹æœ¬æ— æ³•è§£å†³çš„é—®é¢˜ä¸Šæ‰¿è®¤ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç»„ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„ç ”ç©¶ç”Ÿçº§åˆ«çš„é‡å¤§æŒ‘æˆ˜é—®é¢˜æ•°æ®é›†ï¼Œè¯„ä¼°äº†åŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹åœ¨å†…çš„åäºŒä¸ªæœ€å…ˆè¿›çš„LLMsï¼Œè§‚å¯Ÿå®ƒä»¬æ‰¿è®¤æ— çŸ¥çš„å€¾å‘ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹åœ¨æ‰¿è®¤é—®é¢˜è§£å†³æ–¹æ¡ˆæœªçŸ¥çš„å‡†ç¡®ç‡èŒƒå›´ä¸º62%åˆ°68%ï¼Œå¹¶ä¸”åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ä¸Šï¼ˆå¦‚ç”Ÿç‰©å­¦ã€å“²å­¦å’Œæ•°å­¦ï¼‰è¡¨ç°å‡ºæ›´é«˜çš„ä¸ç¡®å®šæ€§æ‰¿è®¤ç‡ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä¸åŒé—®é¢˜ç±»åˆ«ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæ¨¡å‹åœ¨æ‰¿è®¤å‘æ˜å’ŒNPéš¾é¢˜çš„ä¸ç¡®å®šæ€§æ—¶è¡¨ç°è¾ƒå·®ï¼Œè€Œåœ¨å“²å­¦å’Œå¿ƒç†å­¦æŒ‘æˆ˜ä¸­è¡¨ç°ç›¸å¯¹è¾ƒå¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15862",
            "title": "LLMs Do Not Think Step-by-step In Implicit Reasoning",
            "url": "https://huggingface.co/papers/2411.15862",
            "abstract": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs' performance on complex tasks. However, because it also introduces slower inference speeds and higher computational costs, many researches have attempted to use implicit CoT, which does not need LLMs to explicitly generate the intermediate steps. But there is still gap between their efficacy and typical explicit CoT methods. This leaves us a doubt that, does implicit CoT really equal to explicit CoT? Therefore, in this study, we address this question through experiments. We probe the information of intermediate steps from the model's hidden states when it is performing implicit CoT. The results surprisingly indicate that LLMs hardly think about intermediate steps, suggesting they may just rely on experience rather than strict step-by-step reasoning. Moreover, we find LLMs' implicit reasoning capabilities are susceptible and unstable, reaffirming the necessity of explicit CoT to effectively support complex tasks.",
            "score": 3,
            "issue_id": 788,
            "pub_date": "2024-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "8b819b67a91e3ad7",
            "authors": [
                "Yijiong Yu"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15862.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#interpretability",
                    "#inference",
                    "#rl"
                ],
                "emoji": "ğŸ¤”",
                "ru": {
                    "title": "ĞĞµÑĞ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ?",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° 'Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹' (implicit Chain-of-Thought) Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½ĞµÑĞ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Implicit vs. Explicit CoT: Unveiling the True Thinking of LLMs",
                    "desc": "This paper investigates the effectiveness of implicit Chain-of-Thought (CoT) reasoning in large language models (LLMs) compared to explicit CoT methods. The authors conduct experiments to analyze the hidden states of LLMs during implicit CoT tasks, revealing that these models often do not engage in detailed intermediate reasoning steps. Instead, they tend to rely on prior experiences, which leads to inconsistent and unstable performance. The findings highlight the importance of explicit CoT for enhancing LLMs' capabilities in handling complex tasks effectively."
                },
                "zh": {
                    "title": "æ˜¾å¼æ€ç»´é“¾æ˜¯å¤æ‚ä»»åŠ¡çš„å…³é”®",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†éšå¼æ€ç»´é“¾ï¼ˆimplicit CoTï¼‰ä¸æ˜¾å¼æ€ç»´é“¾ï¼ˆexplicit CoTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è¡¨ç°å·®å¼‚ã€‚å°½ç®¡éšå¼æ€ç»´é“¾å¯ä»¥å‡å°‘è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ï¼Œä½†å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨éšå¼æ¨ç†æ—¶å‡ ä¹ä¸è€ƒè™‘ä¸­é—´æ­¥éª¤ã€‚ç›¸åï¼Œå®ƒä»¬æ›´ä¾èµ–äºç»éªŒï¼Œè€Œä¸æ˜¯ä¸¥æ ¼çš„é€æ­¥æ¨ç†ã€‚ç ”ç©¶è¿˜å‘ç°ï¼ŒLLMsçš„éšå¼æ¨ç†èƒ½åŠ›ä¸ç¨³å®šï¼Œè¿›ä¸€æ­¥å¼ºè°ƒäº†æ˜¾å¼æ€ç»´é“¾åœ¨æ”¯æŒå¤æ‚ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15671",
            "title": "Best of Both Worlds: Advantages of Hybrid Graph Sequence Models",
            "url": "https://huggingface.co/papers/2411.15671",
            "abstract": "Modern sequence models (e.g., Transformers, linear RNNs, etc.) emerged as dominant backbones of recent deep learning frameworks, mainly due to their efficiency, representational power, and/or ability to capture long-range dependencies. Adopting these sequence models for graph-structured data has recently gained popularity as the alternative to Message Passing Neural Networks (MPNNs). There is, however, a lack of a common foundation about what constitutes a good graph sequence model, and a mathematical description of the benefits and deficiencies in adopting different sequence models for learning on graphs. To this end, we first present Graph Sequence Model (GSM), a unifying framework for adopting sequence models for graphs, consisting of three main steps: (1) Tokenization, which translates the graph into a set of sequences; (2) Local Encoding, which encodes local neighborhoods around each node; and (3) Global Encoding, which employs a scalable sequence model to capture long-range dependencies within the sequences. This framework allows us to understand, evaluate, and compare the power of different sequence model backbones in graph tasks. Our theoretical evaluations of the representation power of Transformers and modern recurrent models through the lens of global and local graph tasks show that there are both negative and positive sides for both types of models. Building on this observation, we present GSM++, a fast hybrid model that uses the Hierarchical Affinity Clustering (HAC) algorithm to tokenize the graph into hierarchical sequences, and then employs a hybrid architecture of Transformer to encode these sequences. Our theoretical and experimental results support the design of GSM++, showing that GSM++ outperforms baselines in most benchmark evaluations.",
            "score": 3,
            "issue_id": 779,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 23",
                "zh": "11æœˆ23æ—¥"
            },
            "hash": "540358181ae0274e",
            "authors": [
                "Ali Behrouz",
                "Ali Parviz",
                "Mahdi Karami",
                "Clayton Sanford",
                "Bryan Perozzi",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Google Research",
                "New Jersey Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15671.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#architecture",
                    "#benchmark",
                    "#graphs"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Graph Sequence Model (GSM) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. GSM ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾ĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GSM++, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unifying Sequence Models for Enhanced Graph Learning",
                    "desc": "This paper introduces the Graph Sequence Model (GSM), a framework that integrates sequence models with graph-structured data. It consists of three steps: tokenization of graphs into sequences, local encoding of node neighborhoods, and global encoding to capture long-range dependencies. The authors evaluate the strengths and weaknesses of different sequence models, particularly Transformers and recurrent models, in graph tasks. They also propose GSM++, a hybrid model that enhances performance by using hierarchical tokenization and a Transformer architecture, demonstrating superior results in benchmark tests."
                },
                "zh": {
                    "title": "å›¾åºåˆ—æ¨¡å‹ï¼šç»“åˆåºåˆ—ä¸å›¾çš„åŠ›é‡",
                    "desc": "ç°ä»£åºåˆ—æ¨¡å‹ï¼ˆå¦‚å˜æ¢å™¨å’Œçº¿æ€§é€’å½’ç¥ç»ç½‘ç»œï¼‰åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œå› å…¶é«˜æ•ˆæ€§ã€è¡¨ç¤ºèƒ½åŠ›å’Œæ•æ‰é•¿è·ç¦»ä¾èµ–çš„èƒ½åŠ›ã€‚å°†è¿™äº›åºåˆ—æ¨¡å‹åº”ç”¨äºå›¾ç»“æ„æ•°æ®çš„ç ”ç©¶é€æ¸å—åˆ°å…³æ³¨ï¼Œä½œä¸ºæ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†å›¾åºåˆ—æ¨¡å‹ï¼ˆGSMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼ŒåŒ…å«å›¾çš„æ ‡è®°åŒ–ã€å±€éƒ¨ç¼–ç å’Œå…¨å±€ç¼–ç ä¸‰ä¸ªä¸»è¦æ­¥éª¤ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†GSM++ï¼Œä¸€ç§å¿«é€Ÿæ··åˆæ¨¡å‹ï¼Œåˆ©ç”¨å±‚æ¬¡äº²å’Œèšç±»ç®—æ³•å¯¹å›¾è¿›è¡Œåˆ†å±‚åºåˆ—æ ‡è®°ï¼Œå¹¶é‡‡ç”¨å˜æ¢å™¨çš„æ··åˆæ¶æ„è¿›è¡Œç¼–ç ï¼Œå®éªŒç»“æœè¡¨æ˜GSM++åœ¨å¤§å¤šæ•°åŸºå‡†è¯„ä¼°ä¸­ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16665",
            "title": "Edge Weight Prediction For Category-Agnostic Pose Estimation",
            "url": "https://huggingface.co/papers/2411.16665",
            "abstract": "Category-Agnostic Pose Estimation (CAPE) localizes keypoints across diverse object categories with a single model, using one or a few annotated support images. Recent works have shown that using a pose graph (i.e., treating keypoints as nodes in a graph rather than isolated points) helps handle occlusions and break symmetry. However, these methods assume a static pose graph with equal-weight edges, leading to suboptimal results. We introduce EdgeCape, a novel framework that overcomes these limitations by predicting the graph's edge weights which optimizes localization. To further leverage structural priors, we propose integrating Markovian Structural Bias, which modulates the self-attention interaction between nodes based on the number of hops between them. We show that this improves the model's ability to capture global spatial dependencies. Evaluated on the MP-100 benchmark, which includes 100 categories and over 20K images, EdgeCape achieves state-of-the-art results in the 1-shot setting and leads among similar-sized methods in the 5-shot setting, significantly improving keypoint localization accuracy. Our code is publicly available.",
            "score": 2,
            "issue_id": 791,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "f14fbd2f53f85526",
            "authors": [
                "Or Hirschorn",
                "Shai Avidan"
            ],
            "affiliations": [
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16665.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#benchmark",
                    "#cv",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ”‘",
                "ru": {
                    "title": "EdgeCape: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "EdgeCape - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ²ĞµÑĞ° Ñ€Ñ‘Ğ±ĞµÑ€ Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸. EdgeCape Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MP-100 Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Dynamic Edge Weights for Enhanced Pose Estimation",
                    "desc": "The paper presents EdgeCape, a new framework for Category-Agnostic Pose Estimation (CAPE) that enhances keypoint localization across various object categories using minimal annotated images. It addresses the limitations of previous methods that used static pose graphs with equal-weight edges by introducing dynamic edge weight predictions. Additionally, EdgeCape incorporates Markovian Structural Bias to improve the interaction between keypoints based on their spatial relationships. The results demonstrate that EdgeCape achieves state-of-the-art performance on the MP-100 benchmark, particularly excelling in one-shot and five-shot scenarios."
                },
                "zh": {
                    "title": "EdgeCapeï¼šä¼˜åŒ–å…³é”®ç‚¹å®šä½çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåä¸ºEdgeCapeï¼Œç”¨äºåœ¨ä¸åŒç‰©ä½“ç±»åˆ«ä¸­è¿›è¡Œæ— ç±»åˆ«å§¿æ€ä¼°è®¡ã€‚å®ƒé€šè¿‡é¢„æµ‹å›¾çš„è¾¹æƒé‡æ¥ä¼˜åŒ–å…³é”®ç‚¹çš„å®šä½ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­é™æ€å§¿æ€å›¾çš„å±€é™æ€§ã€‚è®ºæ–‡è¿˜æå‡ºäº†é©¬å°”å¯å¤«ç»“æ„åç½®ï¼Œå¢å¼ºäº†èŠ‚ç‚¹ä¹‹é—´çš„è‡ªæ³¨æ„åŠ›äº¤äº’ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰å…¨å±€ç©ºé—´ä¾èµ–å…³ç³»ã€‚ç»è¿‡åœ¨MP-100åŸºå‡†ä¸Šçš„è¯„ä¼°ï¼ŒEdgeCapeåœ¨1-shotå’Œ5-shotè®¾ç½®ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ˜¾è‘—æé«˜äº†å…³é”®ç‚¹å®šä½çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15221",
            "title": "Reflections from the 2024 Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry",
            "url": "https://huggingface.co/papers/2411.15221",
            "abstract": "Here, we present the outcomes from the second Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry, which engaged participants across global hybrid locations, resulting in 34 team submissions. The submissions spanned seven key application areas and demonstrated the diverse utility of LLMs for applications in (1) molecular and material property prediction; (2) molecular and material design; (3) automation and novel interfaces; (4) scientific communication and education; (5) research data management and automation; (6) hypothesis generation and evaluation; and (7) knowledge extraction and reasoning from scientific literature. Each team submission is presented in a summary table with links to the code and as brief papers in the appendix. Beyond team results, we discuss the hackathon event and its hybrid format, which included physical hubs in Toronto, Montreal, San Francisco, Berlin, Lausanne, and Tokyo, alongside a global online hub to enable local and virtual collaboration. Overall, the event highlighted significant improvements in LLM capabilities since the previous year's hackathon, suggesting continued expansion of LLMs for applications in materials science and chemistry research. These outcomes demonstrate the dual utility of LLMs as both multipurpose models for diverse machine learning tasks and platforms for rapid prototyping custom applications in scientific research.",
            "score": 1,
            "issue_id": 797,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "3984b5ab3fb11866",
            "authors": [
                "Yoel Zimmermann",
                "Adib Bazgir",
                "Zartashia Afzal",
                "Fariha Agbere",
                "Qianxiang Ai",
                "Nawaf Alampara",
                "Alexander Al-Feghali",
                "Mehrad Ansari",
                "Dmytro Antypov",
                "Amro Aswad",
                "Jiaru Bai",
                "Viktoriia Baibakova",
                "Devi Dutta Biswajeet",
                "Erik Bitzek",
                "Joshua D. Bocarsly",
                "Anna Borisova",
                "Andres M Bran",
                "L. Catherine Brinson",
                "Marcel Moran Calderon",
                "Alessandro Canalicchio",
                "Victor Chen",
                "Yuan Chiang",
                "Defne Circi",
                "Benjamin Charmes",
                "Vikrant Chaudhary",
                "Zizhang Chen",
                "Min-Hsueh Chiu",
                "Judith Clymo",
                "Kedar Dabhadkar",
                "Nathan Daelman",
                "Archit Datar",
                "Matthew L. Evans",
                "Maryam Ghazizade Fard",
                "Giuseppe Fisicaro",
                "Abhijeet Sadashiv Gangan",
                "Janine George",
                "Jose D. Cojal Gonzalez",
                "Michael GÃ¶tte",
                "Ankur K. Gupta",
                "Hassan Harb",
                "Pengyu Hong",
                "Abdelrahman Ibrahim",
                "Ahmed Ilyas",
                "Alishba Imran",
                "Kevin Ishimwe",
                "Ramsey Issa",
                "Kevin Maik Jablonka",
                "Colin Jones",
                "Tyler R. Josephson",
                "Greg Juhasz",
                "Sarthak Kapoor",
                "Rongda Kang",
                "Ghazal Khalighinejad",
                "Sartaaj Khan",
                "Sascha Klawohn",
                "Suneel Kuman",
                "Alvin Noe Ladines",
                "Sarom Leang",
                "Magdalena Lederbauer",
                "Sheng-Lun Mark Liao",
                "Hao Liu",
                "Xuefeng Liu",
                "Stanley Lo",
                "Sandeep Madireddy",
                "Piyush Ranjan Maharana",
                "Shagun Maheshwari",
                "Soroush Mahjoubi",
                "JosÃ© A. MÃ¡rquez",
                "Rob Mills",
                "Trupti Mohanty",
                "Bernadette Mohr",
                "Seyed Mohamad Moosavi",
                "Alexander MoÃŸhammer",
                "Amirhossein D. Naghdi",
                "Aakash Naik",
                "Oleksandr Narykov",
                "Hampus NÃ¤sstrÃ¶m",
                "Xuan Vu Nguyen",
                "Xinyi Ni",
                "Dana O'Connor",
                "Teslim Olayiwola",
                "Federico Ottomano",
                "Aleyna Beste Ozhan",
                "Sebastian Pagel",
                "Chiku Parida",
                "Jaehee Park",
                "Vraj Patel",
                "Elena Patyukova",
                "Martin Hoffmann Petersen",
                "Luis Pinto",
                "JosÃ© M. Pizarro",
                "Dieter Plessers",
                "Tapashree Pradhan",
                "Utkarsh Pratiush",
                "Charishma Puli",
                "Andrew Qin",
                "Mahyar Rajabi",
                "Francesco Ricci",
                "Elliot Risch",
                "MartiÃ±o RÃ­os-GarcÃ­a",
                "Aritra Roy",
                "Tehseen Rug",
                "Hasan M Sayeed",
                "Markus Scheidgen",
                "Mara Schilling-Wilhelmi",
                "Marcel Schloz",
                "Fabian SchÃ¶ppach",
                "Julia Schumann",
                "Philippe Schwaller",
                "Marcus Schwarting",
                "Samiha Sharlin",
                "Kevin Shen",
                "Jiale Shi",
                "Pradip Si",
                "Jennifer D'Souza",
                "Taylor Sparks",
                "Suraj Sudhakar",
                "Leopold Talirz",
                "Dandan Tang",
                "Olga Taran",
                "Carla Terboven",
                "Mark Tropin",
                "Anastasiia Tsymbal",
                "Katharina Ueltzen",
                "Pablo Andres Unzueta",
                "Archit Vasan",
                "Tirtha Vinchurkar",
                "Trung Vo",
                "Gabriel Vogel",
                "Christoph VÃ¶lker",
                "Jan Weinreich",
                "Faradawn Yang",
                "Mohd Zaki",
                "Chi Zhang",
                "Sylvester Zhang",
                "Weijie Zhang",
                "Ruijie Zhu",
                "Shang Zhu",
                "Jan Janssen",
                "Ian Foster",
                "Ben Blaiszik"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.15221.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#data",
                    "#multimodal",
                    "#dataset",
                    "#science"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "LLM Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ…Ğ¸Ğ¼Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ñ…Ğ°ĞºĞ°Ñ‚Ğ¾Ğ½Ğ° Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ…Ğ¸Ğ¼Ğ¸Ğ¸. Ğ£Ñ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ 34 Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ² ÑĞµĞ¼Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. Ğ¥Ğ°ĞºĞ°Ñ‚Ğ¾Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ğ» Ğ² Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ…Ğ°Ğ±Ğ°Ğ¼Ğ¸ Ğ² ÑˆĞµÑÑ‚Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°Ñ… Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ…Ğ°Ğ±Ğ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… LLM Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ğ¼ Ğ³Ğ¾Ğ´Ğ¾Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Unlocking the Power of LLMs in Materials Science and Chemistry",
                    "desc": "This paper discusses the results of the second Large Language Model (LLM) Hackathon focused on materials science and chemistry, which attracted 34 teams from around the world. The submissions showcased the versatility of LLMs in various applications, including predicting molecular properties, designing materials, and automating research processes. The event also emphasized the advancements in LLM capabilities compared to the previous year, indicating their growing importance in scientific research. Overall, the hackathon served as a platform for collaboration and innovation, highlighting LLMs as powerful tools for both general machine learning tasks and specialized scientific applications."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ææ–™ç§‘å­¦ä¸åŒ–å­¦ä¸­çš„åº”ç”¨æ½œåŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ç¬¬äºŒå±Šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é»‘å®¢é©¬æ‹‰æ¾åœ¨ææ–™ç§‘å­¦å’ŒåŒ–å­¦é¢†åŸŸçš„æˆæœï¼Œå¸å¼•äº†å…¨çƒå‚ä¸è€…ï¼Œå…±æäº¤äº†34ä¸ªå›¢é˜Ÿé¡¹ç›®ã€‚è¿™äº›é¡¹ç›®æ¶µç›–äº†ä¸ƒä¸ªå…³é”®åº”ç”¨é¢†åŸŸï¼Œå±•ç¤ºäº†LLMåœ¨åˆ†å­å’Œææ–™å±æ€§é¢„æµ‹ã€è®¾è®¡ã€è‡ªåŠ¨åŒ–ã€ç§‘å­¦ä¼ æ’­ã€æ•°æ®ç®¡ç†ã€å‡è®¾ç”Ÿæˆå’ŒçŸ¥è¯†æå–ç­‰æ–¹é¢çš„å¤šæ ·åŒ–åº”ç”¨ã€‚æ¯ä¸ªå›¢é˜Ÿçš„æäº¤ç»“æœéƒ½ä»¥æ‘˜è¦è¡¨çš„å½¢å¼å‘ˆç°ï¼Œå¹¶é™„æœ‰ä»£ç é“¾æ¥å’Œç®€è¦è®ºæ–‡ã€‚æ­¤æ¬¡æ´»åŠ¨çªæ˜¾äº†LLMèƒ½åŠ›çš„æ˜¾è‘—æå‡ï¼Œè¡¨æ˜å…¶åœ¨ææ–™ç§‘å­¦å’ŒåŒ–å­¦ç ”ç©¶ä¸­çš„åº”ç”¨å‰æ™¯å¹¿é˜”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.13550",
            "title": "Find Any Part in 3D",
            "url": "https://huggingface.co/papers/2411.13550",
            "abstract": "We study open-world part segmentation in 3D: segmenting any part in any object based on any text query. Prior methods are limited in object categories and part vocabularies. Recent advances in AI have demonstrated effective open-world recognition capabilities in 2D. Inspired by this progress, we propose an open-world, direct-prediction model for 3D part segmentation that can be applied zero-shot to any object. Our approach, called Find3D, trains a general-category point embedding model on large-scale 3D assets from the internet without any human annotation. It combines a data engine, powered by foundation models for annotating data, with a contrastive training method. We achieve strong performance and generalization across multiple datasets, with up to a 3x improvement in mIoU over the next best method. Our model is 6x to over 300x faster than existing baselines. To encourage research in general-category open-world 3D part segmentation, we also release a benchmark for general objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/",
            "score": 1,
            "issue_id": 794,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "d8b296940d5af71d",
            "authors": [
                "Ziqi Ma",
                "Yisong Yue",
                "Georgia Gkioxari"
            ],
            "affiliations": [
                "California Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.13550.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#3d"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Find3D: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Find3D Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Find3D Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with Find3D",
                    "desc": "This paper presents Find3D, a novel model for open-world part segmentation in 3D, allowing segmentation of any object part based on text queries. Unlike previous methods that are restricted to specific object categories and part vocabularies, Find3D utilizes a general-category point embedding model trained on extensive 3D data from the internet without human annotations. The model employs a combination of a data engine and contrastive training, resulting in significant performance improvements, achieving up to 3x higher mean Intersection over Union (mIoU) compared to existing methods. Additionally, Find3D operates at a much faster speed, being 6x to over 300x quicker than current baselines, and introduces a benchmark to promote further research in this area."
                },
                "zh": {
                    "title": "å¼€æ”¾ä¸–ç•Œ3Déƒ¨ä»¶åˆ†å‰²çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬ç ”ç©¶äº†å¼€æ”¾ä¸–ç•Œçš„3Déƒ¨ä»¶åˆ†å‰²ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŸ¥è¯¢å¯¹ä»»ä½•ç‰©ä½“çš„ä»»ä½•éƒ¨ä»¶è¿›è¡Œåˆ†å‰²ã€‚ä»¥å¾€çš„æ–¹æ³•åœ¨ç‰©ä½“ç±»åˆ«å’Œéƒ¨ä»¶è¯æ±‡ä¸Šå­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFind3Dçš„å¼€æ”¾ä¸–ç•Œç›´æ¥é¢„æµ‹æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ä»»ä½•äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå¯¹æ¥è‡ªäº’è”ç½‘çš„å¤§è§„æ¨¡3Dèµ„äº§è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹ç»“åˆäº†æ•°æ®å¼•æ“å’Œå¯¹æ¯”è®­ç»ƒæ–¹æ³•ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒmIoUæå‡è¾¾åˆ°3å€ä»¥ä¸Šï¼ŒåŒæ—¶é€Ÿåº¦æ¯”ç°æœ‰åŸºçº¿å¿«6å€åˆ°300å€ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-25.html",
    "link_next": "2024-11-27.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "25.11",
        "en": "11/25",
        "zh": "11æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.11",
        "en": "11/27",
        "zh": "11æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 12,
        "#agents": 0,
        "#cv": 7,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 4,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 7,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 8,
        "#healthcare": 4,
        "#training": 8,
        "#robotics": 0,
        "#agi": 3,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 3,
        "#graphs": 2,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬å±•ç¤ºäº†Material Anythingï¼Œä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–ã€ç»Ÿä¸€çš„æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ä¸º3Dç‰©ä½“ç”ŸæˆåŸºäºç‰©ç†çš„æè´¨ã€‚ä¸ä¾èµ–å¤æ‚æµæ°´çº¿æˆ–ç‰¹å®šæ¡ˆä¾‹ä¼˜åŒ–çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒMaterial Anythingæä¾›äº†ä¸€ä¸ªé€‚åº”å¤šç§å…‰ç…§æ¡ä»¶ä¸‹ç‰©ä½“çš„å¥å£®ã€ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆä¸‰å¤´æ¶æ„å’Œæ¸²æŸ“æŸå¤±ï¼Œä»¥æé«˜ç¨³å®šæ€§å’Œæè´¨è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç½®ä¿¡æ©ç ä½œä¸ºæ‰©æ•£æ¨¡å‹å†…çš„åŠ¨æ€å¼€å…³ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æœ‰çº¹ç†å’Œæ— çº¹ç†çš„ç‰©ä½“ã€‚é€šè¿‡ä½¿ç”¨è¿™äº›ç½®ä¿¡æ©ç æŒ‡å¯¼çš„æ¸è¿›æè´¨ç”Ÿæˆç­–ç•¥ï¼Œä»¥åŠUVç©ºé—´æè´¨ç²¾ç‚¼å™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†ä¸€è‡´çš„ã€UVå‡†å¤‡å¥½çš„æè´¨è¾“å‡ºã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§ç‰©ä½“ç±»åˆ«å’Œå…‰ç…§æ¡ä»¶ä¸‹ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
        "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
        "pinyin": "WÇ’men zhÇnshÃ¬le Material Anything, yÄ«gÃ¨ wÃ¡nquÃ¡n zÃ¬dÃ²nghuÃ , tÇ’ngyÄ« de kuÃ²sÃ n kuÃ ngjiÃ , zhÇzÃ i wÃ¨i 3D wÃ¹tÇ shÄ“ngchÃ©ng jÄ«yÇ wÃ¹lÇ de cÃ¡izhÃ¬. YÇ” yÄ«lÃ i fÃ¹zÃ¡ xiÃ¹shuÇxiÃ n huÃ² tÃ¨dÃ¬ng Ã nliÃ o yÅuhuÃ  de xiÃ n yÇ’u fÄngfÇ bÃ¹tÃ³ng, Material Anything tÃ­gÅngle yÄ«gÃ¨ shÃ¬yÃ¬ng duÅzhÇ’ng guÄngzhÃ o tiÃ¡ojiÃ n xiÃ  wÃ¹tÇ de jiÃ nkÄng, duÄn dÃ o duÄn de jiÄ›juÃ© fÄng'Ã n. WÇ’men de fÄngfÇ lÃ¬yÃ²ngle yÃ¹ xÃ¹nliÃ n de tÃºxiÃ ng kuÃ²sÃ n mÃ³xÃ­ng, jiÃ©hÃ© sÄn tÃ³u jiÃ gÃ²u hÃ© xuÃ nshÄ“i sÇ”nshÄ«, yÇ tÃ­gÄo wÄ›ndÃ¬ngxÃ¬ng hÃ© cÃ¡izhÃ¬ zhÃ¬liÃ ng. CÇwÃ i, wÇ’men yÇn rÃ¹le zhÃ¬xÃ¬n mÃ³zhÃ o zuÃ²wÃ©i kuÃ²sÃ n mÃ³xÃ­ng nÃ¨i de dÃ²ngtÃ i kÄiguÄn, shÇ qÃ­ nÃ©nggÃ²u yÇ’uxiÃ o chÇ”lÇ yÇ’u wÃ©nlÇ hÃ© wÃº wÃ©nlÇ de wÃ¹tÇ. TÅngguÃ² shÇyÃ²ng zhÃ¨xiÄ“ zhÃ¬xÃ¬n mÃ³zhÃ o zhÇdÇo de jiÃ njÃ¬n cÃ¡izhÃ¬ shÄ“ngchÃ©ng cÃ¨lÃ¼Ã¨, yÇjiÇ UV kÅngjiÄn cÃ¡izhÃ¬ jÄ«ngliÃ nqÃ¬, wÇ’men de fÄngfÇ quÃ¨bÇole yÄ«zhÃ¬ de, UV zhÇ”nbÃ¨i hÇo de cÃ¡izhÃ¬ shÅ«chÅ«. GuÇngfÃ n de shÃ¬yÃ n biÇomÃ­ng, wÇ’men de fÄngfÇ zÃ i gÃ¨zhÇ’ng wÃ¹tÇ lÃ¨ibiÃ© hÃ© guÄngzhÃ o tiÃ¡ojiÃ n xiÃ  yÅu yÃº xiÃ n yÇ’u fÄngfÇ.",
        "vocab": "[{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'display'},\n{'word': 'Material', 'pinyin': 'MÃ©itÃ¨riÇl', 'trans': 'Material'},\n{'word': 'Anything', 'pinyin': 'Ä’nÃ¬thÃ¬ng', 'trans': 'Anything'},\n{'word': 'è‡ªåŠ¨åŒ–', 'pinyin': 'zÃ¬dÃ²nghuÃ ', 'trans': 'automated'},\n{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ngyÄ«', 'trans': 'unified'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'ç‰©ç†', 'pinyin': 'wÃ¹lÇ', 'trans': 'physics'},\n{'word': 'æè´¨', 'pinyin': 'cÃ¡izhÃ¬', 'trans': 'material'},\n{'word': 'ä¾èµ–', 'pinyin': 'yÄ«lÃ i', 'trans': 'rely on'},\n{'word': 'å¤æ‚', 'pinyin': 'fÃ¹zÃ¡', 'trans': 'complex'},\n{'word': 'æµæ°´çº¿', 'pinyin': 'liÃºshuÇxiÃ n', 'trans': 'pipeline'},\n{'word': 'ç‰¹å®š', 'pinyin': 'tÃ¨dÃ¬ng', 'trans': 'specific'},\n{'word': 'æ¡ˆä¾‹', 'pinyin': 'Ã nlÃ¬', 'trans': 'case'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅuhuÃ ', 'trans': 'optimization'},\n{'word': 'å¥å£®', 'pinyin': 'jiÃ nzhuÃ ng', 'trans': 'robust'},\n{'word': 'ç«¯åˆ°ç«¯', 'pinyin': 'duÄndÃ oduÄn', 'trans': 'end-to-end'},\n{'word': 'è§£å†³æ–¹æ¡ˆ', 'pinyin': 'jiÄ›juÃ© fÄngÃ n', 'trans': 'solution'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬yÃ²ng', 'trans': 'utilize'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹xÃ¹nliÃ n', 'trans': 'pre-trained'},\n{'word': 'å›¾åƒ', 'pinyin': 'tÃºxiÃ ng', 'trans': 'image'},\n{'word': 'ç»“åˆ', 'pinyin': 'jiÃ©hÃ©', 'trans': 'combine'},\n{'word': 'ä¸‰å¤´æ¶æ„', 'pinyin': 'sÄntÃ³u jiÃ gÃ²u', 'trans': 'three-headed architecture'},\n{'word': 'æ¸²æŸ“', 'pinyin': 'xuÃ nrÃ¡n', 'trans': 'rendering'},\n{'word': 'æŸå¤±', 'pinyin': 'sÇ”nshÄ«', 'trans': 'loss'},\n{'word': 'ç¨³å®šæ€§', 'pinyin': 'wÄ›ndÃ¬ngxÃ¬ng', 'trans': 'stability'},\n{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬liÃ ng', 'trans': 'quality'},\n{'word': 'ç½®ä¿¡', 'pinyin': 'zhÃ¬xÃ¬n', 'trans': 'confidence'},\n{'word': 'æ©ç ', 'pinyin': 'yÇnmÇ', 'trans': 'mask'},\n{'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ngtÃ i', 'trans': 'dynamic'},\n{'word': 'å¼€å…³', 'pinyin': 'kÄiguÄn', 'trans': 'switch'},\n{'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’uxiÃ o', 'trans': 'effective'},\n{'word': 'çº¹ç†', 'pinyin': 'wÃ©nlÇ', 'trans': 'texture'},\n{'word': 'æ¸è¿›', 'pinyin': 'jiÃ njÃ¬n', 'trans': 'progressive'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'UVç©ºé—´', 'pinyin': 'UV kÅngjiÄn', 'trans': 'UV space'},\n{'word': 'ç²¾ç‚¼å™¨', 'pinyin': 'jÄ«ngliÃ nqÃ¬', 'trans': 'refiner'},\n{'word': 'ç¡®ä¿', 'pinyin': 'quÃ¨bÇo', 'trans': 'ensure'},\n{'word': 'ä¸€è‡´', 'pinyin': 'yÄ«zhÃ¬', 'trans': 'consistent'},\n{'word': 'å‡†å¤‡å¥½', 'pinyin': 'zhÇ”nbÃ¨i hÇo', 'trans': 'ready'},\n{'word': 'å¹¿æ³›', 'pinyin': 'guÇngfÃ n', 'trans': 'extensive'},\n{'word': 'ç±»åˆ«', 'pinyin': 'lÃ¨ibiÃ©', 'trans': 'category'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅuyÃº', 'trans': 'superior to'}]",
        "trans": "We presented Material Anything, a fully automated, unified diffusion framework aimed at generating physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to various lighting conditions for objects. Our approach leverages pre-trained image diffusion models, combined with a tri-head architecture and rendering loss, to enhance stability and material quality. Additionally, we introduced confidence masks as dynamic switches within the diffusion model, enabling it to effectively handle both textured and non-textured objects. By employing a progressive material generation strategy guided by these confidence masks, along with a UV space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate that our method outperforms existing approaches across various object categories and lighting conditions.",
        "update_ts": "2024-11-26 09:11"
    }
}