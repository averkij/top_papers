{
    "date": {
        "ru": "13 февраля",
        "en": "February 13",
        "zh": "2月13日"
    },
    "time_utc": "2025-02-13 04:12",
    "weekday": 3,
    "issue_id": 2187,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.08639",
            "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
            "url": "https://huggingface.co/papers/2502.08639",
            "abstract": "In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.",
            "score": 13,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "3d3890b6b6bf7904",
            "authors": [
                "Qinghe Wang",
                "Yawen Luo",
                "Xiaoyu Shi",
                "Xu Jia",
                "Huchuan Lu",
                "Tianfan Xue",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Kuaishou Technology",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08639.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#diffusion",
                    "#3d",
                    "#games"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "CineMaster: Режиссируйте свое видео в 3D",
                    "desc": "CineMaster - это новая система для создания 3D-ориентированного и контролируемого видео на основе текста. Она позволяет пользователям точно размещать объекты в сцене, гибко манипулировать объектами и камерой в 3D-пространстве. Система работает в два этапа: сначала пользователь интерактивно создает 3D-сигналы управления, затем эти сигналы используются для управления диффузионной моделью генерации видео. Для обучения была разработана автоматизированная система аннотации видеоданных с извлечением 3D-ограничивающих рамок и траекторий камеры."
                },
                "en": {
                    "title": "Empowering Video Creation with 3D Control",
                    "desc": "CineMaster is a new framework designed for generating videos from text while allowing users to control 3D elements like a film director. It operates in two stages: first, users create 3D-aware signals by placing objects and defining camera movements, and second, these signals guide a text-to-video diffusion model to produce the desired video. The framework also includes an automated data annotation system to gather necessary 3D motion and camera data from existing videos. Experiments show that CineMaster outperforms current methods in generating 3D-aware videos from text descriptions."
                },
                "zh": {
                    "title": "CineMaster：让视频生成如导演般可控",
                    "desc": "CineMaster是一个新颖的框架，用于生成具有3D感知和可控性的文本到视频。它使用户能够像专业电影导演一样精确控制场景中的物体位置、灵活操作3D空间中的物体和相机，并直观地布局渲染帧。该框架分为两个阶段：第一阶段通过交互式工作流程构建3D感知的条件信号，第二阶段利用这些信号指导文本到视频的扩散模型生成用户所需的视频内容。此外，CineMaster还建立了一个自动化数据注释管道，以解决缺乏3D物体运动和相机姿态标注的数据集问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08127",
            "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
            "url": "https://huggingface.co/papers/2502.08127",
            "abstract": "Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.",
            "score": 11,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "fa3f08993ba529cd",
            "authors": [
                "Lingfei Qian",
                "Weipeng Zhou",
                "Yan Wang",
                "Xueqing Peng",
                "Jimin Huang",
                "Qianqian Xie"
            ],
            "affiliations": [
                "TheFinAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08127.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#training",
                    "#long_context"
                ],
                "emoji": "💹",
                "ru": {
                    "title": "Специализация языковых моделей - ключ к успеху в финансовом анализе",
                    "desc": "В этом исследовании оценивается эффективность 16 мощных языковых моделей в решении сложных финансовых задач. Авторы обнаружили, что улучшение наборов данных и предварительное обучение повышают способности моделей к финансовым рассуждениям. Они разработали специализированную модель на основе Llama-3.1-8B-Instruct, которая превзошла даже более крупные модели в финансовых задачах. Исследование подчеркивает необходимость адаптации моделей к специфике финансовой области."
                },
                "en": {
                    "title": "Enhancing Financial Reasoning in LLMs with Domain-Specific Adaptations",
                    "desc": "This paper investigates the performance of large language models (LLMs) in financial reasoning tasks, which include interpreting financial text, analyzing tabular data, and solving equations. The authors evaluate 16 advanced LLMs and find that while improved datasets and pretraining enhance financial reasoning, general techniques like Chain-of-Thought (CoT) fine-tuning do not consistently improve results. They propose a new model, enhanced with CoT fine-tuning and reinforcement learning, which shows a significant performance boost of 10% across various financial tasks. The study emphasizes the importance of domain-specific adaptations for better performance in financial reasoning and introduces a leaderboard for future benchmarking."
                },
                "zh": {
                    "title": "金融推理模型的创新与提升",
                    "desc": "本研究评估了16种强大的语言模型在金融推理任务中的表现。这些任务包括金融文本、表格数据和方程式，涉及数值推理、表格解读和金融术语理解等方面。研究结果表明，尽管更好的数据集和预训练可以提升金融推理能力，但通用的增强方法如链式推理微调并不总是有效。为了解决这些问题，我们开发了一种基于Llama-3.1-8B-Instruct的金融推理增强模型，经过微调和强化学习后，在多个任务上实现了10%的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08168",
            "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
            "url": "https://huggingface.co/papers/2502.08168",
            "abstract": "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.",
            "score": 7,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "1ca2b8d38e35b203",
            "authors": [
                "Zhiming Ma",
                "Xiayang Xiao",
                "Sihao Dong",
                "Peidong Wang",
                "HaiPeng Wang",
                "Qingyun Pan"
            ],
            "affiliations": [
                "China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China",
                "China Mobile Internet Company Ltd., Guangzhou, China",
                "School of Computer Science and Engineering, Northeastern University, Shenyang, China",
                "The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China",
                "The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08168.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "SARChat-2M: Революция в интерпретации изображений SAR с помощью мультимодальных диалоговых моделей",
                    "desc": "Статья представляет первый крупномасштабный мультимодальный диалоговый датасет для изображений SAR, названный SARChat-2M. Он содержит около 2 миллионов пар изображение-текст высокого качества и охватывает различные сценарии с детальными аннотациями целей. Датасет поддерживает ключевые задачи, такие как визуальное понимание и обнаружение объектов, а также предоставляет основу для создания мультимодальных датасетов в различных областях дистанционного зондирования. Эффективность датасета была подтверждена экспериментами на 16 основных VLM, что позволило создать первый многозадачный диалоговый бенчмарк в области SAR."
                },
                "en": {
                    "title": "Empowering SAR Image Interpretation with SARChat-2M!",
                    "desc": "This paper introduces SARChat-2M, a large-scale multimodal dialogue dataset specifically designed for synthetic aperture radar (SAR) images. It contains around 2 million image-text pairs that cover various scenarios and include detailed annotations for effective visual understanding and object detection. The dataset serves as a benchmark for evaluating vision language models (VLMs) in the SAR domain, demonstrating their capabilities in interpreting SAR images. By conducting experiments on 16 popular VLMs, the study establishes a new multi-task dialogue benchmark, paving the way for advancements in remote sensing applications."
                },
                "zh": {
                    "title": "推动SAR图像解读的多模态对话数据集",
                    "desc": "在合成孔径雷达（SAR）遥感图像解读领域，尽管视觉语言模型（VLMs）在自然语言处理和图像理解方面取得了显著进展，但由于缺乏专业领域的知识，其应用仍然有限。本文创新性地提出了第一个大规模的SAR图像多模态对话数据集SARChat-2M，包含约200万对高质量的图像-文本配对，涵盖了多种场景和详细的目标注释。该数据集不仅支持视觉理解和目标检测等关键任务，还开发了SAR领域的视觉语言数据集和基准，评估VLMs在SAR图像解读中的能力。通过对16个主流VLM的实验验证，该数据集的有效性得到了充分证明，并成功建立了SAR领域的第一个多任务对话基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07864",
            "title": "TransMLA: Multi-head Latent Attention Is All You Need",
            "url": "https://huggingface.co/papers/2502.07864",
            "abstract": "Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.",
            "score": 4,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "dbff84dafe8c2312",
            "authors": [
                "Fanxu Meng",
                "Zengwei Yao",
                "Muhan Zhang"
            ],
            "affiliations": [
                "Institute for Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, Peking University",
                "Xiaomi Corp., Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07864.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Революция в архитектуре внимания: MLA для эффективных языковых моделей",
                    "desc": "Статья представляет новый метод под названием Multi-head Latent Attention (MLA), который решает проблему коммуникационных узких мест в больших языковых моделях. MLA использует матрицы низкого ранга в слоях ключ-значение, что позволяет сжимать и кэшировать латентные состояния KV. Авторы также предлагают метод TransMLA для преобразования предобученных моделей на основе Group Query Attention (GQA) в модели на основе MLA. Этот подход позволяет значительно уменьшить размер KV-кэша и ускорить вывод, сохраняя при этом выразительность модели."
                },
                "en": {
                    "title": "Transforming Attention: From GQA to Efficient MLA",
                    "desc": "This paper introduces Multi-head Latent Attention (MLA) as a solution to communication bottlenecks in large language models (LLMs) caused by hardware limitations. MLA utilizes low-rank matrices in the key-value (KV) layers to compress KV states, significantly reducing cache size and improving inference speed. The authors demonstrate that Group Query Attention (GQA) can be represented by MLA without increasing KV cache overhead, while MLA offers greater flexibility. To facilitate the adoption of MLA, they propose TransMLA, a method for converting GQA-based models into MLA-based ones, allowing for enhanced expressiveness without additional cache size."
                },
                "zh": {
                    "title": "提升语言模型效率的关键：多头潜在注意力",
                    "desc": "现代大型语言模型（LLMs）在当前硬件上常常面临通信瓶颈，而不仅仅是计算限制。多头潜在注意力（MLA）通过在键值（KV）层中使用低秩矩阵来解决这个问题，从而允许压缩的潜在KV状态被缓存。这种方法显著减少了KV缓存的大小，相比传统的多头注意力，推理速度更快。此外，MLA使用上投影矩阵来增加表达能力，以额外的计算换取减少的通信开销。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07737",
            "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
            "url": "https://huggingface.co/papers/2502.07737",
            "abstract": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.",
            "score": 1,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "8038af0ecacc031f",
            "authors": [
                "Shuhuai Ren",
                "Shuming Ma",
                "Xu Sun",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07737.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#inference",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "NBP: Быстрая и качественная генерация видео блоками",
                    "desc": "Статья представляет новый подход к генерации видео под названием Next-Block Prediction (NBP). В отличие от традиционного метода Next-Token Prediction, NBP использует полуавтореrрессивную модель, разбивая видео на блоки и предсказывая их параллельно. Это позволяет значительно ускорить процесс генерации и улучшить качество результатов. Модель NBP превзошла базовые методы по метрике FVD на датасетах UCF101 и K600, демонстрируя масштабируемость и эффективность подхода."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Next-Block Prediction",
                    "desc": "This paper introduces a new method for generating videos called Next-Block Prediction (NBP), which improves upon the traditional Next-Token Prediction (NTP) approach. NBP changes the way video content is generated by using blocks instead of individual tokens, allowing for simultaneous predictions within each block. This method utilizes bidirectional attention to enhance the understanding of spatial relationships in the video, leading to better quality outputs. As a result, NBP not only speeds up the generation process significantly but also achieves superior performance metrics compared to the standard NTP model."
                },
                "zh": {
                    "title": "视频生成的新突破：下一块预测",
                    "desc": "本文提出了一种新的半自回归框架，称为下一块预测（NBP），用于视频生成。与传统的自回归方法不同，NBP通过将视频内容均匀分解为相等大小的块，使得每个块内的标记可以同时预测下一个块的对应标记，从而捕捉更强的空间依赖性。该方法通过并行预测多个标记，显著减少了生成步骤，提高了推理速度，达到了每秒生成8.89帧的效果。实验结果表明，NBP在多个数据集上表现优于传统模型，展示了其在生成质量和速度上的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07599",
            "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
            "url": "https://huggingface.co/papers/2502.07599",
            "abstract": "Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.",
            "score": 1,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "85d178e03a57421f",
            "authors": [
                "Xiliang Yang",
                "Feng Jiang",
                "Qianen Zhang",
                "Lei Zhao",
                "Xiao Li"
            ],
            "affiliations": [
                "Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University",
                "School of Data Science, The Chinese University of Hong Kong, Shenzhen",
                "School of Mathematics, South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07599.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Контролируемое смещение вероятностей для улучшения обучения языковых моделей",
                    "desc": "Статья представляет новый метод под названием DPO-Shift для улучшения обучения языковых моделей с учетом человеческих предпочтений. Авторы решают проблему смещения вероятности выбранных ответов, которая возникает при использовании метода Direct Preference Optimization (DPO). DPO-Shift позволяет контролируемо смещать распределение вероятности выбранных ответов. Экспериментальные результаты показывают превосходство DPO-Shift над DPO на ряде задач, включая MT-Bench."
                },
                "en": {
                    "title": "Mitigating Likelihood Displacement in Language Model Training",
                    "desc": "This paper addresses the issue of likelihood displacement in Direct Preference Optimization (DPO) for aligning language models with human preferences. The authors introduce a new method, referred to as \textit{method}, which aims to control the distribution of chosen response probabilities during training. They highlight a trade-off between enhancing the chosen probability and the reward margin, supported by both theoretical insights and experimental results. The findings indicate that \textit{method} outperforms DPO in various downstream tasks, providing a promising solution to the challenges posed by likelihood displacement."
                },
                "zh": {
                    "title": "解决选择概率下降的有效方法",
                    "desc": "本文介绍了一种新的方法\textit{method}，旨在解决直接偏好优化（DPO）中出现的选择概率下降问题。研究表明，在训练过程中，模型对选择响应的概率往往会降低，这被称为似然位移。我们的方法可以控制选择概率的分布，从而改善模型的表现。通过理论分析和实验验证，我们证明了\textit{method}在下游任务中优于传统的DPO方法。"
                }
            }
        }
    ],
    "link_prev": "2025-02-12.html",
    "link_next": "2025-02-14.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2月12日"
    },
    "short_date_next": {
        "ru": "14.02",
        "en": "02/14",
        "zh": "2月14日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们展示了强化学习应用于大型语言模型（LLMs）可以显著提升复杂编程和推理任务的表现。我们比较了两个通用推理模型 - OpenAI o1 和 o3 的早期版本，以及一个针对2024年国际信息学奥林匹克（IOI）设计的特定领域系统 o1-ioi。我们在IOI 2024上使用 o1-ioi 参赛，并在放宽的比赛约束下获得了金牌。然而，后续模型如 o3 在没有手工制定的特定领域策略或放宽约束的情况下也能获得金牌。我们的发现表明，虽然专门的流水线如 o1-ioi 带来了显著的改进，但扩展的通用 o3 模型在不依赖手工制定的推理启发式的情况下超越了这些结果。",
        "title": "Competitive Programming with Large Reasoning Models",
        "pinyin": "Wǒmen zhǎnshìle qiáng huà xuéxí yìngyòng yú dàxíng yǔyán móxíng (LLMs) kěyǐ xiǎnzhù tíshēng fùzá bǐan chéng yǔ tuīlǐ rènwù de biǎoxiàn. Wǒmen bǐjiàole liǎng gè tōngyòng tuīlǐ móxíng - OpenAI o1 hé o3 de zǎoqī bǎnběn, yǐjià yīgè zhǐduì 2024 nián guójì xìnxī xué àolínpǐkè (IOI) shèjì de tèdìng yùyí xìtǒng o1-ioi. Wǒmen zài IOI 2024 shàng shǐyòng o1-ioi cānsài, bìng zài fàngkuān de bǐsài yuēshù xià huòdéle jīnpái. Rán'ér, hòuxù móxíng rú o3 zài méiyǒu shǒugōng zhìdìng de tèdìng yùyí cèlüè huò fàngkuān yuēshù de qíngkuàng xià yě néng huòdé jīnpái. Wǒmen de fāxiàn biǎomíng, suīrán zhuānmén de liúshuǐxiàn rú o1-ioi dàilái le xiǎnzhù de gǎijìn, dàn kuòzhǎn de tōngyòng o3 móxíng zài bù yīlài shǒugōng zhìdìng de tuīlǐ qǐfǎshì de qíngkuàng xià chāoyuèle zhèxiē jiéguǒ.",
        "vocab": "[{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'display'}, {'word': '强化学习', 'pinyin': 'qiáng​huà​xué​xí', 'trans': 'reinforcement learning'}, {'word': '应用于', 'pinyin': 'yìng​yòng​yú', 'trans': 'apply to'}, {'word': '大型语言模型', 'pinyin': 'dà​xíng​yǔ​yán​mó​xíng', 'trans': 'large language model'}, {'word': '显著', 'pinyin': 'xiǎn​zhù', 'trans': 'significant'}, {'word': '提升', 'pinyin': 'tí​shēng', 'trans': 'improve'}, {'word': '复杂', 'pinyin': 'fù​zá', 'trans': 'complex'}, {'word': '编程', 'pinyin': 'biān​chéng', 'trans': 'programming'}, {'word': '推理', 'pinyin': 'tuī​lǐ', 'trans': 'reasoning'}, {'word': '表现', 'pinyin': 'biǎo​xiàn', 'trans': 'performance'}, {'word': '比较', 'pinyin': 'bǐ​jiào', 'trans': 'compare'}, {'word': '通用', 'pinyin': 'tōng​yòng', 'trans': 'general-purpose'}, {'word': '针对', 'pinyin': 'zhēn​duì', 'trans': 'target'}, {'word': '特定领域', 'pinyin': 'tè​dìng​lǐng​yù', 'trans': 'specific domain'}, {'word': '系统', 'pinyin': 'xì​tǒng', 'trans': 'system'}, {'word': '国际信息学奥林匹克', 'pinyin': 'guó​jì​xìn​xī​xué​ào​lín​pǐ​kè', 'trans': 'International Olympiad in Informatics'}, {'word': '设计', 'pinyin': 'shè​jì', 'trans': 'design'}, {'word': '参赛', 'pinyin': 'cān​sài', 'trans': 'compete'}, {'word': '放宽', 'pinyin': 'fàng​kuān', 'trans': 'relax'}, {'word': '约束', 'pinyin': 'yuē​shù', 'trans': 'constraint'}, {'word': '获得', 'pinyin': 'huò​dé', 'trans': 'obtain'}, {'word': '金牌', 'pinyin': 'jīn​pái', 'trans': 'gold medal'}, {'word': '后续', 'pinyin': 'hòu​xù', 'trans': 'subsequent'}, {'word': '手工制定', 'pinyin': 'shǒu​gōng​zhì​dìng', 'trans': 'manually specified'}, {'word': '策略', 'pinyin': 'cè​lüè', 'trans': 'strategy'}, {'word': '启发式', 'pinyin': 'qǐ​fā​shì', 'trans': 'heuristic'}, {'word': '依赖', 'pinyin': 'yī​lài', 'trans': 'rely on'}, {'word': '扩展', 'pinyin': 'kuò​zhǎn', 'trans': 'extend'}, {'word': '超越', 'pinyin': 'chāo​yuè', 'trans': 'surpass'}, {'word': '结果', 'pinyin': 'jié​guǒ', 'trans': 'result'}]",
        "trans": "We demonstrated that applying reinforcement learning to large language models (LLMs) can significantly enhance performance in complex programming and reasoning tasks. We compared two general reasoning models—early versions of OpenAI o1 and o3—and a domain-specific system, o1-ioi, designed for the 2024 International Olympiad in Informatics (IOI). We competed in IOI 2024 using o1-ioi and won a gold medal under relaxed competition constraints. However, subsequent models like o3 were able to achieve gold medals without handcrafted domain-specific strategies or relaxed constraints. Our findings indicate that while specialized pipelines like o1-ioi brought significant improvements, the expanded general o3 model surpassed these results without relying on handcrafted reasoning heuristics.",
        "update_ts": "2025-02-12 09:11"
    }
}