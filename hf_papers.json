{
    "date": {
        "ru": "16 января",
        "en": "January 16",
        "zh": "1月16日"
    },
    "time_utc": "2025-01-16 06:14",
    "weekday": 3,
    "issue_id": 1698,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08994",
            "title": "RepVideo: Rethinking Cross-Layer Representation for Video Generation",
            "url": "https://huggingface.co/papers/2501.08994",
            "abstract": "Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of representations on the video generation process. In this paper, we initially investigate the characteristics of features in intermediate layers, finding substantial variations in attention maps across different layers. These variations lead to unstable semantic representations and contribute to cumulative differences between features, which ultimately reduce the similarity between adjacent frames and negatively affect temporal coherence. To address this, we propose RepVideo, an enhanced representation framework for text-to-video diffusion models. By accumulating features from neighboring layers to form enriched representations, this approach captures more stable semantic information. These enhanced representations are then used as inputs to the attention mechanism, thereby improving semantic expressiveness while ensuring feature consistency across adjacent frames. Extensive experiments demonstrate that our RepVideo not only significantly enhances the ability to generate accurate spatial appearances, such as capturing complex spatial relationships between multiple objects, but also improves temporal consistency in video generation.",
            "score": 3,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "0d164d45ba2a5c71",
            "authors": [
                "Chenyang Si",
                "Weichen Fan",
                "Zhengyao Lv",
                "Ziqi Huang",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore, 639798",
                "Shanghai Artificial Intelligence Laboratory, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08994.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "RepVideo: стабильные представления для качественной генерации видео",
                    "desc": "Статья представляет RepVideo - улучшенную систему представлений для диффузионных моделей генерации видео на основе текста. Авторы обнаружили, что вариации в картах внимания между слоями приводят к нестабильным семантическим представлениям и снижают согласованность соседних кадров. RepVideo решает эту проблему путем накопления признаков из соседних слоев для создания обогащенных представлений. Эксперименты показывают, что RepVideo значительно улучшает способность генерировать точные пространственные образы и повышает временную согласованность при генерации видео."
                },
                "en": {
                    "title": "Enhancing Video Generation with Stable Representations",
                    "desc": "This paper presents RepVideo, a new framework designed to improve video generation using text-to-video diffusion models. It identifies issues with unstable semantic representations caused by variations in attention maps across different layers of the model. By accumulating features from neighboring layers, RepVideo creates more stable and enriched representations that enhance the model's ability to maintain consistency between adjacent frames. The results show that RepVideo significantly improves both the spatial accuracy of generated videos and their temporal coherence, leading to more realistic video outputs."
                },
                "zh": {
                    "title": "提升视频生成质量的RepVideo框架",
                    "desc": "本论文探讨了扩散模型在视频生成中的应用，提出了RepVideo框架以改善视频生成的质量。研究发现中间层特征的注意力图存在显著差异，这导致语义表示的不稳定性，进而影响相邻帧之间的相似性和时间一致性。RepVideo通过从相邻层累积特征，形成更丰富的表示，从而捕捉更稳定的语义信息。实验结果表明，RepVideo显著提高了生成视频的空间表现能力和时间一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08983",
            "title": "CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities",
            "url": "https://huggingface.co/papers/2501.08983",
            "abstract": "3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities.",
            "score": 2,
            "issue_id": 1698,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "39cd0826d4232170",
            "authors": [
                "Haozhe Xie",
                "Zhaoxi Chen",
                "Fangzhou Hong",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore 637335"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08983.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "Композиционная генерация 4D-городов с разделением динамики и статики",
                    "desc": "CityDreamer4D - это генеративная модель для создания неограниченных 4D-городов. Она разделяет генерацию динамических объектов (например, транспорта) и статических сцен (зданий, дорог). Модель использует разные типы нейронных полей для зданий, транспорта и фона, применяя специализированные генеративные хеш-сетки и периодические позиционные эмбеддинги. CityDreamer4D демонстрирует передовые результаты в генерации реалистичных 4D-городов и поддерживает различные приложения, включая редактирование объектов и городское моделирование."
                },
                "en": {
                    "title": "Revolutionizing Urban Landscapes: CityDreamer4D for Dynamic City Generation",
                    "desc": "This paper introduces CityDreamer4D, a generative model designed for creating unbounded 4D cities, which include both static and dynamic elements. The model distinguishes between dynamic objects like vehicles and static structures such as buildings, using specialized neural fields for each type. It employs a compact bird's-eye view (BEV) representation to generate realistic traffic scenarios and city layouts. Additionally, the paper provides extensive datasets for training, enabling various applications like instance editing and urban simulation while achieving high-quality results in 4D city generation."
                },
                "zh": {
                    "title": "CityDreamer4D：无限4D城市生成的新突破",
                    "desc": "近年来，3D场景生成受到了越来越多的关注，并取得了显著进展。生成4D城市比3D场景更具挑战性，因为城市环境中存在结构复杂、视觉多样的物体，如建筑和车辆。为了解决这些问题，我们提出了CityDreamer4D，这是一种专门用于生成无限4D城市的组合生成模型。该模型通过将动态物体与静态场景分离，并使用不同类型的神经场来组合城市中的所有物体，从而实现高质量的城市生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08828",
            "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
            "url": "https://huggingface.co/papers/2501.08828",
            "abstract": "Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval.",
            "score": 2,
            "issue_id": 1698,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "bf9a6df8fecd4ec1",
            "authors": [
                "Kuicai Dong",
                "Yujing Chang",
                "Xin Deik Goh",
                "Dexun Li",
                "Ruiming Tang",
                "Yong Liu"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08828.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "MMDocIR: Новый стандарт для мультимодального поиска документов",
                    "desc": "Статья представляет новый бенчмарк MMDocIR для оценки систем мультимодального поиска документов. Бенчмарк включает две задачи: поиск на уровне страниц и на уровне макетов. Датасет содержит экспертные аннотации для 1,685 вопросов и автоматически сгенерированные метки для 173,843 вопросов. Эксперименты показали, что визуальные ретриверы превосходят текстовые, а использование визуально-языковых моделей дает лучшие результаты, чем OCR-текст."
                },
                "en": {
                    "title": "Unlocking Multi-Modal Document Retrieval with MMDocIR",
                    "desc": "This paper addresses the challenge of multi-modal document retrieval, which involves finding various types of content like figures and tables in large documents. It introduces a new benchmark called MMDocIR, which includes two tasks: page-level retrieval for finding relevant pages and layout-level retrieval for identifying specific layouts within those pages. The benchmark is supported by a comprehensive dataset with thousands of annotated questions, facilitating better training and evaluation of retrieval systems. The results show that visual retrieval methods outperform text-based methods, highlighting the importance of incorporating visual information in multi-modal retrieval tasks."
                },
                "zh": {
                    "title": "多模态文档检索的新基准MMDocIR",
                    "desc": "多模态文档检索旨在从大量文档中识别和提取各种形式的内容，如图形、表格、图表和布局信息。尽管其重要性显著，但目前缺乏有效评估多模态文档检索系统性能的基准。为了解决这一问题，本文提出了一个新的基准MMDocIR，包含页面级和布局级检索两个任务。通过严格的实验，我们发现视觉检索器的表现显著优于文本检索器，且MMDocIR训练集能有效促进多模态文档检索的训练过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08809",
            "title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework",
            "url": "https://huggingface.co/papers/2501.08809",
            "abstract": "In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is https://xmusic-project.github.io.",
            "score": 1,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "d4d018c9adb2579c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#audio",
                    "#story_generation",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "XMusic: ИИ-композитор нового поколения с управляемыми эмоциями",
                    "desc": "Статья представляет XMusic - генерализованный фреймворк для генерации символической музыки, поддерживающий различные типы промптов. XMusic состоит из двух ключевых компонентов: XProjector для обработки промптов и XComposer для генерации музыки. Авторы также создали датасет XMIDI, содержащий более 100 тысяч MIDI-файлов с аннотациями эмоций и жанров. Согласно оценкам, XMusic значительно превосходит современные методы по качеству генерируемой музыки."
                },
                "en": {
                    "title": "XMusic: Emotionally Controlled Music Generation Made Easy!",
                    "desc": "This paper introduces XMusic, a new framework for generating symbolic music that can be controlled by emotional prompts. It includes two main components: XProjector, which converts various input types into musical elements, and XComposer, which generates and selects high-quality music. The framework uses a multi-task learning approach to ensure the generated music meets quality, emotional, and genre standards. Additionally, the authors created a large dataset, XMIDI, to support their research and demonstrate that XMusic outperforms existing methods in music generation."
                },
                "zh": {
                    "title": "XMusic：情感可控的高质量音乐生成",
                    "desc": "近年来，人工智能生成内容（AIGC）在图像合成和文本生成领域取得了显著进展，但在音乐生成方面仍面临挑战。本文提出了一种通用的符号音乐生成框架XMusic，能够通过灵活的提示生成可控情感和高质量的符号音乐。XMusic由两个核心组件组成：XProjector和XComposer，前者将多种模态的提示解析为音乐元素，后者则生成和选择高质量的音乐。通过构建大规模的XMIDI数据集和多任务学习方案，XMusic在音乐质量上显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09019",
            "title": "Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion",
            "url": "https://huggingface.co/papers/2501.09019",
            "abstract": "The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency.",
            "score": 0,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "c4c991699f684865",
            "authors": [
                "Jingyuan Chen",
                "Fuchen Long",
                "Jie An",
                "Zhaofan Qiu",
                "Ting Yao",
                "Jiebo Luo",
                "Tao Mei"
            ],
            "affiliations": [
                "HiDream.ai Inc.",
                "University of Rochester, Rochester, NY USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09019.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#diffusion"
                ],
                "emoji": "🐍",
                "ru": {
                    "title": "Бесконечное видео: Ouroboros-Diffusion для непрерывной генерации согласованного контента",
                    "desc": "Эта статья представляет новый метод генерации видео произвольной длины под названием Ouroboros-Diffusion. Метод улучшает структурную и сюжетную согласованность видео с помощью нового подхода к выборке латентного пространства и механизма Subject-Aware Cross-Frame Attention. Авторы также вводят самоповторяющееся руководство, использующее информацию из предыдущих очищенных кадров для улучшения шумных кадров. Эксперименты на бенчмарке VBench показывают превосходство Ouroboros-Diffusion в сохранении согласованности субъектов, плавности движения и временной согласованности."
                },
                "en": {
                    "title": "Ouroboros-Diffusion: Enhancing Long Video Consistency and Coherence",
                    "desc": "The paper introduces Ouroboros-Diffusion, a new framework for improving long video generation using a pre-trained text-to-video model. It addresses the limitations of FIFO-Diffusion, particularly in maintaining long-range temporal consistency across video frames. The proposed method enhances structural consistency through a novel latent sampling technique and improves subject consistency with a Subject-Aware Cross-Frame Attention mechanism. Additionally, self-recurrent guidance is implemented to utilize information from previous frames, resulting in videos with better visual coherence and smoother transitions."
                },
                "zh": {
                    "title": "Ouroboros-Diffusion：提升视频生成一致性的创新框架",
                    "desc": "FIFO视频扩散是一种基于预训练文本到视频模型的长视频生成方法，但在生成视频时常常缺乏长时间的一致性。本文提出了Ouroboros-Diffusion框架，通过引入新的潜在采样技术和主题感知跨帧注意机制，增强了视频的结构和内容一致性。该方法确保了帧之间的平滑过渡，并通过自递归引导技术利用前面清晰帧的信息来改善后面噪声帧的去噪效果。实验结果表明，Ouroboros-Diffusion在主题一致性、运动平滑性和时间一致性方面优于现有方法。"
                }
            }
        }
    ],
    "link_prev": "2025-01-15.html",
    "link_next": "2025-01-17.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "15.01",
        "en": "01/15",
        "zh": "1月15日"
    },
    "short_date_next": {
        "ru": "17.01",
        "en": "01/17",
        "zh": "1月17日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了 MiniMax-01 系列，包括 MiniMax-Text-01 和 MiniMax-VL-01。这些模型在处理长上下文方面具有卓越能力。核心在于闪电注意力和其高效扩展。我们将其与混合专家模型（MoE）集成，创建了一个具有 32 个专家和 4560 亿总参数的模型。我们开发了优化的并行策略和高效的计算通信重叠技术。这使我们能够在数百亿参数的模型上进行高效训练和推理。MiniMax-Text-01 的上下文窗口在训练期间可达到 100 万个标记，并在推理期间扩展到 400 万个标记。MiniMax-VL-01 通过使用 5120 亿视觉语言标记进行持续训练。实验表明，我们的模型在标准和内部基准上的性能与 GPT-4o 和 Claude-3.5-Sonnet 相当，同时提供 20-32 倍的上下文窗口。我们在 https://github.com/MiniMax-AI 公开发布了 MiniMax-01。",
        "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
        "pinyin": "Wǒmen jièshào le MiniMax-01 xìliè, bāokuò MiniMax-Text-01 hé MiniMax-VL-01. Zhèxiē móxíng zài chǔlǐ cháng shàngxìawén fāngmiàn jùyǒu zhuóyuè nénglì. Héxīn zàiyú shǎndiǎn zhùyìlì hé qí gāoxiào kuòzhǎn. Wǒmen jiāng qí yǔ hùn hé zhuānjiā móxíng (MoE) jíchéng, chuàngjiàn le yīgè jùyǒu 32 gè zhuānjiā hé 4560 yì zǒng cānshù de móxíng. Wǒmen kāifā le yōuhuà de bìngxíng cèlüè hé gāoxiào de jìsuàn tōngxìn zhòngdié jìshù. Zhè shǐ wǒmen nénggòu zài shùbǎiyì cānshù de móxíng shàng jìnxíng gāoxiào xùnliàn hé tuìlǐ. MiniMax-Text-01 de shàngxìawén chuāngkǒu zài xùnliàn qījiān kě dádào 100 wàn gè biāojì, bìng zài tuìlǐ qījiān kuòzhǎn dào 400 wàn gè biāojì. MiniMax-VL-01 tōngguò shǐyòng 5120 yì shìjué yǔyán biāojì jìnxíng chíxù xùnliàn. Shìyàn biǎomíng, wǒmen de móxíng zài biāozhǔn hé nèibù jīzhǔn shàng de xiàonénglì yǔ GPT-4o hé Claude-3.5-Sonnet xiāngdāng, tóngshí tígōng 20-32 bèi de shàngxìawén chuāngkǒu. Wǒmen zài https://github.com/MiniMax-AI gōngkāi fābù le MiniMax-01.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"系列\", \"pinyin\": \"xì liè\", \"trans\": \"series\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"process\"},\n    {\"word\": \"上下文\", \"pinyin\": \"shàng xià wén\", \"trans\": \"context\"},\n    {\"word\": \"卓越\", \"pinyin\": \"zhuó yuè\", \"trans\": \"outstanding\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"核心\", \"pinyin\": \"hé xīn\", \"trans\": \"core\"},\n    {\"word\": \"闪电\", \"pinyin\": \"shǎn diàn\", \"trans\": \"lightning\"},\n    {\"word\": \"注意力\", \"pinyin\": \"zhù yì lì\", \"trans\": \"attention\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāo xiào\", \"trans\": \"efficient\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuò zhǎn\", \"trans\": \"expand\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùn hé\", \"trans\": \"hybrid\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuān jiā\", \"trans\": \"expert\"},\n    {\"word\": \"集成\", \"pinyin\": \"jí chéng\", \"trans\": \"integrate\"},\n    {\"word\": \"并行\", \"pinyin\": \"bìng xíng\", \"trans\": \"parallel\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"通信\", \"pinyin\": \"tōng xìn\", \"trans\": \"communication\"},\n    {\"word\": \"重叠\", \"pinyin\": \"chóng dié\", \"trans\": \"overlap\"},\n    {\"word\": \"技术\", \"pinyin\": \"jì shù\", \"trans\": \"technology\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"train\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"窗口\", \"pinyin\": \"chuāng kǒu\", \"trans\": \"window\"},\n    {\"word\": \"标记\", \"pinyin\": \"biāo jì\", \"trans\": \"token\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"语言\", \"pinyin\": \"yǔ yán\", \"trans\": \"language\"},\n    {\"word\": \"持续\", \"pinyin\": \"chí xù\", \"trans\": \"continuous\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōng kāi\", \"trans\": \"public\"},\n    {\"word\": \"发布\", \"pinyin\": \"fā bù\", \"trans\": \"release\"}\n]",
        "trans": "We introduced the MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01. These models excel in handling long contexts, with a core focus on flash attention and its efficient scaling. We integrated them with a Mixture of Experts (MoE) model, creating a model with 32 experts and a total of 4560 billion parameters. We developed optimized parallel strategies and efficient computation-communication overlap techniques. This enables us to perform efficient training and inference on models with hundreds of billions of parameters. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and expands to 4 million tokens during inference. MiniMax-VL-01 undergoes continuous training using 5120 billion vision-language tokens. Experiments show that our models perform comparably to GPT-4o and Claude-3.5-Sonnet on standard and internal benchmarks while providing a 20-32 times larger context window. We have made MiniMax-01 publicly available at https://github.com/MiniMax-AI.",
        "update_ts": "2025-01-15 09:11"
    }
}