{
    "date": {
        "ru": "2 Ğ¼Ğ°Ñ",
        "en": "May 2",
        "zh": "5æœˆ2æ—¥"
    },
    "time_utc": "2025-05-02 02:26",
    "weekday": 4,
    "issue_id": 3548,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.00703",
            "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT",
            "url": "https://huggingface.co/papers/2505.00703",
            "abstract": "Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1",
            "score": 3,
            "issue_id": 3548,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ",
                "en": "May 1",
                "zh": "5æœˆ1æ—¥"
            },
            "hash": "ca564761ff71d15e",
            "authors": [
                "Dongzhi Jiang",
                "Ziyu Guo",
                "Renrui Zhang",
                "Zhuofan Zong",
                "Hao Li",
                "Le Zhuo",
                "Shilin Yan",
                "Pheng-Ann Heng",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "CUHK MiuLar Lab",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00703.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ T2I-R1 - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BiCoT-GRPO Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ¾Ğ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Janus-Pro Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… T2I-CompBench Ğ¸ WISE."
                },
                "en": {
                    "title": "Revolutionizing Text-to-Image Generation with Enhanced Reasoning",
                    "desc": "This paper introduces T2I-R1, a new model for generating images from text that uses advanced reasoning techniques. It employs a bi-level chain-of-thought (CoT) approach, which includes semantic-level reasoning for planning and token-level reasoning for detailed image generation. The model is enhanced by reinforcement learning (RL) and a novel reward system called BiCoT-GRPO, which optimizes both levels of reasoning simultaneously. As a result, T2I-R1 outperforms existing models, achieving significant improvements on benchmark tests."
                },
                "zh": {
                    "title": "åŒå±‚æ€ç»´é“¾æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹T2I-R1ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’ŒåŒå±‚æ€ç»´é“¾æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ€ç»´é“¾ï¼šè¯­ä¹‰å±‚æ¬¡çš„æ€ç»´é“¾ç”¨äºé«˜å±‚æ¬¡çš„æç¤ºè§„åˆ’ï¼Œä»¤ç”Ÿæˆè¿‡ç¨‹æ›´å…·é€»è¾‘æ€§ï¼›è€Œä»¤ç‰Œå±‚æ¬¡çš„æ€ç»´é“¾åˆ™ç”¨äºåœ¨é€å—ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œä½å±‚æ¬¡çš„åƒç´ å¤„ç†ã€‚é€šè¿‡å¼•å…¥BiCoT-GRPOï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨åŒä¸€è®­ç»ƒæ­¥éª¤ä¸­ä¼˜åŒ–è¿™ä¸¤ç§æ€ç»´é“¾ï¼Œä»è€Œæå‡ç”Ÿæˆæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT2I-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-01.html",
    "link_next": "2025-05-05.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "01.05",
        "en": "05/01",
        "zh": "5æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "05.05",
        "en": "05/05",
        "zh": "5æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†é˜¿æ‹‰ä¼¯æ–‡æœ¬æ·»åŠ å‘éŸ³ç¬¦å·çš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºSadeedçš„æ–°æ–¹æ³•ï¼ŒåŸºäºä¸€ä¸ªç»è¿‡å¾®è°ƒçš„è§£ç å™¨è¯­è¨€æ¨¡å‹ã€‚Sadeedåœ¨ç²¾å¿ƒç¼–åˆ¶çš„é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å–å¾—äº†ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è¯„æµ‹åŸºå‡†SadeedDiac-25ï¼Œä»¥è§£å†³å½“å‰è¯„æµ‹æ–¹æ³•çš„å±€é™æ€§ã€‚è¿™äº›å·¥å…·å…±åŒæ¨åŠ¨äº†é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨çš„å‘å±•ã€‚",
        "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†é˜¿æ‹‰ä¼¯æ–‡æœ¬æ·»åŠ å‘éŸ³ç¬¦å·çš„æŒ‘æˆ˜ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le Ä€lÄbÃ³ wÃ©nbÄ›n tiÄnjiÄ fÄyÄ«n fÃºhÃ o de tiÇozhÃ n.\n\nä½œè€…æå‡ºäº†ä¸€ç§åä¸ºSadeedçš„æ–°æ–¹æ³•ï¼Œ\nZuÃ²zhÄ› tÃ­chÅ« le yÄ«zhÇ’ng mÃ­ngwÃ¨i Sadeed de xÄ«n fÄngfÇ,\n\nåŸºäºä¸€ä¸ªç»è¿‡å¾®è°ƒçš„è§£ç å™¨è¯­è¨€æ¨¡å‹ã€‚\nJÄ«yÃº yÄ«gÃ¨ jÄ«ngguÃ² wÄ“itiÃ¡o de jiÄ›mÇqÃ¬ yÇ”yÃ¡n mÃ³xÃ­ng.\n\nSadeedåœ¨ç²¾å¿ƒç¼–åˆ¶çš„é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œ\nSadeed zÃ i jÄ«ngxÄ«n biÄnzhÃ¬ de gÄo zhÃ¬liÃ ng shÃ¹jÃ¹jÃ­ shÃ ng jÃ¬nxÃ­ng wÄ“itiÃ¡o,\n\nå¹¶åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å–å¾—äº†ç«äº‰åŠ›çš„ç»“æœã€‚\nBÃ¬ng zÃ i yÇ’uxiÃ n de jÃ¬suÃ n zÄ«yuÃ¡n xiÃ  qÇ”dÃ© le jÃ¬ngzhÄ“nglÃ¬ de jiÃ©guÇ’.\n\næ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è¯„æµ‹åŸºå‡†SadeedDiac-25ï¼Œ\nCÇwÃ i, wÃ©nzhÄng hÃ¡i jiÃ¨shÃ o le yÄ«gÃ¨ xÄ«n de pÃ­ngcÃ¨ jÄ«zhÇ”n SadeedDiac-25,\n\nä»¥è§£å†³å½“å‰è¯„æµ‹æ–¹æ³•çš„å±€é™æ€§ã€‚\nYÇ jiÄ›juÃ© dÄngqiÃ¡n pÃ­ngcÃ¨ fÄngfÇ de jÃºxiÃ nxÃ¬ng.\n\nè¿™äº›å·¥å…·å…±åŒæ¨åŠ¨äº†é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨çš„å‘å±•ã€‚\nZhÃ¨xiÄ“ gÅngjÃ¹ gÃ²ngtÃ³ng tuÄ«dÃ²ng le Ä€lÄbÃ³yÇ” zÃ¬rÃ¡n yÇ”yÃ¡n chÇ”lÇ yÃ¬ngyÃ²ng de fÄzhÇn.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"é˜¿æ‹‰ä¼¯\", \"pinyin\": \"Ä lÄ bÃ³\", \"trans\": \"Arabic\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©n bÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"æ·»åŠ \", \"pinyin\": \"tiÄn jiÄ\", \"trans\": \"add\"},\n    {\"word\": \"å‘éŸ³\", \"pinyin\": \"fÄ yÄ«n\", \"trans\": \"pronunciation\"},\n    {\"word\": \"ç¬¦å·\", \"pinyin\": \"fÃº hÃ o\", \"trans\": \"symbol\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"åä¸º\", \"pinyin\": \"mÃ­ng wÃ©i\", \"trans\": \"named\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“i tiÃ¡o\", \"trans\": \"fine-tune\"},\n    {\"word\": \"è§£ç å™¨\", \"pinyin\": \"jiÄ› mÇ qÃ¬\", \"trans\": \"decoder\"},\n    {\"word\": \"è¯­è¨€\", \"pinyin\": \"yÇ” yÃ¡n\", \"trans\": \"language\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"ç²¾å¿ƒ\", \"pinyin\": \"jÄ«ng xÄ«n\", \"trans\": \"carefully\"},\n    {\"word\": \"ç¼–åˆ¶\", \"pinyin\": \"biÄn zhÃ¬\", \"trans\": \"compile\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"è¿›è¡Œ\", \"pinyin\": \"jÃ¬n xÃ­ng\", \"trans\": \"conduct\"},\n    {\"word\": \"æœ‰é™\", \"pinyin\": \"yÇ’u xiÃ n\", \"trans\": \"limited\"},\n    {\"word\": \"è®¡ç®—\", \"pinyin\": \"jÃ¬ suÃ n\", \"trans\": \"computational\"},\n    {\"word\": \"èµ„æº\", \"pinyin\": \"zÄ« yuÃ¡n\", \"trans\": \"resources\"},\n    {\"word\": \"å–å¾—\", \"pinyin\": \"qÇ” dÃ©\", \"trans\": \"achieve\"},\n    {\"word\": \"ç«äº‰åŠ›\", \"pinyin\": \"jÃ¬ng zhÄ“ng lÃ¬\", \"trans\": \"competitive\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"results\"},\n    {\"word\": \"æ­¤å¤–\", \"pinyin\": \"cÇ wÃ i\", \"trans\": \"moreover\"},\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"è¯„æµ‹\", \"pinyin\": \"pÃ­ng cÃ¨\", \"trans\": \"evaluation\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"è§£å†³\", \"pinyin\": \"jiÄ› juÃ©\", \"trans\": \"address\"},\n    {\"word\": \"å½“å‰\", \"pinyin\": \"dÄng qiÃ¡n\", \"trans\": \"current\"},\n    {\"word\": \"å±€é™æ€§\", \"pinyin\": \"jÃº xiÃ n xÃ¬ng\", \"trans\": \"limitations\"},\n    {\"word\": \"è¿™äº›\", \"pinyin\": \"zhÃ¨ xiÄ“\", \"trans\": \"these\"},\n    {\"word\": \"å·¥å…·\", \"pinyin\": \"gÅng jÃ¹\", \"trans\": \"tools\"},\n    {\"word\": \"å…±åŒ\", \"pinyin\": \"gÃ²ng tÃ³ng\", \"trans\": \"jointly\"},\n    {\"word\": \"æ¨åŠ¨\", \"pinyin\": \"tuÄ« dÃ²ng\", \"trans\": \"promote\"},\n    {\"word\": \"è‡ªç„¶\", \"pinyin\": \"zÃ¬ rÃ¡n\", \"trans\": \"natural\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"processing\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"å‘å±•\", \"pinyin\": \"fÄ zhÇn\", \"trans\": \"development\"}\n]",
        "trans": "This article discusses the challenges of adding diacritical marks to Arabic texts. The authors propose a new method called Sadeed, based on a fine-tuned decoder language model. Sadeed is fine-tuned on a meticulously curated high-quality dataset and achieves competitive results with limited computational resources. Additionally, the article introduces a new evaluation benchmark, SadeedDiac-25, to address the limitations of current evaluation methods. These tools collectively advance the development of Arabic natural language processing applications.",
        "update_ts": "2025-05-01 09:12"
    }
}