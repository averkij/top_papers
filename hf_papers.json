{
    "date": {
        "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 13",
        "zh": "1æœˆ13æ—¥"
    },
    "time_utc": "2026-01-13 03:40",
    "weekday": 1,
    "issue_id": 544,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.05593",
            "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
            "url": "https://huggingface.co/papers/2601.05593",
            "abstract": "Parallel Coordinated Reasoning enables large-scale test-time compute scaling beyond sequential reasoning limitations through parallel exploration and message-passing architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.",
            "score": 10,
            "issue_id": 544,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "2beebb5ab57389a3",
            "authors": [
                "Jingcheng Hu",
                "Yinmin Zhang",
                "Shijie Shang",
                "Xiaobo Yang",
                "Yue Peng",
                "Zhewei Huang",
                "Hebin Zhou",
                "Xin Wu",
                "Jie Cheng",
                "Fanqi Wan",
                "Xiangwen Kong",
                "Chengyuan Yao",
                "Kaiwen Yan",
                "Ailin Huang",
                "Hongyu Zhou",
                "Qi Han",
                "Zheng Ge",
                "Daxin Jiang",
                "Xiangyu Zhang",
                "Heung-Yeung Shum"
            ],
            "affiliations": [
                "Peking University",
                "StepFun",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05593.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#open_source",
                    "#benchmark",
                    "#math",
                    "#reasoning",
                    "#rl",
                    "#inference"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Parallel Coordinated Reasoning (PaCoRe) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡ĞµĞ¹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ end-to-end Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 94.5% Ğ½Ğ° HMMT 2025 Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 8B, Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°Ñ GPT-5, Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ¾ Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Parallel Reasoning for Enhanced AI Performance",
                    "desc": "Parallel Coordinated Reasoning (PaCoRe) is a novel framework that enhances the capabilities of language models by allowing them to perform reasoning tasks in parallel rather than sequentially. This method utilizes a message-passing architecture to coordinate multiple reasoning trajectories, enabling the model to explore vast amounts of information simultaneously. By training with large-scale reinforcement learning, PaCoRe effectively manages to synthesize findings from these parallel explorations, significantly increasing the test-time compute (TTC) without being limited by context window constraints. The results demonstrate substantial improvements in reasoning tasks, particularly in mathematics, showcasing the potential of PaCoRe to exceed the performance of existing models like GPT-5."
                },
                "zh": {
                    "title": "å¹¶è¡Œåè°ƒæ¨ç†ï¼šè¶…è¶Šé¡ºåºæ¨ç†çš„è®¡ç®—èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¹¶è¡Œåè°ƒæ¨ç†ï¼ˆPaCoReï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶è®¡ç®—èƒ½åŠ›æ‰©å±•æ–¹é¢çš„é™åˆ¶ã€‚PaCoReé€šè¿‡æ¶ˆæ¯ä¼ é€’æ¶æ„å®ç°å¤§è§„æ¨¡çš„å¹¶è¡Œæ¢ç´¢ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿçš„é¡ºåºæ¨ç†æ¨¡å¼ã€‚æ¯ä¸€è½®æ¨ç†ä¼šå¯åŠ¨å¤šä¸ªå¹¶è¡Œè½¨è¿¹ï¼Œå°†ç»“æœå‹ç¼©æˆæ¶ˆæ¯ï¼Œå¹¶åˆæˆè¿™äº›æ¶ˆæ¯ä»¥æŒ‡å¯¼ä¸‹ä¸€è½®æ¨ç†ï¼Œæœ€ç»ˆå¾—å‡ºç­”æ¡ˆã€‚è¯¥æ–¹æ³•ç»è¿‡å¤§è§„æ¨¡çš„åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨ä¸è¶…è¿‡ä¸Šä¸‹æ–‡é™åˆ¶çš„æƒ…å†µä¸‹ï¼Œæ‰©å±•åˆ°æ•°ç™¾ä¸‡ä¸ªæœ‰æ•ˆçš„æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.05823",
            "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
            "url": "https://huggingface.co/papers/2601.05823",
            "abstract": "Latent Diffusion Models generate high-quality images by operating in compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.",
            "score": 8,
            "issue_id": 544,
            "pub_date": "2026-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "d7de342c692a7e3e",
            "authors": [
                "John Page",
                "Xuesong Niu",
                "Kai Wu",
                "Kun Gai"
            ],
            "affiliations": [
                "Kolors Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.05823.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ”Ğ¸sentangled Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: VAE Ğ¿ĞµÑ€ĞµÑƒÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾-Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Send-VAE â€” Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸sentangled Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ VAE Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VAE Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ-Ğ¼Ğ°Ğ¿Ğ¿ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° VAE Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… foundation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Send-VAE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ñ FID 1.21 Ğ½Ğ° ImageNet 256x256, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ flow-based Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Image Generation with Semantic Disentangled VAEs",
                    "desc": "This paper introduces the Semantic Disentangled VAE (Send-VAE), which aims to improve image generation by optimizing Variational Autoencoders (VAEs) for better semantic disentanglement. Unlike traditional methods that use the same alignment targets for both VAEs and Latent Diffusion Models (LDMs), Send-VAE focuses on aligning its latent space with the semantic hierarchy of pre-trained Vision Foundation Models (VFMs). This approach allows VAEs to encode attribute-level information more effectively while maintaining high-level semantic concepts, enhancing the overall image generation process. The results demonstrate that Send-VAE not only speeds up training but also achieves state-of-the-art performance on image quality metrics like FID."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è§£è€¦è¡¨ç¤ºï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡",
                    "desc": "æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰é€šè¿‡åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œé€šå¸¸ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ä½œä¸ºå›¾åƒæ ‡è®°å™¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰è§£è€¦å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆSend-VAEï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–è§£è€¦è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶å°†å…¶æ½œåœ¨ç©ºé—´ä¸é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„è¯­ä¹‰å±‚æ¬¡å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨éçº¿æ€§æ˜ å°„ç½‘ç»œæ¥è½¬æ¢VAEçš„æ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆåœ°è¿æ¥å±æ€§çº§è§£è€¦ä¸é«˜å±‚è¯­ä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSend-VAEæ˜¾è‘—åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶åœ¨ImageNet 256x256ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„FIDå€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04698",
            "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
            "url": "https://huggingface.co/papers/2601.04698",
            "abstract": "TourPlanner addresses travel planning challenges through multi-path reasoning and constraint-gated reinforcement learning to optimize both hard and soft constraints effectively.  \t\t\t\t\tAI-generated summary \t\t\t\t Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.",
            "score": 7,
            "issue_id": 544,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "84584233b4dfbf19",
            "authors": [
                "Yinuo Wang",
                "Mining Tan",
                "Wenxiang Jiao",
                "Xiaoxi Li",
                "Hao Wang",
                "Xuanyu Zhang",
                "Yuan Lu",
                "Weiming Dong"
            ],
            "affiliations": [
                "MAIS, Institute of Automation, Chinese Academy of Sciences",
                "Renmin University of China",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04698.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "âœˆï¸",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "TourPlanner Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ PReSO Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¾Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ¾Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ°ĞºĞ¾Ğ½ĞµÑ†, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ² ÑÑ‚Ğ°Ğ¿ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Optimizing Travel Plans with Multi-Path Reasoning and Reinforcement Learning",
                    "desc": "TourPlanner is a novel framework designed to enhance travel planning by utilizing multi-path reasoning and constraint-gated reinforcement learning. It effectively addresses key challenges such as maintaining a high recall rate for points of interest while optimizing both hard and soft constraints. The framework introduces a Personalized Recall and Spatial Optimization (PReSO) workflow to create a set of relevant candidate locations, and a Competitive Consensus Chain-of-Thought (CCoT) approach to explore diverse solution paths. Experimental results indicate that TourPlanner outperforms existing methods, achieving better feasibility and alignment with user preferences."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ—…è¡Œè§„åˆ’çš„æ–°æ–¹æ³•",
                    "desc": "TourPlanner æ˜¯ä¸€ä¸ªè§£å†³æ—…è¡Œè§„åˆ’æŒ‘æˆ˜çš„æ¡†æ¶ï¼Œé‡‡ç”¨å¤šè·¯å¾„æ¨ç†å’Œçº¦æŸé—¨æ§å¼ºåŒ–å­¦ä¹ æ¥æœ‰æ•ˆä¼˜åŒ–ç¡¬çº¦æŸå’Œè½¯çº¦æŸã€‚å®ƒé¦–å…ˆé€šè¿‡ä¸ªæ€§åŒ–å›å¿†å’Œç©ºé—´ä¼˜åŒ–ï¼ˆPReSOï¼‰å·¥ä½œæµç¨‹æ„å»ºç©ºé—´æ„ŸçŸ¥çš„å€™é€‰å…´è¶£ç‚¹ï¼ˆPOIï¼‰é›†åˆã€‚æ¥ç€ï¼Œä½¿ç”¨ç«äº‰å…±è¯†æ€ç»´é“¾ï¼ˆCCoTï¼‰å¤šè·¯å¾„æ¨ç†èŒƒå¼ï¼Œæå‡æ¢ç´¢å¯è¡Œè§£ç©ºé—´çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTourPlanner åœ¨æ—…è¡Œè§„åˆ’åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06860",
            "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
            "url": "https://huggingface.co/papers/2601.06860",
            "abstract": "ET-Agent is a training framework that calibrates tool-use behavior in large language models through self-evolving data flywheels and behavior calibration training to improve task execution effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of  across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent",
            "score": 6,
            "issue_id": 544,
            "pub_date": "2026-01-11",
            "pub_date_card": {
                "ru": "11 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 11",
                "zh": "1æœˆ11æ—¥"
            },
            "hash": "600061ce305fd03f",
            "authors": [
                "Yifei Chen",
                "Guanting Dong",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06860.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "ĞšĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "ET-Agent â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Calibrating Tool-Use Behavior for Enhanced Task Execution in LLMs",
                    "desc": "ET-Agent is a novel training framework designed to enhance the tool-use behavior of large language models (LLMs) during task execution. It employs a Self-evolving Data Flywheel to generate improved training data, which helps fine-tune the model's exploration capabilities. Additionally, the framework incorporates Behavior Calibration Training to systematically correct and optimize the agent's behavioral patterns when performing Tool-Integrated Reasoning (TIR) tasks. Experimental results demonstrate that ET-Agent significantly improves the correctness, efficiency, and accuracy of tool execution in LLMs."
                },
                "zh": {
                    "title": "ET-Agentï¼šä¼˜åŒ–å·¥å…·ä½¿ç”¨è¡Œä¸ºçš„è®­ç»ƒæ¡†æ¶",
                    "desc": "ET-Agentæ˜¯ä¸€ä¸ªè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªæˆ‘æ¼”åŒ–çš„æ•°æ®é£è½®å’Œè¡Œä¸ºæ ¡å‡†è®­ç»ƒæ¥è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹çš„å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œä»è€Œæé«˜ä»»åŠ¡æ‰§è¡Œçš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å·¥å…·é›†æˆæ¨ç†ä»»åŠ¡ä¸­å¸¸è§çš„è¡Œä¸ºæ¨¡å¼ä¸å¯¹é½é—®é¢˜ï¼Œé¿å…äº†å†—ä½™å’Œä¸è¶³çš„å·¥å…·è°ƒç”¨ã€‚é€šè¿‡ç”Ÿæˆå¢å¼ºæ•°æ®å¹¶å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒET-Agentèƒ½å¤Ÿæœ‰æ•ˆæ¢ç´¢ä»»åŠ¡æ‰§è¡Œçš„æœ€ä½³è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ­£ç¡®æ€§ã€æ•ˆç‡ã€æ¨ç†ç®€æ´æ€§å’Œå·¥å…·æ‰§è¡Œå‡†ç¡®æ€§ç­‰å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06165",
            "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
            "url": "https://huggingface.co/papers/2601.06165",
            "abstract": "Real-world vision-language benchmarks reveal that under-specified user queries pose significant challenges for current models, with explicit query rewriting leading to substantial performance improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.",
            "score": 6,
            "issue_id": 544,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "11228a6956fd7028",
            "authors": [
                "Dasol Choi",
                "Guijin Son",
                "Hanwool Lee",
                "Minhyuk Kim",
                "Hyunwoo Ko",
                "Teabin Lim",
                "Ahn Eungyeol",
                "Jungwhan Kim",
                "Seunghyeok Hong",
                "Youngsook Song"
            ],
            "affiliations": [
                "AIM Intelligence",
                "Doodlin Corp.",
                "Hankuk University of Foreign Studies",
                "Korea University",
                "Lablup Inc.",
                "NAVER Cloud",
                "OneLineAI",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06165.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#low_resource",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½ĞµÑÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² â€” ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ HAERAE-Vision, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 653 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ², ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹. ĞŸÑ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ 39 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° Ğ¾Ğ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (GPT-5, Gemini 2.5 Pro) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 50% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ…. Ğ¯Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 8-22 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ° Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing VLMs with Clearer Queries",
                    "desc": "This paper addresses the challenges faced by vision-language models (VLMs) when dealing with real-world user queries that are often informal and underspecified. The authors introduce HAERAE-Vision, a new benchmark consisting of 653 visual questions sourced from Korean online communities, each accompanied by explicit rewrites to clarify the queries. Their evaluation of 39 VLMs reveals that even the most advanced models struggle with original queries, scoring below 50%. The study highlights that improving query clarity through explicit rewriting can significantly enhance model performance, particularly for smaller models, and underscores the gap between benchmark evaluations and practical applications in real-world scenarios."
                },
                "zh": {
                    "title": "æ˜ç¡®åŒ–æŸ¥è¯¢ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬ç ”ç©¶æ­ç¤ºäº†ç°å®ä¸–ç•Œä¸­è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤„ç†ä¸æ˜ç¡®ç”¨æˆ·æŸ¥è¯¢æ—¶é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†HAERAE-VisionåŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªéŸ©å›½åœ¨çº¿ç¤¾åŒºçš„653ä¸ªçœŸå®è§†è§‰é—®é¢˜ï¼Œå¹¶ä¸ºæ¯ä¸ªé—®é¢˜æä¾›äº†æ˜ç¡®çš„é‡å†™ç‰ˆæœ¬ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤„ç†åŸå§‹æŸ¥è¯¢æ—¶çš„è¡¨ç°ä¹Ÿä¸è¶³50%ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒæŸ¥è¯¢çš„æ˜ç¡®åŒ–å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¯¹è¾ƒå°çš„æ¨¡å‹å½±å“æ›´å¤§ï¼Œå¼ºè°ƒäº†åŸºå‡†è¯„ä¼°ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å…³é”®å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06496",
            "title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
            "url": "https://huggingface.co/papers/2601.06496",
            "abstract": "3D CoCa v2 enhances 3D captioning by combining contrastive vision-language learning with spatially-aware 3D scene encoding and test-time search for improved generalization across diverse environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.",
            "score": 0,
            "issue_id": 544,
            "pub_date": "2026-01-10",
            "pub_date_card": {
                "ru": "10 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 10",
                "zh": "1æœˆ10æ—¥"
            },
            "hash": "ee370f5e69428850",
            "authors": [
                "Hao Tang",
                "Ting Huang",
                "Zeyu Zhang"
            ],
            "affiliations": [
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06496.jpg",
            "data": {
                "categories": [
                    "#open_source"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°",
                    "desc": "3D CoCa v2 â€” ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼ 3D-ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… CLIP Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ Ñ†ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ¸ÑĞº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (TTS), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑĞ¼Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ±ĞµĞ· Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing 3D Captioning with Contrastive Learning and Spatial Awareness",
                    "desc": "3D CoCa v2 is a framework designed to improve 3D captioning by integrating contrastive vision-language learning with advanced 3D scene encoding. It addresses challenges like the irregularity of point clouds and the need for better generalization in diverse environments. The model utilizes a frozen CLIP-based semantic prior and a spatially-aware encoder to enhance understanding of 3D geometry. Additionally, it employs test-time search to generate diverse caption options and select the best one without altering the model's parameters, leading to significant performance gains in various benchmarks."
                },
                "zh": {
                    "title": "æå‡ä¸‰ç»´æè¿°ç”Ÿæˆçš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "3D CoCa v2 æ˜¯ä¸€ç§å¢å¼ºä¸‰ç»´æè¿°ç”Ÿæˆçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¯¹æ¯”è§†è§‰-è¯­è¨€å­¦ä¹ å’Œç©ºé—´æ„ŸçŸ¥çš„ä¸‰ç»´åœºæ™¯ç¼–ç ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ç°æœ‰ä¸‰ç»´æè¿°ç”Ÿæˆå™¨åœ¨ä¸åŒç¯å¢ƒä¸­çš„å¼±åŸºç¡€å’Œæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨æµ‹è¯•æ—¶æœç´¢ï¼ˆTTSï¼‰ï¼Œ3D CoCa v2 æé«˜äº†æè¿°çš„é²æ£’æ€§ï¼Œè€Œæ— éœ€æ›´æ–°æè¿°ç”Ÿæˆå™¨çš„å‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3D CoCa v2 åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æè¿°ç”Ÿæˆçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03666",
            "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
            "url": "https://huggingface.co/papers/2601.03666",
            "abstract": "Omni-modal embedding models face challenges with modality-dependent similarity scaling, ineffective in-batch negatives, and mismatched statistics across modalities, which are addressed through explicit alignment techniques including temperature calibration, controlled negative curriculum, and batch whitening with covariance regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.",
            "score": 0,
            "issue_id": 544,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "c6af8e5fb12ea40a",
            "authors": [
                "Haonan Chen",
                "Sicheng Gao",
                "Radu Timofte",
                "Tetsuya Sakai",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "University of WÃ¼rzburg",
                "Waseda University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03666.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#multimodal",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸŒˆ",
                "ru": {
                    "title": "Ğ¯Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ e5-omni Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°ÑƒĞ´Ğ¸Ğ¾) Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ½ĞµĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑˆĞºĞ°Ğ»Ğ° ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ±Ğ°Ñ‚Ñ‡Ğµ Ğ¸ Ğ½ĞµÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ curriculum Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ°Ñ‚Ñ‡-Ğ¾Ñ‚Ğ±ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MMEB-V2 Ğ¸ AudioCaps Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Aligning Modalities for Better Embeddings",
                    "desc": "This paper addresses the challenges faced by omni-modal embedding models, which integrate different types of data like text, images, and audio into a unified space. The authors identify three main issues: inconsistent similarity scaling across modalities, ineffective negative samples in mixed-modality batches, and mismatched statistical properties of embeddings. To resolve these, they introduce e5-omni, which employs temperature calibration for similarity alignment, a controlled negative curriculum to enhance learning from difficult examples, and batch whitening to harmonize the statistical properties of embeddings. Their experiments demonstrate that e5-omni outperforms existing models, showcasing its effectiveness in creating robust omni-modal embeddings."
                },
                "zh": {
                    "title": "å…¨æ¨¡æ€åµŒå…¥çš„æ˜¾å¼å¯¹é½æŠ€æœ¯",
                    "desc": "æœ¬æ–‡è®¨è®ºäº†å…¨æ¨¡æ€åµŒå…¥æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡æ€ä¾èµ–çš„ç›¸ä¼¼æ€§ç¼©æ”¾ã€æ— æ•ˆçš„æ‰¹å†…è´Ÿæ ·æœ¬å’Œæ¨¡æ€é—´ç»Ÿè®¡ä¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºe5-omniçš„æ˜¾å¼å¯¹é½æŠ€æœ¯ï¼Œç»“åˆäº†æ¸©åº¦æ ¡å‡†ã€å¯æ§è´Ÿæ ·æœ¬è¯¾ç¨‹å’Œæ‰¹æ¬¡ç™½åŒ–ç­‰æ–¹æ³•ã€‚é€šè¿‡è¿™äº›æŠ€æœ¯ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½ä¸åŒæ¨¡æ€çš„ç›¸ä¼¼æ€§å°ºåº¦ï¼Œæé«˜è´Ÿæ ·æœ¬çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ”¹å–„è·¨æ¨¡æ€å‡ ä½•åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œe5-omniåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä¸”èƒ½å¤Ÿæœ‰æ•ˆè¿ç§»åˆ°å…¶ä»–è§†è§‰è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-12.html",
    "link_next": "2026-01-14.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "12.01",
        "en": "01/12",
        "zh": "1æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "14.01",
        "en": "01/14",
        "zh": "1æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}