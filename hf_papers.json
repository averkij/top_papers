{
    "date": {
        "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 6",
        "zh": "11æœˆ6æ—¥"
    },
    "time_utc": "2024-11-06 06:46",
    "weekday": 2,
    "issue_id": 441,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.02959",
            "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
            "url": "https://huggingface.co/papers/2411.02959",
            "abstract": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems.",
            "score": 17,
            "issue_id": 437,
            "pub_date": "2024-11-05",
            "pub_date_card": {
                "ru": "5 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 5",
                "zh": "11æœˆ5æ—¥"
            },
            "hash": "6fb8684374e5fdcb",
            "data": {
                "categories": [
                    "#rag",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "HtmlRAG: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG), Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ HtmlRAG. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ RAG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚, HtmlRAG ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ HTML-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸, ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ HTML Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ HTML Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… RAG."
                },
                "en": {
                    "title": "Harnessing HTML for Enhanced Knowledge Retrieval in RAG Systems",
                    "desc": "This paper introduces HtmlRAG, a novel approach to Retrieval-Augmented Generation (RAG) that utilizes HTML instead of plain text for knowledge retrieval. By leveraging the structural and semantic information present in HTML, HtmlRAG aims to enhance the performance of large language models (LLMs) and reduce the hallucination problem. The authors address the challenges posed by HTML, such as excess tokens and noise, by implementing cleaning, compression, and pruning techniques to streamline the input. Experimental results demonstrate that HtmlRAG outperforms traditional plain-text-based RAG systems across multiple question-answering datasets."
                },
                "zh": {
                    "title": "ç”¨HTMLæå‡æ£€ç´¢å¢å¼ºç”Ÿæˆçš„èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œç§°ä¸ºHtmlRAGï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„çŸ¥è¯†èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰é—®é¢˜ã€‚HtmlRAGä½¿ç”¨HTMLæ ¼å¼è€Œéçº¯æ–‡æœ¬æ¥å¢å¼ºç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œä¿ç•™æ›´å¤šçš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹HTMLä¸­å¤šä½™å†…å®¹å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†HTMLæ¸…ç†ã€å‹ç¼©å’Œä¿®å‰ªç­–ç•¥ï¼Œä»¥å‡å°‘è¾“å…¥çš„å†—ä½™ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHtmlRAGåœ¨å…­ä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„çº¯æ–‡æœ¬RAGç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00871",
            "title": "LLaMo: Large Language Model-based Molecular Graph Assistant",
            "url": "https://huggingface.co/papers/2411.00871",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization and instruction-following capabilities with instruction tuning. The advancements in LLMs and instruction tuning have led to the development of Large Vision-Language Models (LVLMs). However, the competency of the LLMs and instruction tuning have been less explored in the molecular domain. Thus, we propose LLaMo: Large Language Model-based Molecular graph assistant, which is an end-to-end trained large molecular graph-language model. To bridge the discrepancy between the language and graph modalities, we present the multi-level graph projector that transforms graph representations into graph tokens by abstracting the output representations of each GNN layer and motif representations with the cross-attention mechanism. We also introduce machine-generated molecular graph instruction data to instruction-tune the large molecular graph-language model for general-purpose molecule and language understanding. Our extensive experiments demonstrate that LLaMo shows the best performance on diverse tasks, such as molecular description generation, property prediction, and IUPAC name prediction. The code of LLaMo is available at https://github.com/mlvlab/LLaMo.",
            "score": 12,
            "issue_id": 439,
            "pub_date": "2024-10-31",
            "pub_date_card": {
                "ru": "31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 31",
                "zh": "10æœˆ31æ—¥"
            },
            "hash": "d1284691dab4e739",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#agents",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "LLaMo: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ LLaMo - Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. LLaMo Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¼, Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ ÑĞ·Ñ‹ĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLaMo Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ IUPAC."
                },
                "en": {
                    "title": "Bridging Language and Molecules with LLaMo",
                    "desc": "This paper introduces LLaMo, a Large Language Model-based Molecular graph assistant designed to enhance understanding in the molecular domain. It utilizes a multi-level graph projector to convert molecular graph representations into tokens, facilitating better interaction between language and graph data. The model is instruction-tuned using machine-generated molecular graph instruction data, enabling it to perform various tasks like molecular description generation and property prediction. Experimental results show that LLaMo outperforms existing models in these tasks, highlighting its effectiveness in bridging language and molecular graph understanding."
                },
                "zh": {
                    "title": "LLaMoï¼šè¿æ¥è¯­è¨€ä¸åˆ†å­çš„æ¡¥æ¢",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŒ‡ä»¤è°ƒä¼˜æ–¹é¢å±•ç°äº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†LLaMoï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†å­å›¾åŠ©æ‰‹ï¼Œæ—¨åœ¨å¡«è¡¥è¯­è¨€å’Œå›¾å½¢æ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šå±‚å›¾æŠ•å½±å™¨ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶å°†å›¾è¡¨ç¤ºè½¬æ¢ä¸ºå›¾æ ‡è®°ï¼Œå¹¶ä½¿ç”¨æœºå™¨ç”Ÿæˆçš„åˆ†å­å›¾æŒ‡ä»¤æ•°æ®å¯¹æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaMoåœ¨åˆ†å­æè¿°ç”Ÿæˆã€å±æ€§é¢„æµ‹å’ŒIUPACåç§°é¢„æµ‹ç­‰å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02359",
            "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
            "url": "https://huggingface.co/papers/2411.02359",
            "abstract": "MLLMs have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we propose a Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (i.e., power consumption), as well as peak computational consumption (i.e., latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.",
            "score": 7,
            "issue_id": 437,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 4",
                "zh": "11æœˆ4æ—¥"
            },
            "hash": "08c45469caff5fa0",
            "data": {
                "categories": [
                    "#agents",
                    "#inference",
                    "#robots",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ñ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dynamic Early-Exit Framework Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (DeeR-VLA). Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. DeeR Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ CALVIN ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient Robotic Intelligence with Dynamic Early-Exit MLLMs",
                    "desc": "This paper introduces a new framework called DeeR for improving the efficiency of robotic vision-language-action models (MLLMs). DeeR uses a Dynamic Early-Exit approach that allows the model to adjust its size based on the specific task, reducing unnecessary computations. By implementing a multi-exit architecture, the model can stop processing once it has enough information, which helps save power and memory. The results show that DeeR can significantly lower computational costs and memory usage while still performing well on tasks, making it suitable for real-world robotic applications."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´ï¼Œæ™ºèƒ½æœºå™¨äººæ›´é«˜æ•ˆï¼",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€æ—©æœŸé€€å‡ºæ¡†æ¶ï¼ˆDeeR-VLAï¼‰ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å®é™…åº”ç”¨ä¸­çš„è®¡ç®—å’Œå†…å­˜é™åˆ¶é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šå‡ºå£æ¶æ„ï¼Œèƒ½å¤Ÿæ ¹æ®å…·ä½“æƒ…å†µè‡ªåŠ¨è°ƒæ•´æ¿€æ´»çš„æ¨¡å‹å¤§å°ï¼Œä»è€Œé¿å…å†—ä½™è®¡ç®—ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†æ–°ç®—æ³•ï¼Œè®¾å®šæ—©æœŸç»ˆæ­¢æ ‡å‡†ï¼Œä»¥æ»¡è¶³é¢„å®šä¹‰çš„è®¡ç®—éœ€æ±‚ï¼Œå¦‚åŠŸè€—å’Œå»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeeRåœ¨CALVINæœºå™¨äººæ“ä½œåŸºå‡†ä¸Šæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬å’ŒGPUå†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.01493",
            "title": "Sample-Efficient Alignment for LLMs",
            "url": "https://huggingface.co/papers/2411.01493",
            "abstract": "We study methods for efficiently aligning large language models (LLMs) with human preferences given budgeted online feedback. We first formulate the LLM alignment problem in the frame of contextual dueling bandits. This formulation, subsuming recent paradigms such as online RLHF and online DPO, inherently quests for sample-efficient algorithms that incorporate online active exploration. Leveraging insights from bandit theory, we introduce a unified algorithm based on Thompson sampling and highlight its applications in two distinct LLM alignment scenarios. The practical agent that efficiently implements this algorithm, named SEA (Sample-Efficient Alignment), is empirically validated through extensive experiments across three model scales (1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The results demonstrate that SEA achieves highly sample-efficient alignment with oracle's preferences, outperforming recent active exploration methods for LLMs. Additionally, we release the implementation of SEA together with an efficient codebase designed for online alignment of LLMs, aiming to accelerate future research in this field.",
            "score": 5,
            "issue_id": 440,
            "pub_date": "2024-11-03",
            "pub_date_card": {
                "ru": "3 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 3",
                "zh": "11æœˆ3æ—¥"
            },
            "hash": "c5bb9727a6ba6119",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#agents"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ´ÑƒÑĞ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ½Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ´ÑƒÑĞ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¢Ğ¾Ğ¼Ğ¿ÑĞ¾Ğ½Ğ°. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ SEA (Sample-Efficient Alignment), Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ¾Ñ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ€Ğ°ĞºÑƒĞ»Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ SEA Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Efficiently Aligning LLMs with Human Preferences Using SEA",
                    "desc": "This paper explores how to align large language models (LLMs) with human preferences using limited online feedback. It frames the alignment challenge as a contextual dueling bandits problem, which seeks efficient algorithms that can learn from active exploration. The authors propose a new algorithm called SEA (Sample-Efficient Alignment) based on Thompson sampling, which is tested across various model sizes and preference learning methods. The results show that SEA is highly effective in aligning LLMs with human preferences while using fewer samples compared to existing methods."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åœ¨æœ‰é™çš„åœ¨çº¿åé¦ˆä¸‹é«˜æ•ˆåœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½ã€‚æˆ‘ä»¬å°†LLMå¯¹é½é—®é¢˜æ¡†å®šä¸ºä¸Šä¸‹æ–‡å¯¹æŠ—èµŒåšè€…é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ±¤æ™®æ£®é‡‡æ ·çš„ç»Ÿä¸€ç®—æ³•ï¼Œå¹¶åœ¨ä¸¤ä¸ªä¸åŒçš„LLMå¯¹é½åœºæ™¯ä¸­è¿›è¡Œäº†åº”ç”¨ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼Œåä¸ºSEAï¼ˆæ ·æœ¬é«˜æ•ˆå¯¹é½ï¼‰çš„å®ç”¨ä»£ç†åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹å’Œåå¥½å­¦ä¹ ç®—æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¯¹é½æ–¹é¢çš„é«˜æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†SEAçš„å®ç°å’Œé«˜æ•ˆä»£ç åº“ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.01602",
            "title": "DreamPolish: Domain Score Distillation With Progressive Geometry Generation",
            "url": "https://huggingface.co/papers/2411.01602",
            "abstract": "We introduce DreamPolish, a text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on a view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add a surface polishing stage with only a few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find a suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce a novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such a domain. We draw inspiration from the classifier-free guidance (CFG) in textconditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods.",
            "score": 1,
            "issue_id": 439,
            "pub_date": "2024-11-03",
            "pub_date_card": {
                "ru": "3 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 3",
                "zh": "11æœˆ3æ—¥"
            },
            "hash": "403fd08e60540a0a",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "DreamPolish: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸",
                    "desc": "DreamPolish - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑ‚Ğ¾Ğ½Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğµ (DSD), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-image. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DreamPolish ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ñ Ğ¾Ñ‚Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "DreamPolish: Elevating 3D Generation with Refined Geometry and Textures",
                    "desc": "DreamPolish is a text-to-3D generation model that focuses on creating high-quality 3D objects with refined geometry and textures. It improves the geometry construction by using multiple neural representations and an additional normal estimator to reduce artifacts caused by limited guidance. The model also introduces a surface polishing stage that requires minimal training to enhance the geometric details further. For texture generation, it employs a novel domain score distillation (DSD) method, inspired by classifier-free guidance, to achieve photorealistic and consistent textures in the generated 3D assets."
                },
                "zh": {
                    "title": "DreamPolishï¼šç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDreamPolishçš„æ–‡æœ¬åˆ°3Dç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆç²¾ç»†çš„å‡ ä½•å½¢çŠ¶å’Œé«˜è´¨é‡çš„çº¹ç†ã€‚åœ¨å‡ ä½•æ„å»ºé˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šç§ç¥ç»è¡¨ç¤ºæ¥å¢å¼ºåˆæˆè¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œå¹¶å¼•å…¥é¢å¤–çš„æ³•çº¿ä¼°è®¡å™¨æ¥æ”¹å–„å‡ ä½•ç»†èŠ‚ã€‚çº¹ç†ç”Ÿæˆé˜¶æ®µé‡‡ç”¨äº†ä¸€ç§æ–°çš„è¯„åˆ†è’¸é¦ç›®æ ‡ï¼Œç§°ä¸ºé¢†åŸŸè¯„åˆ†è’¸é¦ï¼ˆDSDï¼‰ï¼Œä»¥å¼•å¯¼ç¥ç»è¡¨ç¤ºæœå‘åŒ…å«çœŸå®æ„Ÿå’Œä¸€è‡´æ€§æ¸²æŸ“çš„é€‚å½“é¢†åŸŸã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¡¨é¢å…‰æ»‘ä¸”å…·æœ‰çœŸå®æ„Ÿçº¹ç†çš„3Dèµ„äº§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02657",
            "title": "Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge",
            "url": "https://huggingface.co/papers/2411.02657",
            "abstract": "Rare diseases present unique challenges in healthcare, often suffering from delayed diagnosis and fragmented information landscapes. The scarcity of reliable knowledge in these conditions poses a distinct challenge for Large Language Models (LLMs) in supporting clinical management and delivering precise patient information underscoring the need for focused training on these 'zebra' cases. We present Zebra-Llama, a specialized context-aware language model with high precision Retrieval Augmented Generation (RAG) capability, focusing on Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000 individuals, exemplifies the complexities of rare diseases with its diverse symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing a novel context-aware fine-tuning methodology trained on questions derived from medical literature, patient experiences, and clinical resources, along with expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities in handling EDS-related queries. On a test set of real-world questions collected from EDS patients and clinicians, medical experts evaluated the responses generated by both models, revealing Zebra-Llama's substantial improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama not only provides more accessible and reliable EDS information but also establishes a framework for developing specialized AI solutions for other rare conditions. This work represents a crucial step towards democratizing expert-level knowledge in rare disease management, potentially transforming how healthcare providers and patients navigate the complex landscape of rare diseases.",
            "score": 1,
            "issue_id": 439,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 4",
                "zh": "11æœˆ4æ—¥"
            },
            "hash": "26c0b7bc39488945",
            "data": {
                "categories": [
                    "#rag",
                    "#medicine",
                    "#training",
                    "#alignment"
                ],
                "emoji": "ğŸ¦“",
                "ru": {
                    "title": "Zebra-Llama: Ğ˜Ğ˜-ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ñ€ĞµĞ´ĞºĞ¸Ğ¼ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Zebra-Llama - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ğ´Ñ€Ğ¾Ğ¼ Ğ­Ğ»ĞµÑ€ÑĞ°-Ğ”Ğ°Ğ½Ğ»Ğ¾ÑĞ° (EDS) ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ EDS. Zebra-Llama Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğµ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹. Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Zebra-Llama: Transforming Rare Disease Management with AI",
                    "desc": "This paper introduces Zebra-Llama, a specialized language model designed to improve the management of rare diseases, specifically Ehlers-Danlos Syndrome (EDS). It addresses the challenges posed by limited information and delayed diagnoses in rare conditions by utilizing a context-aware fine-tuning approach. The model employs Retrieval Augmented Generation (RAG) to enhance the precision of responses to EDS-related queries, outperforming the base model in various evaluation metrics. By making Zebra-Llama an open-source resource, the authors aim to democratize access to expert knowledge in rare disease management, paving the way for similar advancements in other rare conditions."
                },
                "zh": {
                    "title": "Zebra-Llamaï¼šç½•è§ç–¾ç—…ç®¡ç†çš„æ–°çªç ´",
                    "desc": "ç½•è§ç–¾ç—…åœ¨åŒ»ç–—ä¿å¥ä¸­é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¸¸å¸¸å¯¼è‡´è¯Šæ–­å»¶è¿Ÿå’Œä¿¡æ¯ç¢ç‰‡åŒ–ã€‚é’ˆå¯¹è¿™äº›ç½•è§ç—…ä¾‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºZebra-Llamaçš„ä¸“é—¨è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡é«˜ç²¾åº¦çš„æ£€ç´¢å¢å¼ºç”Ÿæˆèƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨Ehlers-Danlosç»¼åˆç—‡ï¼ˆEDSï¼‰ã€‚é€šè¿‡ä¸€ç§æ–°é¢–çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¾®è°ƒæ–¹æ³•ï¼ŒZebra-Llamaåœ¨å¤„ç†ä¸EDSç›¸å…³çš„é—®é¢˜æ—¶è¡¨ç°å‡ºå‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†å›ç­”çš„å…¨é¢æ€§ã€å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦ã€‚è¯¥æ¨¡å‹ä½œä¸ºå¼€æºèµ„æºå‘å¸ƒï¼Œä¸ä»…æä¾›äº†æ›´æ˜“è·å–å’Œå¯é çš„EDSä¿¡æ¯ï¼Œè¿˜ä¸ºå…¶ä»–ç½•è§ç–¾ç—…å¼€å‘ä¸“é—¨çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆå¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-05.html",
    "link_next": "2024-11-07.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "05.11",
        "en": "11/05",
        "zh": "11æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "07.11",
        "en": "11/07",
        "zh": "11æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 3,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#medicine": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0,
        "#robots": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è‡ªä¸»ä»£ç†åœ¨ç°å®ä¸–ç•Œäº¤äº’ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯Androidä»£ç†ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹å¼€æºå’Œé—­æºæ¨¡å‹çš„ç³»ç»Ÿç ”ç©¶ã€‚ä½œè€…æå‡ºäº†AndroidLabï¼Œä¸€ä¸ªç³»ç»Ÿçš„Androidä»£ç†æ¡†æ¶ï¼ŒåŒ…å«å¤šç§æ¨¡æ€çš„æ“ä½œç¯å¢ƒå’Œå¯é‡å¤çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨AndroidLabç¯å¢ƒï¼Œä½œè€…å¼€å‘äº†ä¸€ä¸ªAndroidæŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†å…­ä¸ªå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡ã€‚AndroidLabæ˜¯å¼€æºçš„ï¼Œå¯åœ¨GitHubä¸Šè·å–ã€‚",
        "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le zÃ¬zhÇ” dÃ ilÇ zÃ i xiÃ nshÃ­ shÃ¬jiÃ¨ jiÄohÃ¹ zhÅng de zhÃ²ngyÃ oxÃ¬ng, tÃ¨biÃ© shÃ¬ Android dÃ ilÇ. WÃ©nzhÄng zhÇchÅ«, xiÃ nyÇ’u yÃ¡njiÅ« quÄ“fÃ¡ duÃ¬ kÄiyuÃ¡n hÃ© bÃ¬yuÃ¡n mÃ³xÃ­ng de xÃ¬tÇ’ng yÃ¡njiÅ«. ZuÃ²zhÄ› tÃ­chÅ« le AndroidLab, yÄ«gÃ¨ xÃ¬tÇ’ng de Android dÃ ilÇ kuÃ ngjiÃ , bÄohÃ¡n duÅzhÇ’ng mÃ³tÃ i de cÄozuÃ² huÃ¡njÃ¬ng hÃ© kÄ› chÃ³ngfÃ¹ de jÄ«zhÇ”n cÃ¨shÃ¬. GÄi kuÃ ngjiÃ  zhÄ«chÃ­ dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng hÃ© duÅ mÃ³tÃ i mÃ³xÃ­ng. TÅngguÃ² shÇyÃ²ng AndroidLab huÃ¡njÃ¬ng, zuÃ²zhÄ› kÄifÄ le yÄ«gÃ¨ Android zhÇlÇng shÃ¹jÃ¹jÃ­, bÃ¬ng xÃ¹nliÃ n le liÃ¹ gÃ¨ kÄiyuÃ¡n de dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng hÃ© duÅ mÃ³tÃ i mÃ³xÃ­ng, xiÇnzhÃ¹ tÃ­gÄo le rÃ¨nwÃ¹ chÃ©nggÅnglÇœ. AndroidLab shÃ¬ kÄiyuÃ¡n de, kÄ› zÃ i GitHub shÃ ng huÃ²qÇ”.",
        "vocab": "[\n    {\"word\": \"è‡ªä¸»ä»£ç†\", \"pinyin\": \"zÃ¬zhÇ” dÃ ilÇ\", \"trans\": \"autonomous agent\"},\n    {\"word\": \"ç°å®ä¸–ç•Œ\", \"pinyin\": \"xiÃ nshÃ­ shÃ¬jiÃ¨\", \"trans\": \"real world\"},\n    {\"word\": \"äº¤äº’\", \"pinyin\": \"jiÄohÃ¹\", \"trans\": \"interaction\"},\n    {\"word\": \"Androidä»£ç†\", \"pinyin\": \"Android dÃ ilÇ\", \"trans\": \"Android agent\"},\n    {\"word\": \"ç°æœ‰ç ”ç©¶\", \"pinyin\": \"xiÃ nyÇ’u yÃ¡njiÅ«\", \"trans\": \"existing research\"},\n    {\"word\": \"ç¼ºä¹\", \"pinyin\": \"quÄ“fÃ¡\", \"trans\": \"lack\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄiyuÃ¡n\", \"trans\": \"open source\"},\n    {\"word\": \"é—­æº\", \"pinyin\": \"bÃ¬yuÃ¡n\", \"trans\": \"closed source\"},\n    {\"word\": \"ç³»ç»Ÿç ”ç©¶\", \"pinyin\": \"xÃ¬tÇ’ng yÃ¡njiÅ«\", \"trans\": \"systematic study\"},\n    {\"word\": \"AndroidLab\", \"pinyin\": \"AndroidLab\", \"trans\": \"AndroidLab\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"å¤šç§æ¨¡æ€\", \"pinyin\": \"duÅzhÇ’ng mÃ³shÃ¬\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ“ä½œç¯å¢ƒ\", \"pinyin\": \"cÄozuÃ² huÃ¡njÃ¬ng\", \"trans\": \"operating environment\"},\n    {\"word\": \"å¯é‡å¤\", \"pinyin\": \"kÄ› chÃ³ngfÃ¹\", \"trans\": \"reproducible\"},\n    {\"word\": \"åŸºå‡†æµ‹è¯•\", \"pinyin\": \"jÄ«zhÇ”n cÃ¨shÃ¬\", \"trans\": \"benchmark test\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ«chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"å¤§å‹è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"å¤šæ¨¡æ€æ¨¡å‹\", \"pinyin\": \"duÅ mÃ³shÃ¬ mÃ³xÃ­ng\", \"trans\": \"multimodal model\"},\n    {\"word\": \"ä½¿ç”¨\", \"pinyin\": \"shÇyÃ²ng\", \"trans\": \"use\"},\n    {\"word\": \"ç¯å¢ƒ\", \"pinyin\": \"huÃ¡njÃ¬ng\", \"trans\": \"environment\"},\n    {\"word\": \"å¼€å‘\", \"pinyin\": \"kÄifÄ\", \"trans\": \"develop\"},\n    {\"word\": \"AndroidæŒ‡ä»¤æ•°æ®é›†\", \"pinyin\": \"Android zhÇlÃ¬ng shÃ¹jÃ¹jÃ­\", \"trans\": \"Android command dataset\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹nliÃ n\", \"trans\": \"train\"},\n    {\"word\": \"å…­ä¸ª\", \"pinyin\": \"liÃ¹ gÃ¨\", \"trans\": \"six\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇnzhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"ä»»åŠ¡æˆåŠŸç‡\", \"pinyin\": \"rÃ¨nwÃ¹ chÃ©nggÅnglÇœ\", \"trans\": \"task success rate\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]",
        "trans": "This article discusses the importance of autonomous agents in real-world interactions, particularly Android agents. The article notes that existing research lacks systematic studies on open-source and closed-source models. The authors introduce AndroidLab, a systematic Android agent framework that includes multi-modal operation environments and reproducible benchmark tests. This framework supports large language models and multi-modal models. By using the AndroidLab environment, the authors developed an Android instruction dataset and trained six open-source large language models and multi-modal models, significantly improving task success rates. AndroidLab is open-source and available on GitHub.",
        "update_ts": "2024-11-05 10:13"
    }
}