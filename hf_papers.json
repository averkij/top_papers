{
    "date": {
        "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 18",
        "zh": "9æœˆ18æ—¥"
    },
    "time_utc": "2025-09-18 04:13",
    "weekday": 3,
    "issue_id": 5954,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.14232",
            "title": "GenExam: A Multidisciplinary Text-to-Image Exam",
            "url": "https://huggingface.co/papers/2509.14232",
            "abstract": "GenExam is a benchmark for evaluating text-to-image generation in exam-style settings across multiple disciplines, highlighting the challenges in integrating knowledge, reasoning, and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models' ability to integrate knowledge, reasoning, and generation, providing insights on the path to general AGI.",
            "score": 7,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "40b921aaa9b5c031",
            "authors": [
                "Zhaokai Wang",
                "Penghao Yin",
                "Xiangyu Zhao",
                "Changyao Tian",
                "Yu Qiao",
                "Wenhai Wang",
                "Jifeng Dai",
                "Gen Luo"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14232.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agi",
                    "#reasoning",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "GenExam: Ğ­ĞºĞ·Ğ°Ğ¼ĞµĞ½ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "GenExam - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ² ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ğ¼. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1000 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¿Ğ¾ 10 Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼ Ñ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ½Ğ°Ğ±Ğ¶ĞµĞ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-Image-1 Ğ¸ Gemini-2.5-Flash-Image, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 15% ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ°."
                },
                "en": {
                    "title": "GenExam: A New Standard for Text-to-Image Generation Challenges",
                    "desc": "GenExam is a new benchmark designed to evaluate how well AI can generate images based on text prompts in an exam-like format across various subjects. It addresses the gap in existing benchmarks that focus mainly on understanding and reasoning, by including rigorous drawing tasks that require both knowledge and creativity. The benchmark consists of 1,000 samples organized into a four-level taxonomy, each with ground-truth images and detailed scoring criteria for accurate assessment. Results show that even advanced models struggle significantly, achieving less than 15% accuracy, highlighting the complexity of integrating knowledge, reasoning, and image generation in AI."
                },
                "zh": {
                    "title": "GenExamï¼šå¤šå­¦ç§‘æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è€ƒè¯•åŸºå‡†",
                    "desc": "GenExamæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨è€ƒè¯•é£æ ¼çš„è®¾ç½®ä¸­ï¼Œæ¶µç›–å¤šä¸ªå­¦ç§‘ã€‚è¯¥åŸºå‡†å¼ºè°ƒäº†åœ¨çŸ¥è¯†æ•´åˆã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢çš„æŒ‘æˆ˜ã€‚GenExamåŒ…å«1000ä¸ªæ ·æœ¬ï¼Œåˆ†ä¸º10ä¸ªå­¦ç§‘ï¼Œé‡‡ç”¨å››çº§åˆ†ç±»æ³•ç»„ç»‡è€ƒè¯•æç¤ºï¼Œå¹¶é…æœ‰çœŸå®å›¾åƒå’Œç»†è‡´çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥ä¾¿ç²¾ç¡®è¯„ä¼°è¯­ä¹‰æ­£ç¡®æ€§å’Œè§†è§‰å¯ä¿¡åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿéš¾ä»¥åœ¨è¯¥åŸºå‡†ä¸Šå–å¾—é«˜åˆ†ï¼Œæ˜¾ç¤ºå‡ºè¿™ä¸€é¢†åŸŸçš„å·¨å¤§æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14033",
            "title": "SAIL-VL2 Technical Report",
            "url": "https://huggingface.co/papers/2509.14033",
            "abstract": "SAIL-VL2, a vision-language foundation model, achieves state-of-the-art performance across diverse benchmarks through data curation, progressive training, and sparse MoE architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Three core innovations drive its effectiveness. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community.",
            "score": 4,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "e11196999afc4056",
            "authors": [
                "Weijie Yin",
                "Yongjie Ye",
                "Fangxun Shu",
                "Yue Liao",
                "Zijian Kang",
                "Hongyuan Dong",
                "Haiyang Yu",
                "Dingkang Yang",
                "Jiacong Wang",
                "Han Wang",
                "Wenzhuo Liu",
                "Xiao Liang",
                "Shuicheng Yan",
                "Chao Feng"
            ],
            "affiliations": [
                "Douyin SAIL Team, LV-NUS Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14033.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#agi",
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#open_source",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SAIL-VL2: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "SAIL-VL2 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (Mixture-of-Experts). SAIL-VL2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 106 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "SAIL-VL2: Pioneering Multimodal Understanding with Efficiency and Precision",
                    "desc": "SAIL-VL2 is a cutting-edge vision-language foundation model designed for advanced multimodal understanding and reasoning. It utilizes a large-scale data curation process to enhance the quality of training data, which improves the model's performance across various tasks like captioning and question answering. The model employs a progressive training approach that starts with a pre-trained vision encoder and evolves through multimodal training to a hybrid fine-tuning method, enhancing its reasoning capabilities. Additionally, SAIL-VL2 incorporates a sparse Mixture-of-Experts architecture, allowing it to achieve state-of-the-art results on numerous benchmarks while remaining efficient and scalable for the open-source community."
                },
                "zh": {
                    "title": "SAIL-VL2ï¼šè§†è§‰ä¸è¯­è¨€çš„å®Œç¾ç»“åˆ",
                    "desc": "SAIL-VL2æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å¤§è§„æ¨¡æ•°æ®æ•´ç†ã€æ¸è¿›å¼è®­ç»ƒå’Œç¨€ç–ä¸“å®¶æ··åˆæ¶æ„ç­‰åˆ›æ–°æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿè¿›è¡Œç»†è‡´çš„æ„ŸçŸ¥å’Œå¤æ‚çš„æ¨ç†ã€‚SAIL-VL2åœ¨106ä¸ªæ•°æ®é›†ä¸Šå±•ç°äº†ç«äº‰åŠ›ï¼Œå¹¶åœ¨ä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æˆç»©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13761",
            "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning",
            "url": "https://huggingface.co/papers/2509.13761",
            "abstract": "THOR, a tool-integrated hierarchical optimization framework using RL, enhances mathematical reasoning and code generation by constructing high-quality datasets, optimizing reasoning paths, and correcting errors during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR.",
            "score": 3,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "d7bf12df5bb3d467",
            "authors": [
                "Qikai Chang",
                "Zhenrong Zhang",
                "Pengfei Hu",
                "Jiefeng Ma",
                "Yicheng Pan",
                "Jianshu Zhang",
                "Jun Du",
                "Quan Liu",
                "Jianqing Gao"
            ],
            "affiliations": [
                "University of Science and Technology of China",
                "iFLYTEK Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13761.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#math",
                    "#rl",
                    "#dataset",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "THOR: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "THOR - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. THOR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "THOR: Optimizing Reasoning with Reinforcement Learning and Tool Integration",
                    "desc": "THOR is a novel framework that enhances mathematical reasoning and code generation by integrating reinforcement learning (RL) with external tools. It addresses challenges in constructing high-quality datasets and optimizing reasoning paths through a multi-agent actor-critic approach called TIRGen. Additionally, THOR employs a hierarchical optimization strategy that focuses on both problem-solving and code generation, leveraging the success of tool calls to predict the correctness of answers. The framework also features a self-correction mechanism that allows it to dynamically adjust reasoning paths based on immediate feedback, leading to improved performance across various mathematical and coding tasks."
                },
                "zh": {
                    "title": "THORï¼šæå‡æ•°å­¦æ¨ç†ä¸ä»£ç ç”Ÿæˆçš„æ™ºèƒ½å·¥å…·",
                    "desc": "THORæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„å·¥å…·é›†æˆå±‚æ¬¡ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆçš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡æ„å»ºé«˜è´¨é‡çš„æ•°æ®é›†ã€ä¼˜åŒ–æ¨ç†è·¯å¾„å’Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­çº æ­£é”™è¯¯æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚THORå¼•å…¥äº†å¤šæ™ºèƒ½ä½“çš„æ¼”å‘˜-è¯„è®ºå®¶ç®¡é“TIRGenï¼Œä»¥ç”Ÿæˆå·¥å…·é›†æˆæ¨ç†è·¯å¾„çš„æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œç»†ç²’åº¦çš„å±‚æ¬¡ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶è¿˜ç»“åˆäº†è‡ªæˆ‘çº æ­£æœºåˆ¶ï¼Œåˆ©ç”¨å³æ—¶å·¥å…·åé¦ˆåŠ¨æ€ä¿®æ­£æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå±•ç°å‡ºåœ¨å¤šç§æ¨¡å‹ä¸Šçš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14142",
            "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook",
            "url": "https://huggingface.co/papers/2509.14142",
            "abstract": "The MARS2 2025 Challenge focuses on multimodal reasoning with large language models through real-world and specialized scenarios, evaluating 40+ models across three competition tracks using tailored datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this year's MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants' methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page https://github.com/mars2workshop/, where our updates and announcements of upcoming events will be continuously provided.",
            "score": 0,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "00749969a87efa2f",
            "authors": [
                "Peng Xu",
                "Shengwu Xiong",
                "Jiajun Zhang",
                "Yaxiong Chen",
                "Bowen Zhou",
                "Chen Change Loy",
                "David A. Clifton",
                "Kyoung Mu Lee",
                "Luc Van Gool",
                "Ruiming He",
                "Ruilin Yao",
                "Xinwei Long",
                "Jirui Huang",
                "Kai Tian",
                "Sa Yang",
                "Yihua Shao",
                "Jin Feng",
                "Yue Zhong",
                "Jiakai Zhou",
                "Cheng Tang",
                "Tianyu Zou",
                "Yifang Zhang",
                "Junming Liang",
                "Guoyou Li",
                "Zhaoxiang Wang",
                "Qiang Zhou",
                "Yichen Zhao",
                "Shili Xiong",
                "Hyeongjin Nam",
                "Jaerin Lee",
                "Jaeyoung Chung",
                "JoonKyu Park",
                "Junghun Oh",
                "Kanggeon Lee",
                "Wooseok Lee",
                "Juneyoung Ro",
                "Turghun Osman",
                "Can Hu",
                "Chaoyang Liao",
                "Cheng Chen",
                "Chengcheng Han",
                "Chenhao Qiu",
                "Chong Peng",
                "Cong Xu",
                "Dailin Li",
                "Feiyu Wang",
                "Feng Gao",
                "Guibo Zhu",
                "Guopeng Tang",
                "Haibo Lu",
                "Han Fang",
                "Han Qi",
                "Hanxiao Wu",
                "Haobo Cheng",
                "Hongbo Sun",
                "Hongyao Chen",
                "Huayong Hu",
                "Hui Li",
                "Jiaheng Ma",
                "Jiang Yu",
                "Jianing Wang",
                "Jie Yang",
                "Jing He",
                "Jinglin Zhou",
                "Jingxuan Li",
                "Josef Kittler",
                "Lihao Zheng",
                "Linnan Zhao",
                "Mengxi Jia",
                "Muyang Yan",
                "Nguyen Thanh Thien",
                "Pu Luo",
                "Qi Li",
                "Shien Song",
                "Shijie Dong",
                "Shuai Shao",
                "Shutao Li",
                "Taofeng Xue",
                "Tianyang Xu",
                "Tianyi Gao",
                "Tingting Li",
                "Wei Zhang",
                "Weiyang Su",
                "Xiaodong Dong",
                "Xiao-Jun Wu",
                "Xiaopeng Zhou",
                "Xin Chen",
                "Xin Wei",
                "Xinyi You",
                "Xudong Kang",
                "Xujie Zhou",
                "Xusheng Liu",
                "Yanan Wang",
                "Yanbin Huang",
                "Yang Liu",
                "Yang Yang",
                "Yanglin Deng",
                "Yashu Kang",
                "Ye Yuan",
                "Yi Wen",
                "Yicen Tian",
                "Yilin Tao",
                "Yin Tang",
                "Yipeng Lin",
                "Yiqing Wang",
                "Yiting Xi",
                "Yongkang Yu",
                "Yumei Li",
                "Yuxin Qin",
                "Yuying Chen",
                "Yuzhe Cen",
                "Zhaofan Zou",
                "Zhaohong Liu",
                "Zhehao Shen",
                "Zhenglin Du",
                "Zhengyang Li",
                "Zhenni Huang",
                "Zhenwei Shao",
                "Zhilong Song",
                "Zhiyong Feng",
                "Zhiyu Wang",
                "Zhou Yu",
                "Ziang Li",
                "Zihan Zhai",
                "Zijian Zhang",
                "Ziyang Peng",
                "Ziyun Xiao",
                "Zongshu Li"
            ],
            "affiliations": [
                "ByteDance",
                "Meituan",
                "NVIDIA",
                "Samsung"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14142.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#open_source",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜: ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MARS2 2025",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MARS2 2025 Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ’ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ»Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ 40 Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 76 ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ² Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹-Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞºĞ»Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ĞºĞ¾Ğ´ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Advancing Multimodal Reasoning with MARS2 2025 Challenge",
                    "desc": "The MARS2 2025 Challenge is a competition that evaluates multimodal reasoning capabilities of large language models (LLMs) using real-world and specialized scenarios. It features tailored datasets, Lens and AdsQA, designed for general reasoning and domain-specific tasks in advertisement videos. Over 40 models were assessed across three tracks: Visual Grounding in Real-world Scenarios, Visual Question Answering with Spatial Awareness, and Visual Reasoning in Creative Advertisement Videos. The challenge aims to advance the field of multimodal machine learning by providing a comprehensive benchmark and fostering collaboration among researchers."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ¨ç†çš„æœªæ¥æŒ‘æˆ˜",
                    "desc": "MARS2 2025æŒ‘æˆ˜ä¸“æ³¨äºå¤šæ¨¡æ€æ¨ç†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå’Œä¸“ä¸šåœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬é€šè¿‡å®šåˆ¶çš„æ•°æ®é›†ï¼Œè¯„ä¼°äº†40å¤šä¸ªæ¨¡å‹ï¼Œæ¶µç›–äº†è§†è§‰å®šä½ã€è§†è§‰é—®ç­”å’Œåˆ›æ„å¹¿å‘Šè§†é¢‘ç­‰å¤šä¸ªç«èµ›æ–¹å‘ã€‚æ­¤æ¬¡æŒ‘æˆ˜æ—¨åœ¨ä¿ƒè¿›å¤šæ¨¡æ€æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸åŒæ–¹æ³•çš„ç»“åˆï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚æˆ‘ä»¬æä¾›çš„Lenså’ŒAdsQAæ•°æ®é›†æ”¯æŒæ—¥å¸¸åœºæ™¯å’Œå¹¿å‘Šè§†é¢‘ä¸­çš„æ¨ç†ï¼Œä¿ƒè¿›äº†å¤šæ¨¡æ€æ¨ç†åº”ç”¨çš„æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14055",
            "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic\n  Replication",
            "url": "https://huggingface.co/papers/2509.14055",
            "abstract": "Wan-Animate is a unified framework for character animation and replacement, using spatially-aligned skeleton signals and implicit facial features to generate high-fidelity character videos with seamless environmental integration.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code.",
            "score": 0,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "da77342ad4a41fa8",
            "authors": [
                "Gang Cheng",
                "Xin Gao",
                "Li Hu",
                "Siqi Hu",
                "Mingyang Huang",
                "Chaonan Ji",
                "Ju Li",
                "Dechao Meng",
                "Jinwei Qi",
                "Penchong Qiao",
                "Zhen Shen",
                "Yafei Song",
                "Ke Sun",
                "Linrui Tian",
                "Feng Wang",
                "Guangyuan Wang",
                "Qi Wang",
                "Zhongjian Wang",
                "Jiayu Xiao",
                "Sheng Xu",
                "Bang Zhang",
                "Peng Zhang",
                "Xindi Zhang",
                "Zhe Zhang",
                "Jingren Zhou",
                "Lian Zhuo"
            ],
            "affiliations": [
                "HumanAIGC Team Tongyi Lab, Alibaba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14055.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#multimodal",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹: Ğ¾Ñ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ´Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Wan-Animate - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ ÑÑ†ĞµĞ½Ñ‹. Wan-Animate Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Wan Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Seamless Character Animation and Replacement with Wan-Animate",
                    "desc": "Wan-Animate is a comprehensive framework designed for character animation and replacement, utilizing spatially-aligned skeleton signals and implicit facial features. It allows users to animate a character by mimicking the expressions and movements from a reference video, resulting in high-quality character videos. Additionally, it can seamlessly integrate animated characters into existing scenes by matching the lighting and color tones of the environment. The framework employs a modified input approach to unify various tasks into a single symbolic representation, enhancing both controllability and expressiveness in character animation."
                },
                "zh": {
                    "title": "Wan-Animateï¼šæ— ç¼è§’è‰²åŠ¨ç”»ä¸æ›¿æ¢çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "Wan-Animateæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§’è‰²åŠ¨ç”»å’Œæ›¿æ¢æ¡†æ¶ï¼Œåˆ©ç”¨ç©ºé—´å¯¹é½çš„éª¨éª¼ä¿¡å·å’Œéšå¼é¢éƒ¨ç‰¹å¾ç”Ÿæˆé«˜ä¿çœŸçš„è§’è‰²è§†é¢‘ã€‚è¯¥æ¡†æ¶å¯ä»¥æ ¹æ®ç»™å®šçš„è§’è‰²å›¾åƒå’Œå‚è€ƒè§†é¢‘ï¼Œç²¾ç¡®å¤åˆ¶è§’è‰²çš„è¡¨æƒ…å’ŒåŠ¨ä½œï¼Œå®ç°è§’è‰²åŠ¨ç”»ã€‚å®ƒè¿˜å¯ä»¥å°†åŠ¨ç”»è§’è‰²æ— ç¼åœ°é›†æˆåˆ°å‚è€ƒè§†é¢‘ä¸­ï¼Œä¿æŒåœºæ™¯çš„å…‰ç…§å’Œè‰²è°ƒä¸€è‡´ã€‚Wan-Animateé€šè¿‡ä¿®æ”¹è¾“å…¥èŒƒå¼ï¼Œç»Ÿä¸€å¤šä¸ªä»»åŠ¡ä¸ºä¸€ä¸ªå…±åŒçš„ç¬¦å·è¡¨ç¤ºï¼Œå±•ç¤ºäº†åœ¨è§’è‰²åŠ¨ç”»ä»»åŠ¡ä¸­çš„é«˜å¯æ§æ€§å’Œè¡¨ç°åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-17.html",
    "link_next": "2025-09-19.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "17.09",
        "en": "09/17",
        "zh": "9æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.09",
        "en": "09/19",
        "zh": "9æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}