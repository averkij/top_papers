{
    "date": {
        "ru": "3 сентября",
        "en": "September 3",
        "zh": "9月3日"
    },
    "time_utc": "2025-09-03 07:11",
    "weekday": 2,
    "issue_id": 5688,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.00676",
            "title": "LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model",
            "url": "https://huggingface.co/papers/2509.00676",
            "abstract": "Reinforcement learning on preference-labeled critic datasets enhances a generative model's performance, creating a unified multimodal system that excels in both evaluation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.",
            "score": 34,
            "issue_id": 5686,
            "pub_date": "2025-08-31",
            "pub_date_card": {
                "ru": "31 августа",
                "en": "August 31",
                "zh": "8月31日"
            },
            "hash": "804da0110302d00c",
            "authors": [
                "Xiyao Wang",
                "Chunyuan Li",
                "Jianwei Yang",
                "Kai Zhang",
                "Bo Liu",
                "Tianyi Xiong",
                "Furong Huang"
            ],
            "affiliations": [
                "National University of Singapore",
                "The Ohio State University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00676.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#rlhf",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Объединение критика и генератора: новый шаг в мультимодальном ИИ",
                    "desc": "В этой статье предлагается новый подход к обучению мультимодальных языковых моделей, объединяющий функции критика и генератора. Исследователи применили обучение с подкреплением на наборах данных с предпочтениями для улучшения базовой генеративной модели. Результатом стала модель LLaVA-Critic-R1, которая превосходит специализированные модели на 26 визуальных тестах понимания и рассуждения. Этот метод позволяет создать унифицированную модель, эффективную как в оценке, так и в генерации контента."
                },
                "en": {
                    "title": "Bridging Evaluation and Generation in Multimodal Systems",
                    "desc": "This paper introduces a novel approach to improve generative models by applying reinforcement learning (RL) on preference-labeled critic datasets. Traditionally, critic models evaluate outputs but do not generate responses, creating a divide between evaluation and generation tasks. The authors propose a unified model, LLaVA-Critic-R1, which not only serves as a high-performing critic but also functions effectively as a policy model for generating responses. Their findings demonstrate that this integrated approach enhances performance across various visual reasoning benchmarks, showcasing the potential for self-improving multimodal systems."
                },
                "zh": {
                    "title": "强化学习提升生成模型的统一多模态系统",
                    "desc": "本研究提出了一种新的方法，将带有偏好的评论数据集用于强化学习，以提升生成模型的性能。我们重新组织这些评论数据，直接在基础生成模型上进行训练，开发出LLaVA-Critic-R1，这是一种能够优化偏好判断的多模态评论模型，同时保留生成能力。实验结果表明，LLaVA-Critic-R1不仅在评论任务中表现优异，还在多个视觉推理基准测试中与专门的推理模型相媲美，甚至超越它们。最终，我们的研究表明，利用评论数据进行强化学习可以创建一个在评估和生成方面都表现出色的统一模型，推动多模态系统的自我改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02547",
            "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
            "url": "https://huggingface.co/papers/2509.02547",
            "abstract": "Agentic reinforcement learning transforms large language models into autonomous decision-making agents by leveraging temporally extended POMDPs, enhancing capabilities like planning and reasoning through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.",
            "score": 31,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "04a4d0adade32d34",
            "authors": [
                "Guibin Zhang",
                "Hejia Geng",
                "Xiaohang Yu",
                "Zhenfei Yin",
                "Zaibin Zhang",
                "Zelin Tan",
                "Heng Zhou",
                "Zhongzhi Li",
                "Xiangyuan Xue",
                "Yijiang Li",
                "Yifan Zhou",
                "Yang Chen",
                "Chen Zhang",
                "Yutao Fan",
                "Zihu Wang",
                "Songtao Huang",
                "Yue Liao",
                "Hongru Wang",
                "Mengyue Yang",
                "Heng Ji",
                "Michael Littman",
                "Jun Wang",
                "Shuicheng Yan",
                "Philip Torr",
                "Lei Bai"
            ],
            "affiliations": [
                "Brown University",
                "Chinese Academy of Sciences",
                "Dalian University of Technology",
                "Fudan University",
                "Imperial College London",
                "National University of Singapore",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "University College London",
                "University of Bristol",
                "University of California, San Diego",
                "University of California, Santa Barbara",
                "University of Georgia",
                "University of Illinois Urbana-Champaign",
                "University of Oxford",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02547.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#reasoning",
                    "#agi",
                    "#survey",
                    "#rl",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Большие языковые модели становятся автономными агентами",
                    "desc": "Агентное обучение с подкреплением (Agentic RL) трансформирует большие языковые модели в автономных агентов, принимающих решения. Это достигается путем использования частично наблюдаемых марковских процессов принятия решений (POMDP) с временной протяженностью. Agentic RL улучшает такие способности как планирование и рассуждение через обучение с подкреплением. Этот подход представляет собой парадигмальный сдвиг от традиционного применения обучения с подкреплением к большим языковым моделям."
                },
                "en": {
                    "title": "Transforming Language Models into Autonomous Decision-Makers",
                    "desc": "This paper introduces agentic reinforcement learning (Agentic RL), which changes how large language models (LLMs) operate by allowing them to make autonomous decisions in complex environments. It contrasts traditional single-step Markov Decision Processes (MDPs) used in LLMs with more advanced temporally extended partially observable Markov decision processes (POMDPs) that enable better planning and reasoning. The authors propose a taxonomy that categorizes agentic capabilities like memory and self-improvement, as well as their applications in various tasks. Additionally, the paper provides a resource guide for researchers, summarizing over five hundred studies to outline the current state and future directions of this emerging field."
                },
                "zh": {
                    "title": "代理强化学习：从被动生成到自主决策的转变",
                    "desc": "代理强化学习（Agentic RL）将大型语言模型转变为自主决策的智能体，利用时间扩展的部分可观察马尔可夫决策过程（POMDPs），增强了规划和推理等能力。与传统的单步马尔可夫决策过程（MDPs）相比，代理强化学习使得语言模型不再是被动的序列生成器，而是能够在复杂动态环境中自主决策的智能体。本文提出了一种全面的分类法，围绕核心的代理能力，如规划、工具使用、记忆、推理、自我改进和感知进行组织。通过整合开源环境、基准和框架，本文为未来的研究提供了实用的参考。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01215",
            "title": "POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models\n  for Document Conversion",
            "url": "https://huggingface.co/papers/2509.01215",
            "abstract": "A framework for constructing high-quality document extraction datasets and models through synthetic data generation and iterative self-improvement outperforms existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality labeled data is essential for training accurate document conversion models, particularly in domains with complex formats such as tables, formulas, and multi-column text. However, manual annotation is both costly and time-consuming, while automatic labeling using existing models often lacks accuracy in handling such challenging scenarios. Consequently, training student models by distilling outputs from teacher models can significantly limit their performance in real-world applications. In this paper, we propose a fully automated, distillation-free framework comprising two stages for constructing high-quality document extraction datasets and models capable of handling diverse document formats and layouts. In the first stage, we introduce a method for generating large-scale, diverse synthetic data, which enables a model to extract key elements in a unified format with strong initial performance. In the second stage, we present a self-improvement approach that further adapts the model, initially trained on synthetic data, to real-world documents. Specifically, we first use the fine-tuned model to annotate real documents, then apply a suite of filtering strategies to verify annotation quality, and finally retrain the model on the verified dataset. By iteratively repeating this process, we progressively enhance both the model's conversion capabilities and the quality of the generated data. We train a public POINTS-1.5 model to obtain POINTS-Reader, which surpasses many existing public and proprietary models of comparable or larger size. Our model is available at https://github.com/Tencent/POINTS-Reader.",
            "score": 29,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "1f731f4067d86ef7",
            "authors": [
                "Yuan Liu",
                "Zhongyin Zhao",
                "Le Tian",
                "Haicheng Wang",
                "Xubing Ye",
                "Yangxiu You",
                "Zilin Yu",
                "Chuhan Wu",
                "Xiao Zhou",
                "Yang Yu",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent Inc, China",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01215.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "Автоматическое создание высококачественных моделей для извлечения данных из документов",
                    "desc": "Статья представляет новый фреймворк для создания высококачественных наборов данных и моделей для извлечения информации из документов. Метод состоит из двух этапов: генерация синтетических данных и итеративное самоулучшение модели на реальных документах. Авторы используют фильтрацию аннотаций и переобучение для повышения качества модели и данных. Результирующая модель POINTS-Reader превосходит многие существующие публичные и проприетарные решения."
                },
                "en": {
                    "title": "Revolutionizing Document Extraction with Synthetic Data and Self-Improvement",
                    "desc": "This paper presents a new framework for creating high-quality datasets and models for document extraction using synthetic data generation and iterative self-improvement. It addresses the challenges of manual data labeling and the limitations of existing models in handling complex document formats. The proposed method generates diverse synthetic data to train an initial model, which is then refined through a self-improvement process that involves annotating real documents and verifying the quality of these annotations. The resulting model, POINTS-Reader, demonstrates superior performance compared to existing models, making it a valuable tool for document conversion tasks."
                },
                "zh": {
                    "title": "合成数据驱动的文档提取新框架",
                    "desc": "本文提出了一种构建高质量文档提取数据集和模型的框架，利用合成数据生成和迭代自我改进的方法。该框架分为两个阶段：第一阶段生成大规模多样的合成数据，以便模型能够以统一格式提取关键信息；第二阶段通过自我改进方法，将初步训练的模型适应真实文档。通过对真实文档进行标注、质量验证和模型重训练，逐步提升模型的转换能力和生成数据的质量。最终，训练出的POINTS-Reader模型在性能上超越了许多现有的公共和专有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02544",
            "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.02544",
            "abstract": "UI-TARS-2, a native GUI-centered agent model, addresses challenges in data scalability, multi-turn reinforcement learning, and environment stability, achieving significant improvements over its predecessor and strong baselines across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios.",
            "score": 25,
            "issue_id": 5686,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "71474173af3c991b",
            "authors": [
                "Haoming Wang",
                "Haoyang Zou",
                "Huatong Song",
                "Jiazhan Feng",
                "Junjie Fang",
                "Junting Lu",
                "Longxiang Liu",
                "Qinyu Luo",
                "Shihao Liang",
                "Shijue Huang",
                "Wanjun Zhong",
                "Yining Ye",
                "Yujia Qin",
                "Yuwen Xiong",
                "Yuxin Song",
                "Zhiyong Wu",
                "Bo Li",
                "Chen Dun",
                "Chong Liu",
                "Fuxing Leng",
                "Hanbin Wang",
                "Hao Yu",
                "Haobin Chen",
                "Hongyi Guo",
                "Jing Su",
                "Jingjia Huang",
                "Kai Shen",
                "Kaiyu Shi",
                "Lin Yan",
                "Peiyao Zhao",
                "Pengfei Liu",
                "Qinghao Ye",
                "Renjie Zheng",
                "Wayne Xin Zhao",
                "Wen Heng",
                "Wenhao Huang",
                "Wenqian Wang",
                "Xiaobo Qin",
                "Yi Lin",
                "Youbin Wu",
                "Zehui Chen",
                "Zihao Wang",
                "Baoquan Zhong",
                "Xinchun Zhang",
                "Xujing Li",
                "Yuanfan Li",
                "Zhongkai Zhao",
                "Chengquan Jiang",
                "Faming Wu",
                "Haotian Zhou",
                "Jinlin Pang",
                "Li Han",
                "Qianli Ma",
                "Siyao Liu",
                "Songhua Cai",
                "Wenqi Fu",
                "Xin Liu",
                "Zhi Zhang",
                "Bo Zhou",
                "Guoliang Li",
                "Jiajun Shi",
                "Jiale Yang",
                "Jie Tang",
                "Li Li",
                "Taoran Lu",
                "Woyu Lin",
                "Xiaokang Tong",
                "Xinyao Li",
                "Yichi Zhang",
                "Yu Miao",
                "Zhengxuan Jiang",
                "Zili Li",
                "Ziyuan Zhao",
                "Chenxin Li",
                "Dehua Ma",
                "Feng Lin",
                "Ge Zhang",
                "Haihua Yang",
                "Hangyu Guo",
                "Hongda Zhu",
                "Jiaheng Liu",
                "Junda Du",
                "Kai Cai",
                "Kuanye Li",
                "Lichen Yuan",
                "Meilan Han",
                "Minchao Wang",
                "Shuyue Guo",
                "Tianhao Cheng",
                "Xiaobo Ma",
                "Xiaojun Xiao",
                "Xiaolong Huang",
                "Xinjie Chen",
                "Yidi Du",
                "Yilin Chen",
                "Yiwen Wang",
                "Zhaojian Li",
                "Zhenzhu Yang",
                "Zhiyuan Zeng",
                "Chaolin Jin",
                "Chen Li",
                "Hao Chen",
                "Haoli Chen",
                "Jian Chen",
                "Qinghao Zhao",
                "Guang Shi"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02544.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "UI-TARS-2: Новый уровень GUI-агентов с улучшенным обучением и обобщением",
                    "desc": "UI-TARS-2 - это модель агента для графических пользовательских интерфейсов, решающая проблемы масштабируемости данных, многоходового обучения с подкреплением и стабильности окружения. Модель использует систематическую методологию обучения, включающую маховик данных для масштабируемой генерации, стабилизированную структуру многоходового RL и гибридную GUI-среду. UI-TARS-2 значительно превосходит предыдущую версию и сильные бейзлайны на различных бенчмарках, достигая высоких показателей в GUI-задачах и игровых средах. Результаты демонстрируют потенциал UI-TARS-2 для продвижения GUI-агентов и обобщения на реальные интерактивные сценарии."
                },
                "en": {
                    "title": "Revolutionizing GUI Agents with UI-TARS-2",
                    "desc": "UI-TARS-2 is a new model designed for creating intelligent agents that can interact with graphical user interfaces (GUIs). It tackles key issues like handling large amounts of data, improving learning over multiple interactions, and ensuring stable performance in different environments. The model uses innovative techniques such as a data flywheel for efficient data generation and a hybrid environment that combines various systems for better training. Its performance on various benchmarks shows significant improvements over previous models, indicating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "UI-TARS-2：提升图形用户界面智能体的未来",
                    "desc": "UI-TARS-2是一个以图形用户界面（GUI）为中心的智能体模型，旨在解决数据可扩展性、多轮强化学习和环境稳定性等挑战。该模型通过系统化的训练方法，包括可扩展的数据生成、稳定的多轮强化学习框架和集成文件系统与终端的混合GUI环境，显著提升了性能。实证评估显示，UI-TARS-2在多个基准测试中超越了其前身UI-TARS-1.5和其他强基线模型。该模型在长时间信息检索任务和软件工程基准测试中表现出色，展示了其在多样化智能体任务中的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01055",
            "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
            "url": "https://huggingface.co/papers/2509.01055",
            "abstract": "VerlTool is a unified and modular framework for Agentic Reinforcement Learning with Tool use, addressing inefficiencies in existing approaches and providing competitive performance across multiple domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2times speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.",
            "score": 24,
            "issue_id": 5687,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "8d89f7851ae1d950",
            "authors": [
                "Dongfu Jiang",
                "Yi Lu",
                "Zhuofeng Li",
                "Zhiheng Lyu",
                "Ping Nie",
                "Haozhe Wang",
                "Alex Su",
                "Hui Chen",
                "Kai Zou",
                "Chao Du",
                "Tianyu Pang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "HKUST",
                "Independent",
                "National University of Singapore",
                "NetMind.AI",
                "Sea AI Lab",
                "Shanghai University",
                "University of Toronto",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01055.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#rl",
                    "#open_source",
                    "#agi",
                    "#agents",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "VerlTool: Универсальный фреймворк для обучения с подкреплением с использованием инструментов",
                    "desc": "VerlTool - это унифицированный и модульный фреймворк для агентного обучения с подкреплением с использованием инструментов. Он решает проблемы неэффективности существующих подходов, обеспечивая конкурентоспособную производительность в нескольких областях. VerlTool предлагает четыре ключевых улучшения: совместимость с VeRL, унифицированное управление инструментами через стандартизированные API, асинхронное выполнение и комплексную оценку на 6 доменах ARLT. Фреймворк формализует ARLT как многоходовые траектории с мультимодальными токенами наблюдений, расширяя парадигмы одноходового RLVR."
                },
                "en": {
                    "title": "Streamlining Agentic Reinforcement Learning with VerlTool",
                    "desc": "VerlTool is a new framework designed for Agentic Reinforcement Learning that incorporates tool use, aiming to improve efficiency and performance in various tasks. It addresses the limitations of existing methods by providing a unified system that supports multiple modalities and asynchronous execution, which speeds up processing. The framework allows for easy integration of tools through a modular architecture, making it simpler for researchers to develop and test new ideas. By demonstrating strong performance across different domains, VerlTool encourages broader adoption and innovation in the field of reinforcement learning with tools."
                },
                "zh": {
                    "title": "VerlTool：提升代理强化学习的统一框架",
                    "desc": "VerlTool是一个统一且模块化的框架，专注于具有工具使用的代理强化学习，旨在解决现有方法中的低效问题。它通过系统设计原则，提供了四个关键贡献，包括与可验证奖励的上游对齐、统一的工具管理、异步执行以提高速度，以及在多个领域的竞争性表现评估。该框架将代理强化学习形式化为多轮轨迹，支持多模态观察令牌，超越了单轮交互的限制。VerlTool的模块化插件架构使得工具集成变得快速且简单，显著降低了开发成本，为工具增强的强化学习研究提供了可扩展的基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02208",
            "title": "Baichuan-M2: Scaling Medical Capability with Large Verifier System",
            "url": "https://huggingface.co/papers/2509.02208",
            "abstract": "A dynamic verification framework using reinforcement learning and a novel algorithm improves the performance of a large medical language model in real-world clinical decision-making.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment.",
            "score": 22,
            "issue_id": 5687,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "f78da0ee0b5088cb",
            "authors": [
                "Baichuan-M2 Team",
                ":",
                "Chengfeng Dou",
                "Chong Liu",
                "Fan Yang",
                "Fei Li",
                "Jiyuan Jia",
                "Mingyang Chen",
                "Qiang Ju",
                "Shuai Wang",
                "Shunya Dang",
                "Tianpeng Li",
                "Xiangrong Zeng",
                "Yijie Zhou",
                "Chenzheng Zhu",
                "Da Pan",
                "Fei Deng",
                "Guangwei Ai",
                "Guosheng Dong",
                "Hongda Zhang",
                "Jinyang Tai",
                "Jixiang Hong",
                "Kai Lu",
                "Linzhuang Sun",
                "Peidong Guo",
                "Qian Ma",
                "Rihui Xin",
                "Shihui Yang",
                "Shusen Zhang",
                "Yichuan Mo",
                "Zheng Liang",
                "Zhishou Zhang",
                "Hengfu Cui",
                "Zuyi Zhu",
                "Xiaochuan Wang"
            ],
            "affiliations": [
                "Baichuan-M2 Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02208.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#open_source",
                    "#reasoning",
                    "#agents",
                    "#alignment",
                    "#training",
                    "#healthcare"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Динамическая верификация LLM для реальной клинической практики",
                    "desc": "Эта статья представляет новую динамическую систему верификации для улучшения работы больших языковых моделей (LLM) в реальных клинических условиях. Система включает симулятор пациента и генератор клинических рубрик для создания интерактивной среды обучения. На основе этой системы авторы разработали модель Baichuan-M2 с 32 миллиардами параметров, обученную с помощью усиленного обучения. Baichuan-M2 превзошла другие открытые модели в тесте HealthBench, демонстрируя эффективность предложенного подхода для применения LLM в медицине."
                },
                "en": {
                    "title": "Revolutionizing Clinical Decision-Making with Dynamic Verification in AI",
                    "desc": "This paper presents a dynamic verification framework that enhances the performance of large medical language models (LLMs) in real-world clinical settings using reinforcement learning. The framework addresses the limitations of traditional static benchmarks by incorporating a Patient Simulator and a Clinical Rubrics Generator, which create realistic clinical scenarios and dynamic evaluation metrics. The authors introduce Baichuan-M2, a 32B-parameter model trained with an advanced Group Relative Policy Optimization algorithm, which significantly outperforms existing models on the HealthBench benchmark. This work highlights the importance of dynamic verification systems in aligning LLM capabilities with practical healthcare applications, setting a new standard for medical AI performance."
                },
                "zh": {
                    "title": "动态验证提升医疗AI决策能力",
                    "desc": "本论文提出了一种动态验证框架，利用强化学习和新算法提升大型医疗语言模型在真实临床决策中的表现。传统的静态基准测试无法有效反映医疗咨询的动态互动特性，因此我们设计了一个包含患者模拟器和临床评分生成器的系统。通过多阶段强化学习策略和改进的群体相对策略优化算法（GRPO），我们开发了Baichuan-M2模型，并在HealthBench上取得了优异的成绩。我们的研究表明，强大的动态验证系统对于将大型语言模型的能力与实际临床应用对齐至关重要。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02479",
            "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning",
            "url": "https://huggingface.co/papers/2509.02479",
            "abstract": "SimpleTIR stabilizes multi-turn Tool-Integrated Reasoning training by filtering out void turns, achieving state-of-the-art performance on math reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.",
            "score": 21,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "e6fb78d3f5363c7d",
            "authors": [
                "Zhenghai Xue",
                "Longtao Zheng",
                "Qian Liu",
                "Yingru Li",
                "Xiaosen Zheng",
                "Zejun Ma",
                "Bo An"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore",
                "TikTok, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02479.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#math",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Стабильное обучение AI рассуждать с инструментами",
                    "desc": "Статья представляет алгоритм SimpleTIR, стабилизирующий обучение многоходовых моделей с интегрированными инструментами (Tool-Integrated Reasoning, TIR). Метод фильтрует неинформативные шаги, предотвращая взрывной рост градиентов и улучшая процесс обучения. SimpleTIR достигает наилучших результатов на сложных задачах математического рассуждения, значительно повышая показатели базовых моделей. Алгоритм поощряет модель находить разнообразные и сложные паттерны рассуждений, включая самокоррекцию и перекрестную проверку."
                },
                "en": {
                    "title": "Stabilizing Multi-Turn Reasoning with SimpleTIR",
                    "desc": "The paper presents SimpleTIR, an innovative algorithm designed to enhance the stability of multi-turn Tool-Integrated Reasoning (TIR) training in large language models. It addresses the issue of training instability caused by distributional drift from external tool feedback, which leads to the generation of low-probability tokens and catastrophic gradient explosions. By filtering out void turns—those that do not produce useful outputs—SimpleTIR prevents harmful gradients from affecting the learning process. As a result, it achieves state-of-the-art performance on math reasoning tasks, significantly improving scores while promoting the discovery of advanced reasoning strategies."
                },
                "zh": {
                    "title": "SimpleTIR：稳定多轮推理训练的创新算法",
                    "desc": "本文介绍了一种名为SimpleTIR的算法，它通过过滤掉无效回合来稳定多轮工具集成推理（TIR）训练。多轮TIR在使用强化学习时常常面临训练不稳定和性能崩溃的问题，主要是由于外部工具反馈导致的分布漂移。SimpleTIR的核心策略是识别并去除那些既没有代码块也没有最终答案的回合，从而有效阻止有害的高幅度梯度，稳定学习动态。实验结果表明，SimpleTIR在数学推理基准测试中达到了最先进的性能，显著提高了模型的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02522",
            "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for\n  RLVR",
            "url": "https://huggingface.co/papers/2509.02522",
            "abstract": "PACS, a novel RLVR framework, reformulates RLVR as a supervised learning task, improving stability and efficiency in training large language models for reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose PACS, a novel RLVR framework that achieves imPlicit Actor Critic coupling via a Supervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.",
            "score": 17,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "824c46ed359a21d1",
            "authors": [
                "Jiaming Li",
                "Longze Chen",
                "Ze Gong",
                "Yukun Chen",
                "Lu Wang",
                "Wanwei He",
                "Run Luo",
                "Min Yang"
            ],
            "affiliations": [
                "Ritzz-AI",
                "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02522.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "PACS: Эффективное обучение языковых моделей рассуждению через супервизорный RLVR",
                    "desc": "PACS - это новый фреймворк для обучения с подкреплением с проверяемыми наградами (RLVR), который переформулирует задачу RLVR как задачу обучения с учителем. Это позволяет повысить стабильность и эффективность обучения больших языковых моделей для решения задач рассуждения. PACS использует функцию оценки, параметризованную моделью политики и оптимизированную с помощью кросс-энтропийной функции потерь. Этот подход неявно объединяет роли актора и критика, что приводит к более стабильному и эффективному обучению по сравнению с классическими методами RLVR."
                },
                "en": {
                    "title": "PACS: Transforming RLVR into Supervised Learning for Better Reasoning",
                    "desc": "PACS is a new framework that reformulates Reinforcement Learning with Verifiable Rewards (RLVR) as a supervised learning task, which enhances the stability and efficiency of training large language models (LLMs) for reasoning tasks. By treating outcome rewards as predictable labels, PACS transforms the RLVR problem into a supervised learning problem, optimizing it with cross-entropy loss. This approach allows for implicit coupling of actor and critic roles, leading to more stable policy updates and improved training outcomes. Benchmark results show that PACS significantly outperforms traditional RLVR methods like PPO and GRPO in mathematical reasoning tasks, demonstrating its effectiveness in enhancing LLM performance."
                },
                "zh": {
                    "title": "PACS：稳定高效的推理训练新框架",
                    "desc": "PACS是一种新颖的强化学习可验证奖励（RLVR）框架，它将RLVR重新定义为一个监督学习任务，从而提高了大语言模型在推理任务中的稳定性和效率。通过将结果奖励视为可预测的标签，PACS将RLVR问题转化为一个基于策略模型的分数函数的监督学习任务，并使用交叉熵损失进行优化。详细的梯度分析表明，这种监督形式本质上恢复了经典的策略梯度更新，同时隐式地耦合了演员和评论者的角色，从而实现了更稳定和高效的训练。在具有挑战性的数学推理任务上，PACS的表现优于强大的RLVR基线，如PPO和GRPO，显示出其在推理性能上的显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01563",
            "title": "Kwai Keye-VL 1.5 Technical Report",
            "url": "https://huggingface.co/papers/2509.01563",
            "abstract": "Keye-VL-1.5 enhances video understanding through a Slow-Fast encoding strategy, progressive pre-training, and post-training reasoning improvements, outperforming existing models on video tasks while maintaining general multimodal performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks.",
            "score": 17,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "b033203f893abafc",
            "authors": [
                "Biao Yang",
                "Bin Wen",
                "Boyang Ding",
                "Changyi Liu",
                "Chenglong Chu",
                "Chengru Song",
                "Chongling Rao",
                "Chuan Yi",
                "Da Li",
                "Dunju Zang",
                "Fan Yang",
                "Guorui Zhou",
                "Guowang Zhang",
                "Han Shen",
                "Hao Peng",
                "Haojie Ding",
                "Hao Wang",
                "Hengrui Ju",
                "Jiaming Huang",
                "Jiangxia Cao",
                "Jiankang Chen",
                "Jingyun Hua",
                "Kaibing Chen",
                "Kaiyu Jiang",
                "Kaiyu Tang",
                "Kun Gai",
                "Muhao Wei",
                "Qiang Wang",
                "Ruitao Wang",
                "Sen Na",
                "Shengnan Zhang",
                "Siyang Mao",
                "Sui Huang",
                "Tianke Zhang",
                "Tingting Gao",
                "Wei Chen",
                "Wei Yuan",
                "Xiangyu Wu",
                "Xiao Hu",
                "Xingyu Lu",
                "Yi-Fan Zhang",
                "Yiping Yang",
                "Yulong Chen",
                "Zeyi Lu",
                "Zhenhua Wu",
                "Zhixin Ling",
                "Zhuoran Yang",
                "Ziming Li",
                "Di Xu",
                "Haixuan Gao",
                "Hang Li",
                "Jing Wang",
                "Lejian Ren",
                "Qigen Hu",
                "Qianqian Wang",
                "Shiyao Wang",
                "Xinchen Luo",
                "Yan Li",
                "Yuhang Hu",
                "Zixing Zhang"
            ],
            "affiliations": [
                "Kuaishou Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01563.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#video",
                    "#long_context",
                    "#rl",
                    "#training",
                    "#alignment"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Революция в понимании видео: Keye-VL-1.5 объединяет эффективность и точность",
                    "desc": "Keye-VL-1.5 - это новая модель машинного обучения для понимания видео. Она использует стратегию кодирования Slow-Fast, которая обрабатывает ключевые кадры с высоким разрешением, а статичные - с низким. Модель применяет прогрессивное предобучение, увеличивая контекст до 128 тысяч токенов. Послеобучение включает улучшение рассуждений и выравнивание с человеческими предпочтениями."
                },
                "en": {
                    "title": "Revolutionizing Video Understanding with Keye-VL-1.5",
                    "desc": "Keye-VL-1.5 is a new model designed to improve video understanding by using a Slow-Fast encoding strategy that optimizes how videos are processed. This model employs a progressive pre-training approach that allows it to handle longer videos and more complex visual information effectively. Additionally, it includes a post-training pipeline that enhances reasoning capabilities and aligns with human preferences through advanced training techniques. Overall, Keye-VL-1.5 outperforms existing models in video tasks while still performing well in general multimodal applications."
                },
                "zh": {
                    "title": "Keye-VL-1.5：视频理解的新突破",
                    "desc": "Keye-VL-1.5通过慢-快编码策略、渐进式预训练和后训练推理改进，提升了视频理解能力。慢-快编码策略根据帧间相似性动态分配计算资源，处理关键帧时使用高分辨率，而对静态帧则使用低分辨率以增加时间覆盖。渐进式预训练方法将模型的上下文长度从8K扩展到128K，使其能够处理更长的视频和更复杂的视觉内容。经过广泛评估，Keye-VL-1.5在视频理解任务上显著优于现有模型，同时在多模态基准测试中保持竞争力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02534",
            "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations",
            "url": "https://huggingface.co/papers/2509.02534",
            "abstract": "DARLING, a diversity-aware reinforcement learning framework, enhances both the quality and diversity of large language model outputs across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses.",
            "score": 8,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "b3e51a0003bb3957",
            "authors": [
                "Tianjian Li",
                "Yiming Zhang",
                "Ping Yu",
                "Swarnadeep Saha",
                "Daniel Khashabi",
                "Jason Weston",
                "Jack Lanchantin",
                "Tianlu Wang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "FAIR",
                "Johns Hopkins University",
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02534.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#story_generation",
                    "#rlhf",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "🌈",
                "ru": {
                    "title": "DARLING: Качество и разнообразие в гармонии",
                    "desc": "DARLING - это фреймворк обучения с подкреплением, который улучшает как качество, так и разнообразие выходных данных больших языковых моделей. Он вводит обученную функцию разбиения для измерения семантического разнообразия, выходящего за рамки лексических вариаций. DARLING объединяет сигнал разнообразия с оценкой качества во время онлайн-обучения с подкреплением. Эксперименты показывают, что DARLING превосходит базовые модели RL, ориентированные только на качество, создавая результаты более высокого качества и новизны."
                },
                "en": {
                    "title": "Enhancing Creativity with Diversity-Aware Reinforcement Learning",
                    "desc": "DARLING is a framework designed to improve the outputs of large language models by focusing on both quality and diversity. Traditional post-training methods often enhance accuracy but limit the variety of responses, which is crucial for creative tasks. DARLING addresses this by using a learned partition function to assess diversity, allowing the model to generate responses that are not only accurate but also unique. Experiments show that DARLING outperforms standard reinforcement learning approaches, leading to better quality and more diverse outputs across various tasks."
                },
                "zh": {
                    "title": "DARLING：提升语言模型输出的质量与多样性",
                    "desc": "DARLING是一个关注多样性的强化学习框架，旨在提高大型语言模型在各种任务中的输出质量和多样性。传统的后训练方法往往只关注准确性和实用性，导致输出的多样性降低。DARLING通过引入学习的分区函数来衡量多样性，并在在线强化学习中结合质量奖励，鼓励模型生成高质量且独特的输出。实验结果表明，DARLING在非可验证和可验证任务中均表现优异，生成的输出在质量和新颖性上均优于仅关注质量的基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01644",
            "title": "OpenVision 2: A Family of Generative Pretrained Visual Encoders for\n  Multimodal Learning",
            "url": "https://huggingface.co/papers/2509.01644",
            "abstract": "OpenVision 2 simplifies the original architecture by removing the text encoder and contrastive loss, achieving competitive performance with reduced training time and memory usage, and enabling scaling to over 1 billion parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper provides a simplification on OpenVision's architecture and loss design for enhancing its training efficiency. Following the prior vision-language pretraining works CapPa and AIMv2, as well as modern multimodal designs like LLaVA, our changes are straightforward: we remove the text encoder (and therefore the contrastive loss), retaining only the captioning loss as a purely generative training signal. We name this new version OpenVision 2. The initial results are promising: despite this simplification, OpenVision 2 competitively matches the original model's performance on a broad set of multimodal benchmarks while substantially cutting both training time and memory consumption. For example, with ViT-L/14, it reduces training time by about 1.5x (from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB, equivalently allowing the maximum batch size to grow from 2k to 8k). This superior training efficiency also allows us to scale far beyond the largest vision encoder used in OpenVision, reaching more than 1 billion parameters. We hold a strong belief that this lightweight, generative-only paradigm is compelling for future vision encoder development in multimodal foundation models.",
            "score": 8,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "0197aa14702197c7",
            "authors": [
                "Yanqing Liu",
                "Xianhang Li",
                "Letian Zhang",
                "Zirui Wang",
                "Zeyu Zheng",
                "Yuyin Zhou",
                "Cihang Xie"
            ],
            "affiliations": [
                "Apple",
                "University of California Berkeley",
                "University of California Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01644.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Упрощение архитектуры для эффективного обучения мультимодальных моделей",
                    "desc": "OpenVision 2 представляет собой упрощенную версию архитектуры OpenVision, в которой удален текстовый энкодер и контрастивная функция потерь. Эта модификация позволяет значительно сократить время обучения и потребление памяти, сохраняя при этом конкурентоспособную производительность на различных мультимодальных бенчмарках. Модель использует только функцию потерь для генерации подписей к изображениям в качестве сигнала обучения. Благодаря повышенной эффективности обучения, OpenVision 2 может масштабироваться до более чем 1 миллиарда параметров."
                },
                "en": {
                    "title": "Streamlined Efficiency: OpenVision 2 Reimagined",
                    "desc": "OpenVision 2 is a streamlined version of the original OpenVision architecture that eliminates the text encoder and contrastive loss, focusing solely on a generative training approach with captioning loss. This simplification leads to significant improvements in training efficiency, reducing both training time and memory usage while maintaining competitive performance on multimodal benchmarks. For instance, it achieves a 1.5x reduction in training time and a 1.8x decrease in memory consumption, allowing for larger batch sizes. The architecture's ability to scale to over 1 billion parameters suggests a promising direction for future multimodal foundation models."
                },
                "zh": {
                    "title": "简化架构，提升效率——OpenVision 2",
                    "desc": "OpenVision 2通过去除文本编码器和对比损失，简化了原有架构，从而提高了训练效率。该模型仅保留了生成性训练信号的字幕损失，表现出与原始模型相当的性能。尽管进行了简化，OpenVision 2在多模态基准测试中仍然表现出色，同时显著减少了训练时间和内存消耗。我们相信，这种轻量级的生成性范式对未来多模态基础模型的发展具有重要意义。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02040",
            "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm\n  Simulators for Conditional Synthetic Data Generation",
            "url": "https://huggingface.co/papers/2509.02040",
            "abstract": "Genetic Prompt enhances synthetic data quality and diversity in NLP by combining genetic algorithms with LLMs, improving downstream model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains challenging. We propose Genetic Prompt, a novel framework that combines genetic algorithms with LLMs to augment synthetic data generation. Our approach treats semantic text attributes as gene sequences and leverages the LLM to simulate crossover and mutation operations. This genetic process enhances data quality and diversity by creating novel attribute combinations, yielding synthetic distributions closer to real-world data. To optimize parent selection, we also integrate an active learning scheme that expands the offspring search space. Our experiments on multiple NLP tasks reveal several key findings: Genetic Prompt not only significantly outperforms state-of-the-art baselines but also shows robust performance across various generator model sizes and scales. Moreover, we demonstrate that fusing our synthetic data with the original training set significantly boosts downstream model performance, particularly for class-imbalanced scenarios. Our findings validate that Genetic Prompt is an effective method for producing high-quality synthetic data for a wide range of NLP applications.",
            "score": 7,
            "issue_id": 5686,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "085dff767b3e88ce",
            "authors": [
                "Guangzeng Han",
                "Weisi Liu",
                "Xiaolei Huang"
            ],
            "affiliations": [
                "Department of Computer Science, University of Memphis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02040.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#synthetic",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Генетический подход к созданию качественных синтетических данных для NLP",
                    "desc": "Genetic Prompt - это новый метод улучшения качества и разнообразия синтетических данных в обработке естественного языка. Он комбинирует генетические алгоритмы с большими языковыми моделями для создания более реалистичных наборов данных. Метод рассматривает семантические атрибуты текста как генетические последовательности и использует языковую модель для симуляции операций скрещивания и мутации. Эксперименты показали, что Genetic Prompt превосходит современные базовые методы и значительно улучшает производительность моделей машинного обучения, особенно в сценариях с несбалансированными классами."
                },
                "en": {
                    "title": "Genetic Prompt: Evolving Synthetic Data for NLP Excellence",
                    "desc": "The paper introduces Genetic Prompt, a framework that enhances the quality and diversity of synthetic data in natural language processing (NLP) by integrating genetic algorithms with large language models (LLMs). It treats semantic text attributes as gene sequences, applying genetic operations like crossover and mutation to generate diverse data combinations. This method improves the synthetic data's resemblance to real-world distributions, which is crucial for training effective models. Additionally, the framework incorporates an active learning strategy to optimize the selection of parent data, leading to better performance in various NLP tasks, especially in scenarios with class imbalance."
                },
                "zh": {
                    "title": "遗传提示：提升合成数据质量与多样性",
                    "desc": "Genetic Prompt是一种新颖的框架，通过将遗传算法与大型语言模型（LLMs）结合，提升了自然语言处理中的合成数据质量和多样性。该方法将语义文本属性视为基因序列，并利用LLM模拟交叉和突变操作，从而生成新的属性组合。通过这种遗传过程，合成数据的分布更接近真实世界的数据，优化了下游模型的性能。实验结果表明，Genetic Prompt在多个NLP任务中显著优于现有的基准方法，尤其在类别不平衡的情况下，融合合成数据与原始训练集能显著提升模型表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01360",
            "title": "M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via\n  Self-Supervision",
            "url": "https://huggingface.co/papers/2509.01360",
            "abstract": "M3Ret, a unified visual encoder trained on a large-scale hybrid-modality dataset, achieves state-of-the-art zero-shot image-to-image retrieval and cross-modal alignment using generative and contrastive self-supervised learning paradigms.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate a large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, a unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets a new state-of-the-art in zero-shot image-to-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver a promising signal to the medical imaging community, positioning M3Ret as a step toward foundation models for visual SSL in multimodal medical image understanding.",
            "score": 7,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "3dcca4eb6e49e24e",
            "authors": [
                "Che Liu",
                "Zheng Jiang",
                "Chengyu Fang",
                "Heng Guo",
                "Yan-Jie Zhou",
                "Jiaqi Qu",
                "Le Lu",
                "Minfeng Xu"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Imperial College London",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01360.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#transfer_learning",
                    "#optimization",
                    "#healthcare"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Единый энкодер для мультимодальных медицинских изображений",
                    "desc": "M3Ret - это унифицированный визуальный энкодер, обученный на крупномасштабном наборе данных с гибридными модальностями. Он достигает наилучших результатов в задаче поиска изображений по изображению с нулевым обучением и кросс-модальном выравнивании, используя генеративные и контрастивные парадигмы самообучения. M3Ret обучается на наборе данных из 867,653 медицинских изображений, включая 2D рентгеновские снимки, УЗИ, RGB видео эндоскопии и 3D КТ-сканы. Модель превосходит сильные базовые линии, такие как DINOv3 и BMC-CLIP, обученный с использованием текстовой разметки."
                },
                "en": {
                    "title": "Unified Learning for Enhanced Medical Image Retrieval",
                    "desc": "M3Ret is a unified visual encoder designed to improve medical image retrieval by using a large dataset that includes various types of medical images. It employs generative and contrastive self-supervised learning techniques to create transferable visual representations without needing separate models for different image modalities. This approach allows M3Ret to excel in zero-shot image-to-image retrieval and cross-modal alignment, even with unseen data like MRI scans. The results indicate that M3Ret can effectively generalize across different medical imaging tasks, paving the way for more integrated solutions in medical image analysis."
                },
                "zh": {
                    "title": "M3Ret：统一的医学影像检索新突破",
                    "desc": "M3Ret是一种统一的视觉编码器，使用大规模混合模态数据集进行训练，能够在零样本图像检索和跨模态对齐方面达到最先进的水平。该方法结合了生成式和对比自监督学习，克服了传统方法在2D、3D和视频医学数据上分散的局限性。通过构建867,653个医学影像样本的数据集，M3Ret能够学习可迁移的视觉表示，且无需特定模态的定制。研究结果表明，M3Ret在各个模态的零样本图像检索中超越了现有的强基线，展示了其在医学影像理解中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02460",
            "title": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
            "url": "https://huggingface.co/papers/2509.02460",
            "abstract": "A novel Diffusion Transformer pipeline automates video compositing by adaptively injecting identity and motion information, maintaining consistency and enabling user customization.  \t\t\t\t\tAI-generated summary \t\t\t\t Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency.",
            "score": 6,
            "issue_id": 5688,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "cbe52bb6a0af85b6",
            "authors": [
                "Shuzhou Yang",
                "Xiaoyu Li",
                "Xiaodong Cun",
                "Guangzhi Wang",
                "Lingen Li",
                "Ying Shan",
                "Jian Zhang"
            ],
            "affiliations": [
                "ARC Lab, Tencent",
                "GVC Lab, Great Bay University",
                "SECE, Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02460.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Генеративный видеокомпозитинг: новый уровень автоматизации в производстве видео",
                    "desc": "Статья представляет новый подход к автоматизации видеокомпозитинга с использованием генеративных моделей. Авторы разработали пайплайн на основе Diffusion Transformer (DiT), который позволяет адаптивно внедрять информацию об идентичности и движении объектов в целевое видео. Система включает ветвь сохранения фона, блок слияния DiT и расширенное позиционное кодирование (ERoPE) для пользовательского контроля. Для обучения и оценки модели был создан датасет VideoComp, содержащий 61 тысячу наборов видео."
                },
                "en": {
                    "title": "Automating Video Compositing with Adaptive Diffusion Transformers",
                    "desc": "This paper presents a new method for automating video compositing using a Diffusion Transformer (DiT) pipeline. The approach allows for the adaptive injection of identity and motion information into videos, enabling user customization of dynamic elements. By incorporating a background preservation branch and a DiT fusion block, the method maintains video consistency while enhancing the integration of foreground and background elements. The authors also introduce a new dataset, VideoComp, to support their task, demonstrating that their method outperforms existing solutions in terms of fidelity and consistency."
                },
                "zh": {
                    "title": "自动化视频合成的新方法",
                    "desc": "这篇论文介绍了一种新颖的扩散变换器（Diffusion Transformer）管道，用于自动化视频合成。该方法通过自适应注入身份和运动信息，保持视频的一致性，并允许用户进行个性化定制。传统的视频合成需要大量人力和专业知识，而这种生成模型能够显著缩短制作周期，降低成本。实验结果表明，该方法在视频合成的保真度和一致性方面优于现有解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00244",
            "title": "Universal Deep Research: Bring Your Own Model and Strategy",
            "url": "https://huggingface.co/papers/2509.00244",
            "abstract": "Universal Deep Research (UDR) is a flexible system that allows users to customize deep research strategies using any language model without additional training or fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research tools are among the most impactful and most commonly encountered agentic systems today. We observe, however, that each deep research agent introduced so far is hard-coded to carry out a particular research strategy using a fixed choice of tools. We introduce Universal Deep Research (UDR), a generalist agentic system that wraps around any language model and enables the user to create, edit, and refine their own entirely custom deep research strategies without any need for additional training or finetuning. To showcase the generality of our system, we equip UDR with example minimal, expansive, and intensive research strategies, and provide a user interface to facilitate experimentation with the system.",
            "score": 4,
            "issue_id": 5685,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 августа",
                "en": "August 29",
                "zh": "8月29日"
            },
            "hash": "641998602f4f0d54",
            "authors": [
                "Peter Belcak",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "NVIDIA Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00244.jpg",
            "data": {
                "categories": [
                    "#agents"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Универсальный инструмент для настройки стратегий глубокого исследования",
                    "desc": "Universal Deep Research (UDR) - это гибкая система, позволяющая пользователям настраивать стратегии глубокого исследования с использованием любой языковой модели без дополнительного обучения или доводки. UDR обертывает языковую модель и дает возможность создавать, редактировать и улучшать пользовательские стратегии исследования. Система демонстрирует свою универсальность, предоставляя примеры минимальных, расширенных и интенсивных стратегий исследования. UDR также включает пользовательский интерфейс для удобства экспериментов с системой."
                },
                "en": {
                    "title": "Customize Your Research with Universal Deep Research!",
                    "desc": "Universal Deep Research (UDR) is a versatile system that empowers users to design personalized deep research strategies using any language model, eliminating the need for extra training or fine-tuning. Unlike previous deep research agents that are limited to specific strategies and tools, UDR allows for complete customization and flexibility. The system supports various research approaches, including minimal, expansive, and intensive strategies, demonstrating its adaptability. A user-friendly interface is provided to encourage experimentation and enhance the research process."
                },
                "zh": {
                    "title": "通用深度研究：自定义你的研究策略",
                    "desc": "通用深度研究（UDR）是一个灵活的系统，允许用户使用任何语言模型自定义深度研究策略，而无需额外的训练或微调。现有的深度研究工具通常是硬编码的，执行特定的研究策略并使用固定的工具选择。UDR作为一个通用的智能系统，可以围绕任何语言模型进行构建，使用户能够创建、编辑和完善自己的深度研究策略。我们还为UDR提供了示例研究策略，并提供用户界面以便于用户进行实验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01052",
            "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in\n  Diverse Adventure Games",
            "url": "https://huggingface.co/papers/2509.01052",
            "abstract": "FlashAdventure benchmark and COAST framework improve GUI agents' performance in completing full story arcs in Flash-based adventure games by addressing the observation-behavior gap.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.",
            "score": 3,
            "issue_id": 5686,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "909be5826c565f9d",
            "authors": [
                "Jaewoo Ahn",
                "Junseo Kim",
                "Heeseung Yun",
                "Jaehyeon Son",
                "Dongmin Park",
                "Jaewoong Cho",
                "Gunhee Kim"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "KRAFTON",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01052.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#optimization",
                    "#games"
                ],
                "emoji": "🕹️",
                "ru": {
                    "title": "Преодолевая разрыв между наблюдением и действием в играх-квестах",
                    "desc": "В статье представлен новый бенчмарк FlashAdventure для оценки агентов с графическим интерфейсом в Flash-играх жанра квест. Предложена система CUA-as-a-Judge для автоматической оценки игрового процесса. Разработан фреймворк COAST, улучшающий долгосрочную память агентов для планирования и решения последовательных задач. Эксперименты показали, что COAST повышает эффективность агентов, но разрыв с человеческими результатами все еще значителен."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing GUI Agents in Adventure Games",
                    "desc": "This paper introduces the FlashAdventure benchmark and the COAST framework to enhance the performance of GUI agents in completing full story arcs in Flash-based adventure games. The research addresses the observation-behavior gap, which is the difficulty agents face in recalling and utilizing past gameplay information effectively. By providing a diverse set of 34 adventure games, the benchmark allows for a more comprehensive evaluation of agents' abilities to navigate complex narratives. The COAST framework incorporates long-term memory strategies, leading to improved task planning and milestone completion, although a significant performance gap between human players and agents remains."
                },
                "zh": {
                    "title": "提升游戏代理的故事情节完成能力",
                    "desc": "本文介绍了FlashAdventure基准和COAST框架，旨在提高图形用户界面（GUI）代理在完成Flash冒险游戏完整故事情节方面的表现。研究指出，现有的游戏基准缺乏多样性，且很少评估代理完成整个故事线的能力。为了解决观察-行为差距的问题，FlashAdventure基准包含34款Flash冒险游戏，专注于测试完整故事情节的完成。COAST框架通过利用长期线索记忆，帮助代理更好地规划和解决顺序任务，从而提高了里程碑的完成率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01984",
            "title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image\n  Editing",
            "url": "https://huggingface.co/papers/2509.01984",
            "abstract": "VARIN, a noise inversion-based editing technique for visual autoregressive models, enables precise image editing aligned with textual prompts while preserving original details.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach.",
            "score": 2,
            "issue_id": 5686,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "b7fe3e5a42fa90e9",
            "authors": [
                "Quan Dao",
                "Xiaoxiao He",
                "Ligong Han",
                "Ngan Hoai Nguyen",
                "Amin Heyrani Nobar",
                "Faez Ahmed",
                "Han Zhang",
                "Viet Anh Nguyen",
                "Dimitris Metaxas"
            ],
            "affiliations": [
                "MIT",
                "Qualcomm AI Research",
                "Red Hat AI Innovation",
                "ReveAI",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01984.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🖌️",
                "ru": {
                    "title": "VARIN: Точное редактирование изображений с помощью инверсии шума для VAR-моделей",
                    "desc": "VARIN - это новый метод редактирования изображений для визуальных авторегрессионных моделей (VAR). Он использует инверсию шума для точного редактирования изображений в соответствии с текстовыми подсказками. VARIN применяет псевдообратную функцию для argmax-сэмплирования, названную Location-aware Argmax Inversion (LAI), для генерации обратных шумов Гумбеля. Эксперименты показывают, что VARIN эффективно изменяет исходные изображения согласно заданным подсказкам, сохраняя при этом оригинальные детали фона и структуры."
                },
                "en": {
                    "title": "Precise Image Editing with VARIN: Text Meets Visuals",
                    "desc": "This paper presents VARIN, a novel editing technique for visual autoregressive models that allows for precise image modifications based on textual prompts. VARIN utilizes a unique method called Location-aware Argmax Inversion (LAI) to create inverse Gumbel noises, which help in reconstructing the original image while enabling targeted edits. The technique is designed to work without requiring additional training, making it practical for real-world applications. Experimental results show that VARIN successfully alters images according to prompts while maintaining important details and background structures."
                },
                "zh": {
                    "title": "VARIN：精准图像编辑的新方法",
                    "desc": "VARIN是一种基于噪声反演的编辑技术，专为视觉自回归模型设计，能够根据文本提示进行精确的图像编辑，同时保留原始细节。该技术利用了一种新颖的伪逆函数，称为位置感知的Argmax反演（LAI），生成逆Gumbel噪声。这些逆噪声使得源图像的重建更加精确，并支持与文本提示对齐的有针对性的可控编辑。实验结果表明，VARIN能够有效地根据指定提示修改源图像，同时显著保留原始背景和结构细节，验证了其作为实用编辑方法的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02379",
            "title": "MedDINOv3: How to adapt vision foundation models for medical image\n  segmentation?",
            "url": "https://huggingface.co/papers/2509.02379",
            "abstract": "MedDINOv3, a framework adapting DINOv3 with multi-scale token aggregation, achieves state-of-the-art performance in medical image segmentation by overcoming challenges in domain adaptation and backbone performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3.",
            "score": 1,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "bbf123f2c298e53a",
            "authors": [
                "Yuheng Li",
                "Yizhou Wu",
                "Yuxiang Lai",
                "Mingzhe Hu",
                "Xiaofeng Yang"
            ],
            "affiliations": [
                "Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta",
                "Department of Computer Science, Emory University, Atlanta",
                "Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta",
                "Department of Radiation Oncology, Emory University School of Medicine, Atlanta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02379.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset",
                    "#architecture",
                    "#transfer_learning",
                    "#optimization",
                    "#healthcare"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "MedDINOv3: Универсальный сегментатор медицинских изображений на основе фундаментальных моделей",
                    "desc": "MedDINOv3 - это новая архитектура для сегментации медицинских изображений, основанная на адаптации модели DINOv3. Она использует многомасштабную агрегацию токенов и предобучение на большом наборе КТ-снимков для преодоления разрыва между естественными и медицинскими изображениями. MedDINOv3 достигает высоких результатов на нескольких бенчмарках по сегментации, демонстрируя потенциал фундаментальных моделей компьютерного зрения в медицинской визуализации. Эта архитектура предлагает универсальный подход к сегментации различных органов и опухолей на КТ и МРТ-снимках."
                },
                "en": {
                    "title": "Revolutionizing Medical Image Segmentation with MedDINOv3",
                    "desc": "MedDINOv3 is a new framework that enhances the DINOv3 model for better medical image segmentation, particularly in CT and MRI scans. It addresses the challenges of adapting vision foundation models to medical imaging by using multi-scale token aggregation and domain-adaptive pretraining. This approach allows the model to learn robust features from a large dataset of CT images, improving its performance compared to traditional CNNs. As a result, MedDINOv3 achieves state-of-the-art results in various segmentation tasks, showcasing the effectiveness of using advanced vision models in the medical field."
                },
                "zh": {
                    "title": "MedDINOv3：医学图像分割的新突破",
                    "desc": "MedDINOv3是一个将DINOv3与多尺度标记聚合相结合的框架，旨在解决医学图像分割中的领域适应和主干网络性能问题。该框架通过在CT-3M数据集上进行领域自适应预训练，学习到强大的密集特征，从而提高了医学图像分割的准确性。MedDINOv3在四个分割基准测试中达到了或超过了当前的最佳性能，展示了视觉基础模型在医学图像分割中的潜力。该研究表明，视觉基础模型可以作为医学图像分割的统一主干。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02133",
            "title": "AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with\n  Knowledge Augmentation for Robust Constitutional Alignment of Language Models",
            "url": "https://huggingface.co/papers/2509.02133",
            "abstract": "Ambekar framework uses a Constitution-Aware Decoding Layer to mitigate caste and religious biases in LLM outputs through speculative decoding without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, leading to harmful or prejudiced outputs. In the Indian context, our empirical evaluations across a suite of models reveal that biases around caste and religion are particularly salient. Yet, most existing mitigation strategies are Western-centric and fail to address these local nuances. We propose AMBEDKAR, a framework inspired by the egalitarian vision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM outputs toward fairness, neutrality, and inclusion in line with Articles 14 to 17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the AI Constitution of India and applied only at inference time, without any parameter updates to the base model. We incorporate a speculative decoding algorithm that proactively reduces casteist and communal bias during generation. This mitigation layer operates directly within the decoding process, avoiding changes to model internals and lowering the computational and infrastructural costs associated with retraining. We reinterpret speculative decoding not merely as an efficiency tool but as a mechanism for fairness. In this framework, a Small Language Model (SLM) acts as a potentially biased generator, while a constitutionally guided Large Language Model (LLM) serves as the verifier. Rather than accelerating generation, the LLM enforces bias-robust trajectories in the SLM outputs. This inversion of roles gives rise to a fairness-by-speculation paradigm. Our approach yields an absolute reduction of bias up to 26.41 percent compared to baseline. Our source code, datasets, and results are available at https://anonymous.4open.science/r/AMBEDKAR-983B/",
            "score": 1,
            "issue_id": 5688,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "a82d3bc5f04bc839",
            "authors": [
                "Snehasis Mukhopadhyay",
                "Aryan Kasat",
                "Shivam Dubey",
                "Rahul Karthikeyan",
                "Dhruv Sood",
                "Vinija Jain",
                "Aman Chadha",
                "Amitava Das"
            ],
            "affiliations": [
                "Amazon GenAI",
                "Artificial Intelligence Institute, University of South Carolina",
                "BITS Pilani Goa",
                "DTU",
                "IIT Madras",
                "Indian Institute of Information Technology, Kalyani",
                "Meta AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02133.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multilingual",
                    "#ethics",
                    "#open_source",
                    "#alignment",
                    "#inference"
                ],
                "emoji": "🇮🇳",
                "ru": {
                    "title": "Конституционное декодирование для справедливых языковых моделей",
                    "desc": "Фреймворк AMBEDKAR предлагает новый подход к снижению предвзятости в выводах больших языковых моделей (LLM) в индийском контексте. Он использует слой декодирования, учитывающий конституцию, который применяется во время вывода без изменения параметров базовой модели. Метод включает алгоритм спекулятивного декодирования для проактивного уменьшения кастовой и религиозной предвзятости. Этот подход позволяет достичь абсолютного снижения предвзятости до 26.41% по сравнению с базовой линией."
                },
                "en": {
                    "title": "Fairness Through Constitution-Aware Decoding in AI",
                    "desc": "The Ambekar framework introduces a Constitution-Aware Decoding Layer to address caste and religious biases in Large Language Models (LLMs) without the need for retraining. It recognizes that existing bias mitigation strategies often overlook local contexts, particularly in India, where such biases are pronounced. By employing a speculative decoding algorithm, the framework actively reduces harmful outputs during the generation process. This innovative approach not only enhances fairness in AI outputs but also demonstrates a significant reduction in bias, achieving up to 26.41 percent less bias compared to traditional methods."
                },
                "zh": {
                    "title": "公平与中立：Ambekar框架的创新之路",
                    "desc": "Ambekar框架通过引入一个宪法意识解码层，旨在减少大型语言模型（LLM）输出中的种姓和宗教偏见，而无需重新训练模型。该方法在推理阶段应用，利用投机解码算法主动降低生成过程中的偏见。与传统的偏见缓解策略不同，Ambekar框架专注于印度特有的社会背景，确保输出的公平性和中立性。通过这种方式，我们实现了相较于基线模型高达26.41%的偏见绝对减少。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01610",
            "title": "Improving Large Vision and Language Models by Learning from a Panel of\n  Peers",
            "url": "https://huggingface.co/papers/2509.01610",
            "abstract": "A Panel-of-Peers learning framework enhances Large Vision and Language Models by simulating peer reviews, improving performance without extensive human-labeled datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human-curated preference data. Human-generated preference data is costly; machine-generated preference data is limited in quality; and self-supervised preference data often introduces hallucinations. To overcome these limitations, we propose a novel Panel-of-Peers learning framework inspired by collaborative learning among humans. This approach leverages a panel of LVLMs, each evaluating and learning from their collective outputs through an iterative self-improvement process. By simulating a peer review system, our models generate, assess, and refine outputs in response to a curated set of prompts, mimicking a classroom learning environment. We demonstrate that this methodology enhances model performance without requiring extensive human-labeled datasets. Our experiments show significant improvement across multiple benchmarks, demonstrating the potential of peer evaluations as a scalable alternative to self-supervised alignment. Notably, we show that Panel-of-Peers increases the average score on fifteen benchmarks from 48% to 57%",
            "score": 1,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "3edfce8f16cf09cb",
            "authors": [
                "Jefferson Hernandez",
                "Jing Shi",
                "Simon Jenni",
                "Vicente Ordonez",
                "Kushal Kafle"
            ],
            "affiliations": [
                "Adobe Research",
                "Rice University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01610.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#rlhf",
                    "#hallucinations",
                    "#alignment"
                ],
                "emoji": "👥",
                "ru": {
                    "title": "Коллективное обучение ИИ: имитация процесса рецензирования для улучшения языково-визуальных моделей",
                    "desc": "Предложена новая методика обучения больших языково-визуальных моделей (LVLM) под названием Panel-of-Peers. Она имитирует процесс рецензирования, позволяя моделям оценивать и учиться на коллективных результатах друг друга. Этот подход улучшает производительность моделей без необходимости в обширных наборах данных с человеческими разметками. Эксперименты показали значительное улучшение результатов на нескольких бенчмарках, демонстрируя потенциал взаимных оценок как масштабируемой альтернативы самообучению."
                },
                "en": {
                    "title": "Empowering Models through Peer Learning",
                    "desc": "The paper introduces a Panel-of-Peers learning framework that enhances Large Vision and Language Models (LVLMs) by simulating peer reviews. This method addresses the challenges of relying on costly human-curated preference data and the limitations of machine-generated and self-supervised data. By allowing multiple LVLMs to evaluate and learn from each other's outputs, the framework fosters an iterative self-improvement process. The results show significant performance improvements across various benchmarks, highlighting the effectiveness of peer evaluations as a scalable alternative to traditional alignment methods."
                },
                "zh": {
                    "title": "同伴评审，提升模型性能的新方法",
                    "desc": "本文提出了一种新的学习框架，称为“同伴评审学习框架”，旨在提升大型视觉和语言模型（LVLMs）的性能。该框架通过模拟同伴评审的过程，使多个LVLM相互评估和学习，从而实现自我改进。与传统方法依赖昂贵的人类标注数据不同，这种方法能够在没有大量人类标注数据的情况下，显著提高模型的表现。实验结果表明，该框架在多个基准测试中显著提升了模型的平均得分，展示了同伴评审作为一种可扩展的自我监督对齐替代方案的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01363",
            "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task\n  Arithmetic",
            "url": "https://huggingface.co/papers/2509.01363",
            "abstract": "Reasoning capabilities from reinforcement learning can be extracted as a task vector and transferred to other models to improve performance on diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact task vector. We source two publicly available, identically initialized Qwen2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: v_{reason} = theta_{GRPO} - theta_{SFT}. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments.",
            "score": 1,
            "issue_id": 5688,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "1110b5cc006571ff",
            "authors": [
                "Mohammad Zbeeb",
                "Hasan Abed Al Kader Hammoud",
                "Bernard Ghanem"
            ],
            "affiliations": [
                "American University of Beirut (AUB)",
                "King Abdullah University of Science and Technology (KAUST)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01363.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#rl",
                    "#open_source",
                    "#transfer_learning",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Передача способностей к рассуждению между языковыми моделями",
                    "desc": "Исследование показывает, что способности к рассуждению, полученные с помощью обучения с подкреплением, можно извлечь в виде вектора задачи и передать другим моделям. Этот вектор рассуждений улучшает производительность на различных тестах, требующих логического мышления. Метод позволяет повторно использовать вычислительные ресурсы, затраченные на обучение существующих моделей. Простое арифметическое добавление вектора к совместимым моделям значительно повышает их способности к рассуждению."
                },
                "en": {
                    "title": "Transfer Reasoning Skills with Task Vectors!",
                    "desc": "This paper explores how reasoning skills learned through reinforcement learning can be extracted and reused in other models. The authors create a task vector that represents the reasoning capabilities gained from reinforcement learning, allowing it to be transferred to different models. By applying this vector to instruction-tuned models, they achieve significant performance improvements on various reasoning tasks. This approach demonstrates a cost-effective method to enhance model performance by leveraging previously acquired knowledge without the need for extensive retraining."
                },
                "zh": {
                    "title": "提取推理能力，提升模型表现",
                    "desc": "本研究探讨了如何从强化学习中提取推理能力，并将其作为任务向量转移到其他模型中，以提高在不同基准上的表现。我们使用两个相同初始化的Qwen2.5模型，一个经过监督微调（SFT），另一个经过群体相对策略优化（GRPO）。通过计算这两个模型的参数差异，我们提取了一个推理向量，该向量能够捕捉到强化学习所带来的推理能力。将这个向量添加到兼容的指令调优模型中，可以显著提高模型在多个推理基准上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01250",
            "title": "Towards More Diverse and Challenging Pre-training for Point Cloud\n  Learning: Self-Supervised Cross Reconstruction with Decoupled Views",
            "url": "https://huggingface.co/papers/2509.01250",
            "abstract": "Point-PQAE, a two-view cross-reconstruction generative paradigm, enhances 3D self-supervised learning by introducing greater diversity and variance, outperforming single-view methods in point cloud reconstruction tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Point cloud learning, especially in a self-supervised way without manual labels, has gained growing attention in both vision and learning communities due to its potential utility in a wide range of applications. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it may thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to surpass previous single-modal self-reconstruction methods in 3D self-supervised learning. Specifically, it outperforms the self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is available at https://github.com/aHapBean/Point-PQAE.",
            "score": 1,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "3fffc03e184237e0",
            "authors": [
                "Xiangdong Zhang",
                "Shaofeng Zhang",
                "Junchi Yan"
            ],
            "affiliations": [
                "School of AI, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01250.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#synthetic"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Двухракурсное самообучение улучшает 3D реконструкцию облаков точек",
                    "desc": "Статья представляет новый метод самообучения для трехмерных облаков точек под названием Point-PQAE. Этот подход использует двухракурсную парадигму обучения, которая создает два отдельных представления облака точек и затем реконструирует одно из другого. Метод включает новый механизм обрезки для генерации ракурсов облака точек и оригинальное позиционное кодирование для представления относительного положения двух ракурсов. Point-PQAE превосходит существующие одномодальные методы самообучения в задачах 3D реконструкции, показывая улучшение до 7% на наборе данных ScanObjectNN."
                },
                "en": {
                    "title": "Enhancing 3D Learning with Two-View Cross-Reconstruction",
                    "desc": "Point-PQAE is a novel approach in 3D self-supervised learning that utilizes a two-view cross-reconstruction method to enhance point cloud reconstruction tasks. By generating two separate point clouds and reconstructing one from the other, it introduces greater diversity and variance compared to traditional single-view methods. This method employs a unique crop mechanism for generating views and a new positional encoding to capture the 3D relationships between the views. As a result, Point-PQAE significantly outperforms existing self-reconstruction techniques, demonstrating improved performance in various evaluation scenarios."
                },
                "zh": {
                    "title": "双视图学习，提升3D自监督重建效果",
                    "desc": "本文提出了一种名为Point-PQAE的跨重建生成范式，旨在增强3D自监督学习。该方法通过引入双视图的学习方式，增加了点云重建任务中的多样性和方差，超越了单视图方法的表现。我们首次开发了一种点云视图生成的裁剪机制，并提出了一种新颖的位置编码来表示两个解耦视图之间的3D相对位置。实验结果表明，Point-PQAE在ScanObjectNN的三个变体中，分别比自重建基线（Point-MAE）提高了6.5%、7.0%和6.7%的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00581",
            "title": "SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction",
            "url": "https://huggingface.co/papers/2509.00581",
            "abstract": "A multi-agent framework decomposes the Text2SQL task into several components, using in-context learning and taxonomy-guided error modification to achieve state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting natural language queries into SQL queries is a crucial challenge in both industry and academia, aiming to increase access to databases and large-scale applications. This work examines how in-context learning and chain-of-thought can be utilized to develop a robust solution for text-to-SQL systems. We propose SQL-of-Thought: a multi-agent framework that decomposes the Text2SQL task into schema linking, subproblem identification, query plan generation, SQL generation, and a guided correction loop. Unlike prior systems that rely only on execution-based static correction, we introduce taxonomy-guided dynamic error modification informed by in-context learning. SQL-of-Thought achieves state-of-the-art results on the Spider dataset and its variants, combining guided error taxonomy with reasoning-based query planning.",
            "score": 1,
            "issue_id": 5688,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 августа",
                "en": "August 30",
                "zh": "8月30日"
            },
            "hash": "9c1785e2259e0c93",
            "authors": [
                "Saumya Chaturvedi",
                "Aman Chadha",
                "Laurent Bindschaedler"
            ],
            "affiliations": [
                "AWS GenAI Santa Clara, CA, USA",
                "Max Planck Institute for Software Systems Saarbrücken, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00581.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#reasoning",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Мультиагентный подход с обучением в контексте революционизирует Text2SQL",
                    "desc": "Статья представляет новый подход к решению задачи Text2SQL, используя мультиагентную систему. Авторы предлагают фреймворк SQL-of-Thought, который разбивает процесс на несколько этапов, включая связывание схемы, идентификацию подзадач и генерацию SQL-запросов. Система использует обучение в контексте и цепочку рассуждений для улучшения результатов. Благодаря таксономии-guided коррекции ошибок, фреймворк достигает state-of-the-art результатов на датасете Spider."
                },
                "en": {
                    "title": "Transforming Language to SQL with Intelligent Agents",
                    "desc": "This paper presents SQL-of-Thought, a multi-agent framework designed to improve the Text2SQL task by breaking it down into several key components. It leverages in-context learning and chain-of-thought reasoning to enhance the conversion of natural language queries into SQL queries. The framework includes processes such as schema linking, subproblem identification, query plan generation, and a dynamic error correction loop guided by a taxonomy. By integrating these elements, SQL-of-Thought achieves state-of-the-art performance on the Spider dataset, surpassing previous systems that relied solely on static correction methods."
                },
                "zh": {
                    "title": "SQL转换的新思路：多智能体框架",
                    "desc": "本文提出了一种名为SQL-of-Thought的多智能体框架，用于将自然语言查询转换为SQL查询。该框架将Text2SQL任务分解为多个组件，包括模式链接、子问题识别、查询计划生成、SQL生成和引导修正循环。与以往仅依赖静态执行修正的系统不同，我们引入了基于上下文学习的动态错误修改，结合了引导错误分类和推理基础的查询规划。SQL-of-Thought在Spider数据集及其变体上取得了最先进的结果，展示了其在文本到SQL系统中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00578",
            "title": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for\n  High-Fidelity Object Detection",
            "url": "https://huggingface.co/papers/2509.00578",
            "abstract": "Context-Aware Fusion enhances DiffusionDet by integrating global scene context with local features using cross-attention, improving performance on fine-grained object detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains",
            "score": 1,
            "issue_id": 5687,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 августа",
                "en": "August 30",
                "zh": "8月30日"
            },
            "hash": "191ffb7e3bd4bec6",
            "authors": [
                "Abdellah Zakaria Sellam",
                "Ilyes Benaissa",
                "Salah Eddine Bekhouche",
                "Abdenour Hadid",
                "Vito Renó",
                "Cosimo Distante"
            ],
            "affiliations": [
                "CNR-STIIMA, Institute of Intelligent Industrial Systems and Technologies for Advanced Manufacturing, c/o Campus Ecotekne, Via Monteroni, Lecce, 73100, LE, Italy",
                "Department of Innovation Engineering, University of Salento & Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy",
                "Department of Innovation Engineering, University of Salento, Via per Monteroni, Lecce, 73100, Lecce, Italy",
                "Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy",
                "Sorbonne University Abu Dhabi, UAE",
                "UPV/EHU, University of the Basque Country, Sebastian, 20018, Sebastian, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00578.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Контекстное слияние для точной детекции мелких объектов",
                    "desc": "Статья представляет новый метод Context-Aware Fusion (CAF) для улучшения модели DiffusionDet в задаче детекции мелких объектов. CAF использует механизм кросс-внимания для интеграции глобального контекста сцены с локальными признаками объектов. Это позволяет каждому предложению объекта учитывать полное понимание окружающей среды. Эксперименты показывают значительное улучшение результатов на бенчмарке CarDD по сравнению с современными моделями."
                },
                "en": {
                    "title": "Enhancing Object Detection with Context-Aware Fusion",
                    "desc": "This paper introduces Context-Aware Fusion (CAF) to enhance the DiffusionDet model for fine-grained object detection. CAF uses cross-attention mechanisms to combine global scene context with local features, addressing the limitations of local feature conditioning. By employing a dedicated encoder to capture environmental information, the model allows object proposals to utilize a broader understanding of the scene. Experimental results show that this approach significantly improves performance on the CarDD benchmark, setting new standards for context-aware object detection."
                },
                "zh": {
                    "title": "上下文感知融合提升细粒度物体检测",
                    "desc": "本文提出了一种名为上下文感知融合（Context-Aware Fusion, CAF）的方法，旨在提升DiffusionDet在细粒度物体检测任务中的表现。CAF通过交叉注意力机制，将全局场景上下文与局部特征相结合，从而克服了DiffusionDet在上下文依赖场景中的局部特征限制。该方法使用专门的编码器生成全局上下文，捕捉全面的环境信息，使每个物体提议能够关注场景级理解。实验结果表明，CAF在CarDD基准测试中显著超越了现有的最先进模型，为细粒度领域的上下文感知物体检测设立了新的性能基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00404",
            "title": "Metis: Training Large Language Models with Advanced Low-Bit Quantization",
            "url": "https://huggingface.co/papers/2509.00404",
            "abstract": "Metis addresses training instability in low-bit quantized large language models by using spectral decomposition, adaptive learning rates, and dual-range regularization to improve performance and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t This work identifies anisotropic parameter distributions as a fundamental barrier to training large language models (LLMs) with low-bit quantization: a few dominant singular values create wide numerical ranges that conflict with the inherent bias of block-wise quantization. This bias disproportionately preserves high-magnitude values while discarding smaller ones, causing training instability and low model performance. This work introduces Metis, a training framework that combines (i) spectral decomposition with random embedding to efficiently disentangle dominant from long-tail components, compressing broad distributions into quantization-friendly narrow ranges; (ii) adaptive learning rates in the spectral domain to amplify underrepresented directions and better capture diverse features critical for performance; and (iii) a dual-range regularizer that jointly constrains numerical precision and parameter range distribution, ensuring stable, unbiased low-bit training. With Metis, FP8 training surpasses FP32 baselines, and FP4 training achieves accuracy comparable to FP32, paving the way for robust and scalable LLM training under advanced low-bit quantization. The code implementation for Metis is available at: https://github.com/typename-yyf/Metis-quantization.",
            "score": 0,
            "issue_id": 5688,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 августа",
                "en": "August 30",
                "zh": "8月30日"
            },
            "hash": "122a54575f7764a1",
            "authors": [
                "Hengjie Cao",
                "Mengyi Chen",
                "Yifeng Yang",
                "Ruijun Huang",
                "Fang Dong",
                "Jixian Zhou",
                "Anrui Chen",
                "Mingzhi Dong",
                "Yujiang Wang",
                "Jinlong Hou",
                "Yuan Cheng",
                "Fan Wu",
                "Fan Yang",
                "Tun Lu",
                "Ning Gu",
                "Li Shang"
            ],
            "affiliations": [
                "Fudan University",
                "Huawei",
                "Oxford Suzhou Centre for Advanced Research",
                "Shanghai Innovation Institute",
                "University of Bath"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00404.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение LLM с низкой битностью",
                    "desc": "Статья представляет Metis - фреймворк для обучения больших языковых моделей (LLM) с низкобитной квантизацией. Метод использует спектральное разложение, адаптивные скорости обучения и двухдиапазонную регуляризацию для улучшения производительности и стабильности. Metis позволяет обучать модели с 8-битной точностью лучше, чем с 32-битной, а с 4-битной - сравнимо с 32-битной. Это открывает путь к масштабируемому обучению LLM с продвинутой низкобитной квантизацией."
                },
                "en": {
                    "title": "Metis: Stabilizing Low-Bit Training for Large Language Models",
                    "desc": "This paper presents Metis, a novel training framework designed to enhance the stability and performance of low-bit quantized large language models (LLMs). It addresses the issue of anisotropic parameter distributions that hinder effective training by utilizing spectral decomposition to separate dominant and minor components, allowing for better quantization. Additionally, Metis employs adaptive learning rates to focus on underrepresented features, improving model accuracy. The framework also incorporates a dual-range regularization technique to maintain numerical precision and ensure stable training, resulting in significant performance improvements over traditional FP32 training methods."
                },
                "zh": {
                    "title": "Metis：提升低比特量化模型训练稳定性与性能的创新框架",
                    "desc": "Metis是一个训练框架，旨在解决低比特量化大语言模型的训练不稳定性。它通过谱分解、适应性学习率和双范围正则化来提高模型的性能和稳定性。研究发现，参数分布的各向异性是低比特量化训练的主要障碍，导致训练不稳定和模型性能低下。Metis通过有效地分离主导成分和长尾成分，压缩分布范围，从而实现了更好的训练效果。"
                }
            }
        }
    ],
    "link_prev": "2025-09-02.html",
    "link_next": "2025-09-04.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "02.09",
        "en": "09/02",
        "zh": "9月2日"
    },
    "short_date_next": {
        "ru": "04.09",
        "en": "09/04",
        "zh": "9月4日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 4,
        "#benchmark": 11,
        "#agents": 7,
        "#cv": 4,
        "#rl": 10,
        "#rlhf": 4,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 7,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 3,
        "#training": 12,
        "#robotics": 0,
        "#agi": 2,
        "#games": 4,
        "#interpretability": 0,
        "#reasoning": 10,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 18,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 4,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}