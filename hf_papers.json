{
    "date": {
        "ru": "4 Ğ¸ÑĞ½Ñ",
        "en": "June 4",
        "zh": "6æœˆ4æ—¥"
    },
    "time_utc": "2025-06-04 17:11",
    "weekday": 2,
    "issue_id": 4125,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.24726",
            "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.24726",
            "abstract": "A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.",
            "score": 124,
            "issue_id": 4116,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "05f75b6123a35a65",
            "authors": [
                "Shelly Bensal",
                "Umar Jamil",
                "Christopher Bryant",
                "Melisa Russak",
                "Kiran Kamble",
                "Dmytro Mozolevskyi",
                "Muayad Ali",
                "Waseem AlShikh"
            ],
            "affiliations": [
                "Writer, Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24726.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#small_models",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»Ğµ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¿Ñ‹Ñ‚Ğ°ĞµÑ‚ÑÑ ĞµÑ‘ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ•ÑĞ»Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ°, Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Empowering Language Models Through Self-Reflection and Reinforcement Learning",
                    "desc": "This paper presents a novel approach to enhance large language models using self-reflection and reinforcement learning. The method encourages models to analyze their mistakes and generate self-reflective commentary, which is then used to improve subsequent task attempts. By rewarding successful outcomes that follow self-reflection, the model learns to perform better even with minimal feedback. Experimental results indicate significant performance improvements, particularly in smaller models, suggesting a promising direction for developing more effective language models."
                },
                "zh": {
                    "title": "è‡ªæˆ‘åæ€ä¸å¼ºåŒ–å­¦ä¹ æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è‡ªæˆ‘åæ€å’Œå¼ºåŒ–å­¦ä¹ æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨æ¨¡å‹å›ç­”é”™è¯¯æ—¶ï¼Œæ¿€åŠ±å…¶ç”Ÿæˆæ›´å¥½çš„è‡ªæˆ‘åæ€ï¼Œä»è€Œæé«˜è§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæ¨¡å‹åœ¨å¤±è´¥åç”Ÿæˆè‡ªæˆ‘åæ€çš„è¯„è®ºï¼›å…¶æ¬¡ï¼Œæ¨¡å‹åœ¨è€ƒè™‘è‡ªæˆ‘åæ€çš„æƒ…å†µä¸‹å†æ¬¡å°è¯•ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸­ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯å°å‹å¾®è°ƒæ¨¡å‹çš„è¡¨ç°è¶…è¿‡äº†åŒç±»æ›´å¤§æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02387",
            "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
            "url": "https://huggingface.co/papers/2506.02387",
            "abstract": "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VS-Bench comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents' ability to predict others' future moves and optimize for long-term objectives. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return. Extensive experiments of fourteen leading VLMs reveal a significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return. We further conduct in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io.",
            "score": 48,
            "issue_id": 4110,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "829f3546d3ac9345",
            "authors": [
                "Zelai Xu",
                "Zhexuan Xu",
                "Xiangmin Yi",
                "Huining Yuan",
                "Xinlei Chen",
                "Yi Wu",
                "Chao Yu",
                "Yu Wang"
            ],
            "affiliations": [
                "Beijing Zhongguancun Academy",
                "Shanghai Qi Zhi Institute",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02387.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#agents",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "VS-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "VS-Bench - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸, ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ 14 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ VLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Evaluating Strategic Reasoning in Multi-Agent Environments with VS-Bench",
                    "desc": "VS-Bench is a new benchmark created to test Vision Language Models (VLMs) in complex situations where multiple agents interact. Unlike previous benchmarks that focused on single agents or text-only tasks, VS-Bench evaluates how well VLMs can reason and make decisions in environments that include both visual and language elements. It features eight different scenarios that require agents to work together, compete, or navigate mixed motives, assessing their ability to predict actions and achieve long-term goals. The results show that current VLMs still have a long way to go, with significant gaps in their performance, highlighting the need for further research in this area."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„æˆ˜ç•¥æ¨ç†æ–°åŸºå‡†",
                    "desc": "VS-Benchæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤æ‚å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æˆ˜ç•¥æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚ä¸ç°æœ‰çš„å•æ™ºèƒ½ä½“æˆ–ä»…æ–‡æœ¬ç¯å¢ƒçš„åŸºå‡†ä¸åŒï¼ŒVS-Benchè€ƒè™‘äº†å¤šä¸ªæ™ºèƒ½ä½“åœ¨ä¸°å¯Œçš„è§†è§‰å’Œè¯­è¨€èƒŒæ™¯ä¸‹çš„äº’åŠ¨ã€‚è¯¥åŸºå‡†åŒ…æ‹¬å…«ä¸ªåŸºäºè§†è§‰çš„ç¯å¢ƒï¼Œæ¶µç›–åˆä½œã€ç«äº‰å’Œæ··åˆåŠ¨æœºçš„äº’åŠ¨ï¼Œè¯„ä¼°æ™ºèƒ½ä½“é¢„æµ‹ä»–äººæœªæ¥åŠ¨ä½œå’Œä¼˜åŒ–é•¿æœŸç›®æ ‡çš„èƒ½åŠ›ã€‚é€šè¿‡æ ‡å‡†åŒ–è¯„ä¼°ï¼ŒVS-Benchä¸ºæœªæ¥çš„æˆ˜ç•¥å¤šæ¨¡æ€æ™ºèƒ½ä½“ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03147",
            "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
            "url": "https://huggingface.co/papers/2506.03147",
            "abstract": "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.  \t\t\t\t\tAI-generated summary \t\t\t\t Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets.",
            "score": 46,
            "issue_id": 4110,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "34f1d96b37be24a1",
            "authors": [
                "Bin Lin",
                "Zongjian Li",
                "Xinhua Cheng",
                "Yuwei Niu",
                "Yang Ye",
                "Xianyi He",
                "Shenghai Yuan",
                "Wangbo Yu",
                "Shaodong Wang",
                "Yunyang Ge",
                "Yatian Pang",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Peng Cheng Laboratory",
                "Rabbitpre AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03147.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "UniWorld: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "UniWorld - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². UniWorld Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BAGEL Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "UniWorld: Efficient Image Manipulation with Semantic Power",
                    "desc": "UniWorld is a new generative framework that enhances image perception and manipulation by utilizing semantic features from visual-language models. It outperforms the existing model BAGEL while using only 1% of its data, demonstrating efficiency in data usage. The framework leverages insights from the GPT-4o-Image model, which effectively uses semantic encoders instead of traditional Variational Autoencoders (VAEs). UniWorld not only excels in image editing tasks but also maintains strong capabilities in image understanding and generation across various applications."
                },
                "zh": {
                    "title": "UniWorldï¼šå›¾åƒæ„ŸçŸ¥ä¸æ“ä½œçš„æ–°çºªå…ƒ",
                    "desc": "UniWorldæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç‰¹å¾æ¥è¿›è¡Œå›¾åƒæ„ŸçŸ¥å’Œæ“ä½œã€‚ä¸BAGELç›¸æ¯”ï¼ŒUniWorldåœ¨æ•°æ®ä½¿ç”¨ä¸Šå‡å°‘äº†90%ï¼Œä½†åœ¨å›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä¼˜ã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢ä¿æŒç«äº‰åŠ›ï¼Œè¿˜åœ¨å¤šä¸ªå›¾åƒæ„ŸçŸ¥ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½çš„æ•ˆæœã€‚æˆ‘ä»¬å°†æ¨¡å‹æƒé‡ã€è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬ä»¥åŠæ•°æ®é›†å…¨éƒ¨å¼€æºï¼Œæ–¹ä¾¿ç ”ç©¶è€…ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02096",
            "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
            "url": "https://huggingface.co/papers/2506.02096",
            "abstract": "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) trained via reinforcement learning with verifiable reward (RLVR) have shown notable progress in scaling test-time compute effectively. In this work, we investigate how synthesized RL data can further improve RLVR. To this end, we propose SynthRL-a scalable and guaranteed pipeline for automatic data scaling in reasoning-oriented RL training. SynthRL comprises three key stages: (1) selecting seed questions with appropriate distribution, (2) augmenting them into more challenging variants while preserving the original answers, and (3) a guaranteed verification stage that ensures near-perfect correctness and difficulty enhancement. Our empirical experiments demonstrate SynthRL's scalability and effectiveness. When applied to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable, challenging questions from approximately 8K seed samples. Models trained with our synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with a significant improvement over baseline models trained on seed data alone. Notably, detailed analysis reveals that the gains are more pronounced on the most challenging evaluation samples, highlighting SynthRL's effectiveness in eliciting deeper and more complex reasoning patterns.",
            "score": 42,
            "issue_id": 4110,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "9ea84a7af081829e",
            "authors": [
                "Zijian Wu",
                "Jinjie Ni",
                "Xiangyan Liu",
                "Zichen Liu",
                "Hang Yan",
                "Michael Qizhe Shieh"
            ],
            "affiliations": [
                "National University of Singapore",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02096.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#synthetic",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SynthRL: Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "SynthRL - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. SynthRL Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°: Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ¸Ğµ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "SynthRL: Elevating Visual Math Reasoning with Scalable Data Synthesis",
                    "desc": "SynthRL is a novel pipeline designed to enhance reinforcement learning (RL) by synthesizing data with verifiable rewards, specifically for visual math reasoning tasks. It operates in three stages: selecting initial questions, creating more challenging variants while keeping the answers intact, and verifying the correctness of these questions. This approach allows for the generation of over 3,300 additional verifiable questions from a smaller set of seed samples, significantly improving model performance. Empirical results show that models trained with SynthRL's data outperform those trained only on seed data, especially on difficult reasoning tasks, demonstrating its effectiveness in fostering advanced reasoning capabilities."
                },
                "zh": {
                    "title": "SynthRLï¼šæå‡è§†è§‰æ•°å­¦æ¨ç†çš„æ™ºèƒ½åˆæˆç®¡é“",
                    "desc": "SynthRLæ˜¯ä¸€ç§å¯æ‰©å±•çš„æ•°æ®åˆæˆç®¡é“ï¼Œæ—¨åœ¨å¢å¼ºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰æ•°å­¦æ¨ç†æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§å’Œå¯éªŒè¯çš„é—®é¢˜ï¼Œæ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚SynthRLåŒ…æ‹¬ä¸‰ä¸ªå…³é”®é˜¶æ®µï¼šé€‰æ‹©åˆé€‚åˆ†å¸ƒçš„ç§å­é—®é¢˜ã€å°†å…¶å¢å¼ºä¸ºæ›´å…·æŒ‘æˆ˜æ€§çš„å˜ä½“ï¼Œå¹¶ç¡®ä¿ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œéš¾åº¦çš„æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SynthRLåˆæˆçš„æ•°æ®åœ¨å¤šä¸ªè§†è§‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æ ·æœ¬ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24120",
            "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs",
            "url": "https://huggingface.co/papers/2505.24120",
            "abstract": "A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remains inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering.Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning.We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6\\% accuracy.This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA.",
            "score": 42,
            "issue_id": 4116,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "09ca2aeec21b0156",
            "authors": [
                "Ai Jian",
                "Weijie Qiu",
                "Xiaokun Wang",
                "Peiyu Wang",
                "Yunzhuo Hao",
                "Jiangbo Pei",
                "Yichen Wei",
                "Yi Peng",
                "Xuchen Song"
            ],
            "affiliations": [
                "Kunlun Inc.",
                "Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24120.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "CSVQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CSVQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1378 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… STEM-Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑ†ĞµĞ½ĞºĞ° 15 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° CSVQA Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ 49.6%. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "CSVQA: Advancing Scientific Reasoning in Vision-Language Models",
                    "desc": "The paper introduces CSVQA, a new benchmark designed to evaluate the scientific reasoning abilities of vision-language models (VLMs) through domain-specific visual question answering. Unlike existing benchmarks that focus on generic image understanding, CSVQA emphasizes the integration of domain knowledge and visual evidence in STEM contexts. It includes 1,378 question-answer pairs that require higher-order reasoning and authentic scientific content. The evaluation of 15 VLMs on this benchmark reveals significant performance gaps, highlighting the need for improvements in their scientific reasoning capabilities."
                },
                "zh": {
                    "title": "CSVQAï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç§‘å­¦æ¨ç†èƒ½åŠ›",
                    "desc": "CSVQAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡é¢†åŸŸç‰¹å®šçš„è§†è§‰é—®ç­”ï¼Œå¼ºè°ƒäº†è¿™äº›æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†ä¸­çš„ä¸è¶³ã€‚CSVQAåŒ…å«1378ä¸ªç²¾å¿ƒæ„å»ºçš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–å¤šä¸ªSTEMå­¦ç§‘ï¼Œè¦æ±‚æ¨¡å‹æ•´åˆé¢†åŸŸçŸ¥è¯†å’Œè§†è§‰è¯æ®è¿›è¡Œé«˜é˜¶æ¨ç†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡æœ‰äº›æ¨¡å‹è¡¨ç°è¾ƒå¥½ï¼Œä½†æœ€é«˜åˆ†çš„æ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º49.6%ï¼Œè¿™è¡¨æ˜åœ¨ç§‘å­¦æ¨ç†èƒ½åŠ›ä¸Šä»éœ€è¿›ä¸€æ­¥æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00123",
            "title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces",
            "url": "https://huggingface.co/papers/2506.00123",
            "abstract": "VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable progress of Multimodal Large Language Models (MLLMs) has attracted increasing attention to extend them to physical entities like legged robot. This typically requires MLLMs to not only grasp multimodal understanding abilities, but also integrate visual-spatial reasoning and physical interaction capabilities. Nevertheless,existing methods struggle to unify these capabilities due to their fundamental differences.In this paper, we present the Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning, and control in real world. VeBrain reformulates robotic control into common text-based MLLM tasks in the 2D visual space, thus unifying the objectives and mapping spaces of different tasks. Then, a novel robotic adapter is proposed to convert textual control signals from MLLMs to motion policies of real robots. From the data perspective, we further introduce VeBrain-600k, a high-quality instruction dataset encompassing various capabilities of VeBrain. In VeBrain-600k, we take hundreds of hours to collect, curate and annotate the data, and adopt multimodal chain-of-thought(CoT) to mix the different capabilities into a single conversation. Extensive experiments on 13 multimodal benchmarks and 5 spatial intelligence benchmarks demonstrate the superior performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to legged robots and robotic arms, VeBrain shows strong adaptability, flexibility, and compositional capabilities compared to existing methods. For example, compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by +5.6%, but also excels in legged robot tasks with +50% average gains.",
            "score": 27,
            "issue_id": 4114,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "8914927f73dff147",
            "authors": [
                "Gen Luo",
                "Ganlin Yang",
                "Ziyang Gong",
                "Guanzhou Chen",
                "Haonan Duan",
                "Erfei Cui",
                "Ronglei Tong",
                "Zhi Hou",
                "Tianyi Zhang",
                "Zhe Chen",
                "Shenglong Ye",
                "Lewei Lu",
                "Jingbo Wang",
                "Wenhai Wang",
                "Jifeng Dai",
                "Yu Qiao",
                "Rongrong Ji",
                "Xizhou Zhu"
            ],
            "affiliations": [
                "Nanjing University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Science and Technology of China",
                "Xiamen University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00123.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#robotics",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "VeBrain: Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ·Ğ³ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "VeBrain - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ½Ğ¾Ğ³Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. ĞĞ½Ğ° Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² 2D Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. VeBrain Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "VeBrain: Unifying Multimodal Intelligence for Advanced Robotic Control",
                    "desc": "VeBrain is a new framework designed to enhance the capabilities of legged robots by combining multimodal understanding, visual-spatial reasoning, and physical interaction. It reformulates robotic control tasks into text-based tasks that can be processed by Multimodal Large Language Models (MLLMs), allowing for a unified approach to different robotic challenges. The framework includes a unique robotic adapter that translates textual commands from MLLMs into actionable motion policies for robots. Extensive testing shows that VeBrain outperforms existing models, achieving significant improvements in various benchmarks and demonstrating its adaptability in real-world robotic applications."
                },
                "zh": {
                    "title": "VeBrainï¼šå››è¶³æœºå™¨äººæ™ºèƒ½æ§åˆ¶çš„æ–°æ¡†æ¶",
                    "desc": "VeBrainæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤šæ¨¡æ€ç†è§£ã€è§†è§‰ç©ºé—´æ¨ç†å’Œç‰©ç†äº¤äº’æ•´åˆåˆ°å››è¶³æœºå™¨äººä¸­ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æœºå™¨äººæ§åˆ¶é‡æ–°è¡¨è¿°ä¸ºåŸºäºæ–‡æœ¬çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»»åŠ¡ï¼Œä»è€Œç»Ÿä¸€äº†ä¸åŒä»»åŠ¡çš„ç›®æ ‡å’Œæ˜ å°„ç©ºé—´ã€‚VeBrainè¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æœºå™¨äººé€‚é…å™¨ï¼Œå°†MLLMçš„æ–‡æœ¬æ§åˆ¶ä¿¡å·è½¬æ¢ä¸ºçœŸå®æœºå™¨äººçš„è¿åŠ¨ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒVeBrain-600kæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œæ¶µç›–äº†VeBrainçš„å¤šç§èƒ½åŠ›ï¼Œç»è¿‡å¤§é‡æ•°æ®æ”¶é›†å’Œæ³¨é‡Šï¼Œå±•ç¤ºäº†VeBrainåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24714",
            "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
            "url": "https://huggingface.co/papers/2505.24714",
            "abstract": "FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.",
            "score": 27,
            "issue_id": 4110,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "a6dcbb10b5be7f41",
            "authors": [
                "Junyu Luo",
                "Zhizhuo Kou",
                "Liming Yang",
                "Xiao Luo",
                "Jinsheng Huang",
                "Zhiping Xiao",
                "Jingshu Peng",
                "Chengzhong Liu",
                "Jiaming Ji",
                "Xuanzhe Liu",
                "Sirui Han",
                "Ming Zhang",
                "Yike Guo"
            ],
            "affiliations": [
                "HKUST",
                "School of Computer Science, Peking University",
                "State Key Laboratory for Multimedia Information Processing, PKU-Anker LLM Lab",
                "University of California, Los Angeles",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24714.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#science",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "FinMME Ğ¸ FinScore: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ",
                    "desc": "FinMME - ÑÑ‚Ğ¾ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 11 000 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸Ğ· 18 Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ° ĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹. FinScore - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4o Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° FinMME, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°."
                },
                "en": {
                    "title": "FinMME: Elevating Financial AI with Robust Evaluation",
                    "desc": "The paper introduces FinMME, a new multimodal dataset specifically designed for financial research, which includes over 11,000 samples across various financial domains and asset classes. It addresses the gap in specialized evaluation datasets for Multimodal Large Language Models (MLLMs) in finance. The authors also present FinScore, an evaluation system that assesses model performance while penalizing inaccuracies and measuring multiple capabilities. Experimental results reveal that even advanced models like GPT-4o struggle with the challenges posed by FinMME, underscoring the dataset's robustness and reliability."
                },
                "zh": {
                    "title": "æ¨åŠ¨é‡‘èé¢†åŸŸçš„å¤šæ¨¡æ€ç ”ç©¶",
                    "desc": "FinMMEæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€é‡‘èç ”ç©¶æ•°æ®é›†ï¼Œæ—¨åœ¨æ¨åŠ¨é‡‘èé¢†åŸŸçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘å±•ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡11,000ä¸ªé«˜è´¨é‡çš„é‡‘èç ”ç©¶æ ·æœ¬ï¼Œæ¶µç›–18ä¸ªé‡‘èé¢†åŸŸå’Œ6ç§èµ„äº§ç±»åˆ«ï¼Œæä¾›10ç§ä¸»è¦å›¾è¡¨ç±»å‹å’Œ21ç§å­ç±»å‹ã€‚ä¸ºäº†ç¡®ä¿æ•°æ®è´¨é‡ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨äº†20åæ³¨é‡Šå‘˜å’Œç²¾å¿ƒè®¾è®¡çš„éªŒè¯æœºåˆ¶ã€‚æ­¤å¤–ï¼ŒFinScoreè¯„ä¼°ç³»ç»Ÿå¼•å…¥äº†å¹»è§‰æƒ©ç½šå’Œå¤šç»´èƒ½åŠ›è¯„ä¼°ï¼Œä»¥æä¾›å…¬æ­£çš„è¯„ä¼°ç»“æœï¼Œå°½ç®¡å…ˆè¿›æ¨¡å‹å¦‚GPT-4oåœ¨FinMMEä¸Šè¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºå…¶æŒ‘æˆ˜æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03135",
            "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models",
            "url": "https://huggingface.co/papers/2506.03135",
            "abstract": "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning is a key aspect of cognitive psychology and remains a major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research.",
            "score": 26,
            "issue_id": 4116,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "a7cd11e4c04b1336",
            "authors": [
                "Mengdi Jia",
                "Zekun Qi",
                "Shaochen Zhang",
                "Wenyao Zhang",
                "Xinqiang Yu",
                "Jiawei He",
                "He Wang",
                "Li Yi"
            ],
            "affiliations": [
                "Galbot",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Shanghai Qi Zhi Institute",
                "Tsinghua University",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03135.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "OmniSpatial: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "OmniSpatial - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "OmniSpatial: Elevating Spatial Reasoning in Vision-Language Models",
                    "desc": "This paper introduces OmniSpatial, a new benchmark designed to assess the capabilities of vision-language models (VLMs) in advanced spatial reasoning tasks. It highlights that while VLMs can handle basic spatial relations, they struggle with more complex reasoning, which is crucial for understanding human-like spatial interactions. The benchmark includes four main categories of spatial reasoning, with a total of 50 detailed subcategories, and is supported by over 1,500 carefully crafted question-answer pairs. The findings reveal significant shortcomings in current VLMs, prompting a discussion on future research directions to enhance their spatial reasoning abilities."
                },
                "zh": {
                    "title": "OmniSpatialï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºOmniSpatialçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨é«˜çº§ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„ç†è§£èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€æ¨ç†ã€å¤æ‚ç©ºé—´é€»è¾‘ã€ç©ºé—´äº¤äº’å’Œè§†è§’è½¬æ¢ç­‰æ–¹é¢ã€‚OmniSpatialåŸºäºè®¤çŸ¥å¿ƒç†å­¦ï¼Œæ¶µç›–äº†50ä¸ªç»†åˆ†ç±»åˆ«ï¼Œå¹¶é€šè¿‡ç½‘ç»œæ•°æ®çˆ¬å–å’Œäººå·¥æ ‡æ³¨æ„å»ºäº†1500å¤šä¸ªé—®ç­”å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯å¼€æºè¿˜æ˜¯é—­æºçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œåœ¨å…¨é¢çš„ç©ºé—´ç†è§£ä¸Šéƒ½è¡¨ç°ä¸ä½³ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†å¤±è´¥æ¡ˆä¾‹å¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02397",
            "title": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation",
            "url": "https://huggingface.co/papers/2506.02397",
            "abstract": "OThink-R1 is introduced to reduce reasoning redundancy in complex problem-solving by classifying reasoning steps as essential or redundant and dynamically switching thinking modes based on task complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advanced large reasoning models (LRMs) leverage extended chain-of-thought (CoT) reasoning to solve complex tasks, achieving state-of-the-art performance. Despite their success, we identify a critical issue: a substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary. To address this, we systematically analyze the reasoning trajectories of LRMs and present a method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking) for straightforward problems while engaging in deliberate thinking (slow-thinking) for complex problems. Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23\\% on average without compromising accuracy, offering practical guidelines for efficient reasoning models. The code is available at https://github.com/AgenticIR-Lab/OThink-R1.",
            "score": 25,
            "issue_id": 4120,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "12cee2b297810e65",
            "authors": [
                "Shengjia Zhang",
                "Junjie Wu",
                "Jiawei Chen",
                "Changwang Zhang",
                "Xingyu Lou",
                "Wangchunshu Zhou",
                "Sheng Zhou",
                "Can Wang",
                "Jun Wang"
            ],
            "affiliations": [
                "OPPO Research Institute, Shenzhen, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02397.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "OThink-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. OThink-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¸ LLM-Judge Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OThink-R1 ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 23% Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Optimize Reasoning: Think Smart, Not Hard!",
                    "desc": "OThink-R1 is a novel approach designed to enhance reasoning efficiency in complex problem-solving by distinguishing between essential and redundant reasoning steps. It leverages a classification system to identify when complex reasoning is unnecessary, allowing for a switch between fast-thinking and slow-thinking modes based on task complexity. This method significantly reduces reasoning redundancy by approximately 23% while maintaining accuracy in tasks like mathematics and question-answering. The findings suggest that not all tasks require extensive reasoning, and OThink-R1 provides a framework for optimizing reasoning processes in large reasoning models."
                },
                "zh": {
                    "title": "åŠ¨æ€æ€ç»´æ¨¡å¼ï¼Œå‡å°‘æ¨ç†å†—ä½™",
                    "desc": "OThink-R1 æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å¤æ‚é—®é¢˜è§£å†³ä¸­çš„æ¨ç†å†—ä½™ã€‚å®ƒé€šè¿‡å°†æ¨ç†æ­¥éª¤åˆ†ç±»ä¸ºå¿…è¦æˆ–å†—ä½™ï¼Œå¹¶æ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€åˆ‡æ¢æ€ç»´æ¨¡å¼ã€‚å¯¹äºç®€å•é—®é¢˜ï¼ŒOThink-R1 ä½¿ç”¨å¿«é€Ÿæ€ç»´æ¨¡å¼ï¼Œè€Œå¯¹äºå¤æ‚é—®é¢˜åˆ™é‡‡ç”¨æ·±æ€ç†Ÿè™‘çš„æ…¢æ€ç»´æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒOThink-R1 å¹³å‡å‡å°‘äº†è¿‘23%çš„æ¨ç†å†—ä½™ï¼ŒåŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ï¼Œä¸ºé«˜æ•ˆæ¨ç†æ¨¡å‹æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03143",
            "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
            "url": "https://huggingface.co/papers/2506.03143",
            "abstract": "GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.",
            "score": 21,
            "issue_id": 4112,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "34d1779dffecf9d8",
            "authors": [
                "Qianhui Wu",
                "Kanzhi Cheng",
                "Rui Yang",
                "Chaoyun Zhang",
                "Jianwei Yang",
                "Huiqiang Jiang",
                "Jian Mu",
                "Baolin Peng",
                "Bo Qiao",
                "Reuben Tan",
                "Si Qin",
                "Lars Liden",
                "Qingwei Lin",
                "Huan Zhang",
                "Tong Zhang",
                "Jianbing Zhang",
                "Dongmei Zhang",
                "Jianfeng Gao"
            ],
            "affiliations": [
                "Microsoft",
                "Nanjing University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03143.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#games"
                ],
                "emoji": "ğŸ–±ï¸",
                "ru": {
                    "title": "GUI-Actor: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VLM",
                    "desc": "GUI-Actor - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ĞµĞ·ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. ĞĞ½ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. GUI-Actor Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ <ACTOR> ÑĞ¾ Ğ²ÑĞµĞ¼Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing GUI Grounding with Coordinate-Free Attention",
                    "desc": "The paper introduces GUI-Actor, a novel method for visual grounding in graphical user interfaces (GUIs) that leverages vision-language models (VLMs) and attention mechanisms. Unlike traditional approaches that generate screen coordinates, GUI-Actor operates in a coordinate-free manner, aligning an action token with relevant visual patches to identify action regions efficiently. This method addresses limitations such as weak spatial-semantic alignment and ambiguity in supervision targets, leading to better generalization across different screen layouts. The results demonstrate that GUI-Actor significantly outperforms existing methods, achieving state-of-the-art performance on various benchmarks while allowing for efficient fine-tuning."
                },
                "zh": {
                    "title": "GUI-Actorï¼šæ— åæ ‡çš„é«˜æ•ˆGUIå®šä½æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGUI-Actorçš„åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ— åæ ‡çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å®šä½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åŸºäºæ³¨æ„åŠ›çš„åŠ¨ä½œå¤´ï¼Œèƒ½å¤Ÿåœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­å°†ç‰¹å®šçš„<ACTOR>æ ‡è®°ä¸ç›¸å…³çš„è§†è§‰è¡¥ä¸æ ‡è®°å¯¹é½ï¼Œä»è€Œæœ‰æ•ˆåœ°æå‡ºä¸€ä¸ªæˆ–å¤šä¸ªåŠ¨ä½œåŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGUI-Actoråœ¨å¤šä¸ªGUIåŠ¨ä½œå®šä½åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„å±å¹•åˆ†è¾¨ç‡å’Œå¸ƒå±€ä¸Šå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥éªŒè¯å™¨ï¼ŒGUI-Actorèƒ½å¤Ÿåœ¨ä¿æŒVLMä¸»å¹²ä¸å˜çš„æƒ…å†µä¸‹ï¼Œä»…å¾®è°ƒæ–°å¼•å…¥çš„åŠ¨ä½œå¤´ï¼Œä¾¿èƒ½å®ç°ä¸ä¹‹å‰æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01674",
            "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs",
            "url": "https://huggingface.co/papers/2506.01674",
            "abstract": "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advancements in Multimodal Large Language Models (MLLMs), their proficiency in fine-grained video motion understanding remains critically limited. They often lack inter-frame differencing and tend to average or ignore subtle visual cues. Furthermore, while visual prompting has shown potential in static images, its application to video's temporal complexities, particularly for fine-grained motion understanding, remains largely unexplored. We investigate whether inherent capability can be unlocked and boost MLLMs' motion perception and enable distinct visual signatures tailored to decouple object and camera motion cues. In this study, we introduce MotionSight, a novel zero-shot method pioneering object-centric visual spotlight and motion blur as visual prompts to effectively improve fine-grained motion understanding without training. To convert this into valuable data assets, we curated MotionVid-QA, the first large-scale dataset for fine-grained video motion understanding, with hierarchical annotations including SFT and preference data, {\\Theta}(40K) video clips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models. In particular, for fine-grained motion understanding we present a novel zero-shot technique and a large-scale, high-quality dataset. All the code and annotations will be publicly available.",
            "score": 19,
            "issue_id": 4110,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "baebffe54058ae77",
            "authors": [
                "Yipeng Du",
                "Tiehan Fan",
                "Kepan Nan",
                "Rui Xie",
                "Penghao Zhou",
                "Xiang Li",
                "Jian Yang",
                "Zhenheng Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "ByteDance",
                "Nanjing University",
                "Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01674.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#open_source",
                    "#games"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ: MotionSight ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "MotionSight - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MotionVid-QA Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MotionSight Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Fine-Grained Motion Understanding with MotionSight",
                    "desc": "MotionSight is a novel zero-shot method designed to enhance fine-grained video motion understanding by utilizing object-centric visual prompts and motion blur. This approach addresses the limitations of Multimodal Large Language Models (MLLMs) in capturing subtle motion cues across video frames. By introducing a large-scale dataset called MotionVid-QA, which includes hierarchical annotations, the method allows for effective evaluation and improvement of motion perception without the need for extensive training. The results demonstrate that MotionSight achieves state-of-the-art performance, showcasing its potential to unlock advanced motion understanding capabilities in videos."
                },
                "zh": {
                    "title": "MotionSightï¼šæå‡è§†é¢‘è¿åŠ¨ç†è§£çš„æ–°æ–¹æ³•",
                    "desc": "MotionSightæ˜¯ä¸€ç§é›¶æ ·æœ¬æ–¹æ³•ï¼Œåˆ©ç”¨ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è§†è§‰èšç„¦å’Œè¿åŠ¨æ¨¡ç³Šä½œä¸ºæç¤ºï¼Œæ˜¾è‘—æå‡äº†ç»†ç²’åº¦è§†é¢‘è¿åŠ¨ç†è§£çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨MotionVid-QAæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯¥æ•°æ®é›†å…·æœ‰å±‚æ¬¡åŒ–çš„æ³¨é‡Šã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘è¿åŠ¨ç†è§£æ–¹é¢å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç»†å¾®çš„è§†è§‰çº¿ç´¢æ—¶ã€‚é€šè¿‡å¼•å…¥MotionSightï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨è§†è§‰æç¤ºæ¥è§£è€¦ç‰©ä½“å’Œç›¸æœºè¿åŠ¨çº¿ç´¢ï¼Œä»è€Œæé«˜è¿åŠ¨æ„ŸçŸ¥èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03065",
            "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2506.03065",
            "abstract": "Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical FLOP reduction, and actual inference speedups of 1.76times, 1.85times, and 1.58times, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis.",
            "score": 18,
            "issue_id": 4114,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "c51b4ca89c790b8c",
            "authors": [
                "Pengtao Chen",
                "Xianfang Zeng",
                "Maosen Zhao",
                "Peng Ye",
                "Mingzhu Shen",
                "Wei Cheng",
                "Gang Yu",
                "Tao Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Imperial College London",
                "StepFun",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03065.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Sparse-vDiT Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Video Diffusion Transformer (vDiT), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² ĞºĞ°Ñ€Ñ‚Ğ°Ñ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ğ¾Ğ»Ğ¾ÑĞ°Ñ‚Ñ‹Ğ¹. Sparse-vDiT Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Sparse-vDiT Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ vDiT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Accelerating Video Generation with Sparse Attention Patterns",
                    "desc": "The paper introduces Sparse-vDiT, a framework designed to enhance the efficiency of Video Diffusion Transformers (vDiT) by utilizing identified sparsity patterns in attention maps. By analyzing these patterns, the authors found that certain attention heads can be skipped, leading to reduced computational complexity and faster inference times. Sparse-vDiT employs pattern-optimized sparse kernels and an offline search algorithm to determine the best sparse computation strategy for each layer and head. This approach results in significant reductions in theoretical FLOPs and actual inference speedups while preserving high visual quality in generated videos."
                },
                "zh": {
                    "title": "åˆ©ç”¨ç¨€ç–æ€§åŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "Sparse-vDiTé€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›å›¾ä¸­çš„ç¨€ç–æ¨¡å¼ï¼ŒåŠ é€Ÿäº†è§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼ˆvDiTï¼‰ï¼Œåœ¨ä¸æ˜¾è‘—é™ä½è§†è§‰è´¨é‡çš„æƒ…å†µä¸‹ï¼Œå‡å°‘äº†ç†è®ºè®¡ç®—é‡ï¼ˆFLOPsï¼‰å¹¶æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚ç ”ç©¶å‘ç°ï¼ŒvDiTä¸­çš„æ³¨æ„åŠ›å›¾å­˜åœ¨å¯¹è§’çº¿ã€å¤šå¯¹è§’çº¿å’Œå‚ç›´æ¡çº¹ç­‰ä¸‰ç§ç¨€ç–æ¨¡å¼ï¼Œå¹¶ä¸”å¯ä»¥è·³è¿‡3-6%çš„æ³¨æ„åŠ›å¤´ã€‚æˆ‘ä»¬æå‡ºçš„Sparse-vDiTæ¡†æ¶åŒ…æ‹¬ä¼˜åŒ–ç¨€ç–å†…æ ¸å’Œç¦»çº¿ç¨€ç–æ‰©æ•£æœç´¢ç®—æ³•ï¼Œä»¥é€‰æ‹©æ¯å±‚å’Œæ¯ä¸ªå¤´çš„æœ€ä½³ç¨€ç–è®¡ç®—ç­–ç•¥ã€‚é€šè¿‡å°†Sparse-vDiTé›†æˆåˆ°æœ€å…ˆè¿›çš„vDiTæ¨¡å‹ä¸­ï¼Œå–å¾—äº†æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æå‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è§†è§‰ä¿çœŸåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23061",
            "title": "DINGO: Constrained Inference for Diffusion LLMs",
            "url": "https://huggingface.co/papers/2505.23061",
            "abstract": "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference",
            "score": 18,
            "issue_id": 4112,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "c215c7998d0a7928",
            "authors": [
                "Tarun Suresh",
                "Debangshu Banerjee",
                "Shubham Ugare",
                "Sasa Misailovic",
                "Gagandeep Singh"
            ],
            "affiliations": [
                "Department of Computer Science, University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23061.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "DINGO: Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DINGO - Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DINGO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ JSON. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ±Ğ»Ğ¾Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸. DINGO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ñ Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ, ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DINGO: Structuring Success in Diffusion Language Models",
                    "desc": "DINGO is a new decoding strategy that improves diffusion language models by applying structured output constraints. This method allows the models to generate outputs that meet specific requirements, like those found in symbolic math and JSON formats. Unlike traditional autoregressive models that generate text one token at a time, DINGO uses dynamic programming to efficiently handle multiple tokens simultaneously while ensuring the output adheres to user-defined rules. As a result, DINGO significantly enhances the performance of diffusion models, achieving up to a 68 percentage point improvement in relevant tasks."
                },
                "zh": {
                    "title": "DINGOï¼šæå‡æ‰©æ•£æ¨¡å‹çš„ç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›",
                    "desc": "DINGOæ˜¯ä¸€ç§åŸºäºåŠ¨æ€è§„åˆ’çš„è§£ç ç­–ç•¥ï¼Œæ—¨åœ¨å¢å¼ºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å¼ºåˆ¶æ‰§è¡Œç»“æ„åŒ–è¾“å‡ºçº¦æŸï¼Œæ˜¾è‘—æé«˜äº†åœ¨ç¬¦å·æ•°å­¦å’ŒJSONç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ä¸åŒï¼Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œé¢„æµ‹ä¸€ç»„æ ‡è®°ï¼Œè¿™ä½¿å¾—ä¼ ç»Ÿçš„çº¦æŸè§£ç ç®—æ³•æ— æ³•æœ‰æ•ˆåº”ç”¨ã€‚DINGOé€šè¿‡é«˜æ•ˆä¸”å¯è¯æ˜çš„åˆ†å¸ƒä¿æŒæ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºå­—ç¬¦ä¸²ç¬¦åˆç”¨æˆ·æŒ‡å®šçš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œä»è€Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00070",
            "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics",
            "url": "https://huggingface.co/papers/2506.00070",
            "abstract": "Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning.",
            "score": 16,
            "issue_id": 4114,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "0a2372063dd0aba7",
            "authors": [
                "Dongyoung Kim",
                "Sumin Park",
                "Huiwon Jang",
                "Jinwoo Shin",
                "Jaehyung Kim",
                "Younggyo Seo"
            ],
            "affiliations": [
                "KAIST",
                "Real World Inc.",
                "UC Berkeley",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00070.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Robot-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Robot-R1 Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Supervised Fine-Tuning (SFT), Robot-R1 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Robot-R1, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ SFT Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Robot Control with Robot-R1",
                    "desc": "This paper presents Robot-R1, a new framework that improves robot control by using reinforcement learning instead of traditional Supervised Fine-Tuning (SFT). SFT often suffers from issues like catastrophic forgetting and poorly constructed datasets, which can hinder robot performance. Robot-R1 focuses on predicting the next keypoint state needed for task completion by utilizing current scene images and expert demonstration data. The results show that Robot-R1 outperforms SFT methods, even with fewer parameters, in tasks requiring low-level action control and embodied reasoning."
                },
                "zh": {
                    "title": "Robot-R1ï¼šæå‡æœºå™¨äººæ§åˆ¶çš„å…·èº«æ¨ç†æ–°æ¡†æ¶",
                    "desc": "å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æœºå™¨äººæŠ€æœ¯ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œé€šè¿‡ç»“åˆå…·èº«æ¨ç†ä¸æœºå™¨äººæ§åˆ¶æ¥æ¨åŠ¨è¿›æ­¥ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨è®­ç»ƒæ—¶å¸¸å¸¸ä½¿ç”¨å¯å‘å¼æ„å»ºçš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¹¶æœªé’ˆå¯¹æœºå™¨äººæ§åˆ¶è¿›è¡Œä¼˜åŒ–ã€‚SFTæ–¹æ³•è¿˜å¯èƒ½å¯¼è‡´ç¾éš¾æ€§é—å¿˜å’Œæ³›åŒ–æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Robot-R1æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæœºå™¨äººæ§åˆ¶çš„å…·èº«æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03136",
            "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.03136",
            "abstract": "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE",
            "score": 15,
            "issue_id": 4110,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "6a5362c4ed28a3b0",
            "authors": [
                "Yinjie Wang",
                "Ling Yang",
                "Ye Tian",
                "Ke Shen",
                "Mengdi Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Peking University",
                "Princeton University",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03136.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#rl",
                    "#games"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CURE: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ²",
                    "desc": "CURE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ReasonFlux-Coder, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ CURE, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºÑƒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ° Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¼ĞµĞ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CURE Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "CURE: Evolving Code and Tests Together for Better Accuracy",
                    "desc": "CURE is a reinforcement learning framework designed to enhance the accuracy of code and unit test generation without relying on ground-truth supervision. It features a unique reward system that allows coding and testing processes to evolve together based on their interactions. This method enables the unit tester to learn from the coder's errors, leading to improved performance in various coding tasks. The framework's models, such as ReasonFlux-Coder, show significant accuracy improvements over existing models, making it a powerful tool for developers."
                },
                "zh": {
                    "title": "CUREï¼šæ— ç›‘ç£å¼ºåŒ–å­¦ä¹ æå‡ä»£ç ç”Ÿæˆä¸æµ‹è¯•å‡†ç¡®æ€§",
                    "desc": "CUREæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ä»£ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œè€Œæ— éœ€çœŸå®æ ‡ç­¾çš„ç›‘ç£ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡ä¸“é—¨çš„å¥–åŠ±æœºåˆ¶ï¼Œä½¿ç¼–ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆèƒ½åŠ›èƒ½å¤Ÿç›¸äº’æ¼”åŒ–ï¼Œä»è€Œç›´æ¥ä»ç¼–ç è€…çš„é”™è¯¯ä¸­å­¦ä¹ ã€‚ç»è¿‡ä¼˜åŒ–åï¼Œæˆ‘ä»¬çš„ReasonFlux-Coderæ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå‡†ç¡®æ€§ä¸Šæé«˜äº†5.3%ï¼Œåœ¨æœ€ä½³å‡†ç¡®æ€§ä¸Šæé«˜äº†9.0%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­è¡¨ç°å‡º64.8%çš„æ¨ç†æ•ˆç‡ï¼Œå±•ç°äº†å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03126",
            "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation",
            "url": "https://huggingface.co/papers/2506.03126",
            "abstract": "AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in AI-generated content (AIGC) have significantly accelerated animation production. To produce engaging animations, it is essential to generate coherent multi-shot video clips with narrative scripts and character references. However, existing public datasets primarily focus on real-world scenarios with global descriptions, and lack reference images for consistent character guidance. To bridge this gap, we present AnimeShooter, a reference-guided multi-shot animation dataset. AnimeShooter features comprehensive hierarchical annotations and strong visual consistency across shots through an automated pipeline. Story-level annotations provide an overview of the narrative, including the storyline, key scenes, and main character profiles with reference images, while shot-level annotations decompose the story into consecutive shots, each annotated with scene, characters, and both narrative and descriptive visual captions. Additionally, a dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each shot, along with audio descriptions and sound sources. To demonstrate the effectiveness of AnimeShooter and establish a baseline for the reference-guided multi-shot video generation task, we introduce AnimeShooterGen, which leverages Multimodal Large Language Models (MLLMs) and video diffusion models. The reference image and previously generated shots are first processed by MLLM to produce representations aware of both reference and context, which are then used as the condition for the diffusion model to decode the subsequent shot. Experimental results show that the model trained on AnimeShooter achieves superior cross-shot visual consistency and adherence to reference visual guidance, which highlight the value of our dataset for coherent animated video generation.",
            "score": 14,
            "issue_id": 4114,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "3cc5928482bd8262",
            "authors": [
                "Lu Qiu",
                "Yizhuo Li",
                "Yuying Ge",
                "Yixiao Ge",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03126.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#story_generation",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "AnimeShooter: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "AnimeShooter - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AnimeShooterGen, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AnimeShooterGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing Animation with Reference-Guided Multi-Shot Datasets",
                    "desc": "AnimeShooter is a new dataset designed to improve the generation of coherent multi-shot animations by providing detailed hierarchical annotations and ensuring visual consistency. It includes story-level annotations that outline the narrative and character profiles, as well as shot-level annotations that break down the story into individual scenes with specific details. The dataset also features a subset with synchronized audio tracks to enhance the animation experience. To utilize this dataset, AnimeShooterGen employs Multimodal Large Language Models (MLLMs) and video diffusion models, resulting in improved visual consistency and adherence to character references in generated animations."
                },
                "zh": {
                    "title": "æå‡åŠ¨ç”»ç”Ÿæˆçš„è¿è´¯æ€§ä¸ä¸€è‡´æ€§",
                    "desc": "AnimeShooteræ˜¯ä¸€ä¸ªå‚è€ƒå¼•å¯¼çš„å¤šé•œå¤´åŠ¨ç”»æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜è¿è´¯çš„åŠ¨ç”»è§†é¢‘ç”Ÿæˆã€‚è¯¥æ•°æ®é›†é€šè¿‡å…¨é¢çš„å±‚æ¬¡æ³¨é‡Šå’Œè§†è§‰ä¸€è‡´æ€§ï¼Œå¸®åŠ©ç”Ÿæˆå…·æœ‰å™äº‹è„šæœ¬å’Œè§’è‰²å‚è€ƒçš„åŠ¨ç”»ç‰‡æ®µã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†AnimeShooterGenï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå–å¾—äº†æ›´å¥½çš„ç”Ÿæˆæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºAnimeShooterè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨é•œå¤´è§†è§‰ä¸€è‡´æ€§å’Œéµå¾ªå‚è€ƒè§†è§‰æŒ‡å¯¼æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03131",
            "title": "Native-Resolution Image Synthesis",
            "url": "https://huggingface.co/papers/2506.03131",
            "abstract": "A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce native-resolution image synthesis, a novel generative modeling paradigm that enables the synthesis of images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of conventional fixed-resolution, square-image methods by natively handling variable-length visual tokens, a core challenge for traditional techniques. To this end, we introduce the Native-resolution diffusion Transformer (NiT), an architecture designed to explicitly model varying resolutions and aspect ratios within its denoising process. Free from the constraints of fixed formats, NiT learns intrinsic visual distributions from images spanning a broad range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced large language models, NiT, trained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536) and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies.",
            "score": 13,
            "issue_id": 4111,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "91e9b714c6e83924",
            "authors": [
                "Zidong Wang",
                "Lei Bai",
                "Xiangyu Yue",
                "Wanli Ouyang",
                "Yiyuan Zhang"
            ],
            "affiliations": [
                "MMLab, CUHK",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03131.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#cv",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "NiT: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ NiT (Native-resolution diffusion Transformer), ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. NiT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ImageNet Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ zero-shot Ğ½Ğ° Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ²Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Image Synthesis with Native-Resolution Modeling",
                    "desc": "The paper presents a new generative model called the Native-resolution diffusion Transformer (NiT) that can create high-resolution images with various aspect ratios. Unlike traditional models that work with fixed-size images, NiT can handle images of any size by using variable-length visual tokens. This model not only achieves top performance on standard image benchmarks but also shows impressive zero-shot generalization, meaning it can generate high-quality images at new resolutions without additional training. The findings suggest that NiT could significantly advance the field of image synthesis by integrating techniques from both visual generative modeling and large language models."
                },
                "zh": {
                    "title": "åŸç”Ÿåˆ†è¾¨ç‡å»ºæ¨¡ï¼šå›¾åƒç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆæ¨¡å‹â€”â€”åŸç”Ÿåˆ†è¾¨ç‡æ‰©æ•£å˜æ¢å™¨ï¼ˆNiTï¼‰ï¼Œå®ƒèƒ½å¤Ÿåˆæˆé«˜åˆ†è¾¨ç‡å’Œå¤šç§å®½é«˜æ¯”çš„å›¾åƒï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿå›ºå®šåˆ†è¾¨ç‡å›¾åƒæ–¹æ³•çš„å±€é™ï¼Œé€šè¿‡åŸç”Ÿå¤„ç†å¯å˜é•¿åº¦çš„è§†è§‰æ ‡è®°ï¼Œè§£å†³äº†ä¼ ç»ŸæŠ€æœ¯çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚NiTæ¶æ„ä¸“é—¨è®¾è®¡ç”¨äºåœ¨å»å™ªè¿‡ç¨‹ä¸­æ˜¾å¼å»ºæ¨¡ä¸åŒçš„åˆ†è¾¨ç‡å’Œå®½é«˜æ¯”ï¼Œèƒ½å¤Ÿä»å„ç§åˆ†è¾¨ç‡å’Œå®½é«˜æ¯”çš„å›¾åƒä¸­å­¦ä¹ å†…åœ¨çš„è§†è§‰åˆ†å¸ƒã€‚ç ”ç©¶è¡¨æ˜ï¼ŒNiTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œå±•ç¤ºäº†åŸç”Ÿåˆ†è¾¨ç‡å»ºæ¨¡åœ¨è§†è§‰ç”Ÿæˆå»ºæ¨¡å’Œå…ˆè¿›å¤§è¯­è¨€æ¨¡å‹æ–¹æ³•ä¹‹é—´çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02497",
            "title": "LumosFlow: Motion-Guided Long Video Generation",
            "url": "https://huggingface.co/papers/2506.02497",
            "abstract": "LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation has gained increasing attention due to its widespread applications in fields such as entertainment and simulation. Despite advances, synthesizing temporally coherent and visually compelling long sequences remains a formidable challenge. Conventional approaches often synthesize long videos by sequentially generating and concatenating short clips, or generating key frames and then interpolate the intermediate frames in a hierarchical manner. However, both of them still remain significant challenges, leading to issues such as temporal repetition or unnatural transitions. In this paper, we revisit the hierarchical long video generation pipeline and introduce LumosFlow, a framework introduce motion guidance explicitly. Specifically, we first employ the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames with larger motion intervals, thereby ensuring content diversity in the generated long videos. Given the complexity of interpolating contextual transitions between key frames, we further decompose the intermediate frame interpolation into motion generation and post-hoc refinement. For each pair of key frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes complex and large-motion optical flows, while MotionControlNet subsequently refines the warped results to enhance quality and guide intermediate frame generation. Compared with traditional video frame interpolation, we achieve 15x interpolation, ensuring reasonable and continuous motion between adjacent frames. Experiments show that our method can generate long videos with consistent motion and appearance. Code and models will be made publicly available upon acceptance. Our project page: https://jiahaochen1.github.io/LumosFlow/",
            "score": 13,
            "issue_id": 4111,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "c7f37e23b510618b",
            "authors": [
                "Jiahao Chen",
                "Hangjie Yuan",
                "Yichen Qian",
                "Jingyun Liang",
                "Jiazheng Xing",
                "Pengwei Liu",
                "Weihua Chen",
                "Fan Wang",
                "Bing Su"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Hupan Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02497.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "LumosFlow: Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²",
                    "desc": "LumosFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LMTV-DM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ—Ğ°Ñ‚ĞµĞ¼ LOF-DM ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ğ° MotionControlNet ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². LumosFlow Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 15-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "LumosFlow: Smooth Long Video Generation with Motion Guidance",
                    "desc": "LumosFlow is a novel framework designed for generating long videos with smooth transitions and coherent motion. It utilizes the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to create key frames that capture significant motion, enhancing the diversity of the video content. For the interpolation of frames between these key frames, it employs the Latent Optical Flow Diffusion Model (LOF-DM) to generate complex motion flows, followed by MotionControlNet for refining the results. This approach significantly improves the quality of long video generation, achieving 15 times faster interpolation while maintaining consistent motion and appearance throughout the video."
                },
                "zh": {
                    "title": "LumosFlowï¼šé«˜æ•ˆç”Ÿæˆè¿è´¯é•¿è§†é¢‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "LumosFlow æ˜¯ä¸€ä¸ªç”¨äºé•¿è§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œé‡‡ç”¨ LMTV-DM ç”Ÿæˆå…³é”®å¸§ï¼Œå¹¶ä½¿ç”¨ LOF-DM å’Œ MotionControlNet è¿›è¡Œå¹³æ»‘çš„ä¸­é—´å¸§æ’å€¼ã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼å¼•å…¥è¿åŠ¨æŒ‡å¯¼ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶é¢ä¸´çš„æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰è¿è´¯æ€§é—®é¢˜ã€‚LumosFlow é€šè¿‡ç”Ÿæˆå…·æœ‰è¾ƒå¤§è¿åŠ¨é—´éš”çš„å…³é”®å¸§ï¼Œç¡®ä¿äº†ç”Ÿæˆè§†é¢‘å†…å®¹çš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸€è‡´è¿åŠ¨å’Œå¤–è§‚çš„é•¿è§†é¢‘ï¼Œä¸”æ’å€¼é€Ÿåº¦æ¯”ä¼ ç»Ÿæ–¹æ³•å¿« 15 å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02528",
            "title": "RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers",
            "url": "https://huggingface.co/papers/2506.02528",
            "abstract": "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Inspired by the in-context learning mechanism of large language models (LLMs), a new paradigm of generalizable visual prompt-based image editing is emerging. Existing single-reference methods typically focus on style or appearance adjustments and struggle with non-rigid transformations. To address these limitations, we propose leveraging source-target image pairs to extract and transfer content-aware editing intent to novel query images. To this end, we introduce RelationAdapter, a lightweight module that enables Diffusion Transformer (DiT) based models to effectively capture and apply visual transformations from minimal examples. We also introduce Relation252K, a comprehensive dataset comprising 218 diverse editing tasks, to evaluate model generalization and adaptability in visual prompt-driven scenarios. Experiments on Relation252K show that RelationAdapter significantly improves the model's ability to understand and transfer editing intent, leading to notable gains in generation quality and overall editing performance.",
            "score": 11,
            "issue_id": 4111,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "e8011f9f9decd752",
            "authors": [
                "Yan Gong",
                "Yiren Song",
                "Yicheng Li",
                "Chenglin Li",
                "Yin Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Zhe Jiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02528.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#transfer_learning",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "RelationAdapter: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "RelationAdapter - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Diffusion Transformer Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº-Ñ†ĞµĞ»ÑŒ. ĞĞ½ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. RelationAdapter Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Relation252K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 218 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Image Editing with RelationAdapter",
                    "desc": "The paper introduces RelationAdapter, a new lightweight module designed to enhance Diffusion Transformer models for image editing tasks. It focuses on using source-target image pairs to effectively capture and apply visual transformations, which improves the model's performance on diverse editing tasks. By leveraging this approach, RelationAdapter addresses the limitations of existing methods that struggle with non-rigid transformations and generalization. The authors also present Relation252K, a dataset that includes 218 diverse editing tasks to evaluate the model's adaptability and effectiveness in visual prompt-driven scenarios."
                },
                "zh": {
                    "title": "æå‡å›¾åƒç¼–è¾‘æ€§èƒ½çš„è½»é‡çº§æ¨¡å—",
                    "desc": "RelationAdapter æ˜¯ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œæ—¨åœ¨å¢å¼ºæ‰©æ•£å˜æ¢å™¨æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥æœ‰æ•ˆæ•æ‰å’Œåº”ç”¨è§†è§‰å˜æ¢ã€‚å®ƒé€šè¿‡æº-ç›®æ ‡å›¾åƒå¯¹æ¥æå–å’Œè½¬ç§»å†…å®¹æ„ŸçŸ¥çš„ç¼–è¾‘æ„å›¾ï¼Œä»è€Œæ”¹å–„ç¼–è¾‘æ€§èƒ½å’Œåœ¨å¤šæ ·ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰çš„å•å‚è€ƒæ–¹æ³•ä¸åŒï¼ŒRelationAdapter èƒ½å¤Ÿå¤„ç†éåˆšæ€§å˜æ¢ï¼Œæå‡å›¾åƒç¼–è¾‘çš„çµæ´»æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº† Relation252K æ•°æ®é›†ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨è§†è§‰æç¤ºé©±åŠ¨åœºæ™¯ä¸­çš„æ³›åŒ–å’Œé€‚åº”èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00910",
            "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2506.00910",
            "abstract": "ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs -- i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.",
            "score": 10,
            "issue_id": 4113,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "375d26a9868ef2fe",
            "authors": [
                "Seongjae Kang",
                "Dong Bok Lee",
                "Hyungjoon Jang",
                "Dongseop Kim",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "VUNO Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00910.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ActiveKD: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "ActiveKD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ¼ ActiveKD ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Probabilistic CoreSet (PCoreSet), Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° 11 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PCoreSet ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ActiveKD."
                },
                "en": {
                    "title": "Efficient Sample Selection through ActiveKD: Merging Active Learning and Knowledge Distillation",
                    "desc": "ActiveKD is a novel framework that combines active learning (AL) with knowledge distillation (KD) to enhance the selection of diverse, unlabeled samples for annotation. It addresses the challenge of limited labeled data by utilizing large vision-language models (VLMs) that can perform well even with few examples. The framework introduces a selection strategy called Probabilistic CoreSet (PCoreSet), which focuses on maximizing coverage in the probability space, allowing for more effective knowledge transfer from teacher models to student models. Evaluations demonstrate that ActiveKD, through PCoreSet, significantly improves sample selection efficiency compared to traditional methods."
                },
                "zh": {
                    "title": "ä¸»åŠ¨å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦çš„é«˜æ•ˆç»“åˆ",
                    "desc": "ActiveKDæ˜¯ä¸€ä¸ªå°†ä¸»åŠ¨å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦ç›¸ç»“åˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆé€‰æ‹©å¤šæ ·åŒ–çš„æœªæ ‡æ³¨æ ·æœ¬è¿›è¡Œæ ‡æ³¨ã€‚çŸ¥è¯†è’¸é¦é€šå¸¸éœ€è¦è¶³å¤Ÿçš„æ ‡æ³¨æ•°æ®ï¼Œè€Œä¸»åŠ¨å­¦ä¹ åˆ™åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å·¥ä½œï¼Œå› æ­¤ä¸¤è€…çš„ç»“åˆå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ActiveKDåˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬èƒ½åŠ›ï¼Œé€šè¿‡æ¦‚ç‡ç©ºé—´ä¸­çš„ç»“æ„åŒ–é¢„æµ‹åå·®æ¥ä¼˜åŒ–æ ·æœ¬é€‰æ‹©ã€‚æˆ‘ä»¬æå‡ºçš„æ¦‚ç‡æ ¸å¿ƒé›†ï¼ˆPCoreSetï¼‰ç­–ç•¥èƒ½å¤Ÿåœ¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—ä¸‹ï¼Œé€‰æ‹©å…·æœ‰ç±»åˆ«å¤šæ ·æ€§çš„æœªæ ‡æ³¨æ ·æœ¬ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°ä¼ é€’æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01144",
            "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation",
            "url": "https://huggingface.co/papers/2506.01144",
            "abstract": "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video diffusion models are notoriously limited in their ability to model temporal aspects such as motion, physics, and dynamic interactions. Existing approaches address this limitation by retraining the model or introducing external conditioning signals to enforce temporal consistency. In this work, we explore whether a meaningful temporal representation can be extracted directly from the predictions of a pre-trained model without any additional training or auxiliary inputs. We introduce FlowMo, a novel training-free guidance method that enhances motion coherence using only the model's own predictions in each diffusion step. FlowMo first derives an appearance-debiased temporal representation by measuring the distance between latents corresponding to consecutive frames. This highlights the implicit temporal structure predicted by the model. It then estimates motion coherence by measuring the patch-wise variance across the temporal dimension and guides the model to reduce this variance dynamically during sampling. Extensive experiments across multiple text-to-video models demonstrate that FlowMo significantly improves motion coherence without sacrificing visual quality or prompt alignment, offering an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models.",
            "score": 8,
            "issue_id": 4113,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "7bd7aa77c6143afb",
            "authors": [
                "Ariel Shaulov",
                "Itay Hazan",
                "Lior Wolf",
                "Hila Chefer"
            ],
            "affiliations": [
                "School of Computer Science Tel Aviv University, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01144.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "FlowMo - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. FlowMo Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¾Ñ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ¸, Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Motion Coherence in Video Generation Without Training",
                    "desc": "FlowMo is a novel method that improves the motion coherence of pre-trained text-to-video diffusion models without requiring any additional training. It works by using the model's own predictions to create a temporal representation that captures the dynamics of motion across frames. By measuring the variance in motion across patches, FlowMo guides the model to reduce inconsistencies during the video generation process. This approach enhances the temporal fidelity of the generated videos while maintaining high visual quality and alignment with input prompts."
                },
                "zh": {
                    "title": "FlowMoï¼šæå‡è§†é¢‘ç”Ÿæˆè¿åŠ¨ä¸€è‡´æ€§çš„æ–°æ–¹æ³•",
                    "desc": "FlowMoæ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„è¿åŠ¨ä¸€è‡´æ€§ã€‚å®ƒé€šè¿‡åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„é¢„æµ‹ï¼Œå‡å°‘äº†æ—¶é—´ç»´åº¦ä¸Šçš„è¡¥ä¸æ–¹å·®ï¼Œä»è€Œå¢å¼ºäº†è¿åŠ¨çš„è¿è´¯æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒFlowMoä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–å¼•å…¥å¤–éƒ¨æ¡ä»¶ä¿¡å·ï¼Œè€Œæ˜¯ç›´æ¥ä»æ¨¡å‹çš„é¢„æµ‹ä¸­æå–æœ‰æ„ä¹‰çš„æ—¶é—´è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowMoåœ¨å¤šä¸ªæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­æ˜¾è‘—æé«˜äº†è¿åŠ¨ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è§†è§‰è´¨é‡å’Œæç¤ºå¯¹é½ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03123",
            "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video\n  Generation",
            "url": "https://huggingface.co/papers/2506.03123",
            "abstract": "Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models often results in severe degradation of temporal consistency and appearance details. In this paper, by analyzing the training dynamics of Consistency Models, we identify a key conflicting learning dynamics during the distillation process: there is a significant discrepancy in the optimization gradients and loss contributions across different timesteps. This discrepancy prevents the distilled student model from achieving an optimal state, leading to compromised temporal consistency and degraded appearance details. To address this issue, we propose a parameter-efficient Dual-Expert Consistency Model~(DCM), where a semantic expert focuses on learning semantic layout and motion, while a detail expert specializes in fine detail refinement. Furthermore, we introduce Temporal Coherence Loss to improve motion consistency for the semantic expert and apply GAN and Feature Matching Loss to enhance the synthesis quality of the detail expert.Our approach achieves state-of-the-art visual quality with significantly reduced sampling steps, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Our code and models are available at https://github.com/Vchitect/DCM{https://github.com/Vchitect/DCM}.",
            "score": 7,
            "issue_id": 4122,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "b284c16ff31a205f",
            "authors": [
                "Zhengyao Lv",
                "Chenyang Si",
                "Tianlin Pan",
                "Zhaoxi Chen",
                "Kwan-Yee K. Wong",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanjing University",
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Hong Kong",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03123.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Dual-Expert Consistency Model (DCM) Ñ Ğ´Ğ²ÑƒĞ¼Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°."
                },
                "en": {
                    "title": "Expert Specialization for Enhanced Video Synthesis",
                    "desc": "This paper addresses the challenges of using Diffusion Models for video synthesis, particularly the high computational cost due to iterative denoising steps. It highlights the limitations of applying Consistency Models directly to video diffusion, which can lead to poor temporal consistency and loss of detail. The authors propose a Dual-Expert Consistency Model (DCM) that separates the learning tasks into two experts: one for semantic understanding and motion, and another for fine detail refinement. By introducing a Temporal Coherence Loss and utilizing GAN and Feature Matching Loss, the model achieves high visual quality with fewer sampling steps, showcasing the benefits of expert specialization in improving video synthesis."
                },
                "zh": {
                    "title": "ä¸“å®¶ä¸“æ³¨ï¼Œæå‡è§†é¢‘åˆæˆè´¨é‡",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘åˆæˆä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†éœ€è¦å¤šæ¬¡å»å™ªæ­¥éª¤ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ã€‚ä¸€è‡´æ€§æ¨¡å‹åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘æ‰©æ•£æ¨¡å‹æ—¶ï¼Œå¾€å¾€ä¼šå¯¼è‡´æ—¶é—´ä¸€è‡´æ€§å’Œå¤–è§‚ç»†èŠ‚çš„ä¸¥é‡é€€åŒ–ã€‚æœ¬æ–‡é€šè¿‡åˆ†æä¸€è‡´æ€§æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€ï¼Œè¯†åˆ«å‡ºåœ¨è’¸é¦è¿‡ç¨‹ä¸­å­˜åœ¨çš„å…³é”®å†²çªå­¦ä¹ åŠ¨æ€ï¼Œå¯¼è‡´è’¸é¦å­¦ç”Ÿæ¨¡å‹æ— æ³•è¾¾åˆ°æœ€ä½³çŠ¶æ€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„åŒä¸“å®¶ä¸€è‡´æ€§æ¨¡å‹ï¼ˆDCMï¼‰ï¼Œé€šè¿‡è¯­ä¹‰ä¸“å®¶å’Œç»†èŠ‚ä¸“å®¶çš„ä¸“ä¸šåŒ–æ¥æé«˜è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01789",
            "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability",
            "url": "https://huggingface.co/papers/2506.01789",
            "abstract": "High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.",
            "score": 7,
            "issue_id": 4111,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "08a1fd6c4eff1198",
            "authors": [
                "Genta Indra Winata",
                "David Anugraha",
                "Emmy Liu",
                "Alham Fikri Aji",
                "Shou-Yi Hung",
                "Aditya Parashar",
                "Patrick Amadeus Irawan",
                "Ruochen Zhang",
                "Zheng-Xin Yong",
                "Jan Christian Blaise Cruz",
                "Niklas Muennighoff",
                "Seungone Kim",
                "Hanyang Zhao",
                "Sudipta Kar",
                "Kezia Erina Suryoraharjo",
                "M. Farid Adilazuarda",
                "En-Shiun Annie Lee",
                "Ayu Purwarianti",
                "Derry Tanti Wijaya",
                "Monojit Choudhury"
            ],
            "affiliations": [
                "Brown University",
                "Capital One",
                "Carnegie Mellon University",
                "Columbia University",
                "ITB",
                "MBZUAI",
                "Monash University",
                "Ontario Tech University",
                "Oracle",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01789.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "DataRubrics: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ DataRubrics - ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². DataRubrics Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Dataset Quality with DataRubrics",
                    "desc": "This paper discusses the importance of high-quality datasets in machine learning and the challenges in creating them, particularly regarding human annotations. It highlights issues with current dataset submissions, such as lack of originality and inadequate quality control, which are often missed during peer review. The authors propose a new framework called DataRubrics, which introduces systematic, rubric-based evaluation metrics to improve dataset quality assessment. Additionally, they explore methods for generating synthetic data and emphasize the need for reproducibility in evaluations using recent advancements in large language models (LLMs)."
                },
                "zh": {
                    "title": "æå‡æ•°æ®é›†è´¨é‡çš„ç³»ç»ŸåŒ–è¯„ä¼°æ¡†æ¶",
                    "desc": "é«˜è´¨é‡çš„æ•°æ®é›†å¯¹äºè®­ç»ƒå’Œè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†åˆ›å»ºè¿™äº›æ•°æ®é›†å°¤å…¶æ˜¯å‡†ç¡®çš„äººç±»æ ‡æ³¨ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è®¸å¤šæ•°æ®é›†è®ºæ–‡ç¼ºä¹åŸåˆ›æ€§ã€å¤šæ ·æ€§æˆ–ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ï¼Œä¸”è¿™äº›é—®é¢˜åœ¨åŒè¡Œè¯„å®¡ä¸­å¸¸å¸¸è¢«å¿½è§†ã€‚æœ¬æ–‡æå€¡åœ¨æ•°æ®é›†å®¡æŸ¥è¿‡ç¨‹ä¸­æ•´åˆç³»ç»ŸåŒ–çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥æé«˜æ•°æ®è´¨é‡è¯„ä¼°çš„æ ‡å‡†åŒ–å’Œå¯æµ‹é‡æ€§ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†DataRubricsï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°äººç±»å’Œæ¨¡å‹ç”Ÿæˆæ•°æ®é›†è´¨é‡çš„ç»“æ„åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ•°æ®é©±åŠ¨ç ”ç©¶çš„æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01274",
            "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding",
            "url": "https://huggingface.co/papers/2506.01274",
            "abstract": "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), a novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals derived from a reference LMM to reflect the model's intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility.",
            "score": 4,
            "issue_id": 4115,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "401bd39fc17a47b7",
            "authors": [
                "Hosu Lee",
                "Junho Kim",
                "Hyunjun Kim",
                "Yong Man Ro"
            ],
            "affiliations": [
                "Integrated Vision and Language Lab, KAIST, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01274.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#video",
                    "#optimization",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ˜Ğ˜",
                    "desc": "ReFoCUS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ReFoCUS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Optimizing Frame Selection for Better Video Reasoning",
                    "desc": "ReFoCUS is a novel framework that uses reinforcement learning to improve how video-LLMs select frames for video question answering (QA). Instead of relying on fixed rules or external systems, it learns which frames are most relevant to the questions being asked. By optimizing frame selection based on the model's preferences, ReFoCUS enhances the reasoning capabilities of video-LLMs. This method not only simplifies the selection process but also boosts performance on various video QA tasks without needing detailed supervision for each frame."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è§†é¢‘å¸§é€‰æ‹©ï¼Œæå‡æ¨ç†èƒ½åŠ›",
                    "desc": "ReFoCUSæ˜¯ä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è§†é¢‘-å¤§è¯­è¨€æ¨¡å‹ï¼ˆvideo-LLMï¼‰å¸§é€‰æ‹©çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†é¢‘é—®ç­”ä¸­çš„æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ å¸§é€‰æ‹©ç­–ç•¥ï¼Œå…³æ³¨è§†è§‰è¾“å…¥çš„é€‰æ‹©ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–æ–‡æœ¬å“åº”ã€‚ReFoCUSä½¿ç”¨æ¥è‡ªå‚è€ƒå¤§å¤šæ¨¡æ€æ¨¡å‹çš„å¥–åŠ±ä¿¡å·ï¼Œåæ˜ æ¨¡å‹å¯¹æ”¯æŒæ—¶é—´ç›¸å…³å“åº”çš„æœ€ä½³å¸§çš„åå¥½ã€‚é€šè¿‡è‡ªå›å½’æ¡ä»¶é€‰æ‹©æ¶æ„ï¼ŒReFoCUSæœ‰æ•ˆæ¢ç´¢å¸§ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡äº†å¤šä¸ªè§†é¢‘é—®ç­”åŸºå‡†çš„æ¨ç†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03096",
            "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens",
            "url": "https://huggingface.co/papers/2506.03096",
            "abstract": "Contrastive language-image pre-training aligns the features of text-image pairs in a common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text into a single feature vector. As a remedy, it is common practice to use additional modules to merge the features extracted by the unimodal encoders. In this work, we present FuseLIP, an alternative architecture for multimodal embedding. Leveraging recent progress in discrete image tokenizers, we propose to use a single transformer model which operates on an extended vocabulary of text and image tokens. This early fusion approach allows the different modalities to interact at each depth of encoding and obtain richer representations compared to common late fusion. We collect new datasets for multimodal pre-training and evaluation, designing challenging tasks for multimodal encoder models. We show that FuseLIP outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks.",
            "score": 3,
            "issue_id": 4118,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "6a94488665c463be",
            "authors": [
                "Christian Schlarmann",
                "Francesco Croce",
                "Nicolas Flammarion",
                "Matthias Hein"
            ],
            "affiliations": [
                "TÃ¼bingen AI Center University of TÃ¼bingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03096.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#games",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ Ğ°Ğ½Ğ½ÑÑ Ñ„ÑŒÑĞ¶Ğ½ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "FuseLIP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼, FuseLIP Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. FuseLIP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "FuseLIP: Unifying Text and Image with Early Fusion for Better Multimodal Understanding",
                    "desc": "This paper introduces FuseLIP, a novel architecture for multimodal embedding that combines text and image features using a single transformer model. Unlike traditional methods that merge features from separate encoders, FuseLIP employs an early fusion strategy, allowing text and image tokens to interact throughout the encoding process. This results in richer representations and improved performance on multimodal tasks like visual question answering (VQA) and text-guided image retrieval. The authors also present new datasets for training and evaluating multimodal models, demonstrating that FuseLIP outperforms existing methods in multimodal scenarios while maintaining competitive results in unimodal tasks."
                },
                "zh": {
                    "title": "FuseLIPï¼šå¤šæ¨¡æ€åµŒå…¥çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€åµŒå…¥æ¶æ„FuseLIPï¼Œæ—¨åœ¨æ”¹è¿›æ–‡æœ¬å’Œå›¾åƒçš„ç‰¹å¾å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„åæœŸèåˆæ–¹æ³•ä¸åŒï¼ŒFuseLIPé‡‡ç”¨æ—©æœŸèåˆç­–ç•¥ï¼Œé€šè¿‡å•ä¸€çš„å˜æ¢å™¨æ¨¡å‹å¤„ç†æ‰©å±•çš„æ–‡æœ¬å’Œå›¾åƒæ ‡è®°è¯æ±‡ã€‚è¿™æ ·å¯ä»¥åœ¨ç¼–ç çš„æ¯ä¸ªæ·±åº¦ä¸Šå®ç°ä¸åŒæ¨¡æ€çš„äº¤äº’ï¼Œä»è€Œè·å¾—æ›´ä¸°å¯Œçš„è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFuseLIPåœ¨å¤šæ¨¡æ€åµŒå…¥ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒæ—¶åœ¨å•æ¨¡æ€ä»»åŠ¡ä¸Šä¸åŸºçº¿æ¨¡å‹ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03079",
            "title": "ORV: 4D Occupancy-centric Robot Video Generation",
            "url": "https://huggingface.co/papers/2506.03079",
            "abstract": "ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Acquiring real-world robotic simulation data through teleoperation is notoriously time-consuming and labor-intensive. Recently, action-driven generative models have gained widespread adoption in robot learning and simulation, as they eliminate safety concerns and reduce maintenance efforts. However, the action sequences used in these methods often result in limited control precision and poor generalization due to their globally coarse alignment. To address these limitations, we propose ORV, an Occupancy-centric Robot Video generation framework, which utilizes 4D semantic occupancy sequences as a fine-grained representation to provide more accurate semantic and geometric guidance for video generation. By leveraging occupancy-based representations, ORV enables seamless translation of simulation data into photorealistic robot videos, while ensuring high temporal consistency and precise controllability. Furthermore, our framework supports the simultaneous generation of multi-view videos of robot gripping operations - an important capability for downstream robotic learning tasks. Extensive experimental results demonstrate that ORV consistently outperforms existing baseline methods across various datasets and sub-tasks. Demo, Code and Model: https://orangesodahub.github.io/ORV",
            "score": 3,
            "issue_id": 4115,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "2bfb7d794c03a7ed",
            "authors": [
                "Xiuyu Yang",
                "Bohan Li",
                "Shaocong Xu",
                "Nan Wang",
                "Chongjie Ye",
                "Zhaoxi Chen",
                "Minghan Qin",
                "Yikang Ding",
                "Xin Jin",
                "Hang Zhao",
                "Hao Zhao"
            ],
            "affiliations": [
                "AIR, Tsinghua University",
                "Beijing Academy of Artificial Intelligence",
                "ByteDance",
                "Eastern Institute of Technology, Ningbo",
                "IIIS, Tsinghua University",
                "Megvii Technology",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03079.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#robotics",
                    "#optimization",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ORV: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· 4D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚ÑŒ",
                    "desc": "ORV - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 4D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ORV ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "ORV: Revolutionizing Robot Video Generation with 4D Occupancy Sequences",
                    "desc": "The paper introduces ORV, a framework designed for generating robot videos using 4D semantic occupancy sequences. This approach enhances the quality of video generation by providing detailed semantic and geometric information, leading to photorealistic and temporally consistent outputs. ORV addresses the limitations of previous action-driven generative models, which often struggle with control precision and generalization. The framework also allows for the generation of multi-view videos, which is beneficial for various robotic learning applications, demonstrating superior performance compared to existing methods."
                },
                "zh": {
                    "title": "ä»¥å ç”¨ä¸ºä¸­å¿ƒçš„æœºå™¨äººè§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶",
                    "desc": "ORVæ˜¯ä¸€ä¸ªä»¥å ç”¨ä¸ºä¸­å¿ƒçš„æœºå™¨äººè§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨4Dè¯­ä¹‰å ç”¨åºåˆ—ç”Ÿæˆé€¼çœŸçš„ã€æ—¶é—´ä¸€è‡´çš„ã€å¯ç²¾ç¡®æ§åˆ¶çš„æœºå™¨äººè§†é¢‘ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»†ç²’åº¦çš„å ç”¨è¡¨ç¤ºï¼Œæä¾›æ›´å‡†ç¡®çš„è¯­ä¹‰å’Œå‡ ä½•æŒ‡å¯¼ï¼Œä»è€Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„æ§åˆ¶ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ORVèƒ½å¤Ÿæ— ç¼åœ°å°†ä»¿çœŸæ•°æ®è½¬åŒ–ä¸ºé«˜è´¨é‡çš„æœºå™¨äººè§†é¢‘ï¼Œå¹¶æ”¯æŒåŒæ—¶ç”Ÿæˆå¤šè§†è§’çš„è§†é¢‘ï¼Œé€‚ç”¨äºåç»­çš„æœºå™¨äººå­¦ä¹ ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORVåœ¨å¤šä¸ªæ•°æ®é›†å’Œå­ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02338",
            "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL",
            "url": "https://huggingface.co/papers/2506.02338",
            "abstract": "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.  \t\t\t\t\tAI-generated summary \t\t\t\t With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR.",
            "score": 3,
            "issue_id": 4111,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "c96bff52eb2a678f",
            "authors": [
                "Hyungjoo Chae",
                "Dongjin Kang",
                "Jihyuk Kim",
                "Beong-woo Kwak",
                "Sunghyun Park",
                "Haeju Park",
                "Jinyoung Yeo",
                "Moontae Lee",
                "Kyungjae Lee"
            ],
            "affiliations": [
                "LG AI Research",
                "University of Illinois Chicago",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02338.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#transfer_learning",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Long CoT Collection, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering Reasoning with Long CoT Datasets",
                    "desc": "The Long CoT Collection dataset is designed to improve reasoning skills in language models by using short chain-of-thought (CoT) models to generate long CoT inferences. This dataset consists of 100,000 annotated rationales that help models think more deeply and manage their reasoning process better. By training on this dataset, models can achieve reasoning quality similar to the established R1 model while also enhancing their performance in reinforcement learning tasks. The research highlights the potential for developing independent large reasoning models without relying solely on existing ones."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›çš„é•¿é“¾æ€ç»´æ•°æ®é›†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºLong CoT Collectionçš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±çŸ­é“¾æ€ç»´ï¼ˆCoTï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆï¼Œæ—¨åœ¨æå‡ä¸€èˆ¬æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥è·å¾—ä¸ç°æœ‰å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚R1ï¼‰ç›¸å½“çš„æ¨ç†è´¨é‡ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿çŸ­é“¾æ€ç»´æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ›´é•¿æ—¶é—´çš„æ¨ç†ï¼Œå¹¶å¼•å…¥äº†å¯¹æ€ç»´é¢„ç®—çš„å¯æ§æ€§ï¼Œä»¥è§£å†³è¿‡åº¦æ€è€ƒçš„é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè¯¥æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01716",
            "title": "Self-Challenging Language Model Agents",
            "url": "https://huggingface.co/papers/2506.01716",
            "abstract": "The Self-Challenging framework trains intelligent agents using self-generated tasks defined as Code-as-Task, improving performance in multi-turn tool-use benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.",
            "score": 3,
            "issue_id": 4123,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "292019a70795a800",
            "authors": [
                "Yifei Zhou",
                "Sergey Levine",
                "Jason Weston",
                "Xian Li",
                "Sainbayar Sukhbaatar"
            ],
            "affiliations": [
                "FAIR at Meta",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01716.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agi",
                    "#training",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ğ°Ğ³ĞµĞ½Ñ‚ Ğ±Ñ€Ğ¾ÑĞ°ĞµÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ¼Ñƒ ÑĞµĞ±Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Self-Challenging. Ğ’ ÑÑ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Code-as-Task, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3.1-8B-Instruct Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Empowering Agents Through Self-Generated Challenges",
                    "desc": "The Self-Challenging framework enhances the training of intelligent agents by allowing them to create their own tasks, known as Code-as-Task. This approach eliminates the need for extensive human-generated tasks, as the agent generates high-quality tasks through its interactions with tools. Each task includes an instruction, a verification function, and defined success and failure cases, which help ensure the tasks are effective for training. The framework utilizes reinforcement learning to improve the agent's performance, achieving significant gains in benchmarks with only self-generated data."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æŒ‘æˆ˜ï¼Œæ™ºèƒ½ä½“è®­ç»ƒçš„æ–°æ–¹æ³•",
                    "desc": "è‡ªæˆ‘æŒ‘æˆ˜æ¡†æ¶é€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„ä»»åŠ¡æ¥è®­ç»ƒæ™ºèƒ½ä½“ï¼Œè¿™äº›ä»»åŠ¡è¢«å®šä¹‰ä¸ºä»£ç ä»»åŠ¡ï¼Œä»è€Œæé«˜äº†åœ¨å¤šè½®å·¥å…·ä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“åœ¨ä¸å·¥å…·äº’åŠ¨åï¼Œé¦–å…ˆæ‰®æ¼”æŒ‘æˆ˜è€…è§’è‰²ç”Ÿæˆé«˜è´¨é‡ä»»åŠ¡ã€‚ç”Ÿæˆçš„ä»»åŠ¡åŒ…æ‹¬æŒ‡ä»¤ã€éªŒè¯å‡½æ•°ä»¥åŠè§£å†³æ–¹æ¡ˆå’Œå¤±è´¥æ¡ˆä¾‹ï¼Œç¡®ä¿ä»»åŠ¡çš„è´¨é‡ã€‚æœ€ç»ˆï¼Œæ™ºèƒ½ä½“ä½œä¸ºæ‰§è¡Œè€…ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡è¯„ä¼°åé¦ˆä½œä¸ºå¥–åŠ±ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00413",
            "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
            "url": "https://huggingface.co/papers/2506.00413",
            "abstract": "Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.",
            "score": 3,
            "issue_id": 4113,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "42d7cf22876b9eef",
            "authors": [
                "Daniel Israel",
                "Guy Van den Broeck",
                "Aditya Grover"
            ],
            "affiliations": [
                "Department of Computer Science University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00413.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (APD) Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLM). APD Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¼ĞµÑÑŒ Ğ¼Ğ°Ñ€Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ dLLM Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. APD Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ KV-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Boosting Speed in Language Models with Adaptive Parallel Decoding",
                    "desc": "Adaptive parallel decoding (APD) is a new technique that improves the speed of diffusion large language models (dLLMs) by allowing multiple tokens to be generated at the same time. Traditional methods use autoregressive decoding, which predicts tokens one after another, slowing down the process. APD changes this by dynamically adjusting how many tokens are generated in parallel, balancing speed and quality. By combining probabilities from both dLLMs and a smaller autoregressive model, APD achieves higher throughput with only slight reductions in output quality."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”å¹¶è¡Œè§£ç ï¼šæå‡ç”Ÿæˆé€Ÿåº¦ä¸è´¨é‡çš„å¹³è¡¡",
                    "desc": "è‡ªé€‚åº”å¹¶è¡Œè§£ç ï¼ˆAPDï¼‰é€šè¿‡åŠ¨æ€è°ƒæ•´å¹¶è¡Œç”Ÿæˆçš„æ ‡è®°æ•°é‡ï¼Œæå‡äº†æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰çš„ååé‡ï¼Œè€Œä¸ä¼šæ˜¾è‘—é™ä½è´¨é‡ã€‚ä¼ ç»Ÿçš„è‡ªå›å½’è§£ç æ–¹æ³•ä½¿å¾—ç”Ÿæˆé€Ÿåº¦å—åˆ°é™åˆ¶ï¼Œå› ä¸ºæ ‡è®°æ˜¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°é¢„æµ‹çš„ã€‚å°½ç®¡ç†è®ºä¸Šæ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹å…è®¸å¹¶è¡Œç”Ÿæˆæ ‡è®°ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¾€å¾€éš¾ä»¥åœ¨ä¸ç‰ºç‰²è´¨é‡çš„æƒ…å†µä¸‹è¾¾åˆ°è‡ªå›å½’æ¨¡å‹çš„é€Ÿåº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å®šä¹‰dLLMè¾¹é™…æ¦‚ç‡ä¸å°å‹è‡ªå›å½’æ¨¡å‹ä¸‹åºåˆ—çš„è”åˆæ¦‚ç‡ä¹‹é—´çš„ä¹˜æ³•æ··åˆï¼Œæ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå¹¶é€šè¿‡å¯ç”¨KVç¼“å­˜å’Œé™åˆ¶æ©ç è¾“å…¥çš„å¤§å°è¿›ä¸€æ­¥ä¼˜åŒ–APDã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00391",
            "title": "SHARE: An SLM-based Hierarchical Action CorREction Assistant for\n  Text-to-SQL",
            "url": "https://huggingface.co/papers/2506.00391",
            "abstract": "SHARE, an SLM-based Hierarchical Action RECorrection system, enhances LLMs in text-to-SQL by transforming SQL queries into action trajectories and employing a granular refinement process, improving error detection and correction efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for declarative SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose SHARE, an SLM-based Hierarchical Action corREction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.",
            "score": 3,
            "issue_id": 4122,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "2e0ac7f48f4b8959",
            "authors": [
                "Ge Qu",
                "Jinyang Li",
                "Bowen Qin",
                "Xiaolong Li",
                "Nan Huo",
                "Chenhao Ma",
                "Reynold Cheng"
            ],
            "affiliations": [
                "BAAI",
                "The Chinese University of Hong Kong, Shenzhen",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00391.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#training",
                    "#optimization",
                    "#dataset",
                    "#reasoning",
                    "#small_models",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ SQL Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "SHARE - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² SQL. ĞĞ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. SHARE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "SHARE: Enhancing SQL Error Correction with Hierarchical Action Trajectories",
                    "desc": "The paper introduces SHARE, a system designed to improve the self-correction capabilities of large language models (LLMs) in text-to-SQL tasks. It addresses the inefficiencies of traditional self-correction methods that rely heavily on recursive calls, which can be computationally expensive. SHARE utilizes a hierarchical approach with three specialized small language models (SLMs) to convert SQL queries into action trajectories, enhancing error detection and correction. The system also incorporates a novel training strategy that allows it to perform well even with limited data, making it suitable for applications where data privacy is a concern."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬åˆ°SQLçš„è‡ªæˆ‘çº æ­£èƒ½åŠ›",
                    "desc": "SHAREæ˜¯ä¸€ç§åŸºäºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å±‚æ¬¡åŒ–åŠ¨ä½œçº æ­£ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ°SQLè½¬æ¢ä¸­çš„è¡¨ç°ã€‚å®ƒé€šè¿‡å°†SQLæŸ¥è¯¢è½¬åŒ–ä¸ºé€æ­¥çš„åŠ¨ä½œè½¨è¿¹ï¼Œå¸®åŠ©LLMæ›´å¥½åœ°ç†è§£å’Œå®šä½é”™è¯¯ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„ç»†åŒ–è¿‡ç¨‹ï¼Œæé«˜äº†é”™è¯¯æ£€æµ‹å’Œçº æ­£çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒSHAREè¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å±‚æ¬¡è‡ªæˆ‘è¿›åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®èµ„æºæœ‰é™çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆè®­ç»ƒï¼Œç‰¹åˆ«é€‚ç”¨äºæœ‰æ•°æ®éšç§é™åˆ¶çš„æ–‡æœ¬åˆ°SQLåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02510",
            "title": "M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset",
            "url": "https://huggingface.co/papers/2506.02510",
            "abstract": "A new multilingual, multi-sector, and multi-task benchmark, MÂ³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called M^3FinMeeting, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, M^3FinMeeting supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, M^3FinMeeting includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of M^3FinMeeting as a benchmark for assessing LLMs' financial meeting comprehension skills.",
            "score": 2,
            "issue_id": 4110,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "903bb57b5cc664ec",
            "authors": [
                "Jie Zhu",
                "Junhui Li",
                "Yalong Wen",
                "Xiandong Li",
                "Lifan Guo",
                "Feng Chen"
            ],
            "affiliations": [
                "Nanjing University",
                "Qwen DianJin Team, Alibaba Cloud Computing",
                "School of Computer Science and Technology, Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02510.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#machine_translation",
                    "#science",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "MÂ³FinMeeting: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ²ÑÑ‚Ñ€ĞµÑ‡ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MÂ³FinMeeting Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ²ÑÑ‚Ñ€ĞµÑ‡ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼, ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ¸ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ GICS. MÂ³FinMeeting ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞµĞ¼ÑŒÑ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "MÂ³FinMeeting: Bridging Language Gaps in Financial Comprehension",
                    "desc": "The MÂ³FinMeeting benchmark is designed to evaluate large language models (LLMs) in understanding financial meetings across multiple languages and sectors. It addresses the limitations of existing benchmarks that primarily focus on static financial documents like news articles. This new dataset supports English, Chinese, and Japanese, allowing for a broader assessment of multilingual financial discussions. Additionally, it includes tasks such as summarization and question answering, highlighting the need for LLMs to improve their comprehension of dynamic financial interactions."
                },
                "zh": {
                    "title": "MÂ³FinMeetingï¼šé‡‘èä¼šè®®ç†è§£çš„æ–°åŸºå‡†",
                    "desc": "MÂ³FinMeetingæ˜¯ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€ã€å¤šè¡Œä¸šå’Œå¤šä»»åŠ¡çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£é‡‘èä¼šè®®æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ”¯æŒè‹±è¯­ã€ä¸­æ–‡å’Œæ—¥è¯­ï¼Œå¢å¼ºäº†å¯¹ä¸åŒè¯­è¨€ç¯å¢ƒä¸­é‡‘èè®¨è®ºçš„ç†è§£ã€‚å®ƒæ¶µç›–äº†å…¨çƒè¡Œä¸šåˆ†ç±»æ ‡å‡†ï¼ˆGICSï¼‰å®šä¹‰çš„å¤šä¸ªè¡Œä¸šï¼Œç¡®ä¿åŸºå‡†èƒ½å¤Ÿè¦†ç›–å¹¿æ³›çš„é‡‘èæ´»åŠ¨ã€‚é€šè¿‡å¯¹ä¸ƒç§æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®éªŒï¼Œç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨ç†è§£èƒ½åŠ›ä¸Šä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ï¼Œè¯æ˜äº†MÂ³FinMeetingä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹é‡‘èä¼šè®®ç†è§£èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02454",
            "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework",
            "url": "https://huggingface.co/papers/2506.02454",
            "abstract": "A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\\% overall win rate over the baseline method.",
            "score": 2,
            "issue_id": 4111,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "9220f780a7fba411",
            "authors": [
                "Zhaorui Yang",
                "Bo Pan",
                "Han Wang",
                "Yiyao Wang",
                "Xingyu Liu",
                "Minfeng Zhu",
                "Bo Zhang",
                "Wei Chen"
            ],
            "affiliations": [
                "State Key Lab of CAD&CG, Zhejiang University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02454.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "ĞĞ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Multimodal DeepResearcher Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ‚ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Multimodal DeepResearcher Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Reports: Text Meets Visualization with Multimodal DeepResearcher",
                    "desc": "The paper introduces Multimodal DeepResearcher, a framework that allows Large Language Models (LLMs) to create detailed reports that combine text and various visualizations. It addresses the challenge of integrating informative visualizations with text, which has been largely overlooked in previous research. The framework uses a structured representation called Formal Description of Visualization (FDV) to help LLMs generate high-quality visual content. Through a systematic approach involving research, report creation, planning, and multimodal generation, the framework shows significant improvements in report quality compared to existing methods."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ·±åº¦ç ”ç©¶è€…ï¼šæ–‡æœ¬ä¸å¯è§†åŒ–çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºå¤šæ¨¡æ€æ·±åº¦ç ”ç©¶è€…ï¼ˆMultimodal DeepResearcherï¼‰ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€æŠ¥å‘Šï¼Œè¿™äº›æŠ¥å‘Šç»“åˆäº†æ–‡æœ¬å’Œå¤šç§å¯è§†åŒ–å½¢å¼ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–çš„æ–‡æœ¬è¡¨ç¤ºï¼Œè§£å†³äº†æ–‡æœ¬ä¸å¯è§†åŒ–å†…å®¹äº¤ç»‡ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œæå‡äº†ä¿¡æ¯ä¼ è¾¾çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†å¯è§†åŒ–çš„æ­£å¼æè¿°ï¼ˆFDVï¼‰ï¼Œä½¿å¾—LLMsèƒ½å¤Ÿå­¦ä¹ å¹¶ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡å¯è§†åŒ–å›¾è¡¨ã€‚é€šè¿‡å››ä¸ªé˜¶æ®µçš„ä»»åŠ¡åˆ†è§£ï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆå¤šæ¨¡æ€æŠ¥å‘Šæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨æ€§èƒ½ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02295",
            "title": "QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large\n  Language Model Adaptation",
            "url": "https://huggingface.co/papers/2506.02295",
            "abstract": "Qari-OCR, a series of fine-tuned vision-language models, achieves state-of-the-art performance in Arabic OCR through iterative optimization on specialized datasets, handling diacritics, fonts, layouts, and low-resolution images.  \t\t\t\t\tAI-generated summary \t\t\t\t The inherent complexities of Arabic script; its cursive nature, diacritical marks (tashkeel), and varied typography, pose persistent challenges for Optical Character Recognition (OCR). We present Qari-OCR, a series of vision-language models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic through iterative fine-tuning on specialized synthetic datasets. Our leading model, QARI v0.2, establishes a new open-source state-of-the-art with a Word Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling of tashkeel, diverse fonts, and document layouts, alongside impressive performance on low-resolution images. Further explorations (QARI v0.3) showcase strong potential for structural document understanding and handwritten text. This work delivers a marked improvement in Arabic OCR accuracy and efficiency, with all models and datasets released to foster further research.",
            "score": 2,
            "issue_id": 4119,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "b031659260f990f9",
            "authors": [
                "Ahmed Wasfy",
                "Omer Nacar",
                "Abdelakreem Elkhateb",
                "Mahmoud Reda",
                "Omar Elshehy",
                "Adel Ammar",
                "Wadii Boulila"
            ],
            "affiliations": [
                "KAND CA Corp.",
                "NAMAA",
                "Prince Sultan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02295.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#dataset",
                    "#synthetic",
                    "#cv",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Qari-OCR Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ OCR Ğ´Ğ»Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Qari-OCR ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¹ Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ğ°ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°ĞºĞ¸, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑˆÑ€Ğ¸Ñ„Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Arabic OCR with Qari-OCR",
                    "desc": "Qari-OCR is a series of advanced vision-language models specifically designed to improve Optical Character Recognition (OCR) for Arabic text. It addresses the unique challenges of Arabic script, such as its cursive nature and diacritical marks, by using iterative fine-tuning on specialized datasets. The leading model, QARI v0.2, achieves impressive metrics with a Word Error Rate (WER) of 0.160 and a Character Error Rate (CER) of 0.061, setting a new benchmark in the field. This work not only enhances OCR accuracy for Arabic but also supports further research by making all models and datasets publicly available."
                },
                "zh": {
                    "title": "Qari-OCRï¼šé˜¿æ‹‰ä¼¯è¯­OCRçš„æ–°çªç ´",
                    "desc": "Qari-OCRæ˜¯ä¸€ç³»åˆ—ç»è¿‡å¾®è°ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰è¿›è¡Œä¼˜åŒ–ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨ç‰¹å®šçš„åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè¿­ä»£å¾®è°ƒï¼ŒæˆåŠŸå¤„ç†äº†é˜¿æ‹‰ä¼¯è¯­çš„è¿å†™ç‰¹æ€§ã€å…ƒéŸ³ç¬¦å·å’Œå¤šæ ·çš„å­—ä½“å¸ƒå±€ã€‚æˆ‘ä»¬çš„ä¸»è¦æ¨¡å‹QARI v0.2åœ¨å¤„ç†å«æœ‰å…ƒéŸ³ç¬¦å·çš„æ–‡æœ¬æ—¶ï¼Œè¾¾åˆ°äº†0.160çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œ0.061çš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ï¼Œå¹¶åœ¨BLEUè¯„åˆ†ä¸Šå–å¾—äº†0.737çš„ä¼˜å¼‚æˆç»©ã€‚Qari-OCRåœ¨ä½åˆ†è¾¨ç‡å›¾åƒçš„å¤„ç†ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œä¸ºé˜¿æ‹‰ä¼¯è¯­OCRçš„å‡†ç¡®æ€§å’Œæ•ˆç‡å¸¦æ¥äº†æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01565",
            "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation",
            "url": "https://huggingface.co/papers/2506.01565",
            "abstract": "Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation.The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10\\% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42\\%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation.",
            "score": 2,
            "issue_id": 4117,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "0251c50d35bd4a50",
            "authors": [
                "Li Zhou",
                "Lutong Yu",
                "Dongchu Xie",
                "Shaohuan Cheng",
                "Wenyan Li",
                "Haizhou Li"
            ],
            "affiliations": [
                "Chengdu Technological University",
                "Shenzhen Research Institute of Big Data",
                "The Chinese University of Hong Kong, Shenzhen",
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01565.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ‘˜",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Hanfu-Bench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ…Ğ°Ğ½ÑŒÑ„Ñƒ - Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ğµ, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹. Hanfu-Bench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€ĞµĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging Time and Culture with Hanfu-Bench",
                    "desc": "This paper introduces Hanfu-Bench, a new dataset designed to enhance the understanding of cultural evolution over time using vision-language models (VLMs). It focuses on Hanfu, a traditional Chinese garment, to explore both cultural visual understanding and the transformation of traditional attire into modern designs. The study reveals that while closed VLMs perform similarly to non-experts in recognizing cultural features, they still lag behind human experts. Additionally, the transcreation task shows that even the best models struggle to achieve high success rates, highlighting the challenges in temporal cultural understanding and creative adaptation."
                },
                "zh": {
                    "title": "æ¢ç´¢æ—¶é—´ç»´åº¦çš„æ–‡åŒ–ç†è§£",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†Hanfu-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨å¡«è¡¥è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£ä¸­çš„æ—¶é—´ç»´åº¦ç¼ºå¤±ã€‚Hanfuä½œä¸ºä¸­å›½ä¼ ç»Ÿæœé¥°ï¼Œä»£è¡¨äº†æ·±åšçš„æ–‡åŒ–é—äº§ï¼Œåæ˜ äº†ä¸­å›½æ–‡åŒ–çš„æ—¶é—´ç‰¹å¾ã€‚æ•°æ®é›†åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šæ–‡åŒ–è§†è§‰ç†è§£å’Œæ–‡åŒ–å›¾åƒå†åˆ›ä½œï¼Œå‰è€…é€šè¿‡å¤šé€‰è§†è§‰é—®ç­”è¯„ä¼°æ—¶é—´æ–‡åŒ–ç‰¹å¾çš„è¯†åˆ«ï¼Œåè€…åˆ™å…³æ³¨ä¼ ç»Ÿæœé¥°å‘ç°ä»£è®¾è®¡çš„è½¬åŒ–ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°é—­å¼è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ–‡åŒ–ç†è§£ä¸Šä¸éä¸“å®¶ç›¸å½“ï¼Œä½†åœ¨æ—¶é—´æ–‡åŒ–ç†è§£å’Œåˆ›æ„é€‚åº”æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24273",
            "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning",
            "url": "https://huggingface.co/papers/2505.24273",
            "abstract": "This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs.",
            "score": 2,
            "issue_id": 4118,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "73012fc9edd07bd3",
            "authors": [
                "Hongyi James Cai",
                "Junlin Wang",
                "Xiaoyin Chen",
                "Bhuwan Dhingra"
            ],
            "affiliations": [
                "Duke University",
                "Mila - Quebec AI Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24273.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#synthetic",
                    "#reasoning",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…: ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM), Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ñ€Ğ¾Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ° (backtracking) Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¡ÑƒĞ´Ğ¾ĞºÑƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° ÑĞºĞ¾Ñ€ĞµĞµ Ğ¾Ñ‚ Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs through Backtracking and Training Synergy",
                    "desc": "This paper explores how supervised fine-tuning (SFT) and reinforcement learning (RL) work together to improve reasoning in large language models (LLMs). It specifically examines the role of backtracking, a technique that allows models to revisit previous steps in their reasoning process, and how it enhances performance on various reasoning tasks. The study finds that while short chain-of-thought (CoT) sequences help in RL training, their effectiveness decreases with task difficulty. Ultimately, the research provides valuable insights into optimizing training strategies for better reasoning capabilities in LLMs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†èƒ½åŠ›çš„è®­ç»ƒç­–ç•¥",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç›¸äº’ä½œç”¨ï¼Œé‡ç‚¹å…³æ³¨å›æº¯åœ¨å¢å¼ºæ¨ç†èƒ½åŠ›ä¸­çš„ä½œç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹åœ¨æ•°å­¦å’Œé€»è¾‘é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼ŒçŸ­çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åºåˆ—åœ¨SFTé˜¶æ®µå¯¹RLè®­ç»ƒæœ‰ä¸€å®šè´¡çŒ®ï¼Œä½†åœ¨ä»»åŠ¡éš¾åº¦å¢åŠ æ—¶ï¼Œè¿™ç§è´¡çŒ®ä¼šå‡å¼±ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯å®äº†å›æº¯çš„é¢‘ç‡å’Œç»“æ„å¯¹æ¨ç†è®­ç»ƒçš„é‡è¦æ€§ï¼Œæä¾›äº†ä¼˜åŒ–è®­ç»ƒç­–ç•¥çš„å®ç”¨è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18079",
            "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
            "url": "https://huggingface.co/papers/2505.18079",
            "abstract": "The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later.",
            "score": 2,
            "issue_id": 4117,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "6dbac78671d7d992",
            "authors": [
                "Xiaoyi Zhang",
                "Zhaoyang Jia",
                "Zongyu Guo",
                "Jiahao Li",
                "Bin Li",
                "Houqiang Li",
                "Yan Lu"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18079.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Deep Video Discovery Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ³ĞµĞ½Ñ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. DVD Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LVBench."
                },
                "en": {
                    "title": "Revolutionizing Long-Form Video Understanding with Autonomous Agents",
                    "desc": "The Deep Video Discovery (DVD) agent introduces an innovative approach to long-form video understanding by utilizing an autonomous agentic search strategy powered by large language models (LLMs). This method addresses the challenges posed by the complexity of temporal and spatial information in lengthy videos, which traditional models struggle to analyze effectively. By segmenting videos and employing a search-centric toolkit, the DVD agent can dynamically plan and refine its reasoning based on real-time observations. The results demonstrate that this system achieves state-of-the-art performance on benchmarks like LVBench, significantly outperforming previous methods."
                },
                "zh": {
                    "title": "è‡ªä¸»æœç´¢ï¼Œæ·±åº¦ç†è§£é•¿è§†é¢‘",
                    "desc": "æ·±åº¦è§†é¢‘å‘ç°ä»£ç†ä½¿ç”¨è‡ªä¸»æœç´¢ç­–ç•¥ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…‹æœäº†é•¿è§†é¢‘ç†è§£ä¸­çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹åˆ†æ®µè§†é¢‘ç‰‡æ®µè¿›è¡Œæ™ºèƒ½æœç´¢ï¼Œæå‡äº†åœ¨å¤æ‚æ—¶ç©ºèƒŒæ™¯ä¸‹çš„é—®é¢˜å›ç­”èƒ½åŠ›ã€‚ä¸ä»¥å¾€æ‰‹åŠ¨è®¾è®¡å·¥ä½œæµç¨‹çš„è§†é¢‘ä»£ç†ä¸åŒï¼Œæˆ‘ä»¬çš„ä»£ç†å¼ºè°ƒè‡ªä¸»æ€§ï¼Œåˆ©ç”¨å¤šå±‚æ¬¡è§†é¢‘æ•°æ®åº“ä¸­çš„æœç´¢å·¥å…·è¿›è¡Œè§„åˆ’å’Œé€‰æ‹©ã€‚ç»è¿‡å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„ç ”ç©¶æˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16994",
            "title": "R^2ec: Towards Large Recommender Models with Reasoning",
            "url": "https://huggingface.co/papers/2505.16994",
            "abstract": "A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. Current studies usually position LLMs as external reasoning modules to yield auxiliary thought for augmenting conventional recommendation pipelines. However, such decoupled designs are limited in significant resource cost and suboptimal joint optimization. To address these issues, we propose \\name, a unified large recommender model with intrinsic reasoning capabilities. Initially, we reconceptualize the model architecture to facilitate interleaved reasoning and recommendation in the autoregressive process. Subsequently, we propose RecPO, a corresponding reinforcement learning framework that optimizes \\name\\ both the reasoning and recommendation capabilities simultaneously in a single policy update; RecPO introduces a fused reward scheme that solely leverages recommendation labels to simulate the reasoning capability, eliminating dependency on specialized reasoning annotations. Experiments on three datasets with various baselines verify the effectiveness of \\name, showing relative improvements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at https://github.com/YRYangang/RRec.",
            "score": 2,
            "issue_id": 4111,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "6b76c655943245ca",
            "authors": [
                "Runyang You",
                "Yongqi Li",
                "Xinyu Lin",
                "Xin Zhang",
                "Wenjie Wang",
                "Wenjie Li",
                "Liqiang Nie"
            ],
            "affiliations": [
                "Harbin Institute of Technology (Shenzhen)",
                "National University of Singapore",
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16994.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° RecPO Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Hit@5 Ğ¸ NDCG@20 Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Unified Reasoning and Recommendation for Enhanced Performance",
                    "desc": "This paper introduces a new large recommender model that integrates reasoning capabilities directly into the recommendation process. The model, named RecPO, uses a reinforcement learning framework to optimize both reasoning and recommendation in a unified manner. By allowing these two processes to work together, the model reduces resource costs and improves performance compared to traditional methods that treat them separately. Experiments demonstrate significant improvements in recommendation metrics, showcasing the model's effectiveness in real-world applications."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¨èæ¨¡å‹ï¼Œæ¨ç†ä¸æ¨èçš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤§å‹æ¨èæ¨¡å‹ï¼Œå…·å¤‡å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ¨èè¿‡ç¨‹ä¸­å®ç°äº¤é”™æ¨ç†ä¸æ¨èã€‚æˆ‘ä»¬é‡æ–°æ„æ€äº†æ¨¡å‹æ¶æ„ï¼Œä½¿å…¶åœ¨è‡ªå›å½’è¿‡ç¨‹ä¸­åŒæ—¶è¿›è¡Œæ¨ç†å’Œæ¨èã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†RecPOï¼Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å•æ¬¡ç­–ç•¥æ›´æ–°ä¸­åŒæ—¶ä¼˜åŒ–æ¨ç†å’Œæ¨èèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šç›¸è¾ƒäºåŸºçº¿æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03144",
            "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query",
            "url": "https://huggingface.co/papers/2506.03144",
            "abstract": "Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.",
            "score": 1,
            "issue_id": 4118,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "5e57a72b2bce8a15",
            "authors": [
                "Wei Chow",
                "Yuan Gao",
                "Linfeng Li",
                "Xian Wang",
                "Qi Xu",
                "Hang Song",
                "Lingdong Kong",
                "Ran Zhou",
                "Yi Zeng",
                "Yidong Cai",
                "Botian Jiang",
                "Shilin Xu",
                "Jiajun Zhang",
                "Minghui Qiu",
                "Xiangtai Li",
                "Tianshu Yang",
                "Siliang Tang",
                "Juncheng Li"
            ],
            "affiliations": [
                "ByteDance Inc.",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03144.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#multilingual",
                    "#transfer_learning",
                    "#open_source"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ: MERIT Ğ¸ Coral",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MERIT - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Coral, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Coral Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 45.9% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° MERIT."
                },
                "en": {
                    "title": "Revolutionizing Semantic Retrieval with MERIT and Coral",
                    "desc": "This paper addresses the challenges in semantic retrieval, particularly in multilingual and multi-condition scenarios. It introduces MERIT, a new dataset with 320,000 queries across five languages and seven product categories, which highlights the limitations of current models that overlook specific query conditions. The authors propose Coral, a fine-tuning framework that enhances pre-trained multilingual language models by focusing on both global semantics and fine-grained conditional elements. Experimental results show that Coral significantly outperforms traditional methods, paving the way for improved semantic retrieval in complex environments."
                },
                "zh": {
                    "title": "å¼€åˆ›å¤šæ¡ä»¶è¯­ä¹‰æ£€ç´¢çš„æ–°çºªå…ƒ",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è¯­ä¹‰æ£€ç´¢åœ¨ç°ä»£åº”ç”¨ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå½“å‰ç ”ç©¶çš„ä¸è¶³ä¹‹å¤„ã€‚ç°æœ‰çš„æ•°æ®é›†é€šå¸¸åªé™äºå•ä¸€è¯­è¨€æˆ–å•ä¸€å›¾åƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯çš„è¡¨è¾¾èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†MERITï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºäº¤é”™å¤šæ¡ä»¶è¯­ä¹‰æ£€ç´¢çš„å¤šè¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«320,000ä¸ªæŸ¥è¯¢å’Œ135,000ä¸ªäº§å“ï¼Œè¦†ç›–7ä¸ªä¸åŒçš„äº§å“ç±»åˆ«ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Coralï¼Œä¸€ä¸ªæ–°çš„å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆåµŒå…¥é‡å»ºå’Œå¯¹æ¯”å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨MERITä¸Šçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03119",
            "title": "Controllable Human-centric Keyframe Interpolation with Generative Prior",
            "url": "https://huggingface.co/papers/2506.03119",
            "abstract": "Existing interpolation methods use pre-trained video diffusion priors to generate intermediate frames between sparsely sampled keyframes. In the absence of 3D geometric guidance, these methods struggle to produce plausible results for complex, articulated human motions and offer limited control over the synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a novel framework that integrates 3D human guidance signals into the diffusion process for Controllable Human-centric Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for interpolation, our PoseFuse3D, a 3D-informed control model, features a novel SMPL-X encoder that transforms 3D geometry and shape into the 2D latent conditioning space, alongside a fusion network that integrates these 3D cues with 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset annotated with both 2D poses and 3D SMPL-X parameters. We show that PoseFuse3D-KI consistently outperforms state-of-the-art baselines on CHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS. Comprehensive ablations demonstrate that our PoseFuse3D model improves interpolation fidelity.",
            "score": 1,
            "issue_id": 4125,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "1fe3ed71536b27c9",
            "authors": [
                "Zujin Guo",
                "Size Wu",
                "Zhongang Cai",
                "Wei Li",
                "Chen Change Loy"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03119.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ•º",
                "ru": {
                    "title": "3D-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "PoseFuse3D-KI - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ 3D-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ·Ğµ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğµ Ñ‚ĞµĞ»Ğ°. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PoseFuse3D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ² 2D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CHKI-Video Ñ 2D Ğ¸ 3D Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ PSNR Ğ½Ğ° 9% Ğ¸ LPIPS Ğ½Ğ° 38%."
                },
                "en": {
                    "title": "Enhancing Video Frame Interpolation with 3D Human Guidance",
                    "desc": "This paper presents PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a new method for generating intermediate video frames using 3D human motion guidance. Traditional methods struggle with complex human movements and lack control over the generated dynamics, but PoseFuse3D-KI incorporates 3D geometric signals to enhance the interpolation process. The framework utilizes a novel SMPL-X encoder to convert 3D shapes into a 2D latent space, allowing for better integration of spatial cues with 2D pose data. Evaluation on the new CHKI-Video dataset shows significant improvements in interpolation quality, outperforming existing methods in both PSNR and LPIPS metrics."
                },
                "zh": {
                    "title": "PoseFuse3Dï¼šå¯æ§çš„äººä½“å…³é”®å¸§æ’å€¼æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶PoseFuse3Då…³é”®å¸§æ’å€¼å™¨ï¼ˆPoseFuse3D-KIï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å°†3Däººä½“å¼•å¯¼ä¿¡å·æ•´åˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå®ç°å¯æ§çš„äººä½“ä¸­å¿ƒå…³é”®å¸§æ’å€¼ï¼ˆCHKIï¼‰ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPoseFuse3D-KIèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„äººä½“è¿åŠ¨ï¼Œå¹¶æä¾›æ›´é«˜çš„åˆæˆåŠ¨æ€æ§åˆ¶èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿ç”¨äº†ä¸€ç§æ–°é¢–çš„SMPL-Xç¼–ç å™¨ï¼Œå°†3Då‡ ä½•å½¢çŠ¶è½¬æ¢ä¸º2Dæ½œåœ¨æ¡ä»¶ç©ºé—´ï¼Œå¹¶é€šè¿‡èåˆç½‘ç»œå°†è¿™äº›3Dçº¿ç´¢ä¸2Då§¿æ€åµŒå…¥ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPoseFuse3D-KIåœ¨CHKI-Videoæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼ŒPSNRæé«˜äº†9%ï¼ŒLPIPSå‡å°‘äº†38%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02678",
            "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
            "url": "https://huggingface.co/papers/2506.02678",
            "abstract": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.",
            "score": 1,
            "issue_id": 4124,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "7c7528bb32eeb5a4",
            "authors": [
                "Zhong-Zhi Li",
                "Xiao Liang",
                "Zihao Tang",
                "Lei Ji",
                "Peijie Wang",
                "Haotian Xu",
                "Xing W",
                "Haizhen Huang",
                "Weiwei Deng",
                "Ying Nian Wu",
                "Yeyun Gong",
                "Zhijiang Guo",
                "Xiao Liu",
                "Fei Yin",
                "Cheng-Lin Liu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Institute of Automation, Chinese Academy of Sciences",
                "Microsoft",
                "School of Artificial Intelligence, Chinese Academy of Sciences",
                "Tsinghua University",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02678.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#training",
                    "#long_context",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² LLM: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ²ĞµÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ System-1 Ğ¸ System-2. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğ° 40%, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient Reasoning in LLMs with Dynamic Training",
                    "desc": "This paper presents a new training method for Large Language Models (LLMs) that improves their reasoning efficiency during inference, especially for long outputs. The proposed dynamic ratio-based training pipeline balances the model's System-1 and System-2 data without needing complex data annotations. By doing so, it reduces unnecessary reasoning steps while keeping the model's reasoning abilities intact. The results show a significant reduction in output tokens by about 40%, while still achieving high accuracy across various benchmarks."
                },
                "zh": {
                    "title": "åŠ¨æ€æ¯”ä¾‹è®­ç»ƒï¼Œæå‡æ¨ç†æ•ˆç‡ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€æ¯”ä¾‹çš„è®­ç»ƒæµç¨‹ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶çš„æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æé•¿è¾“å‡ºæ—¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¹³è¡¡æ¨¡å‹çš„System-1å’ŒSystem-2æ•°æ®çš„æƒé‡ï¼Œæ¶ˆé™¤å†—ä½™æ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨DeepSeek-R1-Distill-7Bå’ŒDeepSeek-R1-Distill-14Bæ¨¡å‹ä¸Šæ˜¾è‘—å‡å°‘äº†è¿‘40%çš„è¾“å‡ºæ ‡è®°ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†å¾ˆå¿«å…¬å¼€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02138",
            "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability",
            "url": "https://huggingface.co/papers/2506.02138",
            "abstract": "A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available.",
            "score": 1,
            "issue_id": 4115,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "c83ce7f2cc033984",
            "authors": [
                "Yarden Bakish",
                "Itamar Zimerman",
                "Hila Chefer",
                "Lior Wolf"
            ],
            "affiliations": [
                "Tel-Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02138.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Layer-wise Relevance Propagation (LRP) Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ğ°Ñ€ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ-Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° LRP Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Enhancing Transformer Explainability with Positional Encoding in LRP",
                    "desc": "This paper introduces a specialized Layer-wise Relevance Propagation (LRP) method tailored for Transformer models, focusing on the importance of positional encoding in explainability. Traditional LRP methods fail to account for positional encoding, which leads to a loss of critical relevance information tied to the structure and position of tokens. By reformulating the input space into position-token pairs, the authors develop new LRP rules that effectively propagate relevance across different types of positional encodings. The proposed method shows significant improvements over existing techniques in explainability tasks for both vision and natural language processing, validated through extensive experiments."
                },
                "zh": {
                    "title": "æå‡Transformerå¯è§£é‡Šæ€§çš„ä¸“é—¨LRPæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹Transformeræ¨¡å‹çš„ä¸“é—¨åŒ–å±‚æ¬¡ç›¸å…³ä¼ æ’­ï¼ˆLRPï¼‰æ–¹æ³•ï¼Œè€ƒè™‘äº†ä½ç½®ç¼–ç çš„å½±å“ï¼Œä»è€Œæ”¹å–„äº†ç›¸å…³æ€§ä¼ æ’­ã€‚ç°æœ‰çš„LRPæ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨Transformeræ¶æ„ä¸­çš„ä½ç½®ç¼–ç ï¼Œå¯¼è‡´äº†é‡è¦ç›¸å…³æ€§çš„ä¸¢å¤±ã€‚æˆ‘ä»¬é€šè¿‡å°†è¾“å…¥ç©ºé—´é‡æ–°æ„å»ºä¸ºä½ç½®-æ ‡è®°å¯¹ï¼Œæå‡ºäº†ç†è®ºåŸºç¡€çš„LRPè§„åˆ™ï¼Œä»¥é€‚åº”ä¸åŒçš„ä½ç½®ä¿¡æ¯ç¼–ç æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„å¯è§£é‡Šæ€§ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01265",
            "title": "Beyond In-Context Learning: Aligning Long-form Generation of Large\n  Language Models via Task-Inherent Attribute Guidelines",
            "url": "https://huggingface.co/papers/2506.01265",
            "abstract": "In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.",
            "score": 1,
            "issue_id": 4124,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "2f44ffeb11509c5c",
            "authors": [
                "Do Xuan Long",
                "Duong Ngoc Yen",
                "Do Xuan Trong",
                "Luu Anh Tuan",
                "Kenji Kawaguchi",
                "Shafiq Joty",
                "Min-Yen Kan",
                "Nancy F. Chen"
            ],
            "affiliations": [
                "Institute for Infocomm Research (I2R), A*STAR",
                "Nanyang Technological University, Singapore",
                "National University of Singapore",
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01265.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#long_context",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "LongGuide: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ (ICL) Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¸Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ ICL Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ LongGuide, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. LongGuide Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Enhancing Long-Form Generation with LongGuide",
                    "desc": "This paper explores the concept of in-context learning (ICL) in large language models (LLMs), highlighting its effectiveness in improving task performance with minimal examples. However, it identifies limitations in long-form generation tasks, such as summarization, where ICL alone does not adequately teach the necessary language and format distributions. The authors propose a solution called LongGuide, which generates guidelines to enhance model performance by providing explicit exposure to task distributions. LongGuide combines Metric Guidelines and Output Constraint Guidelines to optimize generation, resulting in significant performance improvements across various LLMs in both zero- and few-shot scenarios."
                },
                "zh": {
                    "title": "æå‡ç”Ÿæˆä»»åŠ¡æ€§èƒ½çš„LongGuideæ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…ä¾é ç¤ºä¾‹ï¼ˆdemonstrationsï¼‰ä¸è¶³ä»¥è®©æ¨¡å‹æŒæ¡ç”Ÿæˆä»»åŠ¡çš„è¯­è¨€å’Œæ ¼å¼åˆ†å¸ƒã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†LongGuideï¼Œé€šè¿‡ç”Ÿæˆä¸¤æ¡å¹¶è¡Œçš„æŒ‡å¯¼æµï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡è¦æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLongGuideèƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œå­¦ä¹ èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24362",
            "title": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion",
            "url": "https://huggingface.co/papers/2505.24362",
            "abstract": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well even before a single token is generated, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.",
            "score": 1,
            "issue_id": 4118,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "8bbcc31124760dad",
            "authors": [
                "Anum Afzal",
                "Florian Matthes",
                "Gal Chechik",
                "Yftah Ziser"
            ],
            "affiliations": [
                "Bar-Ilan University",
                "Nvidia Research",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24362.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ½Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑ…Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğ¾Ğ¼ (zero-shot Chain-of-Thought) Ğ´Ğ¾ ĞµĞ³Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½Ğ½ĞµĞµ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ CoT, Ñ…Ğ¾Ñ‚Ñ Ğ¸ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Early Insights in Chain-of-Thought Reasoning",
                    "desc": "This paper explores the predictability of success in zero-shot Chain-of-Thought (CoT) reasoning processes before they are fully completed. The authors find that a probing classifier using representations from large language models (LLMs) can effectively predict outcomes even before generating any tokens, indicating that essential reasoning information is present early on. In contrast, a BERT-based model that relies on generated tokens performs poorly, as it focuses on superficial linguistic features rather than deeper reasoning. The study suggests that early stopping in CoT reasoning can still yield better performance than not using CoT at all, and proposes that future methods could utilize their classifier to determine when early stopping is beneficial."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†æ•ˆç‡ï¼Œæ—©æœŸåœæ­¢ä¹Ÿèƒ½æœ‰æ•ˆ",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†é›¶-shot Chain-of-Thought (CoT) è¿‡ç¨‹çš„æˆåŠŸæ˜¯å¦å¯ä»¥åœ¨å®Œæˆä¹‹å‰è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç¤ºçš„æ¢æµ‹åˆ†ç±»å™¨åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªæ ‡è®°ä¹‹å‰å°±è¡¨ç°è‰¯å¥½ï¼Œè¿™è¡¨æ˜æ¨ç†è¿‡ç¨‹ä¸­çš„å…³é”®ä¿¡æ¯åœ¨åˆå§‹æ­¥éª¤çš„è¡¨ç¤ºä¸­å·²ç»å­˜åœ¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¾èµ–ç”Ÿæˆæ ‡è®°çš„å¼ºBERTåŸºçº¿è¡¨ç°è¾ƒå·®ï¼Œå¯èƒ½æ˜¯å› ä¸ºå®ƒä¾èµ–äºæµ…å±‚è¯­è¨€çº¿ç´¢è€Œéæ›´æ·±å±‚çš„æ¨ç†åŠ¨æ€ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ—©æœŸåœæ­¢æ¨ç†å¯ä»¥æé«˜æ€§èƒ½ï¼Œå°½ç®¡ä¸å®Œæ•´æ¨ç†ç›¸æ¯”ä»æœ‰å·®è·ï¼Œè¿™ä¸ºä¼˜åŒ–CoTçš„æ•ˆç‡æä¾›äº†æ–°çš„æ€è·¯ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-03.html",
    "link_next": "2025-06-05.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "03.06",
        "en": "06/03",
        "zh": "6æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "05.06",
        "en": "06/05",
        "zh": "6æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 17,
        "#data": 4,
        "#benchmark": 17,
        "#agents": 5,
        "#cv": 8,
        "#rl": 11,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 8,
        "#multimodal": 14,
        "#math": 1,
        "#multilingual": 3,
        "#architecture": 8,
        "#healthcare": 0,
        "#training": 20,
        "#robotics": 3,
        "#agi": 1,
        "#games": 8,
        "#interpretability": 1,
        "#reasoning": 17,
        "#transfer_learning": 5,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 19,
        "#survey": 0,
        "#diffusion": 10,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 5,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 2,
        "#science": 3,
        "#low_resource": 2
    },
    "zh": {
        "text": "VS-Benchæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„ç­–ç•¥æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†å¤§å¤šå±€é™äºå•æ™ºèƒ½ä½“æˆ–ä»…æ–‡æœ¬ç¯å¢ƒï¼Œè€ŒVS-BenchåŒ…å«å…«ä¸ªåŸºäºè§†è§‰çš„ç¯å¢ƒï¼Œæ¶µç›–åˆä½œã€ç«äº‰å’Œæ··åˆåŠ¨æœºçš„äº’åŠ¨ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œå½’ä¸€åŒ–å›æŠ¥æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚VS-Benchæ—¨åœ¨æ ‡å‡†åŒ–è¯„ä¼°å¹¶æŒ‡å‡ºç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œæ¨åŠ¨æœªæ¥ç ”ç©¶ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨https://vs-bench.github.ioè·å–ã€‚",
        "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
        "pinyin": "ä»¥ä¸‹æ˜¯æ‚¨æä¾›çš„æ–‡æœ¬çš„æ‹¼éŸ³è½¬å†™ï¼š\n\nVS-Bench shÃ¬ yÄ«gÃ¨ duÅ mÃ³ tÃ i jÄ«zhÇ”n, yÃ²ngyÃº pÃ­nggÇ” shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng zÃ i fÃ¹zÃ¡ dÅu zhÃ¬nÃ©ngtÇ huÃ¡njÃ¬ng zhÅng de cÃ¨lÃ¼Ã¨ tuÄ«lÇ hÃ© juÃ©cÃ¨ nÃ©nglÃ¬. XiÃ n yÇ’u de jÄ«zhÇ”n dÃ duÅ jÃºxÃ¬an zÃ i dÄn zhÃ¬nÃ©ngtÇ huÃ² jÇn wÃ©nbÄ›n huÃ¡njÃ¬ng, Ã©r VS-Bench hÃ¡n yÇ’u bÄ gÃ¨ jÄ«chÇ” shÃ¬juÃ© de huÃ¡njÃ¬ng, hÃ¡nfÃ¹ hÃ©zuÃ², jÃ¬ngzhÄ“ng hÃ© hÃ¹nhÃ© dÃ²ngjÄ« de hÃ¹dÃ²ng. YÃ¡njiÅ« fÄxiÃ n, xiÃ n yÇ’u mÃ³xÃ­ng zÃ i yÃ¹cÃ¨ zhÇ”nquÃ¨xÃ¬ng hÃ© guÄ«yÄ«huÃ  huÃ­bÃ o fÄngmiÃ n cÃºnzÃ i xiÇnzhÃ¹ chÄjÃ¹. VS-Bench zhÇ yÃº biÄozhÇ”nhuÃ  pÃ­nggÇ” Ã©r zhÇchÅ« xiÃ n yÇ’u mÃ³xÃ­ng de jÃºxÃ¬anxÃ¬ng, tuÄ«dÃ²ng wÃ¨ilÃ¡i yÃ¡njiÅ«. DÃ imÇ hÃ© shÃ¹jÃ¹ kÄ› zÃ i https://vs-bench.github.io huÃ²qÇ”.",
        "vocab": "[{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'è§†è§‰è¯­è¨€æ¨¡å‹', 'pinyin': 'shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'vision-language model'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'}, {'word': 'å¤šæ™ºèƒ½ä½“', 'pinyin': 'duÅ zhÃ¬ nÃ©ng tÇ', 'trans': 'multi-agent'}, {'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡n jÃ¬ng', 'trans': 'environment'}, {'word': 'ç­–ç•¥æ¨ç†', 'pinyin': 'cÃ¨ lÃ¼Ã¨ tuÄ« lÇ', 'trans': 'strategic reasoning'}, {'word': 'å†³ç­–', 'pinyin': 'juÃ© cÃ¨', 'trans': 'decision-making'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}, {'word': 'å±€é™äº', 'pinyin': 'jÃº xiÃ n yÃº', 'trans': 'limited to'}, {'word': 'å•æ™ºèƒ½ä½“', 'pinyin': 'dÄn zhÃ¬ nÃ©ng tÇ', 'trans': 'single-agent'}, {'word': 'ä»…', 'pinyin': 'jÇn', 'trans': 'only'}, {'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'}, {'word': 'æ¶µç›–', 'pinyin': 'hÃ¡n gÃ i', 'trans': 'cover'}, {'word': 'åˆä½œ', 'pinyin': 'hÃ© zuÃ²', 'trans': 'cooperation'}, {'word': 'ç«äº‰', 'pinyin': 'jÃ¬ng zhÄ“ng', 'trans': 'competition'}, {'word': 'æ··åˆ', 'pinyin': 'hÃ¹n hÃ©', 'trans': 'hybrid'}, {'word': 'åŠ¨æœº', 'pinyin': 'dÃ²ng jÄ«', 'trans': 'motivation'}, {'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹ dÃ²ng', 'trans': 'interaction'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'å‘ç°', 'pinyin': 'fÄ xiÃ n', 'trans': 'find'}, {'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'prediction'}, {'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”n quÃ¨ xÃ¬ng', 'trans': 'accuracy'}, {'word': 'å½’ä¸€åŒ–', 'pinyin': 'guÄ« yÄ« huÃ ', 'trans': 'normalization'}, {'word': 'å›æŠ¥', 'pinyin': 'huÃ­ bÃ o', 'trans': 'reward'}, {'word': 'æ–¹é¢', 'pinyin': 'fÄng miÃ n', 'trans': 'aspect'}, {'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'}, {'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'}, {'word': 'æ ‡å‡†åŒ–', 'pinyin': 'biÄo zhÇ”n huÃ ', 'trans': 'standardize'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluate'}, {'word': 'æŒ‡å‡º', 'pinyin': 'zhÇ chÅ«', 'trans': 'point out'}, {'word': 'å±€é™æ€§', 'pinyin': 'jÃº xiÃ n xÃ¬ng', 'trans': 'limitation'}, {'word': 'æ¨åŠ¨', 'pinyin': 'tuÄ« dÃ²ng', 'trans': 'promote'}, {'word': 'æœªæ¥', 'pinyin': 'wÃ¨i lÃ¡i', 'trans': 'future'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}, {'word': 'æ•°æ®', 'pinyin': 'shÃ¹ jÃ¹', 'trans': 'data'}, {'word': 'è·å–', 'pinyin': 'huÃ² qÇ”', 'trans': 'obtain'}]",
        "trans": "VS-Bench is a multimodal benchmark designed to evaluate the strategic reasoning and decision-making capabilities of vision-language models in complex multi-agent environments. Existing benchmarks are largely limited to single-agent or text-only environments, whereas VS-Bench includes eight visually-based environments that encompass cooperative, competitive, and mixed-motive interactions. Research has revealed significant gaps in the predictive accuracy and normalized returns of existing models. VS-Bench aims to standardize evaluations and highlight the limitations of current models, driving future research. The code and data are available at https://vs-bench.github.io.",
        "update_ts": "2025-06-04 09:12"
    }
}