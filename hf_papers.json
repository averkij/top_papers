{
    "date": {
        "ru": "3 января",
        "en": "January 3",
        "zh": "1月3日"
    },
    "time_utc": "2025-01-04 18:27",
    "weekday": 4,
    "issue_id": 1497,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.00958",
            "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
            "url": "https://huggingface.co/papers/2501.00958",
            "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality multimodal textbook corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~Our code are available at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}.",
            "score": 60,
            "issue_id": 1475,
            "pub_date": "2025-01-01",
            "pub_date_card": {
                "ru": "1 января",
                "en": "January 1",
                "zh": "1月1日"
            },
            "hash": "b10f0cd62f6334fc",
            "authors": [
                "Wenqi Zhang",
                "Hang Zhang",
                "Xin Li",
                "Jiashuo Sun",
                "Yongliang Shen",
                "Weiming Lu",
                "Deli Zhao",
                "Yueting Zhuang",
                "Lidong Bing"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University",
                "DAMO Academy, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00958.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#cv",
                    "#video"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "Мультимодальный учебник: новый стандарт для обучения VLM",
                    "desc": "Эта статья представляет новый подход к обучению моделей компьютерного зрения и обработки естественного языка (VLM) с использованием мультимодального учебного корпуса. Авторы создали базу данных из 22 000 часов обучающих видео, систематически собранных с помощью таксономии, предложенной языковой моделью (LLM). Этот корпус отличается более высокой плотностью знаний, лучшей связью между изображениями и текстом, а также логической согласованностью по сравнению с существующими наборами данных. Эксперименты показывают превосходную производительность предобучения на этом корпусе, особенно в задачах, требующих глубоких знаний и рассуждений."
                },
                "en": {
                    "title": "Harnessing Instructional Videos for Superior Vision-Language Model Training",
                    "desc": "This paper presents a new approach to training Vision-Language Models (VLMs) using a multimodal textbook corpus derived from instructional videos. Unlike traditional datasets that often suffer from low knowledge density and weak image-text relationships, this corpus offers a richer and more coherent context for VLM pretraining. The authors systematically extract visual, audio, and textual information from over 22,000 hours of instructional content, enhancing the alignment between images and text. Experiments show that VLMs trained on this video-centric dataset perform significantly better on knowledge-intensive tasks, demonstrating improved reasoning and context awareness."
                },
                "zh": {
                    "title": "视频教材：提升视觉语言模型的知识与推理能力",
                    "desc": "本文提出了一种高质量的多模态教材语料库，旨在为视觉语言模型（VLM）提供更丰富的基础知识。该语料库收集了超过2.5年的教学视频，总计22,000小时，系统性地提取了视频中的视觉、音频和文本知识。与现有的数据集相比，这种视频中心的教材提供了更连贯的上下文、更丰富的知识和更好的图像-文本对齐。实验结果表明，基于该教材预训练的VLM在知识和推理密集型任务中表现优异，尤其在ScienceQA和MathVista等任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01257",
            "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings",
            "url": "https://huggingface.co/papers/2501.01257",
            "abstract": "With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is a growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competition-level coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce a unique judging method in which problems are submitted directly to the platform and develop a reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CodeElo, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 20 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies.",
            "score": 36,
            "issue_id": 1475,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "e31430bb6ba5dfc8",
            "authors": [
                "Shanghaoran Quan",
                "Jiaxi Yang",
                "Bowen Yu",
                "Bo Zheng",
                "Dayiheng Liu",
                "An Yang",
                "Xuancheng Ren",
                "Bofei Gao",
                "Yibo Miao",
                "Yunlong Feng",
                "Zekun Wang",
                "Jian Yang",
                "Zeyu Cui",
                "Yang Fan",
                "Yichang Zhang",
                "Binyuan Hui",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01257.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🏆",
                "ru": {
                    "title": "CodeElo: новый стандарт оценки LLM в соревновательном программировании",
                    "desc": "Статья представляет новый бенчмарк CodeElo для оценки способностей больших языковых моделей (LLM) в решении задач по программированию соревновательного уровня. CodeElo основан на платформе CodeForces и включает проблемы с детальной информацией о сложности и алгоритмических тегах. Авторы разработали систему расчета рейтинга Эло, сопоставимую с рейтингами человеческих участников. Результаты тестирования 33 LLM показали, что модели o1-mini и QwQ-32B-Preview значительно превосходят остальные, достигая рейтингов 1578 и 1261 соответственно."
                },
                "en": {
                    "title": "CodeElo: Elevating Code Generation Benchmarks for LLMs",
                    "desc": "This paper presents CodeElo, a new benchmark designed to evaluate the coding abilities of large language models (LLMs) in a competitive setting. Unlike existing benchmarks, CodeElo addresses limitations such as the lack of private test cases and misaligned execution environments by utilizing the CodeForces platform. The benchmark includes a unique judging method and an Elo rating system that allows for fair comparisons between LLMs and human participants. Results indicate that certain models, like o1-mini, perform significantly better than others, highlighting the varying capabilities of LLMs in code generation tasks."
                },
                "zh": {
                    "title": "CodeElo：提升代码生成能力的标准化基准测试",
                    "desc": "随着大型语言模型（LLMs）在代码推理能力上的提升，开发更具挑战性和全面性的基准测试变得愈发重要。现有的基准测试如LiveCodeBench和USACO存在一些不足，例如缺乏私有测试用例和特殊评判支持。为了解决这些问题，我们提出了CodeElo，这是一个标准化的竞赛级代码生成基准，首次有效应对这些挑战。通过在CodeForces平台上编译最近六个月的竞赛问题，我们为30个流行的开源和3个专有LLMs提供了Elo评分，结果显示o1-mini和QwQ-32B-Preview表现突出。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01427",
            "title": "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control",
            "url": "https://huggingface.co/papers/2501.01427",
            "abstract": "Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.",
            "score": 36,
            "issue_id": 1474,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "4c67f688775a3eca",
            "authors": [
                "Yuanpeng Tu",
                "Hao Luo",
                "Xi Chen",
                "Sihui Ji",
                "Xiang Bai",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "HUST",
                "Hupan Lab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01427.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Точная вставка объектов в видео с сохранением деталей",
                    "desc": "В этой статье представлен VideoAnydoor - фреймворк для вставки объектов в видео без предварительного обучения. Он использует экстрактор идентификаторов и последовательность ограничивающих рамок для контроля движения объекта. Ключевым компонентом является пиксельный варпер, который сохраняет детали внешнего вида и позволяет точно управлять движением. Предложенная стратегия обучения с использованием видео и статических изображений улучшает качество вставки объектов."
                },
                "en": {
                    "title": "Seamless Object Insertion in Videos with VideoAnydoor",
                    "desc": "This paper introduces VideoAnydoor, a novel framework for zero-shot video object insertion that excels in maintaining high-fidelity details and precise motion control. The approach begins with a text-to-video model and incorporates an ID extractor to ensure consistent object identity while using a box sequence for motion management. A key innovation is the pixel warper, which adjusts pixel details based on key-point trajectories, enhancing both detail preservation and user control over motion. The proposed training strategy, which combines videos and static images with a reweighted reconstruction loss, significantly improves the quality of object insertion, making VideoAnydoor versatile for various applications without needing specific fine-tuning."
                },
                "zh": {
                    "title": "高保真视频对象插入的新突破",
                    "desc": "尽管视频生成技术取得了显著进展，但将特定对象插入视频仍然是一项具有挑战性的任务。本文提出了VideoAnydoor，这是一个零-shot视频对象插入框架，能够高保真地保留细节并精确控制运动。我们设计了一种像素变形器，能够根据关键点轨迹扭曲像素细节，并与扩散U-Net融合，从而提高细节保留能力。VideoAnydoor在现有方法中表现出显著优势，并支持多种下游应用，无需特定任务的微调。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01423",
            "title": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models",
            "url": "https://huggingface.co/papers/2501.01423",
            "abstract": "Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs--representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: https://github.com/hustvl/LightningDiT.",
            "score": 30,
            "issue_id": 1473,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "173fa21b6e47d04c",
            "authors": [
                "Jingfeng Yao",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01423.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Революция в латентных диффузионных моделях: быстрее, лучше, эффективнее",
                    "desc": "Статья представляет новый подход к улучшению латентных диффузионных моделей с архитектурой Трансформер для генерации изображений высокого качества. Авторы предлагают метод VA-VAE, который выравнивает латентное пространство с предобученными моделями компьютерного зрения. Это позволяет значительно расширить границы реконструкции-генерации и ускорить сходимость Диффузионных Трансформеров в высокоразмерных латентных пространствах. На основе VA-VAE авторы создали улучшенную модель LightningDiT, достигающую современного уровня производительности на задаче генерации изображений ImageNet 256x256."
                },
                "en": {
                    "title": "Accelerating Image Generation with Aligned Latent Spaces",
                    "desc": "This paper discusses the challenges faced by latent diffusion models, particularly when using Transformer architectures for image generation. It highlights an optimization issue where increasing the feature dimensions in visual tokenizers can lead to larger models and longer training times, often resulting in sub-optimal image quality. The authors propose a solution by aligning the latent space with pre-trained vision models, introducing a new framework called VA-VAE to enhance the training process. Their improved model, LightningDiT, achieves state-of-the-art performance in image generation while significantly speeding up the training process."
                },
                "zh": {
                    "title": "优化潜在扩散模型，提升图像生成效率",
                    "desc": "本论文探讨了潜在扩散模型与变换器架构在生成高质量图像时的优化困境。研究表明，虽然增加视觉标记器中的每个标记特征维度可以提高重建质量，但这也导致需要更大的扩散模型和更多的训练迭代。为了解决这一问题，作者提出将潜在空间与预训练的视觉基础模型对齐，从而提高训练效率。最终，提出的VA-VAE模型显著提升了潜在扩散模型的重建生成能力，并在ImageNet数据集上实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00599",
            "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM",
            "url": "https://huggingface.co/papers/2501.00599",
            "abstract": "Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.",
            "score": 29,
            "issue_id": 1474,
            "pub_date": "2025-12-31",
            "pub_date_card": {
                "ru": "31 декабря",
                "en": "December 31",
                "zh": "12月31日"
            },
            "hash": "daee687ce36ef3db",
            "authors": [
                "Yuqian Yuan",
                "Hang Zhang",
                "Wentong Li",
                "Zesen Cheng",
                "Boqiang Zhang",
                "Long Li",
                "Xin Li",
                "Deli Zhao",
                "Wenqiao Zhang",
                "Yueting Zhuang",
                "Jianke Zhu",
                "Lidong Bing"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00599.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Точное пространственно-временное понимание видео с помощью VideoRefer Suite",
                    "desc": "Статья представляет VideoRefer Suite - комплексный подход к улучшению пространственно-временного понимания видео большими языковыми моделями. Авторы разработали масштабный набор данных VideoRefer-700K с инструкциями на уровне объектов, созданный с помощью мультиагентного движка. Они также представили модель VideoRefer с универсальным пространственно-временным кодировщиком объектов. Для оценки возможностей видео-LLM был создан бенчмарк VideoRefer-Bench, охватывающий различные аспекты понимания видео."
                },
                "en": {
                    "title": "Empowering Video LLMs for Fine-Grained Understanding",
                    "desc": "This paper introduces the VideoRefer Suite, which enhances Video Large Language Models (Video LLMs) for better understanding of videos by focusing on fine-grained spatial and temporal details. It addresses the limitations of existing models that primarily focus on overall comprehension and lack high-quality object-level instruction data. The suite includes a new dataset called VideoRefer-700K, a specialized VideoRefer model with a spatial-temporal object encoder, and a benchmark for evaluating video understanding capabilities. Experimental results show that the VideoRefer model significantly improves performance on video referring tasks while also enhancing general video comprehension."
                },
                "zh": {
                    "title": "提升视频理解，细致捕捉空间与时间",
                    "desc": "视频大型语言模型（Video LLMs）在视频理解方面展现了出色的能力，但在捕捉细粒度的空间和时间细节上存在困难。为了应对这些挑战，我们提出了VideoRefer Suite，以增强视频LLM在空间-时间视频理解方面的能力。我们开发了一个多代理数据引擎，创建了一个高质量的对象级视频指令数据集VideoRefer-700K，并提出了VideoRefer模型，配备了多功能的空间-时间对象编码器。最后，我们创建了VideoRefer-Bench，以全面评估视频LLM的空间-时间理解能力，实验结果表明我们的模型在视频引用基准上表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01264",
            "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
            "url": "https://huggingface.co/papers/2501.01264",
            "abstract": "Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools.",
            "score": 21,
            "issue_id": 1473,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "bda3f96e83319526",
            "authors": [
                "Xiaoshuai Song",
                "Yanan Wu",
                "Weixun Wang",
                "Jiaheng Liu",
                "Wenbo Su",
                "Bo Zheng"
            ],
            "affiliations": [
                "Taobao & Tmall Group of Alibaba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01264.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#reasoning",
                    "#interpretability",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ProgCo: Самокоррекция языковых моделей через программно-управляемую верификацию и уточнение",
                    "desc": "Эта статья представляет новый подход к самокоррекции больших языковых моделей (LLM) под названием Program-driven Self-Correction (ProgCo). Метод включает в себя программно-управляемую верификацию (ProgVe), которая использует самогенерируемые и самовыполняющиеся псевдопрограммы для сложной логики проверки. Затем программно-управляемое уточнение (ProgRe) проводит двойную рефлексию и улучшение как ответов, так и программ верификации. Эксперименты показали, что ProgCo эффективен в самокоррекции и может дополнительно улучшить производительность при комбинировании с реальными программными инструментами."
                },
                "en": {
                    "title": "Empowering LLMs with Program-Driven Self-Correction",
                    "desc": "This paper introduces Program-driven Self-Correction (ProgCo) to improve the self-verification and self-refinement capabilities of large language models (LLMs). It addresses the common issue where LLMs struggle to provide accurate feedback, which can lead to incorrect refinements, particularly in complex reasoning tasks. ProgCo utilizes program-driven verification (ProgVe) to create self-executing verification pseudo-programs that enhance the verification process. Additionally, program-driven refinement (ProgRe) allows the model to reflect on and refine both its responses and the verification programs, leading to more reliable self-correction outcomes."
                },
                "zh": {
                    "title": "基于程序的自我纠正：提升语言模型的自我验证能力",
                    "desc": "自我纠正旨在使大型语言模型（LLMs）能够在没有外部反馈的情况下自我验证和自我完善其初始响应。然而，LLMs往往无法有效自我验证并生成正确的反馈，这会进一步误导其完善过程，尤其是在复杂推理任务中。本文提出了基于程序的自我纠正（ProgCo），通过自生成、自执行的验证伪程序实现复杂的验证逻辑和广泛的验证。实验结果表明，ProgCo在三个指令遵循和数学基准测试中实现了有效的自我纠正，并且与真实程序工具结合时可以进一步提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01149",
            "title": "A3: Android Agent Arena for Mobile GUI Agents",
            "url": "https://huggingface.co/papers/2501.01149",
            "abstract": "AI agents have become increasingly prevalent in recent years, driven by significant advancements in the field of large language models (LLMs). Mobile GUI agents, a subset of AI agents, are designed to autonomously perform tasks on mobile devices. While numerous studies have introduced agents, datasets, and benchmarks to advance mobile GUI agent research, many existing datasets focus on static frame evaluations and fail to provide a comprehensive platform for assessing performance on real-world, in-the-wild tasks. To address this gap, we present Android Agent Arena (A3), a novel evaluation platform. Unlike existing in-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as real-time online information retrieval and operational instructions; (2) a larger, more flexible action space, enabling compatibility with agents trained on any dataset; and (3) automated business-level LLM-based evaluation process. A3 includes 21 widely used general third-party apps and 201 tasks representative of common user scenarios, providing a robust foundation for evaluating mobile GUI agents in real-world situations and a new autonomous evaluation process for less human labor and coding expertise. The project is available at https://yuxiangchai.github.io/Android-Agent-Arena/.",
            "score": 20,
            "issue_id": 1474,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "050f155aa526c100",
            "authors": [
                "Yuxiang Chai",
                "Hanhao Li",
                "Jiayu Zhang",
                "Liang Liu",
                "Guozhi Wang",
                "Shuai Ren",
                "Siyuan Huang",
                "Hongsheng Li"
            ],
            "affiliations": [
                "EE department @ CUHK",
                "MMLab @ CUHK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01149.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "A3: Арена для тестирования мобильных AI-агентов в реальном мире",
                    "desc": "Статья представляет новую платформу для оценки мобильных GUI-агентов под названием Android Agent Arena (A3). A3 предлагает реалистичные задачи, широкое пространство действий и автоматизированную оценку на основе больших языковых моделей. Платформа включает 21 популярное стороннее приложение и 201 задачу, отражающую типичные пользовательские сценарии. A3 позволяет оценивать производительность агентов в реальных условиях, что отличает её от существующих статических наборов данных."
                },
                "en": {
                    "title": "Revolutionizing Mobile GUI Agent Evaluation with A3",
                    "desc": "This paper introduces the Android Agent Arena (A3), a new evaluation platform for mobile GUI agents that addresses limitations in existing datasets. A3 focuses on real-world tasks, providing a larger action space that accommodates agents trained on various datasets. It features 21 popular third-party apps and 201 tasks that reflect common user scenarios, enhancing the assessment of agent performance. Additionally, A3 incorporates an automated evaluation process using large language models, reducing the need for extensive human involvement and coding skills."
                },
                "zh": {
                    "title": "Android Agent Arena：移动GUI代理的新评估平台",
                    "desc": "近年来，人工智能代理的应用越来越广泛，尤其是在大型语言模型（LLMs）领域的进步推动下。移动图形用户界面（GUI）代理是人工智能代理的一种，旨在自主执行移动设备上的任务。现有的研究虽然提出了许多代理、数据集和基准，但大多数数据集仅关注静态框架评估，无法全面评估真实世界中的任务表现。为了解决这一问题，我们提出了Android Agent Arena（A3），这是一个新颖的评估平台，提供了实际的任务和更灵活的操作空间，支持基于LLM的自动化评估过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00103",
            "title": "LTX-Video: Realtime Video Latent Diffusion",
            "url": "https://huggingface.co/papers/2501.00103",
            "abstract": "We introduce LTX-Video, a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled by relocating the patchifying operation from the transformer's input to the VAE's input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal self-attention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. Our model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities trained simultaneously. It achieves faster-than-real-time generation, producing 5 seconds of 24 fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all existing models of similar scale. The source code and pre-trained models are publicly available, setting a new benchmark for accessible and scalable video generation.",
            "score": 19,
            "issue_id": 1484,
            "pub_date": "2025-12-30",
            "pub_date_card": {
                "ru": "30 декабря",
                "en": "December 30",
                "zh": "12月30日"
            },
            "hash": "a2358f7cf156ff08",
            "authors": [
                "Yoav HaCohen",
                "Nisan Chiprut",
                "Benny Brazowski",
                "Daniel Shalem",
                "Dudu Moshe",
                "Eitan Richardson",
                "Eran Levin",
                "Guy Shiran",
                "Nir Zabari",
                "Ori Gordon",
                "Poriya Panet",
                "Sapir Weissbuch",
                "Victor Kulikov",
                "Yaki Bitterman",
                "Zeev Melumian",
                "Ofir Bibi"
            ],
            "affiliations": [
                "Lightricks"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00103.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Революция в генерации видео: быстрее реального времени",
                    "desc": "LTX-Video - это трансформерная модель латентной диффузии для генерации видео. Она объединяет функции Video-VAE и шумоподавляющего трансформера, оптимизируя их взаимодействие. Модель использует сильно сжатое латентное пространство, позволяя трансформеру эффективно выполнять полное пространственно-временное самовнимание. LTX-Video поддерживает генерацию видео из текста и изображений, превосходя существующие модели по скорости и качеству."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with LTX-Video",
                    "desc": "LTX-Video is a novel transformer-based latent diffusion model designed for efficient video generation by integrating the roles of Video-VAE and denoising transformers. It achieves a high compression ratio of 1:192, allowing the model to operate in a compressed latent space while maintaining spatiotemporal self-attention for generating high-resolution videos. The model's VAE decoder performs both latent-to-pixel conversion and denoising, enabling the generation of fine details without the need for a separate upsampling module. With capabilities for text-to-video and image-to-video generation, LTX-Video produces videos faster than real-time, setting a new standard in the field."
                },
                "zh": {
                    "title": "LTX-Video：高效视频生成的新标准",
                    "desc": "LTX-Video是一种基于变换器的潜在扩散模型，旨在通过整合视频生成中的Video-VAE和去噪变换器的功能来提高效率和质量。该模型的核心是一个高压缩比的Video-VAE，能够在压缩的潜在空间中高效执行时空自注意力，从而生成高分辨率且具有时间一致性的视频。为了克服高压缩带来的细节损失，VAE解码器同时负责潜在到像素的转换和最终的去噪步骤，直接在像素空间中生成清晰的结果。LTX-Video支持多种应用场景，包括文本到视频和图像到视频的生成，并且在Nvidia H100 GPU上以超实时速度生成视频，设立了视频生成的新基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00316",
            "title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models",
            "url": "https://huggingface.co/papers/2501.00316",
            "abstract": "Recent advancements in foundation models have enhanced AI systems' capabilities in autonomous tool usage and reasoning. However, their ability in location or map-based reasoning - which improves daily life by optimizing navigation, facilitating resource discovery, and streamlining logistics - has not been systematically studied. To bridge this gap, we introduce MapEval, a benchmark designed to assess diverse and complex map-based user queries with geo-spatial reasoning. MapEval features three task types (textual, API-based, and visual) that require collecting world information via map tools, processing heterogeneous geo-spatial contexts (e.g., named entities, travel distances, user reviews or ratings, images), and compositional reasoning, which all state-of-the-art foundation models find challenging. Comprising 700 unique multiple-choice questions about locations across 180 cities and 54 countries, MapEval evaluates foundation models' ability to handle spatial relationships, map infographics, travel planning, and navigation challenges. Using MapEval, we conducted a comprehensive evaluation of 28 prominent foundation models. While no single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro achieved competitive performance overall. However, substantial performance gaps emerged, particularly in MapEval, where agents with Claude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21%, respectively, and the gaps became even more amplified when compared to open-source LLMs. Our detailed analyses provide insights into the strengths and weaknesses of current models, though all models still fall short of human performance by more than 20% on average, struggling with complex map images and rigorous geo-spatial reasoning. This gap highlights MapEval's critical role in advancing general-purpose foundation models with stronger geo-spatial understanding.",
            "score": 19,
            "issue_id": 1477,
            "pub_date": "2025-12-31",
            "pub_date_card": {
                "ru": "31 декабря",
                "en": "December 31",
                "zh": "12月31日"
            },
            "hash": "a4e45c6bd9d30ff4",
            "authors": [
                "Mahir Labib Dihan",
                "Md Tanvir Hassan",
                "Md Tanvir Parvez",
                "Md Hasebul Hasan",
                "Md Almash Alam",
                "Muhammad Aamir Cheema",
                "Mohammed Eunus Ali",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "Bangladesh Computer Council (BCC)",
                "Department of Computer Science and Engineering Bangladesh University of Engineering and Technology (BUET)",
                "Monash University",
                "Qatar Computing Research Institute (QCRI)",
                "Statistics, Islamic University Bangladesh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00316.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "MapEval: Новый рубеж в геопространственном ИИ",
                    "desc": "Статья представляет MapEval - новый бенчмарк для оценки способностей моделей искусственного интеллекта в области пространственных рассуждений и работы с картами. MapEval включает 700 вопросов с множественным выбором, охватывающих 180 городов и 54 страны, и оценивает навыки моделей в понимании пространственных отношений, инфографики карт, планирования путешествий и навигации. Авторы провели оценку 28 ведущих фундаментальных моделей, выявив значительные различия в производительности, при этом все модели все еще отстают от человеческого уровня более чем на 20%. Результаты исследования подчеркивают важность MapEval для развития моделей с более сильным геопространственным пониманием."
                },
                "en": {
                    "title": "Enhancing AI's Geo-Spatial Reasoning with MapEval",
                    "desc": "This paper introduces MapEval, a benchmark designed to evaluate the performance of foundation models in map-based reasoning tasks. It focuses on assessing how well these models can handle complex geo-spatial queries, which are essential for navigation and resource discovery. The benchmark includes various task types that require models to process diverse information, such as travel distances and user reviews, and perform compositional reasoning. The evaluation reveals that while some models perform competitively, they still lag behind human capabilities, indicating a need for further advancements in geo-spatial understanding within AI systems."
                },
                "zh": {
                    "title": "提升地图推理能力的基准评估",
                    "desc": "最近基础模型的进展提升了人工智能系统在自主工具使用和推理方面的能力。然而，它们在基于位置或地图的推理能力上尚未得到系统研究，这对于优化导航、资源发现和物流管理至关重要。为了解决这个问题，我们引入了MapEval，一个旨在评估复杂地图用户查询的基准，涉及地理空间推理。MapEval包含700个关于180个城市和54个国家的独特多项选择题，评估基础模型在处理空间关系、地图信息、旅行规划和导航挑战方面的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00192",
            "title": "MLLM-as-a-Judge for Image Safety without Human Labeling",
            "url": "https://huggingface.co/papers/2501.00192",
            "abstract": "Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks.",
            "score": 18,
            "issue_id": 1474,
            "pub_date": "2025-12-31",
            "pub_date_card": {
                "ru": "31 декабря",
                "en": "December 31",
                "zh": "12月31日"
            },
            "hash": "2a62bcbb87c1b7a5",
            "authors": [
                "Zhenting Wang",
                "Shuming Hu",
                "Shiyu Zhao",
                "Xiaowen Lin",
                "Felix Juefei-Xu",
                "Zhuowei Li",
                "Ligong Han",
                "Harihar Subramanyam",
                "Li Chen",
                "Jianfa Chen",
                "Nan Jiang",
                "Lingjuan Lyu",
                "Shiqing Ma",
                "Dimitris N. Metaxas",
                "Ankit Jain"
            ],
            "affiliations": [
                "GenAI @ Meta",
                "Rutgers University",
                "UMass Amherst",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00192.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#ethics",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Интеллектуальная защита: Zero-shot оценка безопасности изображений с помощью MLLM",
                    "desc": "Статья представляет метод определения безопасности изображений с использованием мультимодальных больших языковых моделей (MLLM) в режиме zero-shot. Авторы предлагают подход, включающий объективизацию правил безопасности, оценку релевантности между правилами и изображениями, и быстрое принятие решений на основе дебиасированных вероятностей токенов. Метод также включает каскадные цепочки рассуждений для более глубокого анализа при необходимости. Эксперименты показывают высокую эффективность предложенного метода для задач оценки безопасности изображений без предварительного обучения."
                },
                "en": {
                    "title": "Zero-Shot Image Safety Detection with MLLMs",
                    "desc": "This paper addresses the challenge of identifying unsafe images in the context of AI-generated content using Multimodal Large Language Models (MLLMs). The authors propose a novel approach that allows for zero-shot detection of harmful images by utilizing predefined safety rules without the need for extensive human labeling. They highlight the limitations of traditional methods, such as the subjectivity of safety rules and the biases present in models. The proposed method enhances safety judgment by objectifying rules, assessing their relevance to images, and employing a reasoning process that simplifies complex safety guidelines."
                },
                "zh": {
                    "title": "利用MLLMs实现零样本图像安全判断",
                    "desc": "随着在线平台视觉媒体的兴起，图像内容安全成为一个重要挑战。许多图像生成模型能够产生有害内容，因此识别不安全图像变得至关重要。我们提出了一种基于预训练多模态大语言模型（MLLMs）的方法，通过查询这些模型来检测不安全图像，而无需依赖人工标注。实验结果表明，我们的方法在零样本图像安全判断任务中非常有效。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01426",
            "title": "Unifying Specialized Visual Encoders for Video Language Models",
            "url": "https://huggingface.co/papers/2501.01426",
            "abstract": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding.",
            "score": 15,
            "issue_id": 1488,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "c868a7ebcbafa704",
            "authors": [
                "Jihoon Chung",
                "Tyler Zhu",
                "Max Gonzalez Saez-Diez",
                "Juan Carlos Niebles",
                "Honglu Zhou",
                "Olga Russakovsky"
            ],
            "affiliations": [
                "Princeton University",
                "Salesforce Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01426.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#video",
                    "#benchmark",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "MERV: Многоэнкодерное представление видео для улучшенного машинного понимания",
                    "desc": "Статья представляет MERV - новый метод для улучшения понимания видео с помощью больших языковых моделей. MERV использует несколько замороженных визуальных энкодеров для создания единого представления видео, что позволяет охватить больший объем визуальной информации. Этот подход превосходит предыдущие методы в точности на стандартных тестах понимания видео. MERV вводит минимальное количество дополнительных параметров и обучается быстрее, чем эквивалентные методы с одним энкодером."
                },
                "en": {
                    "title": "Unlocking Video Understanding with Multi-Encoder Magic!",
                    "desc": "This paper introduces MERV, a method that enhances Video Large Language Models (VideoLLMs) by using multiple visual encoders instead of just one. By combining the outputs of these encoders, MERV creates a richer representation of videos, which helps the model understand complex video content better. The approach allows for improved performance on various video understanding tasks, achieving higher accuracy than previous models. Additionally, MERV is efficient, requiring fewer parameters and training time while effectively leveraging the strengths of each encoder."
                },
                "zh": {
                    "title": "多编码器提升视频理解能力",
                    "desc": "本文介绍了一种名为MERV（多编码器视频表示）的方法，旨在提升视频理解的能力。MERV通过使用多个冻结的视觉编码器，创建视频的统一表示，从而为视频大型语言模型（VideoLLM）提供更全面的视觉知识。通过时空对齐每个编码器的特征，MERV能够更好地处理开放式和多选的视频理解问题，且在准确性上超越了之前的最佳模型。该方法不仅提高了性能，还在参数和训练速度上优于单编码器方法，展示了多视觉编码器在视频理解中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01054",
            "title": "Dynamic Scaling of Unit Tests for Code Reward Modeling",
            "url": "https://huggingface.co/papers/2501.01054",
            "abstract": "Current large language models (LLMs) often struggle to produce accurate responses on the first attempt for complex reasoning tasks like code generation. Prior research tackles this challenge by generating multiple candidate solutions and validating them with LLM-generated unit tests. The execution results of unit tests serve as reward signals to identify correct solutions. As LLMs always confidently make mistakes, these unit tests are not reliable, thereby diminishing the quality of reward signals. Motivated by the observation that scaling the number of solutions improves LLM performance, we explore the impact of scaling unit tests to enhance reward signal quality. Our pioneer experiment reveals a positive correlation between the number of unit tests and reward signal quality, with greater benefits observed in more challenging problems. Based on these insights, we propose CodeRM-8B, a lightweight yet effective unit test generator that enables efficient and high-quality unit test scaling. Additionally, we implement a dynamic scaling mechanism that adapts the number of unit tests based on problem difficulty, further improving efficiency. Experimental results show that our approach significantly improves performance across various models on three benchmarks (e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on HumanEval Plus).",
            "score": 14,
            "issue_id": 1474,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "33b9590f2acb0e48",
            "authors": [
                "Zeyao Ma",
                "Xiaokang Zhang",
                "Jing Zhang",
                "Jifan Yu",
                "Sijia Luo",
                "Jie Tang"
            ],
            "affiliations": [
                "Key Laboratory of Data Engineering and Knowledge Engineering, Beijing, China",
                "School of Information, Renmin University of China",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01054.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#training",
                    "#small_models",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Масштабирование юнит-тестов для повышения точности LLM в сложных задачах",
                    "desc": "Эта статья посвящена улучшению точности больших языковых моделей (LLM) в задачах сложного мышления, таких как генерация кода. Авторы предлагают метод масштабирования юнит-тестов для повышения качества сигналов вознаграждения при оценке решений. Они разработали легковесный генератор юнит-тестов CodeRM-8B и механизм динамического масштабирования, адаптирующийся к сложности задачи. Эксперименты показали значительное улучшение производительности различных моделей на нескольких тестовых наборах."
                },
                "en": {
                    "title": "Enhancing LLM Performance through Scaled Unit Testing",
                    "desc": "This paper addresses the limitations of large language models (LLMs) in generating accurate responses for complex tasks like code generation. It highlights the issue of unreliable reward signals from LLM-generated unit tests, which can lead to incorrect solutions. The authors propose a novel approach, CodeRM-8B, which generates a larger number of unit tests to improve the quality of these reward signals. Their experiments demonstrate that scaling unit tests enhances LLM performance, particularly for more challenging problems, leading to significant improvements across various models."
                },
                "zh": {
                    "title": "提升单元测试质量，增强模型性能",
                    "desc": "当前的大型语言模型（LLMs）在复杂推理任务（如代码生成）中，往往难以在第一次尝试时产生准确的响应。以往的研究通过生成多个候选解决方案并使用LLM生成的单元测试进行验证来应对这一挑战。单元测试的执行结果作为奖励信号，用于识别正确的解决方案。然而，由于LLMs常常自信地犯错，这些单元测试的可靠性不足，从而降低了奖励信号的质量。我们提出了CodeRM-8B，一个轻量级且有效的单元测试生成器，能够高效地扩展单元测试，并根据问题的难度动态调整单元测试的数量，从而进一步提高效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.21015",
            "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
            "url": "https://huggingface.co/papers/2412.21015",
            "abstract": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet Maps, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, a web application that streamlines the creation of reproducible, traceable map-based QA datasets. With its plug-and-play architecture, MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
            "score": 8,
            "issue_id": 1477,
            "pub_date": "2025-12-30",
            "pub_date_card": {
                "ru": "30 декабря",
                "en": "December 30",
                "zh": "12月30日"
            },
            "hash": "0d1081756b5bc4f7",
            "authors": [
                "Mahir Labib Dihan",
                "Mohammed Eunus Ali",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering Bangladesh University of Engineering and Technology (BUET)",
                "Qatar Computing Research Institute (QCRI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.21015.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#reasoning",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "MapQaTor: Революция в создании геопространственных данных для ИИ",
                    "desc": "MapQaTor - это веб-приложение, которое упрощает создание воспроизводимых наборов данных для вопросно-ответных систем на основе карт. Оно интегрируется с любым картографическим API и позволяет собирать и визуализировать данные из различных источников. MapQaTor кэширует ответы API, обеспечивая согласованность данных, и централизует процессы сбора, аннотации и визуализации. Приложение ускоряет процесс аннотации в 30 раз по сравнению с ручными методами, что делает его полезным инструментом для развития геопространственных ресурсов и оценки возможностей больших языковых моделей в области геопространственных рассуждений."
                },
                "en": {
                    "title": "Streamlining Geospatial QA with MapQaTor",
                    "desc": "This paper presents MapQaTor, a web application designed to facilitate the creation of geospatial question answering (QA) datasets using map services. It leverages recent advancements in Large Language Models (LLMs) to improve the handling of natural language queries related to locations. The platform features a plug-and-play architecture that integrates with various maps APIs, allowing users to efficiently gather, annotate, and visualize geospatial data. By caching API responses, MapQaTor ensures consistent and reliable data, significantly speeding up the annotation process and enhancing the evaluation of LLM-based geospatial reasoning capabilities."
                },
                "zh": {
                    "title": "MapQaTor：提升地图问答数据集创建效率的利器",
                    "desc": "本文介绍了MapQaTor，一个用于创建地图问答数据集的网络应用程序。它利用大型语言模型的优势，简化了从地图服务生成可重复和可追溯的数据集的过程。MapQaTor支持与任何地图API的无缝集成，并通过缓存API响应来确保数据的一致性。该平台显著提高了数据标注的效率，展示了在地理空间推理方面的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01320",
            "title": "SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration",
            "url": "https://huggingface.co/papers/2501.01320",
            "abstract": "Video restoration poses non-trivial challenges in maintaining fidelity while recovering temporally consistent details from unknown degradations in the wild. Despite recent advances in diffusion-based restoration, these methods often face limitations in generation capability and sampling efficiency. In this work, we present SeedVR, a diffusion transformer designed to handle real-world video restoration with arbitrary length and resolution. The core design of SeedVR lies in the shifted window attention that facilitates effective restoration on long video sequences. SeedVR further supports variable-sized windows near the boundary of both spatial and temporal dimensions, overcoming the resolution constraints of traditional window attention. Equipped with contemporary practices, including causal video autoencoder, mixed image and video training, and progressive training, SeedVR achieves highly-competitive performance on both synthetic and real-world benchmarks, as well as AI-generated videos. Extensive experiments demonstrate SeedVR's superiority over existing methods for generic video restoration.",
            "score": 6,
            "issue_id": 1479,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "fa277e5baed864a4",
            "authors": [
                "Jianyi Wang",
                "Zhijie Lin",
                "Meng Wei",
                "Yang Zhao",
                "Ceyuan Yang",
                "Chen Change Loy",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01320.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#long_context",
                    "#video",
                    "#training",
                    "#diffusion",
                    "#synthetic"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "SeedVR: Восстановление видео нового поколения с помощью диффузионных трансформеров",
                    "desc": "SeedVR - это диффузионный трансформер для восстановления видео в реальных условиях. Он использует сдвинутое оконное внимание для эффективной обработки длинных видеопоследовательностей. SeedVR поддерживает окна переменного размера на границах пространственных и временных измерений, преодолевая ограничения традиционного оконного внимания. Благодаря современным практикам, таким как каузальный видеоавтоэнкодер и прогрессивное обучение, SeedVR достигает высоких результатов на синтетических и реальных тестовых наборах."
                },
                "en": {
                    "title": "SeedVR: Revolutionizing Video Restoration with Diffusion Transformers",
                    "desc": "This paper introduces SeedVR, a novel diffusion transformer aimed at improving video restoration by effectively managing long sequences and varying resolutions. It utilizes shifted window attention to enhance the restoration process, allowing for better handling of temporal consistency and fidelity in videos. SeedVR incorporates advanced techniques such as causal video autoencoders and mixed training strategies to boost its performance on both synthetic and real-world datasets. The results show that SeedVR outperforms existing video restoration methods, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "SeedVR：高效的视频修复新方法",
                    "desc": "视频修复面临着在恢复未知退化的同时保持细节一致性的挑战。尽管基于扩散的修复方法有所进展，但它们在生成能力和采样效率上仍存在局限性。本文提出了SeedVR，这是一种专为处理任意长度和分辨率的真实视频修复而设计的扩散变换器。SeedVR通过移动窗口注意力机制，有效地处理长视频序列，并在空间和时间维度的边界附近支持可变大小的窗口，克服了传统窗口注意力的分辨率限制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00658",
            "title": "Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing",
            "url": "https://huggingface.co/papers/2501.00658",
            "abstract": "Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias impairs the models' ability to recall distant information and introduces robustness issues. Our scaling experiments then discovered that deeper structures in SSMs can facilitate the learning of long contexts. However, subsequent theoretical analysis reveals that as SSMs increase in depth, they exhibit another inevitable tendency toward over-smoothing, e.g., token representations becoming increasingly indistinguishable. This fundamental dilemma between recency and over-smoothing hinders the scalability of existing SSMs. Inspired by our theoretical findings, we propose to polarize two channels of the state transition matrices in SSMs, setting them to zero and one, respectively, simultaneously addressing recency bias and over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks SSMs to benefit further from deeper architectures. All source codes are released at https://github.com/VITA-Group/SSM-Bottleneck.",
            "score": 5,
            "issue_id": 1476,
            "pub_date": "2025-12-31",
            "pub_date_card": {
                "ru": "31 декабря",
                "en": "December 31",
                "zh": "12月31日"
            },
            "hash": "253304ea64defbe0",
            "authors": [
                "Peihao Wang",
                "Ruisi Cai",
                "Yuehao Wang",
                "Jiajun Zhu",
                "Pragya Srivastava",
                "Zhangyang Wang",
                "Pan Li"
            ],
            "affiliations": [
                "Georgia Tech",
                "Google DeepMind",
                "University of Texas at Austin",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00658.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#long_context",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Преодоление ограничений SSM: баланс между недавностью и сглаживанием",
                    "desc": "Структурированные модели пространства состояний (SSM) рассматриваются как альтернатива трансформерам в обработке длинных последовательностей. Исследование показало, что SSM имеют существенное ограничение в виде сильного смещения к недавним данным, что затрудняет запоминание отдаленной информации. Увеличение глубины SSM улучшает обработку длинных контекстов, но приводит к проблеме чрезмерного сглаживания. Авторы предлагают метод поляризации каналов матриц перехода состояний для решения этих проблем, что улучшает точность ассоциативного извлечения дальних токенов."
                },
                "en": {
                    "title": "Balancing Recency and Over-Smoothing in SSMs",
                    "desc": "This paper discusses Structured State Space Models (SSMs) as alternatives to transformers, highlighting their limitations due to strong recency bias. This bias affects the models' ability to remember distant information and creates robustness issues. The authors propose a solution by polarizing the state transition matrices, which helps mitigate both recency bias and over-smoothing that occurs with deeper architectures. Their experiments show that this new approach improves the accuracy of recalling long-range tokens, allowing SSMs to effectively utilize deeper structures."
                },
                "zh": {
                    "title": "解决近期偏见与过平滑的双重挑战",
                    "desc": "结构状态空间模型（SSMs）作为变换器的替代方案，虽然在捕捉长序列依赖性方面表现出色，但存在强烈的近期偏见限制。我们的实证研究表明，这种偏见影响了模型对远程信息的回忆能力，并引入了鲁棒性问题。通过扩展实验，我们发现SSMs的深层结构可以促进长上下文的学习，但理论分析显示，随着深度增加，模型会出现过平滑的趋势，使得标记表示变得难以区分。我们提出的极化技术通过将状态转移矩阵的两个通道设置为零和一，解决了近期偏见和过平滑的问题，显著提高了长距离标记的关联回忆准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01245",
            "title": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization",
            "url": "https://huggingface.co/papers/2501.01245",
            "abstract": "Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., \"salto backward tucked with 1 turn\"). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model's predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics.",
            "score": 5,
            "issue_id": 1475,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "30d94590a5c78569",
            "authors": [
                "Yongle Huang",
                "Haodong Chen",
                "Zhenbang Xu",
                "Zihan Jia",
                "Haozhou Sun",
                "Dian Shao"
            ],
            "affiliations": [
                "School of Automation, Northwestern Polytechnical University, Xian, China",
                "School of Computer Science, Northwestern Polytechnical University, Xian, China",
                "School of Software, Northwestern Polytechnical University, Xian, China",
                "Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01245.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#transfer_learning",
                    "#multimodal",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🤸",
                "ru": {
                    "title": "SeFAR: Прорыв в распознавании детализированных действий с помощью полу-контролируемого обучения",
                    "desc": "Статья представляет новый подход к задаче распознавания детализированных действий (Fine-grained Action Recognition, FAR) с использованием полу-контролируемого обучения. Авторы предлагают фреймворк SeFAR, который включает в себя двухуровневые временные элементы для более эффективного представления действий и новую стратегию аугментации данных. SeFAR также использует адаптивную регуляцию для стабилизации процесса обучения при работе с неопределенностью в предсказаниях модели-учителя. Эксперименты показывают, что SeFAR достигает лучших результатов на нескольких наборах данных FAR и классических наборах данных для распознавания действий."
                },
                "en": {
                    "title": "SeFAR: Elevating Fine-grained Action Recognition with Semi-supervised Learning",
                    "desc": "This paper focuses on improving Fine-grained Action Recognition (FAR), which identifies specific actions in short time frames. The authors introduce a semi-supervised learning framework called SeFAR, which uses innovative techniques to enhance the learning process despite the challenges of limited labeled data. They develop Dual-level temporal elements for better visual representation and implement a strong augmentation strategy within a Teacher-Student learning setup. The results demonstrate that SeFAR achieves top performance on FAR datasets and enhances multimodal models' understanding of detailed actions."
                },
                "zh": {
                    "title": "细粒度动作识别的新突破",
                    "desc": "人类动作理解对多模态系统的发展至关重要。本文提出了一种新的框架SeFAR，专注于细粒度动作识别（FAR），旨在处理短时间内的详细语义标签。我们采用半监督学习（SSL）来减少对大量标注数据的需求，并通过构建双层时间元素和新的强增强策略来提高模型的表现。实验结果表明，SeFAR在多个数据集上达到了最先进的性能，证明了我们设计的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00712",
            "title": "Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding",
            "url": "https://huggingface.co/papers/2501.00712",
            "abstract": "Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods enforce rigid patterns in attention maps, limiting the ability to model long-range dependencies and adapt to diverse tasks. Additionally, most positional encodings are learned as general biases, lacking the specialization required for different instances within a dataset. To address this, we propose conTextualized equivariAnt Position Embedding (TAPE), a novel framework that enhances positional embeddings by incorporating sequence content across layers. TAPE introduces dynamic, context-aware positional encodings, overcoming the constraints of traditional fixed patterns. By enforcing permutation and orthogonal equivariance, TAPE ensures the stability of positional encodings during updates, improving robustness and adaptability. Our method can be easily integrated into pre-trained transformers, offering parameter-efficient fine-tuning with minimal overhead. Extensive experiments shows that TAPE achieves superior performance in language modeling, arithmetic reasoning, and long-context retrieval tasks compared to existing positional embedding techniques.",
            "score": 4,
            "issue_id": 1485,
            "pub_date": "2025-01-01",
            "pub_date_card": {
                "ru": "1 января",
                "en": "January 1",
                "zh": "1月1日"
            },
            "hash": "e5119d0e83ce2af2",
            "authors": [
                "Jiajun Zhu",
                "Peihao Wang",
                "Ruisi Cai",
                "Jason D. Lee",
                "Pan Li",
                "Zhangyang Wang"
            ],
            "affiliations": [
                "Georgia Tech",
                "Princeton University",
                "University of Texas at Austin",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00712.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Динамические позиционные эмбеддинги для улучшения работы трансформеров",
                    "desc": "Авторы предлагают новый метод позиционного кодирования для трансформеров под названием TAPE. Этот подход учитывает контекст последовательности и создает динамические позиционные эмбеддинги, адаптированные к конкретным задачам. TAPE обеспечивает стабильность кодирования благодаря свойствам перестановочной и ортогональной эквивариантности. Метод легко интегрируется в предобученные модели и показывает превосходные результаты в задачах языкового моделирования, арифметических рассуждений и поиска в длинных контекстах."
                },
                "en": {
                    "title": "Enhancing Transformers with Context-Aware Positional Embeddings",
                    "desc": "This paper introduces a new method called conTextualized equivariAnt Position Embedding (TAPE) to improve how transformers use positional information. Traditional positional encodings often restrict the model's ability to understand long-range relationships in data. TAPE enhances these encodings by making them dynamic and context-aware, allowing them to adapt to different sequences and tasks. The method shows better performance in various applications, such as language modeling and reasoning, while being easy to integrate into existing transformer models."
                },
                "zh": {
                    "title": "提升变换器模型的位置信息处理能力",
                    "desc": "本文提出了一种新的位置编码方法，称为TAPE（conTextualized equivariAnt Position Embedding），旨在提高变换器模型的预测能力。传统的位置编码方法往往限制了模型对长距离依赖关系的建模能力，而TAPE通过引入动态的、上下文感知的位置编码来克服这一问题。该方法确保了位置编码在更新过程中的稳定性，从而提高了模型的鲁棒性和适应性。实验结果表明，TAPE在语言建模、算术推理和长上下文检索任务中表现优于现有的位置编码技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01407",
            "title": "Nested Attention: Semantic-aware Attention Values for Concept Personalization",
            "url": "https://huggingface.co/papers/2501.01407",
            "abstract": "Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model's prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model's existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.",
            "score": 3,
            "issue_id": 1487,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "537e7bcc16fb17f5",
            "authors": [
                "Or Patashnik",
                "Rinon Gal",
                "Daniil Ostashev",
                "Sergey Tulyakov",
                "Kfir Aberman",
                "Daniel Cohen-Or"
            ],
            "affiliations": [
                "Snap Research",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01407.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Nested Attention: новый подход к персонализации генерации изображений",
                    "desc": "Статья представляет новый метод под названием 'Nested Attention' для персонализации моделей text-to-image. Этот механизм внедряет богатое и выразительное представление изображения в существующие слои кросс-внимания модели. Ключевая идея заключается в генерации зависимых от запроса значений субъекта, полученных из вложенных слоев внимания. Метод позволяет достичь высокого сохранения идентичности при соблюдении входных текстовых подсказок."
                },
                "en": {
                    "title": "Nested Attention: Balancing Identity and Text Alignment in Image Generation",
                    "desc": "This paper presents a new method called Nested Attention for personalizing text-to-image models. The method addresses the challenge of balancing identity preservation of subjects with the alignment to text prompts. By using query-dependent subject values from nested attention layers, the model can effectively select relevant features for each part of the generated image. This approach not only maintains high identity fidelity but also allows for the integration of multiple personalized subjects from different domains into a single image."
                },
                "zh": {
                    "title": "嵌套注意力：个性化图像生成的新方法",
                    "desc": "本文介绍了一种新的机制，称为嵌套注意力，用于个性化文本到图像模型。该方法通过在模型的交叉注意力层中注入丰富的图像表示，解决了身份保留与文本提示对齐之间的平衡问题。嵌套注意力层能够为生成图像的每个区域选择相关的主题特征，从而实现高效的个性化。我们的研究表明，这种方法可以在多个领域进行训练，并允许在单个图像中结合来自不同领域的多个个性化主题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00910",
            "title": "Population Aware Diffusion for Time Series Generation",
            "url": "https://huggingface.co/papers/2501.00910",
            "abstract": "Diffusion models have shown promising ability in generating high-quality time series (TS) data. Despite the initial success, existing works mostly focus on the authenticity of data at the individual level, but pay less attention to preserving the population-level properties on the entire dataset. Such population-level properties include value distributions for each dimension and distributions of certain functional dependencies (e.g., cross-correlation, CC) between different dimensions. For instance, when generating house energy consumption TS data, the value distributions of the outside temperature and the kitchen temperature should be preserved, as well as the distribution of CC between them. Preserving such TS population-level properties is critical in maintaining the statistical insights of the datasets, mitigating model bias, and augmenting downstream tasks like TS prediction. Yet, it is often overlooked by existing models. Hence, data generated by existing models often bear distribution shifts from the original data. We propose Population-aware Diffusion for Time Series (PaD-TS), a new TS generation model that better preserves the population-level properties. The key novelties of PaD-TS include 1) a new training method explicitly incorporating TS population-level property preservation, and 2) a new dual-channel encoder model architecture that better captures the TS data structure. Empirical results in major benchmark datasets show that PaD-TS can improve the average CC distribution shift score between real and synthetic data by 5.9x while maintaining a performance comparable to state-of-the-art models on individual-level authenticity.",
            "score": 3,
            "issue_id": 1486,
            "pub_date": "2025-01-01",
            "pub_date_card": {
                "ru": "1 января",
                "en": "January 1",
                "zh": "1月1日"
            },
            "hash": "cd3f9282d55e15f2",
            "authors": [
                "Yang Li",
                "Han Meng",
                "Zhenyu Bi",
                "Ingolv T. Urnes",
                "Haipeng Chen"
            ],
            "affiliations": [
                "Generated Health",
                "Virginia Tech",
                "William & Mary"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00910.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#benchmark",
                    "#dataset",
                    "#data",
                    "#training",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Генерация временных рядов с сохранением свойств популяции",
                    "desc": "Статья представляет новую модель генерации временных рядов под названием PaD-TS (Population-aware Diffusion for Time Series). Модель нацелена на сохранение свойств на уровне популяции, таких как распределения значений и функциональные зависимости между измерениями. PaD-TS использует новый метод обучения, явно включающий сохранение свойств временных рядов на уровне популяции, и новую архитектуру модели с двухканальным энкодером. Эмпирические результаты показывают значительное улучшение в сохранении распределения кросс-корреляций при сравнимой аутентичности на индивидуальном уровне."
                },
                "en": {
                    "title": "Preserving Population Insights in Time Series Generation",
                    "desc": "This paper introduces a new model called Population-aware Diffusion for Time Series (PaD-TS) that focuses on generating time series data while preserving important population-level properties. Unlike previous models that mainly ensure individual data authenticity, PaD-TS emphasizes maintaining the overall statistical characteristics of the dataset, such as value distributions and cross-correlations between different dimensions. The model employs a novel training method and a dual-channel encoder architecture to effectively capture the structure of time series data. Experimental results demonstrate that PaD-TS significantly reduces distribution shifts in generated data while achieving comparable performance in individual-level authenticity to existing state-of-the-art models."
                },
                "zh": {
                    "title": "保留人口级特性，提升时间序列生成质量",
                    "desc": "扩散模型在生成高质量时间序列数据方面表现出色。然而，现有研究主要关注个体数据的真实性，而忽视了整个数据集的人口级特性。我们提出了一种新的时间序列生成模型PaD-TS，旨在更好地保留这些人口级特性，包括值分布和不同维度之间的交叉相关性。实验结果表明，PaD-TS在保持个体级真实性的同时，显著改善了真实数据与合成数据之间的分布差异。"
                }
            }
        }
    ],
    "link_prev": "2025-01-02.html",
    "link_next": "2025-01-06.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "02.01",
        "en": "01/02",
        "zh": "1月2日"
    },
    "short_date_next": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1月6日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 10,
        "#agents": 1,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 6,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 10,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 5,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的多模态教材数据集，用于视觉-语言模型（VLMs）的预训练。与现有的图像-文本数据集相比，这个数据集从网络上的教学视频中提取信息，包含更丰富的基础知识和更好的图像-文本对齐。数据集收集了超过2.5年的教学视频，总计22,000课时。实验表明，使用这个数据集预训练的VLMs在知识和推理密集型任务中表现出色，并展示了优异的上下文意识能力。代码可在GitHub上找到。",
        "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
        "pinyin": "这篇文章介绍了一种新的多模态教材数据集，用于视觉-语言模型（VLMs）的预训练。与现有的图像-文本数据集相比，这个数据集从网络上的教学视频中提取信息，包含更丰富的基础知识和更好的图像-文本对齐。数据集收集了超过2.5年的教学视频，总计22,000课时。实验表明，使用这个数据集预训练的VLMs在知识和推理密集型任务中表现出色，并展示了优异的上下文意识能力。代码可在GitHub上找到。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de duō mó tài jiào cái shù jù, yòng yú shì jué - yǔ yán mó xíng (VLMs) de yù xùn liàn. yǔ xiàn yǒu de tú xiàng - wén běn shù jù xiāng bǐ, zhè gè shù jù cóng wǎng luò shàng de jiào xué shì pín zhōng tí qǔ xìn xī, bāo hán gèng fēng fù de jī chǔ zhī shi hé gèng hǎo de tú xiàng - wén běn duì qí. shù jù shōu jí le chāo guò 2.5 nián de jiào xué shì pín, zǒng jì 22,000 kè shí. shí yàn biǎo míng, shǐ yòng zhè gè shù jù yù xùn liàn de VLMs zài zhī shi hé tuī lǐ mì jī xíng rèn wù zhōng biǎo xiàn chū sè, bìng zhàn shì le yōu yì de shàng xià wén yì shí néng lì. dài mǎ kě zài GitHub shàng zhǎo dào.",
        "vocab": "[{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'},\n{'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-training'},\n{'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language models'},\n{'word': '提取', 'pinyin': 'tí qū', 'trans': 'extract'},\n{'word': '对齐', 'pinyin': 'duì qí', 'trans': 'alignment'},\n{'word': '课时', 'pinyin': 'kè shí', 'trans': 'class hours'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'},\n{'word': '意识', 'pinyin': 'yì shí', 'trans': 'awareness'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}]",
        "trans": "This article introduces a new multimodal educational dataset for the pre-training of vision-language models (VLMs). Unlike existing image-text datasets, this dataset extracts information from educational videos on the web, containing richer foundational knowledge and better image-text alignment. The dataset has collected over 2.5 years of educational videos, totaling 22,000 hours. Experiments show that VLMs pre-trained with this dataset perform excellently in knowledge and reasoning-intensive tasks and demonstrate superior context awareness. The code can be found on GitHub.",
        "update_ts": "2025-01-04 12:39"
    }
}