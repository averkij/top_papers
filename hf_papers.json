{
    "date": {
        "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 23",
        "zh": "10æœˆ23æ—¥"
    },
    "time_utc": "2025-10-23 02:22",
    "weekday": 3,
    "issue_id": 6567,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.19363",
            "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
            "url": "https://huggingface.co/papers/2510.19363",
            "abstract": "LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning over long contexts is essential for large language models. While reinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\" moments in chain-of-thought, the advanced thinking patterns required for long-context reasoning remain largely unexplored, and high-difficulty RL data are scarce. In this paper, we introduce LoongRL, a data-driven RL method for advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents. Solving these tasks requires the model to trace the correct chain step-by-step, identify the true question, retrieve relevant facts and reason over them to answer correctly. RL training on KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning pattern that generalizes far beyond training length. Models trained at 16K effectively solve 128K tasks without prohibitive full-length RL rollout costs. On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5) and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all 128K needle-in-a-haystack stress tests, and preserves short-context reasoning capabilities.",
            "score": 12,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "c9f40dde34631067",
            "authors": [
                "Siyuan Wang",
                "Gaokai Zhang",
                "Li Lyna Zhang",
                "Ning Shang",
                "Fan Yang",
                "Dongyao Chen",
                "Mao Yang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Microsoft Research Asia",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19363.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ UUID",
                    "desc": "LoongRL â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ multi-hop Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ UUID ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Â«Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞ¹-Ğ¸Ñ‰Ğ¸-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ğ¹-Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞ¹Â», ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ´Ğ°Ğ»ĞµĞºĞ¾ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2.5 Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ +23.5% Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ 14B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ frontier Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ o3-mini Ğ¸ DeepSeek-R1."
                },
                "en": {
                    "title": "Unlocking Long-Context Reasoning with LoongRL",
                    "desc": "LoongRL is a novel reinforcement learning method designed to enhance long-context reasoning in large language models. It transforms short multi-hop question-answering tasks into more complex challenges by using UUID chains to obscure the actual question among distracting information. This approach encourages models to develop a systematic reasoning pattern that involves planning, retrieving, and verifying information, which significantly improves their performance on long-context tasks. The results show that models trained with LoongRL achieve substantial accuracy gains and can handle a much larger set of tasks than previously possible, while still maintaining their short-context reasoning abilities."
                },
                "zh": {
                    "title": "LoongRLï¼šæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "LoongRLæ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºé•¿ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡å°†çŸ­å¤šè·³é—®ç­”è½¬åŒ–ä¸ºé«˜éš¾åº¦ä»»åŠ¡ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚LoongRLçš„æ ¸å¿ƒæ˜¯KeyChainæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ’å…¥UUIDé“¾æ¥éšè—çœŸå®é—®é¢˜ï¼Œä»è€Œåœ¨å¤§é‡å¹²æ‰°æ–‡æ¡£ä¸­ç”Ÿæˆé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ã€‚ç»è¿‡KeyChainæ•°æ®çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿå½¢æˆæœ‰æ•ˆçš„æ¨ç†æ¨¡å¼ï¼Œæ˜¾è‘—æé«˜äº†å¤šè·³é—®ç­”çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19336",
            "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents",
            "url": "https://huggingface.co/papers/2510.19336",
            "abstract": "DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability across diverse scenarios. While Multimodal Large Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness in handling multiple mobile phone tasks simultaneously remains limited. Although multitask supervised fine-tuning (SFT) is widely adopted for multitask learning, existing approaches struggle to determine optimal training data compositions for peak performance. To address this challenge, we propose DaMo (Data Mixture Optimizer) - a novel solution employing a trainable network that predicts optimal data mixtures by forecasting downstream task performance for any given dataset ratio. To support comprehensive evaluation, we introduce PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse real-world industrial mobile application scenarios. Demonstrating strong predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo efficiently extrapolates optimal data mixing configurations. Our results show DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to alternative methods. Furthermore, extensive experiments across established benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench reveal DaMo's superior generalization, outperforming other approaches by 2.57% in terms of average score. When used solely for MLLM optimization on the BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably, DaMo maintains robust scalability, preserving its effectiveness when applied to other model architectures. The code and dataset are available at https://github.com/OPPO-Mente-Lab/DaMo.git",
            "score": 4,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "4d330d1d597c2031",
            "authors": [
                "Kai Shi",
                "Jun Yang",
                "Ni Yang",
                "Binqiang Pan",
                "Qingsong Xie",
                "Chao Zhang",
                "Zhenyu Yang",
                "Tianhuang Su",
                "Haonan Lu"
            ],
            "affiliations": [
                "OPPO AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19336.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "DaMo: ÑƒĞ¼Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ°Ñ…",
                    "desc": "DaMo - ÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ÑÑ‚Ğ°Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ multitask Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ RÂ²=0.81 Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ PhoneAgentBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ 1235 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. DaMo Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 3.38% Ğ½Ğ° PhoneAgentBench Ğ¸ Ğ½Ğ° 2.57% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Optimizing Data Mixtures for Enhanced Mobile Task Performance",
                    "desc": "DaMo is a novel trainable network designed to optimize data mixtures for Multimodal Large Language Models (MLLMs), specifically enhancing their performance on mobile phone tasks. It addresses the challenge of determining the best training data compositions for multitask learning, which traditional methods struggle with. By predicting optimal data mixtures based on expected task performance, DaMo demonstrates significant improvements in various benchmarks, including a 3.38% increase on PhoneAgentBench. Additionally, it shows strong generalization capabilities across established benchmarks, outperforming existing methods and maintaining effectiveness across different model architectures."
                },
                "zh": {
                    "title": "DaMoï¼šä¼˜åŒ–å¤šæ¨¡æ€ä»»åŠ¡çš„æ•°æ®ç»„åˆ",
                    "desc": "DaMoæ˜¯ä¸€ç§å¯è®­ç»ƒçš„ç½‘ç»œï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„æ•°æ®ç»„åˆï¼Œä»è€Œæå‡åœ¨æ‰‹æœºä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œç¡®å®šæœ€ä½³çš„æ•°æ®æ··åˆæ¯”ä¾‹ï¼Œè§£å†³äº†å¤šä»»åŠ¡å­¦ä¹ ä¸­æ•°æ®ç»„åˆé€‰æ‹©çš„éš¾é¢˜ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†PhoneAgentBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è¯„ä¼°å¤šæ¨¡æ€æ‰‹æœºä»»åŠ¡çš„åŸºå‡†ï¼ŒåŒ…å«1235ä¸ªé—®ç­”å¯¹ï¼Œè¦†ç›–å¤šç§çœŸå®å·¥ä¸šåº”ç”¨åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDaMoåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨BFCL-v3ä»»åŠ¡ä¸Šæå‡äº†12.47%çš„æŒ‡æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19817",
            "title": "olmOCR 2: Unit Test Rewards for Document OCR",
            "url": "https://huggingface.co/papers/2510.19817",
            "abstract": "olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.  \t\t\t\t\tAI-generated summary \t\t\t\t We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where our rewards are a diverse set of binary unit tests. To scale unit test creation, we develop a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. We show that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, our English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. We release our model, data and code under permissive open licenses.",
            "score": 2,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "f414d75457c4f9ee",
            "authors": [
                "Jake Poznanski",
                "Luca Soldaini",
                "Kyle Lo"
            ],
            "affiliations": [
                "Allen Institute for AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19817.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "OCR Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° unit-Ñ‚ĞµÑÑ‚Ğ°Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ olmOCR 2 Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ vision language model Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ PDF-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR), Ğ³Ğ´Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ unit-Ñ‚ĞµÑÑ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ olmOCR-Bench, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ», Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… layout'Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ´ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ñ‹ Ğ¿Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing OCR with Reinforcement Learning!",
                    "desc": "olmOCR 2 is an advanced optical character recognition (OCR) system that excels in converting printed documents into structured text. It utilizes a vision language model (VLM) with 7 billion parameters, trained through reinforcement learning with verifiable rewards to ensure high accuracy. The model is particularly effective in handling complex tasks such as math formula conversion, table parsing, and multi-column layouts. By generating synthetic documents for training, olmOCR 2 achieves state-of-the-art results on the olmOCR-Bench benchmark, demonstrating significant improvements over its predecessors."
                },
                "zh": {
                    "title": "olmOCR 2ï¼šOCRä»»åŠ¡çš„æœ€ä¼˜è§£",
                    "desc": "olmOCR 2 æ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å…¬å¼è½¬æ¢ã€è¡¨æ ¼è§£æå’Œå¤šåˆ—å¸ƒå±€æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯å¥–åŠ±è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿äº†å…¶åœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ—¶çš„é«˜æ•ˆæ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç”Ÿæˆåˆæˆæ–‡æ¡£çš„ç®¡é“ï¼Œä»¥åˆ›å»ºå¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„å¸ƒå±€ï¼Œå¹¶é€šè¿‡äºŒå…ƒå•å…ƒæµ‹è¯•æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æœ€ç»ˆï¼ŒolmOCR 2 åœ¨æˆ‘ä»¬çš„è‹±è¯­OCRåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ•°å­¦å…¬å¼å’Œè¡¨æ ¼çš„å¤„ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19457",
            "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models",
            "url": "https://huggingface.co/papers/2510.19457",
            "abstract": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' ability to understand time-sensitive knowledge. To address this gap, we propose MINED, a comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios.",
            "score": 2,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "016313b1f4c1861b",
            "authors": [
                "Kailin Jiang",
                "Ning Jiang",
                "Yuchen Ren",
                "Yuchen Li",
                "Yifan Gao",
                "Jinhe Bi",
                "Yunpu Ma",
                "Qingqing Liu",
                "Xianhao Wang",
                "Yifan Jia",
                "Hongbo Jiang",
                "Yaocong Hu",
                "Bin Li",
                "Lei Liu",
                "Yuntao Du"
            ],
            "affiliations": [
                "Anhui Polytechnic University",
                "Beijing Institute of Technology",
                "Ludwig Maximilian University of Munich",
                "Northeast Forestry University",
                "Shandong University",
                "University of Science and Technology of China",
                "University of Sydney",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19457.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "â°",
                "ru": {
                    "title": "MINED: ÑƒÑ‡Ğ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MINED â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2104 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ğ¿Ğ¾ 6 Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ 11 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ² 6 Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ…: ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 15 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… LMM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Gemini-2.5-Pro Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° (63.07%), Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ open-source Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹ÌĞ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ knowledge editing Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹ÌÑ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² LMM Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Temporal Awareness in Large Multimodal Models with MINED",
                    "desc": "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding time-sensitive factual knowledge due to their static representations. The authors introduce MINED, a new benchmark designed to assess LMMs' temporal awareness across six dimensions and eleven tasks, including cognition and reasoning. The benchmark is built from Wikipedia and includes 2,104 samples of time-sensitive knowledge. The evaluation reveals that while some LMMs, like Gemini-2.5-Pro, perform well, many open-source models struggle with temporal understanding, particularly in areas like sports."
                },
                "zh": {
                    "title": "æå‡æ—¶é—´æ•æ„ŸçŸ¥è¯†ç†è§£çš„åŸºå‡†è¯„ä¼°",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰é€šè¿‡è·¨æ¨¡æ€é¢„è®­ç»ƒç¼–ç ä¸°å¯Œçš„äº‹å®çŸ¥è¯†ï¼Œä½†å®ƒä»¬çš„é™æ€è¡¨ç¤ºåœ¨ç†è§£æ—¶é—´æ•æ„Ÿçš„äº‹å®çŸ¥è¯†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ç”±äºè®¾è®¡é™æ€ï¼Œæ— æ³•å……åˆ†è¯„ä¼°LMMså¯¹æ—¶é—´æ•æ„ŸçŸ¥è¯†çš„ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MINEDï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œè¯„ä¼°æ—¶é—´æ„è¯†çš„å…­ä¸ªå…³é”®ç»´åº¦å’Œåä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚é€šè¿‡å¯¹15ä¸ªå¹¿æ³›ä½¿ç”¨çš„LMMsè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°Gemini-2.5-Proåœ¨æ—¶é—´æ•æ„ŸçŸ¥è¯†çš„ç†è§£ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€Œå¤§å¤šæ•°å¼€æºLMMsä»ç„¶ç¼ºä¹è¿™ç§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19386",
            "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
            "url": "https://huggingface.co/papers/2510.19386",
            "abstract": "ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the advancements in hardware, software, and large language model technologies, the interaction between humans and operating systems has evolved from the command-line interface to the rapidly emerging AI agent interactions. Building an operating system (OS) agent capable of executing user instructions and faithfully following user desires is becoming a reality. In this technical report, we present ColorAgent, an OS agent designed to engage in long-horizon, robust interactions with the environment while also enabling personalized and proactive user interaction. To enable long-horizon interactions with the environment, we enhance the model's capabilities through step-wise reinforcement learning and self-evolving training, while also developing a tailored multi-agent framework that ensures generality, consistency, and robustness. In terms of user interaction, we explore personalized user intent recognition and proactive engagement, positioning the OS agent not merely as an automation tool but as a warm, collaborative partner. We evaluate ColorAgent on the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2% and 50.7%, respectively, establishing a new state of the art. Nonetheless, we note that current benchmarks are insufficient for a comprehensive evaluation of OS agents and propose further exploring directions in future work, particularly in the areas of evaluation paradigms, agent collaboration, and security. Our code is available at https://github.com/MadeAgents/mobile-use.",
            "score": 2,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "671d46e29b93bd8f",
            "authors": [
                "Ning Li",
                "Qiqiang Lin",
                "Zheng Wu",
                "Xiaoyun Mo",
                "Weiming Zhang",
                "Yin Zhao",
                "Xiangmou Qu",
                "Jiamu Zhou",
                "Jun Wang",
                "Congmin Zheng",
                "Yuanyi Song",
                "Hongjiang Chen",
                "Heyuan Huang",
                "Jihong Wang",
                "Jiaxin Yin",
                "Jingwei Yu",
                "Junwei Liao",
                "Qiuying Peng",
                "Xingyu Lou",
                "Jun Wang",
                "Weiwen Liu",
                "Zhuosheng Zhang",
                "Weinan Zhang"
            ],
            "affiliations": [
                "OPPO Research Institute",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19386.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#games",
                    "#benchmark",
                    "#security",
                    "#agents",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ColorAgent: ÑƒĞ¼Ğ½Ñ‹Ğ¹ OS-Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ",
                    "desc": "ColorAgent â€” ÑÑ‚Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Android, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ reinforcement learning Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ multi-agent Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AndroidWorld Ğ¸ AndroidLab Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² 77.2% Ğ¸ 50.7% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ ColorAgent Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° ĞºĞ°Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ½Ğ¸Ğ¼."
                },
                "en": {
                    "title": "ColorAgent: Your Personalized OS Partner for Intelligent Interaction",
                    "desc": "ColorAgent is an operating system agent that utilizes step-wise reinforcement learning within a multi-agent framework to enhance user interactions over long periods. It focuses on personalized user engagement, allowing the agent to understand and anticipate user needs, making it more than just an automation tool. The agent has been tested on Android benchmarks, achieving impressive success rates of 77.2% and 50.7%, setting a new standard in the field. The paper also highlights the need for improved evaluation methods for OS agents and suggests future research directions in collaboration and security."
                },
                "zh": {
                    "title": "ColorAgentï¼šæ™ºèƒ½æ“ä½œç³»ç»Ÿçš„ä¸ªæ€§åŒ–äº¤äº’æ–°çºªå…ƒ",
                    "desc": "ColorAgentæ˜¯ä¸€ç§æ“ä½œç³»ç»Ÿä»£ç†ï¼Œé‡‡ç”¨é€æ­¥å¼ºåŒ–å­¦ä¹ å’Œå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨é•¿æ—¶é—´äº¤äº’ä¸­å®ç°é«˜æˆåŠŸç‡ï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–çš„ç”¨æˆ·å‚ä¸ä½“éªŒã€‚è¯¥ä»£ç†é€šè¿‡å¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼Œæ”¯æŒä¸ç¯å¢ƒçš„é•¿æ—¶é—´äº¤äº’ï¼Œå¹¶è¿›è¡Œè‡ªæˆ‘è¿›åŒ–è®­ç»ƒã€‚ColorAgentä¸ä»…æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å·¥å…·ï¼Œæ›´æ˜¯ä¸€ä¸ªæ¸©æš–çš„åˆä½œä¼™ä¼´ï¼Œèƒ½å¤Ÿè¯†åˆ«ç”¨æˆ·æ„å›¾å¹¶ä¸»åŠ¨å‚ä¸ã€‚æˆ‘ä»¬åœ¨AndroidWorldå’ŒAndroidLabåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†ColorAgentï¼Œåˆ†åˆ«å–å¾—äº†77.2%å’Œ50.7%çš„æˆåŠŸç‡ï¼Œåˆ›é€ äº†æ–°çš„æŠ€æœ¯æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19338",
            "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
            "url": "https://huggingface.co/papers/2510.19338",
            "abstract": "The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.",
            "score": 2,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "bf307f4447578ac2",
            "authors": [
                "Ling Team",
                "Bin Han",
                "Caizhi Tang",
                "Chen Liang",
                "Donghao Zhang",
                "Fan Yuan",
                "Feng Zhu",
                "Jie Gao",
                "Jingyu Hu",
                "Longfei Li",
                "Meng Li",
                "Mingyang Zhang",
                "Peijie Jiang",
                "Peng Jiao",
                "Qian Zhao",
                "Qingyuan Yang",
                "Wenbo Shen",
                "Xinxing Yang",
                "Yalin Zhang",
                "Yankun Ren",
                "Yao Zhao",
                "Yibo Cao",
                "Yixuan Sun",
                "Yue Zhang",
                "Yuchen Fang",
                "Zibin Lin",
                "Zixuan Cheng",
                "Jun Zhou"
            ],
            "affiliations": [
                "Ling Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19338.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#long_context",
                    "#benchmark",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ’",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…",
                    "desc": "Ğ¡ĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ring-linear Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¸ softmax Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ring-mini-linear-2.0 (16B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¸ Ring-flash-linear-2.0 (104B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ inference Ğ² 10 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ dense Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ FP8 Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ¾ÑĞ»Ğ° Ğ½Ğ° 50%. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ SOTA Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… reasoning Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ„Ğ°Ğ·Ğµ reinforcement learning."
                },
                "en": {
                    "title": "Efficient Inference with Hybrid Attention Models",
                    "desc": "The Ring-linear model series introduces two advanced models, Ring-mini-linear-2.0 and Ring-flash-linear-2.0, which utilize a hybrid architecture that combines linear and softmax attention mechanisms. This innovative approach significantly reduces inference costs and enhances training efficiency, making it suitable for long-context scenarios. With 16B and 104B parameters respectively, these models achieve a remarkable reduction in computational overhead, cutting inference costs to one-tenth of a comparable dense model. Additionally, the integration of a high-performance FP8 operator library has led to a 50% improvement in training efficiency, ensuring that the models maintain state-of-the-art performance across various complex reasoning tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ä¸è®­ç»ƒçš„æ··åˆæ¶æ„",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†Ring-linearæ¨¡å‹ç³»åˆ—ï¼ŒåŒ…æ‹¬Ring-mini-linear-2.0å’ŒRing-flash-linear-2.0ã€‚è¿™äº›æ¨¡å‹é‡‡ç”¨æ··åˆæ¶æ„ï¼Œç»“åˆäº†çº¿æ€§æ³¨æ„åŠ›å’Œsoftmaxæ³¨æ„åŠ›ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚Ring-mini-linear-2.0æ‹¥æœ‰160äº¿å‚æ•°ï¼Œè€ŒRing-flash-linear-2.0åˆ™æœ‰1040äº¿å‚æ•°ï¼ŒäºŒè€…åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡ä¼˜åŒ–ä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„æ¯”ä¾‹ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†å½“å‰æœ€ä½³çš„æ¨¡å‹ç»“æ„ï¼Œå¹¶åˆ©ç”¨è‡ªç ”çš„é«˜æ€§èƒ½FP8è¿ç®—åº“æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19316",
            "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints",
            "url": "https://huggingface.co/papers/2510.19316",
            "abstract": "KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting. To address this, we propose KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. Unlike general text or image data augmentation, KORE automatically converts individual knowledge items into structured and comprehensive knowledge to ensure that the model accurately learns new knowledge, enabling accurate adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix of LMM's linear layer activations and initializes the adapter by projecting the original weights into the matrix's null space, defining a fine-tuning direction that minimizes interference with previous knowledge, enabling powerful retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B, LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting.",
            "score": 2,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "97796b9efcce99e8",
            "authors": [
                "Kailin Jiang",
                "Hongbo Jiang",
                "Ning Jiang",
                "Zhi Gao",
                "Jinhe Bi",
                "Yuchen Ren",
                "Bin Li",
                "Yuntao Du",
                "Lei Liu",
                "Qing Li"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Ludwig Maximilian University of Munich",
                "Northeast Forestry University",
                "Shandong University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "University of Science and Technology of China",
                "University of Sydney",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19316.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "KORE: Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ñ€Ğ¾Ğ³Ğ¾",
                    "desc": "KORE â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ½ĞµĞµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ KORE ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² LLM Ğ¸ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLaVA Ğ¸ Qwen2.5-VL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ€Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "KORE: Retain the Old, Embrace the New in Multimodal Learning!",
                    "desc": "KORE is a novel method designed to enhance large multimodal models by injecting new knowledge while ensuring that previously learned information is retained. It utilizes structured augmentations and covariance matrix constraints to minimize the risk of catastrophic forgetting, which is when a model loses old knowledge while learning new information. By converting knowledge items into structured formats, KORE allows models to accurately learn and adapt to new data. Experimental results demonstrate that KORE significantly improves the performance of knowledge injection in various multimodal models without compromising their existing knowledge."
                },
                "zh": {
                    "title": "KOREï¼šçŸ¥è¯†æ³¨å…¥ä¸ä¿ç•™çš„å®Œç¾å¹³è¡¡",
                    "desc": "KOREæ˜¯ä¸€ç§å‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ³¨å…¥æ–°çŸ¥è¯†çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™æ—§çŸ¥è¯†ã€‚å®ƒé€šè¿‡ç»“æ„åŒ–å¢å¼ºå’Œåæ–¹å·®çŸ©é˜µçº¦æŸæ¥æœ€å°åŒ–ç¾éš¾æ€§é—å¿˜ã€‚KOREèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†æ–°çŸ¥è¯†é€‚åº”åˆ°æ¨¡å‹ä¸­ï¼ŒåŒæ—¶ç¡®ä¿æ—§çŸ¥è¯†çš„ä¿ç•™ã€‚å®éªŒè¡¨æ˜ï¼ŒKOREåœ¨å¤šç§å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æ–°çŸ¥è¯†æ³¨å…¥çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18313",
            "title": "OmniNWM: Omniscient Driving Navigation World Models",
            "url": "https://huggingface.co/papers/2510.18313",
            "abstract": "OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://github.com/Arlo0o/OmniNWM.",
            "score": 2,
            "issue_id": 6567,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "234f9481e9fb3cf2",
            "authors": [
                "Bohan Li",
                "Zhuang Ma",
                "Dalong Du",
                "Baorui Peng",
                "Zhujin Liang",
                "Zhenqiang Liu",
                "Chao Ma",
                "Yueming Jin",
                "Hao Zhao",
                "Wenjun Zeng",
                "Xin Jin"
            ],
            "affiliations": [
                "Eastern Institute of Technology, Ningbo",
                "National University of Singapore",
                "PhiGent",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18313.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#games",
                    "#agents",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ’ÑĞµĞ²Ğ¸Ğ´ÑÑ‰Ğ°Ñ world model Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3D-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸",
                    "desc": "OmniNWM â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ world model Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ RGB, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾Ğ¹, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ¸ 3D occupancy. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Plucker ray-maps Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ reward Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ 3D occupancy Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "OmniNWM: The Future of Autonomous Driving World Models",
                    "desc": "OmniNWM is a comprehensive world model designed for autonomous driving that excels in generating panoramic videos and encoding actions with Plucker ray-maps. It effectively integrates state, action, and reward dimensions into a single framework, overcoming limitations of previous models. The model generates high-quality panoramic videos that include RGB, semantics, metric depth, and 3D occupancy, while also providing precise control through pixel-level action encoding. By utilizing 3D occupancy for defining dense rewards, OmniNWM ensures compliance and safety in driving, achieving top performance in video generation and control stability."
                },
                "zh": {
                    "title": "OmniNWMï¼šè‡ªåŠ¨é©¾é©¶çš„å…¨æ™¯å¯¼èˆªæ–°æ¨¡å‹",
                    "desc": "OmniNWMæ˜¯ä¸€ç§ç»Ÿä¸€çš„ä¸–ç•Œæ¨¡å‹ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡ï¼Œèƒ½å¤Ÿç”Ÿæˆå…¨æ™¯è§†é¢‘ï¼Œå¹¶ä½¿ç”¨Pluckerå…‰çº¿å›¾ç¼–ç åŠ¨ä½œï¼ŒåŒæ—¶åŸºäº3Då ç”¨å®šä¹‰å¯†é›†å¥–åŠ±ã€‚è¯¥æ¨¡å‹åœ¨çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚OmniNWMç”ŸæˆRGBã€è¯­ä¹‰ã€åº¦é‡æ·±åº¦å’Œ3Då ç”¨çš„å…¨æ™¯è§†é¢‘ï¼Œé‡‡ç”¨çµæ´»çš„å¼ºåˆ¶ç­–ç•¥å®ç°é«˜è´¨é‡çš„é•¿æ—¶é—´è‡ªå›å½’ç”Ÿæˆã€‚é€šè¿‡ç›´æ¥åˆ©ç”¨ç”Ÿæˆçš„3Då ç”¨å®šä¹‰åŸºäºè§„åˆ™çš„å¯†é›†å¥–åŠ±ï¼ŒOmniNWMåœ¨è§†é¢‘ç”Ÿæˆã€æ§åˆ¶ç²¾åº¦å’Œé•¿æœŸç¨³å®šæ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19808",
            "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
            "url": "https://huggingface.co/papers/2510.19808",
            "abstract": "Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models.",
            "score": 1,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "19f1485a76e3707f",
            "authors": [
                "Yusu Qian",
                "Eli Bocek-Rivele",
                "Liangchen Song",
                "Jialing Tong",
                "Yinfei Yang",
                "Jiasen Lu",
                "Wenze Hu",
                "Zhe Gan"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19808.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#synthetic",
                    "#alignment",
                    "#reasoning"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Pico-Banana-400K: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Pico-Banana-400K â€” Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 400 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ¾Ğº Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ¸Ğ· OpenImages Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Nano-Banana Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· MLLM. ĞŸĞ¾Ğ¼Ğ¸Ğ¼Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°: Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (72K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²), Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ reward-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (56K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµÑÑƒÑ€Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ."
                },
                "en": {
                    "title": "Unlocking Advanced Image Editing with Pico-Banana-400K",
                    "desc": "Pico-Banana-400K is a new dataset designed for instruction-based image editing, containing 400,000 high-quality images. It features diverse edit pairs and supports complex editing tasks through multi-turn editing and preference subsets. The dataset is built from real images, ensuring high quality and relevance, and includes specialized subsets for various research needs. This resource aims to enhance the development and evaluation of advanced text-guided image editing models."
                },
                "zh": {
                    "title": "Pico-Banana-400Kï¼šå›¾åƒç¼–è¾‘çš„æ–°åŸºçŸ³",
                    "desc": "Pico-Banana-400Kæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å›¾åƒç¼–è¾‘æ•°æ®é›†ï¼Œä¸“æ³¨äºåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ã€‚è¯¥æ•°æ®é›†åŒ…å«å¤šæ ·çš„ç¼–è¾‘å¯¹ã€å¤šè½®ç¼–è¾‘å’Œåå¥½å­é›†ï¼Œæ”¯æŒå¤æ‚çš„ç¼–è¾‘åœºæ™¯ç ”ç©¶ã€‚é€šè¿‡åˆ©ç”¨Nano-Bananaç”ŸæˆçœŸå®ç…§ç‰‡çš„ç¼–è¾‘å¯¹ï¼ŒPico-Banana-400Kç¡®ä¿äº†ç¼–è¾‘ç±»å‹çš„å…¨é¢è¦†ç›–å’Œå†…å®¹çš„ç²¾ç¡®ä¿ç•™ã€‚è¿™ä¸ªæ•°æ®é›†ä¸ºä¸‹ä¸€ä»£æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹çš„è®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•æä¾›äº†åšå®çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19488",
            "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
            "url": "https://huggingface.co/papers/2510.19488",
            "abstract": "VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation.",
            "score": 1,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "4237f40e97c8b930",
            "authors": [
                "Dunjie Lu",
                "Yiheng Xu",
                "Junli Wang",
                "Haoyuan Wu",
                "Xinyuan Wang",
                "Zekun Wang",
                "Junlin Yang",
                "Hongjin Su",
                "Jixuan Chen",
                "Junda Chen",
                "Yuchen Mao",
                "Jingren Zhou",
                "Junyang Lin",
                "Binyuan Hui",
                "Tao Yu"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Group",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19488.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#video",
                    "#data",
                    "#agents",
                    "#optimization",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° YouTube Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ VideoAgentTrek â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ GUI Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… YouTube-Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Video2Action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ ĞºĞ»Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ²Ğ²ĞµĞ´Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ² 39 Ñ‚Ñ‹ÑÑÑ‡ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° 1.52 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 70% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming YouTube Videos into Training Gold for AI Agents",
                    "desc": "VideoAgentTrek is a novel system that automatically extracts graphical user interface (GUI) interaction data from YouTube videos, addressing the challenge of obtaining large-scale training data for computer-use agents without manual annotation. It utilizes an inverse dynamics module called Video2Action, which includes a video grounding model to identify and time-stamp GUI actions, and an action-content recognizer to capture detailed parameters like click locations and text input. By applying this method to 39,000 tutorial videos, the system generated over 1.5 million interaction steps, significantly enhancing the training dataset. The results show a marked improvement in task success rates and step accuracy, demonstrating the effectiveness of using passive video content for training AI agents."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–æå–è§†é¢‘äº¤äº’æ•°æ®ï¼Œæå‡è®¡ç®—æœºä»£ç†æ€§èƒ½",
                    "desc": "VideoAgentTrek æ˜¯ä¸€ä¸ªè‡ªåŠ¨ä» YouTube è§†é¢‘ä¸­æå– GUI äº¤äº’æ•°æ®çš„ç³»ç»Ÿï¼Œä½¿ç”¨äº†é€†åŠ¨æ€æ¨¡å— Video2Actionï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„ä»»åŠ¡æˆåŠŸç‡å’Œæ­¥éª¤å‡†ç¡®æ€§ã€‚è¯¥ç³»ç»Ÿè§£å†³äº†æ‰‹åŠ¨æ ‡æ³¨å¤§é‡äº¤äº’æ•°æ®çš„é«˜æˆæœ¬é—®é¢˜ï¼Œé€šè¿‡ä»å…¬å¼€çš„å±å¹•å½•åˆ¶è§†é¢‘ä¸­è‡ªåŠ¨æŒ–æ˜è®­ç»ƒæ•°æ®ã€‚Video2Action åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šè§†é¢‘å®šä½æ¨¡å‹å’ŒåŠ¨ä½œå†…å®¹è¯†åˆ«å™¨ï¼Œèƒ½å¤Ÿç²¾ç¡®æ£€æµ‹å’Œæå– GUI æ“ä½œçš„æ—¶é—´è¾¹ç•Œå’Œç»“æ„åŒ–å‚æ•°ã€‚é€šè¿‡å¯¹ 39,000 ä¸ª YouTube æ•™ç¨‹è§†é¢‘çš„åº”ç”¨ï¼Œæˆ‘ä»¬çš„ç®¡é“è‡ªåŠ¨ç”Ÿæˆäº† 152 ä¸‡ä¸ªäº¤äº’æ­¥éª¤ï¼Œå±•ç¤ºäº†è¢«åŠ¨äº’è”ç½‘è§†é¢‘å¯ä»¥è½¬åŒ–ä¸ºé«˜è´¨é‡çš„è®¡ç®—æœºä½¿ç”¨ä»£ç†ç›‘ç£æ•°æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19286",
            "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
            "url": "https://huggingface.co/papers/2510.19286",
            "abstract": "TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Since the introduction of the Model Context Protocol (MCP), the number of available tools for Large Language Models (LLMs) has increased significantly. These task-specific tool sets offer an alternative to general-purpose tools such as web browsers, while being easier to develop and maintain than GUIs. However, current general-purpose agents predominantly rely on web browsers for interacting with the environment. Here, we introduce TheMCPCompany, a benchmark for evaluating tool-calling agents on tasks that involve interacting with various real-world services. We use the REST APIs of these services to create MCP servers, which include over 18,000 tools. We also provide manually annotated ground-truth tools for each task. In our experiments, we use the ground truth tools to show the potential of tool-calling agents for both improving performance and reducing costs assuming perfect tool retrieval. Next, we explore agent performance using tool retrieval to study the real-world practicality of tool-based agents. While all models with tool retrieval perform similarly or better than browser-based agents, smaller models cannot take full advantage of the available tools through retrieval. On the other hand, GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools. Overall, our work shows that the most advanced reasoning models are effective at discovering tools in simpler environments, but seriously struggle with navigating complex enterprise environments. TheMCPCompany reveals that navigating tens of thousands of tools and combining them in non-trivial ways to solve complex problems is still a challenging task for current models and requires both better reasoning and better retrieval models.",
            "score": 1,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "d7d7013ab05a9068",
            "authors": [
                "Reza Esfandiarpoor",
                "Vishwas Suryanarayanan",
                "Stephen H. Bach",
                "Vishal Chowdhary",
                "Anthony Aue"
            ],
            "affiliations": [
                "Brown University",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19286.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ¢Ñ‹ÑÑÑ‡Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² â€” Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ TheMCPCompany â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· REST API Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 18,000 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Model Context Protocol (MCP), Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ performance Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° GPT-5 Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº reasoning, Ñ‚Ğ°Ğº Ğ¸ retrieval Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Navigating Complexity: Evaluating Tool-Calling Agents in Real-World Environments",
                    "desc": "The paper introduces TheMCPCompany, a benchmark designed to evaluate tool-calling agents that interact with real-world services through REST APIs. It highlights the performance of advanced models in simpler environments, where they excel, but reveals their limitations in complex enterprise settings. The study demonstrates that while tool retrieval enhances agent performance, smaller models struggle to utilize the available tools effectively. Ultimately, the findings suggest that improving reasoning and retrieval capabilities is essential for navigating intricate tasks involving numerous tools."
                },
                "zh": {
                    "title": "è¯„ä¼°å·¥å…·è°ƒç”¨ä»£ç†çš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "TheMCPCompanyæ˜¯ä¸€ä¸ªè¯„ä¼°å·¥å…·è°ƒç”¨ä»£ç†çš„åŸºå‡†ï¼Œä½¿ç”¨REST APIä¸çœŸå®ä¸–ç•ŒæœåŠ¡è¿›è¡Œäº¤äº’ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å…ˆè¿›çš„æ¨¡å‹åœ¨ç®€å•ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚çš„ä¼ä¸šç¯å¢ƒä¸­å´é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡18,000ä¸ªå·¥å…·çš„MCPæœåŠ¡å™¨ï¼Œå¹¶æä¾›äº†æ¯ä¸ªä»»åŠ¡çš„æ‰‹åŠ¨æ ‡æ³¨çœŸå®å·¥å…·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ‰€æœ‰æ¨¡å‹åœ¨å·¥å…·æ£€ç´¢æ–¹é¢çš„è¡¨ç°ä¼˜äºåŸºäºæµè§ˆå™¨çš„ä»£ç†ï¼Œä½†è¾ƒå°çš„æ¨¡å‹æ— æ³•å……åˆ†åˆ©ç”¨å¯ç”¨å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16844",
            "title": "FinSight: Towards Real-World Financial Deep Research",
            "url": "https://huggingface.co/papers/2510.16844",
            "abstract": "FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating professional financial reports is a labor-intensive and intellectually demanding process that current AI systems struggle to fully automate. To address this challenge, we introduce FinSight (Financial InSight), a novel multi agent framework for producing high-quality, multimodal financial reports. The foundation of FinSight is the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, enabling flexible data collection, analysis and report generation through executable code. To ensure professional-grade visualization, we propose an Iterative Vision-Enhanced Mechanism that progressively refines raw visual outputs into polished financial charts. Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports, ensuring both analytical depth and structural consistency. Experiments on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.",
            "score": 1,
            "issue_id": 6567,
            "pub_date": "2025-10-19",
            "pub_date_card": {
                "ru": "19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 19",
                "zh": "10æœˆ19æ—¥"
            },
            "hash": "4316f8c0ef12ddf6",
            "authors": [
                "Jiajie Jin",
                "Yuyao Zhang",
                "Yimeng Xu",
                "Hongjin Qian",
                "Yutao Zhu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "BAAI",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16844.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#architecture"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "AI-Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ",
                    "desc": "FinSight â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ² Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ CAVM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´. Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ² ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ ÑĞ¾ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "FinSight: Revolutionizing Financial Reporting with AI",
                    "desc": "FinSight is a multi-agent framework designed to automate the generation of high-quality financial reports. It utilizes the Code Agent with Variable Memory (CAVM) architecture, which allows for flexible data integration and analysis through programmable code. The framework also includes an Iterative Vision-Enhanced Mechanism that improves visual outputs into professional-grade financial charts. Overall, FinSight outperforms existing AI systems in accuracy and presentation, making strides towards achieving human-expert quality in financial reporting."
                },
                "zh": {
                    "title": "FinSightï¼šæ™ºèƒ½ç”Ÿæˆé«˜è´¨é‡é‡‘èæŠ¥å‘Šçš„æœªæ¥",
                    "desc": "FinSightæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé‡‡ç”¨å¯å˜å†…å­˜çš„ä»£ç ä»£ç†æ¶æ„ï¼ˆCAVMï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€é‡‘èæŠ¥å‘Šã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¯æ‰§è¡Œä»£ç çµæ´»åœ°æ”¶é›†å’Œåˆ†ææ•°æ®ï¼Œç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§ã€‚ä¸ºäº†æå‡å¯è§†åŒ–æ•ˆæœï¼ŒFinSightå¼•å…¥äº†è¿­ä»£è§†è§‰å¢å¼ºæœºåˆ¶ï¼Œé€æ­¥ä¼˜åŒ–åŸå§‹è§†è§‰è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFinSightåœ¨å‡†ç¡®æ€§ã€åˆ†ææ·±åº¦å’Œå±•ç¤ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œæ¥è¿‘äººç±»ä¸“å®¶çš„æ°´å¹³ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-22.html",
    "link_next": "2025-10-24.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "22.10",
        "en": "10/22",
        "zh": "10æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.10",
        "en": "10/24",
        "zh": "10æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 5,
        "#cv": 1,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}