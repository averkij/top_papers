{
    "date": {
        "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 30",
        "zh": "4æœˆ30æ—¥"
    },
    "time_utc": "2025-04-30 22:10",
    "weekday": 2,
    "issue_id": 3522,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.20571",
            "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
            "url": "https://huggingface.co/papers/2504.20571",
            "abstract": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR",
            "score": 45,
            "issue_id": 3502,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "5392cdfe5ab1de59",
            "authors": [
                "Yiping Wang",
                "Qing Yang",
                "Zhiyuan Zeng",
                "Liliang Ren",
                "Lucas Liu",
                "Baolin Peng",
                "Hao Cheng",
                "Xuehai He",
                "Kuan Wang",
                "Jianfeng Gao",
                "Weizhu Chen",
                "Shuohang Wang",
                "Simon Shaolei Du",
                "Yelong Shen"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Microsoft",
                "University of California, Santa Cruz",
                "University of Southern California",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20571.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training",
                    "#math",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ - Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ ÑĞºĞ°Ñ‡Ğ¾Ğº Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-Math-1.5B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ĞµĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼ĞµĞ¶Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ÑĞ»Ğµ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ñ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ° 'Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ°'."
                },
                "en": {
                    "title": "Boosting Math Skills in LLMs with 1-Shot RLVR",
                    "desc": "This paper presents a method called 1-shot Reinforcement Learning with Verifiable Reward (RLVR) that significantly enhances the mathematical reasoning abilities of large language models (LLMs). By using just one training example, the authors demonstrate a remarkable increase in performance on the MATH500 benchmark, achieving a score of 73.6% compared to 36.0% before. The study also reveals that this approach leads to improvements across various models and algorithms, highlighting the importance of exploration in training. Additionally, the authors introduce the concept of post-saturation generalization, where performance continues to improve even after training accuracy levels off, suggesting new avenues for research in RLVR."
                },
                "zh": {
                    "title": "ä¸€ä¾‹å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ•°å­¦æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "æœ¬æ–‡å±•ç¤ºäº†ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆ1-shot RLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å°†RLVRåº”ç”¨äºåŸºç¡€æ¨¡å‹Qwen2.5-Math-1.5Bï¼Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªå•ä¸€ç¤ºä¾‹å¯ä»¥å°†æ¨¡å‹åœ¨MATH500ä¸Šçš„è¡¨ç°ä»36.0%æå‡è‡³73.6%ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è§‚å¯Ÿåˆ°åœ¨1-shot RLVRè¿‡ç¨‹ä¸­å‡ºç°äº†ä¸€äº›æœ‰è¶£ç°è±¡ï¼Œå¦‚è·¨é¢†åŸŸæ³›åŒ–å’Œè‡ªæˆ‘åæ€é¢‘ç‡å¢åŠ ã€‚æˆ‘ä»¬éªŒè¯äº†1-shot RLVRçš„æœ‰æ•ˆæ€§ä¸»è¦æºäºç­–ç•¥æ¢¯åº¦æŸå¤±ï¼Œå¹¶å¼ºè°ƒäº†ä¿ƒè¿›æ¢ç´¢åœ¨è®­ç»ƒä¸­çš„å…³é”®ä½œç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20734",
            "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
            "url": "https://huggingface.co/papers/2504.20734",
            "abstract": "Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.",
            "score": 42,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "53427aff6a4d7ed5",
            "authors": [
                "Woongyeong Yeo",
                "Kangsan Kim",
                "Soyeong Jeong",
                "Jinheon Baek",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20734.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "UniversalRAG: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²",
                    "desc": "UniversalRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² RAG, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ğ¼Ğ¸, UniversalRAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 8 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ UniversalRAG Ğ½Ğ°Ğ´ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "UniversalRAG: Bridging Knowledge Across Modalities for Enhanced Retrieval",
                    "desc": "The paper introduces UniversalRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by integrating knowledge from various sources and modalities. Unlike traditional RAG methods that rely on a single type of corpus, UniversalRAG addresses the diverse nature of real-world queries by employing a modality-aware routing mechanism. This mechanism allows the model to dynamically select the most relevant corpus based on the query's modality and granularity. The effectiveness of UniversalRAG is demonstrated through extensive testing on multiple benchmarks, outperforming existing models that focus on either single modalities or unified representations."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€çŸ¥è¯†æ•´åˆçš„å…¨æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºUniversalRAGï¼Œæ—¨åœ¨ä»å¤šç§å¼‚æ„çŸ¥è¯†æºä¸­æ£€ç´¢å’Œæ•´åˆä¿¡æ¯ï¼Œä»¥æé«˜æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§ã€‚ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•é€šå¸¸ä»…é™äºæ–‡æœ¬æ•°æ®ï¼Œè€ŒUniversalRAGèƒ½å¤Ÿå¤„ç†å¤šç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¦‚å›¾åƒå’Œè§†é¢‘ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ¨¡æ€æ„ŸçŸ¥çš„è·¯ç”±æœºåˆ¶ï¼Œèƒ½å¤ŸåŠ¨æ€è¯†åˆ«æœ€åˆé€‚çš„æ¨¡æ€ç‰¹å®šè¯­æ–™åº“ï¼Œä»è€Œè¿›è¡Œé’ˆå¯¹æ€§çš„æ£€ç´¢ã€‚æ­¤å¤–ï¼ŒUniversalRAGè¿˜å°†æ¯ç§æ¨¡æ€ç»„ç»‡ä¸ºå¤šä¸ªç²’åº¦çº§åˆ«ï¼Œä»¥ä¾¿æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§å’ŒèŒƒå›´è¿›è¡Œç²¾ç»†åŒ–æ£€ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20595",
            "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
            "url": "https://huggingface.co/papers/2504.20595",
            "abstract": "We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.",
            "score": 35,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "244cff2e64afeaa0",
            "authors": [
                "Rulin Shao",
                "Rui Qiao",
                "Varsha Kishore",
                "Niklas Muennighoff",
                "Xi Victoria Lin",
                "Daniela Rus",
                "Bryan Kian Hsiang Low",
                "Sewon Min",
                "Wen-tau Yih",
                "Pang Wei Koh",
                "Luke Zettlemoyer"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "FAIR at Meta",
                "Massachusetts Institute of Technology",
                "National University of Singapore",
                "Singapore-MIT Alliance for Research and Technology",
                "Stanford University",
                "University of California, Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20595.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#rag",
                    "#reasoning",
                    "#synthetic",
                    "#open_source",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ReasonIR-8B: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "ReasonIR-8B - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BRIGHT. ĞŸÑ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ RAG, ReasonIR-8B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° MMLU Ğ¸ GPQA Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ baseline Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with ReasonIR-8B",
                    "desc": "ReasonIR-8B is a novel information retrieval model designed specifically for reasoning tasks, addressing the limitations of existing retrievers that focus on simple factual queries. It utilizes a synthetic data generation pipeline to create complex queries and challenging hard negatives, enhancing the training process. By combining this synthetic data with existing datasets, ReasonIR-8B achieves impressive performance metrics on the BRIGHT benchmark, surpassing previous models. Additionally, it demonstrates improved efficiency during test-time by leveraging longer, more informative queries, making it a valuable tool for reasoning-intensive applications."
                },
                "zh": {
                    "title": "æ¨ç†ä»»åŠ¡çš„ä¸“å±æ£€ç´¢å™¨ï¼šReasonIR-8B",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ReasonIR-8Bï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºä¸€èˆ¬æ¨ç†ä»»åŠ¡è®­ç»ƒçš„æ£€ç´¢å™¨ã€‚ç°æœ‰çš„æ£€ç´¢å™¨åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰é™ï¼Œéƒ¨åˆ†åŸå› æ˜¯ç°æœ‰çš„è®­ç»ƒæ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨ä¸æ–‡æ¡£ç›´æ¥ç›¸å…³çš„çŸ­å°äº‹å®æŸ¥è¯¢ä¸Šã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºå…·æœ‰æŒ‘æˆ˜æ€§å’Œç›¸å…³æ€§çš„æŸ¥è¯¢ï¼Œä»¥åŠä¸€ä¸ªçœ‹ä¼¼ç›¸å…³ä½†å®é™…ä¸Šæ— ç”¨çš„å›°éš¾è´Ÿæ ·æœ¬ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®å’Œç°æœ‰å…¬å…±æ•°æ®çš„æ··åˆä¸Šè¿›è¡Œè®­ç»ƒï¼ŒReasonIR-8Båœ¨BRIGHTåŸºå‡†ä¸Šè¾¾åˆ°äº†29.9çš„nDCG@10ï¼ˆä¸ä½¿ç”¨é‡æ’åºå™¨ï¼‰å’Œ36.9çš„nDCG@10ï¼ˆä½¿ç”¨é‡æ’åºå™¨ï¼‰ï¼Œå¹¶åœ¨RAGä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†MMLUå’ŒGPQAçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20879",
            "title": "The Leaderboard Illusion",
            "url": "https://huggingface.co/papers/2504.20879",
            "abstract": "Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field",
            "score": 32,
            "issue_id": 3505,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "c1d3b5cc6840e6e6",
            "authors": [
                "Shivalika Singh",
                "Yiyang Nan",
                "Alex Wang",
                "Daniel D'Souza",
                "Sayash Kapoor",
                "Ahmet ÃœstÃ¼n",
                "Sanmi Koyejo",
                "Yuntian Deng",
                "Shayne Longpre",
                "Noah Smith",
                "Beyza Ermis",
                "Marzieh Fadaee",
                "Sara Hooker"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Cohere",
                "Cohere Labs",
                "Massachusetts Institute of Technology",
                "Princeton University",
                "Stanford University",
                "University of Washington",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20879.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ¾ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Chatbot Arena",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ Chatbot Arena. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·-Ğ·Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Towards Fairer AI Benchmarking: Reforming the Chatbot Arena",
                    "desc": "This paper discusses the challenges in measuring progress in AI through benchmarks, specifically focusing on the Chatbot Arena leaderboard. It highlights how undisclosed private testing practices create an uneven playing field, favoring certain providers who can selectively disclose their best scores. The authors reveal that proprietary models receive more testing opportunities and data access compared to open-weight models, leading to biased performance evaluations. They propose reforms to enhance transparency and fairness in the benchmarking process, ensuring that all AI systems are evaluated on a level playing field."
                },
                "zh": {
                    "title": "æ¨åŠ¨å…¬å¹³é€æ˜çš„AIåŸºå‡†æµ‹è¯•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸä¸­ï¼ŒåŸºå‡†æµ‹è¯•çš„é‡è¦æ€§åŠå…¶æ½œåœ¨çš„æ‰­æ›²é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼ŒæŸäº›æä¾›è€…é€šè¿‡ç§ä¸‹æµ‹è¯•å’Œé€‰æ‹©æ€§æŠ«éœ²æˆç»©ï¼Œå¯¼è‡´äº†Chatbot Arenaçš„è¯„åˆ†åå·®ã€‚ç‰¹åˆ«æ˜¯ï¼ŒMetaåœ¨Llama-4å‘å¸ƒå‰æµ‹è¯•äº†27ä¸ªç§æœ‰LLMå˜ä½“ï¼Œè¿™ä½¿å¾—å…¶è¯„åˆ†æ›´å…·ä¼˜åŠ¿ã€‚æˆ‘ä»¬å»ºè®®å¯¹Chatbot Arenaçš„è¯„ä¼°æ¡†æ¶è¿›è¡Œæ”¹é©ï¼Œä»¥ä¿ƒè¿›æ›´å…¬å¹³å’Œé€æ˜çš„åŸºå‡†æµ‹è¯•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20157",
            "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
            "url": "https://huggingface.co/papers/2504.20157",
            "abstract": "Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared.",
            "score": 28,
            "issue_id": 3504,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "18f16590c380c078",
            "authors": [
                "Zae Myung Kim",
                "Chanwoo Park",
                "Vipul Raheja",
                "Dongyeop Kang"
            ],
            "affiliations": [
                "Grammarly",
                "MIT",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20157.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#rl",
                    "#open_source",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Meta Policy Optimization (MPO) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. MPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ°-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Dynamic Reward Adjustment for Robust LLM Alignment",
                    "desc": "This paper presents Meta Policy Optimization (MPO), a new framework designed to improve reward-based alignment methods for large language models (LLMs). MPO tackles two main issues: the risk of reward hacking and the need for complex prompt engineering. By using a meta-reward model, MPO dynamically adjusts the reward prompts during training, ensuring that the reward signal remains effective and less exploitable. The results show that MPO achieves comparable or superior performance to traditional methods while simplifying the alignment process across various tasks."
                },
                "zh": {
                    "title": "å…ƒç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå…ƒç­–ç•¥ä¼˜åŒ–ï¼ˆMPOï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŸºäºå¥–åŠ±çš„å¯¹é½æ–¹æ³•ä¸­é¢ä¸´çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šå¥–åŠ±é»‘å®¢å’Œè„†å¼±çš„æç¤ºå·¥ç¨‹ã€‚MPOé€šè¿‡å¼•å…¥ä¸€ä¸ªåŠ¨æ€è°ƒæ•´å¥–åŠ±æ¨¡å‹æç¤ºçš„å…ƒå¥–åŠ±æ¨¡å‹ï¼Œæ¥æé«˜å¯¹é½çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§ç¯å¢ƒå˜åŒ–ï¼ŒæŒç»­ä¼˜åŒ–å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå‡å°‘æ‰‹åŠ¨è®¾è®¡å¥–åŠ±æç¤ºçš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMPOåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”ä¸éœ€è¦ä¸“é—¨çš„å¥–åŠ±è®¾è®¡ï¼Œå…·æœ‰æ›´å¼ºçš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20995",
            "title": "TesserAct: Learning 4D Embodied World Models",
            "url": "https://huggingface.co/papers/2504.20995",
            "abstract": "This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.",
            "score": 11,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "6339aba982c02561",
            "authors": [
                "Haoyu Zhen",
                "Qiao Sun",
                "Hongxin Zhang",
                "Junyan Li",
                "Siyuan Zhou",
                "Yilun Du",
                "Chuang Gan"
            ],
            "affiliations": [
                "HKUST",
                "Harvard University",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20995.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#video",
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "4D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… 4D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ 4D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ RGB-DN (RGB, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸), Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ 2D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑÑ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ RGB-DN Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ 4D-ÑÑ†ĞµĞ½Ñƒ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "Revolutionizing 4D Scene Prediction for Embodied Agents",
                    "desc": "This paper introduces a novel method for creating 4D world models that can predict how 3D scenes change over time based on the actions of an embodied agent. By using RGB-DN videos, which include color, depth, and normal information, the approach improves upon traditional 2D models by capturing detailed spatial and temporal dynamics. The authors enhance existing robotic manipulation datasets with depth and normal data, then fine-tune a video generation model to produce accurate RGB-DN predictions for each frame. This results in high-quality 4D scene representations that maintain coherence over time and space, enabling better policy learning and novel view synthesis in dynamic environments."
                },
                "zh": {
                    "title": "å­¦ä¹ å››ç»´ä¸–ç•Œæ¨¡å‹ï¼Œæå‡å…·èº«æ™ºèƒ½çš„é¢„æµ‹èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºå­¦ä¹ æ–°é¢–çš„å››ç»´å…·èº«ä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹ä¸‰ç»´åœºæ™¯åœ¨å…·èº«æ™ºèƒ½ä½“åŠ¨ä½œä¸‹çš„åŠ¨æ€æ¼”å˜ï¼Œç¡®ä¿æ—¶ç©ºä¸€è‡´æ€§ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒRGB-DNï¼ˆRGBã€æ·±åº¦å’Œæ³•çº¿ï¼‰è§†é¢‘æ¥å­¦ä¹ å››ç»´ä¸–ç•Œæ¨¡å‹ï¼Œè¿™ç§æ–¹æ³•è¶…è¶Šäº†ä¼ ç»Ÿçš„äºŒç»´æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†è¯¦ç»†çš„å½¢çŠ¶ã€é…ç½®å’Œæ—¶é—´å˜åŒ–çº³å…¥é¢„æµ‹ä¸­ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨ç°æˆæ¨¡å‹æ‰©å±•ç°æœ‰çš„æœºå™¨äººæ“ä½œè§†é¢‘æ•°æ®é›†ï¼ŒåŠ å…¥æ·±åº¦å’Œæ³•çº¿ä¿¡æ¯ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªæ ‡æ³¨æ•°æ®é›†ä¸Šå¾®è°ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œè”åˆé¢„æµ‹æ¯ä¸€å¸§çš„RGB-DNï¼Œæœ€ç»ˆå°†ç”Ÿæˆçš„RGBã€æ·±åº¦å’Œæ³•çº¿è§†é¢‘ç›´æ¥è½¬æ¢ä¸ºé«˜è´¨é‡çš„å››ç»´åœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20998",
            "title": "YoChameleon: Personalized Vision and Language Generation",
            "url": "https://huggingface.co/papers/2504.20998",
            "abstract": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive\" image generation approach to enhance image quality in a few-shot setting.",
            "score": 9,
            "issue_id": 3502,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "21fed074912b3e2f",
            "authors": [
                "Thao Nguyen",
                "Krishna Kumar Singh",
                "Jing Shi",
                "Trung Bui",
                "Yong Jae Lee",
                "Yuheng Li"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20998.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Yo'Chameleon - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ soft-prompt tuning Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3-5 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Yo'Chameleon Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾Ğ± Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğµ Ğ¸ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'soft-positive' Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Personalizing Multimodal Models with Yo'Chameleon",
                    "desc": "This paper presents Yo'Chameleon, a novel approach to personalize large multimodal models like GPT-4 for specific user concepts. By using 3-5 images of a subject, Yo'Chameleon applies soft-prompt tuning to incorporate personalized information, enabling the model to answer questions and generate contextually relevant images. The training involves a self-prompting optimization mechanism that ensures balanced performance across different modalities, as well as a 'soft-positive' image generation technique to improve image quality in few-shot scenarios. This work addresses the gap in adapting personalization methods for image generation within multimodal frameworks."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å¤šæ¨¡æ€æ¨¡å‹çš„åˆ›æ–°æ¢ç´¢",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚GPT-4ã€Geminiã€Chameleonï¼‰å·²ç»å‘å±•æˆä¸ºå¼ºå¤§çš„å·¥å…·ï¼Œæ‹¥æœ‰æ•°ç™¾ä¸‡ç”¨æˆ·ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶æ˜¯é€šç”¨çš„ï¼Œç¼ºä¹å¯¹ç‰¹å®šç”¨æˆ·æ¦‚å¿µçš„ä¸ªæ€§åŒ–çŸ¥è¯†ã€‚æœ¬æ–‡ä»‹ç»äº†Yo'Chameleonï¼Œè¿™æ˜¯é¦–æ¬¡ç ”ç©¶å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸ªæ€§åŒ–çš„æ–¹æ³•ã€‚Yo'Chameleoné€šè¿‡è½¯æç¤ºè°ƒä¼˜ï¼Œå°†ç‰¹å®šä¸»é¢˜çš„ä¿¡æ¯åµŒå…¥æ¨¡å‹ï¼Œä»¥å›ç­”å…³äºè¯¥ä¸»é¢˜çš„é—®é¢˜å¹¶åœ¨æ–°ç¯å¢ƒä¸­é‡å»ºå›¾åƒçš„åƒç´ çº§ç»†èŠ‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20630",
            "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting",
            "url": "https://huggingface.co/papers/2504.20630",
            "abstract": "Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo.",
            "score": 9,
            "issue_id": 3510,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "e4f8da880ab13ca8",
            "authors": [
                "Yu Zhang",
                "Wenxiang Guo",
                "Changhao Pan",
                "Zhiyuan Zhu",
                "Tao Jin",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Zhejiang Univeristy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20630.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#story_generation",
                    "#dataset"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞŸĞ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ñ€Ğ°Ğ¼Ñƒ: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ñ€Ğ°Ğ¼Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MRSDrama, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ñ€Ğ°Ğ¼Ñ‹, ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ISDrama, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¿Ğ¾Ğ· Ğ¸ Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ñ€Ğ°Ğ¼Ğ°-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Mamba. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ISDrama Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Drama Generation with Multimodal AI",
                    "desc": "This paper introduces a novel approach to generating immersive spatial drama using machine learning techniques. It presents MRSDrama, a unique dataset that includes various multimodal inputs such as binaural audio, scripts, and videos, which are essential for training the model. The proposed ISDrama model utilizes a Multimodal Pose Encoder and an Immersive Drama Transformer to effectively capture spatial dynamics and dramatic prosody. Experimental results demonstrate that ISDrama significantly improves the quality of generated drama compared to existing models, showcasing its potential for applications in augmented and virtual reality."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ²‰æµ¸å¼æˆå‰§ç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "è¿™ç¯‡è®ºæ–‡å…³æ³¨äºå¤šæ¨¡æ€æ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆï¼Œæ—¨åœ¨åŸºäºå¤šæ¨¡æ€æç¤ºåˆ›å»ºè¿ç»­çš„å¤šè¯­è€…åŒè€³è¯­éŸ³ï¼Œå…·æœ‰æˆå‰§æ€§çš„è¯­è°ƒã€‚è¯¥ä»»åŠ¡éœ€è¦åŒæ—¶å»ºæ¨¡ç©ºé—´ä¿¡æ¯å’Œæˆå‰§æ€§è¯­è°ƒï¼Œä½†æ•°æ®æ”¶é›†æˆæœ¬è¾ƒé«˜ã€‚æˆ‘ä»¬æ„å»ºäº†MRSDramaï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤šæ¨¡æ€å½•åˆ¶çš„ç©ºé—´æˆå‰§æ•°æ®é›†ï¼ŒåŒ…å«åŒè€³æˆå‰§éŸ³é¢‘ã€å‰§æœ¬ã€è§†é¢‘ã€å‡ ä½•å§¿æ€å’Œæ–‡æœ¬æç¤ºã€‚æˆ‘ä»¬æå‡ºçš„ISDramaæ¨¡å‹é€šè¿‡å¤šæ¨¡æ€æç¤ºç”Ÿæˆæ²‰æµ¸å¼ç©ºé—´æˆå‰§ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€å§¿æ€ç¼–ç å™¨å’Œæ²‰æµ¸å¼æˆå‰§å˜æ¢å™¨ï¼Œå®éªŒç»“æœè¡¨æ˜ISDramaåœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16046",
            "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
            "url": "https://huggingface.co/papers/2504.16046",
            "abstract": "The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of \"copyright takedown\" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.",
            "score": 9,
            "issue_id": 3504,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "3a2630d279485a85",
            "authors": [
                "Jingyu Zhang",
                "Jiacan Yu",
                "Marc Marone",
                "Benjamin Van Durme",
                "Daniel Khashabi"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16046.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#inference",
                    "#ethics",
                    "#leakage"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "BloomScrub: Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BloomScrub Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ° Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹. BloomScrub Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ Ğ‘Ğ»ÑƒĞ¼Ğ°) Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¸ÑĞºĞ° Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "BloomScrub: Smart Copyright Protection for Language Models",
                    "desc": "This paper addresses the issue of copyright infringement by large language models (LLMs) that may unintentionally generate content similar to copyrighted material. It introduces BloomScrub, a novel method that detects and rewrites long quotes from copyrighted sources during the model's inference phase. By using Bloom filters for efficient quote detection, BloomScrub can effectively screen large datasets while maintaining the model's utility. The approach not only reduces the risk of copyright infringement but also allows for flexible enforcement levels through adaptive abstention when necessary."
                },
                "zh": {
                    "title": "BloomScrubï¼šæœ‰æ•ˆçš„ç‰ˆæƒä¿æŠ¤æ–¹æ³•",
                    "desc": "æœ¬æ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ¥è§¦åˆ°ç‰ˆæƒææ–™æ‰€å¸¦æ¥çš„ç‰ˆæƒä¾µæƒé£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†\"ç‰ˆæƒæ’¤é”€\"æ–¹æ³•ï¼Œæ—¨åœ¨é˜²æ­¢æ¨¡å‹ç”Ÿæˆä¸ç‰ˆæƒå†…å®¹ç›¸ä¼¼çš„æ–‡æœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºBloomScrubçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶æ£€æµ‹å¼•ç”¨å¹¶è¿›è¡Œé‡å†™ï¼Œæ¥æœ‰æ•ˆåœ°å‡å°‘ç‰ˆæƒä¾µæƒé£é™©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBloomScrubåœ¨é™ä½ä¾µæƒé£é™©çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„å®ç”¨æ€§ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ‰§è¡Œä¸¥æ ¼æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20996",
            "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
            "url": "https://huggingface.co/papers/2504.20996",
            "abstract": "We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.",
            "score": 7,
            "issue_id": 3507,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "2427da9ba7d7d3f4",
            "authors": [
                "Sicheng Mo",
                "Thao Nguyen",
                "Xun Huang",
                "Siddharth Srinivasan Iyer",
                "Yijun Li",
                "Yuchen Liu",
                "Abhishek Tandon",
                "Eli Shechtman",
                "Krishna Kumar Singh",
                "Yong Jae Lee",
                "Bolei Zhou",
                "Yuheng Li"
            ],
            "affiliations": [
                "Adobe Research",
                "University of California, Los Angeles",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20996.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#small_models",
                    "#agi",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "X-Fusion: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "X-Fusion - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ±Ğ°ÑˆĞ½ÑĞ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ LLM Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸. X-Fusion Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "X-Fusion: Uniting Language and Vision for Superior Multimodal Performance",
                    "desc": "The paper introduces X-Fusion, a new framework that enhances pretrained Large Language Models (LLMs) for tasks involving both text and images. It uses a dual-tower architecture that allows the model to maintain its language processing abilities while integrating visual information. The results show that X-Fusion outperforms other models in tasks that convert images to text and vice versa. Additionally, the study highlights the importance of using understanding-focused data and feature alignment to improve model performance and training efficiency."
                },
                "zh": {
                    "title": "X-Fusionï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹æ¡†æ¶",
                    "desc": "æˆ‘ä»¬æå‡ºäº†X-Fusionæ¡†æ¶ï¼Œæ—¨åœ¨æ‰©å±•é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒå…¶è¯­è¨€èƒ½åŠ›ã€‚X-Fusioné‡‡ç”¨åŒå¡”è®¾è®¡ï¼Œä½¿ç”¨ç‰¹å®šäºæ¨¡æ€çš„æƒé‡ï¼Œä¿æŒLLMçš„å‚æ•°ä¸å˜ï¼ŒåŒæ—¶æ•´åˆè§†è§‰ç‰¹å®šä¿¡æ¯ä»¥è¿›è¡Œç†è§£å’Œç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-Fusionåœ¨å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºå…¶ä»–æ¶æ„ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œç»“åˆä»¥ç†è§£ä¸ºé‡ç‚¹çš„æ•°æ®å¯ä»¥æé«˜ç”Ÿæˆè´¨é‡ï¼Œå‡å°‘å›¾åƒæ•°æ®å™ªå£°å¯ä»¥å¢å¼ºæ•´ä½“æ€§èƒ½ï¼Œè€Œç‰¹å¾å¯¹é½åŠ é€Ÿäº†å°æ¨¡å‹çš„æ”¶æ•›ï¼Œä½†å¯¹å¤§æ¨¡å‹çš„å½±å“è¾ƒå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20073",
            "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.20073",
            "abstract": "Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.",
            "score": 7,
            "issue_id": 3518,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "80fac0e4c856acbd",
            "authors": [
                "Zihan Wang",
                "Kangrui Wang",
                "Qineng Wang",
                "Pingyue Zhang",
                "Linjie Li",
                "Zhengyuan Yang",
                "Kefan Yu",
                "Minh Nhat Nguyen",
                "Licheng Liu",
                "Eli Gottlieb",
                "Monica Lam",
                "Yiping Lu",
                "Kyunghyun Cho",
                "Jiajun Wu",
                "Li Fei-Fei",
                "Lijuan Wang",
                "Yejin Choi",
                "Manling Li"
            ],
            "affiliations": [
                "Imperial College London",
                "Microsoft",
                "New York University",
                "Northwestern University",
                "Singapore Management University",
                "Stanford University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20073.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#hallucinations",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "StarPO: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ñ‹ÑĞ»ÑÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ StarPO - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ RAGEN - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ 'Echo Trap' Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ ĞµĞµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ· Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering LLMs with StarPO for Enhanced Interactive Learning",
                    "desc": "This paper addresses the challenges of training large language models (LLMs) as interactive agents using reinforcement learning (RL). It introduces StarPO, a framework designed for trajectory-level RL that enhances decision-making over multiple interactions. The authors also present RAGEN, a modular system for training and evaluating these agents, revealing key insights about reward structures and the importance of diverse initial states. Their findings suggest that without detailed reward signals, LLMs struggle to develop deep reasoning capabilities, often resorting to simplistic strategies."
                },
                "zh": {
                    "title": "æå‡ä»£ç†æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºäº¤äº’ä»£ç†çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ—¶é—´å†³ç­–å’Œéšæœºç¯å¢ƒåé¦ˆçš„äº¤äº’ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†StarPOï¼ˆçŠ¶æ€-æ€è€ƒ-è¡ŒåŠ¨-å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè½¨è¿¹çº§ä»£ç†å¼ºåŒ–å­¦ä¹ çš„é€šç”¨æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†RAGENï¼Œä¸€ä¸ªç”¨äºè®­ç»ƒå’Œè¯„ä¼°LLMä»£ç†çš„æ¨¡å—åŒ–ç³»ç»Ÿã€‚ç ”ç©¶å‘ç°ï¼Œä»£ç†å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å­˜åœ¨å›å£°é™·é˜±æ¨¡å¼ï¼Œå¥–åŠ±æ–¹å·®å’Œæ¢¯åº¦æ³¢åŠ¨æ˜¾è‘—ï¼Œæˆ‘ä»¬é€šè¿‡StarPO-Sè¿›è¡Œç¨³å®šåŒ–å¤„ç†ã€‚æœ€åï¼Œæˆ‘ä»¬æŒ‡å‡ºç¼ºä¹ç»†è‡´çš„ã€åŸºäºæ¨ç†çš„å¥–åŠ±ä¿¡å·ä¼šå¯¼è‡´ä»£ç†æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œè¡¨ç°å‡ºæµ…å±‚ç­–ç•¥æˆ–å¹»è§‰æ€ç»´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20690",
            "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context\n  Generation in Large Scale Diffusion Transformer",
            "url": "https://huggingface.co/papers/2504.20690",
            "abstract": "Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/.",
            "score": 5,
            "issue_id": 3515,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "857739e05043be6d",
            "authors": [
                "Zechuan Zhang",
                "Ji Xie",
                "Yu Lu",
                "Zongxin Yang",
                "Yi Yang"
            ],
            "affiliations": [
                "DBMI, HMS, Harvard University",
                "ReLER, CCAI, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20690.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ (DiT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LoRA-MoE Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼."
                },
                "en": {
                    "title": "Efficient and Precise Instruction-Based Image Editing",
                    "desc": "This paper presents a novel approach to instruction-based image editing that balances precision and efficiency. It introduces a framework that allows for zero-shot editing compliance using in-context prompting, which avoids the need for structural changes in images. The authors also propose a hybrid tuning strategy that combines LoRA and MoE techniques to enhance flexibility and reduce the need for extensive retraining. Finally, they implement an inference-time scaling method that improves edit quality by selecting better initial noise using vision-language models, demonstrating superior performance with minimal training data and parameters."
                },
                "zh": {
                    "title": "é«˜æ•ˆç²¾å‡†çš„æŒ‡ä»¤å¼•å¯¼å›¾åƒç¼–è¾‘æ–°èŒƒå¼",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¥æé«˜å›¾åƒç¼–è¾‘çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸Šä¸‹æ–‡ç¼–è¾‘æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨é›¶-shotæƒ…å†µä¸‹éµå¾ªæŒ‡ä»¤ï¼ŒåŒæ—¶é¿å…ç»“æ„æ€§å˜åŒ–ã€‚é€šè¿‡ç»“åˆLoRA-MoEæ··åˆè°ƒä¼˜ç­–ç•¥ï¼Œæˆ‘ä»¬å®ç°äº†çµæ´»çš„é€‚åº”æ€§å’ŒåŠ¨æ€ä¸“å®¶è·¯ç”±ï¼Œè€Œæ— éœ€å¤§é‡é‡æ–°è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ—©æœŸè¿‡æ»¤æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€‰æ‹©æ›´å¥½çš„åˆå§‹å™ªå£°ï¼Œä»è€Œæé«˜ç¼–è¾‘è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18087",
            "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional\n  Talking Portrait Generation",
            "url": "https://huggingface.co/papers/2504.18087",
            "abstract": "Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audio's inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose a novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop a disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identity-agnostic Gaussian distributions. Second, we introduce a correlation-enhanced emotion conditioning module with learnable Emotion Banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our method's superiority, outperforming state-of-the-art approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our method's ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities.",
            "score": 4,
            "issue_id": 3509,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 25",
                "zh": "4æœˆ25æ—¥"
            },
            "hash": "c6ed690774bd93ba",
            "authors": [
                "Weipeng Tan",
                "Chuming Lin",
                "Chengming Xu",
                "FeiFan Xu",
                "Xiaobin Hu",
                "Xiaozhong Ji",
                "Junwei Zhu",
                "Chengjie Wang",
                "Yanwei Fu"
            ],
            "affiliations": [
                "Fudan University",
                "Youtu Lab, Tencent, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18087.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#emotion",
                    "#video",
                    "#diffusion",
                    "#games"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "DICE-Talk: Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DICE-Talk - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. DICE-Talk Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¸Ñ†Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ±."
                },
                "en": {
                    "title": "DICE-Talk: Emotionally Expressive Talking Heads with Identity Preservation",
                    "desc": "This paper presents DICE-Talk, a new framework for Talking Head Generation (THG) that enhances emotional expressiveness while maintaining speaker identity. It addresses key issues in current methods, such as the underutilization of audio emotional cues and identity leakage in emotion representations. The framework employs a disentangled emotion embedder to model emotional cues and a correlation-enhanced emotion conditioning module to capture relationships between emotions. Experimental results show that DICE-Talk outperforms existing methods in emotion accuracy and preserves identity in generated portraits."
                },
                "zh": {
                    "title": "è§£è€¦èº«ä»½ä¸æƒ…æ„Ÿï¼Œç”Ÿæˆä¸°å¯Œçš„è¯´è¯å¤´åƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶DICE-Talkï¼Œç”¨äºç”Ÿæˆæƒ…æ„Ÿä¸°å¯Œçš„è¯´è¯å¤´åƒï¼ŒåŒæ—¶ä¿æŒè¯´è¯è€…çš„èº«ä»½ã€‚æˆ‘ä»¬è¯†åˆ«äº†å½“å‰æƒ…æ„Ÿç”Ÿæˆæ–¹æ³•çš„ä¸‰ä¸ªä¸»è¦é™åˆ¶ï¼ŒåŒ…æ‹¬å¯¹éŸ³é¢‘æƒ…æ„Ÿçº¿ç´¢çš„åˆ©ç”¨ä¸è¶³ã€æƒ…æ„Ÿè¡¨ç¤ºä¸­çš„èº«ä»½æ³„æ¼ä»¥åŠæƒ…æ„Ÿå…³è”çš„å­¤ç«‹å­¦ä¹ ã€‚DICE-Talké€šè¿‡è§£è€¦èº«ä»½ä¸æƒ…æ„Ÿï¼Œå¹¶ç»“åˆç›¸ä¼¼ç‰¹å¾çš„æƒ…æ„Ÿæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æƒ…æ„Ÿå‡†ç¡®æ€§ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„å£å‹åŒæ­¥æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16272",
            "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization",
            "url": "https://huggingface.co/papers/2504.16272",
            "abstract": "Current reinforcement learning from human feedback (RLHF) pipelines for large language model (LLM) alignment typically assign scalar rewards to sequences, using the final token as a surrogate indicator for the quality of the entire sequence. However, this leads to sparse feedback and suboptimal token-level credit assignment. In this work, we frame reward shaping as an optimization problem focused on token-level credit assignment. We propose a reward-shaping function leveraging explainability methods such as SHAP and LIME to estimate per-token rewards from the reward model. To learn parameters of this shaping function, we employ a bilevel optimization framework that integrates Bayesian Optimization and policy training to handle noise from the token reward estimates. Our experiments show that achieving a better balance of token-level reward attribution leads to performance improvements over baselines on downstream tasks and finds an optimal policy faster during training. Furthermore, we show theoretically that explainability methods that are feature additive attribution functions maintain the optimal policy as the original reward.",
            "score": 4,
            "issue_id": 3519,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "4e342702553fd2c1",
            "authors": [
                "Ryan Koo",
                "Ian Yang",
                "Vipul Raheja",
                "Mingyi Hong",
                "Kwang-Sung Jun",
                "Dongyeop Kang"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Grammarly",
                "University of Arizona",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16272.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Token-Level Rewards for Better Language Model Training",
                    "desc": "This paper addresses the limitations of current reinforcement learning from human feedback (RLHF) methods in training large language models (LLMs). It highlights the issue of sparse feedback when using scalar rewards based on the final token, which hampers effective token-level credit assignment. The authors propose a novel reward-shaping function that utilizes explainability techniques like SHAP and LIME to provide more granular, per-token rewards. By employing a bilevel optimization framework that combines Bayesian Optimization with policy training, the approach improves performance on downstream tasks and accelerates the discovery of optimal policies during training."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ ‡è®°çº§å¥–åŠ±ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "å½“å‰çš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½ä¸­ï¼Œé€šå¸¸å°†æ ‡é‡å¥–åŠ±åˆ†é…ç»™åºåˆ—ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæ ‡è®°ä½œä¸ºæ•´ä¸ªåºåˆ—è´¨é‡çš„æ›¿ä»£æŒ‡æ ‡ã€‚è¿™ç§æ–¹æ³•å¯¼è‡´åé¦ˆç¨€ç–å’Œæ ‡è®°çº§åˆ«çš„ä¿¡ç”¨åˆ†é…ä¸ç†æƒ³ã€‚æœ¬æ–‡å°†å¥–åŠ±å¡‘é€ è§†ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œä¸“æ³¨äºæ ‡è®°çº§åˆ«çš„ä¿¡ç”¨åˆ†é…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¯è§£é‡Šæ€§æ–¹æ³•ï¼ˆå¦‚SHAPå’ŒLIMEï¼‰æ¥ä¼°è®¡æ¯ä¸ªæ ‡è®°å¥–åŠ±çš„å¥–åŠ±å¡‘é€ å‡½æ•°ï¼Œå¹¶é€šè¿‡åŒå±‚ä¼˜åŒ–æ¡†æ¶æ¥å­¦ä¹ è¯¥å‡½æ•°çš„å‚æ•°ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´å¿«åœ°æ‰¾åˆ°æœ€ä½³ç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20114",
            "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering",
            "url": "https://huggingface.co/papers/2504.20114",
            "abstract": "Retrieval-augmented generation (RAG) systems face significant challenges in multi-hop question answering (MHQA), where complex queries require synthesizing information across multiple document chunks. Existing approaches typically rely on iterative LLM-based query rewriting and routing, resulting in high computational costs due to repeated LLM invocations and multi-stage processes. To address these limitations, we propose TreeHop, an embedding-level framework without the need for LLMs in query refinement. TreeHop dynamically updates query embeddings by fusing semantic information from prior queries and retrieved documents, enabling iterative retrieval through embedding-space operations alone. This method replaces the traditional \"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined \"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead. Moreover, a rule-based stop criterion is introduced to further prune redundant retrievals, balancing efficiency and recall rate. Experimental results show that TreeHop rivals advanced RAG methods across three open-domain MHQA datasets, achieving comparable performance with only 5\\%-0.4\\% of the model parameter size and reducing the query latency by approximately 99\\% compared to concurrent approaches. This makes TreeHop a faster and more cost-effective solution for deployment in a range of knowledge-intensive applications. For reproducibility purposes, codes and data are available here: https://github.com/allen-li1231/TreeHop.",
            "score": 3,
            "issue_id": 3511,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "56755beaa151585a",
            "authors": [
                "Zhonghao Li",
                "Kunpeng Zhang",
                "Jinghuai Ou",
                "Shuliang Liu",
                "Xuming Hu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20114.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rag"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "TreeHop: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ±ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "TreeHop - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². TreeHop Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ-ĞŸĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ-Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ-Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ' Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ-Ğ­Ğ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³-Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ'. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TreeHop ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RAG, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 5%-0.4% Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 99%."
                },
                "en": {
                    "title": "Streamlining Multi-Hop Question Answering with TreeHop",
                    "desc": "The paper introduces TreeHop, a new framework designed to improve multi-hop question answering (MHQA) in retrieval-augmented generation (RAG) systems. Unlike traditional methods that rely on large language models (LLMs) for query rewriting, TreeHop operates at the embedding level, dynamically updating query embeddings by integrating information from previous queries and retrieved documents. This approach simplifies the retrieval process by replacing the complex cycle of retrieving, rewriting, and vectorizing with a more efficient 'Retrieve-Embed-Retrieve' loop. Experimental results demonstrate that TreeHop achieves competitive performance with significantly lower computational costs and faster query processing times, making it a practical solution for knowledge-intensive applications."
                },
                "zh": {
                    "title": "TreeHopï¼šé«˜æ•ˆçš„å¤šè·³é—®ç­”è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTreeHopçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šè·³é—®ç­”ä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŸ¥è¯¢é‡å†™æ–¹æ³•ä¸åŒï¼ŒTreeHopé€šè¿‡èåˆå…ˆå‰æŸ¥è¯¢å’Œæ£€ç´¢æ–‡æ¡£çš„è¯­ä¹‰ä¿¡æ¯ï¼ŒåŠ¨æ€æ›´æ–°æŸ¥è¯¢åµŒå…¥ã€‚è¯¥æ–¹æ³•ç®€åŒ–äº†æ£€ç´¢è¿‡ç¨‹ï¼Œé‡‡ç”¨äº†â€œæ£€ç´¢-åµŒå…¥-æ£€ç´¢â€çš„å¾ªç¯ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTreeHopåœ¨å¤šä¸ªå¼€æ”¾é¢†åŸŸçš„å¤šè·³é—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸”æ¨¡å‹å‚æ•°é‡ä»…ä¸ºä¼ ç»Ÿæ–¹æ³•çš„5%-0.4%ï¼ŒæŸ¥è¯¢å»¶è¿Ÿå‡å°‘çº¦99%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18942",
            "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes",
            "url": "https://huggingface.co/papers/2504.18942",
            "abstract": "Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Building on these findings, we propose a set of design suggestions, rooted in empirical observations, that align AI assistance with human goals of clarity, completeness, creativity, and efficiency, through hybrid planning, adaptive execution, and decision-point support. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on our project page (https://minnesotanlp.github.io/LawFlow-website/).",
            "score": 3,
            "issue_id": 3520,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 26",
                "zh": "4æœˆ26æ—¥"
            },
            "hash": "045a82785fe0f501",
            "authors": [
                "Debarati Das",
                "Khanh Chi Le",
                "Ritik Sachin Parkar",
                "Karin De Langis",
                "Brendan Madson",
                "Chad M. Berryman",
                "Robin M. Willis",
                "Daniel H. Moses",
                "Brett McDonnell",
                "Daniel Schwarcz",
                "Dongyeop Kang"
            ],
            "affiliations": [
                "Computer Science and Engineering, University of Minnesota",
                "Law School, University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18942.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#data",
                    "#alignment",
                    "#reasoning"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "LawFlow: ĞĞ° Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜ Ğ² ÑÑ€Ğ¸ÑĞ¿Ñ€ÑƒĞ´ĞµĞ½Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LawFlow - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ñƒ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ²-ÑÑ€Ğ¸ÑÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, LawFlow Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ, Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸. Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (Ğ¯Ğœ), Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° (Ğ˜Ğ˜) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸ ÑÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹, ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering Legal Practice with Adaptive AI Workflows",
                    "desc": "This paper introduces LawFlow, a new dataset designed to capture complete legal workflows as performed by trained law students in real-world scenarios. Unlike existing datasets that focus on isolated tasks, LawFlow emphasizes the dynamic and iterative nature of legal reasoning, reflecting the complexities of actual legal practice. The study compares human-generated workflows with those produced by large language models (LLMs), revealing that human workflows are more modular and adaptable, while LLMs tend to follow a more linear and exhaustive approach. The authors suggest that AI should assist legal professionals in supportive roles rather than attempting to execute complex workflows independently, and they provide design recommendations to enhance AI's alignment with human legal reasoning."
                },
                "zh": {
                    "title": "æå‡æ³•å¾‹AIçš„é€‚åº”æ€§ä¸åä½œæ€§",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†LawFlowæ•°æ®é›†ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰æ³•å¾‹AIæ¨¡å‹åœ¨å¤æ‚æ³•å¾‹å·¥ä½œæµä¸­çš„ä¸è¶³ã€‚è¯¥æ•°æ®é›†æ”¶é›†äº†ç»è¿‡åŸ¹è®­çš„æ³•å­¦ç”Ÿåœ¨çœŸå®å•†ä¸šå®ä½“æˆç«‹åœºæ™¯ä¸­çš„å®Œæ•´æ³•å¾‹å·¥ä½œæµï¼Œå¼ºè°ƒäº†åŠ¨æ€ã€æ¨¡å—åŒ–å’Œè¿­ä»£æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼Œäººç±»çš„å·¥ä½œæµæ›´å…·é€‚åº”æ€§å’Œæ¨¡å—åŒ–ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å·¥ä½œæµåˆ™æ›´ä¸ºçº¿æ€§å’Œå…¨é¢ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç³»åˆ—è®¾è®¡å»ºè®®ï¼Œä»¥æé«˜AIåœ¨æ³•å¾‹å®è·µä¸­çš„æ”¯æŒèƒ½åŠ›ï¼Œå¼ºè°ƒæ¸…æ™°æ€§ã€å®Œæ•´æ€§ã€åˆ›é€ æ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18738",
            "title": "A Review of 3D Object Detection with Vision-Language Models",
            "url": "https://huggingface.co/papers/2504.18738",
            "abstract": "This review provides a systematic analysis of comprehensive survey of 3D object detection with vision-language models(VLMs) , a rapidly advancing area at the intersection of 3D vision and multimodal AI. By examining over 100 research papers, we provide the first systematic analysis dedicated to 3D object detection with vision-language models. We begin by outlining the unique challenges of 3D object detection with vision-language models, emphasizing differences from 2D detection in spatial reasoning and data complexity. Traditional approaches using point clouds and voxel grids are compared to modern vision-language frameworks like CLIP and 3D LLMs, which enable open-vocabulary detection and zero-shot generalization. We review key architectures, pretraining strategies, and prompt engineering methods that align textual and 3D features for effective 3D object detection with vision-language models. Visualization examples and evaluation benchmarks are discussed to illustrate performance and behavior. Finally, we highlight current challenges, such as limited 3D-language datasets and computational demands, and propose future research directions to advance 3D object detection with vision-language models. >Object Detection, Vision-Language Models, Agents, VLMs, LLMs, AI",
            "score": 2,
            "issue_id": 3515,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 25",
                "zh": "4æœˆ25æ—¥"
            },
            "hash": "991de2200cc55a55",
            "authors": [
                "Ranjan Sapkota",
                "Konstantinos I Roumeliotis",
                "Rahul Harsha Cheppally",
                "Marco Flores Calero",
                "Manoj Karkee"
            ],
            "affiliations": [
                "Cornell University, USA",
                "Kansas State University, USA",
                "Universidad de las Fuerzas Armadas, Ecuador",
                "University of Peloponnese, Greece"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18738.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#survey",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ 3D-Ğ·Ñ€ĞµĞ½Ğ¸Ñ: VLM Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ 3D-Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº CLIP Ğ¸ 3D LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VLM."
                },
                "en": {
                    "title": "Advancing 3D Object Detection with Vision-Language Models",
                    "desc": "This paper reviews the field of 3D object detection using vision-language models (VLMs), which combine visual and textual information. It analyzes over 100 research papers to identify the unique challenges faced in 3D detection compared to 2D detection, particularly in spatial reasoning and data complexity. The authors compare traditional methods, like point clouds, with modern VLM approaches that allow for open-vocabulary detection and zero-shot generalization. They also discuss key architectures, pretraining strategies, and the importance of prompt engineering in aligning textual and 3D features for improved detection performance."
                },
                "zh": {
                    "title": "æ¨åŠ¨3Dç‰©ä½“æ£€æµ‹çš„è§†è§‰-è¯­è¨€æ¨¡å‹ç ”ç©¶",
                    "desc": "è¿™ç¯‡ç»¼è¿°æ–‡ç« ç³»ç»Ÿåˆ†æäº†ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œ3Dç‰©ä½“æ£€æµ‹çš„ç ”ç©¶è¿›å±•ã€‚é€šè¿‡å®¡æŸ¥è¶…è¿‡100ç¯‡ç ”ç©¶è®ºæ–‡ï¼Œæ–‡ç« é¦–æ¬¡æä¾›äº†ä¸“é—¨é’ˆå¯¹3Dç‰©ä½“æ£€æµ‹çš„ç³»ç»Ÿåˆ†æï¼Œå¼ºè°ƒäº†ä¸2Dæ£€æµ‹åœ¨ç©ºé—´æ¨ç†å’Œæ•°æ®å¤æ‚æ€§æ–¹é¢çš„ä¸åŒæŒ‘æˆ˜ã€‚æ–‡ç« æ¯”è¾ƒäº†ä¼ ç»Ÿçš„ç‚¹äº‘å’Œä½“ç´ ç½‘æ ¼æ–¹æ³•ä¸ç°ä»£çš„è§†è§‰-è¯­è¨€æ¡†æ¶ï¼Œå¦‚CLIPå’Œ3D LLMsï¼Œè¿™äº›æ¡†æ¶æ”¯æŒå¼€æ”¾è¯æ±‡æ£€æµ‹å’Œé›¶æ ·æœ¬æ³›åŒ–ã€‚æœ€åï¼Œæ–‡ç« è®¨è®ºäº†å½“å‰çš„æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æ¨åŠ¨3Dç‰©ä½“æ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17838",
            "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards",
            "url": "https://huggingface.co/papers/2504.17838",
            "abstract": "We investigate reinforcement learning (RL) for privileged planning in autonomous driving. State-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail. RL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning. Contemporary RL approaches for driving use complex shaped rewards that sum multiple individual rewards, \\eg~progress, position, or orientation rewards. We show that PPO fails to optimize a popular version of these rewards when the mini-batch size is increased, which limits the scalability of these approaches. Instead, we propose a new reward design based primarily on optimizing a single intuitive reward term: route completion. Infractions are penalized by terminating the episode or multiplicatively reducing route completion. We find that PPO scales well with higher mini-batch sizes when trained with our simple reward, even improving performance. Training with large mini-batch sizes enables efficient scaling via distributed data parallelism. We scale PPO to 300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The resulting model achieves 64 DS on the CARLA longest6 v2 benchmark, outperforming other RL methods with more complex rewards by a large margin. Requiring only minimal adaptations from its use in CARLA, the same method is the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and 90.6 in reactive traffic on the Val14 benchmark while being an order of magnitude faster than prior work.",
            "score": 1,
            "issue_id": 3519,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "2a2f148d75eae81c",
            "authors": [
                "Bernhard Jaeger",
                "Daniel Dauner",
                "Jens BeiÃŸwenger",
                "Simon Gerstenecker",
                "Kashyap Chitta",
                "Andreas Geiger"
            ],
            "affiliations": [
                "University of TÃ¼bingen, TÃ¼bingen AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17838.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#games",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ° - Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ PPO Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¸Ğ½Ğ¸-Ğ±Ğ°Ñ‚Ñ‡Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… CARLA Ğ¸ nuPlan Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RL Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Simplifying Rewards for Scalable Reinforcement Learning in Autonomous Driving",
                    "desc": "This paper explores the use of reinforcement learning (RL) for planning in autonomous driving, highlighting its advantages over traditional rule-based methods. The authors identify that existing RL techniques often struggle with complex reward structures, particularly when using larger mini-batch sizes. They propose a simplified reward system focused on route completion, which enhances the scalability and performance of the Proximal Policy Optimization (PPO) algorithm. Their approach demonstrates significant improvements in training efficiency and model performance across large datasets, outperforming other methods with more complicated reward designs."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›è‡ªåŠ¨é©¾é©¶çš„é«˜æ•ˆè§„åˆ’",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œç‰¹æƒè§„åˆ’çš„é—®é¢˜ã€‚ç°æœ‰çš„åŸºäºè§„åˆ™çš„æ–¹æ³•åœ¨å¤„ç†é•¿å°¾é—®é¢˜æ—¶æ•ˆæœä¸ä½³ï¼Œè€ŒRLæ–¹æ³•å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”ä¸ä¼šåƒæ¨¡ä»¿å­¦ä¹ é‚£æ ·å‡ºç°ç´¯ç§¯é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±è®¾è®¡ï¼Œä¸»è¦ä¼˜åŒ–å•ä¸€çš„ç›´è§‚å¥–åŠ±é¡¹ï¼šè·¯çº¿å®Œæˆåº¦ï¼Œè¿™æ ·åœ¨å¢åŠ å°æ‰¹é‡å¤§å°æ—¶ï¼ŒPPOçš„ä¼˜åŒ–æ•ˆæœå¾—åˆ°äº†æå‡ã€‚é€šè¿‡åœ¨CARLAå’ŒnuPlanä¸Šè¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºäº†RLåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-29.html",
    "link_next": "2025-05-01.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "29.04",
        "en": "04/29",
        "zh": "4æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "01.05",
        "en": "05/01",
        "zh": "5æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 4,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 3,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 0,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0,
        "#emotion": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œç§°ä¸ºUniversalRAGã€‚å®ƒèƒ½ä»ä¸åŒæ¨¡æ€å’Œç²’åº¦çš„çŸ¥è¯†æºä¸­æ£€ç´¢å’Œæ•´åˆä¿¡æ¯ã€‚ç°æœ‰çš„RAGæ–¹æ³•ä¸»è¦å±€é™äºå•ä¸€æ¨¡æ€çš„è¯­æ–™åº“ï¼Œè€ŒUniversalRAGé€šè¿‡æ¨¡æ€æ„ŸçŸ¥è·¯ç”±æœºåˆ¶åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„æ¨¡æ€ç‰¹å®šè¯­æ–™åº“è¿›è¡Œæ£€ç´¢ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§å’ŒèŒƒå›´ï¼Œå¯¹æ¯ç§æ¨¡æ€è¿›è¡Œå¤šçº§ç²’åº¦çš„ç»„ç»‡ã€‚ç ”ç©¶åœ¨8ä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸ŠéªŒè¯äº†UniversalRAGçš„ä¼˜è¶Šæ€§ã€‚",
        "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œç§°ä¸ºUniversalRAGã€‚å®ƒèƒ½ä»ä¸åŒæ¨¡æ€å’Œç²’åº¦çš„çŸ¥è¯†æºä¸­æ£€ç´¢å’Œæ•´åˆä¿¡æ¯ã€‚ç°æœ‰çš„RAGæ–¹æ³•ä¸»è¦å±€é™äºå•ä¸€æ¨¡æ€çš„è¯­æ–™åº“ï¼Œè€ŒUniversalRAGé€šè¿‡æ¨¡æ€æ„ŸçŸ¥è·¯ç”±æœºåˆ¶åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„æ¨¡æ€ç‰¹å®šè¯­æ–™åº“è¿›è¡Œæ£€ç´¢ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§å’ŒèŒƒå›´ï¼Œå¯¹æ¯ç§æ¨¡æ€è¿›è¡Œå¤šçº§ç²’åº¦çš„ç»„ç»‡ã€‚ç ”ç©¶åœ¨8ä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸ŠéªŒè¯äº†UniversalRAGçš„ä¼˜è¶Šæ€§ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de jiÇn suÇ’ zÄ“ng qiÃ¡ng shÄ“ng chÃ©ng (RAG) kuÃ ng jiÃ , chÄ“ng wÃ©i UniversalRAG. tÄ nÃ©ng cÃ³ng bÃ¹ tÃ³ng mÃ³ tÃ i hÃ© lÃ¬ dÃ¹ de zhÄ« shi yuÃ¡n zhÅng jiÇn suÇ’ hÃ© zhÄ›ng hÃ© xÃ¬n xÄ«. xiÃ n yÇ’u de RAG fÄng fÇ zhÇ” yÃ o jÃº xiÃ n yÄ« dÃ n yÄ« mÃ³ tÃ i de yÇ” liÃ o kÃ¹, Ã©r UniversalRAG tÅng guÃ² mÃ³ tÃ i gÇn juÃ© lÃ¹ yÃ³u jÄ« zhÃ¬ dÃ²ng tÃ i xuÇn zÃ© zuÃ¬ hÃ© shÃ¬ de mÃ³ tÃ i tÃ¨ dÃ¬ng yÇ” liÃ o kÃ¹ jÃ¬n xÃ­ng jiÇn suÇ’. cÇ wÃ i, tÄ hÃ¡i gÄ“n jÃ¹ chÃ¡ xÃºn de fÃº zÃ  xÃ¬ng hÃ© fÃ n wÃ©i, duÃ¬ mÄ›i zhÇ’ng mÃ³ tÃ i jÃ¬n xÃ­ng duÅ jÃ­ lÃ¬ dÃ¹ de zÇ” zhÄ«. yÃ¡n jiÅ« zÃ i 8 gÃ¨ duÅ mÃ³ tÃ i bÇ zhÇ”n shÃ ng yÃ n zhÃ¨ng le UniversalRAG de yÅu yuÃ¨ xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"æ£€ç´¢\", \"pinyin\": \"jiÇnsuÇ’\", \"trans\": \"retrieval\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ngqiÃ¡ng\", \"trans\": \"enhancement\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generation\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"æ¨¡æ€\", \"pinyin\": \"mÃ³ tÃ i\", \"trans\": \"modality\"},\n    {\"word\": \"ç²’åº¦\", \"pinyin\": \"lÃ¬ dÃ¹\", \"trans\": \"granularity\"},\n    {\"word\": \"çŸ¥è¯†æº\", \"pinyin\": \"zhÄ«shi yuÃ¡n\", \"trans\": \"knowledge source\"},\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›nghÃ©\", \"trans\": \"integration\"},\n    {\"word\": \"å±€é™äº\", \"pinyin\": \"jÃº xiÃ n yÃº\", \"trans\": \"limited to\"},\n    {\"word\": \"å•ä¸€\", \"pinyin\": \"dÄn yÄ«\", \"trans\": \"single\"},\n    {\"word\": \"è¯­æ–™åº“\", \"pinyin\": \"yÇ” liÃ o kÃ¹\", \"trans\": \"corpus\"},\n    {\"word\": \"æ„ŸçŸ¥\", \"pinyin\": \"gÇnzhÄ«\", \"trans\": \"perception\"},\n    {\"word\": \"è·¯ç”±\", \"pinyin\": \"lÃ¹ yÃ³u\", \"trans\": \"routing\"},\n    {\"word\": \"æœºåˆ¶\", \"pinyin\": \"jÄ«zhÃ¬\", \"trans\": \"mechanism\"},\n    {\"word\": \"åŠ¨æ€\", \"pinyin\": \"dÃ²ngtÃ i\", \"trans\": \"dynamic\"},\n    {\"word\": \"é€‰æ‹©\", \"pinyin\": \"xuÇnzÃ©\", \"trans\": \"selection\"},\n    {\"word\": \"åˆé€‚\", \"pinyin\": \"hÃ©shÃ¬\", \"trans\": \"appropriate\"},\n    {\"word\": \"ç‰¹å®š\", \"pinyin\": \"tÃ¨dÃ¬ng\", \"trans\": \"specific\"},\n    {\"word\": \"æŸ¥è¯¢\", \"pinyin\": \"chÃ¡xÃºn\", \"trans\": \"query\"},\n    {\"word\": \"å¤æ‚æ€§\", \"pinyin\": \"fÃ¹zÃ¡xÃ¬ng\", \"trans\": \"complexity\"},\n    {\"word\": \"èŒƒå›´\", \"pinyin\": \"fÃ nwÃ©i\", \"trans\": \"scope\"},\n    {\"word\": \"å¤šçº§\", \"pinyin\": \"duÅjÃ­\", \"trans\": \"multi-level\"},\n    {\"word\": \"ç»„ç»‡\", \"pinyin\": \"zÇ”zhÄ«\", \"trans\": \"organization\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡njiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"éªŒè¯\", \"pinyin\": \"yÃ nzhÃ¨ng\", \"trans\": \"validation\"},\n    {\"word\": \"ä¼˜è¶Šæ€§\", \"pinyin\": \"yÅuyuÃ¨xÃ¬ng\", \"trans\": \"superiority\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ«zhÇ”n\", \"trans\": \"benchmark\"}\n]",
        "trans": "This article introduces a new Retrieval-Augmented Generation (RAG) framework called UniversalRAG. It can retrieve and integrate information from knowledge sources of different modalities and granularities. Existing RAG methods are mainly limited to single-modality corpora, while UniversalRAG dynamically selects the most suitable modality-specific corpus for retrieval through a modality-aware routing mechanism. Additionally, it organizes each modality at multiple granularity levels based on the complexity and scope of the query. The superiority of UniversalRAG has been validated on 8 multimodal benchmarks.",
        "update_ts": "2025-04-30 09:12"
    }
}