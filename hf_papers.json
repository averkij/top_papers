{
    "date": {
        "ru": "4 июля",
        "en": "July 4",
        "zh": "7月4日"
    },
    "time_utc": "2025-07-05 01:58",
    "weekday": 4,
    "issue_id": 4660,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.02592",
            "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
            "url": "https://huggingface.co/papers/2507.02592",
            "abstract": "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.",
            "score": 56,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "0a8cc61c0251e5da",
            "authors": [
                "Kuan Li",
                "Zhongwang Zhang",
                "Huifeng Yin",
                "Liwen Zhang",
                "Litu Ou",
                "Jialong Wu",
                "Wenbiao Yin",
                "Baixuan Li",
                "Zhengwei Tao",
                "Xinyu Wang",
                "Weizhou Shen",
                "Junkai Zhang",
                "Dingchu Zhang",
                "Xixi Wu",
                "Yong Jiang",
                "Ming Yan",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02592.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#agents",
                    "#rl"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "WebSailor: навигация в океане неопределенности для ИИ",
                    "desc": "Статья представляет новый метод обучения языковых моделей под названием WebSailor. Он направлен на преодоление когнитивных ограничений человека в задачах поиска сложной информации. WebSailor использует генерацию задач с высокой неопределенностью и обучение с подкреплением для развития способности моделей систематически снижать неопределенность. Результаты показывают, что WebSailor позволяет открытым моделям достичь производительности проприетарных систем в сложных информационных задачах."
                },
                "en": {
                    "title": "Empowering Open-Source Agents to Compete with the Best",
                    "desc": "This paper discusses advancements in training large language models (LLMs) to surpass human cognitive limitations. It highlights the success of proprietary systems like DeepResearch, which excel in complex information-seeking tasks due to their unique reasoning abilities. The authors introduce WebSailor, a post-training methodology that enhances LLMs by generating high-uncertainty tasks and employing a novel reinforcement learning algorithm called Duplicating Sampling Policy Optimization (DUPO). The results show that WebSailor significantly improves the performance of open-source agents, enabling them to compete with proprietary models in challenging information retrieval scenarios."
                },
                "zh": {
                    "title": "超越认知局限，提升信息检索能力",
                    "desc": "本论文探讨了超越人类认知局限性在大型语言模型（LLM）训练中的重要性。我们提出的WebSailor方法通过系统性地减少在广阔信息环境中导航时的极端不确定性，来提升模型的推理能力。该方法结合了结构化采样和信息模糊化等技术，生成新的高不确定性任务，并采用高效的强化学习算法进行训练。实验结果表明，WebSailor在复杂的信息检索任务中显著优于所有开源代理，缩小了与专有代理的能力差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02813",
            "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with\n  TriMap Video Diffusion",
            "url": "https://huggingface.co/papers/2507.02813",
            "abstract": "Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X.",
            "score": 45,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "726c080e7ea88c4b",
            "authors": [
                "Fangfu Liu",
                "Hao Li",
                "Jiawei Chi",
                "Hanyang Wang",
                "Minghui Yang",
                "Fudong Wang",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Ant Group",
                "NTU",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02813.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#3d",
                    "#games",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "Генерация 3D-сцен с языковым пониманием по нескольким изображениям",
                    "desc": "LangScene-X - это новая генеративная система для восстановления и понимания 3D-сцен с открытым словарем на основе 2D-изображений. Она использует видео-диффузионную модель TriMap для генерации согласованной мультимодальной информации (RGB, нормали, сегментация) из ограниченного числа ракурсов. Система включает языковой квантованный компрессор (LQC) для эффективного кодирования языковых эмбеддингов, что позволяет обобщать на новые сцены без переобучения. LangScene-X превосходит современные методы по качеству и обобщаемости при реконструкции 3D-сцен с языковой привязкой."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with Language-Embedded Insights",
                    "desc": "This paper presents LangScene-X, a new framework for creating 3D structures from 2D images using language information. It addresses the limitations of previous methods that relied on dense-view reconstructions, which often resulted in poor quality when only limited views were available. LangScene-X utilizes a TriMap video diffusion model to generate consistent visual and semantic data from sparse inputs, enhancing the reconstruction process. Additionally, it introduces a Language Quantized Compressor to efficiently encode language embeddings, allowing for better generalization across different scenes without needing to retrain for each one."
                },
                "zh": {
                    "title": "LangScene-X：从稀疏视图生成一致的3D场景",
                    "desc": "本文提出了一种新颖的生成框架LangScene-X，用于从稀疏视图中恢复3D结构并进行场景理解。该框架结合了语言信息和多模态数据，能够生成一致的3D场景。我们首先训练了一个TriMap视频扩散模型，从稀疏输入中生成外观、几何和语义信息。通过引入语言量化压缩器（LQC），我们实现了跨场景的泛化，避免了逐场景的重新训练。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02321",
            "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate\n  Features Feedback",
            "url": "https://huggingface.co/papers/2507.02321",
            "abstract": "InnerControl enforces spatial consistency across all diffusion steps by training lightweight convolutional probes to improve control fidelity and generation quality in text-to-image diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth).",
            "score": 33,
            "issue_id": 4646,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "f340ac71ebc152be",
            "authors": [
                "Nina Konovalova",
                "Maxim Nikolaev",
                "Andrey Kuznetsov",
                "Aibek Alanov"
            ],
            "affiliations": [
                "AIRI, Russia",
                "HSE University, Russia",
                "Innopolis, Russia",
                "Sber, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02321.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#alignment",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точный контроль генерации изображений на всех этапах диффузии",
                    "desc": "InnerControl - это новый метод обучения для улучшения пространственного контроля в диффузионных моделях генерации изображений по тексту. Он использует легковесные сверточные зонды для реконструкции входных сигналов управления на всех этапах диффузии. Метод минимизирует расхождение между предсказанными и целевыми условиями на протяжении всего процесса, что улучшает точность контроля и качество генерации. В сочетании с существующими техниками InnerControl достигает передовых результатов для различных методов кондиционирования."
                },
                "en": {
                    "title": "Enhancing Spatial Consistency in Diffusion Models with InnerControl",
                    "desc": "InnerControl is a novel training strategy designed to enhance spatial consistency in text-to-image diffusion models. It utilizes lightweight convolutional probes to reconstruct control signals from intermediate features during the denoising process. By applying an alignment loss that minimizes the difference between predicted and target conditions at every diffusion step, InnerControl significantly improves control fidelity and overall generation quality. This approach, when combined with existing methods like ControlNet++, achieves state-of-the-art results across various conditioning techniques."
                },
                "zh": {
                    "title": "InnerControl：提升扩散模型的空间一致性与生成质量",
                    "desc": "InnerControl是一种训练策略，旨在增强文本到图像扩散模型在所有扩散步骤中的空间一致性。通过训练轻量级卷积探针，InnerControl能够在每个去噪步骤中从中间UNet特征中重建输入控制信号（如边缘和深度）。这种方法有效地提取信号，即使在高度噪声的潜在空间中，也能为训练提供伪真实控制。通过最小化整个扩散过程中的预测条件与目标条件之间的差异，InnerControl显著提高了控制的准确性和生成的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02025",
            "title": "IntFold: A Controllable Foundation Model for General and Specialized\n  Biomolecular Structure Prediction",
            "url": "https://huggingface.co/papers/2507.02025",
            "abstract": "We introduce IntFold, a controllable foundation model for both general and specialized biomolecular structure prediction. IntFold demonstrates predictive accuracy comparable to the state-of-the-art AlphaFold3, while utilizing a superior customized attention kernel. Beyond standard structure prediction, IntFold can be adapted to predict allosteric states, constrained structures, and binding affinity through the use of individual adapters. Furthermore, we introduce a novel confidence head to estimate docking quality, offering a more nuanced assessment for challenging targets such as antibody-antigen complexes. Finally, we share insights gained during the training process of this computationally intensive model.",
            "score": 31,
            "issue_id": 4642,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "5e8a0f59d9493778",
            "authors": [
                "The IntFold Team",
                "Leon Qiao",
                "Wayne Bai",
                "He Yan",
                "Gary Liu",
                "Nova Xi",
                "Xiang Zhang"
            ],
            "affiliations": [
                "IntelliGen AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02025.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#healthcare"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "IntFold: Гибкий инструмент для точного прогнозирования биомолекулярных структур",
                    "desc": "IntFold - это новая управляемая базовая модель для предсказания как общих, так и специализированных биомолекулярных структур. Она демонстрирует точность прогнозирования, сравнимую с современной моделью AlphaFold3, используя улучшенное пользовательское ядро внимания. IntFold может быть адаптирована для предсказания аллостерических состояний, ограниченных структур и аффинности связывания с помощью индивидуальных адаптеров. Модель также включает новый блок оценки уверенности для определения качества докинга, что особенно полезно для сложных целей, таких как комплексы антитело-антиген."
                },
                "en": {
                    "title": "IntFold: Advancing Biomolecular Structure Prediction with Precision and Flexibility",
                    "desc": "IntFold is a new foundation model designed for predicting the structures of biomolecules, both in general and specialized contexts. It achieves high accuracy in predictions, rivaling the leading model AlphaFold3, by employing a unique attention mechanism. The model is versatile, allowing for adaptations to predict various states and properties of biomolecules, such as allosteric states and binding affinities, through the use of specific adapters. Additionally, IntFold includes a confidence head that assesses the quality of docking predictions, particularly for complex targets like antibody-antigen interactions, and shares valuable insights from its training process."
                },
                "zh": {
                    "title": "IntFold：生物分子结构预测的新突破",
                    "desc": "我们介绍了IntFold，这是一种可控的基础模型，用于一般和专业的生物分子结构预测。IntFold的预测准确性与最先进的AlphaFold3相当，同时采用了更优的定制注意力核。除了标准的结构预测，IntFold还可以通过使用单独的适配器来预测变构状态、受限结构和结合亲和力。此外，我们引入了一种新颖的置信度头，以评估对接质量，为抗体-抗原复合物等具有挑战性的目标提供更细致的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01352",
            "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy",
            "url": "https://huggingface.co/papers/2507.01352",
            "abstract": "Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality.",
            "score": 31,
            "issue_id": 4638,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "955bbdefa8606d12",
            "authors": [
                "Chris Yuhao Liu",
                "Liang Zeng",
                "Yuzhen Xiao",
                "Jujie He",
                "Jiacai Liu",
                "Chaojie Wang",
                "Rui Yan",
                "Wei Shen",
                "Fuxiang Zhang",
                "Jiacheng Xu",
                "Yang Liu",
                "Yahui Zhou"
            ],
            "affiliations": [
                "2050 Research, Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01352.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#rlhf",
                    "#training",
                    "#alignment",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Синергия человека и ИИ для создания передовых моделей вознаграждения",
                    "desc": "Статья представляет новый набор данных предпочтений SynPref-40M, содержащий 40 миллионов пар предпочтений, созданный с помощью двухэтапного процесса, сочетающего человеческую аннотацию и масштабируемость ИИ. На основе этих данных авторы разработали набор моделей вознаграждения Skywork-Reward-V2 с параметрами от 0.6B до 8B. Модели демонстрируют высокую эффективность в различных задачах, включая соответствие человеческим предпочтениям, объективную корректность и безопасность. Исследование подчеркивает важность качественной курации данных и синергии человека и ИИ для улучшения моделей вознаграждения в обучении с подкреплением на основе обратной связи от человека (RLHF)."
                },
                "en": {
                    "title": "Unlocking Human Preferences with Skywork-Reward-V2",
                    "desc": "This paper addresses the limitations of current reward models (RMs) in reinforcement learning from human feedback (RLHF), which struggle to accurately reflect complex human preferences. The authors propose a new large-scale preference dataset, SynPref-40M, containing 40 million preference pairs, to improve the training of RMs. They introduce a two-stage human-AI curation pipeline that combines human annotation with AI scalability to ensure high-quality data. The resulting Skywork-Reward-V2 models, trained on a refined subset of this dataset, demonstrate superior performance across various benchmarks, showcasing the importance of quality data in enhancing reward model effectiveness."
                },
                "zh": {
                    "title": "提升奖励模型的质量与性能",
                    "desc": "本论文探讨了奖励模型在从人类反馈中进行强化学习的重要性。当前的开放奖励模型在评估基准上表现不佳，无法有效捕捉人类偏好的复杂性。为了解决这一问题，作者提出了一个包含4000万对偏好的大规模数据集SynPref-40M，并设计了一个人机协作的两阶段数据处理流程。通过高质量的数据标注和AI的自动化处理，作者训练了Skywork-Reward-V2系列奖励模型，展示了其在多项基准测试中的优越性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23918",
            "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and\n  Future Frontiers",
            "url": "https://huggingface.co/papers/2506.23918",
            "abstract": "Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental \"semantic gap\" between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as a dynamic mental sketchpad. A similar evolution is now unfolding in AI, marking a fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from a passive input into a dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along a trajectory of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the think with image paradigm and its three-stage framework. (2) We provide a comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. By providing this structured overview, we aim to offer a clear roadmap for future research towards more powerful and human-aligned multimodal AI.",
            "score": 26,
            "issue_id": 4640,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 июня",
                "en": "June 30",
                "zh": "6月30日"
            },
            "hash": "8526b6b1e8d4b31e",
            "authors": [
                "Zhaochen Su",
                "Peng Xia",
                "Hangyu Guo",
                "Zhenhua Liu",
                "Yan Ma",
                "Xiaoye Qu",
                "Jiaqi Liu",
                "Yanshu Li",
                "Kaide Zeng",
                "Zhengyuan Yang",
                "Linjie Li",
                "Yu Cheng",
                "Heng Ji",
                "Junxian He",
                "Yi R. Fung"
            ],
            "affiliations": [
                "Microsoft",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "UIUC",
                "UNC-Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23918.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#alignment",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "От мышления об изображениях к мышлению изображениями: новая эра мультимодального ИИ",
                    "desc": "Эта статья описывает новую парадигму в мультимодальном искусственном интеллекте, где визуальная информация используется как динамический инструмент мышления, а не просто статичный входной контекст. Авторы выделяют три ключевых этапа развития этой парадигмы: от внешнего исследования инструментов через программную манипуляцию к внутреннему воображению. В работе представлен всесторонний обзор методов, характерных для каждого этапа, а также анализ критически важных эталонных тестов и трансформационных приложений. Статья также определяет значительные проблемы и намечает перспективные направления будущих исследований в области мультимодального ИИ."
                },
                "en": {
                    "title": "Transforming Vision into Dynamic Thought in AI",
                    "desc": "This paper discusses the advancements in multimodal reasoning, particularly focusing on the 'think with images' paradigm. It highlights the limitations of traditional text-based reasoning, which treats visual data as static, leading to a disconnect between perception and thought. The authors propose a three-stage framework that evolves from using images as tools to integrating them into cognitive processes, allowing for dynamic manipulation of visual information. The survey aims to provide a structured overview of this emerging field, outlining foundational principles, core methods, evaluation benchmarks, and future research directions."
                },
                "zh": {
                    "title": "从图像思考到思考图像的转变",
                    "desc": "这篇论文探讨了多模态推理的最新进展，特别是文本链式思维（CoT）在语言推理中的应用。作者指出，传统的文本中心方法将视觉视为静态背景，导致感知数据与符号思维之间存在“语义差距”。论文提出了一种新的思维模式，强调视觉信息在思维过程中的动态作用，使其成为可操作的认知工作空间。通过建立三阶段框架，论文为未来的多模态人工智能研究提供了清晰的路线图。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02754",
            "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
            "url": "https://huggingface.co/papers/2507.02754",
            "abstract": "Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.   In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that 2-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.",
            "score": 13,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "a9492490e5a70bc4",
            "authors": [
                "Aurko Roy",
                "Timothy Chou",
                "Sai Surya Duvvuri",
                "Sijia Chen",
                "Jiecao Yu",
                "Xiaodong Wang",
                "Manzil Zaheer",
                "Rohan Anil"
            ],
            "affiliations": [
                "Department of Computer Science University of Texas at Austin",
                "Meta Menlo Park, CA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02754.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#reasoning",
                    "#math",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Повышение эффективности языковых моделей с помощью 2-симплициального внимания",
                    "desc": "В статье исследуется архитектура 2-симплициального трансформера, обобщающая стандартное внимание на основе скалярного произведения до трилинейных функций. Авторы показывают, что эта архитектура достигает лучшей эффективности использования токенов по сравнению со стандартными трансформерами. При фиксированном бюджете токенов модели сопоставимого размера превосходят аналоги со скалярным произведением в задачах математики, программирования, рассуждений и логики. Количественно преимущества выражаются в изменении показателя степени в законах масштабирования для задач, связанных со знаниями и рассуждениями."
                },
                "en": {
                    "title": "Enhancing Token Efficiency with 2-Simplicial Transformers",
                    "desc": "This paper explores the limitations of current scaling laws in machine learning, particularly in the context of large language models that are no longer purely compute-bound due to their reliance on vast datasets. It introduces the 2-simplicial Transformer, a new architecture that enhances standard attention mechanisms by using trilinear functions, which improves token efficiency. The authors show that this new architecture allows models to perform better on various tasks, such as mathematics and reasoning, while using the same number of tokens. By quantifying the improvements, they reveal that the 2-simplicial attention modifies the scaling laws, leading to better performance in knowledge and reasoning tasks compared to traditional dot-product attention."
                },
                "zh": {
                    "title": "提升标记效率的2-单纯形变换器",
                    "desc": "最近的研究表明，训练损失与模型大小和标记数量呈幂律关系，达到计算最优模型需要同时扩大模型大小和标记数量。然而，这些缩放法则假设数据是无限的，并主要适用于计算受限的环境。随着现代大型语言模型越来越依赖于大规模互联网数据集，这种假设变得不再有效。因此，我们需要优先考虑标记效率的架构。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02652",
            "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search",
            "url": "https://huggingface.co/papers/2507.02652",
            "abstract": "Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.",
            "score": 13,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "e833cc483ac9b10c",
            "authors": [
                "Jiajie Jin",
                "Xiaoxi Li",
                "Guanting Dong",
                "Yuyao Zhang",
                "Yutao Zhu",
                "Yang Zhao",
                "Hongjin Qian",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02652.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rag",
                    "#benchmark",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "HiRA: Иерархический подход к сложному информационному поиску",
                    "desc": "Статья представляет HiRA - иерархическую систему для сложных информационных запросов. HiRA разделяет стратегическое планирование и специализированное выполнение задач, что позволяет эффективнее обрабатывать многоэтапные запросы. Система декомпозирует сложные задачи на подзадачи и назначает их специализированным агентам с внешними инструментами. Эксперименты показали, что HiRA превосходит современные RAG-системы и агентные подходы по качеству ответов и эффективности."
                },
                "en": {
                    "title": "HiRA: Enhancing Search Efficiency through Hierarchical Reasoning",
                    "desc": "This paper presents HiRA, a new framework designed to improve complex information retrieval tasks by separating high-level planning from detailed execution. Traditional methods often struggle because they use a single model for both tasks, which can lead to inefficiencies. HiRA addresses this by breaking down complex search tasks into smaller subtasks, each handled by specialized agents that have their own tools and reasoning abilities. The results show that HiRA outperforms existing systems in both the quality of answers and overall efficiency, demonstrating the benefits of this hierarchical approach."
                },
                "zh": {
                    "title": "HiRA：分层框架提升搜索效率与质量",
                    "desc": "在现实世界的搜索场景中，复杂的信息需求需要深度推理和知识综合，而传统的检索增强生成（RAG）管道难以有效应对。当前的推理方法存在一个根本性限制：它们使用单一模型处理高层次规划和详细执行，导致推理效率低下和可扩展性有限。本文提出了HiRA，一个分层框架，将战略规划与专业执行分开。我们的研究表明，HiRA在复杂的跨模态深度搜索基准测试中显著优于现有的RAG和基于代理的系统，提升了答案质量和系统效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02726",
            "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
            "url": "https://huggingface.co/papers/2507.02726",
            "abstract": "Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.",
            "score": 12,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "42e132c4863440b8",
            "authors": [
                "Matthieu Zimmer",
                "Xiaotong Ji",
                "Rasul Tutunov",
                "Anthony Bordg",
                "Jun Wang",
                "Haitham Bou Ammar"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "Imperial College London",
                "Lagrange Center",
                "UCL Centre for AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02726.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#benchmark",
                    "#agents",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самогенерируемые цели улучшают автоматическое доказательство теорем",
                    "desc": "Статья представляет новый подход к автоматическому доказательству теорем с использованием больших языковых моделей (LLM). Авторы предлагают фреймворк sG-MDP, в котором агенты генерируют и преследуют подцели на основе текущего состояния доказательства. Они применяют алгоритмы, подобные Monte Carlo Tree Search, для решения sG-MDP. Результатом является система Bourbaki (7B), которая достигает новых рекордных результатов на бенчмарке PutnamBench для моделей своего масштаба."
                },
                "en": {
                    "title": "Empowering Reasoning with Self-Generated Goals in Theorem Proving",
                    "desc": "This paper addresses the difficulties that large language models (LLMs) face in reasoning tasks, particularly in automated theorem proving (ATP) where rewards are sparse and proofs are complex. The authors propose a novel framework called self-generated goal-conditioned MDPs (sG-MDPs), which allows agents to create and pursue subgoals based on the current state of the proof. By structuring goal generation, the problem becomes easier to navigate and search. The framework is implemented in a system called Bourbaki (7B), which utilizes multiple LLMs to enhance subgoal generation and tactic synthesis, achieving state-of-the-art results on the challenging PutnamBench benchmark."
                },
                "zh": {
                    "title": "自生成目标助力推理挑战",
                    "desc": "本文探讨了大型语言模型（LLMs）在自动定理证明（ATP）中的推理挑战，尤其是在稀疏奖励和证明规模庞大的情况下。为了解决这些问题，提出了一种新的框架——自生成目标条件马尔可夫决策过程（sG-MDPs），使得智能体能够根据不断变化的证明状态生成和追求子目标。通过这种结构化的目标生成，问题变得更易于搜索。最后，应用类似蒙特卡洛树搜索（MCTS）的算法解决sG-MDP，并在Bourbaki（7B）系统中实现，成功在PutnamBench上解决了26个问题，创造了新的最先进结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02694",
            "title": "Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers",
            "url": "https://huggingface.co/papers/2507.02694",
            "abstract": "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.",
            "score": 9,
            "issue_id": 4641,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "d7b392be540c08ba",
            "authors": [
                "Zhijian Xu",
                "Yilun Zhao",
                "Manasi Patwardhan",
                "Lovekesh Vig",
                "Arman Cohan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.02694.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rag",
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#science"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "ИИ на страже научной объективности: LLM как помощник в рецензировании",
                    "desc": "Статья посвящена использованию больших языковых моделей (LLM) для помощи в процессе рецензирования научных работ, особенно в выявлении ограничений исследований. Авторы представляют таксономию типов ограничений в научных исследованиях и создают бенчмарк LimitGen для оценки способности LLM поддерживать ранние этапы обратной связи. LimitGen включает синтетический набор данных и реальные ограничения, написанные людьми. Исследователи улучшают способность LLM выявлять ограничения, дополняя их поиском по научной литературе."
                },
                "en": {
                    "title": "Empowering Peer Review with AI: LimitGen for Identifying Research Limitations",
                    "desc": "This paper addresses the challenges of peer review in scientific research due to the increasing number of publications. It introduces a taxonomy of limitation types specifically for AI research, which helps in understanding the weaknesses of scientific papers. The authors present LimitGen, a benchmark designed to evaluate how well large language models (LLMs) can assist in identifying these limitations and provide feedback. By incorporating literature retrieval, the study enhances LLMs' ability to generate relevant and constructive critiques of research papers."
                },
                "zh": {
                    "title": "提升同行评审的智能化支持",
                    "desc": "同行评审是科学研究的重要环节，但随着出版物数量的增加，这一过程面临越来越大的挑战。本文提出了一种全面的科学研究局限性分类法，特别关注人工智能领域。我们介绍了LimitGen，这是第一个评估大型语言模型（LLM）在支持早期反馈和补充人类评审方面能力的基准测试。通过结合文献检索，我们增强了LLM系统识别研究局限性的能力，从而能够提供更具体和建设性的反馈。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02778",
            "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs",
            "url": "https://huggingface.co/papers/2507.02778",
            "abstract": "Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending \"Wait\" notably improves their ability to correct errors in their outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.",
            "score": 6,
            "issue_id": 4645,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "4b54d8d384329494",
            "authors": [
                "Ken Tsui"
            ],
            "affiliations": [
                "Independent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02778.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#alignment",
                    "#benchmark",
                    "#hallucinations"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Самокоррекция: ключ к надежности LLM",
                    "desc": "Исследование выявляет, что у больших языковых моделей (LLM) есть проблема с самокоррекцией, так как они обучаются на данных без ошибок. Это приводит к тому, что модели не могут исправлять собственные ошибки, хотя и могут находить ошибки в пользовательском вводе. Введение фразы \"Wait\" помогает моделям активировать способность к самокоррекции, что значительно уменьшает количество ошибок. Работа подчеркивает важность улучшения надежности LLM через обучение на данных с ошибками и их исправлениями."
                },
                "en": {
                    "title": "Unlocking Self-Correction in Language Models",
                    "desc": "This paper introduces Self-Correction Bench, a framework designed to measure the self-correction capabilities of large language models (LLMs). It identifies a significant issue known as the 'Self-Correction Blind Spot', where LLMs fail to correct errors in their own outputs despite being able to recognize errors in user inputs. The study reveals that this blind spot is largely due to the training data, which often consists of error-free examples rather than sequences that include error corrections. Notably, the simple addition of the word 'Wait' to prompts can significantly enhance the models' self-correction abilities, indicating that these capabilities can be activated with the right cues."
                },
                "zh": {
                    "title": "激活自我纠正，提升语言模型的可靠性",
                    "desc": "这篇论文介绍了自我纠正基准（Self-Correction Bench），用于测量大型语言模型（LLM）在自我纠正方面的盲点。研究发现，主要在无错误的响应上进行训练会导致模型在自身输出中无法纠正相同的错误。通过对14个模型进行测试，发现平均有64.5%的盲点率。简单地在输出中添加“等待”一词可以将盲点减少89.3%，这表明模型具备自我纠正的能力，但需要激活。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02092",
            "title": "Energy-Based Transformers are Scalable Learners and Thinkers",
            "url": "https://huggingface.co/papers/2507.02092",
            "abstract": "Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question \"Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?\" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.",
            "score": 6,
            "issue_id": 4642,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "9f33fd27885f443d",
            "authors": [
                "Alexi Gladstone",
                "Ganesh Nanduru",
                "Md Mofijul Islam",
                "Peixuan Han",
                "Hyeonjeong Ha",
                "Aman Chadha",
                "Yilun Du",
                "Heng Ji",
                "Jundong Li",
                "Tariq Iqbal"
            ],
            "affiliations": [
                "Amazon GenAI",
                "Harvard University",
                "Stanford University",
                "UIUC",
                "UVA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02092.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "EBTs: Обучение мышлению через неконтролируемое обучение",
                    "desc": "Эта статья представляет новый класс моделей - Energy-Based Transformers (EBTs), которые обучаются проверять совместимость входных данных и предсказаний. EBTs переформулируют задачу предсказания как оптимизацию относительно верификатора, что позволяет им применять методы мышления 'Системы 2' без дополнительного обучения. Исследования показывают, что EBTs масштабируются быстрее, чем стандартные трансформеры, и демонстрируют лучшую производительность на задачах обработки текста и изображений. Авторы утверждают, что EBTs представляют собой многообещающую новую парадигму для масштабирования как обучающих, так и мыслительных способностей моделей."
                },
                "en": {
                    "title": "Energy-Based Transformers: Scaling Learning and Thinking in AI",
                    "desc": "This paper introduces Energy-Based Transformers (EBTs), a new type of model that enhances inference-time computation by mimicking human System 2 Thinking. EBTs learn to verify the compatibility between inputs and predictions without needing additional supervision, making them more generalizable across different modalities and tasks. The authors demonstrate that EBTs can scale faster than existing models like Transformer++ and achieve superior performance in both language and image tasks. Overall, EBTs represent a significant advancement in the efficiency and effectiveness of machine learning models."
                },
                "zh": {
                    "title": "能量基础变换器：无监督学习的新思维方式",
                    "desc": "本文探讨了一种新的模型——能量基础变换器（EBTs），旨在通过无监督学习来实现更好的推理能力。EBTs通过显式验证输入与候选预测之间的兼容性，将预测问题重新框架为优化问题，从而提高模型的性能。研究表明，EBTs在训练过程中比传统的Transformer++方法具有更快的扩展速度，并在推理时在语言任务上提高了29%的性能。总体而言，EBTs在大多数下游任务中表现优于现有模型，显示出更好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01663",
            "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training",
            "url": "https://huggingface.co/papers/2507.01663",
            "abstract": "Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the corresponding resource idling and workload imbalance. Moreover, most existing frameworks are tightly coupled with LLM training or inference engines, making it difficult to support custom-designed engines. To address these challenges, we propose AsyncFlow, an asynchronous streaming RL framework for efficient post-training. Specifically, we introduce a distributed data storage and transfer module that provides a unified data management and fine-grained scheduling capability in a fully streamed manner. This architecture inherently facilitates automated pipeline overlapping among RL tasks and dynamic load balancing. Moreover, we propose a producer-consumer-based asynchronous workflow engineered to minimize computational idleness by strategically deferring parameter update process within staleness thresholds. Finally, the core capability of AsynFlow is architecturally decoupled from underlying training and inference engines and encapsulated by service-oriented user interfaces, offering a modular and customizable user experience. Extensive experiments demonstrate an average of 1.59 throughput improvement compared with state-of-the-art baseline. The presented architecture in this work provides actionable insights for next-generation RL training system designs.",
            "score": 4,
            "issue_id": 4639,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "8a3f43a4a9e735d7",
            "authors": [
                "Zhenyu Han",
                "Ansheng You",
                "Haibo Wang",
                "Kui Luo",
                "Guang Yang",
                "Wenqi Shi",
                "Menglong Chen",
                "Sicheng Zhang",
                "Zeshun Lan",
                "Chunshi Deng",
                "Huazhong Ji",
                "Wenjie Liu",
                "Yu Huang",
                "Yixiang Zhang",
                "Chenyi Pan",
                "Jing Wang",
                "Xin Huang",
                "Chunsheng Li",
                "Jianping Wu"
            ],
            "affiliations": [
                "Huawei"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01663.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "AsyncFlow: Асинхронное обучение с подкреплением для больших языковых моделей",
                    "desc": "AsyncFlow - это новая асинхронная потоковая система обучения с подкреплением для эффективной пост-обработки больших языковых моделей. Она вводит распределенный модуль хранения и передачи данных, обеспечивающий унифицированное управление данными и детальное планирование в потоковом режиме. AsyncFlow использует асинхронный рабочий процесс на основе модели производитель-потребитель для минимизации простоев вычислений. Система показывает среднее улучшение пропускной способности в 1,59 раза по сравнению с современными аналогами."
                },
                "en": {
                    "title": "AsyncFlow: Revolutionizing RL for Large Language Models",
                    "desc": "This paper introduces AsyncFlow, a new framework for reinforcement learning (RL) that enhances the post-training phase of large language models (LLMs). It addresses scalability issues found in traditional RL frameworks by implementing a distributed data storage and transfer system, which allows for efficient data management and scheduling. The framework also features an asynchronous workflow that reduces idle computation time by optimizing the timing of parameter updates. Overall, AsyncFlow is designed to be modular and customizable, making it easier to integrate with various training and inference engines while improving throughput significantly."
                },
                "zh": {
                    "title": "AsyncFlow：高效的异步流式强化学习框架",
                    "desc": "强化学习（RL）在大型语言模型（LLM）的后训练阶段变得至关重要。传统的任务共存RL框架面临可扩展性瓶颈，而任务分离的RL框架在复杂数据流和资源闲置方面存在挑战。为了解决这些问题，我们提出了AsyncFlow，一个高效的异步流式RL框架，能够实现自动化的管道重叠和动态负载平衡。我们的实验表明，与最先进的基线相比，AsyncFlow在吞吐量上平均提高了1.59倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01004",
            "title": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention",
            "url": "https://huggingface.co/papers/2507.01004",
            "abstract": "A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.",
            "score": 4,
            "issue_id": 4645,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 июля",
                "en": "July 1",
                "zh": "7月1日"
            },
            "hash": "c104abc218e38a97",
            "authors": [
                "Yuhong Chou",
                "Zehao Liu",
                "Ruijie Zhu",
                "Xinyi Wan",
                "Tianjian Li",
                "Congying Chu",
                "Qian Liu",
                "Jibin Wu",
                "Zejun Ma"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "National University of Singapore",
                "The Hong Kong Polytechnic University",
                "TikTok",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01004.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "ZeCO: Революция в обучении языковых моделей с ультрадлинными последовательностями",
                    "desc": "ZeCO - это новый метод параллелизма последовательностей с нулевыми накладными расходами на коммуникацию, разработанный для эффективного обучения больших языковых моделей с ультрадлинными последовательностями на нескольких устройствах. В основе ZeCO лежит новый примитив коллективной коммуникации All-Scan, который обеспечивает каждому рангу SP точное начальное состояние оператора при минимальных затратах на коммуникацию. Теоретически доказана оптимальность ZeCO, показывающая, что он вносит лишь незначительные временные и пространственные накладные расходы. Эмпирически продемонстрировано, что на 256 GPU с длиной последовательности 8M ZeCO достигает ускорения на 60% по сравнению с современными методами SP."
                },
                "en": {
                    "title": "ZeCO: Revolutionizing Long Sequence Training with Zero Communication Overhead",
                    "desc": "This paper presents ZeCO, a novel sequence parallelism method that eliminates communication overhead during the training of large language models (LLMs) with ultra-long sequences. By utilizing linear attention mechanisms, ZeCO allows for efficient processing of sequences up to 1 million tokens across multiple devices without the typical bottlenecks caused by communication delays. The core innovation, All-Scan, enables each device to access the necessary operator state with minimal communication, resulting in near-linear scalability for long sequence training. Empirical results show that ZeCO significantly outperforms existing methods, achieving a 60% speedup on 256 GPUs with an 8M sequence length, paving the way for training next-generation LLMs."
                },
                "zh": {
                    "title": "ZeCO：高效训练超长序列的大型语言模型",
                    "desc": "本文介绍了一种新的零通信开销序列并行方法ZeCO，旨在高效训练具有超长序列的大型语言模型。ZeCO通过引入All-Scan这一新的集体通信原语，显著减少了设备间的通信开销，从而实现了接近线性的可扩展性。与传统的序列并行方法相比，ZeCO在多个设备上处理1M序列时的训练时间与在单个设备上处理16k序列的时间相当。实验结果表明，ZeCO在256个GPU上处理8M序列时，相较于现有的最佳序列并行方法实现了60%的速度提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22813",
            "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity\n  Recognition with Large Language Models",
            "url": "https://huggingface.co/papers/2506.22813",
            "abstract": "Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework.",
            "score": 4,
            "issue_id": 4644,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 июня",
                "en": "June 28",
                "zh": "6月28日"
            },
            "hash": "762c8f77cac2babf",
            "authors": [
                "Zhuojun Ding",
                "Wei Wei",
                "Chenghao Fan"
            ],
            "affiliations": [
                "School of Computer Science & Technology, Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22813.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#transfer_learning",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Динамическое объединение экспертов для адаптивного извлечения информации",
                    "desc": "Статья представляет новый подход SaM для адаптации моделей извлечения информации к различным предметным областям. Вместо обучения единой модели, SaM динамически выбирает и объединяет предобученные экспертные модели на этапе вывода. Этот метод улучшает обобщение на новые домены без дополнительного обучения и обеспечивает масштабируемость. Эксперименты показывают, что SaM превосходит единую модель в среднем на 10% по различным бенчмаркам."
                },
                "en": {
                    "title": "Dynamic Expert Selection for Enhanced Domain Adaptation",
                    "desc": "The paper introduces the SaM framework, which enhances the performance of large language models in information extraction tasks by dynamically selecting and merging expert models tailored to specific domains. Instead of training a single model for all domains, SaM identifies domain-specific experts based on their relevance and performance, allowing for better adaptation to target tasks. This approach not only improves generalization across various domains but also offers scalability by enabling the addition or removal of experts without retraining. Experimental results show that the SaM framework outperforms traditional unified models by an average of 10%, highlighting its effectiveness in optimizing task-specific performance."
                },
                "zh": {
                    "title": "动态选择与合并专家模型，提升跨领域性能",
                    "desc": "监督微调（SFT）广泛应用于将大型语言模型（LLM）与信息提取（IE）任务对齐，例如命名实体识别（NER）。然而，标注这些细粒度标签和训练特定领域的模型成本高昂。现有方法通常在多个领域训练统一模型，但这种方法缺乏适应性和可扩展性，因为并非所有训练数据都能惠及目标领域。我们提出了SaM框架，在推理时动态选择和合并专家模型，从而提高了跨领域的泛化能力，而无需额外训练。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23121",
            "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for\n  Multi-Organ Segmentation",
            "url": "https://huggingface.co/papers/2506.23121",
            "abstract": "Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP\\_SAM2.git.",
            "score": 2,
            "issue_id": 4660,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 июня",
                "en": "June 29",
                "zh": "6月29日"
            },
            "hash": "925d84357946a3a1",
            "authors": [
                "Xinlei Yu",
                "Chanmiao Wang",
                "Hui Jin",
                "Ahmed Elazab",
                "Gangyong Jia",
                "Xiang Wan",
                "Changqing Zou",
                "Ruiquan Ge"
            ],
            "affiliations": [
                "Hangzhou Dianzi University, Hangzhou, China",
                "Shenzhen Research Institute of Big Data, Shenzhen, China",
                "Shenzhen University, Shenzhen, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23121.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#healthcare"
                ],
                "emoji": "🩻",
                "ru": {
                    "title": "CRISP-SAM2: Революция в сегментации медицинских изображений с помощью кросс-модального ИИ",
                    "desc": "Статья представляет новую модель CRISP-SAM2 для сегментации нескольких органов на медицинских изображениях. Модель использует кросс-модальное взаимодействие и семантические подсказки на основе SAM2, что позволяет улучшить детализацию и устранить зависимость от геометрических подсказок. CRISP-SAM2 применяет механизм прогрессивного кросс-внимания для объединения визуальной и текстовой информации, а также стратегию семантических подсказок для улучшения распознавания сложных целей. Эксперименты на семи публичных наборах данных показали превосходство CRISP-SAM2 над существующими моделями."
                },
                "en": {
                    "title": "Enhancing Medical Image Segmentation with CRISP-SAM2",
                    "desc": "This paper presents CRISP-SAM2, a new model for multi-organ medical segmentation that improves upon existing methods by addressing common issues like detail accuracy and reliance on geometric prompts. The model utilizes a cross-modal interaction mechanism to integrate visual and textual data, enhancing the understanding of medical images. By implementing a semantic prompting strategy, CRISP-SAM2 reduces dependency on geometric cues, allowing for better identification of complex organ structures. Experimental results show that this model significantly outperforms previous approaches across multiple datasets, confirming its effectiveness in medical image processing."
                },
                "zh": {
                    "title": "CRISP-SAM2：提升多脏器医学分割的创新模型",
                    "desc": "多脏器医学分割是医学图像处理中的重要环节，帮助医生进行准确诊断和制定有效治疗方案。尽管该领域已有显著进展，但现有的多脏器分割模型常常面临细节不准确、依赖几何提示和空间信息丢失等问题。为了解决这些挑战，我们提出了一种新模型CRISP-SAM2，基于SAM2的跨模态交互和语义提示。该模型通过逐步的跨注意力交互机制，将视觉和文本输入转换为跨模态上下文语义，从而增强对视觉信息的细致理解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21546",
            "title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation\n  Hallucination Evaluation",
            "url": "https://huggingface.co/papers/2506.21546",
            "abstract": "HalluSegBench provides a benchmark for evaluating hallucinations in vision-language segmentation models by analyzing counterfactual scene edits.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in vision-language segmentation has significantly advanced grounded visual understanding. However, these models often exhibit hallucinations by producing segmentation masks for objects not grounded in the image content or by incorrectly labeling irrelevant regions. Existing evaluation protocols for segmentation hallucination primarily focus on label or textual hallucinations without manipulating the visual context, limiting their capacity to diagnose critical failures. In response, we introduce HalluSegBench, the first benchmark specifically designed to evaluate hallucinations in visual grounding through the lens of counterfactual visual reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual instance pairs spanning 281 unique object classes, and a set of newly introduced metrics that quantify hallucination sensitivity under visually coherent scene edits. Experiments on HalluSegBench with state-of-the-art vision-language segmentation models reveal that vision-driven hallucinations are significantly more prevalent than label-driven ones, with models often persisting in false segmentation, highlighting the need for counterfactual reasoning to diagnose grounding fidelity.",
            "score": 1,
            "issue_id": 4655,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 июня",
                "en": "June 26",
                "zh": "6月26日"
            },
            "hash": "d577c3d7e6c15a10",
            "authors": [
                "Xinzhuo Li",
                "Adheesh Juvekar",
                "Xingyou Liu",
                "Muntasir Wahed",
                "Kiet A. Nguyen",
                "Ismini Lourentzou"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21546.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#hallucinations",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Раскрываем галлюцинации ИИ через контрфактический анализ",
                    "desc": "HalluSegBench - это новый бенчмарк для оценки галлюцинаций в моделях сегментации изображений с использованием языка. Он включает набор данных из 1340 пар контрфактических изображений и новые метрики для количественной оценки чувствительности к галлюцинациям при визуально согласованном редактировании сцен. Эксперименты показали, что галлюцинации, вызванные визуальной составляющей, более распространены, чем вызванные текстовыми метками. Бенчмарк подчеркивает необходимость контрфактического рассуждения для диагностики точности привязки моделей к изображениям."
                },
                "en": {
                    "title": "Evaluating Hallucinations in Vision-Language Models with Counterfactuals",
                    "desc": "HalluSegBench is a new benchmark designed to evaluate hallucinations in vision-language segmentation models by using counterfactual scene edits. It addresses the limitations of existing evaluation methods that mainly focus on label or textual hallucinations without altering the visual context. The benchmark includes a dataset of 1340 counterfactual instance pairs across 281 object classes and introduces metrics to measure hallucination sensitivity. Experiments show that vision-driven hallucinations are more common than label-driven ones, emphasizing the importance of counterfactual reasoning for assessing model accuracy in visual grounding."
                },
                "zh": {
                    "title": "HalluSegBench：评估视觉幻觉的新基准",
                    "desc": "HalluSegBench是一个用于评估视觉-语言分割模型中幻觉现象的基准。该基准通过分析反事实场景编辑，帮助识别模型在图像内容中未正确定位的对象。研究发现，现有的评估方法主要关注标签或文本幻觉，而忽视了视觉上下文的变化。HalluSegBench提供了1340对反事实实例和新的评估指标，揭示了视觉驱动的幻觉现象比标签驱动的更为普遍，强调了反事实推理在诊断模型准确性中的重要性。"
                }
            }
        }
    ],
    "link_prev": "2025-07-03.html",
    "link_next": "2025-07-07.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "03.07",
        "en": "07/03",
        "zh": "7月3日"
    },
    "short_date_next": {
        "ru": "07.07",
        "en": "07/07",
        "zh": "7月7日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 2,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 8,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}