{
    "date": {
        "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 29",
        "zh": "9æœˆ29æ—¥"
    },
    "time_utc": "2025-09-29 04:14",
    "weekday": 0,
    "issue_id": 6131,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.22622",
            "title": "LongLive: Real-time Interactive Long Video Generation",
            "url": "https://huggingface.co/papers/2509.22622",
            "abstract": "LongLive is a frame-level autoregressive framework for real-time and interactive long video generation, addressing efficiency and quality challenges through causal attention, KV-recache, streaming long tuning, and short window attention.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.",
            "score": 65,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "ced1fe458c87fa25",
            "authors": [
                "Shuai Yang",
                "Wei Huang",
                "Ruihang Chu",
                "Yicheng Xiao",
                "Yuyang Zhao",
                "Xianbang Wang",
                "Muyang Li",
                "Enze Xie",
                "Yingcong Chen",
                "Yao Lu",
                "Song Han",
                "Yukang Chen"
            ],
            "affiliations": [
                "HKU",
                "HKUST(GZ)",
                "MIT",
                "NVIDIA",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22622.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#inference",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "LongLive Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ KV-recache Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµĞ½Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğµ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ frame sink Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. LongLive Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 20.7 FPS Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ NVIDIA H100 Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¾ 240 ÑĞµĞºÑƒĞ½Ğ´ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Real-Time Long Video Generation with LongLive",
                    "desc": "LongLive is a novel autoregressive framework designed for generating long videos in real-time, addressing both efficiency and quality issues. It utilizes causal attention and a KV-recache mechanism to enhance inference speed while maintaining high visual quality. The framework also incorporates streaming long tuning to align training with inference, allowing for interactive content creation where users can influence the narrative dynamically. With its innovative design, LongLive can generate videos up to 240 seconds long at a high frame rate, demonstrating significant advancements in long video generation capabilities."
                },
                "zh": {
                    "title": "å®æ—¶äº’åŠ¨é•¿è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "LongLiveæ˜¯ä¸€ä¸ªç”¨äºå®æ—¶å’Œäº’åŠ¨é•¿è§†é¢‘ç”Ÿæˆçš„å¸§çº§è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ•ˆç‡å’Œè´¨é‡çš„æŒ‘æˆ˜ã€‚å®ƒé‡‡ç”¨å› æœæ³¨æ„åŠ›æœºåˆ¶å’ŒKVç¼“å­˜æŠ€æœ¯ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒè§†é¢‘è´¨é‡ã€‚é€šè¿‡æµå¼é•¿è°ƒä¼˜å’ŒçŸ­çª—å£æ³¨æ„åŠ›ï¼ŒLongLiveèƒ½å¤Ÿåœ¨è®­ç»ƒå’Œæ¨ç†ä¸­å®ç°ä¸€è‡´æ€§ï¼Œæ”¯æŒç”¨æˆ·å®æ—¶å¼•å¯¼å†…å®¹åˆ›ä½œã€‚è¯¥æ¡†æ¶åœ¨å•ä¸ªNVIDIA H100ä¸Šå®ç°äº†æ¯ç§’20.7å¸§çš„æ¨ç†é€Ÿåº¦ï¼Œèƒ½å¤Ÿç”Ÿæˆæœ€é•¿240ç§’çš„è§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22576",
            "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.22576",
            "abstract": "Entropy-regularized Policy Optimization (EPO) addresses exploration-exploitation challenges in multi-turn environments with sparse rewards, improving performance in tasks like ScienceWorld and ALFWorld.  \t\t\t\t\tAI-generated summary \t\t\t\t Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.",
            "score": 45,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "9f6eee9878b5d876",
            "authors": [
                "Xu Wujiang",
                "Wentian Zhao",
                "Zhenting Wang",
                "Li Yu-Jhe",
                "Jin Can",
                "Jin Mingyu",
                "Mei Kai",
                "Wan Kun",
                "Metaxas Dimitris"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22576.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#games",
                    "#optimization"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ£ĞºÑ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°ÑĞºĞ°Ğ´Ğ° Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸, Ğ³Ğ´Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ 30+ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° - ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ-ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğº Ğ¿Ğ»Ğ¾Ñ…Ğ¸Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ñ…Ğ°Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ EPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°: ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ· Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸. EPO Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 152% Ğ½Ğ° ScienceWorld Ğ¸ Ğ´Ğ¾ 19.8% Ğ½Ğ° ALFWorld."
                },
                "en": {
                    "title": "Revolutionizing Exploration in Sparse Reward Environments with EPO",
                    "desc": "Entropy-regularized Policy Optimization (EPO) is a new approach designed to tackle the exploration-exploitation dilemma in reinforcement learning, especially in environments where rewards are sparse and tasks require many interactions. The paper identifies a unique failure mode in these settings, where agents prematurely settle on poor strategies due to limited feedback, leading to a collapse in their learning process. EPO introduces three key mechanisms: enhanced entropy regularization for better exploration, a smoothing regularizer to stabilize policy changes, and adaptive weighting to balance exploration and exploitation effectively. This framework has shown significant performance improvements in complex tasks, highlighting the need for tailored entropy management in multi-turn environments."
                },
                "zh": {
                    "title": "ç†µæ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¤šå›åˆå­¦ä¹ æ€§èƒ½çš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç†µæ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤šå›åˆç¯å¢ƒä¸­ç¨€ç–å¥–åŠ±ä¸‹çš„æ¢ç´¢ä¸åˆ©ç”¨æŒ‘æˆ˜ã€‚EPOé€šè¿‡ä¸‰ç§æœºåˆ¶æ‰“ç ´äº†æ¢ç´¢-åˆ©ç”¨çš„å¤±è´¥å¾ªç¯ï¼Œå¢å¼ºäº†ç­–ç•¥çš„æ¢ç´¢èƒ½åŠ›ï¼Œå¹¶é˜²æ­¢äº†ç­–ç•¥ç†µçš„å‰§çƒˆæ³¢åŠ¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒEPOåœ¨ScienceWorldå’ŒALFWorldä»»åŠ¡ä¸­åˆ†åˆ«æé«˜äº†152%å’Œ19.8%çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•å¼ºè°ƒäº†åœ¨å¤šå›åˆç¨€ç–å¥–åŠ±è®¾ç½®ä¸­ï¼Œç†µæ§åˆ¶éœ€è¦ä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æœ‰æ ¹æœ¬æ€§çš„ä¸åŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22611",
            "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
            "url": "https://huggingface.co/papers/2509.22611",
            "abstract": "Quantile Advantage Estimation stabilizes reinforcement learning with verifiable rewards by addressing entropy issues and improving performance on large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often oscillates between {entropy collapse} and {entropy explosion}. We trace both hazards to the mean baseline used in value-free RL (e.g., GRPO and DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile baseline. QAE induces a response-level, two-regime gate: on hard queries (p <= 1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it targets remaining failures. Under first-order softmax updates, we prove {two-sided entropy safety}, giving lower and upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results identify {baseline design} -- rather than token-level heuristics -- as the primary mechanism for scaling RLVR.",
            "score": 43,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "44c9fe6a9098712d",
            "authors": [
                "Junkang Wu",
                "Kexin Huang",
                "Jiancan Wu",
                "An Zhang",
                "Xiang Wang",
                "Xiangnan He"
            ],
            "affiliations": [
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22611.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ñ‹Ğ¹ baseline",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. ĞĞ½Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ baseline Ğ½Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² (QAE), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¸ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ° ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ: ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ´ĞºĞ¸Ğµ ÑƒÑĞ¿ĞµÑ…Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Stabilizing Reinforcement Learning with Quantile Advantage Estimation",
                    "desc": "This paper introduces Quantile Advantage Estimation (QAE) to enhance reinforcement learning with verifiable rewards, particularly in large language models. It addresses the instability caused by traditional mean baselines, which can lead to entropy collapse or explosion during training. By using a K-quantile baseline, QAE effectively manages the reward distribution, reinforcing rare successes on difficult queries while focusing on failures in easier ones. The proposed method stabilizes entropy and improves performance, demonstrating that baseline design is crucial for scaling reinforcement learning with verifiable rewards."
                },
                "zh": {
                    "title": "é‡åŒ–ä¼˜åŠ¿ä¼°è®¡ï¼šç¨³å®šå¼ºåŒ–å­¦ä¹ çš„æ–°æ–¹æ³•",
                    "desc": "é‡åŒ–ä¼˜åŠ¿ä¼°è®¡ï¼ˆQAEï¼‰é€šè¿‡è§£å†³ç†µé—®é¢˜ï¼Œç¨³å®šäº†å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ï¼Œå¹¶æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ç”¨K-åˆ†ä½æ•°åŸºçº¿æ›¿ä»£äº†ä¼ ç»Ÿçš„å‡å€¼åŸºçº¿ï¼Œé¿å…äº†åœ¨å¥–åŠ±å¼‚å¸¸å€¼ä¸‹å¯¹è´Ÿä¼˜åŠ¿æ ·æœ¬çš„ä¸å½“æƒ©ç½šã€‚QAEåœ¨å¤„ç†å›°éš¾æŸ¥è¯¢æ—¶å¼ºåŒ–ç¨€æœ‰æˆåŠŸï¼Œè€Œåœ¨ç®€å•æŸ¥è¯¢æ—¶åˆ™é’ˆå¯¹å‰©ä½™å¤±è´¥ï¼Œä»è€Œå®ç°äº†åŒå‘ç†µå®‰å…¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æœ€å°ä¿®æ”¹æœ‰æ•ˆç¨³å®šäº†ç†µï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22637",
            "title": "Variational Reasoning for Language Models",
            "url": "https://huggingface.co/papers/2509.22637",
            "abstract": "A variational reasoning framework treats thinking traces as latent variables, optimizing them through variational inference to improve language model reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning.",
            "score": 26,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "f8f5b5200a8c3ac8",
            "authors": [
                "Xiangxin Zhou",
                "Zichen Liu",
                "Haonan Wang",
                "Chao Du",
                "Min Lin",
                "Chongxuan Li",
                "Liang Wang",
                "Tianyu Pang"
            ],
            "affiliations": [
                "CASIA",
                "NUS",
                "RUC",
                "Sea AI Lab",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22637.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ RL Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ evidence lower bound (ELBO) Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ ĞµĞ³Ğ¾ Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ñ€Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ rejection sampling fine-tuning Ğ¸ reinforcement learning Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ forward-KL Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Qwen 2.5 Ğ¸ Qwen 3 Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Language Model Reasoning with Variational Inference",
                    "desc": "This paper presents a variational reasoning framework that enhances the reasoning capabilities of language models by treating thinking traces as latent variables. It optimizes these traces using variational inference, starting from the evidence lower bound (ELBO) and extending it to a multi-trace objective for improved performance. The authors introduce a forward-KL formulation to stabilize training and demonstrate that techniques like rejection sampling finetuning and binary-reward reinforcement learning can be viewed as local forward-KL objectives. The framework is empirically validated on the Qwen 2.5 and Qwen 3 models, showing its effectiveness across various reasoning tasks."
                },
                "zh": {
                    "title": "å˜åˆ†æ¨ç†æ¡†æ¶æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å˜åˆ†æ¨ç†æ¡†æ¶ï¼Œç”¨äºè¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œå°†æ€ç»´è½¨è¿¹è§†ä¸ºæ½œåœ¨å˜é‡ï¼Œå¹¶é€šè¿‡å˜åˆ†æ¨ç†è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬ä»è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰å‡ºå‘ï¼Œæ‰©å±•åˆ°å¤šè½¨è¿¹ç›®æ ‡ï¼Œä»¥è·å¾—æ›´ç´§çš„ç•Œé™ï¼Œå¹¶æå‡ºäº†ä¸€ç§å‰å‘KLå…¬å¼ï¼Œä»¥ç¨³å®šå˜åˆ†åéªŒçš„è®­ç»ƒã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ‹’ç»é‡‡æ ·å¾®è°ƒå’ŒäºŒå…ƒå¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚GRPOï¼‰å¯ä»¥è¢«è§£é‡Šä¸ºå±€éƒ¨å‰å‘KLç›®æ ‡ï¼Œå…¶ä¸­æ¨¡å‹å‡†ç¡®åº¦çš„éšå¼åŠ æƒè‡ªç„¶åœ°ä»æ¨å¯¼ä¸­äº§ç”Ÿï¼Œå¹¶æ­ç¤ºäº†å¯¹ç®€å•é—®é¢˜çš„åè§ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨Qwen 2.5å’ŒQwen 3æ¨¡å‹ç³»åˆ—ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæ¶µç›–äº†å¹¿æ³›çš„æ¨ç†ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22638",
            "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
            "url": "https://huggingface.co/papers/2509.22638",
            "abstract": "Feedback-conditional policy (FCP) enables LLMs to learn from verbal feedback by treating it as a conditioning signal, improving expressiveness over scalar rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.",
            "score": 24,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "b97a5b11be70ca26",
            "authors": [
                "Renjie Luo",
                "Zichen Liu",
                "Xiangyan Liu",
                "Chao Du",
                "Min Lin",
                "Wenhu Chen",
                "Wei Lu",
                "Tianyu Pang"
            ],
            "affiliations": [
                "NTU",
                "NUS",
                "SUTD",
                "Sea AI Lab",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22638.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rl",
                    "#optimization",
                    "#rlhf"
                ],
                "emoji": "ğŸ’¬",
                "ru": {
                    "title": "ĞÑ‚ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğº Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ĞœĞµÑ‚Ğ¾Ğ´ Feedback-Conditional Policy (FCP) Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²ÑÑ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚-Ğ¾Ñ‚Ğ·Ñ‹Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²ĞµĞ¶ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹."
                },
                "en": {
                    "title": "Learning from Words: Enhancing LLMs with Feedback-Conditional Policy",
                    "desc": "The paper introduces Feedback-Conditional Policy (FCP), a method that allows large language models (LLMs) to learn from verbal feedback instead of relying solely on scalar rewards. This approach treats verbal feedback as a conditioning signal, which preserves the richness of the feedback and avoids the scale imbalance often seen in traditional reinforcement learning methods. FCP utilizes response-feedback pairs to train the model through maximum likelihood, enhancing its ability to generate responses based on nuanced feedback. Additionally, an online bootstrapping stage is implemented to continuously refine the model by generating responses under positive conditions and incorporating new feedback."
                },
                "zh": {
                    "title": "åé¦ˆæ¡ä»¶ç­–ç•¥ï¼šè®©æ¨¡å‹æ›´å¥½åœ°ç†è§£å£å¤´åé¦ˆ",
                    "desc": "åé¦ˆæ¡ä»¶ç­–ç•¥ï¼ˆFCPï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡å°†å£å¤´åé¦ˆè§†ä¸ºæ¡ä»¶ä¿¡å·æ¥å­¦ä¹ ï¼Œä»è€Œæé«˜äº†è¡¨è¾¾èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸å°†ç»†è‡´çš„åé¦ˆå‹ç¼©ä¸ºæ ‡é‡å¥–åŠ±ï¼Œå¯¼è‡´ä¿¡æ¯çš„ä¸¢å¤±å’Œè§„æ¨¡ä¸å¹³è¡¡ã€‚FCPç›´æ¥ä»å“åº”-åé¦ˆå¯¹ä¸­å­¦ä¹ ï¼Œé€šè¿‡æœ€å¤§ä¼¼ç„¶è®­ç»ƒæ¥è¿‘ä¼¼åé¦ˆæ¡ä»¶åéªŒã€‚è¯¥æ–¹æ³•å°†åé¦ˆé©±åŠ¨çš„å­¦ä¹ é‡æ–°å®šä¹‰ä¸ºæ¡ä»¶ç”Ÿæˆï¼Œè€Œä¸æ˜¯å¥–åŠ±ä¼˜åŒ–ï¼Œä¸ºLLMsæä¾›äº†ä¸€ç§æ›´å…·è¡¨ç°åŠ›çš„æ–¹å¼æ¥ç›´æ¥å­¦ä¹ å£å¤´åé¦ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21679",
            "title": "ReviewScore: Misinformed Peer Review Detection with Large Language\n  Models",
            "url": "https://huggingface.co/papers/2509.21679",
            "abstract": "An automated engine evaluates the factuality of review points in AI conference papers, demonstrating moderate agreement with human experts and higher accuracy at the premise level.  \t\t\t\t\tAI-generated summary \t\t\t\t Peer review serves as a backbone of academic research, but in most AI conferences, the review quality is degrading as the number of submissions explodes. To reliably detect low-quality reviews, we define misinformed review points as either \"weaknesses\" in a review that contain incorrect premises, or \"questions\" in a review that can be already answered by the paper. We verify that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce ReviewScore indicating if a review point is misinformed. To evaluate the factuality of each premise of weaknesses, we propose an automated engine that reconstructs every explicit and implicit premise from a weakness. We build a human expert-annotated ReviewScore dataset to check the ability of LLMs to automate ReviewScore evaluation. Then, we measure human-model agreements on ReviewScore using eight current state-of-the-art LLMs and verify moderate agreements. We also prove that evaluating premise-level factuality shows significantly higher agreements than evaluating weakness-level factuality. A thorough disagreement analysis further supports a potential of fully automated ReviewScore evaluation.",
            "score": 22,
            "issue_id": 6130,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "547504330158a42b",
            "authors": [
                "Hyun Ryu",
                "Doohyuk Jang",
                "Hyemin S. Lee",
                "Joonhyun Jeong",
                "Gyeongman Kim",
                "Donghyeon Cho",
                "Gyouk Chu",
                "Minyeong Hwang",
                "Hyeongwon Jang",
                "Changhun Kim",
                "Haechan Kim",
                "Jina Kim",
                "Joowon Kim",
                "Yoonjeon Kim",
                "Kwanhyung Lee",
                "Chanjae Park",
                "Heecheol Yun",
                "Gregor Betz",
                "Eunho Yang"
            ],
            "affiliations": [
                "KAIST, AITRICS",
                "KIT",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21679.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#dataset",
                    "#ethics",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AI",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ğ½Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ AI. ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ 15.2% ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¼ĞµÑÑ‚ Ğ¸ 26.4% Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸ÑÑ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ½ĞµĞ²ĞµÑ€Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ»Ğ¸ ÑƒĞ¶Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‡ĞµĞ½Ñ‹ Ğ² ÑÑ‚Ğ°Ñ‚ÑŒĞµ. Ğ”Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ğ±Ñ‹Ğ»Ğ° Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° ReviewScore, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾ÑÑ‹Ğ»Ğ¾Ğº."
                },
                "en": {
                    "title": "Automating Review Quality: Enhancing Factuality in AI Conference Papers",
                    "desc": "This paper presents an automated engine designed to assess the factual accuracy of review points in AI conference papers. It identifies misinformed review points, which are categorized as incorrect weaknesses or unnecessary questions, and introduces a metric called ReviewScore to quantify these inaccuracies. The authors create a dataset annotated by human experts to evaluate the performance of large language models (LLMs) in determining ReviewScore, finding moderate agreement with human evaluations. The study reveals that evaluating the factuality of individual premises yields better agreement than assessing the overall weaknesses, suggesting a pathway towards fully automated review evaluations."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–è¯„ä¼°AIè®ºæ–‡è¯„å®¡çš„çœŸå®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–å¼•æ“ï¼Œç”¨äºè¯„ä¼°AIä¼šè®®è®ºæ–‡è¯„å®¡æ„è§çš„çœŸå®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œ15.2%çš„è¯„å®¡æ„è§ä¸­çš„â€œå¼±ç‚¹â€æ˜¯é”™è¯¯çš„ï¼Œ26.4%çš„â€œé—®é¢˜â€æ˜¯å¯ä»¥é€šè¿‡è®ºæ–‡å›ç­”çš„ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªäººç±»ä¸“å®¶æ ‡æ³¨çš„ReviewScoreæ•°æ®é›†ï¼ŒéªŒè¯äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–è¯„ä¼°ReviewScoreæ–¹é¢çš„èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨è¯„ä¼°å‰æçš„çœŸå®æ€§æ—¶ï¼Œæ¨¡å‹ä¸äººç±»ä¸“å®¶çš„åè®®ç¨‹åº¦æ˜¾è‘—é«˜äºè¯„ä¼°æ•´ä½“å¼±ç‚¹çš„çœŸå®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22651",
            "title": "VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,\n  Speaking, and Viewing",
            "url": "https://huggingface.co/papers/2509.22651",
            "abstract": "VoiceAssistant-Eval is a benchmark for evaluating AI assistants across listening, speaking, and viewing tasks, revealing insights into model performance and identifying areas for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ .",
            "score": 18,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "fee6beb54706ecb4",
            "authors": [
                "Ke Wang",
                "Houxing Ren",
                "Zimu Lu",
                "Mingjie Zhan",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "CUHK MMLab",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22651.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#interpretability",
                    "#multimodal",
                    "#audio",
                    "#benchmark",
                    "#open_source",
                    "#small_models"
                ],
                "emoji": "ğŸ”Š",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ·Ğ²ÑƒĞº, Ñ€ĞµÑ‡ÑŒ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ",
                    "desc": "VoiceAssistant-Eval â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ°, Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 10,497 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² 13 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¸, Ğ¼ÑƒĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµÑ‡Ğ¸, Ğ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "VoiceAssistant-Eval: A New Standard for AI Assistant Evaluation",
                    "desc": "VoiceAssistant-Eval is a new benchmark created to evaluate AI assistants on their listening, speaking, and viewing abilities. It includes a large dataset of 10,497 examples across 13 different tasks, covering various audio and visual scenarios. The evaluation of 21 models, including GPT-4o-Audio, shows that while many models perform well in speaking, they struggle with audio comprehension. This benchmark highlights the strengths and weaknesses of current AI assistants, paving the way for improvements in multimodal capabilities and safety measures."
                },
                "zh": {
                    "title": "è¯„ä¼°AIåŠ©æ‰‹çš„å…¨é¢åŸºå‡†æµ‹è¯•",
                    "desc": "VoiceAssistant-Evalæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°AIåŠ©æ‰‹åœ¨å¬ã€è¯´ã€çœ‹ä»»åŠ¡ä¸­çš„è¡¨ç°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹æ€§èƒ½å¹¶è¯†åˆ«æ”¹è¿›é¢†åŸŸã€‚è¯¥åŸºå‡†åŒ…å«10,497ä¸ªç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„ç¤ºä¾‹ï¼Œæ¶µç›–13ä¸ªä»»åŠ¡ç±»åˆ«ï¼ŒåŒ…æ‹¬è‡ªç„¶å£°éŸ³ã€éŸ³ä¹å’Œå¯¹è¯ç­‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¸å¤šæ¨¡å‹åœ¨è¯´è¯ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éŸ³é¢‘ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æ­¤å¤–ï¼Œè®¾è®¡è‰¯å¥½çš„å°å‹æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šå¯ä»¥ä¸å¤§å‹æ¨¡å‹ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22644",
            "title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level\n  Feedback and Step-Level Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.22644",
            "abstract": "WebGen-Agent enhances website code generation by integrating visual feedback and GUI-agent testing with a backtracking mechanism and Step-GRPO training to improve accuracy and appearance scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce Step-GRPO with Screenshot and GUI-agent Feedback to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model's website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7.",
            "score": 10,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "b5f5be0955af5d3f",
            "authors": [
                "Zimu Lu",
                "Houxing Ren",
                "Yunqiao Yang",
                "Ke Wang",
                "Zhuofan Zong",
                "Junting Pan",
                "Mingjie Zhan",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Ace Robotics",
                "Multimedia Laboratory (MMLab), The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22644.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#reasoning",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ WebGen-Agent - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ğ¾Ğ² Ğ¸ GUI-Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ backtracking Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Step-GRPO Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ 26.4% Ğ´Ğ¾ 51.9% Ğ´Ğ»Ñ Claude-3.5-Sonnet Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº."
                },
                "en": {
                    "title": "Revolutionizing Website Code Generation with Visual Feedback",
                    "desc": "WebGen-Agent is a new system designed to improve the generation of website code by using visual feedback and testing methods. It combines a backtracking mechanism with a training approach called Step-GRPO, which helps refine the code based on visual quality scores. This system uses a visual language model to provide detailed feedback on the generated websites, ensuring that the code not only works but also looks good. As a result, WebGen-Agent significantly enhances the accuracy and appearance of website code compared to previous methods."
                },
                "zh": {
                    "title": "WebGen-Agentï¼šæå‡ç½‘ç«™ç”Ÿæˆçš„æ™ºèƒ½ä»£ç†",
                    "desc": "WebGen-Agent æ˜¯ä¸€ç§æ–°å‹çš„ç½‘ç«™ç”Ÿæˆä»£ç†ï¼Œç»“åˆäº†è§†è§‰åé¦ˆå’Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æµ‹è¯•ï¼Œä»¥æé«˜ç½‘ç«™ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå¤–è§‚è¯„åˆ†ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æè¿°å’Œå»ºè®®ï¼Œå¹¶é€šè¿‡å›æº¯æœºåˆ¶å’Œé€‰æ‹©æœ€ä½³ç­–ç•¥æ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨æˆªå›¾å’ŒGUIä»£ç†çš„è¯„åˆ†ä½œä¸ºå¥–åŠ±ï¼ŒWebGen-Agent èƒ½å¤Ÿæä¾›å¯†é›†ä¸”å¯é çš„è¿‡ç¨‹ç›‘ç£ä¿¡å·ï¼Œä»è€Œæ˜¾è‘—æå‡ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebGen-Agent åœ¨ WebGen-Bench æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22414",
            "title": "LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale\n  Diffusion Transformer",
            "url": "https://huggingface.co/papers/2509.22414",
            "abstract": "LucidFlux, a caption-free UIR framework using a diffusion transformer, achieves robust image restoration through adaptive conditioning and SigLIP features without text prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while preserving semantics -- conditions under which discriminative restorers and UNet-based diffusion priors often oversmooth, hallucinate, or drift. We present LucidFlux, a caption-free UIR framework that adapts a large diffusion transformer (Flux.1) without image captions. LucidFlux introduces a lightweight dual-branch conditioner that injects signals from the degraded input and a lightly restored proxy to respectively anchor geometry and suppress artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed to route these cues across the backbone's hierarchy, in order to yield coarse-to-fine and context-aware updates that protect the global structure while recovering texture. After that, to avoid the latency and instability of text prompts or MLLM captions, we enforce caption-free semantic alignment via SigLIP features extracted from the proxy. A scalable curation pipeline further filters large-scale data for structure-rich supervision. Across synthetic and in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source and commercial baselines, and ablation studies verify the necessity of each component. LucidFlux shows that, for large DiTs, when, where, and what to condition on -- rather than adding parameters or relying on text prompts -- is the governing lever for robust and caption-free universal image restoration in the wild.",
            "score": 10,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "34883cbd4b91fb74",
            "authors": [
                "Song Fei",
                "Tian Ye",
                "Lujia Wang",
                "Lei Zhu"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22414.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#diffusion",
                    "#hallucinations"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹",
                    "desc": "LucidFlux Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ±ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Flux.1 Ñ‡ĞµÑ€ĞµĞ· Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸. Ğ”Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ SigLIP Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞµĞº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ robust Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "LucidFlux: Caption-Free Image Restoration with Smart Conditioning",
                    "desc": "LucidFlux is a novel framework for universal image restoration (UIR) that operates without the need for text captions. It utilizes a diffusion transformer to effectively restore images that have been degraded by various factors while maintaining their semantic integrity. The framework features a dual-branch conditioning system that helps to stabilize the restoration process by anchoring geometric details and minimizing artifacts. By employing a unique modulation schedule and leveraging SigLIP features, LucidFlux achieves superior performance in restoring images compared to existing methods, demonstrating that strategic conditioning is key to effective image restoration."
                },
                "zh": {
                    "title": "LucidFluxï¼šæ— æ–‡æœ¬æç¤ºçš„å¼ºå¤§å›¾åƒæ¢å¤æ¡†æ¶",
                    "desc": "LucidFluxæ˜¯ä¸€ä¸ªæ— æ–‡æœ¬æç¤ºçš„é€šç”¨å›¾åƒæ¢å¤æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨å®ç°å¼ºå¤§çš„å›¾åƒæ¢å¤ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªé€‚åº”æ¡ä»¶å’ŒSigLIPç‰¹å¾ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„è¿‡å¹³æ»‘å’Œä¼ªå½±é—®é¢˜ã€‚LucidFluxå¼•å…¥äº†è½»é‡çº§çš„åŒåˆ†æ”¯è°ƒèŠ‚å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é”šå®šå‡ ä½•ç»“æ„å¹¶æŠ‘åˆ¶ä¼ªå½±ã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè¿›è¡Œç»“æ„ä¸°å¯Œçš„ç›‘ç£ï¼ŒLucidFluxåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨æ— æ–‡æœ¬æç¤ºçš„å›¾åƒæ¢å¤ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22647",
            "title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2509.22647",
            "abstract": "CapRL, a novel reinforcement learning framework, enhances image captioning by using a vision-free language model to evaluate caption quality through multiple-choice questions, leading to improved performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \"good\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Code is available here: https://github.com/InternLM/CapRL.",
            "score": 9,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "eeedb125ad75e30e",
            "authors": [
                "Long Xing",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuhang Cao",
                "Jianze Liang",
                "Qidong Huang",
                "Jiaqi Wang",
                "Feng Wu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Alibaba Cloud",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22647.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#rl",
                    "#games",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "ğŸ“¸",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ CapRL â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ reinforcement learning. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° LVLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ LLM Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° 12 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ supervised fine-tuning."
                },
                "en": {
                    "title": "Revolutionizing Image Captioning with CapRL: Quality Through Questioning",
                    "desc": "CapRL is a new reinforcement learning framework designed to improve image captioning by using a language model that does not rely on visual input to assess the quality of captions. It addresses the limitations of traditional supervised fine-tuning methods, which often lead to models that memorize specific answers rather than generating diverse descriptions. By employing a two-stage pipeline, CapRL generates captions and evaluates them based on how well a separate language model can answer questions about the images using those captions. This innovative approach not only enhances performance across various benchmarks but also provides a scalable solution for training image captioning models."
                },
                "zh": {
                    "title": "CapRLï¼šæå‡å›¾åƒæè¿°è´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "CapRLæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨æ— è§†è§‰çš„è¯­è¨€æ¨¡å‹æ¥è¯„ä¼°å›¾åƒæè¿°çš„è´¨é‡ï¼Œä»è€Œæå‡å›¾åƒæè¿°çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œå…‹æœäº†ä¼ ç»Ÿç›‘ç£å¾®è°ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œé¿å…äº†å¯¹æ˜‚è´µçš„äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚CapRLé€šè¿‡ä¸€ä¸ªè§£è€¦çš„ä¸¤é˜¶æ®µæµç¨‹ç”Ÿæˆæè¿°ï¼Œå¹¶æ ¹æ®æ— è§†è§‰è¯­è¨€æ¨¡å‹å›ç­”å¤šé¡¹é€‰æ‹©é¢˜çš„å‡†ç¡®æ€§æ¥å®šä¹‰æè¿°è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCapRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å›¾åƒæè¿°çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21710",
            "title": "Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on\n  Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval",
            "url": "https://huggingface.co/papers/2509.21710",
            "abstract": "ToG-3, a novel framework, enhances LLMs with external knowledge using a dynamic, multi-agent system that evolves queries and subgraphs for precise evidence retrieval and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses a critical limitation of prior Graph-based RAG methods, which typically construct a static graph index in a single pass without adapting to the actual query. A multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build a targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.",
            "score": 9,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "bd2996a79ff96ede",
            "authors": [
                "Xiaojun Wu",
                "Cehao Yang",
                "Xueyuan Lin",
                "Chengjin Xu",
                "Xuhui Jiang",
                "Yuanliang Sun",
                "Hui Xiong",
                "Jia Li",
                "Jian Guo"
            ],
            "affiliations": [
                "DataArc Tech Ltd.",
                "Hithink RoyalFlush Information Network Co., Ltd",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "IDEA Research, International Digital Economy Academy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21710.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#benchmark",
                    "#reasoning",
                    "#rag",
                    "#graphs"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”ÑƒĞ¼Ğ°Ğ¹-Ğ½Ğ°-Ğ³Ñ€Ğ°Ñ„Ğµ: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "ToG-3 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LLM Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ MACER, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„-Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ¸Ğ· Ñ‡Ğ°Ğ½ĞºĞ¾Ğ², Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° - Constructor, Retriever, Reflector Ğ¸ Responser, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Graph-based RAG Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ¶Ğµ Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼Ğ¸ LLM."
                },
                "en": {
                    "title": "Dynamic Knowledge Integration for Enhanced Reasoning in LLMs",
                    "desc": "ToG-3 is a new framework that improves Large Language Models (LLMs) by integrating external knowledge through a dynamic multi-agent system. It introduces a mechanism called Multi-Agent Context Evolution and Retrieval (MACER) that allows for the adaptive construction of a heterogeneous graph index. This system evolves both the queries and the subgraphs in real-time, enabling more accurate evidence retrieval and reasoning. By overcoming the limitations of static graph structures, ToG-3 enhances the reasoning capabilities of LLMs, even when using smaller models."
                },
                "zh": {
                    "title": "åŠ¨æ€æ¼”åŒ–ï¼Œç²¾å‡†æ¨ç†çš„æœªæ¥",
                    "desc": "ToG-3æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¤–éƒ¨çŸ¥è¯†çš„ç»“åˆã€‚å®ƒå¼•å…¥äº†å¤šæ™ºèƒ½ä½“ä¸Šä¸‹æ–‡æ¼”åŒ–å’Œæ£€ç´¢æœºåˆ¶ï¼ˆMACERï¼‰ï¼Œè§£å†³äº†ç°æœ‰å›¾åŸºæ–¹æ³•åœ¨æ„å»ºçŸ¥è¯†å›¾è°±æ—¶çš„å±€é™æ€§ã€‚ToG-3çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåŠ¨æ€æ„å»ºå’Œä¼˜åŒ–å¼‚æ„å›¾ç´¢å¼•ï¼Œé‡‡ç”¨äº†æ¼”åŒ–æŸ¥è¯¢å’Œæ¼”åŒ–å­å›¾çš„åŒé‡æ¼”åŒ–æœºåˆ¶ï¼Œä»¥å®ç°ç²¾ç¡®çš„è¯æ®æ£€ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒToG-3åœ¨æ·±åº¦å’Œå¹¿åº¦æ¨ç†åŸºå‡†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†MACERæ¡†æ¶å„ç»„æˆéƒ¨åˆ†çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21766",
            "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon\n  Scenarios",
            "url": "https://huggingface.co/papers/2509.21766",
            "abstract": "UltraHorizon is a new benchmark that evaluates long-horizon and partially observable tasks for autonomous agents, highlighting gaps in their sustained reasoning, planning, memory, and tool use capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce UltraHorizon a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average 200k+ tokens and 400+ tool calls, whereas in standard configurations they still exceed 35k tokens and involve more than 60 tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. https://github.com/StarDewXXX/UltraHorizon{Our code will be available here.}",
            "score": 8,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "5e2b3fc1a4b26823",
            "authors": [
                "Haotian Luo",
                "Huaisong Zhang",
                "Xuelin Zhang",
                "Haoyu Wang",
                "Zeyu Qin",
                "Wenjie Lu",
                "Guozheng Ma",
                "Haiying He",
                "Yingsha Xie",
                "Qiyang Zhou",
                "Zixuan Hu",
                "Hongze Mi",
                "Yibo Wang",
                "Naiqiang Tan",
                "Hong Chen",
                "Yi R. Fung",
                "Chun Yuan",
                "Li Shen"
            ],
            "affiliations": [
                "China Agricultural University",
                "Didichuxing Co. Ltd",
                "HKUST",
                "Huazhong Agricultural University",
                "Nanyang Technological University",
                "Sun Yat-sen University",
                "Tianjin University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21766.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ”­",
                "ru": {
                    "title": "Ğ”Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸",
                    "desc": "UltraHorizon - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. Ğ¢Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² ÑĞ°Ğ¼Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 200 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ 400+ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Bridging the Gap in Long-Horizon Reasoning for AI Agents",
                    "desc": "UltraHorizon is a benchmark designed to assess the performance of autonomous agents in long-horizon and partially observable tasks, which are crucial for real-world applications. Unlike traditional evaluations that focus on short tasks, UltraHorizon emphasizes the need for sustained reasoning, planning, memory management, and effective tool use. The benchmark reveals that current large language model (LLM) agents struggle significantly in these complex scenarios, often underperforming compared to human participants. Through detailed analysis, the study identifies specific errors in agent performance, highlighting fundamental gaps in their capabilities."
                },
                "zh": {
                    "title": "è¯„ä¼°è‡ªä¸»æ™ºèƒ½ä½“çš„é•¿æ—¶é—´è·¨åº¦èƒ½åŠ›",
                    "desc": "UltraHorizonæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è‡ªä¸»æ™ºèƒ½ä½“åœ¨é•¿æ—¶é—´è·¨åº¦å’Œéƒ¨åˆ†å¯è§‚å¯Ÿä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¼ºè°ƒå…¶åœ¨æŒç»­æ¨ç†ã€è§„åˆ’ã€è®°å¿†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰çš„è¯„ä¼°å¤§å¤šé›†ä¸­åœ¨çŸ­æ—¶é—´è·¨åº¦å’Œå®Œå…¨å¯è§‚å¯Ÿçš„ä»»åŠ¡ä¸Šï¼Œè€Œè®¸å¤šç°å®ä¸–ç•Œçš„å…³é”®ä»»åŠ¡åˆ™éœ€è¦åœ¨é•¿æ—¶é—´è·¨åº¦å’Œéƒ¨åˆ†å¯è§‚å¯Ÿçš„åœºæ™¯ä¸­è¿›è¡Œã€‚é€šè¿‡æ¢ç´¢ä½œä¸ºç»Ÿä¸€ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒç¯å¢ƒä¸­éªŒè¯äº†æ™ºèƒ½ä½“çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨è¿™äº›è®¾ç½®ä¸­è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†æ™ºèƒ½ä½“åœ¨é•¿æ—¶é—´è·¨åº¦èƒ½åŠ›ä¸Šçš„æŒç»­å·®è·ï¼Œå¹¶åˆ†æäº†å¯¼è‡´é”™è¯¯çš„ä¸»è¦åŸå› ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21760",
            "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
            "url": "https://huggingface.co/papers/2509.21760",
            "abstract": "A pre-trained video diffusion transformer, UniVid, is fine-tuned to handle diverse vision tasks without task-specific modifications, demonstrating cross-modal and cross-source generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.",
            "score": 6,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "e74140613312626b",
            "authors": [
                "Lan Chen",
                "Yuchao Gu",
                "Qi Mao"
            ],
            "affiliations": [
                "Communication University of China",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21760.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ UniVid - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ĞºĞ°Ğº ÑĞ°Ğ¼Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. UniVid Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ÑÑÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "UniVid: A Unified Framework for Diverse Vision Tasks",
                    "desc": "The paper introduces UniVid, a pre-trained video diffusion transformer that can be fine-tuned for various vision tasks without needing specific modifications for each task. It leverages the concept of visual sentences, where tasks are represented in a sequence that guides the model's output. UniVid demonstrates strong generalization capabilities across different modalities, such as images and videos, and can adapt to tasks from various sources without requiring extensive pre-training. This approach shows that pre-trained video generation models can provide a scalable and unified framework for handling diverse vision challenges."
                },
                "zh": {
                    "title": "UniVidï¼šç»Ÿä¸€è§†è§‰ä»»åŠ¡çš„å¼ºå¤§å·¥å…·",
                    "desc": "UniVidæ˜¯ä¸€ä¸ªç»è¿‡é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œç‰¹å®šä»»åŠ¡ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œé€‚åº”å¤šç§è§†è§‰ä»»åŠ¡ã€‚å®ƒé€šè¿‡å°†ä»»åŠ¡è¡¨ç¤ºä¸ºè§†è§‰å¥å­ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡åºåˆ—æ¥å®šä¹‰ä»»åŠ¡å’ŒæœŸæœ›çš„è¾“å‡ºå½¢å¼ã€‚UniVidåœ¨è·¨æ¨¡æ€æ¨ç†å’Œè·¨æºä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°½ç®¡ä»…åœ¨è‡ªç„¶è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¯ä»¥ä½œä¸ºè§†è§‰å»ºæ¨¡çš„ç»Ÿä¸€å’Œå¯æ‰©å±•åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21799",
            "title": "D-Artemis: A Deliberative Cognitive Framework for Mobile GUI\n  Multi-Agents",
            "url": "https://huggingface.co/papers/2509.21799",
            "abstract": "D-Artemis, a novel deliberative framework, enhances GUI automation by leveraging app-specific tips, proactive alignment, and reflection, achieving state-of-the-art results with general-purpose multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) agents aim to automate a wide spectrum of human tasks by emulating user interaction. Despite rapid advancements, current approaches are hindered by several critical challenges: data bottleneck in end-to-end training, high cost of delayed error detection, and risk of contradictory guidance. Inspired by the human cognitive loop of Thinking, Alignment, and Reflection, we present D-Artemis -- a novel deliberative framework in this paper. D-Artemis leverages a fine-grained, app-specific tip retrieval mechanism to inform its decision-making process. It also employs a proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC) Check module and Action Correction Agent (ACA) work in concert to mitigate the risk of execution failures. A post-execution Status Reflection Agent (SRA) completes the cognitive loop, enabling strategic learning from experience. Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal large language models (MLLMs) for GUI tasks without the need for training on complex trajectory datasets, demonstrating strong generalization. D-Artemis establishes new state-of-the-art (SOTA) results across both major benchmarks, achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2. Extensive ablation studies further demonstrate the significant contribution of each component to the framework.",
            "score": 5,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "12de0e125e96b91e",
            "authors": [
                "Hongze Mi",
                "Yibo Feng",
                "Wenjie Lu",
                "Yuqi Wang",
                "Jinyuan Li",
                "Song Cao",
                "He Cui",
                "Tengfei Tian",
                "Xuelin Zhang",
                "Haotian Luo",
                "Di Sun",
                "Naiqiang Tan",
                "Gang Pan"
            ],
            "affiliations": [
                "Didichuxing Co. Ltd",
                "Huazhong Agricultural University",
                "Sichuan University",
                "The Chinese University of Hong Kong, Shenzhen",
                "Tianjin University",
                "Tianjin University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21799.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#multimodal",
                    "#agents",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI Ñ Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ D-Artemis â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. D-Artemis Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AndroidWorld (75.8%) Ğ¸ ScreenSpot-V2 (96.8%), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing GUI Automation with D-Artemis!",
                    "desc": "D-Artemis is a new framework designed to improve the automation of Graphical User Interfaces (GUIs) by using a structured approach inspired by human thinking processes. It incorporates a mechanism for retrieving specific tips related to applications, which aids in making better decisions during automation tasks. The framework includes proactive checks and corrections before actions are executed, as well as a reflection phase after actions to learn from outcomes. By utilizing general-purpose multimodal large language models, D-Artemis achieves impressive results on benchmark tests without needing extensive training on complex datasets."
                },
                "zh": {
                    "title": "D-Artemisï¼šæå‡GUIè‡ªåŠ¨åŒ–çš„æ–°æ¡†æ¶",
                    "desc": "D-Artemisæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ·±æ€æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ç‰¹å®šåº”ç”¨çš„æç¤ºã€ä¸»åŠ¨å¯¹é½å’Œåæ€ï¼Œæå‡äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†å½“å‰æ–¹æ³•åœ¨ç«¯åˆ°ç«¯è®­ç»ƒä¸­çš„æ•°æ®ç“¶é¢ˆã€å»¶è¿Ÿé”™è¯¯æ£€æµ‹çš„é«˜æˆæœ¬å’ŒçŸ›ç›¾æŒ‡å¯¼çš„é£é™©ã€‚D-Artemisé‡‡ç”¨ç»†ç²’åº¦çš„åº”ç”¨ç‰¹å®šæç¤ºæ£€ç´¢æœºåˆ¶ï¼Œå¹¶åœ¨æ‰§è¡Œå‰è¿›è¡Œä¸»åŠ¨å¯¹é½ï¼Œä»¥å‡å°‘æ‰§è¡Œå¤±è´¥çš„é£é™©ã€‚é€šè¿‡åœ¨æ‰§è¡Œåè¿›è¡ŒçŠ¶æ€åæ€ï¼ŒD-Artemisèƒ½å¤Ÿä»ç»éªŒä¸­è¿›è¡Œæˆ˜ç•¥å­¦ä¹ ï¼Œå±•ç¤ºäº†åœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æ–°çš„æœ€å…ˆè¿›ç»“æœçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21574",
            "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
            "url": "https://huggingface.co/papers/2509.21574",
            "abstract": "X-Streamer is a unified multimodal framework using dual-transformer architecture for real-time, open-ended interactions across text, speech, and video, leveraging large language-speech models and autoregressive diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.",
            "score": 5,
            "issue_id": 6129,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "addda28b1d3ecc10",
            "authors": [
                "You Xie",
                "Tianpei Gu",
                "Zenan Li",
                "Chenxu Zhang",
                "Guoxian Song",
                "Xiaochen Zhao",
                "Chao Liang",
                "Jianwen Jiang",
                "Hongyi Xu",
                "Linjie Luo"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21574.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#architecture",
                    "#multimodal",
                    "#agi",
                    "#interpretability",
                    "#games",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ X-Streamer â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚, Ñ€ĞµÑ‡ÑŒ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ dual-transformer Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Thinker-Actor, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Thinker Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ñ‹, Ğ° Actor Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ chunk-wise autoregressive diffusion Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… GPU A100, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‡Ğ°ÑĞ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‡Ğ°Ñ‚Ñ‹ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "X-Streamer: Real-Time Multimodal Interactions Redefined",
                    "desc": "X-Streamer is a cutting-edge framework that combines text, speech, and video interactions using a dual-transformer architecture. It allows for real-time communication by transforming a static image into a dynamic digital human capable of engaging in endless conversations. The framework consists of two main components: the Thinker, which processes and understands user inputs, and the Actor, which generates synchronized multimodal outputs. By utilizing advanced models and attention mechanisms, X-Streamer ensures smooth and coherent interactions, making it a significant advancement in creating interactive digital agents."
                },
                "zh": {
                    "title": "X-Streamerï¼šå®æ—¶å¤šæ¨¡æ€äº’åŠ¨çš„æ–°çºªå…ƒ",
                    "desc": "X-Streameræ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œé‡‡ç”¨åŒå˜æ¢å™¨æ¶æ„ï¼Œèƒ½å¤Ÿå®ç°æ–‡æœ¬ã€è¯­éŸ³å’Œè§†é¢‘ä¹‹é—´çš„å®æ—¶äº’åŠ¨ã€‚å®ƒé€šè¿‡æµå¼å¤šæ¨¡æ€è¾“å…¥ï¼Œå°†é™æ€è‚–åƒè½¬å˜ä¸ºæŒä¹…çš„æ™ºèƒ½è§†å¬äº’åŠ¨ã€‚æ¡†æ¶çš„æ ¸å¿ƒæ˜¯Thinker-ActoråŒå˜æ¢å™¨ï¼ŒThinkeræ¨¡å—è´Ÿè´£æ„ŸçŸ¥å’Œæ¨ç†ç”¨æˆ·è¾“å…¥ï¼Œè€ŒActoræ¨¡å—åˆ™å°†è¿™äº›ä¿¡æ¯å®æ—¶è½¬æ¢ä¸ºåŒæ­¥çš„å¤šæ¨¡æ€è¾“å‡ºã€‚X-Streameråœ¨ä¸¤å—A100 GPUä¸Šå®æ—¶è¿è¡Œï¼Œèƒ½å¤Ÿæ”¯æŒæ•°å°æ—¶çš„æŒç»­è§†é¢‘èŠå¤©ï¼Œæ¨åŠ¨äº’åŠ¨æ•°å­—äººç±»çš„ç»Ÿä¸€ä¸–ç•Œå»ºæ¨¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22186",
            "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document Parsing",
            "url": "https://huggingface.co/papers/2509.22186",
            "abstract": "MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead.",
            "score": 4,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "136108b6a4721246",
            "authors": [
                "Junbo Niu",
                "Zheng Liu",
                "Zhuangcheng Gu",
                "Bin Wang",
                "Linke Ouyang",
                "Zhiyuan Zhao",
                "Tao Chu",
                "Tianyao He",
                "Fan Wu",
                "Qintong Zhang",
                "Zhenjiang Jin",
                "Guang Liang",
                "Rui Zhang",
                "Wenzheng Zhang",
                "Yuan Qu",
                "Zhifei Ren",
                "Yuefeng Sun",
                "Yuanhong Zheng",
                "Dongsheng Ma",
                "Zirui Tang",
                "Boyu Niu",
                "Ziyang Miao",
                "Hejun Dong",
                "Siyi Qian",
                "Junyuan Zhang",
                "Jingzhou Chen",
                "Fangdong Wang",
                "Xiaomeng Zhao",
                "Liqun Wei",
                "Wei Li",
                "Shasha Wang",
                "Ruiliang Xu",
                "Yuanyuan Cao",
                "Lu Chen",
                "Qianqian Wu",
                "Huaiyu Gu",
                "Lindong Lu",
                "Keming Wang",
                "Dechen Lin",
                "Guanlin Shen",
                "Xuanhe Zhou",
                "Linfeng Zhang",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Jiaqi Wang",
                "Bo Zhang",
                "Lei Bai",
                "Pei Chu",
                "Weijia Li",
                "Jiang Wu",
                "Lijun Wu",
                "Zhenxiang Li",
                "Guangyu Wang",
                "Zhongying Tu",
                "Chao Xu",
                "Kai Chen",
                "Yu Qiao",
                "Bowen Zhou",
                "Dahua Lin",
                "Wentao Zhang",
                "Conghui He"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22186.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° MinerU2.5 - vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. MinerU2.5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Efficient Document Parsing with MinerU2.5",
                    "desc": "MinerU2.5 is a vision-language model designed for document parsing, featuring 1.2 billion parameters. It utilizes a coarse-to-fine parsing strategy that separates the analysis of document layout from the recognition of content, enhancing efficiency. The model first conducts layout analysis on downsampled images to identify structural elements, then focuses on detailed content recognition using high-resolution crops. This innovative approach allows MinerU2.5 to achieve top performance on various benchmarks while reducing computational costs compared to other models."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ–‡æ¡£è§£æçš„æ–°æ ‡æ†",
                    "desc": "MinerU2.5æ˜¯ä¸€ç§å…·æœ‰12äº¿å‚æ•°çš„æ–‡æ¡£è§£æè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨ç²—åˆ°ç»†çš„è§£æç­–ç•¥ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„è¯†åˆ«å‡†ç¡®ç‡å’Œè®¡ç®—æ•ˆç‡ã€‚è¯¥æ¨¡å‹çš„ç¬¬ä¸€é˜¶æ®µåœ¨é™é‡‡æ ·å›¾åƒä¸Šè¿›è¡Œé«˜æ•ˆçš„å¸ƒå±€åˆ†æï¼Œä»¥è¯†åˆ«ç»“æ„å…ƒç´ ï¼Œä»è€Œé¿å…å¤„ç†é«˜åˆ†è¾¨ç‡è¾“å…¥çš„è®¡ç®—å¼€é”€ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åœ¨åŸå§‹å›¾åƒä¸­æå–çš„åŸç”Ÿåˆ†è¾¨ç‡è£å‰ªå›¾ä¸Šè¿›è¡Œç›®æ ‡å†…å®¹è¯†åˆ«ï¼Œä¿ç•™äº†å¯†é›†æ–‡æœ¬ã€å¤æ‚å…¬å¼å’Œè¡¨æ ¼ä¸­çš„ç»†èŠ‚ã€‚é€šè¿‡å¼€å‘å…¨é¢çš„æ•°æ®å¼•æ“ï¼ŒMinerU2.5èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“ï¼Œæ”¯æŒé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œæœ€ç»ˆåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22624",
            "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
            "url": "https://huggingface.co/papers/2509.22624",
            "abstract": "SPARK, a synergistic policy and reward co-evolving framework, enhances LLMs and LVLMs by recycling rollouts and correctness data to train a generative reward model, reducing reliance on human preferences and external reward models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks. However, RLHF incurs high costs and potential reward-policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization.",
            "score": 2,
            "issue_id": 6131,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "63cb194710260ca4",
            "authors": [
                "Ziyu Liu",
                "Yuhang Zang",
                "Shengyuan Ding",
                "Yuhang Cao",
                "Xiaoyi Dong",
                "Haodong Duan",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22624.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#architecture",
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#alignment"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "SPARK - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ, SPARK Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "SPARK: Revolutionizing Model Training with Self-Improving Rewards",
                    "desc": "SPARK is a new framework that improves Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) by using a method called reinforcement learning. It recycles data from previous model outputs and correctness checks to train a generative reward model, which reduces the need for human feedback and external reward systems. This approach creates a feedback loop where better reward accuracy leads to improved model performance, allowing the model to learn and refine itself continuously. SPARK has shown significant performance improvements across various benchmarks, demonstrating its effectiveness and versatility in enhancing model capabilities."
                },
                "zh": {
                    "title": "SPARKï¼šæå‡æ¨¡å‹æ€§èƒ½çš„ååŒæ¡†æ¶",
                    "desc": "SPARKæ˜¯ä¸€ä¸ªååŒæ”¿ç­–å’Œå¥–åŠ±å…±åŒæ¼”åŒ–çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å›æ”¶å›æ”¾å’Œæ­£ç¡®æ€§æ•°æ®æ¥è®­ç»ƒç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œä»è€Œå‡å°‘å¯¹äººç±»åå¥½å’Œå¤–éƒ¨å¥–åŠ±æ¨¡å‹çš„ä¾èµ–ã€‚SPARKåˆ©ç”¨å¤šç§ç›®æ ‡è¿›è¡Œè¾…åŠ©è®­ç»ƒï¼Œæ•™ä¼šæ¨¡å‹è¯„ä¼°å’Œæ”¹è¿›è‡ªèº«çš„å“åº”ï¼Œå½¢æˆæ­£å‘çš„å…±åŒæ¼”åŒ–åé¦ˆå¾ªç¯ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªæ¨ç†å’Œå¥–åŠ±åŸºå‡†ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§å’Œå¹¿æ³›çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22601",
            "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive\n  Exploration for Agentic Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.22601",
            "abstract": "SPEAR, a curriculum-based self-imitation learning method, manages exploration-exploitation balance in reinforcement learning for LLMs by using intrinsic rewards and trajectory-level entropy control.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.",
            "score": 2,
            "issue_id": 6130,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "609629206f21aeac",
            "authors": [
                "Yulei Qin",
                "Xiaoyu Tan",
                "Zhengbao He",
                "Gang Li",
                "Haojia Lin",
                "Zongyi Li",
                "Zihan Xu",
                "Yuchen Shi",
                "Siqi Cai",
                "Renting Rui",
                "Shaofei Cai",
                "Yuzheng Cai",
                "Xuan Zhang",
                "Sheng Ye",
                "Ke Li",
                "Xing Sun"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22601.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#games",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² RL Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SPEAR - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ LLM, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ curriculum learning. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ ÑƒĞ¶Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. SPEAR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ replay buffer Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑĞ¼ĞµÑ‰Ğ°Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚ Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Balancing Exploration and Exploitation in LLMs with SPEAR",
                    "desc": "This paper introduces SPEAR, a method that enhances reinforcement learning for large language models (LLMs) by balancing exploration and exploitation through self-imitation learning. It addresses the challenge of maintaining stability during training by using intrinsic rewards and managing trajectory-level entropy. SPEAR employs a curriculum-based approach, gradually guiding the policy evolution while ensuring that exploration remains effective without leading to instability. By leveraging a replay buffer of successful experiences, the method allows LLMs to refine their tool-use skills and adapt to complex environments efficiently."
                },
                "zh": {
                    "title": "å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„è‡ªæˆ‘æ¨¡ä»¿å­¦ä¹ ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSPEARçš„è‡ªæˆ‘æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡å†…åœ¨å¥–åŠ±å’Œè½¨è¿¹çº§ç†µæ§åˆ¶æ¥ç®¡ç†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å­¦ä¹ è¿‡ç¨‹ã€‚SPEARåˆ©ç”¨è¯¾ç¨‹å­¦ä¹ çš„ç­–ç•¥ï¼Œé€æ­¥å¼•å¯¼ç­–ç•¥æ¼”å˜ï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒé€‚å½“çš„ç†µæ°´å¹³ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ç§¯ç´¯å·¥å…·ä½¿ç”¨æŠ€èƒ½çš„åŒæ—¶ï¼Œé¿å…è®­ç»ƒä¸ç¨³å®šå’Œç­–ç•¥æ¼‚ç§»çš„é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22496",
            "title": "Where MLLMs Attend and What They Rely On: Explaining Autoregressive\n  Token Generation",
            "url": "https://huggingface.co/papers/2509.22496",
            "abstract": "EAGLE is a lightweight framework that explains token generation in multimodal large language models by attributing tokens to visual regions and quantifying the influence of language and perceptual evidence.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE.",
            "score": 2,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "ad53ca1f5bbc2302",
            "authors": [
                "Ruoyu Chen",
                "Xiaoqing Guo",
                "Kangwei Liu",
                "Siyuan Liang",
                "Shiming Liu",
                "Qunli Zhang",
                "Hua Zhang",
                "Xiaochun Cao"
            ],
            "affiliations": [
                "Department of Computer Science, Hong Kong Baptist University",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "RAMS Lab, Huawei Technologies Co., Ltd.",
                "RAMS Lab, Munich Research Center, Huawei Technologies DÃ¼sseldorf GmbH",
                "School of Computing, NUS",
                "School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University",
                "School of Cyber Security, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22496.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#inference",
                    "#multimodal",
                    "#interpretability",
                    "#open_source"
                ],
                "emoji": "ğŸ¦…",
                "ru": {
                    "title": "ĞĞ±ÑŠÑÑĞ½ÑĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½: ĞºĞ°Ğº MLLM Ğ²Ğ¸Ğ´ÑÑ‚ Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚",
                    "desc": "EAGLE - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ ÑĞ²ÑĞ·Ğ°Ğ½ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğº Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EAGLE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğº GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "EAGLE: Unraveling Token Generation in Multimodal Models",
                    "desc": "EAGLE is a new framework designed to explain how multimodal large language models (MLLMs) generate tokens by linking them to specific visual areas. It quantifies the influence of both language and visual information on token generation, enhancing the interpretability of these models. The framework uses an objective function to measure how essential and sufficient different visual regions are for generating specific tokens, optimizing this through a greedy search method. EAGLE has been shown to outperform existing methods in terms of accuracy and efficiency while using less computational resources, making it a practical tool for understanding MLLM behavior."
                },
                "zh": {
                    "title": "EAGLEï¼šæå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§çš„è½»é‡çº§æ¡†æ¶",
                    "desc": "EAGLEæ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œç”¨äºè§£é‡Šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä»¤ç‰Œç”Ÿæˆã€‚å®ƒé€šè¿‡å°†ä»¤ç‰Œå½’å› äºè§†è§‰åŒºåŸŸï¼Œå¹¶é‡åŒ–è¯­è¨€å’Œæ„ŸçŸ¥è¯æ®çš„å½±å“ï¼Œæ¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œç»Ÿä¸€äº†å……åˆ†æ€§å’Œå¿…è¦æ€§è¯„åˆ†ï¼Œå¹¶é€šè¿‡è´ªå©ªæœç´¢ä¼˜åŒ–ç¨€ç–å›¾åƒåŒºåŸŸã€‚EAGLEåœ¨å¤šä¸ªå¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨å¯ä¿¡åº¦ã€å®šä½å’Œå¹»è§‰è¯Šæ–­æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†GPUå†…å­˜çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22630",
            "title": "StateX: Enhancing RNN Recall via Post-training State Expansion",
            "url": "https://huggingface.co/papers/2509.22630",
            "abstract": "StateX is a post-training pipeline that expands the state size of pre-trained RNNs, enhancing recall and in-context learning without significant additional costs.  \t\t\t\t\tAI-generated summary \t\t\t\t While Transformer-based models have demonstrated remarkable language modeling performance, their high complexities result in high costs when processing long contexts. In contrast, recurrent neural networks (RNNs) such as linear attention and state space models have gained popularity due to their constant per-token complexities. However, these recurrent models struggle with tasks that require accurate recall of contextual information from long contexts, because all contextual information is compressed into a constant-size recurrent state. Previous works have shown that recall ability is positively correlated with the recurrent state size, yet directly training RNNs with larger recurrent states results in high training costs. In this paper, we introduce StateX, a training pipeline for efficiently expanding the states of pre-trained RNNs through post-training. For two popular classes of RNNs, linear attention and state space models, we design post-training architectural modifications to scale up the state size with no or negligible increase in model parameters. Experiments on models up to 1.3B parameters demonstrate that StateX efficiently enhances the recall and in-context learning ability of RNNs without incurring high post-training costs or compromising other capabilities.",
            "score": 1,
            "issue_id": 6131,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "ba2dc999ea74187f",
            "authors": [
                "Xingyu Shen",
                "Yingfa Chen",
                "Zhen Leng Thai",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "Department of Science and Technology, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22630.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#long_context",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ RNN Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ StateX - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (RNN). ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ RNN Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½, Ğ½Ğ¾ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸Ğ·-Ğ·Ğ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²ÑĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. StateX Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ state space Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±ĞµĞ· Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Enhancing RNN Recall with StateX: Bigger States, Lower Costs!",
                    "desc": "StateX is a novel post-training pipeline designed to increase the state size of pre-trained recurrent neural networks (RNNs), which improves their ability to recall information and learn from context. Traditional RNNs face challenges with long contexts because they compress all information into a fixed-size state, limiting their recall capabilities. StateX addresses this issue by implementing architectural modifications that allow for larger states without significantly increasing the model's parameters or training costs. Experiments show that StateX effectively enhances the performance of RNNs, particularly in recall and in-context learning, while maintaining efficiency and other functionalities."
                },
                "zh": {
                    "title": "StateXï¼šé«˜æ•ˆæ‰©å±•RNNçŠ¶æ€çš„åè®­ç»ƒç®¡é“",
                    "desc": "StateX æ˜¯ä¸€ä¸ªåè®­ç»ƒç®¡é“ï¼Œæ—¨åœ¨æ‰©å±•é¢„è®­ç»ƒé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„çŠ¶æ€å¤§å°ï¼Œä»è€Œåœ¨ä¸æ˜¾è‘—å¢åŠ æˆæœ¬çš„æƒ…å†µä¸‹å¢å¼ºè®°å¿†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚å°½ç®¡åŸºäºå˜æ¢å™¨çš„æ¨¡å‹åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é«˜å¤æ‚æ€§å¯¼è‡´å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶æˆæœ¬é«˜æ˜‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œçº¿æ€§æ³¨æ„åŠ›å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ç­‰é€’å½’ç¥ç»ç½‘ç»œå› å…¶æ¯ä¸ªæ ‡è®°çš„å¤æ‚åº¦æ’å®šè€Œå—åˆ°æ¬¢è¿ï¼Œä½†åœ¨éœ€è¦å‡†ç¡®å›å¿†é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚StateX é€šè¿‡åè®­ç»ƒæœ‰æ•ˆåœ°æ‰©å¤§äº†é¢„è®­ç»ƒ RNN çš„çŠ¶æ€å¤§å°ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å¢å¼º RNN çš„è®°å¿†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22244",
            "title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image\n  Editing",
            "url": "https://huggingface.co/papers/2509.22244",
            "abstract": "FlashEdit enables real-time, high-fidelity image editing with diffusion models through efficient inversion, background preservation, and localized attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150times speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit.",
            "score": 0,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "ea3d5abcdd7f6370",
            "authors": [
                "Junyi Wu",
                "Zhiteng Li",
                "Haotong Qin",
                "Xiaohong Liu",
                "Linghe Kong",
                "Yulun Zhang",
                "Xiaokang Yang"
            ],
            "affiliations": [
                "ETH Zurich",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22244.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#inference",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "FlashEdit â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (OSIE), Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ñ„Ğ¾Ğ½Ğ° (BG-Shield) Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (SSCA). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° 0.2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ² 150 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². FlashEdit ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ğ½Ğ° Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Real-Time Image Editing Revolutionized with FlashEdit",
                    "desc": "FlashEdit is a cutting-edge framework that allows for real-time image editing using diffusion models, significantly improving the editing speed and quality. It introduces a One-Step Inversion-and-Editing (OSIE) pipeline that eliminates the need for slow iterative processes, enabling quick modifications. The Background Shield (BG-Shield) technique ensures that the background remains intact while only the desired features are altered, enhancing the overall visual coherence. Additionally, the Sparsified Spatial Cross-Attention (SSCA) mechanism allows for precise edits by minimizing unwanted changes to the background, achieving edits in under 0.2 seconds, which is over 150 times faster than previous methods."
                },
                "zh": {
                    "title": "FlashEditï¼šå®æ—¶é«˜ä¿çœŸå›¾åƒç¼–è¾‘çš„æ–°çªç ´",
                    "desc": "FlashEdit æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶ã€é«˜ä¿çœŸçš„å›¾åƒç¼–è¾‘ã€‚å®ƒé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°æé«˜äº†æ•ˆç‡ï¼šé¦–å…ˆï¼Œé‡‡ç”¨äº†ä¸€ç§ä¸€æ­¥åæ¼”ä¸ç¼–è¾‘ï¼ˆOSIEï¼‰æµç¨‹ï¼Œé¿å…äº†è€—æ—¶çš„è¿­ä»£è¿‡ç¨‹ï¼›å…¶æ¬¡ï¼ŒèƒŒæ™¯ä¿æŠ¤æŠ€æœ¯ï¼ˆBG-Shieldï¼‰ç¡®ä¿äº†èƒŒæ™¯çš„ä¸€è‡´æ€§ï¼Œä»…åœ¨ç¼–è¾‘åŒºåŸŸå†…è¿›è¡Œç‰¹å¾ä¿®æ”¹ï¼›æœ€åï¼Œç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›ï¼ˆSSCAï¼‰æœºåˆ¶ç¡®ä¿äº†ç²¾ç¡®çš„å±€éƒ¨ç¼–è¾‘ï¼Œé˜²æ­¢è¯­ä¹‰ä¿¡æ¯æ³„æ¼åˆ°èƒŒæ™¯ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashEdit åœ¨ä¿æŒèƒŒæ™¯ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œç¼–è¾‘é€Ÿåº¦è¶…è¿‡0.2ç§’ï¼Œæ¯”ä¹‹å‰çš„å¤šæ­¥éª¤æ–¹æ³•å¿«150å€ä»¥ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19768",
            "title": "CHURRO: Making History Readable with an Open-Weight Large\n  Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition",
            "url": "https://huggingface.co/papers/2509.19768",
            "abstract": "CHURRO, a 3B-parameter open-weight vision-language model, outperforms existing models in historical text recognition using the largest dataset to date, CHURRO-DS, and is more cost-effective.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate text recognition for historical documents can greatly advance the study and preservation of cultural heritage. Existing vision-language models (VLMs), however, are designed for modern, standardized texts and are not equipped to read the diverse languages and scripts, irregular layouts, and frequent degradation found in historical materials.   This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for historical text recognition. The model is trained on CHURRO-DS, the largest historical text recognition dataset to date. CHURRO-DS unifies 155 historical corpora comprising 99,491 pages, spanning 22 centuries of textual heritage across 46 language clusters, including historical variants and dead languages.   We evaluate several open-weight and closed VLMs and optical character recognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all other VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and 70.1% (handwritten) normalized Levenshtein similarity, surpassing the second-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being 15.5 times more cost-effective.   By releasing the model and dataset, we aim to enable community-driven research to improve the readability of historical texts and accelerate scholarship.",
            "score": 0,
            "issue_id": 6129,
            "pub_date": "2025-09-24",
            "pub_date_card": {
                "ru": "24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 24",
                "zh": "9æœˆ24æ—¥"
            },
            "hash": "8759baf8bb974573",
            "authors": [
                "Sina J. Semnani",
                "Han Zhang",
                "Xinyan He",
                "Merve TekgÃ¼rler",
                "Monica S. Lam"
            ],
            "affiliations": [
                "Stanford University, Stanford, CA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19768.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#science",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸ“œ",
                "ru": {
                    "title": "CHURRO: AI Ğ´Ğ»Ñ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ²Ğ½Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑĞ»ĞµĞ´Ğ¸Ñ",
                    "desc": "CHURRO - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞµĞ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ CHURRO-DS, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ 99,491 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° 46 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ñ… Ğ·Ğ° 22 Ğ²ĞµĞºĞ°. CHURRO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ VLM Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ (82.3%) Ğ¸ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ½Ğ¾Ğ³Ğ¾ (70.1%) Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾Ğ¿ĞµÑ€ĞµĞ¶Ğ°Ñ Gemini 2.5 Pro Ğ½Ğ° 1.4% Ğ¸ 6.5% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² 15.5 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Unlocking Historical Texts with CHURRO",
                    "desc": "The paper introduces CHURRO, a vision-language model specifically designed for recognizing historical texts. It is trained on CHURRO-DS, the largest dataset for this purpose, which includes a wide variety of historical documents across multiple languages and scripts. CHURRO outperforms existing models in accuracy and cost-effectiveness, achieving high similarity scores in recognizing both printed and handwritten texts. By making CHURRO and its dataset publicly available, the authors aim to foster research that enhances the understanding and preservation of cultural heritage."
                },
                "zh": {
                    "title": "CHURROï¼šå†å²æ–‡æœ¬è¯†åˆ«çš„æ–°çªç ´",
                    "desc": "CHURROæ˜¯ä¸€ç§å…·æœ‰30äº¿å‚æ•°çš„å¼€æ”¾æƒé‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå†å²æ–‡æœ¬è¯†åˆ«ã€‚å®ƒåœ¨CHURRO-DSæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¯¥æ•°æ®é›†æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å†å²æ–‡æœ¬è¯†åˆ«æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª22ä¸ªä¸–çºªçš„155ä¸ªå†å²è¯­æ–™åº“ã€‚CHURROåœ¨è¯†åˆ«å°åˆ·å’Œæ‰‹å†™æ–‡æœ¬æ–¹é¢çš„è¡¨ç°ä¼˜äºæ‰€æœ‰ç°æœ‰æ¨¡å‹ï¼Œä¸”æˆæœ¬æ•ˆç›Šæ›´é«˜ã€‚é€šè¿‡å‘å¸ƒè¯¥æ¨¡å‹å’Œæ•°æ®é›†ï¼Œæˆ‘ä»¬å¸Œæœ›ä¿ƒè¿›ç¤¾åŒºé©±åŠ¨çš„ç ”ç©¶ï¼Œæå‡å†å²æ–‡æœ¬çš„å¯è¯»æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-26.html",
    "link_next": "2025-09-30.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "26.09",
        "en": "09/26",
        "zh": "9æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "30.09",
        "en": "09/30",
        "zh": "9æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 8,
        "#agents": 5,
        "#cv": 5,
        "#rl": 7,
        "#rlhf": 6,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 2,
        "#games": 4,
        "#interpretability": 4,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 11,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}