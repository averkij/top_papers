{
    "date": {
        "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 27",
        "zh": "8æœˆ27æ—¥"
    },
    "time_utc": "2025-08-27 02:22",
    "weekday": 2,
    "issue_id": 5562,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.19247",
            "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
            "url": "https://huggingface.co/papers/2508.19247",
            "abstract": "VoxHammer is a training-free method that performs precise and coherent 3D editing in latent space, ensuring consistency in preserved regions and high-quality overall results.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.",
            "score": 3,
            "issue_id": 5562,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "1b1c0dd833778383",
            "authors": [
                "Lin Li",
                "Zehuan Huang",
                "Haoran Feng",
                "Gengxiong Zhuang",
                "Rui Chen",
                "Chunchao Guo",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "Renmin University of China",
                "Tencent Hunyuan",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19247.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#dataset",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ”¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "VoxHammer - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. VoxHammer Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "VoxHammer: Precision and Coherence in 3D Latent Space Editing",
                    "desc": "VoxHammer is a novel method for editing 3D models in latent space without the need for training. It focuses on maintaining the consistency of unedited regions while ensuring high-quality results in the edited areas. By predicting an inversion trajectory and utilizing key-value tokens, VoxHammer effectively integrates changes while preserving contextual features. This approach outperforms existing techniques, making it valuable for applications in the gaming industry and robotics."
                },
                "zh": {
                    "title": "VoxHammerï¼šæ— è®­ç»ƒçš„ç²¾ç¡®3Dç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "VoxHammeræ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç²¾ç¡®ä¸”è¿è´¯çš„3Dç¼–è¾‘ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†ä¿ç•™åŒºåŸŸçš„ä¸€è‡´æ€§å’Œæ•´ä½“ç»“æœçš„é«˜è´¨é‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒVoxHammerç›´æ¥åœ¨3Dæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç¼–è¾‘ï¼Œé¿å…äº†å¤šè§†å›¾å›¾åƒæ¸²æŸ“å’Œé‡å»ºæ¨¡å‹çš„å¤æ‚æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVoxHammeråœ¨ä¿ç•™åŒºåŸŸçš„3Dä¸€è‡´æ€§å’Œæ•´ä½“è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19205",
            "title": "VibeVoice Technical Report",
            "url": "https://huggingface.co/papers/2508.19205",
            "abstract": "VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models.",
            "score": 1,
            "issue_id": 5562,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "fc5dc3d2ea656b66",
            "authors": [
                "Zhiliang Peng",
                "Jianwei Yu",
                "Wenhui Wang",
                "Yaoyao Chang",
                "Yutao Sun",
                "Li Dong",
                "Yi Zhu",
                "Weijiang Xu",
                "Hangbo Bao",
                "Zehua Wang",
                "Shaohan Huang",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19205.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#long_context",
                    "#diffusion",
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "VibeVoice: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸",
                    "desc": "VibeVoice - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ€ĞµÑ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 90 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ñ€ĞµÑ‡Ğ¸ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ´Ğ¾ 4 Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Encodec, Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ VibeVoice ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 80 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ. VibeVoice Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "VibeVoice: Revolutionizing Multi-Speaker Speech Synthesis",
                    "desc": "VibeVoice is a new model that creates long speeches with multiple speakers using a technique called next-token diffusion. This method generates speech by predicting the next part of the audio in a smart way, making it efficient for continuous data. The model also introduces a special speech tokenizer that compresses data much better than existing models, allowing it to maintain high audio quality while being faster. With VibeVoice, users can generate up to 90 minutes of realistic multi-speaker conversations, outperforming other dialogue systems."
                },
                "zh": {
                    "title": "VibeVoiceï¼šé«˜æ•ˆåˆæˆå¤šè¯´è¯äººé•¿è¯­éŸ³çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "VibeVoiceæ˜¯ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œæ—¨åœ¨åˆæˆé•¿æ—¶é—´çš„å¤šè¯´è¯äººè¯­éŸ³ã€‚å®ƒé‡‡ç”¨äº†ä¸‹ä¸€æ­¥æ‰©æ•£çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªå›å½’ç”Ÿæˆæ½œåœ¨å‘é‡æ¥å»ºæ¨¡è¿ç»­æ•°æ®ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è¿ç»­è¯­éŸ³æ ‡è®°å™¨ï¼Œä¸æµè¡Œçš„Encodecæ¨¡å‹ç›¸æ¯”ï¼Œæ•°æ®å‹ç¼©æé«˜äº†80å€ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„æ€§èƒ½ã€‚VibeVoiceèƒ½å¤Ÿåˆæˆæœ€é•¿å¯è¾¾90åˆ†é’Ÿçš„è¯­éŸ³ï¼Œæ•æ‰çœŸå®çš„å¯¹è¯æ°›å›´ï¼Œè¶…è¶Šäº†å¼€æºå’Œä¸“æœ‰çš„å¯¹è¯æ¨¡å‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-26.html",
    "link_next": "2025-08-28.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "26.08",
        "en": "08/26",
        "zh": "8æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "28.08",
        "en": "08/28",
        "zh": "8æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}