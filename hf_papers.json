{
    "date": {
        "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 1",
        "zh": "9æœˆ1æ—¥"
    },
    "time_utc": "2025-09-01 19:10",
    "weekday": 0,
    "issue_id": 5654,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.21113",
            "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning",
            "url": "https://huggingface.co/papers/2508.21113",
            "abstract": "R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.",
            "score": 80,
            "issue_id": 5638,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "e3b0726caba25eb1",
            "authors": [
                "Jie Jiang",
                "Qi Yang",
                "Bolin Ni",
                "Shiming Xiang",
                "Han Hu",
                "Houwen Peng"
            ],
            "affiliations": [
                "Institute of Automation, CAS",
                "Tencent Hunyuan Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21113.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "R-4B: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "R-4B - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³ Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. R-4B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 25 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "R-4B: Smart Thinking for Efficient Problem Solving",
                    "desc": "R-4B is an innovative multimodal large language model (MLLM) that enhances problem-solving efficiency by using bi-mode annealing and Bi-mode Policy Optimization. This model intelligently decides when to engage in complex reasoning, avoiding unnecessary computations for simpler tasks. By training on a diverse dataset that includes both thinking and non-thinking examples, R-4B learns to optimize its responses based on the complexity of the problem. The results demonstrate that R-4B achieves top performance on 25 benchmarks while maintaining lower computational costs compared to larger models."
                },
                "zh": {
                    "title": "R-4Bï¼šæ™ºèƒ½æ€è€ƒä¸é«˜æ•ˆè§£å†³çš„ç»“åˆ",
                    "desc": "R-4Bæ˜¯ä¸€ç§è‡ªåŠ¨æ€è€ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„å¤æ‚æ€§è‡ªé€‚åº”åœ°å†³å®šä½•æ—¶è¿›è¡Œæ€è€ƒã€‚å®ƒé‡‡ç”¨åŒæ¨¡é€€ç«å’ŒåŒæ¨¡ç­–ç•¥ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹åœ¨è§£å†³é—®é¢˜æ—¶çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒR-4Bèƒ½å¤Ÿåœ¨ç®€å•é—®é¢˜ä¸Šé¿å…å†—ä½™çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR-4Båœ¨25ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21112",
            "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control",
            "url": "https://huggingface.co/papers/2508.21112",
            "abstract": "EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.",
            "score": 54,
            "issue_id": 5638,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "5bbbfa48bbd5fb7c",
            "authors": [
                "Delin Qu",
                "Haoming Song",
                "Qizhi Chen",
                "Zhaoqing Chen",
                "Xianqiang Gao",
                "Xinyi Ye",
                "Qi Lv",
                "Modi Shi",
                "Guanghui Ren",
                "Cheng Ruan",
                "Maoqing Yao",
                "Haoran Yang",
                "Jiacheng Bao",
                "Bin Zhao",
                "Dong Wang"
            ],
            "affiliations": [
                "AgiBot",
                "Fudan University",
                "Northwestern Polytechnical University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21112.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#agents",
                    "#multimodal",
                    "#agi",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: EO-Robotics Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ",
                    "desc": "EO-Robotics Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ÑƒÑ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ EO-1 Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° EO-Data1.5M, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ EO-1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ EO-Data1.5M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Empowering Robots with Multimodal Reasoning and Control",
                    "desc": "EO-Robotics introduces a new model called EO-1, which enhances robot control and reasoning by integrating vision, text, and action in its training process. The EO-Data1.5M dataset, containing over 1.5 million samples, supports this model by providing diverse multimodal data for better understanding and interaction. EO-1 utilizes a unified architecture that processes various inputs simultaneously, allowing for more flexible and human-like responses in real-world scenarios. The research demonstrates that interleaved learning of vision, text, and action significantly improves the robot's ability to perform complex tasks and adapt to different environments."
                },
                "zh": {
                    "title": "æå‡æœºå™¨äººæ§åˆ¶çš„å¤šæ¨¡æ€æ¨ç†æ–°çªç ´",
                    "desc": "EO-Roboticsæ˜¯ä¸€ä¸ªæ–°æ¨¡å‹ï¼ŒåŒ…å«EO-1æ¨¡å‹å’ŒEO-Data1.5Mæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡äº¤æ›¿çš„è§†è§‰-æ–‡æœ¬-åŠ¨ä½œé¢„è®­ç»ƒæ¥æå‡å¤šæ¨¡æ€çš„å…·èº«æ¨ç†å’Œæœºå™¨äººæ§åˆ¶èƒ½åŠ›ã€‚EO-1æ¨¡å‹èƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€è§†é¢‘å’ŒåŠ¨ä½œç­‰å¤šç§è¾“å…¥ï¼Œå±•ç°å‡ºåœ¨å¤šæ¨¡æ€å…·èº«æ¨ç†å’Œæœºå™¨äººæ§åˆ¶æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚è¯¥æ¨¡å‹çš„è®­ç»ƒä¾èµ–äºä¸€ä¸ªåŒ…å«è¶…è¿‡150ä¸‡æ ·æœ¬çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¼ºè°ƒäº¤æ›¿çš„è§†è§‰-æ–‡æœ¬-åŠ¨ä½œç†è§£ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒéªŒè¯äº†äº¤æ›¿å­¦ä¹ åœ¨å¼€æ”¾ä¸–ç•Œç†è§£å’Œæ³›åŒ–ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæä¾›äº†æ„å»ºå…ˆè¿›å…·èº«åŸºç¡€æ¨¡å‹çš„å®è´µè§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18106",
            "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
            "url": "https://huggingface.co/papers/2508.18106",
            "abstract": "A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, preserving full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading LLMs on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, ``fast-thinking'' decoding strategies consistently outperform complex, ``slow-thinking'' reasoning for security patching.",
            "score": 46,
            "issue_id": 5638,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "7b691aaa7b52bcfd",
            "authors": [
                "Keke Lian",
                "Bin Wang",
                "Lei Zhang",
                "Libo Chen",
                "Junjie Wang",
                "Ziming Zhao",
                "Yujiu Yang",
                "Haotong Duan",
                "Haoran Zhao",
                "Shuang Liao",
                "Mingda Guo",
                "Jiazheng Quan",
                "Yilu Zhong",
                "Chenhao He",
                "Zichuan Chen",
                "Jie Wu",
                "Haoling Li",
                "Zhaoxuan Li",
                "Jiongchi Yu",
                "Hui Li",
                "Dong Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "Peking University",
                "Shanghai Jiao Tong University",
                "Singapore Management University",
                "Tencent",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18106.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#security",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "A.S.E: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°",
                    "desc": "A.S.E - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. A.S.E ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Claude-3.7-Sonnet Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½ĞµĞ²ĞµĞ»Ğ¸Ğº."
                },
                "en": {
                    "title": "A.S.E: Elevating Security Evaluation for AI-Generated Code",
                    "desc": "The paper introduces A.S.E, a benchmark designed to evaluate the security of code generated by large language models (LLMs) in a more realistic context. Unlike previous benchmarks that only assess isolated code snippets, A.S.E uses real-world repositories and incorporates expert-defined rules to ensure reproducibility and stability in evaluations. It focuses on repository-level secure code generation, taking into account the entire context, including build systems and cross-file dependencies. The findings indicate that certain models excel in security performance, and simpler decoding strategies are more effective for generating secure code patches."
                },
                "zh": {
                    "title": "A.S.Eï¼šæå‡ä»£ç ç”Ÿæˆå®‰å…¨æ€§çš„åŸºå‡†è¯„ä¼°",
                    "desc": "A.S.Eæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†ï¼Œåˆ©ç”¨çœŸå®ä¸–ç•Œçš„ä»£ç åº“å’Œä¸“å®¶å®šä¹‰çš„è§„åˆ™ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆè¿æ¥è¾“å…¥ä¸Šä¸‹æ–‡çš„è´¨é‡ä¸è¾“å‡ºçš„å®‰å…¨æ€§ã€‚A.S.Eé€šè¿‡æ„å»ºçœŸå®ä»£ç åº“ä¸­çš„ä»»åŠ¡ï¼Œä¿ç•™å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›å¯é‡å¤çš„å®‰å…¨è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒClaude-3.7-Sonnetåœ¨æ•´ä½“è¡¨ç°ä¸Šæœ€ä½³ï¼Œè€ŒQwen3-235B-A22B-Instructåœ¨å®‰å…¨æ€§è¯„åˆ†ä¸Šè¡¨ç°çªå‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20470",
            "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
            "url": "https://huggingface.co/papers/2508.20470",
            "abstract": "Using video data to provide commonsense priors enhances 3D asset generation, enabling spatial consistency and semantic plausibility in 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/.",
            "score": 31,
            "issue_id": 5642,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "2bbd88d7f14f5a78",
            "authors": [
                "Xiaochuan Li",
                "Guoguang Du",
                "Runze Zhang",
                "Liang Jin",
                "Qi Jia",
                "Lihua Lu",
                "Zhenhua Guo",
                "Yaqian Zhao",
                "Haiyang Liu",
                "Tianqi Wang",
                "Changsheng Li",
                "Xiaoli Gong",
                "Rengang Li",
                "Baoyu Fan"
            ],
            "affiliations": [
                "IEIT System Co., Ltd.",
                "Nankai University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20470.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#multimodal",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ´Ğ»Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Droplet3D-4M Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Droplet3D, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… 3D-Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Enhancing 3D Asset Generation with Video Commonsense Priors",
                    "desc": "This paper discusses how using video data can improve the generation of 3D assets by providing commonsense knowledge that helps maintain spatial consistency and semantic accuracy. It highlights the challenge of limited 3D data compared to other media types, like text and images, and proposes leveraging videos as a solution. The authors introduce Droplet3D-4M, a large dataset with multi-view annotations, and a generative model called Droplet3D that can process both images and text. Their experiments show that this approach not only enhances the quality of 3D content but also allows for broader applications in scene generation."
                },
                "zh": {
                    "title": "åˆ©ç”¨è§†é¢‘æ•°æ®æå‡3Dç”Ÿæˆçš„ç©ºé—´ä¸è¯­ä¹‰ä¸€è‡´æ€§",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è§†é¢‘æ•°æ®æ¥å¢å¼º3Dèµ„äº§ç”Ÿæˆï¼Œæä¾›ç©ºé—´ä¸€è‡´æ€§å’Œè¯­ä¹‰åˆç†æ€§ã€‚ç”±äº3Dé¢†åŸŸçš„æ•°æ®ç¨€ç¼ºï¼Œè§†é¢‘ä¸­çš„å¸¸è¯†å…ˆéªŒæˆä¸ºäº†ä¸€ç§æœ‰æ•ˆçš„æ›¿ä»£ç›‘ç£ä¿¡å·ã€‚è§†é¢‘æ•æ‰çš„å¤šè§†è§’ä¿¡æ¯ä¸º3Dç”Ÿæˆæä¾›äº†ç©ºé—´ä¸€è‡´æ€§ï¼Œè€Œä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯åˆ™ä½¿ç”Ÿæˆçš„å†…å®¹æ›´ç¬¦åˆæ–‡æœ¬æç¤ºã€‚æˆ‘ä»¬ä»‹ç»äº†Droplet3D-4Mæ•°æ®é›†å’ŒDroplet3Dç”Ÿæˆæ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨3Då†…å®¹ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13618",
            "title": "TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis",
            "url": "https://huggingface.co/papers/2508.13618",
            "abstract": "TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven talking head synthesis has achieved remarkable photorealism, yet state-of-the-art (SOTA) models exhibit a critical failure: they lack generalization to the full spectrum of human diversity in ethnicity, language, and age groups. We argue that this generalization gap is a direct symptom of limitations in existing training data, which lack the necessary scale, quality, and diversity. To address this challenge, we introduce TalkVid, a new large-scale, high-quality, and diverse dataset containing 1244 hours of video from 7729 unique speakers. TalkVid is curated through a principled, multi-stage automated pipeline that rigorously filters for motion stability, aesthetic quality, and facial detail, and is validated against human judgments to ensure its reliability. Furthermore, we construct and release TalkVid-Bench, a stratified evaluation set of 500 clips meticulously balanced across key demographic and linguistic axes. Our experiments demonstrate that a model trained on TalkVid outperforms counterparts trained on previous datasets, exhibiting superior cross-dataset generalization. Crucially, our analysis on TalkVid-Bench reveals performance disparities across subgroups that are obscured by traditional aggregate metrics, underscoring its necessity for future research. Code and data can be found in https://github.com/FreedomIntelligence/TalkVid",
            "score": 14,
            "issue_id": 5639,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 19",
                "zh": "8æœˆ19æ—¥"
            },
            "hash": "8baf01eb014bc50c",
            "authors": [
                "Shunian Chen",
                "Hejin Huang",
                "Yexin Liu",
                "Zihan Ye",
                "Pengcheng Chen",
                "Chenghao Zhu",
                "Michael Guan",
                "Rongsheng Wang",
                "Junying Chen",
                "Guanbin Li",
                "Ser-Nam Lim",
                "Harry Yang",
                "Benyou Wang"
            ],
            "affiliations": [
                "Sun Yat-sen University",
                "The Chinese University of Hong Kong, Shenzhen",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13618.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#ethics",
                    "#transfer_learning",
                    "#dataset",
                    "#cv",
                    "#data"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "TalkVid: Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TalkVid Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1244 Ñ‡Ğ°ÑĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ 7729 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° TalkVid, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TalkVid-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ…."
                },
                "en": {
                    "title": "Bridging the Diversity Gap in AI with TalkVid",
                    "desc": "The paper introduces TalkVid, a new dataset designed to improve audio-driven talking head synthesis by addressing the generalization gap in existing models. This gap arises from the lack of diversity in training data, which often fails to represent various ethnicities, languages, and age groups. TalkVid consists of 1244 hours of video from 7729 unique speakers, curated through a rigorous process to ensure high quality and diversity. The study also presents TalkVid-Bench, an evaluation set that highlights performance disparities among different demographic groups, emphasizing the importance of diverse datasets in machine learning research."
                },
                "zh": {
                    "title": "TalkVidï¼šæå‡è™šæ‹Ÿäººå¤´åˆæˆçš„å¤šæ ·æ€§ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "TalkVidæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºéŸ³é¢‘çš„è™šæ‹Ÿäººå¤´åˆæˆæŠ€æœ¯ã€‚ç°æœ‰çš„æ¨¡å‹åœ¨å¤„ç†ä¸åŒç§æ—ã€è¯­è¨€å’Œå¹´é¾„ç¾¤ä½“æ—¶å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè®­ç»ƒæ•°æ®çš„è§„æ¨¡å’Œå¤šæ ·æ€§ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTalkVidåŒ…å«äº†1244å°æ—¶æ¥è‡ª7729ä¸ªç‹¬ç‰¹è¯´è¯è€…çš„è§†é¢‘ï¼Œç»è¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè‡ªåŠ¨åŒ–ç­›é€‰ï¼Œç¡®ä¿äº†æ•°æ®çš„ç¨³å®šæ€§å’Œç¾å­¦è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºTalkVidè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºä»¥å¾€çš„æ•°æ®é›†ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†ä¸åŒå­ç¾¤ä½“ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21148",
            "title": "A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers",
            "url": "https://huggingface.co/papers/2508.21148",
            "abstract": "Sci-LLMs are evolving through a co-development with scientific data, addressing unique challenges like multimodal and domain-specific information, and are moving towards autonomous, closed-loop systems in scientific research.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.",
            "score": 12,
            "issue_id": 5645,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "3097c905f2f36541",
            "authors": [
                "Ming Hu",
                "Chenglong Ma",
                "Wei Li",
                "Wanghan Xu",
                "Jiamin Wu",
                "Jucheng Hu",
                "Tianbin Li",
                "Guohang Zhuang",
                "Jiaqi Liu",
                "Yingzhou Lu",
                "Ying Chen",
                "Chaoyang Zhang",
                "Cheng Tan",
                "Jie Ying",
                "Guocheng Wu",
                "Shujian Gao",
                "Pengcheng Chen",
                "Jiashi Lin",
                "Haitao Wu",
                "Lulu Chen",
                "Fengxiang Wang",
                "Yuanyuan Zhang",
                "Xiangyu Zhao",
                "Feilong Tang",
                "Encheng Su",
                "Junzhi Ning",
                "Xinyao Liu",
                "Ye Du",
                "Changkai Ji",
                "Cheng Tang",
                "Huihui Xu",
                "Ziyang Chen",
                "Ziyan Huang",
                "Jiyao Liu",
                "Pengfei Jiang",
                "Yizhou Wang",
                "Chen Tang",
                "Jianyu Wu",
                "Yuchen Ren",
                "Siyuan Yan",
                "Zhonghua Wang",
                "Zhongxing Xu",
                "Shiyan Su",
                "Shangquan Sun",
                "Runkai Zhao",
                "Zhisheng Zhang",
                "Yu Liu",
                "Fudi Wang",
                "Yuanfeng Ji",
                "Yanzhou Su",
                "Hongming Shan",
                "Chunmei Feng",
                "Jiahao Xu",
                "Jiangtao Yan",
                "Wenhao Tang",
                "Diping Song",
                "Lihao Liu",
                "Yanyan Huang",
                "Lequan Yu",
                "Bin Fu",
                "Shujun Wang",
                "Xiaomeng Li",
                "Xiaowei Hu",
                "Yun Gu",
                "Ben Fei",
                "Zhongying Deng",
                "Benyou Wang",
                "Yuewen Cao",
                "Minjie Shen",
                "Haodong Duan",
                "Jie Xu",
                "Yirong Chen",
                "Fang Yan",
                "Hongxia Hao",
                "Jielan Li",
                "Jiajun Du",
                "Yanbo Wang",
                "Imran Razzak",
                "Chi Zhang",
                "Lijun Wu",
                "Conghui He",
                "Zhaohui Lu",
                "Jinhai Huang",
                "Yihao Liu",
                "Fenghua Ling",
                "Yuqiang Li",
                "Aoran Wang",
                "Qihao Zheng",
                "Nanqing Dong",
                "Tianfan Fu",
                "Dongzhan Zhou",
                "Yan Lu",
                "Wenlong Zhang",
                "Jin Ye",
                "Jianfei Cai",
                "Wanli Ouyang",
                "Yu Qiao",
                "Zongyuan Ge",
                "Shixiang Tang",
                "Junjun He",
                "Chunfeng Song",
                "Lei Bai",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Beijing Institute of Heart, Lung and Blood Vessel Diseases",
                "China Pharmaceutical University",
                "Chinese Academy of Sciences",
                "Fudan University",
                "Fuzhou University",
                "Monash University",
                "Purdue University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "South China University",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "The Hong Kong Polytechnic University",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong",
                "UNC-Chapel Hill",
                "University College Dublin",
                "University College London",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21148.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#survey",
                    "#multimodal",
                    "#data",
                    "#dataset",
                    "#science"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Sci-LLMs: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Sci-LLMs) Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ² Ñ‚ĞµÑĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ñ€ĞµÑˆĞ°Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Sci-LLMs, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 270 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ². ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Sci-LLMs, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑÑÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Transforming Science with Autonomous Language Models",
                    "desc": "This paper discusses the evolution of Scientific Large Language Models (Sci-LLMs) and their interaction with scientific data. It highlights the unique challenges posed by scientific datasets, which are often multimodal and domain-specific, requiring specialized approaches compared to general natural language processing. The authors propose a taxonomy of scientific data and review various Sci-LLMs, emphasizing the need for models that can handle heterogeneous and uncertain information. Ultimately, the paper advocates for the development of autonomous systems that can actively engage in scientific research, contributing to a dynamic knowledge base."
                },
                "zh": {
                    "title": "ç§‘å­¦ç ”ç©¶ä¸­çš„æ™ºèƒ½åˆä½œä¼™ä¼´",
                    "desc": "ç§‘å­¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSci-LLMsï¼‰æ­£åœ¨é€šè¿‡ä¸ç§‘å­¦æ•°æ®çš„å…±åŒå‘å±•è€Œä¸æ–­æ¼”å˜ï¼Œè§£å†³å¤šæ¨¡æ€å’Œç‰¹å®šé¢†åŸŸä¿¡æ¯ç­‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç»¼åˆæ¡†æ¶ï¼Œå°†Sci-LLMsçš„å‘å±•è§†ä¸ºæ¨¡å‹ä¸å…¶åŸºç¡€æ•°æ®ä¹‹é—´çš„å…±åŒè¿›åŒ–ã€‚æˆ‘ä»¬å»ºç«‹äº†ç§‘å­¦æ•°æ®çš„ç»Ÿä¸€åˆ†ç±»æ³•å’Œç§‘å­¦çŸ¥è¯†çš„å±‚æ¬¡æ¨¡å‹ï¼Œå¼ºè°ƒäº†ç§‘å­¦è¯­æ–™åº“ä¸ä¸€èˆ¬è‡ªç„¶è¯­è¨€å¤„ç†æ•°æ®é›†ä¹‹é—´çš„åŒºåˆ«ã€‚æœ€åï¼Œæˆ‘ä»¬å±•æœ›äº†å‘é—­ç¯ç³»ç»Ÿçš„è½¬å˜ï¼Œå¼ºè°ƒåŸºäºSci-LLMsçš„è‡ªä¸»ä»£ç†å¦‚ä½•ç§¯æå®éªŒã€éªŒè¯å¹¶ä¸ºä¸æ–­å‘å±•çš„çŸ¥è¯†åº“åšå‡ºè´¡çŒ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21365",
            "title": "Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models",
            "url": "https://huggingface.co/papers/2508.21365",
            "abstract": "Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks.",
            "score": 9,
            "issue_id": 5639,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "cbcbd196468063e9",
            "authors": [
                "Yi Liao",
                "Yu Gu",
                "Yuan Sui",
                "Zining Zhu",
                "Yifan Lu",
                "Guohua Tang",
                "Zhongqian Sun",
                "Wei Yang"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21365.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#optimization",
                    "#reasoning",
                    "#interpretability",
                    "#multimodal",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñ‹: Ğ¾Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğº ÑƒĞ¼ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Think in Games (TiG), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸. TiG Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, TiG Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ."
                },
                "en": {
                    "title": "Empowering Language Models to Learn by Playing",
                    "desc": "The Think in Games (TiG) framework allows large language models (LLMs) to learn how to perform tasks through interaction with game environments, enhancing their procedural knowledge. This approach addresses the gap between knowing facts (declarative knowledge) and knowing how to apply them (procedural knowledge), which LLMs often struggle with in interactive scenarios. By treating decision-making in reinforcement learning as a language modeling task, TiG enables LLMs to create and refine policies based on feedback from their environment. The results show that TiG not only requires less data and computation than traditional methods but also offers clear explanations for its actions, making it more transparent and interpretable."
                },
                "zh": {
                    "title": "é€šè¿‡æ¸¸æˆæ€ç»´æå‡AIçš„å­¦ä¹ èƒ½åŠ›",
                    "desc": "Think in Games (TiG) æ¡†æ¶ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡äº’åŠ¨æ¸¸æˆç¯å¢ƒå‘å±•ç¨‹åºæ€§çŸ¥è¯†ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒTiG åœ¨æ•°æ®å’Œè®¡ç®—éœ€æ±‚ä¸Šæ˜¾è‘—é™ä½ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ¨ç†å’Œè§£é‡Šèƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å†³ç­–è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¯­è¨€æŒ‡å¯¼çš„ç­–ç•¥ï¼Œå¹¶é€šè¿‡ç¯å¢ƒåé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTiG æˆåŠŸå¼¥è¡¥äº†å£°æ˜æ€§çŸ¥è¯†ä¸ç¨‹åºæ€§çŸ¥è¯†ä¹‹é—´çš„å·®è·ï¼Œæå‡äº†å¤æ‚äº’åŠ¨ä»»åŠ¡çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17677",
            "title": "TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training",
            "url": "https://huggingface.co/papers/2508.17677",
            "abstract": "Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t The data mixture used in the pre-training of a language model is a cornerstone of its final performance. However, a static mixing strategy is suboptimal, as the model's learning preferences for various data domains shift dynamically throughout training. Crucially, observing these evolving preferences in a computationally efficient manner remains a significant challenge. To address this, we propose TiKMiX, a method that dynamically adjusts the data mixture according to the model's evolving preferences. TiKMiX introduces Group Influence, an efficient metric for evaluating the impact of data domains on the model. This metric enables the formulation of the data mixing problem as a search for an optimal, influence-maximizing distribution. We solve this via two approaches: TiKMiX-D for direct optimization, and TiKMiX-M, which uses a regression model to predict a superior mixture. We trained models with different numbers of parameters, on up to 1 trillion tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like REGMIX while using just 20% of the computational resources. TiKMiX-M leads to an average performance gain of 2% across 9 downstream benchmarks. Our experiments reveal that a model's data preferences evolve with training progress and scale, and we demonstrate that dynamically adjusting the data mixture based on Group Influence, a direct measure of these preferences, significantly improves performance by mitigating the underdigestion of data seen with static ratios.",
            "score": 7,
            "issue_id": 5639,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "8c118ab21cea1eb2",
            "authors": [
                "Yifan Wang",
                "Binbin Liu",
                "Fengze Liu",
                "Yuanfan Guo",
                "Jiyao Deng",
                "Xuecheng Wu",
                "Weidong Zhou",
                "Xiaohuan Zhou",
                "Taifeng Wang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17677.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TiKMiX - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ¼ĞµÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Group Influence Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. TiKMiX Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑÑ‚Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TiKMiX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Dynamic Data Mixing for Enhanced Language Model Performance",
                    "desc": "This paper introduces TiKMiX, a novel method for dynamically adjusting the data mixture used in training language models. It leverages a metric called Group Influence to assess how different data domains impact the model's learning preferences, which change over time. By optimizing the data mixture based on these evolving preferences, TiKMiX significantly enhances model performance while being computationally efficient. The results show that TiKMiX outperforms existing methods and achieves better results with fewer resources, demonstrating the importance of adaptive data strategies in machine learning."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´æ•°æ®æ··åˆï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTiKMiXçš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ¨¡å‹çš„å­¦ä¹ åå¥½åŠ¨æ€è°ƒæ•´æ•°æ®æ··åˆï¼Œä»¥æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„é™æ€æ•°æ®æ··åˆç­–ç•¥æ— æ³•é€‚åº”æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å˜åŒ–çš„åå¥½ã€‚TiKMiXå¼•å…¥äº†Group Influenceè¿™ä¸€é«˜æ•ˆæŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ä¸åŒæ•°æ®é¢†åŸŸå¯¹æ¨¡å‹çš„å½±å“ï¼Œä»è€Œä¼˜åŒ–æ•°æ®æ··åˆçš„åˆ†å¸ƒã€‚é€šè¿‡TiKMiX-Då’ŒTiKMiX-Mä¸¤ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†åœ¨è®¡ç®—èµ„æºä½¿ç”¨ä¸Šæ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21767",
            "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning",
            "url": "https://huggingface.co/papers/2508.21767",
            "abstract": "UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application.",
            "score": 6,
            "issue_id": 5639,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "4e413654a21d562e",
            "authors": [
                "Zhixiong Zeng",
                "Jing Huang",
                "Liming Zheng",
                "Wenkang Han",
                "Yufeng Zhong",
                "Lei Chen",
                "Longrong Yang",
                "Yingjie Chu",
                "Yuzhi He",
                "Lin Ma"
            ],
            "affiliations": [
                "Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21767.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#agi",
                    "#dataset",
                    "#data",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "UItron: Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "UItron - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. UItron Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "UItron: Advancing GUI Agents for Real-World Applications",
                    "desc": "UItron is an open-source foundational model designed to enhance the capabilities of GUI agents, focusing on visual understanding and task planning. It addresses challenges in developing GUI agents, such as limited operation trajectories and the need for interactive infrastructure. By employing advanced perception, grounding, and planning techniques, UItron systematically improves training through data engineering and a curriculum reinforcement learning framework. The model demonstrates exceptional performance in Chinese app scenarios, filling a gap in existing solutions and moving closer to practical applications of GUI agents."
                },
                "zh": {
                    "title": "UItronï¼šæ¨åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ä»£ç†çš„æœªæ¥",
                    "desc": "UItronæ˜¯ä¸€ä¸ªå¼€æºçš„åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†è®¾è®¡ï¼Œæå‡äº†è§†è§‰ç†è§£å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å…ˆè¿›çš„æ„ŸçŸ¥ã€å®šä½å’Œè§„åˆ’åŠŸèƒ½ï¼Œåœ¨ä¸­æ–‡åº”ç”¨åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚UItronå¼ºè°ƒäº†ç³»ç»Ÿæ•°æ®å·¥ç¨‹å’Œäº¤äº’åŸºç¡€è®¾æ–½åœ¨GUIä»£ç†å¼€å‘ä¸­çš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥å¢å¼ºè®­ç»ƒæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUItronåœ¨ä¸­æ–‡åº”ç”¨åœºæ™¯ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½¿GUIä»£ç†æ›´æ¥è¿‘å®é™…åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21290",
            "title": "Efficient Code Embeddings from Code Generation Models",
            "url": "https://huggingface.co/papers/2508.21290",
            "abstract": "Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  \t\t\t\t\tAI-generated summary \t\t\t\t jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction.",
            "score": 5,
            "issue_id": 5640,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "484a770fa8f460fc",
            "authors": [
                "Daria Kryvosheieva",
                "Saba Sturua",
                "Michael GÃ¼nther",
                "Scott Martens",
                "Han Xiao"
            ],
            "affiliations": [
                "Jina AI GmbH",
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21290.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#transfer_learning",
                    "#data",
                    "#dataset",
                    "#games",
                    "#plp",
                    "#small_models",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼",
                    "desc": "Jina-code-embeddings - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ ĞºĞ¾Ğ´Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ´Ğ°, Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ´Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Code Retrieval with Smart Embeddings",
                    "desc": "Jina-code-embeddings is a new model that creates embeddings for code, which helps in finding code based on natural language questions and identifying similar code snippets. It uses an autoregressive backbone that has been trained on both text and code, allowing it to understand and generate relevant embeddings effectively. The model employs a technique called last-token pooling to produce these embeddings, which enhances its performance. Despite being smaller in size compared to other models, it achieves state-of-the-art results in code retrieval and question-answering tasks."
                },
                "zh": {
                    "title": "åˆ›æ–°ä»£ç åµŒå…¥æ¨¡å‹ï¼Œæå‡ä»£ç æ£€ç´¢ä¸é—®ç­”èƒ½åŠ›",
                    "desc": "Jina-code-embeddings æ˜¯ä¸€ç§æ–°å‹çš„ä»£ç åµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ£€ç´¢ä»£ç ã€è¿›è¡ŒæŠ€æœ¯é—®ç­”ä»¥åŠè¯†åˆ«ä¸åŒç¼–ç¨‹è¯­è¨€ä¸­è¯­ä¹‰ç›¸ä¼¼çš„ä»£ç ç‰‡æ®µã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°ä½¿ç”¨äº†ä¸€ä¸ªåœ¨æ–‡æœ¬å’Œä»£ç ä¸Šé¢„è®­ç»ƒçš„è‡ªå›å½’éª¨å¹²ç½‘ç»œï¼Œé€šè¿‡æœ€åä¸€ä¸ªæ ‡è®°çš„æ± åŒ–ç”ŸæˆåµŒå…¥ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å°½ç®¡æ¨¡å‹ç›¸å¯¹è¾ƒå°ï¼Œä½†ä»èƒ½å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™éªŒè¯äº†è¿™ç§ä»£ç åµŒå…¥æ¨¡å‹æ„å»ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21456",
            "title": "Morae: Proactively Pausing UI Agents for User Choices",
            "url": "https://huggingface.co/papers/2508.21456",
            "abstract": "Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.  \t\t\t\t\tAI-generated summary \t\t\t\t User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.",
            "score": 3,
            "issue_id": 5639,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "3d7bd580c525eaa6",
            "authors": [
                "Yi-Hao Peng",
                "Dingzeyu Li",
                "Jeffrey P. Bigham",
                "Amy Pavel"
            ],
            "affiliations": [
                "Adobe Research",
                "Carnegie Mellon University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21456.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#agi",
                    "#multimodal",
                    "#healthcare",
                    "#agents"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Morae: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Morae - Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Morae Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Morae Ğ²Ğ¾Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Morae Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Empowering BLV Users with Interactive Decision-Making",
                    "desc": "Morae is a user interface (UI) agent designed to improve accessibility for blind and low-vision (BLV) users by actively involving them in decision-making during task execution. Unlike traditional UI agents that operate autonomously, Morae identifies key decision points and pauses to allow users to make informed choices, enhancing their agency. It leverages large multimodal models to interpret user queries and UI elements, ensuring that users are aware of their options. In studies, Morae demonstrated improved task completion and user satisfaction compared to standard agents, showcasing a mixed-initiative approach that balances automation with user preference expression."
                },
                "zh": {
                    "title": "Moraeï¼šè®©ç›²äººå’Œä½è§†åŠ›ç”¨æˆ·å‚ä¸å†³ç­–çš„æ™ºèƒ½ä»£ç†",
                    "desc": "Moraeæ˜¯ä¸€ç§ç”¨æˆ·ç•Œé¢ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡è®©ç›²äººå’Œä½è§†åŠ›ç”¨æˆ·å‚ä¸å†³ç­–è¿‡ç¨‹æ¥æé«˜å¯è®¿é—®æ€§ã€‚å®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ¥ç†è§£ç”¨æˆ·æŸ¥è¯¢å’Œç”¨æˆ·ç•Œé¢å…ƒç´ ï¼Œå¹¶åœ¨ä»»åŠ¡æ‰§è¡Œä¸­è‡ªåŠ¨è¯†åˆ«å†³ç­–ç‚¹ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åšå‡ºé€‰æ‹©ã€‚ä¸ä¼ ç»Ÿçš„å…¨è‡ªåŠ¨ä»£ç†ä¸åŒï¼ŒMoraeåœ¨å…³é”®æ—¶åˆ»æš‚åœï¼Œæç¤ºç”¨æˆ·è¿›è¡Œæ¾„æ¸…ï¼Œä»è€Œå¢å¼ºç”¨æˆ·çš„è‡ªä¸»æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMoraeå¸®åŠ©ç”¨æˆ·å®Œæˆæ›´å¤šä»»åŠ¡ï¼Œå¹¶é€‰æ‹©æ›´ç¬¦åˆä»–ä»¬åå¥½çš„é€‰é¡¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21376",
            "title": "AHELM: A Holistic Evaluation of Audio-Language Models",
            "url": "https://huggingface.co/papers/2508.21376",
            "abstract": "AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness (p=0.01) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 5th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is intended to be a living benchmark and new datasets and models will be added over time.",
            "score": 1,
            "issue_id": 5639,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "fccdd3f91aa4aebd",
            "authors": [
                "Tony Lee",
                "Haoqin Tu",
                "Chi Heem Wong",
                "Zijun Wang",
                "Siwei Yang",
                "Yifan Mai",
                "Yuyin Zhou",
                "Cihang Xie",
                "Percy Liang"
            ],
            "affiliations": [
                "Hitachi America, Ltd.",
                "Stanford University",
                "University of California, Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21376.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#ethics",
                    "#reasoning",
                    "#multimodal",
                    "#audio",
                    "#dataset"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "AHELM: Ğ’ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "AHELM - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ALM). ĞĞ½ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ 10 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Gemini 2.5 Pro Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² 5 Ğ¸Ğ· 10 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ½ĞµÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ASR."
                },
                "en": {
                    "title": "AHELM: A Holistic Benchmark for Evaluating Audio-Language Models",
                    "desc": "AHELM is a new benchmark designed to evaluate audio-language models (ALMs) on multiple important aspects such as fairness, safety, and reasoning. It combines various datasets, including two new ones, PARADE and CoRe-Bench, to provide a comprehensive assessment of ALMs across ten critical dimensions. By standardizing evaluation methods and metrics, AHELM allows for fair comparisons between different models, addressing the limitations of previous benchmarks. The initial testing of 14 ALMs reveals insights into their performance, highlighting both strengths and weaknesses in areas like bias and group fairness."
                },
                "zh": {
                    "title": "AHELMï¼šéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å…¨é¢è¯„ä¼°åŸºå‡†",
                    "desc": "AHELMæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMsï¼‰ï¼Œæ¶µç›–å…¬å¹³æ€§ã€å®‰å…¨æ€§å’Œæ¨ç†ç­‰å¤šä¸ªæ–¹é¢ã€‚è¯¥åŸºå‡†æ•´åˆäº†å¤šç§æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–°çš„åˆæˆéŸ³é¢‘-æ–‡æœ¬æ•°æ®é›†PARADEå’ŒCoRe-Benchï¼Œä»¥å…¨é¢æµ‹é‡ALMsåœ¨éŸ³é¢‘æ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†ç­‰10ä¸ªé‡è¦æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡æ ‡å‡†åŒ–æç¤ºã€æ¨ç†å‚æ•°å’Œè¯„ä¼°æŒ‡æ ‡ï¼ŒAHELMç¡®ä¿äº†æ¨¡å‹ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚æˆ‘ä»¬çš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡Gemini 2.5 Proåœ¨10ä¸ªæ–¹é¢ä¸­æœ‰5ä¸ªæ’åç¬¬ä¸€ï¼Œä½†åœ¨ASRä»»åŠ¡ä¸Šè¡¨ç°å‡ºç¾¤ä½“ä¸å…¬å¹³æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21188",
            "title": "Model-Task Alignment Drives Distinct RL Outcomes",
            "url": "https://huggingface.co/papers/2508.21188",
            "abstract": "Reinforcement learning applied to large language models shows counterintuitive results that depend on pre-existing model-task alignment, with standard RL methods remaining robust in challenging scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in applying reinforcement learning (RL) to large language models (LLMs) have led to substantial progress. In particular, a series of remarkable yet often counterintuitive phenomena have been reported in LLMs, exhibiting patterns not typically observed in traditional RL settings. For example, notable claims include that a single training example can match the performance achieved with an entire dataset, that the reward signal does not need to be very accurate, and that training solely with negative samples can match or even surpass sophisticated reward-based methods. However, the precise conditions under which these observations hold - and, critically, when they fail - remain unclear. In this work, we identify a key factor that differentiates RL observations: whether the pretrained model already exhibits strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated task. Through a systematic and comprehensive examination of a series of counterintuitive claims, supported by rigorous experimental validation across different model architectures and task domains, our findings show that while standard RL training remains consistently robust across settings, many of these counterintuitive results arise only when the model and task already exhibit strong model-task alignment. In contrast, these techniques fail to drive substantial learning in more challenging regimes, where standard RL methods remain effective.",
            "score": 1,
            "issue_id": 5648,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "f7ee68376b3660e0",
            "authors": [
                "Haoze Wu",
                "Cheng Wang",
                "Wenshuo Zhao",
                "Junxian He"
            ],
            "affiliations": [
                "HKUST",
                "National University of Singapore",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21188.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#alignment",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ˜Ğ˜ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ’ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… ÑÑ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Understanding RL Success in Language Models: The Role of Model-Task Alignment",
                    "desc": "This paper explores the application of reinforcement learning (RL) to large language models (LLMs) and reveals unexpected results that depend on the alignment between the model and the task. It highlights that certain counterintuitive phenomena, such as achieving high performance with minimal training data or inaccurate reward signals, occur primarily when there is strong model-task alignment. The authors conduct rigorous experiments to validate these findings across various model architectures and tasks, demonstrating that standard RL methods are robust in challenging scenarios. However, they also note that the effectiveness of these counterintuitive results diminishes when the model-task alignment is weak."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ ä¸æ¨¡å‹ä»»åŠ¡å¯¹é½çš„å¥¥ç§˜",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨ï¼Œå‘ç°äº†ä¸€äº›åç›´è§‰çš„ç°è±¡ã€‚è¿™äº›ç°è±¡çš„å‡ºç°ä¸é¢„è®­ç»ƒæ¨¡å‹ä¸ä»»åŠ¡ä¹‹é—´çš„å¯¹é½ç¨‹åº¦å¯†åˆ‡ç›¸å…³ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨¡å‹ä¸ä»»åŠ¡å…·æœ‰å¼ºå¯¹é½æ—¶ï¼ŒæŸäº›è®­ç»ƒæ–¹æ³•å¯ä»¥å–å¾—æ„æƒ³ä¸åˆ°çš„æ•ˆæœï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­ï¼Œä¼ ç»Ÿçš„RLæ–¹æ³•ä»ç„¶æœ‰æ•ˆã€‚é€šè¿‡ç³»ç»Ÿçš„å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¿™äº›åç›´è§‰ç»“æœçš„æ¡ä»¶å’Œå±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20085",
            "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation",
            "url": "https://huggingface.co/papers/2508.20085",
            "abstract": "HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.",
            "score": 1,
            "issue_id": 5640,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "ad1271c7a1097c84",
            "authors": [
                "Zhecheng Yuan",
                "Tianming Wei",
                "Langzhe Gu",
                "Pu Hua",
                "Tianhai Liang",
                "Yuanpei Chen",
                "Huazhe Xu"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai Qi Zhi Institute",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20085.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#optimization",
                    "#transfer_learning",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº ÑƒĞ¼ĞµĞ»Ñ‹Ğ¼ Ñ€ÑƒĞºĞ°Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°",
                    "desc": "HERMES - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ€ÑƒĞº. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². HERMES ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Bridging Human Motion and Robotic Dexterity with HERMES",
                    "desc": "HERMES is a framework that helps robots learn to mimic human hand movements for better manipulation tasks. It uses reinforcement learning to convert various human motions into actions that robots can perform, even with complex hand structures. The framework also addresses the challenge of transferring learned behaviors from simulated environments to real-world applications. By incorporating advanced localization techniques, HERMES enables robots to operate effectively in diverse and unpredictable settings."
                },
                "zh": {
                    "title": "HERMESï¼šäººæœºåä½œçš„çµå·§æ“ä½œæ–°æ¡†æ¶",
                    "desc": "HERMESæ˜¯ä¸€ä¸ªäººæœºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å°†äººç±»æ‰‹éƒ¨åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººè¡Œä¸ºã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œä»¿çœŸåˆ°ç°å®çš„è½¬ç§»æŠ€æœ¯ï¼Œå¸®åŠ©æœºå™¨äººåœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­è¿›è¡Œçµæ´»çš„æ“ä½œã€‚HERMESèƒ½å¤Ÿå°†æ¥è‡ªå¤šä¸ªæ¥æºçš„äººç±»æ‰‹éƒ¨åŠ¨ä½œç»Ÿä¸€è½¬åŒ–ä¸ºå¯è¡Œçš„æœºå™¨äººè¡Œä¸ºï¼Œå¹¶é€šè¿‡æ·±åº¦å›¾åƒå®ç°æ›´å¥½çš„ç°å®é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHERMESåœ¨å„ç§å¤æ‚çš„ç§»åŠ¨åŒæ‰‹çµå·§æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17380",
            "title": "Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula\n  Discovery",
            "url": "https://huggingface.co/papers/2508.17380",
            "abstract": "VIPER-R1, a multimodal model combining visual perception, trajectory data, and symbolic reasoning, discovers physical laws with higher accuracy and interpretability than existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated discovery of physical laws from observational data in the real world is a grand challenge in AI. Current methods, relying on symbolic regression or LLMs, are limited to uni-modal data and overlook the rich, visual phenomenological representations of motion that are indispensable to physicists. This \"sensory deprivation\" severely weakens their ability to interpret the inherent spatio-temporal patterns within dynamic phenomena. To address this gap, we propose VIPER-R1, a multimodal model that performs Visual Induction for Physics-based Equation Reasoning to discover fundamental symbolic formulas. It integrates visual perception, trajectory data, and symbolic reasoning to emulate the scientific discovery process. The model is trained via a curriculum of Motion Structure Induction (MSI), using supervised fine-tuning to interpret kinematic phase portraits and to construct hypotheses guided by a Causal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration (RGSC) to refine the formula structure with reinforcement learning. During inference, the trained VIPER-R1 acts as an agent: it first posits a high-confidence symbolic ansatz, then proactively invokes an external symbolic regression tool to perform Symbolic Residual Realignment (SR^2). This final step, analogous to a physicist's perturbation analysis, reconciles the theoretical model with empirical data. To support this research, we introduce PhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that VIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy and interpretability, enabling more precise discovery of physical laws. Project page: https://jiaaqiliu.github.io/VIPER-R1/",
            "score": 1,
            "issue_id": 5648,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "87afb7a989f84636",
            "authors": [
                "Jiaqi Liu",
                "Songning Lai",
                "Pengze Li",
                "Di Yu",
                "Wenjie Zhou",
                "Yiyang Zhou",
                "Peng Xia",
                "Zijun Wang",
                "Xi Chen",
                "Shixiang Tang",
                "Lei Bai",
                "Wanli Ouyang",
                "Mingyu Ding",
                "Huaxiu Yao",
                "Aoran Wang"
            ],
            "affiliations": [
                "Fudan University",
                "HKUST (Guangzhou)",
                "Nankai University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Innovation Institute",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "UC Santa Cruz",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17380.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#rl",
                    "#multimodal",
                    "#interpretability",
                    "#dataset",
                    "#science"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "VIPER-R1: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸",
                    "desc": "VIPER-R1 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ², ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑƒÑ€ÑĞ° Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. VIPER-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ PhysSymbol Ñ 5000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Physical Laws with Multimodal Insights",
                    "desc": "VIPER-R1 is a multimodal model designed to discover physical laws by integrating visual perception, trajectory data, and symbolic reasoning. Unlike traditional methods that focus on single data types, VIPER-R1 utilizes a combination of visual and motion data to better understand complex physical phenomena. The model employs a structured training approach, including Motion Structure Induction and reinforcement learning, to refine its symbolic formulas. Experimental results demonstrate that VIPER-R1 achieves higher accuracy and interpretability compared to existing models, making it a significant advancement in automated scientific discovery."
                },
                "zh": {
                    "title": "VIPER-R1ï¼šå¤šæ¨¡æ€æ¨¡å‹åŠ©åŠ›ç‰©ç†å®šå¾‹å‘ç°",
                    "desc": "VIPER-R1æ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰æ„ŸçŸ¥ã€è½¨è¿¹æ•°æ®å’Œç¬¦å·æ¨ç†ï¼Œèƒ½å¤Ÿä»¥æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§å‘ç°ç‰©ç†å®šå¾‹ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç¬¦å·å›å½’æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šå¸¸åªå¤„ç†å•ä¸€æ¨¡æ€æ•°æ®ï¼Œå¿½è§†äº†è¿åŠ¨çš„ä¸°å¯Œè§†è§‰è¡¨å¾ã€‚VIPER-R1é€šè¿‡è¿åŠ¨ç»“æ„å½’çº³ï¼ˆMSIï¼‰å’Œå¥–åŠ±å¼•å¯¼çš„ç¬¦å·æ ¡å‡†ï¼ˆRGSCï¼‰æ¥è®­ç»ƒï¼Œæ¨¡æ‹Ÿç§‘å­¦å‘ç°è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒVIPER-R1åœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å‘ç°ç‰©ç†å®šå¾‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14197",
            "title": "CLIPSym: Delving into Symmetry Detection with CLIP",
            "url": "https://huggingface.co/papers/2508.14197",
            "abstract": "CLIPSym, a vision-language model using CLIP, enhances symmetry detection through a rotation-equivariant decoder and semantic-aware prompting, outperforming existing methods on standard datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. With the recent advances in vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and G-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique. The code is available at https://github.com/timyoung2333/CLIPSym.",
            "score": 1,
            "issue_id": 5642,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 19",
                "zh": "8æœˆ19æ—¥"
            },
            "hash": "fe61d1db34c28a8d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#optimization",
                    "#multimodal",
                    "#cv",
                    "#games"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "CLIPSym: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "CLIPSym - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CLIP. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° CLIP Ñ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¸ G-ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑƒÑ‡ĞµÑ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. CLIPSym Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Symmetry Detection with CLIPSym",
                    "desc": "CLIPSym is a vision-language model that improves the detection of symmetry in images by using a rotation-equivariant decoder and a new prompting technique. It builds on the capabilities of the CLIP model, which combines image and text understanding, to enhance symmetry detection by incorporating semantic information from image descriptions. The model employs a hybrid architecture that integrates Transformer and G-Convolution to effectively recognize both rotation and reflection symmetries. Experimental results demonstrate that CLIPSym surpasses existing methods on multiple standard datasets, confirming the effectiveness of its innovative approaches."
                },
                "zh": {
                    "title": "CLIPSymï¼šæå‡å¯¹ç§°æ€§æ£€æµ‹çš„æ–°æ–¹æ³•",
                    "desc": "CLIPSymæ˜¯ä¸€ç§åŸºäºCLIPçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å¯¹ç§°æ€§æ£€æµ‹çš„èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ—‹è½¬ç­‰å˜è§£ç å™¨å’Œè¯­ä¹‰æ„ŸçŸ¥æç¤ºæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è‡ªç„¶å›¾åƒæè¿°ä¸­çš„å¯¹ç§°æ€§çº¿ç´¢ã€‚é€šè¿‡ç»“åˆTransformerå’ŒG-å·ç§¯çš„æ··åˆç»“æ„ï¼ŒCLIPSymèƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹æ—‹è½¬å’Œåå°„å¯¹ç§°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLIPSymåœ¨ä¸‰ä¸ªæ ‡å‡†å¯¹ç§°æ€§æ£€æµ‹æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21172",
            "title": "Deep Residual Echo State Networks: exploring residual orthogonal\n  connections in untrained Recurrent Neural Networks",
            "url": "https://huggingface.co/papers/2508.21172",
            "abstract": "Deep Residual Echo State Networks (DeepResESNs) enhance long-term temporal modeling and memory capacity through hierarchical untrained residual layers, outperforming traditional shallow and deep reservoir computing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Echo State Networks (ESNs) are a particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce a novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. A thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on a variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC.",
            "score": 0,
            "issue_id": 5650,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "a0a4fea9c9a49fe6",
            "authors": [
                "Matteo Pinna",
                "Andrea Ceni",
                "Claudio Gallicchio"
            ],
            "affiliations": [
                "Department of Computer Science, University of Pisa, 56127 Pisa, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21172.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#math",
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ĞÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ­Ñ…Ğ¾-Ğ“Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¡ĞµÑ‚Ğ¸ (DeepResESNs), Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ½ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞµÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ·ĞµÑ€Ğ²ÑƒĞ°Ñ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Boosting Memory with Deep Residual Echo State Networks",
                    "desc": "Deep Residual Echo State Networks (DeepResESNs) are a new type of untrained Recurrent Neural Network designed to improve the handling of long-term temporal data. They use a structure of hierarchical residual layers that do not require training, which enhances their memory capacity. This paper explores how different configurations of these residual connections can affect the network's performance and stability. The results demonstrate that DeepResESNs outperform traditional reservoir computing methods in various time series tasks."
                },
                "zh": {
                    "title": "æ·±æ®‹å·®ç½‘ç»œï¼Œæå‡è®°å¿†ä¸å»ºæ¨¡èƒ½åŠ›",
                    "desc": "æ·±æ®‹å·®å›å£°çŠ¶æ€ç½‘ç»œï¼ˆDeepResESNsï¼‰é€šè¿‡å±‚æ¬¡åŒ–çš„æœªè®­ç»ƒæ®‹å·®å±‚å¢å¼ºäº†é•¿æœŸæ—¶é—´å»ºæ¨¡å’Œè®°å¿†èƒ½åŠ›ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æµ…å±‚å’Œæ·±å±‚æ°´åº“è®¡ç®—æ–¹æ³•ã€‚å›å£°çŠ¶æ€ç½‘ç»œï¼ˆESNsï¼‰æ˜¯ä¸€ç§ç‰¹æ®Šç±»å‹çš„æœªè®­ç»ƒé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œåœ¨æ°´åº“è®¡ç®—æ¡†æ¶ä¸­å› å…¶å¿«é€Ÿé«˜æ•ˆçš„å­¦ä¹ è€Œå—åˆ°æ¬¢è¿ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ESNsåœ¨å¤„ç†é•¿æœŸä¿¡æ¯æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´æ®‹å·®è¿æ¥çš„æ–°å‹æ·±åº¦æœªè®­ç»ƒRNNï¼Œå±•ç¤ºäº†åˆ©ç”¨æœªè®­ç»ƒçš„æ®‹å·®é€’å½’å±‚çš„å±‚æ¬¡ç»“æ„æ˜¾è‘—æå‡äº†è®°å¿†å®¹é‡å’Œé•¿æœŸæ—¶é—´å»ºæ¨¡èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19600",
            "title": "Quantization Robustness to Input Degradations for Object Detection",
            "url": "https://huggingface.co/papers/2508.19600",
            "abstract": "Post-training quantization of YOLO models is evaluated for robustness to real-world degradations, with a focus on the effectiveness of a degradation-aware calibration strategy for Static INT8 quantization.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training quantization (PTQ) is crucial for deploying efficient object detection models, like YOLO, on resource-constrained devices. However, the impact of reduced precision on model robustness to real-world input degradations such as noise, blur, and compression artifacts is a significant concern. This paper presents a comprehensive empirical study evaluating the robustness of YOLO models (nano to extra-large scales) across multiple precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8 (TensorRT). We introduce and evaluate a degradation-aware calibration strategy for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix of clean and synthetically degraded images. Models were benchmarked on the COCO dataset under seven distinct degradation conditions (including various types and levels of noise, blur, low contrast, and JPEG compression) and a mixed-degradation scenario. Results indicate that while Static INT8 TensorRT engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop (~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did not yield consistent, broad improvements in robustness over standard clean-data calibration across most models and degradations. A notable exception was observed for larger model scales under specific noise conditions, suggesting model capacity may influence the efficacy of this calibration approach. These findings highlight the challenges in enhancing PTQ robustness and provide insights for deploying quantized detectors in uncontrolled environments. All code and evaluation tables are available at https://github.com/AllanK24/QRID.",
            "score": 0,
            "issue_id": 5649,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "ab400b3d8dc110c8",
            "authors": [
                "Toghrul Karimov",
                "Hassan Imani",
                "Allan Kazakov"
            ],
            "affiliations": [
                "Bahcesehir University, Baku, Azerbaijan",
                "Bahcesehir University, Istanbul, Turkey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19600.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#security",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ YOLO: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ YOLO Ğ½Ğ° Ğ¸Ñ… ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ INT8 ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… COCO Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑˆÑƒĞ¼, Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğ¸ JPEG-ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğµ Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing YOLO Robustness with Degradation-Aware Calibration",
                    "desc": "This paper investigates the post-training quantization (PTQ) of YOLO object detection models to assess their robustness against real-world image degradations. It specifically focuses on a degradation-aware calibration strategy for Static INT8 quantization, which aims to improve model performance when faced with various input distortions like noise and blur. The study evaluates different precision formats and benchmarks the models on the COCO dataset under multiple degradation scenarios. Results show that while Static INT8 quantization improves processing speed, the proposed calibration method does not consistently enhance robustness, particularly for smaller models, although larger models may benefit under certain conditions."
                },
                "zh": {
                    "title": "æå‡YOLOæ¨¡å‹é²æ£’æ€§çš„é‡åŒ–ç­–ç•¥",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†YOLOæ¨¡å‹çš„åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰åœ¨çœŸå®ä¸–ç•Œé€€åŒ–ä¸‹çš„é²æ£’æ€§ï¼Œç‰¹åˆ«å…³æ³¨é™æ€INT8é‡åŒ–çš„é€€åŒ–æ„ŸçŸ¥æ ¡å‡†ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶é™æ€INT8 TensorRTå¼•æ“åœ¨å¹²å‡€æ•°æ®ä¸Šæä¾›äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼Œä½†åœ¨å¤§å¤šæ•°æ¨¡å‹å’Œé€€åŒ–æ¡ä»¶ä¸‹ï¼Œé€€åŒ–æ„ŸçŸ¥æ ¡å‡†å¹¶æœªå¸¦æ¥ä¸€è‡´çš„é²æ£’æ€§æ”¹å–„ã€‚å¯¹äºç‰¹å®šå™ªå£°æ¡ä»¶ä¸‹çš„å¤§å‹æ¨¡å‹ï¼Œæ ¡å‡†æ•ˆæœæœ‰æ‰€ä¸åŒï¼Œè¡¨æ˜æ¨¡å‹å®¹é‡å¯èƒ½å½±å“æ ¡å‡†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ä¸å—æ§ç¯å¢ƒä¸­éƒ¨ç½²é‡åŒ–æ£€æµ‹å™¨æä¾›äº†é‡è¦è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17008",
            "title": "EduRABSA: An Education Review Dataset for Aspect-based Sentiment\n  Analysis Tasks",
            "url": "https://huggingface.co/papers/2508.17008",
            "abstract": "EduRABSA is a public dataset and ASQE-DPT is a tool for aspect-based sentiment analysis in education reviews, addressing the lack of resources in this domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Every year, most educational institutions seek and receive an enormous volume of text feedback from students on courses, teaching, and overall experience. Yet, turning this raw feedback into useful insights is far from straightforward. It has been a long-standing challenge to adopt automatic opinion mining solutions for such education review text data due to the content complexity and low-granularity reporting requirements. Aspect-based Sentiment Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level opinion mining capabilities. However, existing ABSA research and resources are very heavily focused on the commercial domain. In education, they are scarce and hard to develop due to limited public datasets and strict data protection. A high-quality, annotated dataset is urgently needed to advance research in this under-resourced area. In this work, we present EduRABSA (Education Review ABSA), the first public, annotated ABSA education review dataset that covers three review subject types (course, teaching staff, university) in the English language and all main ABSA tasks, including the under-explored implicit aspect and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool), an offline, lightweight, installation-free manual data annotation tool that generates labelled datasets for comprehensive ABSA tasks from a single-task annotation. Together, these resources contribute to the ABSA community and education domain by removing the dataset barrier, supporting research transparency and reproducibility, and enabling the creation and sharing of further resources. The dataset, annotation tool, and scripts and statistics for dataset processing and sampling are available at https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.",
            "score": 0,
            "issue_id": 5649,
            "pub_date": "2025-08-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 23",
                "zh": "8æœˆ23æ—¥"
            },
            "hash": "22b921b4352ed731",
            "authors": [
                "Yan Cathy Hua",
                "Paul Denny",
                "JÃ¶rg Wicker",
                "Katerina Taskova"
            ],
            "affiliations": [
                "School of Computer Science, University of Auckland, New Zealand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17008.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#low_resource",
                    "#data"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ñ…",
                    "desc": "EduRABSA - ÑÑ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ñ…. ASQE-DPT - Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ABSA Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ¾ÑĞ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ABSA Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹ Ğ¾ ĞºÑƒÑ€ÑĞ°Ñ…, Ğ¿Ñ€ĞµĞ¿Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ĞµĞ»ÑÑ… Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ."
                },
                "en": {
                    "title": "Empowering Education Insights with EduRABSA and ASQE-DPT",
                    "desc": "EduRABSA is a newly introduced public dataset specifically designed for aspect-based sentiment analysis (ABSA) in educational reviews, addressing the scarcity of resources in this area. The dataset includes annotated reviews covering various subjects such as courses, teaching staff, and universities, facilitating detailed opinion mining. Additionally, the ASQE-DPT tool allows for efficient manual data annotation, enabling researchers to create labeled datasets for comprehensive ABSA tasks. This work aims to enhance research in education by providing essential resources that promote transparency and reproducibility in sentiment analysis."
                },
                "zh": {
                    "title": "æ¨åŠ¨æ•™è‚²è¯„è®ºæƒ…æ„Ÿåˆ†æçš„èµ„æºåˆ›æ–°",
                    "desc": "EduRABSAæ˜¯ä¸€ä¸ªå…¬å…±æ•°æ®é›†ï¼Œä¸“æ³¨äºæ•™è‚²è¯„è®ºçš„åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰ï¼Œè§£å†³äº†è¯¥é¢†åŸŸèµ„æºåŒ®ä¹çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†è¯¾ç¨‹ã€æ•™å­¦äººå‘˜å’Œå¤§å­¦ä¸‰ç§è¯„è®ºä¸»é¢˜ï¼Œå¹¶æ”¯æŒå¤šç§ABSAä»»åŠ¡ï¼ŒåŒ…æ‹¬éšå«æ–¹é¢å’Œéšå«æ„è§çš„æå–ã€‚ASQE-DPTæ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ‰‹åŠ¨æ•°æ®æ³¨é‡Šå·¥å…·ï¼Œå¯ä»¥ä»å•ä¸€ä»»åŠ¡æ³¨é‡Šç”Ÿæˆæ ‡è®°æ•°æ®é›†ï¼Œä¿ƒè¿›äº†æ•™è‚²é¢†åŸŸçš„ç ”ç©¶é€æ˜æ€§å’Œå¯é‡å¤æ€§ã€‚é€šè¿‡è¿™äº›èµ„æºï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨æ•™è‚²è¯„è®ºçš„æƒ…æ„Ÿåˆ†æç ”ç©¶ï¼Œå¡«è¡¥ç°æœ‰çš„ç ”ç©¶ç©ºç™½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-29.html",
    "link_next": "2025-09-02.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "29.08",
        "en": "08/29",
        "zh": "8æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "02.09",
        "en": "09/02",
        "zh": "9æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 12,
        "#data": 6,
        "#benchmark": 7,
        "#agents": 6,
        "#cv": 2,
        "#rl": 5,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 9,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 1,
        "#agi": 3,
        "#games": 3,
        "#interpretability": 2,
        "#reasoning": 7,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 3,
        "#security": 2,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    }
}