{
    "date": {
        "ru": "11 декабря",
        "en": "December 11",
        "zh": "12月11日"
    },
    "time_utc": "2024-12-11 12:19",
    "weekday": 2,
    "issue_id": 1066,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.05210",
            "title": "Evaluating and Aligning CodeLLMs on Human Preference",
            "url": "https://huggingface.co/papers/2412.05210",
            "abstract": "Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\\url{https://codearenaeval.github.io/ }",
            "score": 38,
            "issue_id": 1060,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "0232aabe01d37826",
            "authors": [
                "Jian Yang",
                "Jiaxi Yang",
                "Ke Jin",
                "Yibo Miao",
                "Lei Zhang",
                "Liqun Yang",
                "Zeyu Cui",
                "Yichang Zhang",
                "Binyuan Hui",
                "Junyang Lin"
            ],
            "affiliations": [
                "Alibaba Group",
                "Shanghai Jiao Tong University",
                "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05210.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#alignment",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "🏆",
                "ru": {
                    "title": "CodeArena: новый стандарт оценки ИИ-помощников программиста",
                    "desc": "В статье представлен новый бенчмарк CodeArena для оценки языковых моделей кода с учетом предпочтений пользователей. Авторы создали набор из 397 высококачественных примеров задач по программированию на 44 языках. Также был разработан синтетический набор инструкций SynCode-Instruct объемом около 20 миллиардов токенов для дообучения моделей. Эксперименты показали значительный разрыв в производительности между открытыми и проприетарными моделями кода, подчеркивая важность согласования с предпочтениями человека."
                },
                "en": {
                    "title": "Bridging Code Generation and Human Preferences with CodeArena",
                    "desc": "This paper discusses the advancements in code large language models (codeLLMs) for code generation tasks. It highlights the limitations of existing benchmarks that primarily focus on code correctness without considering human preferences in real-world applications. To address this, the authors introduce CodeArena, a human-curated benchmark that includes diverse coding tasks across multiple programming languages. Additionally, they present SynCode-Instruct, a large synthetic instruction dataset that enhances the training of codeLLMs, revealing significant performance differences between various models when evaluated against human-aligned benchmarks."
                },
                "zh": {
                    "title": "提升代码生成模型与人类偏好的对齐",
                    "desc": "本文介绍了一种新的基准测试工具CodeArena，用于评估代码生成大语言模型（code LLMs）的性能。现有的基准测试主要关注代码片段的正确性，而忽视了与人类偏好的对齐。CodeArena通过提供397个高质量样本，涵盖40个类别和44种编程语言，模拟真实编码任务的复杂性和多样性。研究还提出了一个多样化的合成指令语料库SynCode-Instruct，以验证大规模合成指令微调的有效性，结果显示开源代码LLMs与专有LLMs之间存在显著的性能差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07589",
            "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation",
            "url": "https://huggingface.co/papers/2412.07589",
            "abstract": "Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limitations, we propose a new task: customized manga generation and introduce DiffSensei, an innovative framework specifically designed for generating manga with dynamic multi-character control. DiffSensei integrates a diffusion-based image generator with a multimodal large language model (MLLM) that acts as a text-compatible identity adapter. Our approach employs masked cross-attention to seamlessly incorporate character features, enabling precise layout control without direct pixel transfer. Additionally, the MLLM-based adapter adjusts character features to align with panel-specific text cues, allowing flexible adjustments in character expressions, poses, and actions. We also introduce MangaZero, a large-scale dataset tailored to this task, containing 43,264 manga pages and 427,147 annotated panels, supporting the visualization of varied character interactions and movements across sequential frames. Extensive experiments demonstrate that DiffSensei outperforms existing models, marking a significant advancement in manga generation by enabling text-adaptable character customization. The project page is https://jianzongwu.github.io/projects/diffsensei/.",
            "score": 22,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "8a1bb8ed9ae040f2",
            "authors": [
                "Jianzong Wu",
                "Chao Tang",
                "Jingbo Wang",
                "Yanhong Zeng",
                "Xiangtai Li",
                "Yunhai Tong"
            ],
            "affiliations": [
                "Bytedance Seed Project",
                "Nanyang Technological University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07589.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#story_generation",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "DiffSensei: ИИ-художник манги с контролем персонажей",
                    "desc": "Статья представляет DiffSensei - новую систему для генерации манги с возможностью контроля нескольких персонажей. DiffSensei объединяет диффузионный генератор изображений с мультимодальной языковой моделью для адаптации идентичности персонажей. Система использует маскированное кросс-внимание для точного контроля компоновки и позволяет гибко настраивать выражения и позы персонажей. Авторы также представили набор данных MangaZero для обучения подобных моделей."
                },
                "en": {
                    "title": "Dynamic Manga Generation with Character Control",
                    "desc": "This paper presents DiffSensei, a novel framework for generating manga that allows for dynamic control of multiple characters based on textual descriptions. It combines a diffusion-based image generator with a multimodal large language model (MLLM) to effectively manage character identities and interactions in complex scenes. The framework utilizes masked cross-attention to integrate character features, enabling precise layout control and adjustments in expressions, poses, and actions according to panel-specific text cues. Additionally, the authors introduce MangaZero, a comprehensive dataset designed for this task, which significantly enhances the training and evaluation of manga generation models."
                },
                "zh": {
                    "title": "定制漫画生成的新突破",
                    "desc": "本论文提出了一种新的任务：定制漫画生成，并介绍了DiffSensei框架，旨在实现动态多角色控制的漫画生成。该框架结合了基于扩散的图像生成器和多模态大语言模型（MLLM），通过掩蔽交叉注意力机制有效整合角色特征。DiffSensei能够根据面板特定的文本提示灵活调整角色的表情、姿势和动作，从而实现精确的布局控制。我们还推出了MangaZero数据集，包含43,264页漫画和427,147个注释面板，支持多样化角色交互和动作的可视化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07730",
            "title": "STIV: Scalable Text and Image Conditioned Video Generation",
            "url": "https://huggingface.co/papers/2412.07730",
            "abstract": "The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in a simple and scalable text-image-conditioned video generation method, named STIV. Our framework integrates image condition into a Diffusion Transformer (DiT) through frame replacement, while incorporating text conditioning via a joint image-text conditional classifier-free guidance. This design enables STIV to perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks simultaneously. Additionally, STIV can be easily extended to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation, etc. With comprehensive ablation studies on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple design. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V, surpassing both leading open and closed-source models like CogVideoX-5B, Pika, Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result of 90.1 on VBench I2V task at 512 resolution. By providing a transparent and extensible recipe for building cutting-edge video generation models, we aim to empower future research and accelerate progress toward more versatile and reliable video generation solutions.",
            "score": 18,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "a43bca3bdca1a7ba",
            "authors": [
                "Zongyu Lin",
                "Wei Liu",
                "Chen Chen",
                "Jiasen Lu",
                "Wenze Hu",
                "Tsu-Jui Fu",
                "Jesse Allardice",
                "Zhengfeng Lai",
                "Liangchen Song",
                "Bowen Zhang",
                "Cha Chen",
                "Yiran Fei",
                "Yifan Jiang",
                "Lezhi Li",
                "Yizhou Sun",
                "Kai-Wei Chang",
                "Yinfei Yang"
            ],
            "affiliations": [
                "Apple",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07730.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#games",
                    "#diffusion",
                    "#training",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "STIV: универсальный рецепт для масштабируемой генерации видео",
                    "desc": "Статья представляет комплексное исследование в области генерации видео, предлагая метод STIV. STIV интегрирует условие изображения в Diffusion Transformer и использует совместное текстово-изображательное условное безклассификаторное управление. Этот подход позволяет STIV выполнять задачи как текст-в-видео (T2V), так и текст-изображение-в-видео (TI2V). Модель STIV объемом 8.7B показывает высокие результаты в задачах T2V и I2V, превосходя ведущие открытые и закрытые модели."
                },
                "en": {
                    "title": "STIV: Simplifying Video Generation with Text and Image Conditioning",
                    "desc": "This paper introduces STIV, a novel framework for video generation that effectively combines text and image inputs to create videos. It utilizes a Diffusion Transformer architecture, enhancing its capabilities through a method called frame replacement and a joint image-text conditional classifier-free guidance. STIV is designed to handle both text-to-video (T2V) and text-image-to-video (TI2V) tasks, making it versatile for various applications like video prediction and frame interpolation. The model demonstrates impressive performance metrics, outperforming existing models while maintaining a straightforward design, thus providing a clear pathway for future advancements in video generation."
                },
                "zh": {
                    "title": "视频生成的简单与强大",
                    "desc": "本研究探讨了视频生成模型的架构、训练方法和数据策划策略之间的关系，提出了一种简单且可扩展的视频生成方法STIV。STIV通过帧替换将图像条件集成到扩散变换器中，同时利用无条件分类器引导实现文本条件。该框架能够同时执行文本到视频（T2V）和文本-图像到视频（TI2V）任务，并且可以扩展到视频预测、帧插值等多种应用。通过全面的消融研究，STIV在多个任务上表现出色，展示了其强大的性能和简单的设计。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04653",
            "title": "Hidden in the Noise: Two-Stage Robust Watermarking for Images",
            "url": "https://huggingface.co/papers/2412.04653",
            "abstract": "As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks.",
            "score": 13,
            "issue_id": 1060,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "8032f89319a70b88",
            "authors": [
                "Kasra Arabi",
                "Benjamin Feuer",
                "R. Teal Witter",
                "Chinmay Hegde",
                "Niv Cohen"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04653.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#security",
                    "#rag",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Невидимые и неуязвимые: революция в защите AI-изображений",
                    "desc": "Статья представляет новый метод водяных знаков для изображений, генерируемых искусственным интеллектом. Авторы предлагают двухэтапный подход, основанный на использовании начального шума диффузионной модели и дополнительных паттернов Фурье. Этот метод позволяет эффективно обнаруживать водяные знаки без искажения распределения генерируемых изображений. Предложенный подход демонстрирует высокую устойчивость к атакам подделки и удаления, превосходя существующие методы."
                },
                "en": {
                    "title": "Robust Watermarking for AI-Generated Images",
                    "desc": "This paper presents a novel approach to image watermarking that aims to enhance the security of AI-generated content against forgery and removal attacks. The authors introduce a distortion-free watermarking method that utilizes the initial noise from a diffusion model, which helps to avoid revealing watermarking techniques. They propose a two-stage framework that first augments the initial noise with Fourier patterns to embed watermark information, and then efficiently detects the watermark by comparing it to a group of initial noises. This method demonstrates significant robustness against various attacks, setting a new standard in the field of image watermarking."
                },
                "zh": {
                    "title": "无失真水印，保护AI生成内容的未来",
                    "desc": "随着图像生成技术的不断进步，深度伪造成为社会讨论的热点。图像水印技术可以帮助模型拥有者检测和标记他们生成的内容，从而减少潜在的危害。然而，现有的水印方法在面对伪造和去除攻击时仍然存在脆弱性。本文提出了一种无失真水印方法，并通过两阶段框架提高了水印的检测效率，显著增强了对各种攻击的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07774",
            "title": "UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics",
            "url": "https://huggingface.co/papers/2412.07774",
            "abstract": "We introduce UniReal, a unified framework designed to address various image generation and editing tasks. Existing solutions often vary by tasks, yet share fundamental principles: preserving consistency between inputs and outputs while capturing visual variations. Inspired by recent video generation models that effectively balance consistency and variation across frames, we propose a unifying approach that treats image-level tasks as discontinuous video generation. Specifically, we treat varying numbers of input and output images as frames, enabling seamless support for tasks such as image generation, editing, customization, composition, etc. Although designed for image-level tasks, we leverage videos as a scalable source for universal supervision. UniReal learns world dynamics from large-scale videos, demonstrating advanced capability in handling shadows, reflections, pose variation, and object interaction, while also exhibiting emergent capability for novel applications.",
            "score": 13,
            "issue_id": 1058,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "64e24bea8dffc31d",
            "authors": [
                "Xi Chen",
                "Zhifei Zhang",
                "He Zhang",
                "Yuqian Zhou",
                "Soo Ye Kim",
                "Qing Liu",
                "Yijun Li",
                "Jianming Zhang",
                "Nanxuan Zhao",
                "Yilin Wang",
                "Hui Ding",
                "Zhe Lin",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "Adobe Research",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07774.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная модель для генерации и редактирования изображений на основе видеоданных",
                    "desc": "UniReal - это унифицированная модель для генерации и редактирования изображений. Она рассматривает различные задачи обработки изображений как прерывистую генерацию видео, используя входные и выходные изображения в качестве кадров. Модель обучается на масштабных видеоданных, что позволяет ей эффективно обрабатывать тени, отражения, изменения поз и взаимодействие объектов. UniReal демонстрирует универсальность и возможности применения в различных задачах компьютерного зрения."
                },
                "en": {
                    "title": "UniReal: Unifying Image Generation and Editing through Video Dynamics",
                    "desc": "UniReal is a comprehensive framework that simplifies various image generation and editing tasks by treating them as a form of video generation. It focuses on maintaining consistency between input and output images while allowing for visual variations, similar to how video frames work. By using videos as a source of universal supervision, UniReal learns complex world dynamics, enabling it to manage challenges like shadows, reflections, and object interactions effectively. This approach not only enhances traditional image tasks but also opens up new possibilities for innovative applications in image processing."
                },
                "zh": {
                    "title": "统一框架，图像生成与编辑的未来",
                    "desc": "UniReal是一个统一的框架，旨在解决各种图像生成和编辑任务。该框架通过将图像任务视为不连续的视频生成，来保持输入和输出之间的一致性，同时捕捉视觉变化。UniReal利用大规模视频作为通用监督源，学习世界动态，从而在处理阴影、反射、姿态变化和物体交互方面展现出先进的能力。该方法不仅适用于图像任务，还展现出对新应用的潜在能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07626",
            "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
            "url": "https://huggingface.co/papers/2412.07626",
            "abstract": "Document content extraction is crucial in computer vision, especially for meeting the high-quality data needs of large language models (LLMs) and retrieval-augmented generation (RAG) technologies. However, current document parsing methods suffer from significant limitations in terms of diversity and comprehensive evaluation. To address these challenges, we introduce OmniDocBench, a novel multi-source benchmark designed to advance automated document content extraction. OmniDocBench includes a meticulously curated and annotated high-quality evaluation dataset comprising nine diverse document types, such as academic papers, textbooks, slides, among others. Our benchmark provides a flexible and comprehensive evaluation framework with 19 layout category labels and 14 attribute labels, enabling multi-level assessments across entire datasets, individual modules, or specific data types. Using OmniDocBench, we perform an exhaustive comparative analysis of existing modular pipelines and multimodal end-to-end methods, highlighting their limitations in handling document diversity and ensuring fair evaluation. OmniDocBench establishes a robust, diverse, and fair evaluation standard for the document content extraction field, offering crucial insights for future advancements and fostering the development of document parsing technologies. The codes and dataset is available in https://github.com/opendatalab/OmniDocBench.",
            "score": 12,
            "issue_id": 1065,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "4bba9d3934addbcc",
            "authors": [
                "Linke Ouyang",
                "Yuan Qu",
                "Hongbin Zhou",
                "Jiawei Zhu",
                "Rui Zhang",
                "Qunshu Lin",
                "Bin Wang",
                "Zhiyuan Zhao",
                "Man Jiang",
                "Xiaomeng Zhao",
                "Jin Shi",
                "Fan Wu",
                "Pei Chu",
                "Minghao Liu",
                "Zhenxiang Li",
                "Chao Xu",
                "Bo Zhang",
                "Botian Shi",
                "Zhongying Tu",
                "Conghui He"
            ],
            "affiliations": [
                "Abaka AI",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07626.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "Универсальный бенчмарк для оценки извлечения контента из документов",
                    "desc": "OmniDocBench - это новый многоисточниковый бенчмарк для улучшения автоматизированного извлечения контента из документов. Он включает тщательно подобранный и аннотированный набор данных из девяти различных типов документов. Бенчмарк предоставляет гибкую систему оценки с 19 метками категорий макета и 14 метками атрибутов. Используя OmniDocBench, авторы провели сравнительный анализ существующих модульных пайплайнов и мультимодальных end-to-end методов."
                },
                "en": {
                    "title": "OmniDocBench: Elevating Document Content Extraction Standards",
                    "desc": "This paper introduces OmniDocBench, a new benchmark aimed at improving document content extraction in computer vision. It addresses the limitations of current parsing methods by providing a diverse and comprehensive evaluation framework with a curated dataset of nine document types. The benchmark includes 19 layout category labels and 14 attribute labels, allowing for detailed assessments of various extraction methods. By conducting a thorough analysis of existing techniques, OmniDocBench sets a new standard for evaluating document parsing technologies, promoting advancements in the field."
                },
                "zh": {
                    "title": "OmniDocBench：文档提取的新标准",
                    "desc": "文档内容提取在计算机视觉中至关重要，尤其是满足大型语言模型和增强检索生成技术的高质量数据需求。然而，目前的文档解析方法在多样性和全面评估方面存在显著局限。为了解决这些挑战，我们提出了OmniDocBench，这是一个新颖的多源基准，旨在推动自动化文档内容提取的发展。OmniDocBench提供了一个灵活且全面的评估框架，能够对文档多样性进行深入分析，并为未来的文档解析技术发展提供重要见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07674",
            "title": "FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models",
            "url": "https://huggingface.co/papers/2412.07674",
            "abstract": "Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable attributes from the source images. Current methods attempt to distill identity and style from source images. However, \"style\" is a broad concept that includes texture, color, and artistic elements, but does not cover other important attributes such as lighting and dynamics. Additionally, a simplified \"style\" adaptation prevents combining multiple attributes from different sources into one generated image. In this work, we formulate a more effective approach to decompose the aesthetics of a picture into specific visual attributes, allowing users to apply characteristics such as lighting, texture, and dynamics from different images. To achieve this goal, we constructed the first fine-grained visual attributes dataset (FiVA) to the best of our knowledge. This FiVA dataset features a well-organized taxonomy for visual attributes and includes around 1 M high-quality generated images with visual attribute annotations. Leveraging this dataset, we propose a fine-grained visual attribute adaptation framework (FiVA-Adapter), which decouples and adapts visual attributes from one or more source images into a generated one. This approach enhances user-friendly customization, allowing users to selectively apply desired attributes to create images that meet their unique preferences and specific content requirements.",
            "score": 12,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "339e4e8d1664472e",
            "authors": [
                "Tong Wu",
                "Yinghao Xu",
                "Ryan Po",
                "Mengchen Zhang",
                "Guandao Yang",
                "Jiaqi Wang",
                "Ziwei Liu",
                "Dahua Lin",
                "Gordon Wetzstein"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "S-Lab, NTU",
                "Shanghai Artificial Intelligence Laboratory",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07674.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#synthetic"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точный контроль визуальных атрибутов в генерации изображений",
                    "desc": "Статья описывает новый подход к генерации изображений с использованием глубокого обучения. Авторы создали датасет FiVA с разметкой визуальных атрибутов и разработали фреймворк FiVA-Adapter для их адаптации. Это позволяет пользователям более точно контролировать такие характеристики генерируемых изображений, как освещение, текстура и динамика. Предложенный метод улучшает кастомизацию и дает возможность комбинировать атрибуты из разных источников."
                },
                "en": {
                    "title": "Customize Your Images with Fine-Grained Visual Attributes!",
                    "desc": "This paper presents a new method for text-to-image generation that allows users to customize images by selecting specific visual attributes from different source images. The authors introduce the FiVA dataset, which contains around 1 million high-quality images annotated with fine-grained visual attributes like lighting, texture, and dynamics. Their proposed framework, FiVA-Adapter, effectively decouples these attributes, enabling users to combine them in a single generated image. This approach improves the flexibility and user-friendliness of image generation, making it easier for non-experts to achieve their desired visual outcomes."
                },
                "zh": {
                    "title": "细粒度视觉属性适配，定制你的图像！",
                    "desc": "最近，文本到图像生成的技术取得了显著进展，能够创建高质量的图像，应用广泛。然而，准确描述所需的视觉属性对非专业人士来说可能很困难。本文提出了一种新的方法，将图像的美学分解为具体的视觉属性，使用户能够从不同的图像中应用特征，如光照、纹理和动态。我们构建了第一个细粒度视觉属性数据集（FiVA），并提出了FiVA-Adapter框架，以便用户能够根据个人偏好定制生成的图像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03548",
            "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
            "url": "https://huggingface.co/papers/2412.03548",
            "abstract": "Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection. Yet, MLMs can not produce intermediate depth or boxes to reason over. Finetuning MLMs on relevant data doesn't generalize well and outsourcing computation to specialized vision tools is too compute-intensive and memory-inefficient. To address this, we introduce Perception Tokens, intrinsic image representations designed to assist reasoning tasks where language is insufficient. Perception tokens act as auxiliary reasoning tokens, akin to chain-of-thought prompts in language models. For example, in a depth-related task, an MLM augmented with perception tokens can reason by generating a depth map as tokens, enabling it to solve the problem effectively. We propose AURORA, a training method that augments MLMs with perception tokens for improved reasoning over visual inputs. AURORA leverages a VQVAE to transform intermediate image representations, such as depth maps into a tokenized format and bounding box tokens, which is then used in a multi-task training framework. AURORA achieves notable improvements across counting benchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench, outperforming finetuning approaches in generalization across datasets. It also improves on relative depth: over +6% on BLINK. With perception tokens, AURORA expands the scope of MLMs beyond language-based reasoning, paving the way for more effective visual reasoning capabilities.",
            "score": 9,
            "issue_id": 1060,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "b1047666846bb684",
            "authors": [
                "Mahtab Bigverdi",
                "Zelun Luo",
                "Cheng-Yu Hsieh",
                "Ethan Shen",
                "Dongping Chen",
                "Linda G. Shapiro",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Google Research",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03548.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Токены восприятия: новый инструмент для визуальных рассуждений в мультимодальных моделях",
                    "desc": "Статья представляет новый подход к улучшению мультимодальных языковых моделей (MLM) в задачах визуального восприятия. Авторы вводят концепцию 'токенов восприятия' - вспомогательных элементов для рассуждений, подобных промптам в цепочке рассуждений языковых моделей. Предложенный метод AURORA использует VQVAE для преобразования промежуточных представлений изображений в токенизированный формат. Результаты показывают значительное улучшение производительности на различных бенчмарках, включая задачи подсчета и оценки относительной глубины."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning in MLMs with Perception Tokens",
                    "desc": "This paper introduces Perception Tokens to enhance multimodal language models (MLMs) in visual reasoning tasks. Traditional MLMs struggle with tasks like depth estimation and object detection because they cannot generate necessary intermediate representations. The proposed AURORA method integrates these perception tokens into MLMs, allowing them to produce depth maps and bounding boxes as tokens for better reasoning. AURORA shows significant performance improvements on various benchmarks, demonstrating its effectiveness in expanding the reasoning capabilities of MLMs beyond just language."
                },
                "zh": {
                    "title": "感知标记：提升多模态语言模型的视觉推理能力",
                    "desc": "多模态语言模型（MLMs）在基本视觉感知任务中仍面临挑战，专门模型在这些任务中表现更好。为了解决这一问题，本文提出了感知标记（Perception Tokens），这是一种内在的图像表示，旨在辅助语言不足的推理任务。通过引入感知标记，MLMs能够生成深度图等中间表示，从而有效解决与深度相关的问题。我们提出的AURORA训练方法通过将感知标记与MLMs结合，显著提高了视觉输入的推理能力，尤其在计数基准测试中表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07334",
            "title": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation",
            "url": "https://huggingface.co/papers/2412.07334",
            "abstract": "Interpretability is a key challenge in fostering trust for Large Language Models (LLMs), which stems from the complexity of extracting reasoning from model's parameters. We present the Frame Representation Hypothesis, a theoretically robust framework grounded in the Linear Representation Hypothesis (LRH) to interpret and control LLMs by modeling multi-token words. Prior research explored LRH to connect LLM representations with linguistic concepts, but was limited to single token analysis. As most words are composed of several tokens, we extend LRH to multi-token words, thereby enabling usage on any textual data with thousands of concepts. To this end, we propose words can be interpreted as frames, ordered sequences of vectors that better capture token-word relationships. Then, concepts can be represented as the average of word frames sharing a common concept. We showcase these tools through Top-k Concept-Guided Decoding, which can intuitively steer text generation using concepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3 families, demonstrating gender and language biases, exposing harmful content, but also potential to remediate them, leading to safer and more transparent LLMs. Code is available at https://github.com/phvv-me/frame-representation-hypothesis.git",
            "score": 8,
            "issue_id": 1062,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "ed0e99c50709fbaf",
            "authors": [
                "Pedro H. V. Valois",
                "Lincon S. Souza",
                "Erica K. Shimomoto",
                "Kazuhiro Fukui"
            ],
            "affiliations": [
                "National Institute of Advanced Industrial Science and Technology (AIST)",
                "University of Tsukuba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07334.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#ethics",
                    "#data",
                    "#interpretability",
                    "#multimodal",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Фреймовое представление слов: ключ к интерпретации и контролю языковых моделей",
                    "desc": "Статья представляет гипотезу фреймового представления для интерпретации и контроля больших языковых моделей (LLM). Авторы расширяют линейную гипотезу представления для анализа многотокенных слов, моделируя их как упорядоченные последовательности векторов. Предложенный подход позволяет интуитивно управлять генерацией текста с помощью выбранных концепций. Исследование демонстрирует применимость метода для выявления и потенциального исправления гендерных и языковых предубеждений в различных LLM."
                },
                "en": {
                    "title": "Unlocking LLMs: Interpreting Words as Frames for Safer AI",
                    "desc": "This paper addresses the challenge of interpreting Large Language Models (LLMs) by introducing the Frame Representation Hypothesis, which builds on the Linear Representation Hypothesis (LRH). It extends the analysis from single-token to multi-token words, allowing for a more comprehensive understanding of how words are represented in LLMs. By modeling words as frames—ordered sequences of vectors—the authors provide a method to capture the relationships between tokens and concepts more effectively. The proposed Top-k Concept-Guided Decoding technique enables controlled text generation based on selected concepts, revealing biases and harmful content while also offering pathways for remediation."
                },
                "zh": {
                    "title": "提升大型语言模型可解释性的框架表示假设",
                    "desc": "本文提出了框架表示假设，旨在提高大型语言模型（LLMs）的可解释性和可控性。我们扩展了线性表示假设（LRH），将其应用于多标记词的分析，以更好地理解和控制模型的输出。通过将词语视为框架，利用向量序列捕捉标记与词之间的关系，我们能够在文本生成中引入概念指导。我们的研究在多个模型上验证了这些方法，揭示了性别和语言偏见，同时展示了改善这些问题的潜力，从而推动更安全和透明的LLMs。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07776",
            "title": "Video Motion Transfer with Diffusion Transformers",
            "url": "https://huggingface.co/papers/2412.07776",
            "abstract": "We propose DiTFlow, a method for transferring the motion of a reference video to a newly synthesized one, designed specifically for Diffusion Transformers (DiT). We first process the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal called the Attention Motion Flow (AMF). We guide the latent denoising process in an optimization-based, training-free, manner by optimizing latents with our AMF loss to generate videos reproducing the motion of the reference one. We also apply our optimization strategy to transformer positional embeddings, granting us a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow against recently published methods, outperforming all across multiple metrics and human evaluation.",
            "score": 8,
            "issue_id": 1062,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "57a229123b5b2c38",
            "authors": [
                "Alexander Pondaven",
                "Aliaksandr Siarohin",
                "Sergey Tulyakov",
                "Philip Torr",
                "Fabio Pizzati"
            ],
            "affiliations": [
                "MBZUAI",
                "Snap Inc.",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07776.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#transfer_learning",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "DiTFlow: Перенос движения в видео с помощью диффузионных трансформеров",
                    "desc": "DiTFlow - это метод переноса движения из эталонного видео в новое синтезированное видео, разработанный специально для Диффузионных Трансформеров (DiT). Метод анализирует карты внимания между кадрами с помощью предобученного DiT и извлекает сигнал движения на уровне патчей, названный Attention Motion Flow (AMF). DiTFlow оптимизирует латентные представления во время процесса шумоподавления, используя функцию потерь AMF, чтобы воспроизвести движение из эталонного видео. Метод также применяет стратегию оптимизации к позиционным эмбеддингам трансформера, улучшая возможности переноса движения без дополнительного обучения."
                },
                "en": {
                    "title": "Seamlessly Transfer Motion with DiTFlow!",
                    "desc": "DiTFlow is a novel approach for transferring motion from a reference video to a newly created video using Diffusion Transformers (DiT). The method involves analyzing the reference video to extract Attention Motion Flow (AMF), which captures motion signals across frames. By optimizing the latent space with the AMF loss, DiTFlow effectively guides the video generation process without requiring additional training. The technique also enhances zero-shot motion transfer by optimizing transformer positional embeddings, demonstrating superior performance compared to existing methods in various evaluations."
                },
                "zh": {
                    "title": "DiTFlow：高效的视频运动转移方法",
                    "desc": "我们提出了一种名为DiTFlow的方法，用于将参考视频的运动转移到新合成的视频上，特别为扩散变换器（DiT）设计。首先，我们使用预训练的DiT处理参考视频，分析跨帧注意力图，并提取称为注意力运动流（AMF）的补丁级运动信号。通过优化AMF损失，我们以无训练的优化方式引导潜在去噪过程，从而生成再现参考视频运动的视频。我们还将优化策略应用于变换器的位置嵌入，显著提升了零样本运动转移的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06674",
            "title": "EMOv2: Pushing 5M Vision Model Frontier",
            "url": "https://huggingface.co/papers/2412.06674",
            "abstract": "This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the infrastructure for lightweight CNNs, but no counterparts have been recognized by attention-based design. Our work rethinks the lightweight infrastructure of efficient IRB and practical components in Transformer from a unified perspective, extending CNN-based IRB to attention-based models and abstracting a one-residual Meta Mobile Block (MMBlock) for lightweight model design. Following neat but effective design criterion, we deduce a modern Improved Inverted Residual Mobile Block (i2RMB) and improve a hierarchical Efficient MOdel (EMOv2) with no elaborate complex structures. Considering the imperceptible latency for mobile users when downloading models under 4G/5G bandwidth and ensuring model performance, we investigate the performance upper limit of lightweight models with a magnitude of 5M. Extensive experiments on various vision recognition, dense prediction, and image generation tasks demonstrate the superiority of our EMOv2 over state-of-the-art methods, e.g., EMOv2-1M/2M/5M achieve 72.3, 75.8, and 79.4 Top-1 that surpass equal-order CNN-/Attention-based models significantly. At the same time, EMOv2-5M equipped RetinaNet achieves 41.5 mAP for object detection tasks that surpasses the previous EMO-5M by +2.6. When employing the more robust training recipe, our EMOv2-5M eventually achieves 82.9 Top-1 accuracy, which elevates the performance of 5M magnitude models to a new level. Code is available at https://github.com/zhangzjn/EMOv2.",
            "score": 8,
            "issue_id": 1061,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "a890d1e09015a7a6",
            "authors": [
                "Jiangning Zhang",
                "Teng Hu",
                "Haoyang He",
                "Zhucun Xue",
                "Yabiao Wang",
                "Chengjie Wang",
                "Yong Liu",
                "Xiangtai Li",
                "Dacheng Tao"
            ],
            "affiliations": [
                "Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",
                "Nanyang Technological University, Singapore",
                "Shanghai Jiao Tong University, Shanghai, China",
                "Youtu Lab, Tencent, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06674.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "EMOv2: Новый рубеж для легковесных моделей компьютерного зрения",
                    "desc": "Эта работа посвящена разработке эффективных по параметрам и легковесных моделей для плотных предсказаний, балансируя между количеством параметров, FLOP и производительностью. Авторы предлагают новый блок i2RMB, основанный на инвертированном остаточном блоке (IRB) и компонентах Transformer. На основе этого блока создана иерархическая модель EMOv2, которая превосходит современные методы в различных задачах компьютерного зрения. Эксперименты показывают, что EMOv2 с 5 миллионами параметров достигает точности 82.9% Top-1 на ImageNet, устанавливая новый рекорд для легковесных моделей."
                },
                "en": {
                    "title": "Lightweight Models, Heavyweight Performance!",
                    "desc": "This paper presents a new approach to creating lightweight models for dense prediction tasks in machine learning, focusing on efficiency in terms of parameters and computational cost (FLOPs). The authors introduce a novel architecture called the Improved Inverted Residual Mobile Block (i2RMB) that extends the Inverted Residual Block (IRB) concept to attention-based models. They demonstrate that their Efficient Model version 2 (EMOv2) outperforms existing models in various tasks, achieving impressive accuracy with only 5 million parameters. The results indicate that lightweight models can achieve high performance without complex structures, making them suitable for mobile applications."
                },
                "zh": {
                    "title": "轻量级模型的新突破：5M量级的高效设计",
                    "desc": "本研究致力于开发参数高效且轻量级的模型，以实现密集预测，同时在参数、FLOPs和性能之间进行权衡。我们旨在为各种下游任务建立5M量级轻量级模型的新前沿。我们重新思考了高效的反向残差块（IRB）和Transformer中的实用组件，从统一的角度扩展了基于CNN的IRB到基于注意力的模型，并抽象出一种轻量级模型设计的元残差移动块（MMBlock）。通过简洁而有效的设计标准，我们推导出一种现代化的改进反向残差移动块（i2RMB），并在没有复杂结构的情况下改进了分层高效模型（EMOv2）。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07759",
            "title": "3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation",
            "url": "https://huggingface.co/papers/2412.07759",
            "abstract": "This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster",
            "score": 7,
            "issue_id": 1064,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "0e2f96c50d396f1d",
            "authors": [
                "Xiao Fu",
                "Xian Liu",
                "Xintao Wang",
                "Sida Peng",
                "Menghan Xia",
                "Xiaoyu Shi",
                "Ziyang Yuan",
                "Pengfei Wan",
                "Di Zhang",
                "Dahua Lin"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07759.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Управление 3D-движением объектов в генерации видео",
                    "desc": "Статья представляет 3DTrajMaster - контроллер для управления динамикой множества объектов в 3D-пространстве при генерации видео. Ключевой компонент - инжектор объектов, учитывающий 3D-траектории через механизм гейтированного самовнимания. Для улучшения качества видео используются доменный адаптер и стратегия отжига при сэмплировании. Авторы также создали датасет 360-Motion для обучения модели."
                },
                "en": {
                    "title": "Mastering 3D Motion Control in Video Generation",
                    "desc": "This paper presents 3DTrajMaster, a novel approach for controlling multi-entity motions in 3D video generation. Unlike previous methods that rely on 2D control signals, 3DTrajMaster utilizes 6DoF pose sequences to accurately represent the 3D dynamics of objects. The method incorporates a gated self-attention mechanism to integrate multiple entities with their 3D trajectories, enhancing the realism of generated videos. Additionally, the authors introduce a 360-Motion Dataset to improve training data quality, leading to superior performance in both accuracy and generalization for 3D motion control."
                },
                "zh": {
                    "title": "掌控三维运动，重塑视频生成",
                    "desc": "本文旨在操控视频生成中的多实体三维运动。以往的可控视频生成方法主要依赖二维控制信号来操控物体运动，但二维信号在表达三维运动特性方面存在局限。为了解决这个问题，我们提出了3DTrajMaster，这是一种强大的控制器，可以根据用户期望的六自由度姿态序列调节三维空间中的多实体动态。我们的研究通过一个插件式的三维运动基础物体注入器，结合多个输入实体及其相应的三维轨迹，利用门控自注意力机制实现了这一目标。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07724",
            "title": "Granite Guardian",
            "url": "https://huggingface.co/papers/2412.07724",
            "abstract": "We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, and hallucination-related risks such as context relevance, groundedness, and answer relevance for retrieval-augmented generation (RAG). Trained on a unique dataset combining human annotations from diverse sources and synthetic data, Granite Guardian models address risks typically overlooked by traditional risk detection models, such as jailbreaks and RAG-specific issues. With AUC scores of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks respectively, Granite Guardian is the most generalizable and competitive model available in the space. Released as open-source, Granite Guardian aims to promote responsible AI development across the community.   https://github.com/ibm-granite/granite-guardian",
            "score": 7,
            "issue_id": 1058,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "9d3367b124b8b792",
            "authors": [
                "Inkit Padhi",
                "Manish Nagireddy",
                "Giandomenico Cornacchia",
                "Subhajit Chaudhury",
                "Tejaswini Pedapati",
                "Pierre Dognin",
                "Keerthiram Murugesan",
                "Erik Miehling",
                "Martín Santillán Cooper",
                "Kieran Fraser",
                "Giulio Zizzo",
                "Muhammad Zaid Hameed",
                "Mark Purcell",
                "Michael Desmond",
                "Qian Pan",
                "Inge Vejsbjerg",
                "Elizabeth M. Daly",
                "Michael Hind",
                "Werner Geyer",
                "Ambrish Rawat",
                "Kush R. Varshney",
                "Prasanna Sattigeri"
            ],
            "affiliations": [
                "IBM Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07724.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#hallucinations",
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#ethics",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Защитник ИИ: обеспечение безопасности языковых моделей",
                    "desc": "Представлены модели Granite Guardian - набор инструментов для обнаружения рисков в запросах и ответах больших языковых моделей (LLM). Эти модели охватывают различные аспекты риска, включая социальные предубеждения, ненормативную лексику, насилие, сексуальный контент, неэтичное поведение и риски, связанные с галлюцинациями. Обученные на уникальном наборе данных, сочетающем человеческие аннотации и синтетические данные, модели Granite Guardian демонстрируют высокую производительность в обнаружении рисков. Проект выпущен с открытым исходным кодом для продвижения ответственного развития искусственного интеллекта."
                },
                "en": {
                    "title": "Granite Guardian: Safeguarding AI with Comprehensive Risk Detection",
                    "desc": "The Granite Guardian models are designed to enhance the safety of large language models (LLMs) by detecting various risks in prompts and responses. They cover a wide range of risk factors, including social bias, profanity, and hallucination-related issues, which are often missed by traditional models. These models are trained on a unique dataset that combines human annotations and synthetic data, making them effective at identifying risks like jailbreaking and retrieval-augmented generation (RAG) problems. With high AUC scores, Granite Guardian represents a significant advancement in responsible AI development and is available as open-source for community use."
                },
                "zh": {
                    "title": "Granite Guardian：安全使用大型语言模型的守护者",
                    "desc": "Granite Guardian模型是一套旨在提供风险检测的安全保障工具，适用于大型语言模型（LLM）的安全和负责任使用。这些模型覆盖多个风险维度，包括社会偏见、粗俗语言、暴力、性内容、不道德行为、越狱和幻觉相关风险。Granite Guardian模型通过结合来自多种来源的人类注释和合成数据进行训练，解决了传统风险检测模型通常忽视的风险。作为开源项目，Granite Guardian旨在促进社区内负责任的人工智能发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06673",
            "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
            "url": "https://huggingface.co/papers/2412.06673",
            "abstract": "In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining -- over four times fewer than what is typically needed -- while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing.",
            "score": 7,
            "issue_id": 1058,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "a8071141959ac48a",
            "authors": [
                "Chunwei Wang",
                "Guansong Lu",
                "Junwei Yang",
                "Runhui Huang",
                "Jianhua Han",
                "Lu Hou",
                "Wei Zhang",
                "Hang Xu"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06673.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#dataset",
                    "#benchmark",
                    "#interpretability",
                    "#training",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ILLUME: Единая мультимодальная ИИ-модель с улучшенной эффективностью обучения",
                    "desc": "ILLUME - это унифицированная мультимодальная большая языковая модель, объединяющая возможности понимания и генерации в рамках единой архитектуры. Модель использует эффективный визуальный токенизатор и многоэтапное обучение, что позволяет достичь высоких результатов на меньшем объеме данных. Внедрена схема самоусиливающегося мультимодального выравнивания для улучшения согласованности между текстовыми описаниями и сгенерированными изображениями. ILLUME демонстрирует конкурентоспособные результаты в различных задачах мультимодального понимания, генерации и редактирования."
                },
                "en": {
                    "title": "ILLUME: Efficient Multimodal Mastery in One Model",
                    "desc": "The paper presents ILLUME, a unified multimodal large language model (MLLM) that combines understanding and generation of both text and images. It introduces a vision tokenizer that uses semantic information to improve data efficiency, allowing for effective training with a smaller dataset of only 15 million samples. The model employs a self-enhancing multimodal alignment scheme to ensure that the generated images accurately reflect the text descriptions, reducing errors in image generation. Through extensive testing, ILLUME demonstrates competitive performance against existing models in multimodal tasks."
                },
                "zh": {
                    "title": "ILLUME：多模态理解与生成的统一模型",
                    "desc": "本文介绍了ILLUME，这是一种统一的多模态大语言模型（MLLM），它通过统一的下一个标记预测公式，将多模态理解和生成能力无缝集成在一个模型中。为了应对通常需要的大规模数据集，我们设计了一种视觉标记器，结合了语义信息，并采用渐进式多阶段训练程序，从而将预训练所需的数据集大小减少到仅1500万，远低于通常所需的数量，同时在性能上与现有的统一MLLM（如Janus）竞争或更优。我们还引入了一种新颖的自我增强多模态对齐方案，监督模型自我评估文本描述与自生成图像之间的一致性，从而提高图像理解的准确性，避免因生成图像不一致而导致的不现实和错误预测。通过广泛的实验，ILLUME在多模态理解、生成和编辑的各类基准测试中表现突出，能够与最先进的统一MLLM和专业模型竞争。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07721",
            "title": "ObjCtrl-2.5D: Training-free Object Control with Camera Poses",
            "url": "https://huggingface.co/papers/2412.07721",
            "abstract": "This study aims to achieve more precise and versatile object control in image-to-video (I2V) generation. Current methods typically represent the spatial movement of target objects with 2D trajectories, which often fail to capture user intention and frequently produce unnatural results. To enhance control, we present ObjCtrl-2.5D, a training-free object control approach that uses a 3D trajectory, extended from a 2D trajectory with depth information, as a control signal. By modeling object movement as camera movement, ObjCtrl-2.5D represents the 3D trajectory as a sequence of camera poses, enabling object motion control using an existing camera motion control I2V generation model (CMC-I2V) without training. To adapt the CMC-I2V model originally designed for global motion control to handle local object motion, we introduce a module to isolate the target object from the background, enabling independent local control. In addition, we devise an effective way to achieve more accurate object control by sharing low-frequency warped latent within the object's region across frames. Extensive experiments demonstrate that ObjCtrl-2.5D significantly improves object control accuracy compared to training-free methods and offers more diverse control capabilities than training-based approaches using 2D trajectories, enabling complex effects like object rotation. Code and results are available at https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/.",
            "score": 6,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "de32794ee8780d29",
            "authors": [
                "Zhouxia Wang",
                "Yushi Lan",
                "Shangchen Zhou",
                "Chen Change Loy"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07721.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "3D траектории для улучшенного контроля объектов в генерации видео",
                    "desc": "Это исследование представляет новый подход к управлению объектами в генерации видео из изображений, называемый ObjCtrl-2.5D. Метод использует 3D траекторию вместо 2D для более точного и разнообразного контроля движения объектов. ObjCtrl-2.5D моделирует движение объекта как движение камеры, что позволяет использовать существующую модель генерации видео с контролем движения камеры без дополнительного обучения. Подход включает модуль для изоляции целевого объекта от фона и метод совместного использования низкочастотных искаженных латентных представлений в области объекта для повышения точности контроля."
                },
                "en": {
                    "title": "Revolutionizing Object Control in I2V with 3D Trajectories",
                    "desc": "This paper introduces ObjCtrl-2.5D, a novel approach for enhancing object control in image-to-video (I2V) generation. Unlike traditional methods that rely on 2D trajectories, ObjCtrl-2.5D utilizes 3D trajectories, incorporating depth information to better reflect user intentions. By treating object movement as camera movement, it allows for precise control without the need for additional training. The method also includes a module for isolating the target object from the background, enabling more accurate and diverse control over object motion, including complex effects like rotation."
                },
                "zh": {
                    "title": "提升图像到视频生成中的物体控制精度与多样性",
                    "desc": "本研究旨在提高图像到视频生成中的物体控制精度和多样性。现有方法通常使用二维轨迹表示目标物体的空间运动，难以捕捉用户意图，且常常产生不自然的结果。我们提出了ObjCtrl-2.5D，这是一种无训练的物体控制方法，利用带深度信息的三维轨迹作为控制信号。通过将物体运动建模为相机运动，ObjCtrl-2.5D能够在不进行训练的情况下，使用现有的相机运动控制模型实现物体运动控制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05148",
            "title": "LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation",
            "url": "https://huggingface.co/papers/2412.05148",
            "abstract": "Recent advancements in image generation models have enabled personalized image creation with both user-defined subjects (content) and styles. Prior works achieved personalization by merging corresponding low-rank adaptation parameters (LoRAs) through optimization-based methods, which are computationally demanding and unsuitable for real-time use on resource-constrained devices like smartphones. To address this, we introduce LoRA.rar, a method that not only improves image quality but also achieves a remarkable speedup of over 4000times in the merging process. LoRA.rar pre-trains a hypernetwork on a diverse set of content-style LoRA pairs, learning an efficient merging strategy that generalizes to new, unseen content-style pairs, enabling fast, high-quality personalization. Moreover, we identify limitations in existing evaluation metrics for content-style quality and propose a new protocol using multimodal large language models (MLLM) for more accurate assessment. Our method significantly outperforms the current state of the art in both content and style fidelity, as validated by MLLM assessments and human evaluations.",
            "score": 4,
            "issue_id": 1066,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "ef1bd7ea8522423b",
            "authors": [
                "Donald Shenaj",
                "Ondrej Bohdal",
                "Mete Ozay",
                "Pietro Zanuttigh",
                "Umberto Michieli"
            ],
            "affiliations": [
                "Samsung R&D Institute (SRUK)",
                "University of Padova"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05148.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#synthetic",
                    "#benchmark",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "LoRA.rar: Сверхбыстрая персонализация генерации изображений",
                    "desc": "Статья представляет метод LoRA.rar для персонализированной генерации изображений. Этот метод использует предобученную гиперсеть для быстрого объединения параметров LoRA для контента и стиля, что значительно ускоряет процесс по сравнению с предыдущими подходами. Авторы также предлагают новый протокол оценки качества контента и стиля с использованием мультимодальных языковых моделей (MLLM). Результаты показывают, что LoRA.rar превосходит существующие методы по точности передачи контента и стиля."
                },
                "en": {
                    "title": "Revolutionizing Image Personalization with Lightning Speed",
                    "desc": "This paper presents LoRA.rar, a novel approach for personalized image generation that significantly enhances both speed and quality. Unlike previous methods that relied on computationally intensive optimization to merge low-rank adaptation parameters (LoRAs), LoRA.rar achieves over 4000 times faster merging by utilizing a pre-trained hypernetwork. This hypernetwork learns to efficiently combine content and style from diverse LoRA pairs, allowing for quick adaptation to new combinations. Additionally, the authors propose a new evaluation protocol using multimodal large language models to better assess the quality of generated images, demonstrating that their method surpasses existing techniques in both content and style fidelity."
                },
                "zh": {
                    "title": "个性化图像生成的快速解决方案",
                    "desc": "最近，图像生成模型的进步使得个性化图像创建成为可能，用户可以定义图像的内容和风格。以往的个性化方法通过优化合并低秩适应参数（LoRAs），但这种方法计算量大，不适合在资源有限的设备上实时使用。为了解决这个问题，我们提出了LoRA.rar，这种方法不仅提高了图像质量，还在合并过程中实现了超过4000倍的速度提升。我们还提出了一种新的评估协议，利用多模态大型语言模型（MLLM）来更准确地评估内容和风格的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06845",
            "title": "Fully Open Source Moxin-7B Technical Report",
            "url": "https://huggingface.co/papers/2412.06845",
            "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be \"open-source,\" which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of \"open science\" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation.",
            "score": 4,
            "issue_id": 1058,
            "pub_date": "2024-12-08",
            "pub_date_card": {
                "ru": "8 декабря",
                "en": "December 8",
                "zh": "12月8日"
            },
            "hash": "410471f06d9e6883",
            "authors": [
                "Pu Zhao",
                "Xuan Shen",
                "Zhenglun Kong",
                "Yixin Shen",
                "Sung-En Chang",
                "Timothy Rupprecht",
                "Lei Lu",
                "Enfu Nan",
                "Changdi Yang",
                "Yumei He",
                "Xingchen Xu",
                "Yu Huang",
                "Wei Wang",
                "Yue Chen",
                "Yong He",
                "Yanzhi Wang"
            ],
            "affiliations": [
                "AIBAO LLC",
                "Cornell University",
                "Futurewei Technologies",
                "Harvard University",
                "Northeastern University",
                "Roboraction.ai",
                "Tulane University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06845.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#open_source",
                    "#ethics",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🔓",
                "ru": {
                    "title": "Moxin 7B: Открытая LLM для прозрачных инноваций в ИИ",
                    "desc": "В статье представлена модель Moxin 7B - полностью открытая большая языковая модель (LLM), разработанная в соответствии с Моделью открытости (MOF). Moxin 7B достигает высшего уровня открытости по классификации MOF благодаря полному раскрытию кода, конфигураций, наборов данных и промежуточных результатов. Эксперименты показывают, что модель превосходит популярные 7B-модели в zero-shot оценке и конкурентоспособна в few-shot оценке. Авторы подчеркивают важность открытости и прозрачности в развитии LLM для стимулирования инноваций и исследований."
                },
                "en": {
                    "title": "Moxin 7B: Leading the Way in Open-Source Language Models",
                    "desc": "This paper discusses the evolution of Large Language Models (LLMs), highlighting the contrast between proprietary models like GPT-4 and open-source alternatives such as LLaMA. It emphasizes the importance of transparency and reproducibility in AI, noting that many open-source models do not fully disclose their training processes or data. To address these issues, the authors introduce Moxin 7B, an open-source LLM that adheres to the Model Openness Framework (MOF), ensuring comprehensive access to its training code and datasets. The results demonstrate that Moxin 7B outperforms other 7B models in zero-shot tasks and remains competitive in few-shot scenarios, showcasing the potential of fully open-source LLMs."
                },
                "zh": {
                    "title": "Moxin 7B：开源语言模型的新标杆",
                    "desc": "最近，大型语言模型（LLMs）经历了显著的变革，受到了广泛关注。以GPT-4和GPT-o1为代表的专有LLMs展现了卓越的性能和多样性，同时开源LLMs如LLaMA和Mistral也因其易于定制和部署而受到青睐。尽管开源LLMs为创新和研究提供了前所未有的机会，但其商业化带来了透明性、可重复性和安全性方面的担忧。为了解决这些问题，我们推出了Moxin 7B，这是一个完全开源的LLM，遵循模型开放框架（MOF），并在透明性和开放性方面达到了最高标准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07338",
            "title": "Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation",
            "url": "https://huggingface.co/papers/2412.07338",
            "abstract": "AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.",
            "score": 2,
            "issue_id": 1066,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "8b3e1c2da99f56d4",
            "authors": [
                "Lorenzo Cima",
                "Alessio Miaschi",
                "Amaury Trujillo",
                "Marco Avvenuti",
                "Felice Dell'Orletta",
                "Stefano Cresci"
            ],
            "affiliations": [
                "IIT-CNR",
                "ILC-CNR",
                "University of Pisa"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07338.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#ethics",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🗨️",
                "ru": {
                    "title": "ИИ на страже онлайн-этикета: персонализированные ответы против токсичности",
                    "desc": "Данная статья посвящена использованию искусственного интеллекта для генерации контекстуализированных ответов на токсичные комментарии в интернете. Авторы предлагают стратегии для создания персонализированных ответов с помощью языковой модели LLaMA2-13B. Результаты показывают, что контекстуализированные ответы значительно превосходят обобщенные варианты по адекватности и убедительности. Исследование также выявило слабую корреляцию между количественными показателями и оценками людей, что подчеркивает необходимость более тщательных методов оценки."
                },
                "en": {
                    "title": "Tailored AI Counterspeech: A New Era in Online Discourse",
                    "desc": "This paper discusses a new approach to generating counterspeech using AI to combat online toxicity. The authors focus on creating personalized and context-aware responses rather than generic replies, which can be more effective in promoting civil discourse. They utilize the LLaMA2-13B model and test various configurations to enhance the persuasiveness of the generated counterspeech. The study finds that tailored responses significantly outperform standard methods, highlighting the need for better evaluation techniques that consider both quantitative metrics and human feedback."
                },
                "zh": {
                    "title": "个性化反言论：提升在线交流的有效性",
                    "desc": "本文提出了一种基于人工智能的反言论策略，旨在通过直接回复来减少在线毒性。当前的反言论方法缺乏针对性，无法适应不同的管理环境和用户需求。我们使用LLaMA2-13B模型生成个性化的反言论，并通过多种配置进行实验，以评估其有效性。研究结果表明，针对特定上下文的反言论在适当性和说服力上显著优于传统的通用反言论，强调了人机协作在内容管理中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04835",
            "title": "Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment",
            "url": "https://huggingface.co/papers/2412.04835",
            "abstract": "Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment.",
            "score": 2,
            "issue_id": 1064,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "1fe129433c4c71a5",
            "authors": [
                "Ran Tian",
                "Yilin Wu",
                "Chenfeng Xu",
                "Masayoshi Tomizuka",
                "Jitendra Malik",
                "Andrea Bajcsy"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04835.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Эффективное обучение роботов с минимальным участием человека",
                    "desc": "Статья представляет метод RAPL для обучения визуальных наград с использованием меньшего количества обратной связи от человека. В отличие от традиционного обучения с подкреплением по обратной связи человека (RLHF), RAPL фокусируется на дообучении предварительно обученных энкодеров зрения для согласования с визуальным представлением конечного пользователя. Метод показал эффективность в симуляционных экспериментах и задачах манипуляции роботом. RAPL позволяет настраивать политики с использованием в 5 раз меньше реальных данных о предпочтениях человека."
                },
                "en": {
                    "title": "Efficiently Aligning Robot Policies with Minimal Human Feedback",
                    "desc": "This paper introduces Representation-Aligned Preference-based Learning (RAPL), a novel approach to align visuomotor robot policies with human preferences using minimal feedback. RAPL improves upon traditional reinforcement learning from human feedback (RLHF) by focusing on fine-tuning pre-trained vision encoders, allowing for the construction of dense visual rewards through feature matching. The method is validated through simulations and hardware experiments, showing that it can effectively learn rewards that align with human preferences while requiring significantly less human input. Overall, RAPL represents a significant step towards efficient alignment of robot policies in various manipulation tasks."
                },
                "zh": {
                    "title": "减少人类反馈，实现机器人策略对齐的突破",
                    "desc": "这篇论文提出了一种新的方法，称为基于表示对齐的偏好学习（RAPL），旨在减少对人类反馈的需求，以便更好地对齐视觉运动机器人策略。传统的强化学习方法需要大量的人类反馈来学习视觉奖励函数，而RAPL通过优化预训练的视觉编码器来实现这一目标。该方法通过特征匹配在对齐的表示空间中构建密集的视觉奖励，从而提高了学习效率。实验结果表明，RAPL能够在减少人类偏好数据的情况下，有效地对齐机器人策略。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05983",
            "title": "Chimera: Improving Generalist Model with Domain-Specific Experts",
            "url": "https://huggingface.co/papers/2412.05983",
            "abstract": "Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. To address these challenges, we introduce Chimera, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs.",
            "score": 2,
            "issue_id": 1064,
            "pub_date": "2024-12-08",
            "pub_date_card": {
                "ru": "8 декабря",
                "en": "December 8",
                "zh": "12月8日"
            },
            "hash": "991103a0d9d85ad4",
            "authors": [
                "Tianshuo Peng",
                "Mingsheng Li",
                "Hongbin Zhou",
                "Renqiu Xia",
                "Renrui Zhang",
                "Lei Bai",
                "Song Mao",
                "Bin Wang",
                "Conghui He",
                "Aojun Zhou",
                "Botian Shi",
                "Tao Chen",
                "Bo Zhang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "Fudan University",
                "MMLab, The Chinese University of Hong Kong",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05983.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#cv",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Chimera: Универсальная LMM с экспертными знаниями",
                    "desc": "Статья представляет Chimera - новый подход к улучшению крупных мультимодальных моделей (LMM) для специализированных задач. Авторы предлагают прогрессивную стратегию обучения для интеграции экспертных моделей в обобщенную LMM. Для решения проблемы несбалансированной оптимизации вводится механизм маскирования совместной работы обобщенной и специализированной моделей (GSCM). Результатом является универсальная модель, показывающая высокие результаты в задачах мультимодальных рассуждений и извлечения визуального контента в специализированных областях."
                },
                "en": {
                    "title": "Chimera: Bridging Generalist and Specialist Models for Enhanced Multi-modal Learning",
                    "desc": "This paper presents Chimera, a new approach to enhance Large Multi-modal Models (LMMs) by integrating domain-specific expert models. The authors highlight that while LMMs perform well on general tasks, they struggle with specialized tasks due to their training on broad datasets. To improve this, Chimera employs a progressive training strategy that incorporates expert features into the generalist model's input. Additionally, the Generalist-Specialist Collaboration Masking (GSCM) mechanism is introduced to balance optimization between the generalist and specialist models, leading to superior performance in multi-modal reasoning and visual content extraction tasks."
                },
                "zh": {
                    "title": "Chimera：提升多模态模型的领域专长",
                    "desc": "本文介绍了一种名为Chimera的多模态模型，旨在提升现有大型多模态模型（LMMs）在特定领域的能力。通过引入领域专家模型的特征，Chimera采用渐进式训练策略，解决了通用模型与专家模型之间的表示差距和优化不平衡问题。特别地，文章提出了一种新的通用-专家协作掩码机制（GSCM），以优化模型性能。最终，Chimera在图表、表格、数学和文档等领域的多模态推理和视觉内容提取任务中表现出色，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07187",
            "title": "A New Federated Learning Framework Against Gradient Inversion Attacks",
            "url": "https://huggingface.co/papers/2412.07187",
            "abstract": "Federated Learning (FL) aims to protect data privacy by enabling clients to collectively train machine learning models without sharing their raw data. However, recent studies demonstrate that information exchanged during FL is subject to Gradient Inversion Attacks (GIA) and, consequently, a variety of privacy-preserving methods have been integrated into FL to thwart such attacks, such as Secure Multi-party Computing (SMC), Homomorphic Encryption (HE), and Differential Privacy (DP). Despite their ability to protect data privacy, these approaches inherently involve substantial privacy-utility trade-offs. By revisiting the key to privacy exposure in FL under GIA, which lies in the frequent sharing of model gradients that contain private data, we take a new perspective by designing a novel privacy preserve FL framework that effectively ``breaks the direct connection'' between the shared parameters and the local private data to defend against GIA. Specifically, we propose a Hypernetwork Federated Learning (HyperFL) framework that utilizes hypernetworks to generate the parameters of the local model and only the hypernetwork parameters are uploaded to the server for aggregation. Theoretical analyses demonstrate the convergence rate of the proposed HyperFL, while extensive experimental results show the privacy-preserving capability and comparable performance of HyperFL. Code is available at https://github.com/Pengxin-Guo/HyperFL.",
            "score": 1,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "772d62408694e82b",
            "authors": [
                "Pengxin Guo",
                "Shuang Zeng",
                "Wenhao Chen",
                "Xiaodan Zhang",
                "Weihong Ren",
                "Yuyin Zhou",
                "Liangqiong Qu"
            ],
            "affiliations": [
                "College of Computer Science, Beijing University of Technology",
                "Department of Computer Science and Engineering, UC Santa Cruz",
                "Department of Mathematics, The University of Hong Kong",
                "School of Computing and Data Science, The University of Hong Kong",
                "School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07187.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#healthcare",
                    "#ethics",
                    "#security",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Защита конфиденциальности в федеративном обучении с помощью гиперсетей",
                    "desc": "В статье представлен новый подход к федеративному обучению (FL), направленный на защиту конфиденциальности данных от атак инверсии градиента (GIA). Авторы предлагают фреймворк HyperFL, использующий гиперсети для генерации параметров локальной модели, при этом на сервер для агрегации отправляются только параметры гиперсети. Теоретический анализ демонстрирует скорость сходимости предложенного метода HyperFL. Экспериментальные результаты показывают способность HyperFL сохранять конфиденциальность данных при сопоставимой производительности."
                },
                "en": {
                    "title": "Revolutionizing Privacy in Federated Learning with Hypernetworks",
                    "desc": "Federated Learning (FL) allows multiple clients to train machine learning models while keeping their data private. However, it is vulnerable to Gradient Inversion Attacks (GIA), which can expose sensitive information through shared model gradients. To address this, the paper introduces a new framework called Hypernetwork Federated Learning (HyperFL), which uses hypernetworks to generate model parameters, ensuring that only hypernetwork parameters are shared. This approach effectively reduces the risk of privacy breaches while maintaining strong performance and convergence rates in training."
                },
                "zh": {
                    "title": "超网络联邦学习：保护隐私的新方法",
                    "desc": "联邦学习（FL）旨在通过让客户端共同训练机器学习模型而不共享原始数据来保护数据隐私。然而，最近的研究表明，在FL过程中交换的信息可能会受到梯度反演攻击（GIA）的威胁，因此许多隐私保护方法被整合进FL中以抵御这些攻击。这些方法如安全多方计算（SMC）、同态加密（HE）和差分隐私（DP）虽然能保护数据隐私，但通常会涉及显著的隐私与效用之间的权衡。本文提出了一种新的隐私保护FL框架——超网络联邦学习（HyperFL），通过设计超网络生成本地模型参数，从而有效“打破”共享参数与本地私有数据之间的直接联系，以抵御GIA。"
                }
            }
        }
    ],
    "link_prev": "2024-12-10.html",
    "link_next": "2024-12-12.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12月10日"
    },
    "short_date_next": {
        "ru": "12.12",
        "en": "12/12",
        "zh": "12月12日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 8,
        "#rl": 0,
        "#rlhf": 2,
        "#rag": 2,
        "#plp": 1,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 12,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 12,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 5,
        "#security": 2,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 4,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了代码大语言模型（codeLLMs）在代码生成方面的进展。以前的代码相关基准测试主要关注生成正确的代码片段，但忽略了与人类偏好的一致性。为了弥补这一差距，作者提出了一个名为CodeArena的严格人工编制基准测试，模拟真实世界编码任务的复杂性和多样性。通过系统实验，作者发现在开源代码LLMs和专有LLMs之间存在显著的性能差距，强调了人类偏好一致性的重要性。",
        "title": "Evaluating and Aligning CodeLLMs on Human Preference",
        "pinyin": "这篇文章讨论了代码大语言模型（codeLLMs）在代码生成方面的进展。以前的代码相关基准测试主要关注生成正确的代码片段，但忽略了与人类偏好的一致性。为了弥补这一差距，作者提出了一个名为CodeArena的严格人工编制基准测试，模拟真实世界编码任务的复杂性和多样性。通过系统实验，作者发现在开源代码LLMs和专有LLMs之间存在显著的性能差距，强调了人类偏好一致性的重要性。\n\nzhè piān wén zhāng tǎo lùn le dài mǎ dà yǔ yán mó xíng (codeLLMs) zài dài mǎ shēng chéng fāng miàn de jìn zhàn. yǐ qián de dài mǎ xiāng guān jī zhǔn cè shì zhǔ yào guān zhù shēng chéng zhèng què de dài mǎ piàn duàn, dàn hū lüè le yǔ rén lèi piān hào de yī zhì xìng. wèi le mí bǔ zhè yī chā jù, zuò zhě tí chū le yī gè míng wèi CodeArena de yán gé rén gōng biān zhì jī zhǔn cè shì, mó nǐ zhēn shí shì jiè biān mǎ rèn wù de fú zà xìng hé duō yàng xìng. tōng guò xì tǒng shí yàn, zuò zhě fā xiàn zài kāi yuán dài mǎ LLMs hé zhuān yǒu LLMs zhī jiān cún zài xiǎn zhù de xìng néng chā jù, qiáng diào le rén lèi piān hào yī zhì xìng de zhòng yào xìng.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'},\n{'word': '以前', 'pinyin': 'yǐ qián', 'trans': 'before'},\n{'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'},\n{'word': '基准测试', 'pinyin': 'jī zhǔn cè shì', 'trans': 'benchmark test'},\n{'word': '主要', 'pinyin': 'zhǔ yào', 'trans': 'main'},\n{'word': '关注', 'pinyin': 'guān zhù', 'trans': 'focus on'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'},\n{'word': '正确', 'pinyin': 'zhèng què', 'trans': 'correct'},\n{'word': '片段', 'pinyin': 'piàn duàn', 'trans': 'segment'},\n{'word': '忽略', 'pinyin': 'hū lüè', 'trans': 'ignore'},\n{'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'},\n{'word': '弥补', 'pinyin': 'mí bǔ', 'trans': 'make up for'},\n{'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '严格', 'pinyin': 'yán gé', 'trans': 'strict'},\n{'word': '人工编制', 'pinyin': 'rén gōng biān zhì', 'trans': 'artificially compiled'},\n{'word': '模拟', 'pinyin': 'mó nǐ', 'trans': 'simulate'},\n{'word': '真实世界', 'pinyin': 'zhēn shí shì jiè', 'trans': 'real world'},\n{'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'},\n{'word': '多样性', 'pinyin': 'duō yàng xìng', 'trans': 'diversity'},\n{'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open source'},\n{'word': '专有', 'pinyin': 'zhuān yǒu', 'trans': 'proprietary'},\n{'word': '存在', 'pinyin': 'cún zài', 'trans': 'exist'},\n{'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '强调', 'pinyin': 'qiáng diào', 'trans': 'emphasize'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}]",
        "trans": "This article discusses the advancements in code generation by code large language models (codeLLMs). Previous code-related benchmark tests primarily focused on generating correct code snippets but overlooked consistency with human preferences. To address this gap, the authors propose a rigorous, human-crafted benchmark test called CodeArena, which simulates the complexity and diversity of real-world coding tasks. Through systematic experiments, the authors found a significant performance gap between open-source codeLLMs and proprietary LLMs, highlighting the importance of consistency with human preferences.",
        "update_ts": "2024-12-11 09:11"
    }
}