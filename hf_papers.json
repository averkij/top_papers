{
    "date": {
        "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 28",
        "zh": "8æœˆ28æ—¥"
    },
    "time_utc": "2025-08-28 03:30",
    "weekday": 3,
    "issue_id": 5586,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.19652",
            "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
            "url": "https://huggingface.co/papers/2508.19652",
            "abstract": "Vision-SR1 uses reinforcement learning to enhance visual reasoning in vision-language models by decomposing the process into visual perception and language reasoning stages, improving accuracy and reducing hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.",
            "score": 28,
            "issue_id": 5585,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "990502a2cc19d192",
            "authors": [
                "Zongxia Li",
                "Wenhao Yu",
                "Chengsong Huang",
                "Rui Liu",
                "Zhenwen Liang",
                "Fuxiao Liu",
                "Jingxi Che",
                "Dian Yu",
                "Jordan Boyd-Graber",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab, Seattle",
                "University of Maryland, College Park",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19652.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#hallucinations",
                    "#training",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Vision-SR1 - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Vision-SR1 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Self-Rewarding Learning",
                    "desc": "Vision-SR1 is a novel approach that enhances visual reasoning in vision-language models (VLMs) by using reinforcement learning. It breaks down the reasoning process into two distinct stages: visual perception and language reasoning, allowing for more focused training. By generating self-contained visual perceptions, the model can validate its understanding without needing to refer back to the original image. This method reduces visual hallucinations and reliance on language shortcuts, leading to improved accuracy in various vision-language tasks."
                },
                "zh": {
                    "title": "Vision-SR1ï¼šæå‡è§†è§‰æ¨ç†çš„è‡ªæˆ‘å¥–åŠ±æ–¹æ³•",
                    "desc": "Vision-SR1æ˜¯ä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºè§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†ä¸¤ä¸ªé˜¶æ®µï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å¹¶å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚é€šè¿‡è‡ªæˆ‘å¥–åŠ±æœºåˆ¶ï¼ŒVision-SR1ä¸ä¾èµ–å¤–éƒ¨è§†è§‰ç›‘ç£ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è®­ç»ƒæ¨¡å‹è¿›è¡Œæ›´å¥½çš„è§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVision-SR1åœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­æ˜¾è‘—æ”¹å–„äº†è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œé™ä½äº†å¯¹è¯­è¨€æ·å¾„çš„ä¾èµ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19493",
            "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered\n  Smartphone Agents",
            "url": "https://huggingface.co/papers/2508.19493",
            "abstract": "A large-scale benchmark evaluates the privacy awareness of smartphone agents powered by Multimodal Large Language Models, revealing significant gaps in their ability to protect sensitive user information.  \t\t\t\t\tAI-generated summary \t\t\t\t Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users' personal information during this operation. To gain a thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents' privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with a higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utility-privacy tradeoff about smartphone agents. Our code and benchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
            "score": 4,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "d7a43858b898f00a",
            "authors": [
                "Zhixin Lin",
                "Jungang Li",
                "Shidong Pan",
                "Yibo Shi",
                "Yue Yao",
                "Dongliang Xu"
            ],
            "affiliations": [
                "Columbia University",
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Shandong University",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19493.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#ethics",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "Ğ¡Ğ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° MLLM Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM), Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 7138 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‰ÑƒÑ 60% Ğ´Ğ°Ğ¶Ğµ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°Ñ‚ÑŒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Rethinking Privacy in Smartphone Agents: A Call for Better Awareness",
                    "desc": "This paper evaluates the privacy awareness of smartphone agents that use Multimodal Large Language Models (MLLMs). It presents a large-scale benchmark consisting of 7,138 scenarios to assess how well these agents protect sensitive user information. The findings reveal that most agents perform poorly in privacy awareness, with scores below 60%, and closed-source agents generally outperform open-source ones. The study highlights the need for a better balance between utility and privacy in the design of smartphone agents."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ‰‹æœºåŠ©æ‰‹çš„éšç§ä¿æŠ¤èƒ½åŠ›äºŸå¾…æå‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡è¯„ä¼°äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½æ‰‹æœºåŠ©æ‰‹åœ¨éšç§ä¿æŠ¤æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›åŠ©æ‰‹åœ¨å¤„ç†æ•æ„Ÿç”¨æˆ·ä¿¡æ¯æ—¶å­˜åœ¨æ˜¾è‘—çš„éšç§æ„è¯†ç¼ºå£ã€‚é€šè¿‡å¯¹7138ä¸ªåœºæ™¯è¿›è¡Œå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºå‡ ä¹æ‰€æœ‰è¢«æµ‹è¯•çš„åŠ©æ‰‹éšç§æ„è¯†è¡¨ç°ä¸ä½³ï¼Œå¾—åˆ†å‡ä½äº60%ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œé—­æºåŠ©æ‰‹çš„éšç§ä¿æŠ¤èƒ½åŠ›æ™®éä¼˜äºå¼€æºåŠ©æ‰‹ï¼Œä¸”åœºæ™¯çš„æ•æ„Ÿæ€§ä¸éšç§æ£€æµ‹èƒ½åŠ›å¯†åˆ‡ç›¸å…³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19229",
            "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
            "url": "https://huggingface.co/papers/2508.19229",
            "abstract": "A generative judge model, StepWiser, uses reinforcement learning to provide step-by-step reasoning feedback, improving both training and inference performance of policy models.  \t\t\t\t\tAI-generated summary \t\t\t\t As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search.",
            "score": 3,
            "issue_id": 5586,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "6ffd72787adea812",
            "authors": [
                "Wei Xiong",
                "Wenting Zhao",
                "Weizhe Yuan",
                "Olga Golovneva",
                "Tong Zhang",
                "Jason Weston",
                "Sainbayar Sukhbaatar"
            ],
            "affiliations": [
                "FAIR at Meta",
                "NYU",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19229.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞµÑ‚Ğ°-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "StepWiser - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑÑƒĞ´ÑŒÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. StepWiser Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "StepWiser: Enhancing Reasoning with Generative Feedback",
                    "desc": "The paper introduces StepWiser, a generative judge model that enhances the performance of policy models through reinforcement learning. It addresses the challenge of supervising multi-step reasoning by providing detailed feedback on each reasoning step, rather than just classifying them. Unlike traditional methods that rely on static datasets, StepWiser reframes the task to focus on reasoning itself, allowing for better generalization. The results demonstrate that StepWiser improves judgment accuracy, aids in training policy models, and enhances inference-time search capabilities."
                },
                "zh": {
                    "title": "é€æ­¥æ¨ç†çš„ç”Ÿæˆæ€§è¯„åˆ¤æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆæ€§è¯„åˆ¤æ¨¡å‹StepWiserï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æä¾›é€æ­¥æ¨ç†åé¦ˆï¼Œä»è€Œæå‡ç­–ç•¥æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ€§èƒ½ã€‚éšç€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨å¤šæ­¥æ¨ç†ç­–ç•¥æ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œç›‘ç£è¿™äº›ä¸­é—´æ­¥éª¤çš„é€»è¾‘æœ‰æ•ˆæ€§æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶æŒ‘æˆ˜ã€‚ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è™½ç„¶æä¾›é€æ­¥åé¦ˆï¼Œä½†é€šå¸¸ä»…ä½œä¸ºåˆ†ç±»å™¨ï¼Œç¼ºä¹è§£é‡Šï¼Œå¹¶ä¸”ä¾èµ–é™æ€æ•°æ®é›†çš„ç›‘ç£å¾®è°ƒé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚StepWiseré€šè¿‡å°†é€æ­¥å¥–åŠ±å»ºæ¨¡é‡æ–°æ„å»ºä¸ºæ¨ç†ä»»åŠ¡ï¼Œèƒ½å¤Ÿåœ¨ç»™å‡ºæœ€ç»ˆåˆ¤æ–­ä¹‹å‰è¾“å‡ºæ€è€ƒä»¤ç‰Œï¼Œä»è€Œæé«˜ä¸­é—´æ­¥éª¤çš„åˆ¤æ–­å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20096",
            "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer\n  Use Agent with Decoupled Reinforcement Learning",
            "url": "https://huggingface.co/papers/2508.20096",
            "abstract": "CODA, a trainable compositional framework, combines a generalist planner and specialist executor to achieve robust execution and cross-domain generalization in scientific computing GUIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.",
            "score": 2,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "8449940bace28db5",
            "authors": [
                "Zeyi Sun",
                "Yuhang Cao",
                "Jianze Liang",
                "Qiushi Sun",
                "Ziyu Liu",
                "Zhixiong Zhang",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Kai Chen",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20096.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#science"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CODA: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ñ‚Ğ°Ğ½Ğ´ĞµĞ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… GUI",
                    "desc": "CODA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº (Cerebrum) Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ (Cerebellum), Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñƒ. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº, Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. CODA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ScienceBoard, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "CODA: Bridging Planning and Execution for Robust AI in Science",
                    "desc": "CODA is a novel framework designed to improve the performance of autonomous agents in scientific computing GUIs by combining a generalist planner and a specialist executor. It addresses the limitations of existing methods that struggle with the trade-off between planning and execution. CODA employs a two-stage training process: first, it specializes the planner for individual tasks, and then it generalizes by aggregating successful task trajectories for fine-tuning. This approach allows CODA to achieve robust execution and effective cross-domain generalization, outperforming previous models in benchmark evaluations."
                },
                "zh": {
                    "title": "CODAï¼šç§‘å­¦è®¡ç®—çš„æ™ºèƒ½æ‰§è¡Œæ–°æ¡†æ¶",
                    "desc": "CODAæ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„ç»„åˆæ¡†æ¶ï¼Œç»“åˆäº†é€šç”¨è§„åˆ’å™¨å’Œä¸“ä¸šæ‰§è¡Œå™¨ï¼Œä»¥å®ç°ç§‘å­¦è®¡ç®—å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„ç¨³å¥æ‰§è¡Œå’Œè·¨é¢†åŸŸæ³›åŒ–ã€‚ç°æœ‰æ–¹æ³•åœ¨é€šç”¨ä»£ç†å’Œä¸“ä¸šä»£ç†ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œé€šç”¨ä»£ç†åœ¨è§„åˆ’ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†æ‰§è¡Œèƒ½åŠ›è¾ƒå·®ï¼Œè€Œä¸“ä¸šä»£ç†åˆ™ç›¸åã€‚CODAé€šè¿‡ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆä¸ºæ¯ä¸ªç§‘å­¦åº”ç”¨è®­ç»ƒä¸“å®¶è§„åˆ’å™¨ï¼Œç„¶åèšåˆæˆåŠŸçš„è½¨è¿¹è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä»è€Œå…‹æœäº†æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ç»è¿‡è¯„ä¼°ï¼ŒCODAåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åº”ç”¨ä¸­æ˜¾è‘—è¶…è¶Šäº†åŸºçº¿ï¼Œç¡®ç«‹äº†å¼€æºæ¨¡å‹çš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20072",
            "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies",
            "url": "https://huggingface.co/papers/2508.20072",
            "abstract": "Discrete Diffusion VLA uses a single-transformer policy with discrete diffusion to model actions, improving decoding order, consistency, and performance over autoregressive and continuous diffusion methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.",
            "score": 1,
            "issue_id": 5586,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "ec096752edb34e60",
            "authors": [
                "Zhixuan Liang",
                "Yizhuo Li",
                "Tianshuo Yang",
                "Chengyue Wu",
                "Sitong Mao",
                "Liuao Pei",
                "Xiaokang Yang",
                "Jiangmiao Pang",
                "Yao Mu",
                "Ping Luo"
            ],
            "affiliations": [
                "Huawei Cloud Computing Technologies Co., Ltd.",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20072.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#games",
                    "#cv",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² VLA Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Discrete Diffusion VLA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (Vision-Language-Action, VLA). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Discrete Diffusion VLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Action Modeling with Discrete Diffusion VLA",
                    "desc": "The paper introduces Discrete Diffusion VLA, a novel approach that utilizes a single-transformer policy to model actions through discrete diffusion. This method enhances the decoding process by allowing actions to be generated in an adaptive order, addressing simpler actions before more complex ones. By maintaining compatibility with the discrete token interface of vision-language models (VLMs), it simplifies training and improves performance without the need for specialized iterative sampling. The results demonstrate significant improvements in action modeling accuracy and consistency, paving the way for scaling VLA applications to larger datasets and models."
                },
                "zh": {
                    "title": "ç¦»æ•£æ‰©æ•£VLAï¼šæå‡åŠ¨ä½œå»ºæ¨¡ä¸ä¸€è‡´æ€§",
                    "desc": "ç¦»æ•£æ‰©æ•£VLAä½¿ç”¨å•ä¸€å˜æ¢å™¨ç­–ç•¥ï¼Œé€šè¿‡ç¦»æ•£æ‰©æ•£æ¥å»ºæ¨¡åŠ¨ä½œï¼Œæ”¹å–„äº†è§£ç é¡ºåºã€ä¸€è‡´æ€§å’Œæ€§èƒ½ï¼Œä¼˜äºè‡ªå›å½’å’Œè¿ç»­æ‰©æ•£æ–¹æ³•ã€‚è¯¥æ¨¡å‹å°†å›¾åƒå’ŒæŒ‡ä»¤æ˜ å°„åˆ°æœºå™¨äººåŠ¨ä½œï¼Œé‡‡ç”¨ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç›¸åŒçš„äº¤å‰ç†µç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚è®¾è®¡ä¿ç•™äº†æ‰©æ•£çš„æ¸è¿›ç»†åŒ–èŒƒå¼ï¼ŒåŒæ—¶ä¸VLMçš„ç¦»æ•£ä»¤ç‰Œæ¥å£å…¼å®¹ã€‚ç¦»æ•£æ‰©æ•£VLAåœ¨LIBEROä¸Šå®ç°äº†96.3%çš„å¹³å‡æˆåŠŸç‡ï¼Œè¡¨æ˜å…¶æ”¯æŒç²¾ç¡®çš„åŠ¨ä½œå»ºæ¨¡å’Œä¸€è‡´çš„è®­ç»ƒï¼Œä¸ºå°†VLAæ‰©å±•åˆ°æ›´å¤§æ¨¡å‹å’Œæ•°æ®é›†å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-27.html",
    "link_next": "2025-08-29.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "27.08",
        "en": "08/27",
        "zh": "8æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "29.08",
        "en": "08/29",
        "zh": "8æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 3,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}