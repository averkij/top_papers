{
    "date": {
        "ru": "29 апреля",
        "en": "April 29",
        "zh": "4月29日"
    },
    "time_utc": "2025-04-29 02:26",
    "weekday": 1,
    "issue_id": 3479,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.19838",
            "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
            "url": "https://huggingface.co/papers/2504.19838",
            "abstract": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.",
            "score": 6,
            "issue_id": 3479,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 апреля",
                "en": "April 28",
                "zh": "4月28日"
            },
            "hash": "367cc59f20116daa",
            "authors": [
                "Guangyi Liu",
                "Pengxiang Zhao",
                "Liang Liu",
                "Yaxuan Guo",
                "Han Xiao",
                "Weifeng Lin",
                "Yuxiang Chai",
                "Yue Han",
                "Shuai Ren",
                "Hao Wang",
                "Xiaoyu Liang",
                "Wenhao Wang",
                "Tianze Wu",
                "Linghao Li",
                "Hao Wang",
                "Guanjing Xiong",
                "Yong Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Zhejiang University",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19838.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#survey",
                    "#dataset",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#security"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "LLM революционизируют автоматизацию мобильных интерфейсов",
                    "desc": "Статья представляет систематический обзор агентов графического интерфейса для телефонов, управляемых большими языковыми моделями (LLM). Рассматривается эволюция от скриптовой автоматизации к интеллектуальным адаптивным системам, решающим проблемы ограниченной универсальности, высоких затрат на обслуживание и слабого понимания намерений. Авторы предлагают таксономию, охватывающую основные фреймворки агентов, подходы к моделированию и ключевые наборы данных. Обсуждаются открытые проблемы, такие как разнообразие данных, эффективность развертывания на устройствах и вопросы безопасности."
                },
                "en": {
                    "title": "Transforming Phone Automation with Intelligent Language Models",
                    "desc": "This paper reviews the advancements in phone automation driven by large language models (LLMs). It discusses how LLMs have evolved phone GUI agents from simple script-based systems to intelligent, adaptive agents that can understand user intent better. The authors identify key challenges in the field, such as limited generality and high maintenance needs, and explain how LLMs improve these areas through better language understanding and decision-making. Additionally, the paper proposes a taxonomy for agent frameworks and modeling approaches, while addressing ongoing challenges and future directions for research in this area."
                },
                "zh": {
                    "title": "大型语言模型驱动的手机自动化变革",
                    "desc": "随着大型语言模型（LLMs）的快速发展，手机自动化发生了变革性变化。本文系统回顾了基于LLM的手机图形用户界面（GUI）代理，强调了它们从基于脚本的自动化到智能自适应系统的演变。我们首先阐明了关键挑战，包括有限的通用性、高维护成本和意图理解薄弱，并展示了LLM如何通过先进的语言理解、多模态感知和强大的决策能力来解决这些问题。最后，我们讨论了数据集多样性、设备端部署效率、以用户为中心的适应性和安全性等开放挑战，为这一快速发展的领域提供前瞻性见解。"
                }
            }
        }
    ],
    "link_prev": "2025-04-28.html",
    "link_next": "2025-04-30.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "28.04",
        "en": "04/28",
        "zh": "4月28日"
    },
    "short_date_next": {
        "ru": "30.04",
        "en": "04/30",
        "zh": "4月30日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 0,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了 CameraBench，一个大规模的数据集和基准，旨在评估和改进摄像机运动理解。CameraBench 包含约 3,000 个多样化的网络视频，由专家通过严格的多阶段质量控制过程进行标注。我们与摄影师合作，设计了摄像机运动基元的分类法。例如，某些运动如“跟随”需要理解场景内容，如移动的主体。我们进行了大规模的人类研究，以量化人类标注性能，发现领域专业知识和基于教程的培训可显著提高准确性。使用 CameraBench，我们评估了结构从运动（SfM）和视频语言模型（VLMs），发现 SfM 模型难以捕捉依赖场景内容的语义基元，而 VLMs 难以捕捉需要精确轨迹估计的几何基元。我们随后在 CameraBench 上微调了一个生成 VLM，以实现两者的优势结合，并展示其应用，包括运动增强的字幕、视频问答和视频文本检索。我们希望我们的分类法、基准和教程能推动未来努力，实现理解任何视频中摄像机运动的最终目标。",
        "title": "Towards Understanding Camera Motions in Any Video",
        "pinyin": "Wǒmen jièshào le CameraBench, yīgè dàguīmó de shùjùjí hé jīzhǔn, zhǐyǐn pínggǔ hé gǎijìn shèxiàngjī yùndòng lǐjiě. CameraBench bāohán yuē 3,000 gè duōyànghuà de wǎngluò shìpǐn, yóu zhuānjiā tōngguò yánzhòng de duō jiēduàn zhìliàng kòngzhì guòchéng jìnxiàng biāozhù. Wǒmen yǔ shèyǐngshī hézuò, shèjì le shèxiàngjī yùndòng jīyuǎn de fēnlèi fǎ. Lìrú, mǒuxiē yùndòng rú “gēnsuí” xūyào lǐjiě chǎngjīng nèiróng, rú yídòng de zhǔtǐ. Wǒmen jìnxíng le dàguīmó de rénlèi yánjiū, yǐ liàngzhì rénlèi biāozhù xìngnéng, fāxiàn lǐngyù zhuānjì zhīshi hé jīyú jiàoxué de péixùn kě xiǎnzhù tīgāo zhùnquèxìng. Shǐyòng CameraBench, wǒmen pínggǔ le jiégòu cóng yùndòng (SfM) hé shìpǐn yǔyán móxíng (VLMs), fāxiàn SfM móxíng nán yǐ bǔzhòu yīlài chǎngjīng nèiróng de yǔyán jīyuǎn, ér VLMs nán yǐ bǔzhòu xūyào jīngquè guǐjì gūjì de jǐhé jīyuǎn. Wǒmen shùhòu zài CameraBench shàng wēitiáo le yīgè shēngchéng VLM, yǐ shíxiàn liǎng zhě de yōushì jiēhé, bìng zhǎnshì qí yìngyòng, bāokuò yùndòng zēngqiáng de zìmǔ, shìpǐn wèndá hé shìpǐn wénběn cháxún. Wǒmen xīwàng wǒmen de fēnlèi fǎ, jīzhǔn hé jiàoxué néng tuīdòng wèilái nǔlì, shíxiàn lǐjiě rènhé shìpǐn zhōng shèxiàngjī yùndòng de zhòngdiǎn mùbiāo.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improve\"},\n    {\"word\": \"摄像机\", \"pinyin\": \"shè xiàng jī\", \"trans\": \"camera\"},\n    {\"word\": \"运动\", \"pinyin\": \"yùn dòng\", \"trans\": \"motion\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understand\"},\n    {\"word\": \"多样化\", \"pinyin\": \"duō yàng huà\", \"trans\": \"diverse\"},\n    {\"word\": \"网络视频\", \"pinyin\": \"wǎng luò shì pín\", \"trans\": \"online video\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuān jiā\", \"trans\": \"expert\"},\n    {\"word\": \"严格\", \"pinyin\": \"yán gé\", \"trans\": \"strict\"},\n    {\"word\": \"多阶段\", \"pinyin\": \"duō jiē duàn\", \"trans\": \"multi-stage\"},\n    {\"word\": \"质量控制\", \"pinyin\": \"zhì liàng kòng zhì\", \"trans\": \"quality control\"},\n    {\"word\": \"过程\", \"pinyin\": \"guò chéng\", \"trans\": \"process\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotate\"},\n    {\"word\": \"摄影师\", \"pinyin\": \"shè yǐng shī\", \"trans\": \"photographer\"},\n    {\"word\": \"设计\", \"pinyin\": \"shè jì\", \"trans\": \"design\"},\n    {\"word\": \"基元\", \"pinyin\": \"jī yuán\", \"trans\": \"primitive\"},\n    {\"word\": \"分类法\", \"pinyin\": \"fēn lèi fǎ\", \"trans\": \"classification method\"},\n    {\"word\": \"例如\", \"pinyin\": \"lì rú\", \"trans\": \"for example\"},\n    {\"word\": \"某些\", \"pinyin\": \"mǒu xiē\", \"trans\": \"some\"},\n    {\"word\": \"跟随\", \"pinyin\": \"gēn suí\", \"trans\": \"follow\"},\n    {\"word\": \"需要\", \"pinyin\": \"xū yào\", \"trans\": \"need\"},\n    {\"word\": \"场景\", \"pinyin\": \"chǎng jǐng\", \"trans\": \"scene\"},\n    {\"word\": \"内容\", \"pinyin\": \"nèi róng\", \"trans\": \"content\"},\n    {\"word\": \"主体\", \"pinyin\": \"zhǔ tǐ\", \"trans\": \"subject\"},\n    {\"word\": \"移动\", \"pinyin\": \"yí dòng\", \"trans\": \"move\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"study\"},\n    {\"word\": \"量化\", \"pinyin\": \"liàng huà\", \"trans\": \"quantify\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"field\"},\n    {\"word\": \"专业知识\", \"pinyin\": \"zhuān yè zhī shi\", \"trans\": \"professional knowledge\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"教程\", \"pinyin\": \"jiào chéng\", \"trans\": \"tutorial\"},\n    {\"word\": \"培训\", \"pinyin\": \"péi xùn\", \"trans\": \"training\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"准确性\", \"pinyin\": \"zhǔn què xìng\", \"trans\": \"accuracy\"},\n    {\"word\": \"结构从运动\", \"pinyin\": \"jié gòu cóng yùn dòng\", \"trans\": \"Structure from Motion (SfM)\"},\n    {\"word\": \"视频语言模型\", \"pinyin\": \"shì pín yǔ yán mó xíng\", \"trans\": \"Video Language Model (VLM)\"},\n    {\"word\": \"捕捉\", \"pinyin\": \"bǔ zhuō\", \"trans\": \"capture\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantic\"},\n    {\"word\": \"几何\", \"pinyin\": \"jǐ hé\", \"trans\": \"geometric\"},\n    {\"word\": \"轨迹\", \"pinyin\": \"guǐ jī\", \"trans\": \"trajectory\"},\n    {\"word\": \"估计\", \"pinyin\": \"gū jì\", \"trans\": \"estimate\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tune\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"优势\", \"pinyin\": \"yōu shì\", \"trans\": \"advantage\"},\n    {\"word\": \"结合\", \"pinyin\": \"jié hé\", \"trans\": \"combine\"},\n    {\"word\": \"展示\", \"pinyin\": \"zhǎn shì\", \"trans\": \"demonstrate\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"包括\", \"pinyin\": \"bāo kuò\", \"trans\": \"include\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēng qiáng\", \"trans\": \"enhance\"},\n    {\"word\": \"字幕\", \"pinyin\": \"zì mù\", \"trans\": \"subtitle\"},\n    {\"word\": \"问答\", \"pinyin\": \"wèn dá\", \"trans\": \"question and answer\"},\n    {\"word\": \"文本检索\", \"pinyin\": \"wén běn jiǎn suǒ\", \"trans\": \"text retrieval\"},\n    {\"word\": \"希望\", \"pinyin\": \"xī wàng\", \"trans\": \"hope\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuī dòng\", \"trans\": \"promote\"},\n    {\"word\": \"未来\", \"pinyin\": \"wèi lái\", \"trans\": \"future\"},\n    {\"word\": \"努力\", \"pinyin\": \"nǔ lì\", \"trans\": \"effort\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"最终\", \"pinyin\": \"zuì zhōng\", \"trans\": \"ultimate\"},\n    {\"word\": \"目标\", \"pinyin\": \"mù biāo\", \"trans\": \"goal\"}\n]",
        "trans": "We introduce CameraBench, a large-scale dataset and benchmark aimed at evaluating and improving the understanding of camera motion. CameraBench contains approximately 3,000 diverse web videos, annotated by experts through a rigorous multi-stage quality control process. We collaborated with photographers to design a classification scheme for camera motion primitives. For instance, certain motions like \"following\" require an understanding of scene content, such as moving subjects. We conducted large-scale human studies to quantify human annotation performance and found that domain expertise and tutorial-based training can significantly improve accuracy. Using CameraBench, we evaluated Structure-from-Motion (SfM) and Video Language Models (VLMs), finding that SfM models struggle to capture semantic primitives dependent on scene content, while VLMs struggle with geometric primitives that require precise trajectory estimation. Subsequently, we fine-tuned a generative VLM on CameraBench to combine the strengths of both, demonstrating its applications, including motion-enhanced captioning, video question answering, and video-text retrieval. We hope that our classification scheme, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motion in any video.",
        "update_ts": "2025-04-28 10:45"
    }
}