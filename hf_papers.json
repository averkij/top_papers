{
    "date": {
        "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 19",
        "zh": "12æœˆ19æ—¥"
    },
    "time_utc": "2025-12-19 05:23",
    "weekday": 4,
    "issue_id": 141,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.13507",
            "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
            "url": "https://huggingface.co/papers/2512.13507",
            "abstract": "Seedance 1.5 pro, a dual-branch Diffusion Transformer model, achieves high-quality audio-visual synchronization and generation through cross-modal integration, post-training optimizations, and an acceleration framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
            "score": 19,
            "issue_id": 140,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "79abcc313b354256",
            "authors": [
                "Heyi Chen",
                "Siyan Chen",
                "Xin Chen",
                "Yanfei Chen",
                "Ying Chen",
                "Zhuo Chen",
                "Feng Cheng",
                "Tianheng Cheng",
                "Xinqi Cheng",
                "Xuyan Chi",
                "Jian Cong",
                "Jing Cui",
                "Qinpeng Cui",
                "Qide Dong",
                "Junliang Fan",
                "Jing Fang",
                "Zetao Fang",
                "Chengjian Feng",
                "Han Feng",
                "Mingyuan Gao",
                "Yu Gao",
                "Dong Guo",
                "Qiushan Guo",
                "Boyang Hao",
                "Qingkai Hao",
                "Bibo He",
                "Qian He",
                "Tuyen Hoang",
                "Ruoqing Hu",
                "Xi Hu",
                "Weilin Huang",
                "Zhaoyang Huang",
                "Zhongyi Huang",
                "Donglei Ji",
                "Siqi Jiang",
                "Wei Jiang",
                "Yunpu Jiang",
                "Zhuo Jiang",
                "Ashley Kim",
                "Jianan Kong",
                "Zhichao Lai",
                "Shanshan Lao",
                "Yichong Leng",
                "Ai Li",
                "Feiya Li",
                "Gen Li",
                "Huixia Li",
                "JiaShi Li",
                "Liang Li",
                "Ming Li",
                "Shanshan Li",
                "Tao Li",
                "Xian Li",
                "Xiaojie Li",
                "Xiaoyang Li",
                "Xingxing Li",
                "Yameng Li",
                "Yifu Li",
                "Yiying Li",
                "Chao Liang",
                "Han Liang",
                "Jianzhong Liang",
                "Ying Liang",
                "Zhiqiang Liang",
                "Wang Liao",
                "Yalin Liao",
                "Heng Lin",
                "Kengyu Lin",
                "Shanchuan Lin",
                "Xi Lin",
                "Zhijie Lin",
                "Feng Ling",
                "Fangfang Liu",
                "Gaohong Liu",
                "Jiawei Liu",
                "Jie Liu",
                "Jihao Liu",
                "Shouda Liu",
                "Shu Liu",
                "Sichao Liu",
                "Songwei Liu",
                "Xin Liu",
                "Xue Liu",
                "Yibo Liu",
                "Zikun Liu",
                "Zuxi Liu",
                "Junlin Lyu",
                "Lecheng Lyu",
                "Qian Lyu",
                "Han Mu",
                "Xiaonan Nie",
                "Jingzhe Ning",
                "Xitong Pan",
                "Yanghua Peng",
                "Lianke Qin",
                "Xueqiong Qu",
                "Yuxi Ren",
                "Kai Shen",
                "Guang Shi",
                "Lei Shi",
                "Yan Song",
                "Yinglong Song",
                "Fan Sun",
                "Li Sun",
                "Renfei Sun",
                "Yan Sun",
                "Zeyu Sun",
                "Wenjing Tang",
                "Yaxue Tang",
                "Zirui Tao",
                "Feng Wang",
                "Furui Wang",
                "Jinran Wang",
                "Junkai Wang",
                "Ke Wang",
                "Kexin Wang",
                "Qingyi Wang",
                "Rui Wang",
                "Sen Wang",
                "Shuai Wang",
                "Tingru Wang",
                "Weichen Wang",
                "Xin Wang",
                "Yanhui Wang",
                "Yue Wang",
                "Yuping Wang",
                "Yuxuan Wang",
                "Ziyu Wang",
                "Guoqiang Wei",
                "Wanru Wei",
                "Di Wu",
                "Guohong Wu",
                "Hanjie Wu",
                "Jian Wu",
                "Jie Wu",
                "Ruolan Wu",
                "Xinglong Wu",
                "Yonghui Wu",
                "Ruiqi Xia",
                "Liang Xiang",
                "Fei Xiao",
                "XueFeng Xiao",
                "Pan Xie",
                "Shuangyi Xie",
                "Shuang Xu",
                "Jinlan Xue",
                "Shen Yan",
                "Bangbang Yang",
                "Ceyuan Yang",
                "Jiaqi Yang",
                "Runkai Yang",
                "Tao Yang",
                "Yang Yang",
                "Yihang Yang",
                "ZhiXian Yang",
                "Ziyan Yang",
                "Songting Yao",
                "Yifan Yao",
                "Zilyu Ye",
                "Bowen Yu",
                "Jian Yu",
                "Chujie Yuan",
                "Linxiao Yuan",
                "Sichun Zeng",
                "Weihong Zeng",
                "Xuejiao Zeng",
                "Yan Zeng",
                "Chuntao Zhang",
                "Heng Zhang",
                "Jingjie Zhang",
                "Kuo Zhang",
                "Liang Zhang",
                "Liying Zhang",
                "Manlin Zhang",
                "Ting Zhang",
                "Weida Zhang",
                "Xiaohe Zhang",
                "Xinyan Zhang",
                "Yan Zhang",
                "Yuan Zhang",
                "Zixiang Zhang",
                "Fengxuan Zhao",
                "Huating Zhao",
                "Yang Zhao",
                "Hao Zheng",
                "Jianbin Zheng",
                "Xiaozheng Zheng",
                "Yangyang Zheng",
                "Yijie Zheng",
                "Jiexin Zhou",
                "Jiahui Zhu",
                "Kuan Zhu",
                "Shenhan Zhu",
                "Wenjia Zhu",
                "Benhui Zou",
                "Feilong Zuo"
            ],
            "affiliations": [
                "ByteDance",
                "Volcano Engine"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13507.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#open_source",
                    "#video",
                    "#architecture",
                    "#diffusion",
                    "#audio",
                    "#inference",
                    "#multimodal",
                    "#training",
                    "#multilingual"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹",
                    "desc": "Seedance 1.5 pro â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Diffusion Transformer, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¾ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ supervised fine-tuning Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ reinforcement learning from human feedback Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 10 Ñ€Ğ°Ğ·, Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ³ÑƒĞ± Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Audio-Visual Generation with Seedance 1.5 Pro",
                    "desc": "Seedance 1.5 pro is a dual-branch Diffusion Transformer model designed for high-quality audio-visual generation. It utilizes a cross-modal joint module to synchronize audio and video effectively, ensuring that they match perfectly. The model undergoes post-training optimizations like Supervised Fine-Tuning and Reinforcement Learning from Human Feedback to enhance its performance. Additionally, it features an acceleration framework that significantly increases inference speed, making it suitable for professional content creation with advanced capabilities like multilingual lip-syncing and dynamic camera control."
                },
                "zh": {
                    "title": "éŸ³è§†é¢‘ç”Ÿæˆçš„æ–°çºªå…ƒï¼šSeedance 1.5 pro",
                    "desc": "Seedance 1.5 pro æ˜¯ä¸€ç§åŒåˆ†æ”¯æ‰©æ•£å˜æ¢å™¨æ¨¡å‹ï¼Œä¸“æ³¨äºéŸ³è§†é¢‘çš„åŒæ­¥ç”Ÿæˆã€‚è¯¥æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€é›†æˆå’Œå¤šé˜¶æ®µæ•°æ®å¤„ç†ç®¡é“ï¼Œå®ç°äº†é«˜è´¨é‡çš„éŸ³è§†é¢‘åŒæ­¥ã€‚ä¸ºäº†æé«˜å®ç”¨æ€§ï¼Œç ”ç©¶è€…è¿›è¡Œäº†ç»†è‡´çš„åæœŸè®­ç»ƒä¼˜åŒ–ï¼ŒåŒ…æ‹¬åœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šçš„ç›‘ç£å¾®è°ƒå’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ã€‚è¯¥æ¨¡å‹è¿˜å¼•å…¥äº†åŠ é€Ÿæ¡†æ¶ï¼Œä½¿æ¨ç†é€Ÿåº¦æå‡è¶…è¿‡10å€ï¼Œé€‚ç”¨äºä¸“ä¸šå†…å®¹åˆ›ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16913",
            "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
            "url": "https://huggingface.co/papers/2512.16913",
            "abstract": "A panoramic metric depth foundation model using DINOv3-Large and a three-stage pseudo-label pipeline achieves robust performance across diverse real-world scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP_website/ {https://insta360-research-team.github.io/DAP\\_website/}",
            "score": 18,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "9da6c70ec001e519",
            "authors": [
                "Xin Lin",
                "Meixi Song",
                "Dizhe Zhang",
                "Wenxuan Lu",
                "Haodong Li",
                "Bo Du",
                "Ming-Hsuan Yang",
                "Truong Nguyen",
                "Lu Qi"
            ],
            "affiliations": [
                "Insta360 Research",
                "University of California, Merced",
                "University of California, San Diego",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16913.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#architecture",
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° UE5 Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ‚Ğ¾ Ñ Ğ²ĞµĞ±. Ğ§Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ domain gap Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸/ÑƒĞ»Ğ¸Ñ†ĞµĞ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸/Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ ground truth. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ DINOv3-Large ĞºĞ°Ğº backbone Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ range mask head, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµĞ·ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…."
                },
                "en": {
                    "title": "Robust Depth Prediction Across Diverse Scenes with DINOv3-Large",
                    "desc": "This paper introduces a panoramic metric depth foundation model that effectively handles various real-world scenes using DINOv3-Large. The authors utilize a data-in-the-loop approach, combining public datasets, synthetic data, and real images to create a comprehensive dataset. To address the challenges of domain gaps, they implement a three-stage pseudo-label pipeline that generates reliable ground truth for unlabeled images. The model demonstrates strong performance and zero-shot generalization across multiple benchmarks, showcasing its robustness in predicting depth metrics in diverse environments."
                },
                "zh": {
                    "title": "å…¨æ™¯æ·±åº¦æ¨¡å‹ï¼šè·¨åœºæ™¯çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ™¯åº¦é‡æ·±åº¦åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨DINOv3-Largeå’Œä¸‰é˜¶æ®µä¼ªæ ‡ç­¾ç®¡é“ï¼Œåœ¨å¤šæ ·åŒ–çš„çœŸå®åœºæ™¯ä¸­å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨æ•°æ®å¾ªç¯çš„æ–¹å¼ï¼Œç»“åˆå…¬å…±æ•°æ®é›†ã€é«˜è´¨é‡åˆæˆæ•°æ®å’ŒçœŸå®å…¨æ™¯å›¾åƒï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºäº†å‡å°‘å®¤å†…/å®¤å¤–å’Œåˆæˆ/çœŸå®æ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰é˜¶æ®µä¼ªæ ‡ç­¾ç­–åˆ’ç®¡é“ï¼Œä»¥ç”Ÿæˆå¯é çš„æ— æ ‡ç­¾å›¾åƒçš„çœŸå®æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„çœŸå®åœºæ™¯ä¸­æä¾›ç¨³å®šçš„åº¦é‡é¢„æµ‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16922",
            "title": "Next-Embedding Prediction Makes Strong Vision Learners",
            "url": "https://huggingface.co/papers/2512.16922",
            "abstract": "Generative pretraining using next embedding prediction outperforms traditional self-supervised methods in visual learning tasks, achieving high accuracy on ImageNet and effective transfer to semantic segmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.",
            "score": 15,
            "issue_id": 140,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "b0d231fe2492fa1b",
            "authors": [
                "Sihan Xu",
                "Ziqiao Ma",
                "Wenhao Chai",
                "Xuweiyi Chen",
                "Weiyang Jin",
                "Joyce Chai",
                "Saining Xie",
                "Stella X. Yu"
            ],
            "affiliations": [
                "New York University",
                "Princeton University",
                "University of Michigan",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16922.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ NEPA (Next-Embedding Predictive Autoregression), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸ stop gradient. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ· NLP Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ±ĞµĞ· Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹: 83.8% Ğ¸ 85.3% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ top-1 Ğ½Ğ° ImageNet-1K Ñ ViT-B Ğ¸ ViT-L ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ fine-tuning. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Next-Embedding Prediction: A New Era in Visual Learning",
                    "desc": "This paper introduces a new approach called Next-Embedding Predictive Autoregression (NEPA) for self-supervised visual learning. Instead of traditional methods that focus on learning representations, NEPA trains models to predict future embeddings based on past ones, using a simple Transformer architecture. The results show that this method achieves high accuracy on ImageNet and performs well in transferring to tasks like semantic segmentation. Overall, NEPA offers a scalable and straightforward alternative to existing visual self-supervised learning techniques."
                },
                "zh": {
                    "title": "ä¸‹ä¸€åµŒå…¥é¢„æµ‹ï¼šè§†è§‰å­¦ä¹ çš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£è§†è§‰å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºä¸‹ä¸€åµŒå…¥é¢„æµ‹è‡ªå›å½’ï¼ˆNEPAï¼‰ã€‚ä¸ä¼ ç»Ÿçš„è‡ªç›‘ç£æ–¹æ³•ä¸åŒï¼ŒNEPAé€šè¿‡é¢„æµ‹æœªæ¥çš„åµŒå…¥æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸æ˜¯ä»…ä»…è¾“å‡ºç‰¹å¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•çš„ç®€å•Transformeræ¨¡å‹åœ¨ImageNet-1Kä¸Šè¾¾åˆ°äº†83.8%å’Œ85.3%çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°è‰¯å¥½ã€‚è¯¥æ–¹æ³•ä¿æŒäº†æ¶æ„çš„ç®€å•æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¸ºè§†è§‰è‡ªç›‘ç£å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16915",
            "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
            "url": "https://huggingface.co/papers/2512.16915",
            "abstract": "StereoPilot, a feed-forward model leveraging a learnable domain switcher and cycle consistency loss, synthesizes high-quality stereo video directly without depth maps, outperforming existing methods in visual fidelity and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.",
            "score": 14,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "9098462f35e68746",
            "authors": [
                "Guibao Shen",
                "Yihua Du",
                "Wenhang Ge",
                "Jing He",
                "Chirui Chang",
                "Donghao Zhou",
                "Zhen Yang",
                "Luozhou Wang",
                "Xin Tao",
                "Ying-Cong Chen"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "HKUST(GZ)",
                "Kling Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16915.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸÑ€ÑĞ¼Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑÑ‚ĞµÑ€ĞµĞ¾Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ StereoPilot, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ÑÑ‚ĞµÑ€ĞµĞ¾Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UniStereo Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ° ÑÑ‚ĞµÑ€ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ StereoPilot Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Revolutionizing Stereo Video Synthesis with StereoPilot",
                    "desc": "StereoPilot is a novel feed-forward model designed for synthesizing high-quality stereo video without the need for depth maps. It utilizes a learnable domain switcher and cycle consistency loss to enhance adaptability across different stereo formats. This approach addresses common issues in traditional Monocular-to-Stereo conversion methods, such as error propagation and depth ambiguity. Extensive testing shows that StereoPilot surpasses existing techniques in both visual quality and computational efficiency, making it a significant advancement in stereo video generation."
                },
                "zh": {
                    "title": "StereoPilotï¼šé«˜æ•ˆåˆæˆç«‹ä½“è§†é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "StereoPilotæ˜¯ä¸€ç§å‰é¦ˆæ¨¡å‹ï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„é¢†åŸŸåˆ‡æ¢å™¨å’Œå¾ªç¯ä¸€è‡´æ€§æŸå¤±ï¼Œèƒ½å¤Ÿç›´æ¥åˆæˆé«˜è´¨é‡çš„ç«‹ä½“è§†é¢‘ï¼Œè€Œæ— éœ€æ·±åº¦å›¾ã€‚è¯¥æ¨¡å‹åœ¨è§†è§‰ä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿå•ç›®åˆ°ç«‹ä½“è½¬æ¢ä¸­çš„æ·±åº¦æ¨¡ç³Šå’Œæ ¼å¼ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚é€šè¿‡å¼•å…¥UniStereoæ•°æ®é›†ï¼ŒStereoPilotèƒ½å¤Ÿåœ¨ä¸åŒçš„ç«‹ä½“æ ¼å¼ä¹‹é—´æ— ç¼é€‚åº”ï¼Œå¹¶å®ç°æ›´å¥½çš„ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStereoPilotåœ¨è§†è§‰è´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16776",
            "title": "Kling-Omni Technical Report",
            "url": "https://huggingface.co/papers/2512.16776",
            "abstract": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
            "score": 13,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "d9dc39f81fe6fbef",
            "authors": [
                "Kling Team",
                "Jialu Chen",
                "Yuanzheng Ci",
                "Xiangyu Du",
                "Zipeng Feng",
                "Kun Gai",
                "Sainan Guo",
                "Feng Han",
                "Jingbin He",
                "Kang He",
                "Xiao Hu",
                "Xiaohua Hu",
                "Boyuan Jiang",
                "Fangyuan Kong",
                "Hang Li",
                "Jie Li",
                "Qingyu Li",
                "Shen Li",
                "Xiaohan Li",
                "Yan Li",
                "Jiajun Liang",
                "Borui Liao",
                "Yiqiao Liao",
                "Weihong Lin",
                "Quande Liu",
                "Xiaokun Liu",
                "Yilun Liu",
                "Yuliang Liu",
                "Shun Lu",
                "Hangyu Mao",
                "Yunyao Mao",
                "Haodong Ouyang",
                "Wenyu Qin",
                "Wanqi Shi",
                "Xiaoyu Shi",
                "Lianghao Su",
                "Haozhi Sun",
                "Peiqin Sun",
                "Pengfei Wan",
                "Chao Wang",
                "Chenyu Wang",
                "Meng Wang",
                "Qiulin Wang",
                "Runqi Wang",
                "Xintao Wang",
                "Xuebo Wang",
                "Zekun Wang",
                "Min Wei",
                "Tiancheng Wen",
                "Guohao Wu",
                "Xiaoshi Wu",
                "Zhenhua Wu",
                "Da Xie",
                "Yingtong Xiong",
                "Yulong Xu",
                "Sile Yang",
                "Zikang Yang",
                "Weicai Ye",
                "Ziyang Yuan",
                "Shenglong Zhang",
                "Shuaiyu Zhang",
                "Yuanxing Zhang",
                "Yufan Zhang",
                "Wenzheng Zhao",
                "Ruiliang Zhou",
                "Yan Zhou",
                "Guosheng Zhu",
                "Yongjie Zhu"
            ],
            "affiliations": [
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16776.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ»ÑĞ±Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Kling-Omni â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. Kling-Omni Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."
                },
                "en": {
                    "title": "Kling-Omni: Unifying Video Creation through Multimodal Inputs",
                    "desc": "Kling-Omni is a cutting-edge generative framework that creates high-quality videos from various types of inputs, such as text and images. It combines video generation, editing, and reasoning into a single, cohesive system, unlike traditional methods that treat these tasks separately. By processing multimodal inputs into a unified representation, Kling-Omni can produce cinematic-quality videos that intelligently respond to user instructions. This framework is built on a robust data system and enhanced by large-scale pre-training, making it a significant step towards advanced multimodal content creation and interaction."
                },
                "zh": {
                    "title": "Kling-Omniï¼šå¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆçš„æœªæ¥",
                    "desc": "Kling-Omniæ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥ä»å¤šæ¨¡æ€è¾“å…¥åˆæˆé«˜è´¨é‡çš„è§†é¢‘ã€‚å®ƒå°†è§†é¢‘ç”Ÿæˆã€ç¼–è¾‘å’Œæ¨ç†ä»»åŠ¡æ•´åˆä¸ºä¸€ä¸ªç»Ÿä¸€çš„ç³»ç»Ÿï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯çš„è§†è§’ã€‚ä¸ä¼ ç»Ÿçš„åˆ†ç¦»å¼æµç¨‹ä¸åŒï¼ŒKling-Omniæ”¯æŒå¤šç§ç”¨æˆ·è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬æŒ‡ä»¤ã€å‚è€ƒå›¾åƒå’Œè§†é¢‘ä¸Šä¸‹æ–‡ï¼Œèƒ½å¤Ÿç”Ÿæˆç”µå½±çº§åˆ«çš„æ™ºèƒ½è§†é¢‘å†…å®¹ã€‚è¯¥æ¡†æ¶é€šè¿‡é«˜æ•ˆçš„å¤§è§„æ¨¡é¢„è®­ç»ƒç­–ç•¥å’ŒåŸºç¡€è®¾æ–½ä¼˜åŒ–ï¼Œå±•ç¤ºäº†åœ¨ä¸Šä¸‹æ–‡ç”Ÿæˆã€åŸºäºæ¨ç†çš„ç¼–è¾‘å’Œå¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšæ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16301",
            "title": "Adaptation of Agentic AI",
            "url": "https://huggingface.co/papers/2512.16301",
            "abstract": "This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
            "score": 13,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "e95fabb54f043168",
            "authors": [
                "Pengcheng Jiang",
                "Jiacheng Lin",
                "Zhiyi Shi",
                "Zifeng Wang",
                "Luxi He",
                "Yichen Wu",
                "Ming Zhong",
                "Peiyang Song",
                "Qizheng Zhang",
                "Heng Wang",
                "Xueqiang Xu",
                "Hanwen Xu",
                "Pengrui Han",
                "Dylan Zhang",
                "Jiashuo Sun",
                "Chaoqi Yang",
                "Kun Qian",
                "Tian Wang",
                "Changran Hu",
                "Manling Li",
                "Quanzheng Li",
                "Hao Peng",
                "Sheng Wang",
                "Jingbo Shang",
                "Chao Zhang",
                "Jiaxuan You",
                "Liyuan Liu",
                "Pan Lu",
                "Yu Zhang",
                "Heng Ji",
                "Yejin Choi",
                "Dawn Song",
                "Jimeng Sun",
                "Jiawei Han"
            ],
            "affiliations": [
                "Harvard",
                "Northwestern",
                "TAMU",
                "UCSD",
                "UIUC",
                "UW",
                "Unity"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16301.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² AI ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° (Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°) Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ°Ñ Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´ ĞµĞ³Ğ¾ supervision). ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ trade-offs Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ, Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing Agentic AI: A Framework for Adaptation Strategies",
                    "desc": "This paper introduces a framework for enhancing agentic AI systems through effective adaptation of both agents and tools. It categorizes adaptations into two main types: those signaled by tool execution and those signaled by agent output, as well as distinguishing between agent-agnostic and agent-supervised tool adaptations. By clarifying the design space and trade-offs of these adaptation strategies, the framework provides practical guidance for improving AI performance and reliability. The paper also reviews existing approaches, highlighting their strengths and challenges, while identifying future research opportunities in the field."
                },
                "zh": {
                    "title": "æ„å»ºæ›´å¼ºå¤§çš„æ™ºèƒ½ä»£ç†AIç³»ç»Ÿçš„é€‚åº”æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºæ™ºèƒ½ä»£ç†ç³»ç»Ÿä¸­ä»£ç†å’Œå·¥å…·é€‚åº”çš„æ¡†æ¶ï¼Œæ˜ç¡®äº†è®¾è®¡ç­–ç•¥å¹¶è¯†åˆ«äº†æé«˜AIèƒ½åŠ›çš„å¼€æ”¾æŒ‘æˆ˜ã€‚éšç€è¿™äº›ç³»ç»Ÿèƒ½åŠ›å’ŒèŒƒå›´çš„å¢é•¿ï¼Œé€‚åº”æ€§æˆä¸ºæé«˜æ€§èƒ½ã€å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„æ ¸å¿ƒæœºåˆ¶ã€‚æˆ‘ä»¬å°†å¿«é€Ÿæ‰©å±•çš„ç ”ç©¶é¢†åŸŸç»Ÿä¸€ä¸ºä¸€ä¸ªç³»ç»Ÿæ¡†æ¶ï¼Œæ¶µç›–ä»£ç†é€‚åº”å’Œå·¥å…·é€‚åº”ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åˆ†ä¸ºå·¥å…·æ‰§è¡Œä¿¡å·å’Œä»£ç†è¾“å‡ºä¿¡å·çš„é€‚åº”å½¢å¼ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›ä¸€ä¸ªæ¦‚å¿µåŸºç¡€å’Œå®ç”¨è·¯çº¿å›¾ï¼Œä»¥æ„å»ºæ›´å¼ºå¤§ã€é«˜æ•ˆå’Œå¯é çš„æ™ºèƒ½ä»£ç†AIç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16923",
            "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
            "url": "https://huggingface.co/papers/2512.16923",
            "abstract": "Generative Refocusing uses semi-supervised learning with DeblurNet and BokehNet to achieve high-quality single-image refocusing with controllable bokeh and text-guided adjustments.  \t\t\t\t\tAI-generated summary \t\t\t\t Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
            "score": 10,
            "issue_id": 141,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "f1622e9f5293d17d",
            "authors": [
                "Chun-Wei Tuan Mu",
                "Jia-Bin Huang",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "National Yang Ming Chiao Tung University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16923.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#cv",
                    "#synthetic",
                    "#optimization"
                ],
                "emoji": "ğŸ“¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ¾ĞºÑƒÑĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ»ÑƒÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ±Ğ¾ĞºĞµ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Generative Refocusing â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€ĞµĞ·ĞºĞ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑƒÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: DeblurNet Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ·ĞºĞ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° BokehNet ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ñ„Ğ¾Ğ½Ğ° Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ Ğ±Ğ¾ĞºĞµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ EXIF Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ´Ğ¸Ğ°Ñ„Ñ€Ğ°Ğ³Ğ¼Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Image Refocusing with Generative Techniques",
                    "desc": "Generative Refocusing is a novel approach that enhances single-image refocusing using semi-supervised learning techniques. It employs DeblurNet to generate all-in-focus images from various input types and BokehNet to create customizable bokeh effects. The method innovatively combines synthetic paired data with unpaired real bokeh images, leveraging EXIF metadata to improve optical realism. Our results demonstrate superior performance in defocus deblurring and bokeh synthesis, while also enabling text-guided adjustments for personalized photography."
                },
                "zh": {
                    "title": "ç”Ÿæˆé‡èšç„¦ï¼šæ™ºèƒ½æ§åˆ¶è™šåŒ–ä¸é‡èšç„¦çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "ç”Ÿæˆé‡èšç„¦ï¼ˆGenerative Refocusingï¼‰æ˜¯ä¸€ç§ä½¿ç”¨åŠç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œç»“åˆDeblurNetå’ŒBokehNetï¼Œå®ç°é«˜è´¨é‡çš„å•å›¾åƒé‡èšç„¦ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ§åˆ¶è™šåŒ–æ•ˆæœï¼Œå¹¶é€šè¿‡æ–‡æœ¬æŒ‡å¯¼è¿›è¡Œè°ƒæ•´ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆé‡èšç„¦å…‹æœäº†å¯¹å…¨ç„¦è¾“å…¥çš„ä¾èµ–ï¼Œåˆ©ç”¨EXIFå…ƒæ•°æ®æ•æ‰çœŸå®å…‰å­¦ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å»æ¨¡ç³Šã€è™šåŒ–åˆæˆå’Œé‡èšç„¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16625",
            "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
            "url": "https://huggingface.co/papers/2512.16625",
            "abstract": "DeContext defends against unauthorized in-context image editing by weakening cross-attention pathways in multimodal attention layers, preserving visual quality while blocking unwanted modifications.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.",
            "score": 10,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "de2aaa1b8be6e0b0",
            "authors": [
                "Linghui Shen",
                "Mingyue Cui",
                "Xingyi Yang"
            ],
            "affiliations": [
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16625.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#security",
                    "#diffusion",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑĞ»Ğ°Ğ±Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DeContext Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ½ĞµÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ñ‚ÑÑ Ğ² Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑÑÑ‚ ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DeContext ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DeContext: Safeguarding Images from Unauthorized Edits",
                    "desc": "DeContext is a method designed to protect images from unauthorized editing by disrupting the flow of information in multimodal attention layers. It does this by applying small perturbations that weaken cross-attention pathways, which are crucial for linking input images to their edited outputs. This approach not only prevents unwanted modifications but also maintains the visual quality of the images. The research demonstrates that focusing on specific transformer blocks and early denoising steps enhances the effectiveness of this defense against image manipulation."
                },
                "zh": {
                    "title": "DeContextï¼šä¿æŠ¤å›¾åƒå…å—æœªç»æˆæƒç¼–è¾‘çš„æœ‰æ•ˆé˜²å¾¡",
                    "desc": "DeContextæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä¿æŠ¤å›¾åƒå…å—æœªç»æˆæƒçš„ç¼–è¾‘ã€‚å®ƒé€šè¿‡å‰Šå¼±å¤šæ¨¡æ€æ³¨æ„åŠ›å±‚ä¸­çš„äº¤å‰æ³¨æ„åŠ›é€šé“ï¼Œæ¥é˜»æ­¢ä¸å¿…è¦çš„ä¿®æ”¹ï¼ŒåŒæ—¶ä¿æŒè§†è§‰è´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ³¨å…¥å°çš„ã€æœ‰é’ˆå¯¹æ€§çš„æ‰°åŠ¨ï¼Œæ‰“ç ´è¾“å…¥ä¸è¾“å‡ºä¹‹é—´çš„è”ç³»ï¼Œä»è€Œæœ‰æ•ˆåœ°é˜²æ­¢å›¾åƒè¢«æ¶æ„ç¯¡æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeContextåœ¨é˜»æ­¢ä¸å½“å›¾åƒç¼–è¾‘çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒå›¾åƒçš„è§†è§‰æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15745",
            "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
            "url": "https://huggingface.co/papers/2512.15745",
            "abstract": "LLaDA2.0 converts auto-regressive models into discrete diffusion large language models using a block-level training scheme, improving efficiency and performance at large scales.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
            "score": 10,
            "issue_id": 139,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "1ebc58b9d434a25e",
            "authors": [
                "Tiwei Bie",
                "Maosong Cao",
                "Kun Chen",
                "Lun Du",
                "Mingliang Gong",
                "Zhuochen Gong",
                "Yanmei Gu",
                "Jiaqi Hu",
                "Zenan Huang",
                "Zhenzhong Lan",
                "Chengxi Li",
                "Chongxuan Li",
                "Jianguo Li",
                "Zehuan Li",
                "Huabin Liu",
                "Ling Liu",
                "Guoshan Lu",
                "Xiaocheng Lu",
                "Yuxin Ma",
                "Jianfeng Tan",
                "Lanning Wei",
                "Ji-Rong Wen",
                "Yipeng Xing",
                "Xiaolu Zhang",
                "Junbo Zhao",
                "Da Zheng",
                "Jun Zhou",
                "Junlin Zhou",
                "Zhanchao Zhou",
                "Liwang Zhu",
                "Yihong Zhuang"
            ],
            "affiliations": [
                "Ant Group",
                "HongKong University of Science and Technology",
                "Renmin University of China",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15745.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#transfer_learning",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#alignment"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğº Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° LLaDA2.0 â€” Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ Ğ´Ğ¾ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ñ‚Ñ€Ñ‘Ñ…Ñ„Ğ°Ğ·Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaDA2.0-mini Ğ¸ LLaDA2.0-flash Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SFT Ğ¸ DPO, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Transforming AR Models for Efficient Large-Scale Language Processing",
                    "desc": "LLaDA2.0 is a new approach that transforms auto-regressive (AR) models into discrete diffusion large language models (dLLMs) to enhance their efficiency and performance. It uses a unique block-level training method that involves three phases: warming up with small blocks, stabilizing with full-sequence diffusion, and then decaying back to compact blocks. This method allows for the retention of knowledge from pre-trained AR models, making the training process more efficient. The resulting models, LLaDA2.0-mini and LLaDA2.0-flash, are optimized for practical use and demonstrate improved performance at large scales while being open-sourced for wider accessibility."
                },
                "zh": {
                    "title": "LLaDA2.0ï¼šé«˜æ•ˆè½¬æ¢è‡ªå›å½’æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "LLaDA2.0 æ˜¯ä¸€ç§å°†è‡ªå›å½’æ¨¡å‹è½¬æ¢ä¸ºç¦»æ•£æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿæ‰©å±•åˆ° 1000 äº¿ä¸ªå‚æ•°ã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿçš„è½¬æ¢è¿‡ç¨‹ï¼Œé¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒçš„é«˜æˆæœ¬ï¼Œä¿ç•™äº†çŸ¥è¯†ç»§æ‰¿å’Œé«˜æ•ˆè®¾è®¡åŸåˆ™ã€‚LLaDA2.0 é‡‡ç”¨ä¸‰é˜¶æ®µçš„å—çº§è®­ç»ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬çƒ­èº«é˜¶æ®µã€ç¨³å®šé˜¶æ®µå’Œè¡°å‡é˜¶æ®µï¼Œä»¥å®ç°é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚æœ€ç»ˆï¼ŒLLaDA2.0-mini å’Œ LLaDA2.0-flash ä¸¤ä¸ªä¼˜åŒ–ç‰ˆæœ¬è¢«å¼€æºï¼Œå±•ç°äº†åœ¨å‰æ²¿è§„æ¨¡ä¸‹çš„å“è¶Šæ€§èƒ½å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16905",
            "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
            "url": "https://huggingface.co/papers/2512.16905",
            "abstract": "Alchemist, a meta-gradient-based framework, automatically selects high-quality subsets from large-scale text-image datasets to improve visual quality and training efficiency in Text-to-Image models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.",
            "score": 9,
            "issue_id": 140,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "597b2e9497d7680f",
            "authors": [
                "Kaixin Ding",
                "Yang Zhou",
                "Xi Chen",
                "Miao Yang",
                "Jiarong Ou",
                "Rui Chen",
                "Xin Tao",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "South China University of Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16905.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#data",
                    "#dataset"
                ],
                "emoji": "âš—ï¸",
                "ru": {
                    "title": "ĞœĞµÑ‚Ğ°Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ°Ğ»Ñ…Ğ¸Ğ¼Ğ¸Ñ: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Alchemist â€” ÑÑ‚Ğ¾ meta-gradient-based Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Text-to-Image. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ÑƒÑ Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ¸Ğ»Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Alchemist Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ meta-learning Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¹Ñ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Shift-Gsampling Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 50% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Alchemist, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ."
                },
                "en": {
                    "title": "Alchemist: Smart Data Selection for Better Text-to-Image Models",
                    "desc": "Alchemist is a novel framework that enhances the training of Text-to-Image models by automatically selecting high-quality subsets from large datasets. It addresses the issue of low-quality and redundant samples in training data, which can hinder model performance. By utilizing a meta-gradient approach, Alchemist evaluates the influence of each data sample and optimizes the selection process through data rating and pruning. Experiments show that models trained on Alchemist-selected data achieve better visual quality and performance compared to those trained on the entire dataset."
                },
                "zh": {
                    "title": "Alchemistï¼šè‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡æ•°æ®çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "Alchemist æ˜¯ä¸€ä¸ªåŸºäºå…ƒæ¢¯åº¦çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»å¤§è§„æ¨¡æ–‡æœ¬-å›¾åƒæ•°æ®é›†ä¸­è‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡çš„å­é›†ï¼Œä»¥æé«˜æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è§†è§‰è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ä¼˜åŒ–æ¨¡å‹ï¼Œä»æ•°æ®ä¸­å¿ƒçš„è§’åº¦è¯„ä¼°æ¯ä¸ªæ ·æœ¬çš„å½±å“ã€‚Alchemist åŒ…å«ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šæ•°æ®è¯„åˆ†å’Œæ•°æ®ä¿®å‰ªï¼Œä½¿ç”¨è½»é‡çº§è¯„åˆ†å™¨å’Œ Shift-Gsampling ç­–ç•¥æ¥é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„å­é›†ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ Alchemist é€‰æ‹©çš„ 50% æ•°æ®è¿›è¡Œè®­ç»ƒçš„æ•ˆæœä¼˜äºä½¿ç”¨å®Œæ•´æ•°æ®é›†çš„è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16912",
            "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
            "url": "https://huggingface.co/papers/2512.16912",
            "abstract": "Reinforcement learning with verifiable rewards improves LLM reasoning through spurious rewards and entropy minimization, despite seemingly paradoxical effects, by reducing clipping bias and policy entropy.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
            "score": 6,
            "issue_id": 139,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "af4ba1587be600c7",
            "authors": [
                "Peter Chen",
                "Xiaopeng Li",
                "Ziniu Li",
                "Wotao Yin",
                "Xi Chen",
                "Tianyi Lin"
            ],
            "affiliations": [
                "CUHK SZ",
                "Columbia",
                "DAMO, Alibaba US",
                "NYU Stern"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16912.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#alignment"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ°Ğ¶ÑƒÑ‰ĞµĞµÑÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ñ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¸ Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ RLVR."
                },
                "en": {
                    "title": "Unlocking LLM Reasoning: The Power of Spurious Rewards and Entropy Minimization",
                    "desc": "This paper explores how reinforcement learning with verifiable rewards (RLVR) can enhance the reasoning abilities of Large Language Models (LLMs). It identifies two key mechanisms: spurious rewards, which can mislead the model by rewarding incorrect outcomes, and entropy minimization, which encourages the model to produce more confident responses. The study reveals that both mechanisms, while seemingly contradictory, can lead to improved reasoning performance by reducing clipping bias and policy entropy. Ultimately, the research provides insights into how these dynamics work together and offers guidelines for optimizing RLVR training."
                },
                "zh": {
                    "title": "å¯éªŒè¯å¥–åŠ±æå‡LLMæ¨ç†èƒ½åŠ›çš„å¥¥ç§˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLVRé€šè¿‡ä¸¤ç§çœ‹ä¼¼çŸ›ç›¾çš„æœºåˆ¶æ¥å¢å¼ºLLMsçš„æ•°å­¦æ¨ç†ï¼šè™šå‡å¥–åŠ±å’Œç†µæœ€å°åŒ–ã€‚è™šå‡å¥–åŠ±é€šè¿‡å¥–åŠ±ä¸çœŸå®ç»“æœæ— å…³çš„ç»“æœæ¥æŠ‘åˆ¶åˆ©ç”¨ï¼Œè€Œç†µæœ€å°åŒ–åˆ™é€šè¿‡æ¨åŠ¨æ¨¡å‹æœå‘æ›´è‡ªä¿¡å’Œç¡®å®šçš„è¾“å‡ºï¼ŒæŠ‘åˆ¶æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†è™šå‡å¥–åŠ±å¦‚ä½•é€šè¿‡å‡å°‘ç­–ç•¥ç†µæ¥æé«˜æ¨ç†æ€§èƒ½ï¼Œå¹¶æå‡ºäº†å¥–åŠ±ä¸ä¸€è‡´æ¨¡å‹ï¼Œä»¥è§£é‡Šè™šå‡å¥–åŠ±åœ¨å—æ±¡æŸ“ç¯å¢ƒä¸­ä»èƒ½æå‡æ€§èƒ½çš„åŸå› ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16864",
            "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
            "url": "https://huggingface.co/papers/2512.16864",
            "abstract": "RePlan, a plan-then-execute framework, enhances instruction-based image editing by combining a vision-language planner with a diffusion editor, achieving superior performance in complex and intricate editing tasks using limited data.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io",
            "score": 5,
            "issue_id": 140,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "fad34df0b49504b2",
            "authors": [
                "Tianyuan Qu",
                "Lei Ke",
                "Xiaohang Zhan",
                "Longxiang Tang",
                "Yuqi Liu",
                "Bohao Peng",
                "Bei Yu",
                "Dong Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16864.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#diffusion",
                    "#reasoning",
                    "#multimodal",
                    "#synthetic",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞ»Ğ°Ğ½ Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ğ²ÑĞµĞ³Ğ¾: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· vision-language Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ",
                    "desc": "RePlan â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°. ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾ Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ attention-region injection Ğ±ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ reinforcement learning Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GRPO Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 1000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ñ Ğ·Ğ°Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "RePlan: Precision Editing through Smart Planning",
                    "desc": "RePlan is a novel framework designed for instruction-based image editing that effectively combines a vision-language planner with a diffusion editor. It addresses the challenges posed by Instruction-Visual Complexity (IV-Complexity) by breaking down complex instructions into manageable steps and grounding them to specific image regions. The framework utilizes a training-free attention-region injection mechanism, allowing for precise edits across multiple regions simultaneously. By employing GRPO-based reinforcement learning on a limited dataset, RePlan demonstrates significant improvements in reasoning accuracy and editing fidelity compared to existing models."
                },
                "zh": {
                    "title": "RePlanï¼šç²¾å‡†çš„å›¾åƒç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "RePlanæ˜¯ä¸€ç§è®¡åˆ’-æ‰§è¡Œæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè§†è§‰-è¯­è¨€è§„åˆ’å™¨å’Œæ‰©æ•£ç¼–è¾‘å™¨ï¼Œæå‡äº†åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„æŒ‡ä»¤å’Œæ··ä¹±çš„åœºæ™¯ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹åœ¨æŒ‡ä»¤-è§†è§‰å¤æ‚æ€§ï¼ˆIV-Complexityï¼‰ä¸‹çš„ä¸è¶³ã€‚è§„åˆ’å™¨é€šè¿‡é€æ­¥æ¨ç†å°†æŒ‡ä»¤åˆ†è§£ï¼Œå¹¶æ˜ç¡®å°†å…¶ä¸ç›®æ ‡åŒºåŸŸå¯¹é½ï¼›ç¼–è¾‘å™¨åˆ™åˆ©ç”¨æ— è®­ç»ƒçš„æ³¨æ„åŠ›åŒºåŸŸæ³¨å…¥æœºåˆ¶è¿›è¡Œç²¾ç¡®çš„å¤šåŒºåŸŸç¼–è¾‘ã€‚é€šè¿‡ä½¿ç”¨åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ï¼ŒRePlanåœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ ¼å¼å¯é æ€§ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16921",
            "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
            "url": "https://huggingface.co/papers/2512.16921",
            "abstract": "AuditDM, an automated framework using reinforcement learning, identifies and rectifies failure modes in multimodal LLMs by generating challenging examples, leading to improved performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
            "score": 3,
            "issue_id": 140,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "67ac17beb0bedeab",
            "authors": [
                "Qihao Liu",
                "Chengzhi Mao",
                "Yaojie Liu",
                "Alan Yuille",
                "Wen-Sheng Chu"
            ],
            "affiliations": [
                "Google",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16921.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#interpretability",
                    "#security",
                    "#multimodal",
                    "#synthetic",
                    "#training",
                    "#small_models"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ñ… ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "AuditDM â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ reinforcement learning Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ AuditDM Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 16 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸."
                },
                "en": {
                    "title": "AuditDM: Enhancing MLLM Performance through Targeted Auditing",
                    "desc": "AuditDM is an innovative framework that uses reinforcement learning to identify and fix failure modes in multimodal large language models (MLLMs). It generates challenging examples, such as difficult questions and counterfactual images, to highlight discrepancies between models. By fine-tuning an MLLM to act as an auditor, AuditDM uncovers various weaknesses in models without needing additional annotated data. The framework has shown to enhance the performance of state-of-the-art models across multiple benchmarks, demonstrating its effectiveness in model evaluation and improvement."
                },
                "zh": {
                    "title": "AuditDMï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½çš„æ™ºèƒ½å®¡è®¡æ¡†æ¶",
                    "desc": "AuditDMæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥è¯†åˆ«å’Œä¿®æ­£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¤±è´¥æ¨¡å¼ã€‚å®ƒé€šè¿‡ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„ç¤ºä¾‹ï¼Œå¸®åŠ©å‘ç°æ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡å®¡è®¡æ¨¡å‹çš„åå·®ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªå®¡è®¡è€…ï¼Œèƒ½å¤Ÿç”Ÿæˆæœ€å¤§åŒ–æ¨¡å‹ä¸ä¸€è‡´æ€§çš„éš¾é¢˜å’Œåäº‹å®å›¾åƒã€‚åº”ç”¨äºå…ˆè¿›æ¨¡å‹æ—¶ï¼ŒAuditDMå‘ç°äº†è¶…è¿‡20ç§ä¸åŒçš„å¤±è´¥ç±»å‹ï¼Œå¹¶åœ¨16ä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­æ”¹å–„äº†æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16561",
            "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
            "url": "https://huggingface.co/papers/2512.16561",
            "abstract": "N3D-VLM integrates native 3D perception and reasoning in vision-language models, enabling precise 3D localization and spatial understanding with a large-scale dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.",
            "score": 3,
            "issue_id": 141,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "25abf732d24e0f5e",
            "authors": [
                "Yuxin Wang",
                "Lei Ke",
                "Boqiang Zhang",
                "Tianyuan Qu",
                "Hanxun Yu",
                "Zhenpeng Huang",
                "Meng Yu",
                "Dan Xu",
                "Dong Yu"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "NJU",
                "Tencent AI Lab",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16561.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğµ 3D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "N3D-VLM â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ 3D Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ 3D Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 2D Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² 3D ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ² ÑˆĞµÑÑ‚ÑŒ Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ 3D Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾æ¡†æ¶ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing 3D Understanding in Vision-Language Models",
                    "desc": "N3D-VLM is a new framework that combines 3D object perception with vision-language models to improve understanding of spatial relationships in 3D environments. Unlike traditional models that only work with 2D images, N3D-VLM can accurately locate objects in 3D space using text descriptions. It also enhances reasoning about spatial arrangements, making the model's outputs more interpretable. The framework is supported by a large dataset that allows for effective training in both 3D localization and reasoning tasks."
                },
                "zh": {
                    "title": "N3D-VLMï¼šèåˆ3Dæ„ŸçŸ¥ä¸æ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹",
                    "desc": "N3D-VLMæ˜¯ä¸€ç§æ–°é¢–çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç»“åˆäº†åŸç”Ÿçš„3Dç‰©ä½“æ„ŸçŸ¥å’Œ3Dè§†è§‰æ¨ç†ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®çš„3Då®šä½å’Œç©ºé—´ç†è§£ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡å‹ä¸åŒï¼ŒN3D-VLMç›´æ¥æ ¹æ®æ–‡æœ¬æè¿°åœ¨3Dç©ºé—´ä¸­å®šä½ç‰©ä½“ï¼Œæå‡äº†å¯¹ç©ºé—´å…³ç³»çš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºå¯æ‰©å±•çš„æ•°æ®ç®¡é“ï¼Œå°†å¤§è§„æ¨¡çš„2Dæ³¨é‡Šæå‡åˆ°3Dç©ºé—´ï¼Œæ˜¾è‘—å¢åŠ äº†3Dç‰©ä½“å®šä½æ•°æ®çš„å¤šæ ·æ€§å’Œè¦†ç›–é¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒN3D-VLMåœ¨3Då®šä½ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨3Dç©ºé—´æ¨ç†æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16918",
            "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
            "url": "https://huggingface.co/papers/2512.16918",
            "abstract": "AdaTooler-V, a multimodal large language model, adaptively uses vision tools based on reinforcement learning, improving performance and reducing unnecessary tool invocations in visual reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
            "score": 2,
            "issue_id": 140,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "9c82df4fad99b3bd",
            "authors": [
                "Chaoyang Wang",
                "Kaituo Feng",
                "Dongyang Chen",
                "Zhongyu Wang",
                "Zhixun Li",
                "Sicheng Gao",
                "Meng Meng",
                "Xu Zhou",
                "Manyuan Zhang",
                "Yuzhang Shang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "DB Group, CUHK",
                "JMU",
                "MMLab, CUHK",
                "SJTU",
                "Sangfor",
                "THU",
                "UCF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16918.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#rl",
                    "#benchmark",
                    "#video",
                    "#inference",
                    "#reasoning",
                    "#multimodal",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° AdaTooler-V â€” Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ AT-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑĞ¾Ğ²Ñ‹Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…: AdaTooler-V-CoT-100k Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ AdaTooler-V-300k Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AdaTooler-V Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 89.8% Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ¼ Ñ‚ĞµÑÑ‚Ğµ V*, Ğ¾Ğ¿ĞµÑ€ĞµĞ¶Ğ°Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¸ Gemini 1.5 Pro."
                },
                "en": {
                    "title": "Smart Tool Use for Better Visual Reasoning",
                    "desc": "AdaTooler-V is a multimodal large language model that enhances visual reasoning by using reinforcement learning to adaptively decide when to use vision tools. This model addresses the issue of unnecessary tool invocation, which can slow down performance and increase computational costs. By implementing the AT-GRPO algorithm, AdaTooler-V adjusts its tool usage based on the Tool Benefit Score, ensuring that tools are only used when they genuinely improve outcomes. The model has been trained on two specialized datasets and has demonstrated superior reasoning capabilities, achieving high accuracy on various benchmarks compared to existing models."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”å·¥å…·ä½¿ç”¨ï¼Œæå‡è§†è§‰æ¨ç†èƒ½åŠ›",
                    "desc": "AdaTooler-Væ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®å¼ºåŒ–å­¦ä¹ è‡ªé€‚åº”åœ°ä½¿ç”¨è§†è§‰å·¥å…·ï¼Œä»è€Œæé«˜è§†è§‰æ¨ç†ä»»åŠ¡çš„æ€§èƒ½å¹¶å‡å°‘ä¸å¿…è¦çš„å·¥å…·è°ƒç”¨ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥AT-GRPOç®—æ³•ï¼ŒåŠ¨æ€è°ƒæ•´å¥–åŠ±å°ºåº¦ï¼Œç¡®ä¿åªæœ‰åœ¨å·¥å…·ç¡®å®èƒ½å¸¦æ¥æ”¹è¿›æ—¶æ‰ä¼šè¢«è°ƒç”¨ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«ç”¨äºå†·å¯åŠ¨å’Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaTooler-Våœ¨å¤šç§è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡è¶…è¿‡äº†ç°æœ‰çš„å•†ä¸šæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15907",
            "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
            "url": "https://huggingface.co/papers/2512.15907",
            "abstract": "TabReX is a reference-less framework using graph-based reasoning to evaluate the quality of tables generated by LLMs, offering structural and factual fidelity scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
            "score": 0,
            "issue_id": 139,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "20df4be45558984f",
            "authors": [
                "Tejas Anvekar",
                "Juhna Park",
                "Aparna Garimella",
                "Vivek Gupta"
            ],
            "affiliations": [
                "Adobe",
                "Arizona State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15907.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "TabReX â€” ÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. TabReX Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆĞµĞ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ‡ĞµĞµĞº."
                },
                "en": {
                    "title": "Revolutionizing Table Quality Evaluation with TabReX",
                    "desc": "TabReX is a novel framework designed to assess the quality of tables produced by large language models (LLMs) without relying on predefined references. It utilizes graph-based reasoning to convert both the input text and the generated tables into canonical knowledge graphs, allowing for a more structured evaluation. By aligning these graphs through a matching process guided by LLMs, TabReX computes scores that reflect both structural and factual accuracy. The framework also introduces TabReX-Bench, a comprehensive benchmark for testing the robustness of the evaluation metrics across various domains and perturbation types, demonstrating high correlation with expert assessments."
                },
                "zh": {
                    "title": "TabReXï¼šæ— å‚è€ƒçš„è¡¨æ ¼ç”Ÿæˆè´¨é‡è¯„ä¼°æ–°æ¡†æ¶",
                    "desc": "TabReXæ˜¯ä¸€ä¸ªæ— å‚è€ƒçš„æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºå›¾çš„æ¨ç†æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„è¡¨æ ¼è´¨é‡ã€‚å®ƒå°†æºæ–‡æœ¬å’Œç”Ÿæˆçš„è¡¨æ ¼è½¬æ¢ä¸ºè§„èŒƒçŸ¥è¯†å›¾è°±ï¼Œé€šè¿‡LLMå¼•å¯¼çš„åŒ¹é…è¿‡ç¨‹è¿›è¡Œå¯¹é½ï¼Œå¹¶è®¡ç®—å¯è§£é‡Šçš„ã€ç¬¦åˆè¯„åˆ†æ ‡å‡†çš„åˆ†æ•°ï¼Œä»¥é‡åŒ–ç»“æ„å’Œäº‹å®çš„å‡†ç¡®æ€§ã€‚TabReXæä¾›äº†çµæ•åº¦å’Œç‰¹å¼‚æ€§ä¹‹é—´çš„å¯æ§æƒè¡¡ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸äººç±»åˆ¤æ–­ä¸€è‡´çš„è¯„ä¼°ç»“æœã€‚é€šè¿‡å¼•å…¥TabReX-BenchåŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°äº†è¯¥æŒ‡æ ‡çš„ç¨³å¥æ€§ï¼Œæ˜¾ç¤ºTabReXåœ¨ä¸“å®¶æ’åä¸­å…·æœ‰æœ€é«˜çš„ç›¸å…³æ€§ï¼Œå¹¶åœ¨æ›´å¤æ‚çš„æ‰°åŠ¨ä¸‹ä¿æŒç¨³å®šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15528",
            "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
            "url": "https://huggingface.co/papers/2512.15528",
            "abstract": "EmoCaliber, a confidence-aware Multimodal Large Language Model, enhances Visual Emotion Comprehension by verbalizing confidence in emotion predictions, leading to improved reliability and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.",
            "score": 0,
            "issue_id": 140,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "1dbe700b686cbc95",
            "authors": [
                "Daiqing Wu",
                "Dongbao Yang",
                "Can Ma. Yu Zhou"
            ],
            "affiliations": [
                "IIE, Chinese Academy of Sciences",
                "Nankai University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15528.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ˜Š",
                "ru": {
                    "title": "Ğ£Ğ²ĞµÑ€ĞµĞ½Ğ½Ğ°Ñ Ğ² ÑĞµĞ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ EmoCaliber â€” Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ñƒ ÑĞ¼Ğ¾Ñ†Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ…, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VECBench Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ EmoCaliber Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "EmoCaliber: Enhancing Emotion Understanding with Confidence!",
                    "desc": "EmoCaliber is a confidence-aware Multimodal Large Language Model designed to improve Visual Emotion Comprehension (VEC) by verbalizing its confidence in emotion predictions. Traditional models often provide a single emotion label, which can overlook the subjective nature of emotions. By allowing the model to express confidence levels, EmoCaliber offers insights into the plausibility of different emotional interpretations. The model is trained through a three-stage framework that enhances its reasoning, confidence verbalization, and calibration, leading to superior performance in emotion prediction and confidence estimation on the VECBench benchmark."
                },
                "zh": {
                    "title": "EmoCaliberï¼šæå‡è§†è§‰æƒ…æ„Ÿç†è§£çš„ç½®ä¿¡åº¦è¡¨è¾¾",
                    "desc": "EmoCaliberæ˜¯ä¸€ç§å…³æ³¨ç½®ä¿¡åº¦çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è¡¨è¾¾æƒ…æ„Ÿé¢„æµ‹çš„ç½®ä¿¡åº¦æ¥å¢å¼ºè§†è§‰æƒ…æ„Ÿç†è§£ã€‚è¯¥æ¨¡å‹è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•å°†è§†è§‰æƒ…æ„Ÿç†è§£è§†ä¸ºç¡®å®šæ€§ä»»åŠ¡çš„å±€é™æ€§ï¼Œå…è®¸æ¨¡å‹è¾“å‡ºå¤šä¸ªæƒ…æ„Ÿæ ‡ç­¾çš„å¯èƒ½æ€§ã€‚é€šè¿‡å¼•å…¥ç½®ä¿¡åº¦è¡¨è¾¾ï¼ŒEmoCaliberä¸ä»…æé«˜äº†æƒ…æ„Ÿé¢„æµ‹çš„å¯é æ€§ï¼Œè¿˜å¢å¼ºäº†ç”¨æˆ·å¯¹ä¸åŒæƒ…æ„Ÿè§£è¯»çš„ç†è§£ã€‚ç»è¿‡åœ¨ç»Ÿä¸€åŸºå‡†VECBenchä¸Šçš„è¯„ä¼°ï¼ŒEmoCaliberåœ¨æƒ…æ„Ÿé¢„æµ‹å’Œç½®ä¿¡åº¦ä¼°è®¡æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.12576",
            "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
            "url": "https://huggingface.co/papers/2512.12576",
            "abstract": "CoVRL, a hybrid approach combining variational inference and reinforcement learning, enhances language model reasoning by coupling prior and posterior distributions, improving performance and coherence.  \t\t\t\t\tAI-generated summary \t\t\t\t While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\b{Coupled Variational Reinforcement Learning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
            "score": 0,
            "issue_id": 141,
            "pub_date": "2025-12-14",
            "pub_date_card": {
                "ru": "14 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 14",
                "zh": "12æœˆ14æ—¥"
            },
            "hash": "56b54572cc68ed0b",
            "authors": [
                "Xueru Wen",
                "Jie Lou",
                "Yanjiang Liu",
                "Hongyu Lin",
                "Ben He",
                "Xianpei Han",
                "Le Sun",
                "Yaojie Lu",
                "Debing Zhang"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.12576.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "CoVRL â€” ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ CoVRL Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 12,4% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ 2,3% Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°."
                },
                "en": {
                    "title": "Enhancing Language Model Reasoning with CoVRL",
                    "desc": "CoVRL is a novel approach that combines variational inference with reinforcement learning to enhance the reasoning capabilities of language models. By coupling prior and posterior distributions, it allows for more efficient exploration of reasoning paths while maintaining coherence between the reasoning process and the final answers. This method addresses the limitations of traditional reinforcement learning, which often relies on verifiable rewards and can lead to inefficient sampling. Experimental results demonstrate that CoVRL significantly improves performance on reasoning tasks, outperforming existing verifier-free RL methods."
                },
                "zh": {
                    "title": "è€¦åˆå˜åˆ†å¼ºåŒ–å­¦ä¹ ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "CoVRLæ˜¯ä¸€ç§ç»“åˆå˜åˆ†æ¨æ–­å’Œå¼ºåŒ–å­¦ä¹ çš„æ··åˆæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡è€¦åˆå…ˆéªŒåˆ†å¸ƒå’ŒåéªŒåˆ†å¸ƒï¼Œé‡‡ç”¨æ··åˆé‡‡æ ·ç­–ç•¥æ¥æé«˜æ€§èƒ½å’Œä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒCoVRLèƒ½å¤Ÿæœ‰æ•ˆæ¢ç´¢ï¼ŒåŒæ—¶ä¿æŒæ€ç»´è¿‡ç¨‹ä¸æœ€ç»ˆç­”æ¡ˆä¹‹é—´çš„å¼ºä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoVRLåœ¨æ•°å­¦å’Œä¸€èˆ¬æ¨ç†åŸºå‡†ä¸Šæ¯”åŸºç¡€æ¨¡å‹æé«˜äº†12.4%çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ— éªŒè¯å™¨çš„å¼ºåŒ–å­¦ä¹ åŸºå‡†ä¸Šé¢å¤–æé«˜äº†2.3%ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-18.html",
    "link_next": "2025-12-22.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "18.12",
        "en": "12/18",
        "zh": "12æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "22.12",
        "en": "12/22",
        "zh": "12æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 5,
        "#rl": 5,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 2,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 10,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}