{
    "date": {
        "ru": "20 ноября",
        "en": "November 20",
        "zh": "11月20日"
    },
    "time_utc": "2024-11-20 10:11",
    "weekday": 2,
    "issue_id": 681,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.11925",
            "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2411.11925",
            "abstract": "Continuous-valued Autoregressive (AR) image generation models have demonstrated notable superiority over their discrete-token counterparts, showcasing considerable reconstruction quality and higher generation fidelity. However, the computational demands of the autoregressive framework result in significant inference overhead. While speculative decoding has proven effective in accelerating Large Language Models (LLMs), their adaptation to continuous-valued visual autoregressive models remains unexplored. This work generalizes the speculative decoding algorithm from discrete tokens to continuous space. By analyzing the intrinsic properties of output distribution, we establish a tailored acceptance criterion for the diffusion distributions prevalent in such models. To overcome the inconsistency that occurred in speculative decoding output distributions, we introduce denoising trajectory alignment and token pre-filling methods. Additionally, we identify the hard-to-sample distribution in the rejection phase. To mitigate this issue, we propose a meticulous acceptance-rejection sampling method with a proper upper bound, thereby circumventing complex integration. Experimental results show that our continuous speculative decoding achieves a remarkable 2.33times speed-up on off-the-shelf models while maintaining the output distribution. Codes will be available at https://github.com/MarkXCloud/CSpD",
            "score": 4,
            "issue_id": 674,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 ноября",
                "en": "November 18",
                "zh": "11月18日"
            },
            "hash": "17049106ecc06192",
            "authors": [
                "Zili Wang",
                "Robert Zhang",
                "Kun Ding",
                "Qi Yang",
                "Fei Li",
                "Shiming Xiang"
            ],
            "affiliations": [
                "China Tower Corporation Limited",
                "Institute of Automation, Chinese Academy of Sciences, China",
                "University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11925.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Ускорение генерации изображений: от дискретного к непрерывному",
                    "desc": "Статья представляет новый метод ускорения генерации изображений с помощью авторегрессионных моделей с непрерывными значениями. Авторы адаптируют алгоритм спекулятивного декодирования, ранее применявшийся для ускорения больших языковых моделей, к непрерывному пространству. Они вводят специальный критерий принятия для диффузионных распределений и предлагают методы выравнивания траектории шумоподавления и предварительного заполнения токенов. Экспериментальные результаты показывают 2.33-кратное ускорение без ухудшения качества выходных данных."
                },
                "en": {
                    "title": "Speeding Up Image Generation with Continuous Speculative Decoding",
                    "desc": "This paper presents a new approach to improve the speed of continuous-valued autoregressive image generation models. It adapts speculative decoding, a technique previously used in large language models, to work with continuous data. The authors introduce methods to align denoising trajectories and pre-fill tokens to enhance the output quality during the decoding process. Their experiments demonstrate that this new method can significantly speed up the generation process by over two times while preserving the quality of the generated images."
                },
                "zh": {
                    "title": "加速连续值自回归图像生成的推测解码",
                    "desc": "本文提出了一种针对连续值自回归图像生成模型的推测解码算法，旨在提高生成速度。通过分析输出分布的内在特性，建立了适合扩散分布的接受标准。为了解决推测解码输出分布的不一致性，本文引入了去噪轨迹对齐和令牌预填充方法。实验结果表明，该方法在保持输出分布的同时，实现了2.33倍的速度提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12734",
            "title": "Soft Robotic Dynamic In-Hand Pen Spinning",
            "url": "https://huggingface.co/papers/2411.12734",
            "abstract": "Dynamic in-hand manipulation remains a challenging task for soft robotic systems that have demonstrated advantages in safe compliant interactions but struggle with high-speed dynamic tasks. In this work, we present SWIFT, a system for learning dynamic tasks using a soft and compliant robotic hand. Unlike previous works that rely on simulation, quasi-static actions and precise object models, the proposed system learns to spin a pen through trial-and-error using only real-world data without requiring explicit prior knowledge of the pen's physical attributes. With self-labeled trials sampled from the real world, the system discovers the set of pen grasping and spinning primitive parameters that enables a soft hand to spin a pen robustly and reliably. After 130 sampled actions per object, SWIFT achieves 100% success rate across three pens with different weights and weight distributions, demonstrating the system's generalizability and robustness to changes in object properties. The results highlight the potential for soft robotic end-effectors to perform dynamic tasks including rapid in-hand manipulation. We also demonstrate that SWIFT generalizes to spinning items with different shapes and weights such as a brush and a screwdriver which we spin with 10/10 and 5/10 success rates respectively. Videos, data, and code are available at https://soft-spin.github.io.",
            "score": 3,
            "issue_id": 676,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "f75a2283a0a8a06f",
            "authors": [
                "Yunchao Yao",
                "Uksang Yoo",
                "Jean Oh",
                "Christopher G. Atkeson",
                "Jeffrey Ichnowski"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.12734.jpg",
            "data": {
                "categories": [
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Мягкая роботизированная рука осваивает динамичные манипуляции",
                    "desc": "Статья представляет систему SWIFT для обучения динамическим задачам с использованием мягкой роботизированной руки. В отличие от предыдущих подходов, система учится вращать ручку методом проб и ошибок, используя только данные реального мира без предварительных знаний о физических свойствах объекта. После 130 пробных действий SWIFT достигает 100% успеха при вращении трех ручек с разным весом и распределением массы. Система также демонстрирует обобщающую способность на предметах другой формы и веса."
                },
                "en": {
                    "title": "SWIFT: Mastering Dynamic Manipulation with Soft Robotics",
                    "desc": "This paper introduces SWIFT, a novel system designed for dynamic in-hand manipulation using a soft robotic hand. Unlike traditional methods that depend on simulations or precise object models, SWIFT learns to perform tasks like spinning a pen through real-world trial-and-error. The system effectively identifies optimal grasping and spinning parameters without needing prior knowledge of the object's characteristics. SWIFT demonstrates impressive generalizability, achieving a 100% success rate in spinning various pens and also successfully manipulating other objects like brushes and screwdrivers."
                },
                "zh": {
                    "title": "软机器人动态操作的新突破",
                    "desc": "本研究提出了一种名为SWIFT的系统，用于学习动态任务，特别是在软机器人手中进行快速的物体操作。与以往依赖于模拟和精确物体模型的方法不同，SWIFT通过真实世界的数据进行试错学习，能够在没有物体物理属性先验知识的情况下，成功地旋转笔。经过130次采样操作，SWIFT在三种不同重量和分布的笔上实现了100%的成功率，展示了其在物体属性变化下的通用性和鲁棒性。该系统还能够推广到其他形状和重量的物体，如刷子和螺丝刀，分别实现了10/10和5/10的成功率，显示了软机器人在动态任务中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12240",
            "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
            "url": "https://huggingface.co/papers/2411.12240",
            "abstract": "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer's superior handling of Indic languages, GPT-4o's advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.",
            "score": 1,
            "issue_id": 675,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "aee934b73b340b71",
            "authors": [
                "S. Tamang",
                "D. J. Bora"
            ],
            "affiliations": [
                "Department of IT The Assam Kaziranga University Jorhat, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12240.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🇮🇳",
                "ru": {
                    "title": "SUTRA: Лидер токенизации для индийских языков в больших языковых моделях",
                    "desc": "В статье представлен анализ токенизаторов, используемых в 12 больших языковых моделях (LLM) для 22 официальных языков Индии. Исследователи использовали метрику нормализованной длины последовательности (NSL) для оценки эффективности токенизации. Результаты показали, что токенизатор SUTRA превзошел другие модели, включая специализированные для индийских языков, показав лучшие результаты для 14 языков. Исследование подчеркивает важность разработки целевых стратегий токенизации для многоязычных моделей и моделей, ориентированных на индийские языки."
                },
                "en": {
                    "title": "Optimizing Tokenization for Multilingual Mastery",
                    "desc": "This paper evaluates the effectiveness of tokenizers used in Large Language Models (LLMs) for all 22 official languages of India, emphasizing the importance of tokenization in multilingual contexts. The study introduces the Normalized Sequence Length (NSL) as a metric to assess the efficiency of different tokenizers. Results indicate that the SUTRA tokenizer significantly outperforms other models, particularly in handling Indic languages. The findings highlight the need for specialized tokenization strategies to improve the performance of LLMs in diverse linguistic settings."
                },
                "zh": {
                    "title": "优化多语言模型的分词策略",
                    "desc": "本论文探讨了基于变换器架构的大型语言模型（LLMs）中的分词技术，特别是在印度官方语言中的应用。我们对12种LLMs使用的分词器进行了全面评估，重点比较了它们的分词效率。研究结果显示，SUTRA分词器在14种语言中表现优于其他模型，尤其是在处理印度语言方面。该研究强调了为多语言和以印度语言为中心的模型开发针对性分词策略的重要性，以提高语言覆盖率和模型效率。"
                }
            }
        }
    ],
    "link_prev": "2024-11-19.html",
    "link_next": "2024-11-21.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "19.11",
        "en": "11/19",
        "zh": "11月19日"
    },
    "short_date_next": {
        "ru": "21.11",
        "en": "11/21",
        "zh": "11月21日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "连续值自回归（AR）图像生成模型展示了比离散符号模型更高的重建质量和生成保真度。然而，自回归框架的计算需求导致显著的推理开销。虽然推测解码已被证明有效加速大型语言模型（LLMs），但其应用于连续值视觉自回归模型仍未被探索。本文将推测解码算法从离散符号推广到连续空间。通过分析输出分布的内在特性，我们为这些模型中常见的扩散分布建立了定制的接受标准。实验结果显示，我们的连续推测解码在维持输出分布的同时，实现了显著的2.33倍加速。代码将在https://github.com/MarkXCloud/CSpD上提供。",
        "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
        "pinyin": "连续值自回归（AR）图像生成模型展示了比离散符号模型更高的重建质量和生成保真度。然而，自回归框架的计算需求导致显著的推理开销。虽然推测解码已被证明有效加速大型语言模型（LLMs），但其应用于连续值视觉自回归模型仍未被探索。本文将推测解码算法从离散符号推广到连续空间。通过分析输出分布的内在特性，我们为这些模型中常见的扩散分布建立了定制的接受标准。实验结果显示，我们的连续推测解码在维持输出分布的同时，实现了显著的2.33倍加速。代码将在https://github.com/MarkXCloud/CSpD上提供。\n\nlián xù zhí zì huí guī (AR) tú xiàng shēng chéng mó xíng zhàn shì le bǐ lí sàn fú hào mó xíng gèng gāo de chóng jiàn zhì liàng hé shēng chéng bǎo zhēn dù. rán ér, zì huí guī kuàng jià de jì suǎn xū qiú dǎo zhì xiǎn zhù de tuī lǐ kāi xiāo. suī rán, tuī cè jiě mǎ yǐ bèi zhèng míng yǒu xiào jí sù dà xíng yǔ yán mó xíng (LLMs), dàn qí yìng yòng yú lián xù zhí shì jù shí zì huí guī mó xíng réng wèi bèi tuàn suǒ. běn wén jiāng tuī cè jiě mǎ suàn fǎ cóng lí sàn fú hào tuī guǎng dào lián xù kōng jiān. tōng guò fēn xī chū zhì fēn bù de nèi zài tè xìng, wǒ men wèi zhè xiē mó xíng zhōng cháng jiàn de kuò sàn fēn bù jiàn lì le dìng zhì de jiē shòu biāo zhǔn. shí yàn jié guǒ xiǎn shì, wǒ men de lián xù tuī cè jiě mǎ zài wéi chí chū zhì fēn bù de tóng shí, shí xiàn le xiǎn zhù de 2.33 bèi jí sù. dài mǎ jiāng zài https://github.com/MarkXCloud/CSpD shàng tí gōng.",
        "vocab": "[{'word': '连续值', 'pinyin': 'lián xù zhí', 'trans': 'continuous value'},\n{'word': '自回归', 'pinyin': 'zì huí guī', 'trans': 'autoregressive'},\n{'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'},\n{'word': '离散', 'pinyin': 'lí sàn', 'trans': 'discrete'},\n{'word': '符号', 'pinyin': 'fú hào', 'trans': 'symbol'},\n{'word': '重建', 'pinyin': 'chóng jiàn', 'trans': 'reconstruction'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generation'},\n{'word': '保真度', 'pinyin': 'bǎo zhēn dù', 'trans': 'fidelity'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '计算', 'pinyin': 'jì suàn', 'trans': 'computation'},\n{'word': '需求', 'pinyin': 'xū qiú', 'trans': 'demand'},\n{'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'result in'},\n{'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'},\n{'word': '开销', 'pinyin': 'kāi xiāo', 'trans': 'cost'},\n{'word': '推测', 'pinyin': 'tuī cè', 'trans': 'speculative'},\n{'word': '解码', 'pinyin': 'jiě mǎ', 'trans': 'decoding'},\n{'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'},\n{'word': '语言', 'pinyin': 'yǔ yán', 'trans': 'language'},\n{'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'},\n{'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'application'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '推广', 'pinyin': 'tuī guǎng', 'trans': 'extend'},\n{'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'space'},\n{'word': '分析', 'pinyin': 'fēn xī', 'trans': 'analysis'},\n{'word': '输出', 'pinyin': 'shū chū', 'trans': 'output'},\n{'word': '分布', 'pinyin': 'fēn bù', 'trans': 'distribution'},\n{'word': '内在', 'pinyin': 'nèi zài', 'trans': 'intrinsic'},\n{'word': '特性', 'pinyin': 'tè xìng', 'trans': 'characteristic'},\n{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'},\n{'word': '建立', 'pinyin': 'jiàn lì', 'trans': 'establish'},\n{'word': '定制', 'pinyin': 'dìng zhì', 'trans': 'custom'},\n{'word': '接受', 'pinyin': 'jiē shòu', 'trans': 'acceptance'},\n{'word': '标准', 'pinyin': 'biāo zhǔn', 'trans': 'standard'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'},\n{'word': '维持', 'pinyin': 'wéi chí', 'trans': 'maintain'},\n{'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'},\n{'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'},\n{'word': '提供', 'pinyin': 'tí gōng', 'trans': 'provide'}]",
        "trans": "Continuous-valued autoregressive (AR) image generation models have demonstrated higher reconstruction quality and generative fidelity compared to discrete symbol models. However, the computational requirements of the autoregressive framework result in significant inference overhead. While speculative decoding has been proven effective in accelerating large language models (LLMs), its application to continuous-valued visual autoregressive models has not been explored. This paper extends the speculative decoding algorithm from discrete symbols to continuous space. By analyzing the intrinsic properties of the output distribution, we establish custom acceptance criteria for the diffusion distributions commonly found in these models. Experimental results show that our continuous speculative decoding achieves a significant 2.33-fold speedup while maintaining the output distribution. The code will be available at https://github.com/MarkXCloud/CSpD.",
        "update_ts": "2024-11-20 09:11"
    }
}