{
    "date": {
        "ru": "25 –¥–µ–∫–∞–±—Ä—è",
        "en": "December 25",
        "zh": "12Êúà25Êó•"
    },
    "time_utc": "2024-12-25 07:10",
    "weekday": 2,
    "issue_id": 1308,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.18153",
            "title": "DepthLab: From Partial to Complete",
            "url": "https://huggingface.co/papers/2412.18153",
            "abstract": "Missing values remain a common challenge for depth data across its wide range of applications, stemming from various causes like incomplete data acquisition and perspective alteration. This work bridges this gap with DepthLab, a foundation depth inpainting model powered by image diffusion priors. Our model features two notable strengths: (1) it demonstrates resilience to depth-deficient regions, providing reliable completion for both continuous areas and isolated points, and (2) it faithfully preserves scale consistency with the conditioned known depth when filling in missing values. Drawing on these advantages, our approach proves its worth in various downstream tasks, including 3D scene inpainting, text-to-3D scene generation, sparse-view reconstruction with DUST3R, and LiDAR depth completion, exceeding current solutions in both numerical performance and visual quality. Our project page with source code is available at https://johanan528.github.io/depthlab_web/.",
            "score": 19,
            "issue_id": 1305,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 24",
                "zh": "12Êúà24Êó•"
            },
            "hash": "c319c831137b3ce6",
            "authors": [
                "Zhiheng Liu",
                "Ka Leong Cheng",
                "Qiuyu Wang",
                "Shuzhe Wang",
                "Hao Ouyang",
                "Bin Tan",
                "Kai Zhu",
                "Yujun Shen",
                "Qifeng Chen",
                "Ping Luo"
            ],
            "affiliations": [
                "Aalto University",
                "Ant Group",
                "HKU",
                "HKUST",
                "Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18153.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "üï≥Ô∏è",
                "ru": {
                    "title": "DepthLab: –í–æ—Å–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö –≥–ª—É–±–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é –ò–ò",
                    "desc": "DepthLab - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–æ—Ä–∞—Ö. –û–Ω–∞ —Å–ø–æ—Å–æ–±–Ω–∞ –Ω–∞–¥–µ–∂–Ω–æ –∑–∞–ø–æ–ª–Ω—è—Ç—å –∫–∞–∫ –±–æ–ª—å—à–∏–µ –æ–±–ª–∞—Å—Ç–∏, —Ç–∞–∫ –∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –≥–ª—É–±–∏–Ω—ã. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∞ —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π –≥–ª—É–±–∏–Ω–æ–π –ø—Ä–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤. DepthLab –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥ 3D-—Å—Ü–µ–Ω –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö LiDAR."
                },
                "en": {
                    "title": "DepthLab: Bridging the Gap in Depth Data Completion",
                    "desc": "This paper presents DepthLab, a novel model designed to address the issue of missing values in depth data, which often occurs due to incomplete data collection or changes in perspective. DepthLab utilizes image diffusion priors to effectively inpaint depth information, ensuring that both continuous and isolated missing regions are filled accurately. The model maintains scale consistency with known depth values, which is crucial for realistic depth completion. DepthLab outperforms existing methods in various applications, such as 3D scene inpainting and LiDAR depth completion, demonstrating superior numerical and visual results."
                },
                "zh": {
                    "title": "Ê∑±Â∫¶‰øÆÂ§çÊñ∞Á™ÅÁ†¥ÔºöDepthLabÊ®°Âûã",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DepthLabÁöÑÊ∑±Â∫¶ÂõæÂÉè‰øÆÂ§çÊ®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥Ê∑±Â∫¶Êï∞ÊçÆ‰∏≠ÁöÑÁº∫Â§±ÂÄºÈóÆÈ¢ò„ÄÇËØ•Ê®°ÂûãÂà©Áî®ÂõæÂÉèÊâ©Êï£ÂÖàÈ™åÔºåËÉΩÂ§üÊúâÊïàÂ°´Ë°•Ê∑±Â∫¶‰∏çË∂≥ÁöÑÂå∫ÂüüÔºåÁ°Æ‰øùËøûÁª≠Âå∫ÂüüÂíåÂ≠§Á´ãÁÇπÁöÑÂèØÈù†‰øÆÂ§ç„ÄÇDepthLabÂú®Â°´Ë°•Áº∫Â§±ÂÄºÊó∂ÔºåËÉΩÂ§ü‰øùÊåÅ‰∏éÂ∑≤Áü•Ê∑±Â∫¶ÁöÑ‰∏ÄËá¥ÊÄßÔºåÁ°Æ‰øùÂ∞∫Â∫¶ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÈÄöËøáËøô‰∫õ‰ºòÂäøÔºåËØ•Ê®°ÂûãÂú®3DÂú∫ÊôØ‰øÆÂ§ç„ÄÅÊñáÊú¨Âà∞3DÂú∫ÊôØÁîüÊàê„ÄÅÁ®ÄÁñèËßÜÂõæÈáçÂª∫ÂíåLiDARÊ∑±Â∫¶Ë°•ÂÖ®Á≠â‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17739",
            "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization",
            "url": "https://huggingface.co/papers/2412.17739",
            "abstract": "Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become a trend. While existing works mainly address RoPE's limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs Fourier Series and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales show that, within varying context windows, FoPE can maintain a more stable perplexity and a more consistent accuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several analyses and ablations bring further support to our method and theoretical modeling.",
            "score": 13,
            "issue_id": 1306,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 23",
                "zh": "12Êúà23Êó•"
            },
            "hash": "1ce9c827a32ec3c5",
            "authors": [
                "Ermo Hua",
                "Che Jiang",
                "Xingtai Lv",
                "Kaiyan Zhang",
                "Ning Ding",
                "Youbang Sun",
                "Biqing Qi",
                "Yuchen Fan",
                "Xue Kai Zhu",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Northeastern University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17739.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#long_context",
                    "#architecture"
                ],
                "emoji": "üåä",
                "ru": {
                    "title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –§—É—Ä—å–µ",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Fourier Position Embedding (FoPE). FoPE —É–ª—É—á—à–∞–µ—Ç —á–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –º–µ—Ç–æ–¥–∞ Rotary Position Embedding (RoPE) —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ç–µ–æ—Ä–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ FoPE –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç —Ä—è–¥—ã –§—É—Ä—å–µ –∏ –æ–±–Ω—É–ª—è–µ—Ç –¥–µ—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –ø–æ–≤—ã—à–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º —Å–ø–µ–∫—Ç—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FoPE –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—É—é –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–ª–∏–Ω–∞—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å RoPE –∏ ALiBi."
                },
                "en": {
                    "title": "Enhancing Language Models with Fourier Position Embedding for Better Context Handling",
                    "desc": "This paper explores the limitations of Rotary Position Embedding (RoPE) in Language Models (LMs) and its impact on length generalization. It reveals that while RoPE allows for periodic attention through Non-Uniform Discrete Fourier Transform, this capability is compromised by linear layers and insufficient training of frequency components. The authors introduce Fourier Position Embedding (FoPE), which improves the frequency-domain characteristics of attention by eliminating harmful frequency components. Experimental results demonstrate that FoPE outperforms RoPE and ALiBi in maintaining stability in perplexity and accuracy across different context lengths."
                },
                "zh": {
                    "title": "ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶‰∏éÊ≥õÂåñËÉΩÂäõ",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÈÄöËøáÊîπËøõÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÊù•Êâ©Â±ïËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜRoPEÂú®Ê≥®ÊÑèÂäõÊú∫Âà∂‰πãÂ§ñÁöÑÂêÑ‰∏™ÈÉ®ÂàÜÁöÑÂΩ±ÂìçÔºåÂèëÁé∞ÂÖ∂ÂØπÈïøÂ∫¶Ê≥õÂåñÁöÑË¥üÈù¢ÊïàÂ∫î„ÄÇÂü∫‰∫éÁ¶ªÊï£‰ø°Âè∑Â§ÑÁêÜÁêÜËÆ∫ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂÇÖÈáåÂè∂‰ΩçÁΩÆÂµåÂÖ•ÔºàFoPEÔºâÔºåÂÆÉÈÄöËøáÊûÑÂª∫ÂÇÖÈáåÂè∂Á∫ßÊï∞Êù•Â¢ûÂº∫Ê≥®ÊÑèÂäõÁöÑÈ¢ëÂüüÁâπÊÄßÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFoPEÂú®‰∏çÂêå‰∏ä‰∏ãÊñáÁ™óÂè£‰∏ãËÉΩÂ§ü‰øùÊåÅÊõ¥Á®≥ÂÆöÁöÑÂõ∞ÊÉëÂ∫¶Âíå‰∏ÄËá¥ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14711",
            "title": "ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing",
            "url": "https://huggingface.co/papers/2412.14711",
            "abstract": "Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.",
            "score": 5,
            "issue_id": 1305,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 19",
                "zh": "12Êúà19Êó•"
            },
            "hash": "0b43c3f140601a96",
            "authors": [
                "Ziteng Wang",
                "Jianfei Chen",
                "Jun Zhu"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14711.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "ReMoE: –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Mixture-of-Experts",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É ReMoE –¥–ª—è –º–æ–¥–µ–ª–µ–π Mixture-of-Experts (MoE). ReMoE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ ReLU –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ TopK+Softmax. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ –∏ —Å–ª–æ—è–º–∏, –∞ —Ç–∞–∫–∂–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ –¥–æ–º–µ–Ω–∞–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ReMoE –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—ã—á–Ω—ã–µ MoE –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –º–æ–¥–µ–ª–µ–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤."
                },
                "en": {
                    "title": "ReMoE: Revolutionizing Mixture-of-Experts with Differentiable Routing",
                    "desc": "This paper introduces ReMoE, a new architecture for Mixture-of-Experts (MoE) models that improves upon traditional TopK routers by making them fully differentiable. By using ReLU as the routing mechanism, ReMoE allows for continuous optimization, which enhances performance and scalability. The authors also present techniques to manage the sparsity of the router and ensure an even distribution of workload among experts. Experimental results show that ReMoE outperforms conventional MoE models in various scenarios, demonstrating better scalability with an increasing number of experts."
                },
                "zh": {
                    "title": "ReMoEÔºöÊèêÂçáÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÁöÑÊÄßËÉΩ‰∏éÂèØÊâ©Â±ïÊÄß",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ®ÄÁñèÊøÄÊ¥ªÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãReMoEÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ‰∏é‰º†ÁªüÁöÑTopKË∑ØÁî±Âô®‰∏çÂêåÔºåReMoEÈááÁî®‰∫ÜÂÆåÂÖ®ÂèØÂæÆÂàÜÁöÑÊû∂ÊûÑÔºå‰ΩøÁî®ReLU‰Ωú‰∏∫Ë∑ØÁî±Âô®Ôºå‰ªéËÄåÂÖãÊúç‰∫ÜÈùûËøûÁª≠ÊÄßÂ∏¶Êù•ÁöÑÈôêÂà∂„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜË∞ÉËäÇË∑ØÁî±Âô®Á®ÄÁñèÊÄßÁöÑÊñπÊ≥ïÔºå‰ª•Âπ≥Ë°°‰∏ìÂÆ∂‰πãÈó¥ÁöÑË¥üËΩΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReMoEÂú®‰∏çÂêåÊ®°ÂûãËßÑÊ®°Âíå‰∏ìÂÆ∂Êï∞Èáè‰∏ãÔºåÂùá‰ºò‰∫é‰º†ÁªüÁöÑTopKË∑ØÁî±Ê∑∑Âêà‰∏ìÂÆ∂Ê®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18597",
            "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
            "url": "https://huggingface.co/papers/2412.18597",
            "abstract": "Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training.",
            "score": 3,
            "issue_id": 1307,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 24",
                "zh": "12Êúà24Êó•"
            },
            "hash": "210ce3ba0e7e45d2",
            "authors": [
                "Minghong Cai",
                "Xiaodong Cun",
                "Xiaoyu Li",
                "Wenze Liu",
                "Zhaoyang Zhang",
                "Yong Zhang",
                "Ying Shan",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "GVC Lab, Great Bay University",
                "MMLab, The Chinese University of Hong Kong",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18597.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#games",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–ü–ª–∞–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ DiTCtrl –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Multi-Modal Diffusion Transformer (MM-DiT) –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –µ–≥–æ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏–¥–µ–æ —Å –ø–ª–∞–≤–Ω—ã–º–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–º –¥–≤–∏–∂–µ–Ω–∏–µ–º –æ–±—ä–µ–∫—Ç–æ–≤. –¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MPVBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Multi-Prompt Control",
                    "desc": "This paper introduces DiTCtrl, a novel method for generating videos using multiple prompts without the need for additional training. It leverages the Multi-Modal Diffusion Transformer (MM-DiT) architecture to facilitate smooth transitions and coherent object motion across sequential prompts. By analyzing the attention mechanism of MM-DiT, the authors enable precise semantic control, allowing for effective multi-prompt video generation. The proposed method outperforms existing techniques and is evaluated using a new benchmark called MPVBench, specifically designed for this purpose."
                },
                "zh": {
                    "title": "Êó†ËÆ≠ÁªÉÁöÑÂ§öÊèêÁ§∫ËßÜÈ¢ëÁîüÊàêÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÊèêÁ§∫ËßÜÈ¢ëÁîüÊàêÊñπÊ≥ïDiTCtrlÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊ®°ÂûãÂú®Â§ÑÁêÜÂ§ö‰∏™È°∫Â∫èÊèêÁ§∫Êó∂ÁöÑÂõ∞Èöæ„ÄÇÊàë‰ª¨Âà©Áî®MM-DiTÊû∂ÊûÑÔºåÈÄöËøáÂàÜÊûêÂÖ∂Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂÆûÁé∞‰∫ÜÂú®Â§öÊèêÁ§∫ËßÜÈ¢ëÁîüÊàê‰∏≠Âπ≥ÊªëËøáÊ∏°Âíå‰∏ÄËá¥ÁöÑÁâ©‰ΩìËøêÂä®„ÄÇDiTCtrl‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÔºåËÉΩÂ§üÂú®Â§ö‰∏™ÊèêÁ§∫‰∏ãÁîüÊàêËá™ÁÑ∂ÊµÅÁïÖÁöÑËßÜÈ¢ë„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜMPVBenchÂü∫ÂáÜÔºå‰ª•ËØÑ‰º∞Â§öÊèêÁ§∫ÁîüÊàêÁöÑÊÄßËÉΩÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéËØ•ÊñπÊ≥ïÂú®Êó†È¢ùÂ§ñËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15443",
            "title": "SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval",
            "url": "https://huggingface.co/papers/2412.15443",
            "abstract": "Retrieval-Augmented Generation (RAG) systems have become pivotal in leveraging vast corpora to generate informed and contextually relevant responses, notably reducing hallucinations in Large Language Models. Despite significant advancements, these systems struggle to efficiently process and retrieve information from large datasets while maintaining a comprehensive understanding of the context. This paper introduces SKETCH, a novel methodology that enhances the RAG retrieval process by integrating semantic text retrieval with knowledge graphs, thereby merging structured and unstructured data for a more holistic comprehension. SKETCH, demonstrates substantial improvements in retrieval performance and maintains superior context integrity compared to traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER, NarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline approaches on key RAGAS metrics such as answer_relevancy, faithfulness, context_precision and context_recall. Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics. These results highlight SKETCH's capability in delivering more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems.",
            "score": 3,
            "issue_id": 1305,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 19",
                "zh": "12Êúà19Êó•"
            },
            "hash": "2d16e57527037cb7",
            "authors": [
                "Aakash Mahalingam",
                "Vinesh Kumar Gande",
                "Aman Chadha",
                "Vinija Jain",
                "Divya Chaudhary"
            ],
            "affiliations": [
                "Amazon AI",
                "Meta",
                "Northeastern University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15443.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#dataset",
                    "#hallucinations",
                    "#benchmark",
                    "#rag",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "SKETCH: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SKETCH, —É–ª—É—á—à–∞—é—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (RAG). SKETCH –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ —Å –≥—Ä–∞—Ñ–∞–º–∏ –∑–Ω–∞–Ω–∏–π, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–æ–ª–µ–µ —Ü–µ–ª–æ—Å—Ç–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏. SKETCH –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º RAGAS –Ω–∞ —á–µ—Ç—ã—Ä–µ—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤."
                },
                "en": {
                    "title": "SKETCH: Elevating RAG with Semantic and Structured Data Integration",
                    "desc": "This paper presents SKETCH, a new method that improves Retrieval-Augmented Generation (RAG) systems by combining semantic text retrieval with knowledge graphs. This integration allows for better processing of large datasets while ensuring a deeper understanding of context. SKETCH shows significant enhancements in retrieval performance and context integrity compared to traditional RAG methods. The results from various datasets demonstrate that SKETCH achieves high scores in answer relevancy and context precision, establishing new standards for retrieval systems."
                },
                "zh": {
                    "title": "SKETCHÔºöÊèêÂçáÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÁ≥ªÁªüÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SKETCHÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÂçáÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁ≥ªÁªüÁöÑÊÄßËÉΩ„ÄÇSKETCHÈÄöËøáÂ∞ÜËØ≠‰πâÊñáÊú¨Ê£ÄÁ¥¢‰∏éÁü•ËØÜÂõæË∞±Áõ∏ÁªìÂêàÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Â§ÑÁêÜÂíåÊ£ÄÁ¥¢Â§ßÊï∞ÊçÆÈõÜ‰∏≠ÁöÑ‰ø°ÊÅØÔºåÂêåÊó∂‰øùÊåÅÂØπ‰∏ä‰∏ãÊñáÁöÑÂÖ®Èù¢ÁêÜËß£„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåSKETCHÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊÑèÂ§ßÂà©ÁæéÈ£üÊï∞ÊçÆÈõÜ‰∏äÔºåËææÂà∞‰∫Ü0.94ÁöÑÁ≠îÊ°àÁõ∏ÂÖ≥ÊÄßÂíå0.99ÁöÑ‰∏ä‰∏ãÊñáÁ≤æÂ∫¶„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåSKETCHËÉΩÂ§üÊèê‰æõÊõ¥ÂáÜÁ°ÆÂíå‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÁöÑÂìçÂ∫îÔºå‰∏∫Êú™Êù•ÁöÑÊ£ÄÁ¥¢Á≥ªÁªüËÆæÂÆö‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18608",
            "title": "PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models",
            "url": "https://huggingface.co/papers/2412.18608",
            "abstract": "Text- or image-to-3D generators and 3D scanners can now produce 3D assets with high-quality shapes and textures. These assets typically consist of a single, fused representation, like an implicit neural field, a Gaussian mixture, or a mesh, without any useful structure. However, most applications and creative workflows require assets to be made of several meaningful parts that can be manipulated independently. To address this gap, we introduce PartGen, a novel approach that generates 3D objects composed of meaningful parts starting from text, an image, or an unstructured 3D object. First, given multiple views of a 3D object, generated or rendered, a multi-view diffusion model extracts a set of plausible and view-consistent part segmentations, dividing the object into parts. Then, a second multi-view diffusion model takes each part separately, fills in the occlusions, and uses those completed views for 3D reconstruction by feeding them to a 3D reconstruction network. This completion process considers the context of the entire object to ensure that the parts integrate cohesively. The generative completion model can make up for the information missing due to occlusions; in extreme cases, it can hallucinate entirely invisible parts based on the input 3D asset. We evaluate our method on generated and real 3D assets and show that it outperforms segmentation and part-extraction baselines by a large margin. We also showcase downstream applications such as 3D part editing.",
            "score": 1,
            "issue_id": 1308,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 –¥–µ–∫–∞–±—Ä—è",
                "en": "December 24",
                "zh": "12Êúà24Êó•"
            },
            "hash": "7f6d99dea7ea25bc",
            "authors": [
                "Minghao Chen",
                "Roman Shapovalov",
                "Iro Laina",
                "Tom Monnier",
                "Jianyuan Wang",
                "David Novotny",
                "Andrea Vedaldi"
            ],
            "affiliations": [
                "Meta AI",
                "Visual Geometry Group, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18608.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#hallucinations",
                    "#diffusion"
                ],
                "emoji": "üß©",
                "ru": {
                    "title": "PartGen: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "PartGen - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ –∑–Ω–∞—á–∏–º—ã—Ö —á–∞—Å—Ç–µ–π, –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ 3D-–æ–±—ä–µ–∫—Ç–∞. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É–ª—å—Ç–∏–≤–∏–¥–æ–≤—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç –≤—Ç–æ—Ä—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –æ–∫–∫–ª—é–∑–∏–π –∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D-—Ñ–æ—Ä–º—ã –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏. PartGen –º–æ–∂–µ—Ç –¥–∞–∂–µ –≤–æ—Å—Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–≤–∏–¥–∏–º—ã–µ —á–∞—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —á–∞—Å—Ç–µ–π, –∞ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –µ–≥–æ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 3D-—á–∞—Å—Ç–µ–π."
                },
                "en": {
                    "title": "PartGen: Transforming 3D Generation with Meaningful Parts",
                    "desc": "This paper presents PartGen, a new method for generating 3D objects that are composed of meaningful, manipulable parts from various inputs like text, images, or unstructured 3D objects. It utilizes a multi-view diffusion model to segment the 3D object into plausible parts based on multiple views, ensuring consistency across different perspectives. A second diffusion model then reconstructs each part by filling in occlusions and integrating them into a cohesive whole, even generating parts that are not visible in the input. The results demonstrate that PartGen significantly outperforms existing methods for segmentation and part extraction, enabling advanced applications like 3D part editing."
                },
                "zh": {
                    "title": "PartGenÔºöÁîüÊàêÂèØÊìç‰ΩúÁöÑ3DÁâ©‰ΩìÈÉ®ÂàÜ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫PartGenÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®‰ªéÊñáÊú¨„ÄÅÂõæÂÉèÊàñÈùûÁªìÊûÑÂåñ3DÂØπË±°ÁîüÊàêÁî±ÊúâÊÑè‰πâÈÉ®ÂàÜÁªÑÊàêÁöÑ3DÁâ©‰Ωì„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖàÂà©Áî®Â§öËßÜËßíÊâ©Êï£Ê®°ÂûãÊèêÂèñ3DÂØπË±°ÁöÑÈÉ®ÂàÜÂàÜÂâ≤ÔºåÂ∞ÜÂØπË±°ÂàíÂàÜ‰∏∫Â§ö‰∏™ÂèØÁã¨Á´ãÊìç‰ΩúÁöÑÈÉ®ÂàÜ„ÄÇÊé•ÁùÄÔºåÁ¨¨‰∫å‰∏™Â§öËßÜËßíÊâ©Êï£Ê®°ÂûãÂØπÊØè‰∏™ÈÉ®ÂàÜËøõË°åÂ°´ÂÖÖÂíå3DÈáçÂª∫ÔºåÁ°Æ‰øùÂêÑÈÉ®ÂàÜÂú®Êï¥‰Ωì‰∏ä‰∏ãÊñá‰∏≠ÂíåË∞êËûçÂêà„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPartGenÂú®ÁîüÊàêÂíåÁúüÂÆû3DËµÑ‰∫ß‰∏äÂùáÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÂàÜÂâ≤ÂíåÈÉ®ÂàÜÊèêÂèñÊñπÊ≥ï„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2024-12-24.html",
    "link_next": "2024-12-26.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "24.12",
        "en": "12/24",
        "zh": "12Êúà24Êó•"
    },
    "short_date_next": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12Êúà26Êó•"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂú®Áº∫‰πèÂ§ßÈáè‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ®°ÂûãÈÄöËøáËá™ÊàëÊîπËøõÊù•ÊèêÈ´òÊÄßËÉΩÁöÑÊñπÊ≥ï„ÄÇÊñáÁ´†ÊåáÂá∫ÔºåËá™ÊàëÊîπËøõÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÂíåÊú∫Âà∂Â∞ö‰∏çÊ∏ÖÊ•ö„ÄÇÁ†îÁ©∂ËÄÖËØÜÂà´Âπ∂ÊèêÂá∫‰∫ÜÁõëÊéß‰∏§‰∏™ÈáçË¶ÅÂõ†Á¥†ÁöÑÊñπÊ≥ïÔºöÊ®°ÂûãÁîüÊàêÂ§öÊ†∑ÂåñÂìçÂ∫îÁöÑËÉΩÂäõÔºàÊé¢Á¥¢ÔºâÂíåÂ§ñÈÉ®Â•ñÂä±Âå∫ÂàÜÈ´òË¥®ÈáèÂÄôÈÄâÁöÑÊúâÊïàÊÄßÔºàÂà©Áî®Ôºâ„ÄÇÈÄöËøáÊï∞Â≠¶Êé®ÁêÜÁöÑÊ°à‰æãÁ†îÁ©∂ÔºåÂèëÁé∞Ê®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõÂíåÂà©Áî®Â§ñÈÉ®Â•ñÂä±ÁöÑÊúâÊïàÊÄßÂú®Ëø≠‰ª£‰∏≠ËøÖÈÄü‰∏ãÈôç„ÄÇÂõ†Ê≠§ÔºåÁ†îÁ©∂ËÄÖÊèêÂá∫‰∫ÜB-STaRÊ°ÜÊû∂ÔºåËá™Âä®Ë∞ÉÊï¥ÈÖçÁΩÆ‰ª•Âπ≥Ë°°Êé¢Á¥¢ÂíåÂà©Áî®Ôºå‰ºòÂåñËá™ÊàëÊîπËøõÁöÑÊïàÊûú„ÄÇÂÆûÈ™åË°®ÊòéÔºåB-STaRÂú®Êï∞Â≠¶Êé®ÁêÜ„ÄÅÁºñÁ®ãÂíåÂ∏∏ËØÜÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ",
        "title": "B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners",
        "pinyin": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂú®Áº∫‰πèÂ§ßÈáè‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ®°ÂûãÈÄöËøáËá™ÊàëÊîπËøõÊù•ÊèêÈ´òÊÄßËÉΩÁöÑÊñπÊ≥ï„ÄÇ\nZh√® piƒÅn w√©nzhƒÅng t«éol√πn le z√†i quƒìf√° d√†li√†ng r√©ng≈çng biƒÅozh√π sh√πj√π de q√≠ngku√†ng xi√†, m√≥x√≠ng t≈çnggu√≤ z√¨w«í g«éij√¨n l√°i t√≠gƒÅo x√¨ngn√©ng de fƒÅngf«é.\n\nÊñáÁ´†ÊåáÂá∫ÔºåËá™ÊàëÊîπËøõÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÂíåÊú∫Âà∂Â∞ö‰∏çÊ∏ÖÊ•ö„ÄÇ\nW√©nzhƒÅng zh«êch≈´, z√¨w«í g«éij√¨n de gu«énji√†n yƒ´ns√π h√© jƒ´zh√¨ sh√†ng b√π qƒ´ngch«î.\n\nÁ†îÁ©∂ËÄÖËØÜÂà´Âπ∂ÊèêÂá∫‰∫ÜÁõëÊéß‰∏§‰∏™ÈáçË¶ÅÂõ†Á¥†ÁöÑÊñπÊ≥ïÔºöÊ®°ÂûãÁîüÊàêÂ§öÊ†∑ÂåñÂìçÂ∫îÁöÑËÉΩÂäõÔºàÊé¢Á¥¢ÔºâÂíåÂ§ñÈÉ®Â•ñÂä±Âå∫ÂàÜÈ´òË¥®ÈáèÂÄôÈÄâÁöÑÊúâÊïàÊÄßÔºàÂà©Áî®Ôºâ„ÄÇ\nY√°nji√πzhƒõ sh√≠bi√© b√¨ng t√≠ch≈´ le ji√†nk√≤ng li«éng g√® zh√≤ngy√†o yƒ´ns√π de fƒÅngf«é: m√≥x√≠ng shƒìngchƒìng du≈çy√†nghu√† xi«éngy√¨ng de n√©ngl√¨ (t√†nsu«í) h√© w√†ib√π ji«éngl√¨ q≈´fƒìn gƒÅo zh√¨li√†ng h√≤uxu«én de y«íuxi√†ox√¨ng (l√¨y√≤ng).\n\nÈÄöËøáÊï∞Â≠¶Êé®ÁêÜÁöÑÊ°à‰æãÁ†îÁ©∂ÔºåÂèëÁé∞Ê®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõÂíåÂà©Áî®Â§ñÈÉ®Â•ñÂä±ÁöÑÊúâÊïàÊÄßÂú®Ëø≠‰ª£‰∏≠ËøÖÈÄü‰∏ãÈôç„ÄÇ\nT≈çnggu√≤ sh√πxu√© tuƒ´l«ê de √†nl√¨ y√°nji≈´, fƒÅxi√†n m√≥x√≠ng de t√†nsu«í n√©ngl√¨ h√© l√¨y√≤ng w√†ib√π ji«éngl√¨ de y«íuxi√†ox√¨ng z√†i di√©d«éi zh≈çng x√πns√π xi√†ji√†ng.\n\nÂõ†Ê≠§ÔºåÁ†îÁ©∂ËÄÖÊèêÂá∫‰∫ÜB-STaRÊ°ÜÊû∂ÔºåËá™Âä®Ë∞ÉÊï¥ÈÖçÁΩÆ‰ª•Âπ≥Ë°°Êé¢Á¥¢ÂíåÂà©Áî®Ôºå‰ºòÂåñËá™ÊàëÊîπËøõÁöÑÊïàÊûú„ÄÇ\nYƒ´nc«ê, y√°nji√πzhƒõ t√≠ch≈´ le B-STaR ku√†ngji√†, z√¨d√≤ng ti√°ozhƒõng p√®izh√¨ y«ê p√≠ngh√©ng t√†nsu«í h√© l√¨y√≤ng, y≈çuhu√† z√¨w«í g«éij√¨n de xi√†ogu«í.\n\nÂÆûÈ™åË°®ÊòéÔºåB-STaRÂú®Êï∞Â≠¶Êé®ÁêÜ„ÄÅÁºñÁ®ãÂíåÂ∏∏ËØÜÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ\nSh√≠y√†n bi«éom√≠ng, B-STaR z√†i sh√πxu√© tuƒ´l«ê, biƒÅnch√©ng h√© ch√°ngsh√≠ tuƒ´l«ê r√®nw√π zh≈çng bi«éoxi√†n y≈çuy√¨.",
        "vocab": "[\n    {\"word\": \"ËÆ®ËÆ∫\", \"pinyin\": \"t«éo l√πn\", \"trans\": \"discuss\"},\n    {\"word\": \"Áº∫‰πè\", \"pinyin\": \"quƒì f√°\", \"trans\": \"lack\"},\n    {\"word\": \"‰∫∫Â∑•Ê†áÊ≥®\", \"pinyin\": \"r√©n g≈çng biƒÅo zh√π\", \"trans\": \"manual annotation\"},\n    {\"word\": \"Êï∞ÊçÆ\", \"pinyin\": \"sh√π j√π\", \"trans\": \"data\"},\n    {\"word\": \"ÊÉÖÂÜµ\", \"pinyin\": \"q√≠ng ku√†ng\", \"trans\": \"situation\"},\n    {\"word\": \"Ê®°Âûã\", \"pinyin\": \"m√≥ x√≠ng\", \"trans\": \"model\"},\n    {\"word\": \"Ëá™ÊàëÊîπËøõ\", \"pinyin\": \"z√¨ w«í g«éi j√¨n\", \"trans\": \"self-improvement\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ng n√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ÊñπÊ≥ï\", \"pinyin\": \"fƒÅng f«é\", \"trans\": \"method\"},\n    {\"word\": \"ÊåáÂá∫\", \"pinyin\": \"zh«ê ch≈´\", \"trans\": \"point out\"},\n    {\"word\": \"ÂÖ≥ÈîÆÂõ†Á¥†\", \"pinyin\": \"guƒÅn ji√†n yƒ´n s√π\", \"trans\": \"key factors\"},\n    {\"word\": \"Êú∫Âà∂\", \"pinyin\": \"jƒ´ zh√¨\", \"trans\": \"mechanism\"},\n    {\"word\": \"Â∞ö‰∏çÊ∏ÖÊ•ö\", \"pinyin\": \"sh√†ng b√π qƒ´ng ch«î\", \"trans\": \"not clear\"},\n    {\"word\": \"ËØÜÂà´\", \"pinyin\": \"sh√≠ bi√©\", \"trans\": \"identify\"},\n    {\"word\": \"ÊèêÂá∫\", \"pinyin\": \"t√≠ ch≈´\", \"trans\": \"propose\"},\n    {\"word\": \"ÁõëÊéß\", \"pinyin\": \"ji√†n k√≤ng\", \"trans\": \"monitor\"},\n    {\"word\": \"Â§öÊ†∑Âåñ\", \"pinyin\": \"du≈ç y√†ng hu√†\", \"trans\": \"diversify\"},\n    {\"word\": \"ÂìçÂ∫î\", \"pinyin\": \"xi«éng y√¨ng\", \"trans\": \"response\"},\n    {\"word\": \"ËÉΩÂäõ\", \"pinyin\": \"n√©ng l√¨\", \"trans\": \"ability\"},\n    {\"word\": \"Êé¢Á¥¢\", \"pinyin\": \"t√†n su«í\", \"trans\": \"explore\"},\n    {\"word\": \"Â§ñÈÉ®Â•ñÂä±\", \"pinyin\": \"w√†i b√π ji«éng l√¨\", \"trans\": \"external reward\"},\n    {\"word\": \"Âå∫ÂàÜ\", \"pinyin\": \"q≈´ fƒìn\", \"trans\": \"distinguish\"},\n    {\"word\": \"È´òË¥®Èáè\", \"pinyin\": \"gƒÅo zh√¨ li√†ng\", \"trans\": \"high quality\"},\n    {\"word\": \"ÂÄôÈÄâ\", \"pinyin\": \"h√≤u xu«én\", \"trans\": \"candidate\"},\n    {\"word\": \"ÊúâÊïàÊÄß\", \"pinyin\": \"y«íu xi√†o x√¨ng\", \"trans\": \"effectiveness\"},\n    {\"word\": \"Âà©Áî®\", \"pinyin\": \"l√¨ y√≤ng\", \"trans\": \"utilize\"},\n    {\"word\": \"Êï∞Â≠¶Êé®ÁêÜ\", \"pinyin\": \"sh√π xu√© tuƒ´ l«ê\", \"trans\": \"mathematical reasoning\"},\n    {\"word\": \"Ê°à‰æãÁ†îÁ©∂\", \"pinyin\": \"√†n l√¨ y√°n ji≈´\", \"trans\": \"case study\"},\n    {\"word\": \"ÂèëÁé∞\", \"pinyin\": \"fƒÅ xi√†n\", \"trans\": \"discover\"},\n    {\"word\": \"Ëø≠‰ª£\", \"pinyin\": \"di√© d√†i\", \"trans\": \"iteration\"},\n    {\"word\": \"ËøÖÈÄü\", \"pinyin\": \"x√πn s√π\", \"trans\": \"rapidly\"},\n    {\"word\": \"‰∏ãÈôç\", \"pinyin\": \"xi√† ji√†ng\", \"trans\": \"decline\"},\n    {\"word\": \"Âõ†Ê≠§\", \"pinyin\": \"yƒ´n c«ê\", \"trans\": \"therefore\"},\n    {\"word\": \"Ê°ÜÊû∂\", \"pinyin\": \"ku√†ng ji√†\", \"trans\": \"framework\"},\n    {\"word\": \"Ëá™Âä®Ë∞ÉÊï¥\", \"pinyin\": \"z√¨ d√≤ng ti√°o zhƒõng\", \"trans\": \"automatic adjustment\"},\n    {\"word\": \"ÈÖçÁΩÆ\", \"pinyin\": \"p√®i zh√¨\", \"trans\": \"configuration\"},\n    {\"word\": \"Âπ≥Ë°°\", \"pinyin\": \"p√≠ng h√©ng\", \"trans\": \"balance\"},\n    {\"word\": \"‰ºòÂåñ\", \"pinyin\": \"y≈çu hu√†\", \"trans\": \"optimize\"},\n    {\"word\": \"ÊïàÊûú\", \"pinyin\": \"xi√†o gu«í\", \"trans\": \"effect\"},\n    {\"word\": \"ÂÆûÈ™å\", \"pinyin\": \"sh√≠ y√†n\", \"trans\": \"experiment\"},\n    {\"word\": \"Ë°®Êòé\", \"pinyin\": \"bi«éo m√≠ng\", \"trans\": \"indicate\"},\n    {\"word\": \"ÁºñÁ®ã\", \"pinyin\": \"biƒÅn ch√©ng\", \"trans\": \"programming\"},\n    {\"word\": \"Â∏∏ËØÜÊé®ÁêÜ\", \"pinyin\": \"ch√°ng sh√≠ tuƒ´ l«ê\", \"trans\": \"commonsense reasoning\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n w√π\", \"trans\": \"task\"},\n    {\"word\": \"Ë°®Áé∞\", \"pinyin\": \"bi«éo xi√†n\", \"trans\": \"performance\"},\n    {\"word\": \"‰ºòÂºÇ\", \"pinyin\": \"y≈çu y√¨\", \"trans\": \"excellent\"}\n]",
        "trans": "This article discusses methods for improving model performance through self-improvement in the absence of large amounts of manually labeled data. The article notes that the key factors and mechanisms of self-improvement are not yet clear. Researchers have identified and proposed methods to monitor two important factors: the model's ability to generate diverse responses (exploration) and the effectiveness of external rewards in distinguishing high-quality candidates (exploitation). Through a case study on mathematical reasoning, it was found that the model's exploration ability and the effectiveness of utilizing external rewards rapidly decline during iterations. Therefore, the researchers proposed the B-STaR framework, which automatically adjusts configurations to balance exploration and exploitation, optimizing the effects of self-improvement. Experiments show that B-STaR performs excellently in mathematical reasoning, programming, and common sense reasoning tasks.",
        "update_ts": "2024-12-24 09:10"
    }
}