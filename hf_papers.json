{
    "date": {
        "ru": "3 декабря",
        "en": "December 3",
        "zh": "12月3日"
    },
    "time_utc": "2024-12-03 04:12",
    "weekday": 1,
    "issue_id": 910,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.18671",
            "title": "TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video",
            "url": "https://huggingface.co/papers/2411.18671",
            "abstract": "In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its point tracking robustness in long videos. TAPTRv2 is a simple DETR-like framework that can accurately track any point in real-world videos without requiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in querying high quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial and temporal dimensions for more robust tracking in long videos. For better spatial feature querying, we present Context-aware Cross-Attention (CCA), which leverages surrounding spatial context to enhance the quality of attention scores when querying image features. For better temporal feature querying, we introduce Visibility-aware Long-Temporal Attention (VLTA) to conduct temporal attention to all past frames while considering their corresponding visibilities, which effectively addresses the feature drifting problem in TAPTRv2 brought by its RNN-like long-temporal modeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the challenging datasets and obtains state-of-the-art performance. Even when compared with methods trained with large-scale extra internal data, TAPTRv3 is still competitive.",
            "score": 6,
            "issue_id": 909,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "f99e1015e9222dc6",
            "authors": [
                "Jinyuan Qu",
                "Hongyang Li",
                "Shilong Liu",
                "Tianhe Ren",
                "Zhaoyang Zeng",
                "Lei Zhang"
            ],
            "affiliations": [
                "International Digital Economy Academy (IDEA)",
                "South China University of Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18671.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#video",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Контекстное внимание для надежного отслеживания точек в видео",
                    "desc": "TAPTRv3 - это улучшенная версия TAPTRv2 для более надежного отслеживания точек в длинных видео. Система использует пространственный и временной контекст для повышения качества запроса признаков. Введены два новых механизма: Context-aware Cross-Attention (CCA) для улучшения пространственного запроса и Visibility-aware Long-Temporal Attention (VLTA) для временного запроса. TAPTRv3 значительно превосходит предыдущую версию и достигает наилучших результатов на сложных наборах данных."
                },
                "en": {
                    "title": "Enhancing Video Point Tracking with TAPTRv3",
                    "desc": "TAPTRv3 is an advanced framework designed to enhance point tracking in lengthy videos, building on the previous version, TAPTRv2. It improves the ability to query high-quality features by incorporating both spatial and temporal contexts, which helps in maintaining robust tracking despite variations over time. The paper introduces Context-aware Cross-Attention (CCA) for better spatial feature querying and Visibility-aware Long-Temporal Attention (VLTA) for improved temporal feature querying, effectively addressing issues like feature drifting. TAPTRv3 demonstrates significant performance improvements over TAPTRv2 and competes well against other state-of-the-art methods, even those trained on larger datasets."
                },
                "zh": {
                    "title": "TAPTRv3：长视频点跟踪的新突破",
                    "desc": "本文介绍了TAPTRv3，这是在TAPTRv2基础上开发的，旨在提高长视频中的点跟踪鲁棒性。TAPTRv2是一个简单的类似DETR的框架，可以准确跟踪现实视频中的任意点，而无需成本体积。TAPTRv3通过利用空间和时间上下文来改善特征查询，从而在长视频中实现更稳健的跟踪。我们提出了上下文感知交叉注意力（CCA）和可见性感知长时间注意力（VLTA），显著提升了特征查询的质量，超越了TAPTRv2，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01316",
            "title": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation",
            "url": "https://huggingface.co/papers/2412.01316",
            "abstract": "We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/.",
            "score": 1,
            "issue_id": 910,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "5bca597a08ec0a14",
            "authors": [
                "Xin Yan",
                "Yuxuan Cai",
                "Qiuyue Wang",
                "Yuan Zhou",
                "Wenhao Huang",
                "Huan Yang"
            ],
            "affiliations": [
                "01.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01316.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#architecture",
                    "#diffusion",
                    "#long_context"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Presto: Революция в генерации длинных видео с помощью ИИ",
                    "desc": "Presto - это новая модель диффузии видео, разработанная для генерации 15-секундных видеороликов с долгосрочной согласованностью и богатым содержанием. Модель использует стратегию Segmented Cross-Attention (SCA), которая разделяет скрытые состояния на сегменты вдоль временного измерения, позволяя каждому сегменту перекрестно обращаться к соответствующей подписи. Для обучения модели был создан датасет LongTake-HD, содержащий 261 тысячу видео с богатым содержанием и согласованностью сценариев. Эксперименты показывают, что Presto превосходит существующие методы генерации видео по показателям семантической оценки и степени динамичности."
                },
                "en": {
                    "title": "Presto: Revolutionizing Video Generation with Long-Range Coherence",
                    "desc": "Presto is a new video diffusion model that generates 15-second videos while ensuring long-range coherence and rich content. It introduces a Segmented Cross-Attention (SCA) strategy that divides hidden states into segments, allowing each segment to focus on specific sub-captions without needing extra parameters. To support this model, the LongTake-HD dataset was created, containing 261,000 videos with coherent scenarios and detailed captions. Experiments show that Presto outperforms existing methods, achieving high scores in semantic understanding and dynamic content generation."
                },
                "zh": {
                    "title": "Presto：生成长时间一致性视频的新方法",
                    "desc": "我们介绍了一种新的视频扩散模型Presto，旨在生成具有长时间一致性和丰富内容的15秒视频。为了解决在长时间内保持场景多样性的挑战，我们提出了一种分段交叉注意力(SCA)策略，该策略将隐藏状态沿时间维度分段，使每个段能够与相应的子标题进行交叉关注。SCA不需要额外的参数，可以无缝集成到现有的基于DiT的架构中。我们的实验表明，Presto在视频生成方面显著优于现有的最先进方法，提升了内容丰富性和长距离一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17459",
            "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
            "url": "https://huggingface.co/papers/2411.17459",
            "abstract": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.",
            "score": 1,
            "issue_id": 909,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "9b68162718865b81",
            "authors": [
                "Zongjian Li",
                "Bin Lin",
                "Yang Ye",
                "Liuhan Chen",
                "Xinhua Cheng",
                "Shenghai Yuan",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University",
                "Peng Cheng Laboratory",
                "Rabbitpre Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17459.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#data",
                    "#video",
                    "#architecture",
                    "#open_source",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Эффективное кодирование видео с помощью вейвлетов",
                    "desc": "Статья представляет новый метод кодирования видео под названием Wavelet Flow VAE (WF-VAE). Этот автоэнкодер использует многоуровневое вейвлет-преобразование для эффективного кодирования низкочастотной информации в видео. WF-VAE решает проблему вычислительных ограничений при обработке видео высокого разрешения и большой длительности в латентных видео-диффузионных моделях (LVDM). Метод также включает технику Causal Cache для сохранения целостности латентного пространства при поблочной обработке."
                },
                "en": {
                    "title": "Efficient Video Encoding with Wavelet Flow VAE",
                    "desc": "The paper introduces Wavelet Flow VAE (WF-VAE), a novel approach to encoding videos into a low-dimensional latent space using wavelet transforms. This method addresses the computational bottleneck of traditional Video VAEs, especially when generating high-resolution and long-duration videos. By decomposing videos into frequency-domain components, WF-VAE enhances the efficiency of encoding critical information. Additionally, the proposed Causal Cache method ensures continuity in the latent space during block-wise inference, resulting in improved performance metrics compared to existing video VAEs."
                },
                "zh": {
                    "title": "小波流VAE：高效视频编码的新方法",
                    "desc": "视频变分自编码器（VAE）将视频编码为低维潜在空间，是大多数潜在视频扩散模型（LVDMs）的关键组成部分，能够降低模型训练成本。然而，随着生成视频的分辨率和时长增加，视频VAE的编码成本成为训练LVDMs的瓶颈。此外，大多数LVDMs采用的块状推理方法在处理长时长视频时可能导致潜在空间的不连续性。为了解决计算瓶颈，我们提出了小波流VAE（WF-VAE），通过多级小波变换有效编码视频的关键信息，并引入因果缓存方法以保持潜在空间的完整性。"
                }
            }
        }
    ],
    "link_prev": "2024-12-02.html",
    "link_next": "2024-12-04.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12月2日"
    },
    "short_date_next": {
        "ru": "04.12",
        "en": "12/04",
        "zh": "12月4日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "近年来，通用多模态大语言模型（MLLMs）取得了快速发展。然而，将这些通用模型应用到特定领域，如科学和工业，仍然较少探索。本文系统地研究了通过后训练进行MLLMs的领域适应，重点是数据合成、训练流水线和任务评估。我们开发了一个视觉指令合成器，能够从特定领域的图片-标题对中生成多样化的视觉指令任务。我们的合成任务在增强MLLMs的领域特定性能方面优于手动规则、GPT-4和GPT-4V生成的任务。我们还使用单阶段训练流水线来增强特定领域的任务多样性。我们在生物医学和食品领域进行了实验，并评估了不同来源和规模的MLLMs在各种特定领域任务上的性能。为支持进一步研究，我们将开源我们的实现。",
        "title": "On Domain-Specific Post-Training for Multimodal Large Language Models",
        "pinyin": "Jìn nián lái, tōngyòng duō mó tài dà yǔyán móxíng (MLLMs) qǔdé le kuàisù fāzhǎn. Rán'ér, jiāng zhèxiē tōngyòng móxíng yìngyòng dào tèdìng lǐngyù, rú kēxué hé gōngyè, réngrán jiàoshǎo tànsuǒ. Běnwén xìtǒng de yánjiū le tōngguò hòu xùnliàn jìnxíng MLLMs de lǐngyù shìyìng, zhòngdiǎn shì shùjù héchéng, xùnliàn liúshuǐxiàn hé rènwù pínggǔ. Wǒmen kāifā le yīgè shìjiào zhǐlìng héchéngqì, nénggòu cóng tèdìng lǐngyù de túpiàn-biāotí duì zhōng shēngchéng duōyànghuà de shìjiào zhǐlìng rènwù. Wǒmen de héchéng rènwù zài zēngqiáng MLLMs de lǐngyù tèdìng xìngnéng fāngmiàn yōu shǒudòng guīzé, GPT-4 hé GPT-4V shēngchéng de rènwù. Wǒmen hái shǐyòng dān jiēduàn xùnliàn liúshuǐxiàn lái zēngqiáng tèdìng lǐngyù de rènwù duōyànghuàxìng. Wǒmen zài shēngwù yīxiào hé shípǐn lǐngyù jìnxíng le shíyàn, bìng pínggǔ le bùtóng láiyuán hé guīmó de MLLMs zài gèzhǒng tèdìng lǐngyù rènwù shàng de xìngnéng. Wèi zhīchí jìnfā yánjiū, wǒmen jiāng kāiyuán wǒmen de shíxiàn.",
        "vocab": "[{'word': '近年来', 'pinyin': 'jìn nián lái', 'trans': 'in recent years'},\n{'word': '通用', 'pinyin': 'tōng yòng', 'trans': 'universal'},\n{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'},\n{'word': '快速', 'pinyin': 'kuài sù', 'trans': 'rapid'},\n{'word': '发展', 'pinyin': 'fā zhǎn', 'trans': 'development'},\n{'word': '然而', 'pinyin': 'rán ér', 'trans': 'however'},\n{'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'apply'},\n{'word': '特定', 'pinyin': 'tè dìng', 'trans': 'specific'},\n{'word': '领域', 'pinyin': 'lǐng yù', 'trans': 'field'},\n{'word': '科学', 'pinyin': 'kē xué', 'trans': 'science'},\n{'word': '工业', 'pinyin': 'gōng yè', 'trans': 'industry'},\n{'word': '探索', 'pinyin': 'tàn suǒ', 'trans': 'explore'},\n{'word': '系统地', 'pinyin': 'xì tǒng de', 'trans': 'systematically'},\n{'word': '研究', 'pinyin': 'yán jiū', 'trans': 'study'},\n{'word': '后训练', 'pinyin': 'hòu xùn liàn', 'trans': 'post-training'},\n{'word': '进行', 'pinyin': 'jìn xíng', 'trans': 'conduct'},\n{'word': '适应', 'pinyin': 'shì yìng', 'trans': 'adaptation'},\n{'word': '重点', 'pinyin': 'zhòng diǎn', 'trans': 'focus'},\n{'word': '数据', 'pinyin': 'shù jù', 'trans': 'data'},\n{'word': '合成', 'pinyin': 'hé chéng', 'trans': 'synthesis'},\n{'word': '流水线', 'pinyin': 'liú shuǐ xiàn', 'trans': 'pipeline'},\n{'word': '任务', 'pinyin': 'rèn wù', 'trans': 'task'},\n{'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluation'},\n{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'develop'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '指令', 'pinyin': 'zhǐ lìng', 'trans': 'instruction'},\n{'word': '合成器', 'pinyin': 'hé chéng qì', 'trans': 'synthesizer'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'},\n{'word': '图片', 'pinyin': 'tú piàn', 'trans': 'image'},\n{'word': '标题', 'pinyin': 'biāo tí', 'trans': 'title'},\n{'word': '对', 'pinyin': 'duì', 'trans': 'pair'},\n{'word': '多样化', 'pinyin': 'duō yàng huà', 'trans': 'diversify'},\n{'word': '增强', 'pinyin': 'zēng qiáng', 'trans': 'enhance'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'},\n{'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'},\n{'word': '手动', 'pinyin': 'shǒu dòng', 'trans': 'manual'},\n{'word': '规则', 'pinyin': 'guī zé', 'trans': 'rule'},\n{'word': '单阶段', 'pinyin': 'dān jiē duàn', 'trans': 'single-stage'},\n{'word': '生物医学', 'pinyin': 'shēng wù yī xué', 'trans': 'biomedical'},\n{'word': '食品', 'pinyin': 'shí pǐn', 'trans': 'food'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '来源', 'pinyin': 'lái yuán', 'trans': 'source'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'},\n{'word': '支持', 'pinyin': 'zhī chí', 'trans': 'support'},\n{'word': '进一步', 'pinyin': 'jìn yī bù', 'trans': 'further'},\n{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open-source'},\n{'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'implementation'}]",
        "trans": "In recent years, general-purpose multimodal large language models (MLLMs) have made rapid progress. However, the application of these general models to specific domains, such as science and industry, remains relatively unexplored. This paper systematically investigates the domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. We developed a visual instruction synthesizer capable of generating diverse visual instruction tasks from domain-specific image-caption pairs. Our synthesized tasks outperform those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. We also employed a single-stage training pipeline to enhance the diversity of domain-specific tasks. We conducted experiments in the biomedical and food domains and evaluated the performance of MLLMs from different sources and scales on various domain-specific tasks. To support further research, we will open-source our implementation.",
        "update_ts": "2024-12-02 09:12"
    }
}