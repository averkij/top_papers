{
    "date": {
        "ru": "28 мая",
        "en": "May 28",
        "zh": "5月28日"
    },
    "time_utc": "2025-05-28 06:17",
    "weekday": 2,
    "issue_id": 3994,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.18445",
            "title": "OmniConsistency: Learning Style-Agnostic Consistency from Paired\n  Stylization Data",
            "url": "https://huggingface.co/papers/2505.18445",
            "abstract": "OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose OmniConsistency, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.",
            "score": 45,
            "issue_id": 3990,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 мая",
                "en": "May 24",
                "zh": "5月24日"
            },
            "hash": "0a5e56835e542da2",
            "authors": [
                "Yiren Song",
                "Cheng Liu",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18445.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#diffusion",
                    "#cv",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная согласованность стиля в генерации изображений",
                    "desc": "OmniConsistency - это универсальный плагин для улучшения согласованности стилизации в задачах преобразования изображений. Он использует крупномасштабные Диффузионные Трансформеры (DiTs) и обучается на парах выровненных изображений для лучшей генерализации. Плагин применяет двухэтапную стратегию прогрессивного обучения, разделяющую изучение стиля и сохранение согласованности. OmniConsistency совместим с произвольными стилевыми LoRA и значительно повышает визуальную согласованность и эстетическое качество изображений."
                },
                "en": {
                    "title": "Achieving Consistent and High-Quality Image Stylization with OmniConsistency",
                    "desc": "OmniConsistency is a novel approach that improves the consistency and generalization of image stylization using large-scale Diffusion Transformers. It addresses two main challenges in image-to-image pipelines: ensuring consistent stylization across complex scenes and preventing degradation of style when using style LoRAs. The method introduces a learning framework that focuses on maintaining consistency while allowing for flexible style application. Experimental results demonstrate that OmniConsistency achieves visual quality and coherence comparable to leading commercial models."
                },
                "zh": {
                    "title": "OmniConsistency：提升图像风格一致性的创新方案",
                    "desc": "OmniConsistency 是一种利用大规模扩散变换器（Diffusion Transformers）来增强图像到图像管道中的风格一致性和泛化能力的方法。该方法解决了在复杂场景中保持一致风格和防止风格退化的两个主要挑战。OmniConsistency 提供了一种基于对齐图像对的上下文一致性学习框架，并采用两阶段的渐进学习策略来分离风格学习与一致性保持。实验结果表明，OmniConsistency 显著提高了视觉连贯性和美学质量，性能可与商业最先进模型 GPT-4o 相媲美。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21497",
            "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers",
            "url": "https://huggingface.co/papers/2505.21497",
            "abstract": "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.",
            "score": 37,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "7f740f76be754bce",
            "authors": [
                "Wei Pang",
                "Kevin Qinghong Lin",
                "Xiangru Jian",
                "Xi He",
                "Philip Torr"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Oxford",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21497.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#agents",
                    "#science"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Автоматическая генерация научных постеров: от статьи к визуализации",
                    "desc": "Эта статья представляет первый эталонный тест и набор метрик для генерации академических постеров, сопоставляя недавние научные статьи с постерами, созданными авторами. Исследователи предлагают PosterAgent - многоагентный конвейер, который включает в себя Parser для извлечения ключевой информации, Planner для создания структуры постера и Painter-Commenter для визуального оформления. Оценка показывает, что открытые модели на основе Qwen-2.5 превосходят существующие системы по большинству метрик, используя на 87% меньше токенов. Исследование открывает путь к следующему поколению полностью автоматизированных моделей для создания постеров."
                },
                "en": {
                    "title": "Revolutionizing Academic Poster Generation with PosterAgent",
                    "desc": "This paper addresses the challenge of generating academic posters from lengthy scientific documents by introducing a benchmark and metric suite for evaluation. It presents PosterAgent, a multi-agent pipeline that includes a Parser for structuring content, a Planner for layout design, and a Painter-Commenter loop for refining visuals based on feedback. The study evaluates the effectiveness of generated posters using metrics like visual quality, textual coherence, and the ability to convey core content through quizzes. The results show that their open-source approach significantly outperforms existing models while being more efficient in token usage, paving the way for future advancements in automated poster generation."
                },
                "zh": {
                    "title": "自动化学术海报生成的新纪元",
                    "desc": "本论文介绍了一种新的学术海报生成基准和评估指标，旨在将长篇文档压缩为视觉上连贯的单页海报。我们提出了PosterAgent，一个多代理管道，能够有效地解析、规划和绘制海报内容。通过对比不同模型的输出，我们发现人类设计的海报在视觉语义上更具吸引力，而GPT-4o模型虽然外观美观，但文本质量和信息传达能力较差。我们的开源变体在多个指标上超越了现有系统，并且显著减少了所需的计算资源。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21327",
            "title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
            "url": "https://huggingface.co/papers/2505.21327",
            "abstract": "MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  \t\t\t\t\tAI-generated summary \t\t\t\t Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.",
            "score": 30,
            "issue_id": 3991,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "a022fb524c5969bf",
            "authors": [
                "Jiakang Yuan",
                "Tianshuo Peng",
                "Yilei Jiang",
                "Yiting Lu",
                "Renrui Zhang",
                "Kaituo Feng",
                "Chaoyou Fu",
                "Tao Chen",
                "Lei Bai",
                "Bo Zhang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "Fudan University",
                "MMLab, The Chinese University of Hong Kong",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21327.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрывая пробелы в логике искусственного интеллекта",
                    "desc": "MME-Reasoning - это новый комплексный бенчмарк для оценки способностей мультимодальных больших языковых моделей (MLLM) к логическому рассуждению. Он охватывает индуктивное, дедуктивное и абдуктивное рассуждения, фокусируясь именно на логике, а не на восприятии или знаниях. Оценка показала существенные ограничения современных MLLM в комплексных логических рассуждениях, с заметными различиями в производительности для разных типов рассуждений. Анализ также выявил ограниченную эффективность популярных подходов, таких как 'режим мышления' и обучение с подкреплением на основе правил, для улучшения способностей к рассуждению."
                },
                "en": {
                    "title": "Unveiling the Reasoning Gaps in Multimodal AI",
                    "desc": "The paper introduces MME-Reasoning, a benchmark designed to assess the logical reasoning abilities of multimodal large language models (MLLMs). It categorizes reasoning into three types: inductive, deductive, and abductive, addressing gaps in existing evaluations that often overlook these distinctions. The study reveals that even advanced MLLMs struggle with logical reasoning tasks, showing significant performance imbalances across the different reasoning types. Additionally, the paper analyzes common methods aimed at improving reasoning, highlighting the persistent limitations of current MLLMs in effectively handling diverse logical reasoning challenges."
                },
                "zh": {
                    "title": "评估多模态模型的逻辑推理能力",
                    "desc": "MME-Reasoning 是一个评估多模态大型语言模型（MLLMs）逻辑推理能力的基准，揭示了在归纳、演绎和溯因推理类型上的显著局限性和性能不平衡。尽管多模态推理取得了显著进展，但现有基准未能全面评估其推理能力，缺乏对逻辑推理类型的明确分类。我们设计了 MME-Reasoning，涵盖所有三种推理类型的问题，确保每个问题有效评估推理能力，而非感知技能或知识广度。评估结果显示，当前最先进的 MLLMs 在全面的逻辑推理评估中表现有限，且在不同推理类型之间存在明显的性能差异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19000",
            "title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied\n  Iterative Policy Optimization",
            "url": "https://huggingface.co/papers/2505.19000",
            "abstract": "A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Applying Reinforcement Learning (RL) to Video Large Language Models (Video-LLMs) shows significant promise for complex video reasoning. However, popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group Relative Policy Optimization (GRPO), are limited by data preparation bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the quality of long chain-of-thoughts (CoTs) and downstream performance.To address these limitations, we propose VerIPO, a Verifier-guided Iterative Policy Optimization method designed to gradually improve video LLMs' capacity for generating deep, long-term reasoning chains. The core component is Rollout-Aware Verifier, positioned between the GRPO and Direct Preference Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop. This verifier leverages small LLMs as a judge to assess the reasoning logic of rollouts, enabling the construction of high-quality contrastive data, including reflective and contextually consistent CoTs. These curated preference samples drive the efficient DPO stage (7x faster than GRPO), leading to marked improvements in reasoning chain quality, especially in terms of length and contextual consistency. This training loop benefits from GRPO's expansive search and DPO's targeted optimization. Experimental results demonstrate: 1) Significantly faster and more effective optimization compared to standard GRPO variants, yielding superior performance; 2) Our trained models exceed the direct inference of large-scale instruction-tuned Video-LLMs, producing long and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long reasoning models (e.g., Video-R1), highlighting its effectiveness and stability.",
            "score": 30,
            "issue_id": 3991,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 мая",
                "en": "May 25",
                "zh": "5月25日"
            },
            "hash": "058fcf46b0f20cc6",
            "authors": [
                "Yunxin Li",
                "Xinyu Chen",
                "Zitao Li",
                "Zhenyu Liu",
                "Longyue Wang",
                "Wenhan Luo",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Alibaba International Group",
                "Division of AMC and Department of ECE, HKUST",
                "Harbin Institute of Technology, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19000.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#video",
                    "#optimization",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VerIPO: Улучшение рассуждений видео-LLM с помощью верификатора",
                    "desc": "Статья представляет метод VerIPO для улучшения способностей видео-LLM к рассуждениям. Метод использует Verifier между фазами GRPO и DPO для оптимизации генерации длинных цепочек рассуждений. VerIPO позволяет создавать качественные контрастные данные, что ускоряет обучение в 7 раз по сравнению с GRPO. Эксперименты показывают превосходство VerIPO над существующими методами в задачах видео-рассуждений."
                },
                "en": {
                    "title": "Enhancing Video Reasoning with Verifier-guided Optimization",
                    "desc": "This paper introduces VerIPO, a new method for improving Video Large Language Models (Video-LLMs) using a Verifier-guided Iterative Policy Optimization approach. It addresses the limitations of existing Reinforcement Fine-Tuning methods by incorporating a Rollout-Aware Verifier that enhances the quality of reasoning chains during training. By creating high-quality contrastive data, this method allows for faster and more effective optimization, achieving results that are significantly better than traditional methods. Experimental findings show that VerIPO not only speeds up the training process but also improves the contextual consistency and length of reasoning outputs in video tasks."
                },
                "zh": {
                    "title": "验证者引导的迭代优化，提升视频推理能力",
                    "desc": "本文提出了一种名为VerIPO的验证者引导迭代策略优化方法，旨在提升视频大型语言模型（Video-LLMs）的推理能力。该方法通过在GRPO和DPO阶段之间引入一个回滚感知验证器，形成GRPO-验证器-DPO训练循环，从而实现更快且更有效的优化。验证器利用小型语言模型评估推理逻辑，生成高质量的对比数据，促进了长链推理的生成。实验结果表明，VerIPO在优化速度和推理质量上均显著优于传统的GRPO方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21374",
            "title": "Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?",
            "url": "https://huggingface.co/papers/2505.21374",
            "abstract": "Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a \"Holmes-test\" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes.",
            "score": 24,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "0683137770e562ab",
            "authors": [
                "Junhao Cheng",
                "Yuying Ge",
                "Teng Wang",
                "Yixiao Ge",
                "Jing Liao",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "City University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21374.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Шерлок Холмс для ИИ: новый вызов в понимании видео",
                    "desc": "Video-Holmes - это новый бенчмарк для оценки способностей мультимодальных языковых моделей к сложным рассуждениям на основе видео. Он использует короткометражные фильмы-саспенс и состоит из 1837 вопросов по 270 аннотированным видео, охватывающих 7 специально разработанных задач. Бенчмарк выявил значительные трудности современных моделей в интеграции информации и поиске ключевых подсказок по сравнению с экспертами-людьми. Лучшая модель Gemini-2.5-Pro достигла точности всего 45%, что подчеркивает сложность задачи и необходимость дальнейших исследований в этой области."
                },
                "en": {
                    "title": "Video-Holmes: A New Benchmark for Complex Video Reasoning",
                    "desc": "The Video-Holmes benchmark assesses the complex video reasoning abilities of Multimodal Language Models (MLLMs) using suspense short films. It highlights the challenges these models face in integrating information compared to human experts, particularly in real-world reasoning scenarios. The benchmark includes 1,837 questions based on 270 annotated films, requiring models to connect multiple visual clues across different segments. Despite advancements in reasoning techniques, the evaluation shows that even the best models struggle with accuracy, achieving only 45%, indicating significant room for improvement in multimodal reasoning."
                },
                "zh": {
                    "title": "Video-Holmes：激励模型更像人类推理的基准测试",
                    "desc": "Video-Holmes基准测试评估了多模态大语言模型（MLLMs）在复杂视频推理方面的能力，特别是通过悬疑短片来揭示与人类专家相比的信息整合挑战。该基准包含来自270部手动注释的悬疑短片的1837个问题，设计了七个任务，要求模型主动寻找和连接分散在不同视频片段中的多个相关视觉线索。尽管现有的MLLMs在视觉感知方面表现良好，但在信息整合上却面临重大困难，许多模型的准确率低于40%。我们希望Video-Holmes能够激励模型更像人类进行推理，并强调这一领域的持续挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20355",
            "title": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient\n  Fine-Tuning",
            "url": "https://huggingface.co/papers/2505.20355",
            "abstract": "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git",
            "score": 23,
            "issue_id": 3990,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "d4035428ea14ce6b",
            "authors": [
                "Yeonjoon Jung",
                "Daehyun Ahn",
                "Hyungjun Kim",
                "Taesu Kim",
                "Eunhyeok Park"
            ],
            "affiliations": [
                "POSTECH",
                "SqueezeBits"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20355.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "GraLoRA: Гранулярная низкоранговая адаптация для эффективной настройки генеративных моделей",
                    "desc": "Статья представляет новый метод адаптации моделей машинного обучения под названием Granular Low-Rank Adaptation (GraLoRA). GraLoRA преодолевает ограничения популярного метода Low-Rank Adaptation (LoRA), связанные с переобучением при увеличении ранга. Метод разбивает весовые матрицы на подблоки, каждый со своим низкоранговым адаптером, что позволяет эффективно увеличить репрезентативную способность модели. Эксперименты показывают, что GraLoRA превосходит LoRA и другие базовые методы в задачах генерации кода и здравого смысла, достигая улучшения до 8.5% в метрике Pass@1 на датасете HumanEval+."
                },
                "en": {
                    "title": "GraLoRA: Unlocking the Power of Fine-Tuning with Granular Adaptation",
                    "desc": "This paper introduces Granular Low-Rank Adaptation (GraLoRA), a new method designed to improve the performance of Low-Rank Adaptation (LoRA) in fine-tuning generative models. LoRA is effective but struggles with overfitting when the rank is increased, leading to poor accuracy compared to full fine-tuning. GraLoRA addresses this issue by dividing weight matrices into smaller sub-blocks, allowing each to have its own low-rank adapter, which enhances gradient propagation and reduces entanglement. Experimental results demonstrate that GraLoRA significantly outperforms LoRA and other methods, achieving notable improvements in various benchmarks without increasing computational costs."
                },
                "zh": {
                    "title": "颗粒低秩适应：超越LoRA的高效微调",
                    "desc": "低秩适应（LoRA）是一种流行的参数高效微调方法，因其简单有效而受到重视。尽管最近有所改进，LoRA仍然面临一个根本性限制：当瓶颈加宽时容易过拟合。我们提出了一种新结构，称为颗粒低秩适应（GraLoRA），它将权重矩阵划分为子块，每个子块都有自己的低秩适配器，从而克服了LoRA的局限性。实验表明，GraLoRA在代码生成和常识推理基准上表现优于LoRA，具有更强的可扩展性和鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21333",
            "title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios",
            "url": "https://huggingface.co/papers/2505.21333",
            "abstract": "MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.",
            "score": 22,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "25639980b7f7add5",
            "authors": [
                "Yang Shi",
                "Huanqian Wang",
                "Wulin Xie",
                "Huanyao Zhang",
                "Lijie Zhao",
                "Yi-Fan Zhang",
                "Xinfeng Li",
                "Chaoyou Fu",
                "Zhuoer Wen",
                "Wenting Liu",
                "Zhuoran Zhang",
                "Xinlong Chen",
                "Bohan Zeng",
                "Sihan Yang",
                "Yuanxing Zhang",
                "Pengfei Wan",
                "Haotian Wang",
                "Wenjing Yang"
            ],
            "affiliations": [
                "CASIA",
                "CUHKSZ",
                "Kuaishou",
                "NTU",
                "PKU",
                "THU",
                "XJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21333.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#benchmark",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Ограничения мультимодальных моделей в задаче OCR на видео",
                    "desc": "Мультимодальные большие языковые модели (MLLM) показывают невысокую точность в задаче оптического распознавания символов (OCR) на видео из-за размытия при движении, временных вариаций и визуальных эффектов. Авторы представляют бенчмарк MME-VideoOCR, включающий 10 категорий задач и 25 отдельных заданий для оценки возможностей MLLM в видео OCR. Эксперименты на 18 современных MLLM выявили ограничения в пространственно-временном рассуждении и языковых предубеждениях моделей. Исследование подчеркивает важность высокого разрешения и достаточного временного охвата для надежного OCR в динамических видеосценариях."
                },
                "en": {
                    "title": "Enhancing Video OCR: Bridging the Gap in MLLM Performance",
                    "desc": "This paper discusses the challenges faced by Multimodal Large Language Models (MLLMs) in performing Optical Character Recognition (OCR) on videos. It highlights that factors like motion blur and temporal variations significantly reduce their accuracy compared to static images. To address these issues, the authors introduce the MME-VideoOCR benchmark, which includes a variety of tasks designed to test spatio-temporal reasoning and language understanding in video contexts. The evaluation of 18 MLLMs reveals that even the best models struggle with comprehensive video comprehension, particularly in scenarios requiring integration of information across multiple frames."
                },
                "zh": {
                    "title": "提升视频OCR的多模态基准挑战",
                    "desc": "多模态大型语言模型（MLLMs）在静态图像的光学字符识别（OCR）中表现良好，但在视频OCR中效果显著下降。这是由于视频内容中的运动模糊、时间变化和视觉效果等因素影响。为了解决这些问题，我们提出了MME-VideoOCR基准，涵盖了多种视频OCR应用场景，包含10个任务类别和25个具体任务。我们的研究表明，现有的MLLMs在处理需要整体视频理解的任务时能力有限，尤其是在时空推理和跨帧信息整合方面。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21297",
            "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset",
            "url": "https://huggingface.co/papers/2505.21297",
            "abstract": "A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar.",
            "score": 18,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "1fdb1a9edd20c73b",
            "authors": [
                "Yifei Liu",
                "Li Lyna Zhang",
                "Yi Zhu",
                "Bingcheng Dong",
                "Xudong Zhou",
                "Ning Shang",
                "Fan Yang",
                "Mao Yang"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Microsoft Research Asia",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21297.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "rStar-Coder: прорыв в обучении языковых моделей рассуждениям о коде",
                    "desc": "Исследователи представили rStar-Coder - крупномасштабный датасет для улучшения способностей языковых моделей (LLM) в рассуждениях о коде. Датасет содержит 418 тысяч задач по программированию соревновательного уровня, 580 тысяч подробных решений и тестовые примеры различной сложности. Использование rStar-Coder значительно повысило производительность моделей Qwen на различных бенчмарках, позволив им достичь результатов, сравнимых с передовыми LLM для рассуждений о коде, но при гораздо меньших размерах моделей. На бенчмарке LiveCodeBench модель Qwen2.5-14B улучшила свой результат с 23.3% до 62.5% после обучения на rStar-Coder."
                },
                "en": {
                    "title": "Unlocking Code Reasoning with rStar-Coder",
                    "desc": "The paper presents rStar-Coder, a large-scale dataset designed to enhance code reasoning capabilities in large language models (LLMs). It addresses the challenge of limited high-difficulty datasets by providing 418,000 verified code problems and 580,000 long-reasoning solutions, complete with diverse test cases. The dataset is created through a three-step process that includes curating competitive programming problems, synthesizing input-output test cases, and verifying solutions. Experiments show that models trained on rStar-Coder significantly outperform existing benchmarks, demonstrating its effectiveness in improving code reasoning tasks."
                },
                "zh": {
                    "title": "提升代码推理能力的rStar-Coder数据集",
                    "desc": "rStar-Coder是一个大规模的数据集，旨在提升大语言模型（LLMs）在代码推理方面的能力。该数据集包含418,000个竞争级别的代码问题和580,000个长推理解决方案，配备丰富的测试用例，涵盖不同难度。通过三项核心贡献，rStar-Coder提供了可验证的输入输出测试案例，确保了解决方案的有效性。实验结果显示，使用rStar-Coder的数据集，Qwen模型在多个代码推理基准测试中表现优异，显著提高了模型的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20292",
            "title": "OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for\n  Subject-to-Video Generation",
            "url": "https://huggingface.co/papers/2505.20292",
            "abstract": "Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and (ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, to accurately align human preferences with S2V benchmarks, we propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to separately quantify subject consistency, naturalness, and text relevance in generated videos. Building on this, we conduct a comprehensive evaluation of 16 representative S2V models, highlighting their strengths and weaknesses across different content. Moreover, we create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality 720P subject-text-video triples. Specifically, we ensure subject-information diversity in our dataset by (1) segmenting subjects and building pairing information via cross-video associations and (2) prompting GPT-Image-1 on raw frames to synthesize multi-view representations. Through OpenS2V-Nexus, we deliver a robust infrastructure to accelerate future S2V generation research.",
            "score": 16,
            "issue_id": 3990,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "e2a8d12789199cde",
            "authors": [
                "Shenghai Yuan",
                "Xianyi He",
                "Yufan Deng",
                "Yang Ye",
                "Jinfa Huang",
                "Bin Lin",
                "Chongyang Ma",
                "Jiebo Luo",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Rabbitpre AI",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20292.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "OpenS2V-Nexus: Революция в генерации видео на основе заданного содержания",
                    "desc": "Статья представляет OpenS2V-Nexus - инфраструктуру для генерации видео на основе заданного содержания (Subject-to-Video, S2V). Она включает в себя OpenS2V-Eval - детальный бенчмарк для оценки качества генерируемых видео, и OpenS2V-5M - крупномасштабный датасет из 5 миллионов триплетов субъект-текст-видео. Авторы предлагают три автоматические метрики для оценки согласованности субъекта, естественности и релевантности текста в сгенерированных видео. Проведена комплексная оценка 16 репрезентативных S2V моделей, выявляющая их сильные и слабые стороны."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Subject Fidelity",
                    "desc": "The paper introduces Subject-to-Video (S2V) generation, which focuses on creating videos that accurately reflect reference content. It presents OpenS2V-Nexus, a framework that includes OpenS2V-Eval, a detailed benchmark for evaluating video generation, and OpenS2V-5M, a large dataset of five million subject-text-video pairs. Unlike previous benchmarks, OpenS2V-Eval emphasizes the generation of videos that maintain subject consistency and natural appearance. The authors also propose three new metrics to assess generated videos based on subject fidelity, naturalness, and relevance to the input text, facilitating a comprehensive evaluation of various S2V models."
                },
                "zh": {
                    "title": "构建视频生成的新基准与数据集",
                    "desc": "本论文提出了Subject-to-Video (S2V) 生成的基础设施OpenS2V-Nexus，旨在创建忠实于参考内容的视频。我们引入了OpenS2V-Eval，一个细粒度的基准，专注于生成具有自然外观和身份保真度的一致视频。为了评估生成视频的质量，我们设计了三种自动化指标，分别量化主题一致性、自然性和文本相关性。最后，我们创建了一个包含五百万个高质量720P主题-文本-视频三元组的开放源代码数据集OpenS2V-5M，以支持未来的S2V生成研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18943",
            "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems",
            "url": "https://huggingface.co/papers/2505.18943",
            "abstract": "MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) a Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.",
            "score": 16,
            "issue_id": 3990,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 мая",
                "en": "May 25",
                "zh": "5月25日"
            },
            "hash": "718f1062d34a47a7",
            "authors": [
                "Xuanming Zhang",
                "Yuxuan Chen",
                "Min-Hsuan Yeh",
                "Yixuan Li"
            ],
            "affiliations": [
                "Tsinghua University",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18943.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#agents",
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MetaMind: Искусственный интеллект с человеческим социальным пониманием",
                    "desc": "MetaMind - это многоагентная система, улучшающая способность больших языковых моделей выполнять задачи теории сознания. Она разбивает социальное понимание на генерацию гипотез, их уточнение и генерацию ответов. MetaMind достигает уровня человека в ключевых задачах теории сознания, показывая улучшение на 35.7% в реальных социальных сценариях. Система использует три агента: агент теории сознания, доменный агент и агент ответов, что позволяет балансировать контекстуальную правдоподобность, социальную уместность и адаптацию к пользователю."
                },
                "en": {
                    "title": "Empowering AI with Human-like Social Intelligence",
                    "desc": "MetaMind is a multi-agent framework that enhances large language models (LLMs) by improving their ability to understand human social interactions through Theory of Mind (ToM) tasks. It breaks down social understanding into three key stages: generating hypotheses about mental states, refining these hypotheses with cultural and ethical considerations, and producing contextually appropriate responses. This approach allows LLMs to achieve human-like performance in social reasoning, showing significant improvements in real-world scenarios and ToM reasoning tasks. The framework demonstrates the importance of each component in achieving a balance between contextual relevance and social appropriateness, paving the way for more empathetic AI interactions."
                },
                "zh": {
                    "title": "MetaMind：提升AI的社会智能",
                    "desc": "MetaMind是一个多智能体框架，灵感来源于元认知，旨在提升大型语言模型（LLMs）在心智理论任务中的表现。该框架将社会理解分解为三个协作阶段：首先，心智理论代理生成用户心理状态的假设；其次，领域代理利用文化规范和伦理约束来细化这些假设；最后，响应代理生成符合上下文的适当回应。通过这种方式，MetaMind在三个具有挑战性的基准测试中实现了最先进的性能，首次使LLMs在关键的心智理论任务上达到人类水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18875",
            "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via\n  Semantic-Aware Permutation",
            "url": "https://huggingface.co/papers/2505.18875",
            "abstract": "SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30x and 1.89x speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively.",
            "score": 16,
            "issue_id": 3990,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 мая",
                "en": "May 24",
                "zh": "5月24日"
            },
            "hash": "bc68e232d8897ad4",
            "authors": [
                "Shuo Yang",
                "Haocheng Xi",
                "Yilong Zhao",
                "Muyang Li",
                "Jintao Zhang",
                "Han Cai",
                "Yujun Lin",
                "Xiuyu Li",
                "Chenfeng Xu",
                "Kelly Peng",
                "Jianfei Chen",
                "Song Han",
                "Kurt Keutzer",
                "Ion Stoica"
            ],
            "affiliations": [
                "MIT",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18875.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#video",
                    "#optimization"
                ],
                "emoji": "🎞️",
                "ru": {
                    "title": "Семантическая оптимизация для быстрой и качественной генерации видео",
                    "desc": "SVG2 - это фреймворк для улучшения эффективности и качества генерации видео без дополнительного обучения. Он использует семантически-ориентированную перестановку для точной идентификации и обработки критических токенов. SVG2 применяет кластеризацию k-means для группировки токенов по семантическому сходству, что повышает точность представления. Фреймворк также включает динамический контроль бюджета top-p и оптимизированные ядра, достигая ускорения до 2.30x при сохранении высокого качества генерации."
                },
                "en": {
                    "title": "Maximizing Video Generation Efficiency with SVG2",
                    "desc": "SVG2 is a novel framework designed to improve the efficiency and quality of video generation without the need for extensive training. It focuses on accurately identifying critical tokens through semantic-aware permutation, which groups tokens based on their meanings rather than just their positions. This method reduces computational waste by ensuring that critical tokens are processed together, optimizing GPU usage. By implementing dynamic budget control, SVG2 achieves significant speed improvements while maintaining high video quality, demonstrating a balance between performance and resource efficiency."
                },
                "zh": {
                    "title": "SVG2：提升视频生成效率与质量的创新框架",
                    "desc": "SVG2是一个无需训练的框架，通过准确识别和处理关键标记，提升视频生成的效率和质量。它采用语义感知的排列和动态预算控制，解决了现有方法在计算预算下生成质量不佳的问题。SVG2通过k-means聚类和重新排列标记，确保了精确的聚类表示，从而提高了识别准确性，并减少了计算浪费。该框架在保持生成质量的同时，实现了高达2.30倍的加速。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16459",
            "title": "MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks",
            "url": "https://huggingface.co/papers/2505.16459",
            "abstract": "The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems.",
            "score": 14,
            "issue_id": 3991,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "d18f80036a817d7c",
            "authors": [
                "Guiyao Tie",
                "Xueyang Zhou",
                "Tianhe Gu",
                "Ruihang Zhang",
                "Chaoran Hu",
                "Sizhe Zhang",
                "Mengqu Sun",
                "Yan Zhang",
                "Pan Zhou",
                "Lichao Sun"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Lehigh University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16459.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MMMR: Новый стандарт оценки мультимодального мышления ИИ",
                    "desc": "Статья представляет новый бенчмарк MMMR для оценки мультимодального рассуждения в крупных языковых моделях (MLLM). MMMR включает набор данных из 1083 сложных вопросов, охватывающих шесть типов рассуждений, и конвейер оценки качества рассуждений (RTEP). Результаты показывают, что MLLM с промежуточными этапами мышления превосходят модели без них, но даже лучшие модели страдают от несогласованности и чрезмерного анализа. Бенчмарк выявляет разрыв между точностью и качеством рассуждений, предоставляя основу для улучшения мультимодальных систем рассуждений."
                },
                "en": {
                    "title": "Evaluating Multi-Modal Reasoning: The MMMR Benchmark",
                    "desc": "The MMMR benchmark is designed to evaluate the reasoning abilities of Multi-Modal Large Language Models (MLLMs) by focusing on their thinking quality across various reasoning types. It includes a challenging dataset with 1,083 questions that require complex reasoning, and a modular evaluation pipeline to assess reasoning quality beyond just accuracy. The study finds that while MLLMs with intermediate thinking traces perform better than those without, they still exhibit issues like inconsistency and overthinking. This benchmark aims to bridge the gap between accuracy and reasoning quality, providing a structured approach for future advancements in multi-modal reasoning systems."
                },
                "zh": {
                    "title": "多模态推理的新基准：MMMR",
                    "desc": "MMMR基准测试评估多模态大语言模型（MLLMs）的推理能力，重点在于通过多样的推理类型和模块化评估流程来评估思维质量。尽管MLLMs在语言、视觉和结构化输入的统一处理上取得了进展，但其推理能力仍然不够清晰，缺乏标准化的评估基准。MMMR包含一个高难度的数据集和一个推理追踪评估管道，旨在超越准确性评估，关注推理的相关性、一致性和结构化错误注释。通过实证结果，MMMR揭示了准确性与推理质量之间的差距，为未来模型的发展提供了可操作的评估框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21505",
            "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective",
            "url": "https://huggingface.co/papers/2505.21505",
            "abstract": "The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual output transformation, and vocabulary space outputting.  \t\t\t\t\tAI-generated summary \t\t\t\t Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs.",
            "score": 10,
            "issue_id": 3994,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "b77701cf90420b4c",
            "authors": [
                "Shimao Zhang",
                "Zhejian Lai",
                "Xiang Liu",
                "Shuaijie She",
                "Xiao Liu",
                "Yeyun Gong",
                "Shujian Huang",
                "Jiajun Chen"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "National Key Laboratory for Novel Software Technology, Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21505.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#alignment"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Раскрытие тайн многоязычности в нейронах LLM",
                    "desc": "Исследование предлагает алгоритм более точной идентификации нейронов для обнаружения языково-специфичных и языково-агностических нейронов в больших языковых моделях (LLM). Авторы анализируют влияние этого подхода на многоязычное выравнивание и возможности моделей. Исследование рассматривает многоязычное понимание, общее семантическое рассуждение, многоязычное преобразование выходных данных и вывод в пространстве словаря. Работа предоставляет эмпирические результаты и ценные выводы для лучшего понимания многоязычного выравнивания и многоязычных возможностей LLM."
                },
                "en": {
                    "title": "Enhancing Multilingual Capabilities in LLMs through Neuron Identification",
                    "desc": "This research introduces a new algorithm for identifying neurons in large language models (LLMs) that are specific to certain languages as well as those that are language-agnostic. It explores how these neurons contribute to the model's ability to understand and generate text in multiple languages, enhancing multilingual alignment. The study categorizes the internal processes of LLMs into four key areas: understanding multiple languages, reasoning in a shared semantic space, transforming outputs across languages, and managing vocabulary. By analyzing the behavior of different types of neurons, the research provides insights into how LLMs can better support low-resource languages through learned multilingual capabilities."
                },
                "zh": {
                    "title": "细粒度神经元识别，提升多语言能力",
                    "desc": "本研究提出了一种更细粒度的神经元识别算法，用于检测大型语言模型（LLMs）中的语言特定神经元和语言无关神经元。我们分析了多语言理解、共享语义推理、多语言输出转换和词汇空间输出等方面对多语言对齐和能力的影响。研究表明，存在在处理不同语言时选择性激活的语言特定神经元，这为深入理解LLMs在多语言场景中的机制提供了新视角。通过对不同类型神经元的系统分析，我们为更好地理解LLMs的多语言对齐和能力提供了实证结果和有价值的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21496",
            "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\n  Mobile GUI Agents",
            "url": "https://huggingface.co/papers/2505.21496",
            "abstract": "In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie.",
            "score": 9,
            "issue_id": 3993,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "db6e0d226a2c500e",
            "authors": [
                "Han Xiao",
                "Guozhi Wang",
                "Yuxiang Chai",
                "Zimu Lu",
                "Weifeng Lin",
                "Hao He",
                "Lue Fan",
                "Liuyang Bian",
                "Rui Hu",
                "Liang Liu",
                "Shuai Ren",
                "Yafei Wen",
                "Xiaoxin Chen",
                "Aojun Zhou",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "CUHK MMLab",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21496.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#synthetic",
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#training",
                    "#data"
                ],
                "emoji": "🧞",
                "ru": {
                    "title": "UI-Genie: самообучающийся ИИ-помощник для графических интерфейсов",
                    "desc": "UI-Genie - это самосовершенствующаяся система для агентов графического интерфейса, решающая проблемы верификации результатов и масштабирования качественных обучающих данных. Система включает модель вознаграждения UI-Genie-RM с архитектурой, объединяющей изображения и текст, и конвейер самосовершенствования для расширения решаемых задач. Для обучения модели созданы наборы данных UI-Genie-RM-517k и UI-Genie-Agent-16k без ручной разметки. Экспериментальные результаты показывают, что UI-Genie достигает наилучших показателей в нескольких эталонных тестах для агентов графического интерфейса."
                },
                "en": {
                    "title": "UI-Genie: Revolutionizing GUI Agents with Self-Improvement and Reward Models",
                    "desc": "This paper presents UI-Genie, a framework designed to improve GUI agents by tackling the challenges of verifying outcomes and scaling high-quality training data. It introduces a reward model, UI-Genie-RM, which uses an image-text interleaved architecture to effectively process historical data and combine different levels of rewards. The framework also includes innovative data generation strategies to create training data without manual effort, such as rule-based verification and hard negative mining. Experimental results indicate that UI-Genie outperforms existing methods in GUI agent tasks, showcasing the effectiveness of its self-improvement approach."
                },
                "zh": {
                    "title": "UI-Genie：自我改进的GUI代理框架",
                    "desc": "本文介绍了UI-Genie，这是一个自我改进的框架，旨在解决GUI代理中的两个主要挑战：轨迹结果的验证困难和高质量训练数据的可扩展性。我们通过奖励模型和自我改进管道来解决这些问题。奖励模型UI-Genie-RM采用图像-文本交错架构，有效处理历史上下文，并统一了动作级和任务级奖励。自我改进管道通过奖励引导探索和动态环境中的结果验证，逐步扩展可解决的复杂GUI任务，从而提升代理和奖励模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21457",
            "title": "Active-O3: Empowering Multimodal Large Language Models with Active\n  Perception via GRPO",
            "url": "https://huggingface.co/papers/2505.21457",
            "abstract": "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.",
            "score": 8,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "6d338aff50466f43",
            "authors": [
                "Muzhi Zhu",
                "Hao Zhong",
                "Canyu Zhao",
                "Zongze Du",
                "Zheng Huang",
                "Mingyu Liu",
                "Hao Chen",
                "Cheng Zou",
                "Jingdong Chen",
                "Ming Yang",
                "Chunhua Shen"
            ],
            "affiliations": [
                "Ant Group, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21457.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "ACTIVE-O3: Наделение MLLM активным восприятием для эффективного принятия решений",
                    "desc": "Статья представляет ACTIVE-O3 - фреймворк обучения с подкреплением для наделения мультимодальных больших языковых моделей (MLLM) возможностями активного восприятия. Авторы определяют задачи активного восприятия для MLLM и создают комплексный набор тестов для оценки ACTIVE-O3 в различных сценариях. Фреймворк демонстрирует улучшенную эффективность поиска и точность выбора регионов по сравнению с предыдущими подходами. ACTIVE-O3 также показывает сильные способности к рассуждениям с нулевым обучением на эталонном тесте V*."
                },
                "en": {
                    "title": "Empowering MLLMs with Active Perception for Smarter Decision-Making",
                    "desc": "This paper introduces ACTIVE-O3, a reinforcement learning framework designed to enhance Multimodal Large Language Models (MLLMs) with active perception capabilities. Active perception involves strategically selecting where to focus attention to gather relevant information, which is crucial for effective decision-making in robotics. The authors highlight the limitations of the existing GPT-o3 model in terms of search efficiency and region selection accuracy. By establishing a benchmark suite for evaluating ACTIVE-O3, the paper aims to advance research in active perception for MLLMs, demonstrating strong performance in various tasks without needing explicit reasoning data."
                },
                "zh": {
                    "title": "提升机器人主动感知能力的创新框架",
                    "desc": "主动视觉，也称为主动感知，是指主动选择观察的方式和位置，以获取与任务相关的信息。本文探讨了多模态大型语言模型（MLLMs）在机器人系统中的主动感知能力，提出了一种基于强化学习的训练框架ACTIVE-O3。我们定义了MLLMs的主动感知任务，并指出现有模型在搜索效率和区域选择上存在不足。通过建立综合基准测试套件，ACTIVE-O3在多个任务中展示了强大的零-shot推理能力，推动了主动感知的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20275",
            "title": "ImgEdit: A Unified Image Editing Dataset and Benchmark",
            "url": "https://huggingface.co/papers/2505.20275",
            "abstract": "Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce ImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2 million carefully curated edit pairs, which contain both novel and complex single-turn edits, as well as challenging multi-turn tasks. To ensure the data quality, we employ a multi-stage pipeline that integrates a cutting-edge vision-language model, a detection model, a segmentation model, alongside task-specific in-painting procedures and strict post-processing. ImgEdit surpasses existing datasets in both task novelty and data quality. Using ImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to process the reference image and editing prompt, which outperforms existing open-source models on multiple tasks, highlighting the value of ImgEdit and model design. For comprehensive evaluation, we introduce ImgEdit-Bench, a benchmark designed to evaluate image editing performance in terms of instruction adherence, editing quality, and detail preservation. It includes a basic testsuite, a challenging single-turn suite, and a dedicated multi-turn suite. We evaluate both open-source and proprietary models, as well as ImgEdit-E1, providing deep analysis and actionable insights into the current behavior of image-editing models. The source data are publicly available on https://github.com/PKU-YuanGroup/ImgEdit.",
            "score": 8,
            "issue_id": 3990,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "ab1a7940b7d4c31c",
            "authors": [
                "Yang Ye",
                "Xianyi He",
                "Zongjian Li",
                "Bin Lin",
                "Shenghai Yuan",
                "Zhiyuan Yan",
                "Bohan Hou",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Peng Cheng Laboratory",
                "Rabbitpre AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20275.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#optimization",
                    "#cv",
                    "#data"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "ImgEdit: прорыв в редактировании изображений с помощью ИИ",
                    "desc": "Исследователи представили ImgEdit - крупномасштабный набор данных для редактирования изображений, содержащий 1,2 миллиона тщательно отобранных пар изображений до и после редактирования. На основе этого набора данных была обучена модель ImgEdit-E1, использующая мультимодальную языковую модель для обработки изображений и текстовых инструкций. Авторы также разработали бенчмарк ImgEdit-Bench для оценки моделей редактирования изображений. Результаты показывают, что ImgEdit-E1 превосходит существующие открытые модели по нескольким задачам редактирования."
                },
                "en": {
                    "title": "Empowering Open-Source Image Editing with ImgEdit Dataset",
                    "desc": "This paper presents ImgEdit, a new dataset designed to improve open-source image-editing models by providing 1.2 million high-quality image edit pairs. The dataset includes both simple and complex editing tasks, ensuring a wide range of challenges for model training. To maintain high data quality, a multi-stage pipeline is used, incorporating advanced models for vision-language processing, detection, and segmentation. The authors also introduce ImgEdit-E1, an editing model that outperforms existing open-source models, and ImgEdit-Bench, a benchmark for evaluating image editing performance across various tasks."
                },
                "zh": {
                    "title": "ImgEdit：高质量图像编辑的突破",
                    "desc": "最近生成模型的进展使得高保真文本到图像的生成成为可能。然而，开源图像编辑模型仍然落后于专有模型，主要是由于高质量数据的缺乏和基准测试不足。为了解决这些问题，我们推出了ImgEdit，这是一个大规模的高质量图像编辑数据集，包含120万个精心策划的编辑对，涵盖新颖和复杂的单轮编辑以及具有挑战性的多轮任务。我们使用多阶段流程确保数据质量，整合了先进的视觉语言模型、检测模型、分割模型以及特定任务的修复程序和严格的后处理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21500",
            "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2505.21500",
            "abstract": "Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.",
            "score": 6,
            "issue_id": 3993,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "c8cec2407a7def0e",
            "authors": [
                "Dingming Li",
                "Hongxing Li",
                "Zixuan Wang",
                "Yuchen Yan",
                "Hang Zhang",
                "Siqi Chen",
                "Guiyang Hou",
                "Shengpei Jiang",
                "Wenqi Zhang",
                "Yongliang Shen",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "University of Electronic Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21500.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#reasoning",
                    "#3d",
                    "#games"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый рубеж в пространственном интеллекте моделей компьютерного зрения",
                    "desc": "Статья представляет новый бенчмарк ViewSpatial-Bench для оценки способностей моделей компьютерного зрения к пространственному рассуждению с разных точек зрения. Авторы выявили, что существующие vision-language модели хорошо справляются с эгоцентрическим пространственным мышлением, но испытывают трудности с аллоцентрическими задачами. Бенчмарк включает пять типов задач и использует автоматизированный 3D-конвейер аннотаций. Тонкая настройка моделей на многоракурсном пространственном датасете позволила улучшить общую производительность на 46.24%."
                },
                "en": {
                    "title": "Enhancing VLMs with Multi-Viewpoint Spatial Reasoning",
                    "desc": "This paper addresses the limitations of vision-language models (VLMs) in understanding spatial relationships from different viewpoints. It highlights that while VLMs perform well in egocentric spatial reasoning, they struggle with allocentric perspectives, which are essential for tasks requiring understanding from another entity's viewpoint. The authors introduce ViewSpatial-Bench, a new benchmark for evaluating multi-viewpoint spatial localization, along with a 3D annotation pipeline for accurate directional labeling. By fine-tuning VLMs on this dataset, they demonstrate a significant performance improvement, emphasizing the importance of 3D spatial modeling in enhancing VLMs' spatial reasoning capabilities."
                },
                "zh": {
                    "title": "提升视觉语言模型的空间推理能力",
                    "desc": "视觉语言模型（VLMs）在理解和推理视觉内容方面表现出色，但在需要跨视角理解和空间推理的任务中仍面临重大挑战。我们发现当前的VLMs主要在自我中心的空间推理上表现良好，但在需要采用其他实体的空间参考框架时，无法很好地推广到他心中心视角。为此，我们提出了ViewSpatial-Bench，这是第一个专门为多视角空间定位识别评估设计的综合基准，支持自动化的3D标注流程生成精确的方向标签。通过在我们的多视角空间数据集上微调VLMs，我们在各项任务上实现了46.24%的整体性能提升，证明了我们方法的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21491",
            "title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2505.21491",
            "abstract": "Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce a new dataset curated semi-automatically, a comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines.",
            "score": 6,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "cf3b062e968f421f",
            "authors": [
                "Boyang Wang",
                "Xuweiyi Chen",
                "Matheus Gadelha",
                "Zezhou Cheng"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21491.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#games",
                    "#diffusion",
                    "#architecture",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Управляемая генерация видео: новый уровень контроля над объектами в кадре",
                    "desc": "Статья посвящена улучшению контролируемости, временной согласованности и детализации в генерации видео. Авторы предлагают новый подход к технике Frame In и Frame Out, позволяющий пользователям управлять объектами, входящими в кадр или выходящими из него. Для решения этой задачи был создан новый датасет, разработан протокол оценки и предложена архитектура Diffusion Transformer для генерации видео с сохранением идентичности объектов. Результаты показывают значительное превосходство предложенного метода над существующими базовыми подходами."
                },
                "en": {
                    "title": "Mastering Video Generation with Motion Control",
                    "desc": "This paper addresses key challenges in video generation, specifically focusing on controllability, temporal coherence, and detail synthesis. It introduces a novel technique called Frame In and Frame Out, allowing users to manipulate objects in a video scene based on specified motion trajectories. The authors present a new dataset and evaluation protocol tailored for this task, along with a Diffusion Transformer architecture that preserves identity while enabling motion control. Results demonstrate that their method significantly improves upon existing video generation models."
                },
                "zh": {
                    "title": "提升视频生成的可控性与一致性",
                    "desc": "本论文关注视频生成中的可控性、时间一致性和细节合成等关键挑战。我们提出了一种名为“帧进帧出”的电影技术，允许用户控制图像中的对象自然地离开或进入场景。为支持这一任务，我们引入了一个半自动策划的新数据集，并制定了针对该设置的综合评估协议。我们的评估结果表明，所提出的方法在性能上显著优于现有基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19099",
            "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.19099",
            "abstract": "SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75\\%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\\% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.",
            "score": 6,
            "issue_id": 3991,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 мая",
                "en": "May 25",
                "zh": "5月25日"
            },
            "hash": "e7a5589d299cea0e",
            "authors": [
                "Kun Xiang",
                "Heng Li",
                "Terry Jingchen Zhang",
                "Yinya Huang",
                "Zirong Liu",
                "Peixin Qu",
                "Jixi He",
                "Jiaqi Chen",
                "Yu-Jie Yuan",
                "Jianhua Han",
                "Hang Xu",
                "Hanhui Li",
                "Mrinmaya Sachan",
                "Xiaodan Liang"
            ],
            "affiliations": [
                "ETH Zurich",
                "Huawei Noahs Ark Lab",
                "Sun Yat-sen University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19099.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#benchmark",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "SeePhys: выявление ограничений визуального мышления LLM в физике",
                    "desc": "SeePhys - это новый мультимодальный бенчмарк для оценки способностей больших языковых моделей (LLM) в области визуального мышления и решения задач по физике. Он охватывает 7 фундаментальных разделов физики и включает 21 категорию разнородных диаграмм. Бенчмарк содержит 75% задач, требующих обязательной интерпретации визуальной информации. Тестирование показало, что даже самые продвинутые модели визуального мышления достигают точности менее 60% на этом бенчмарке."
                },
                "en": {
                    "title": "SeePhys: Bridging Visual Reasoning and Physics in LLMs",
                    "desc": "SeePhys is a new benchmark designed to test how well large language models (LLMs) can solve physics problems that involve visual reasoning. It includes a wide range of questions, from middle school to PhD level, and features many different types of diagrams that are crucial for finding the right answers. Unlike previous benchmarks, SeePhys requires models to extract visual information for 75% of the problems, making it essential for them to interpret diagrams accurately. The results show that even the best models struggle with these tasks, highlighting significant gaps in their ability to connect visual data with physics reasoning without relying heavily on text."
                },
                "zh": {
                    "title": "SeePhys：挑战视觉推理与物理问题解决的基准",
                    "desc": "SeePhys是一个大型的多模态基准，旨在评估大型语言模型（LLM）在物理问题上的推理能力，涵盖从中学到博士资格考试的范围。该基准涉及物理学的7个基本领域，并包含21类高度异质的图表。与以往的研究不同，SeePhys中75%的问题需要提取视觉信息才能得出正确答案，显示出视觉信息在解决问题中的重要性。评估结果表明，即使是最先进的视觉推理模型，其准确率也未能超过60%，揭示了当前大型语言模型在视觉理解方面的根本挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20322",
            "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering\n  Target Atoms",
            "url": "https://huggingface.co/papers/2505.20322",
            "abstract": "Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.",
            "score": 6,
            "issue_id": 3990,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "63cd6df71ddaefa4",
            "authors": [
                "Mengru Wang",
                "Ziwen Xu",
                "Shengyu Mao",
                "Shumin Deng",
                "Zhaopeng Tu",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "NUS-NCS Joint Lab, Singapore",
                "National University of Singapore",
                "Tencent AI Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20322.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#security",
                    "#training",
                    "#alignment"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точное управление языковыми моделями через атомарные компоненты знаний",
                    "desc": "Статья представляет новый метод под названием Steering Target Atoms (STA) для точного контроля над генерацией языковых моделей. Метод использует разреженные автоэнкодеры для выделения атомарных компонентов знаний в высокоразмерных пространствах. Эксперименты показывают эффективность STA в повышении безопасности и надежности языковых моделей. Метод демонстрирует особую устойчивость и гибкость в сценариях состязательного машинного обучения."
                },
                "en": {
                    "title": "Enhancing Control in Language Models with Steering Target Atoms",
                    "desc": "This paper introduces a new method called Steering Target Atoms (STA) to improve the control over language model generation. It addresses the challenge of intertwined internal representations in large models, which can hinder precise steering and lead to unintended consequences. By isolating and manipulating specific knowledge components, STA enhances the safety and reliability of model outputs. The experiments show that this approach is effective, especially in adversarial situations, and it also improves reasoning control in large models."
                },
                "zh": {
                    "title": "提升语言模型安全性的创新方法",
                    "desc": "本文提出了一种新的方法，称为引导目标原子（STA），旨在提高语言模型生成的安全性和可靠性。通过使用稀疏自编码器（SAE），该方法能够分离和操控高维空间中的知识组件，从而增强对模型行为的控制。实验结果表明，STA在对抗性场景中表现出更强的鲁棒性和灵活性。此外，我们还将这一引导策略应用于大型推理模型，验证了其在精确推理控制中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21473",
            "title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via\n  Next-Detail Prediction",
            "url": "https://huggingface.co/papers/2505.21473",
            "abstract": "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing a more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose a parallel inference mechanism with self-correction that accelerates generation speed by approximately 8x while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2x faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlow's superior generation quality and efficiency compared to existing state-of-the-art methods.",
            "score": 4,
            "issue_id": 3992,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "d4df44fe3325795e",
            "authors": [
                "Yiheng Liu",
                "Liao Qu",
                "Huichao Zhang",
                "Xu Wang",
                "Yi Jiang",
                "Yiming Gao",
                "Hu Ye",
                "Xian Li",
                "Shuai Wang",
                "Daniel K. Du",
                "Shu Cheng",
                "Zehuan Yuan",
                "Xinglong Wu"
            ],
            "affiliations": [
                "ByteDance Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21473.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#training",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Эффективная генерация изображений от общего к частному",
                    "desc": "DetailFlow - это новый метод генерации изображений, использующий авторегрессионный подход с последовательным уточнением деталей. Модель обучается на токенизированных последовательностях изображений с прогрессивно ухудшающимся разрешением, что позволяет начинать генерацию с глобальной структуры и постепенно добавлять детали. Этот подход позволяет достичь высокого качества синтеза изображений при значительно меньшем количестве токенов по сравнению с предыдущими методами. Авторы также предлагают механизм параллельного вывода с самокоррекцией, который ускоряет генерацию примерно в 8 раз."
                },
                "en": {
                    "title": "DetailFlow: Efficient 1D Image Generation with Coarse-to-Fine Refinement",
                    "desc": "This paper introduces DetailFlow, a new method for generating images using a 1D autoregressive approach. It employs a next-detail prediction strategy that allows the model to start with a broad image structure and gradually add finer details. By using a resolution-aware token sequence and a parallel inference mechanism, DetailFlow significantly improves generation speed and reduces the number of tokens needed for high-quality image synthesis. The results show that DetailFlow outperforms existing models in both image quality and efficiency, achieving better performance with fewer resources."
                },
                "zh": {
                    "title": "DetailFlow：高效的自回归图像生成方法",
                    "desc": "本文提出了一种名为DetailFlow的粗到细的1D自回归图像生成方法，采用了一种新颖的下一个细节预测策略来建模图像。通过学习一个分辨率感知的标记序列，并使用逐步降级的图像进行监督，DetailFlow使生成过程能够从全局结构开始，逐步细化细节。该粗到细的1D标记序列与自回归推理机制很好地对齐，为自回归模型生成复杂视觉内容提供了一种更自然和高效的方法。实验结果表明，DetailFlow在图像合成质量和效率上优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21334",
            "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
            "url": "https://huggingface.co/papers/2505.21334",
            "abstract": "HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference.",
            "score": 2,
            "issue_id": 3994,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "b07c0c110f38f89c",
            "authors": [
                "Kele Shao",
                "Keda Tao",
                "Can Qin",
                "Haoxuan You",
                "Yang Sui",
                "Huan Wang"
            ],
            "affiliations": [
                "Columbia University",
                "Rice University",
                "Salesforce AI Research",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21334.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "HoliTom: Революционное сжатие токенов для эффективных видео-LLM",
                    "desc": "HoliTom - это новый фреймворк для эффективного сжатия токенов в видео-LLM моделях. Он сочетает внешнюю обрезку LLM через глобальную временную сегментацию с внутренним объединением токенов на основе сходства. Такой подход позволяет значительно снизить вычислительные затраты без ущерба для производительности. В результате удается сократить количество операций с плавающей запятой до 6,9% от исходного, сохранив 99,1% производительности."
                },
                "en": {
                    "title": "HoliTom: Efficient Video LLMs through Smart Token Pruning",
                    "desc": "HoliTom is a novel framework designed to enhance the efficiency of video large language models (LLMs) by reducing computational redundancy in video tokens. It combines outer-LLM pruning, which segments video data globally to identify and eliminate redundant tokens, with inner-LLM token merging based on similarity to further optimize performance. This dual approach allows for a significant reduction in visual tokens by over 90%, while still maintaining high performance levels, achieving 99.1% of the original output. The method also improves processing speed, reducing computational costs to just 6.9% of FLOPs and accelerating decoding throughput by 1.32 times, making it a practical solution for efficient video LLM inference."
                },
                "zh": {
                    "title": "HoliTom：高效视频LLM的全新剪枝策略",
                    "desc": "HoliTom是一种新颖的训练无关的整体令牌合并框架，旨在通过全球时间分割进行外部LLM剪枝，显著减少视频大语言模型（视频LLM）的计算效率问题。该方法结合了外部LLM剪枝和内部LLM基于令牌相似性的合并，能够在不牺牲性能的情况下，减少视觉令牌超过90%。通过这种方式，HoliTom有效缓解了LLM的计算负担，同时保持了99.1%的原始性能。评估结果显示，该方法在计算成本和解码速度上均有显著提升，展示了其在高效视频LLM推理中的实际应用价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21070",
            "title": "Minute-Long Videos with Dual Parallelisms",
            "url": "https://huggingface.co/papers/2505.21070",
            "abstract": "Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54times lower latency and 1.48times lower memory cost on 8timesRTX 4090 GPUs.",
            "score": 2,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "c777be11d4844462",
            "authors": [
                "Zeqing Wang",
                "Bowen Zheng",
                "Xingyi Yang",
                "Yuecong Xu",
                "Xinchao Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "National University of Singapore",
                "Xidian University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21070.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#video",
                    "#inference"
                ],
                "emoji": "🎞️",
                "ru": {
                    "title": "Ускорение генерации длинных видео с помощью распределенного вывода",
                    "desc": "Статья представляет новую стратегию распределенного вывода для видео-диффузионных моделей на основе Diffusion Transformer (DiT), называемую DualParal. Основная идея заключается в параллелизации как временных кадров, так и слоев модели между GPU для снижения задержки обработки и затрат памяти при генерации длинных видео. Авторы используют блочную схему шумоподавления и кэширование признаков для оптимизации производительности. Метод позволяет эффективно генерировать видео длиной 1025 кадров с до 6,54 раз меньшей задержкой и 1,48 раз меньшими затратами памяти на 8 GPU RTX 4090."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with DualPar Efficiency!",
                    "desc": "The paper presents a new method called DualPar for improving the efficiency of video generation using Diffusion Transformer (DiT) models. It addresses the high latency and memory costs associated with generating long videos by distributing the workload across multiple GPUs. The method uses a block-wise denoising approach to maintain synchronized noise levels while allowing for parallel processing of frames and model layers. Additionally, it introduces a feature cache and coordinated noise initialization to optimize performance, resulting in faster and more efficient video generation without artifacts."
                },
                "zh": {
                    "title": "高效生成无限长视频的创新策略",
                    "desc": "本论文提出了一种基于扩散变换器（DiT）的视频扩散模型，旨在解决长视频生成时的处理延迟和内存成本问题。我们提出了一种新的分布式推理策略，称为DualParal，通过在多个GPU上并行处理时间帧和模型层来提高效率。为了克服扩散模型对噪声水平同步的要求，我们采用了块级去噪方案，使得每个GPU处理特定的帧块和层，同时实现异步计算和通信。最终，我们的方法在生成高质量视频时显著降低了延迟和内存消耗，能够高效生成无限长的视频。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19314",
            "title": "SoloSpeech: Enhancing Intelligibility and Quality in Target Speech\n  Extraction through a Cascaded Generative Pipeline",
            "url": "https://huggingface.co/papers/2505.19314",
            "abstract": "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.",
            "score": 2,
            "issue_id": 3990,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 мая",
                "en": "May 25",
                "zh": "5月25日"
            },
            "hash": "d58bb66dfe2fc291",
            "authors": [
                "Helin Wang",
                "Jiarui Hai",
                "Dongchao Yang",
                "Chen Chen",
                "Kai Li",
                "Junyi Peng",
                "Thomas Thebaud",
                "Laureano Moro Velazquez",
                "Jesus Villalba",
                "Najim Dehak"
            ],
            "affiliations": [
                "Brno University of Technology",
                "Johns Hopkins University",
                "Nanyang Technological University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19314.jpg",
            "data": {
                "categories": [
                    "#audio"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "SoloSpeech: генеративное извлечение целевой речи нового поколения",
                    "desc": "SoloSpeech - это новый генеративный подход к извлечению целевой речи из смеси голосов. Он использует каскадный пайплайн, включающий сжатие, извлечение, реконструкцию и коррекцию. Ключевая особенность - экстрактор целевой речи, работающий без эмбеддингов говорящего и использующий латентное пространство вспомогательного аудио. SoloSpeech достигает наилучших результатов по разборчивости и качеству на датасете Libri2Mix, демонстрируя отличную обобщающую способность."
                },
                "en": {
                    "title": "SoloSpeech: Revolutionizing Target Speech Extraction with Generative Techniques",
                    "desc": "This paper introduces SoloSpeech, a new approach for Target Speech Extraction (TSE) that effectively isolates a target speaker's voice from a mix of multiple speakers. Unlike traditional discriminative models that can produce artifacts and lack naturalness, SoloSpeech employs a cascaded generative pipeline that includes processes for compression, extraction, reconstruction, and correction. It utilizes a speaker-embedding-free target extractor that aligns the latent spaces of cue audio and mixture audio, enhancing performance and reducing discrepancies. Evaluated on the Libri2Mix dataset, SoloSpeech sets a new benchmark for intelligibility and quality in TSE, showing strong generalization capabilities in diverse real-world scenarios."
                },
                "zh": {
                    "title": "SoloSpeech：提升目标语音提取的新方法",
                    "desc": "目标语音提取（TSE）旨在从多个说话者的混合音频中分离出目标说话者的声音，通常利用作为辅助音频的说话者特征线索。尽管近期的TSE进展主要采用了高感知质量的判别模型，但这些模型常常引入不必要的伪影，降低自然性，并对训练和测试环境之间的差异敏感。另一方面，生成模型在感知质量和可懂性方面滞后。为了解决这些问题，我们提出了SoloSpeech，这是一种新颖的级联生成管道，集成了压缩、提取、重建和修正过程，能够在目标语音提取和语音分离任务中实现新的最先进的可懂性和质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21205",
            "title": "Sci-Fi: Symmetric Constraint for Frame Inbetweening",
            "url": "https://huggingface.co/papers/2505.21205",
            "abstract": "Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably can't make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines.",
            "score": 1,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "a20a9eb43b42208f",
            "authors": [
                "Liuhan Chen",
                "Xiaodong Cun",
                "Xiaoyu Li",
                "Xianyi He",
                "Shenghai Yuan",
                "Jie Chen",
                "Ying Shan",
                "Li Yuan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "GVC Lab, Great Bay University",
                "Rabbitpre Intelligence",
                "Shenzhen Graduate School, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21205.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#video",
                    "#training"
                ],
                "emoji": "🎞️",
                "ru": {
                    "title": "Симметричное внедрение граничных кадров для улучшенного синтеза видео",
                    "desc": "Статья представляет новый подход к синтезу промежуточных видеокадров между заданными начальным и конечным кадрами. Авторы предлагают фреймворк Sci-Fi, который решает проблему асимметричного влияния начального и конечного кадров в существующих методах, основанных на диффузионных моделях преобразования изображения в видео (I2V-DM). Sci-Fi вводит специальный модуль EF-Net для более эффективного внедрения информации о конечном кадре. Эксперименты показывают превосходство предложенного метода над существующими подходами в создании плавных и согласованных переходов между кадрами."
                },
                "en": {
                    "title": "Achieving Harmony in Video Frame Synthesis with Sci-Fi",
                    "desc": "This paper presents a new approach called Sci-Fi for generating intermediate video sequences from given start and end frames. The authors identify a limitation in existing methods that treat the start and end frame constraints equally, which can lead to poor video quality. Sci-Fi introduces a novel mechanism using a lightweight module, EF-Net, to enhance the influence of the end frame, ensuring it has a similar impact as the start frame. The results show that Sci-Fi produces smoother and more consistent transitions in generated videos compared to current state-of-the-art techniques."
                },
                "zh": {
                    "title": "实现起始帧与结束帧的对称约束",
                    "desc": "本文提出了一种新的框架，称为Sci-Fi，用于在给定的起始帧和结束帧之间合成中间视频序列。现有的方法主要依赖于大型预训练的图像到视频扩散模型（I2V-DMs），但在引入结束帧约束时存在设计缺陷。我们发现，简单地使用与起始帧相同的机制来引入结束帧约束，可能无法有效影响中间内容，从而导致生成帧的运动不一致或外观崩溃。Sci-Fi通过引入一种改进的机制和轻量级模块EF-Net，使结束帧约束的影响力与起始帧相当，从而实现更和谐的过渡效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20561",
            "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.20561",
            "abstract": "BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL.",
            "score": 1,
            "issue_id": 3994,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "4b7b0470932e5c49",
            "authors": [
                "Shenao Zhang",
                "Yaqing Wang",
                "Yinxiao Liu",
                "Tianqi Liu",
                "Peter Grabowski",
                "Eugene Ie",
                "Zhaoran Wang",
                "Yunxuan Li"
            ],
            "affiliations": [
                "Google",
                "Google DeepMind",
                "Northwestern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20561.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "BARL: Умнее исследуем, эффективнее рассуждаем",
                    "desc": "BARL - это новая система Байесовского адаптивного обучения с подкреплением для улучшения работы больших языковых моделей. Она интегрирует рефлексивное рассуждение и эффективное исследование, что приводит к лучшей эффективности использования токенов. BARL инструктирует языковую модель переключать стратегии на основе наблюдаемых результатов. Эмпирические результаты показывают, что BARL превосходит стандартные марковские подходы обучения с подкреплением при тестировании."
                },
                "en": {
                    "title": "Enhancing LLMs with Reflective Exploration through BARL",
                    "desc": "The paper introduces BARL, a Bayes-Adaptive Reinforcement Learning framework designed to improve the performance of Large Language Models (LLMs) by incorporating reflective reasoning and efficient exploration strategies. Traditional Markovian RL methods limit exploration to the training phase and rely solely on current state information, which may hinder the emergence of reflective reasoning during training. BARL addresses this limitation by optimizing expected returns using a Bayesian approach, allowing the model to adaptively explore and exploit based on updated beliefs about the environment. Empirical results show that BARL significantly enhances token efficiency and effectiveness in reasoning tasks compared to standard Markovian RL methods."
                },
                "zh": {
                    "title": "BARL：提升LLM性能的贝叶斯自适应强化学习框架",
                    "desc": "BARL是一种贝叶斯自适应强化学习框架，通过整合反思推理和高效探索，提升了大型语言模型（LLM）的性能。传统的马尔可夫强化学习限制了探索过程，仅依赖当前状态的历史上下文来学习最优策略，导致反思推理的出现和其在测试时的好处不明确。BARL框架通过优化马尔可夫决策过程的后验分布，鼓励模型在奖励最大化和信息收集之间进行平衡。实验结果表明，BARL在合成和数学推理任务中优于传统的马尔可夫强化学习方法，展现出更高的令牌效率和更有效的探索能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20289",
            "title": "VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual\n  Tool Selection",
            "url": "https://huggingface.co/papers/2505.20289",
            "abstract": "VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dynamically explore, select, and combine tools from a diverse library based on empirical performance. Existing methods for tool-augmented reasoning either rely on training-free prompting or large-scale fine-tuning; both lack active tool exploration and typically assume limited tool diversity, and fine-tuning methods additionally demand extensive human supervision. In contrast, VisTA leverages end-to-end reinforcement learning to iteratively refine sophisticated, query-specific tool selection strategies, using task outcomes as feedback signals. Through Group Relative Policy Optimization (GRPO), our framework enables an agent to autonomously discover effective tool-selection pathways without requiring explicit reasoning supervision. Experiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. These results highlight VisTA's ability to enhance generalization, adaptively utilize diverse tools, and pave the way for flexible, experience-driven visual reasoning systems.",
            "score": 1,
            "issue_id": 3991,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "06184525a85012f4",
            "authors": [
                "Zeyi Huang",
                "Yuyang Ji",
                "Anirudh Sundara Rajan",
                "Zefan Cai",
                "Wen Xiao",
                "Junjie Hu",
                "Yong Jae Lee"
            ],
            "affiliations": [
                "Microsoft",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20289.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#benchmark",
                    "#optimization",
                    "#cv",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VisTA: Автономное улучшение визуального мышления с помощью обучения с подкреплением",
                    "desc": "VisTA - это новая система обучения с подкреплением для улучшения визуального мышления. Она позволяет агентам автономно выбирать и комбинировать инструменты из разнообразной библиотеки на основе эмпирической производительности. В отличие от существующих методов, VisTA использует сквозное обучение с подкреплением для итеративного улучшения стратегий выбора инструментов. Эксперименты показывают, что VisTA достигает значительных улучшений производительности по сравнению с базовыми моделями, особенно на примерах вне распределения обучающей выборки."
                },
                "en": {
                    "title": "Empowering Visual Agents with Autonomous Tool Selection",
                    "desc": "VisTA is a reinforcement learning framework designed to improve visual reasoning by allowing agents to autonomously select and combine tools from a diverse library. Unlike traditional methods that either require extensive human supervision or lack active exploration, VisTA uses end-to-end reinforcement learning to refine tool selection strategies based on task outcomes. The framework employs Group Relative Policy Optimization (GRPO) to enable agents to discover effective pathways for tool selection without explicit reasoning guidance. Experiments show that VisTA significantly outperforms existing methods, particularly in challenging scenarios, demonstrating its potential for enhancing generalization and adaptability in visual reasoning tasks."
                },
                "zh": {
                    "title": "VisTA：自主选择工具的视觉推理新框架",
                    "desc": "VisTA是一个强化学习框架，旨在通过自主选择和组合多样化工具来增强视觉推理能力。与现有方法不同，VisTA不依赖于训练前提示或大规模微调，而是通过端到端的强化学习来优化工具选择策略。该框架利用任务结果作为反馈信号，允许智能体自主发现有效的工具选择路径。实验结果表明，VisTA在多个基准测试中表现优异，特别是在处理分布外示例时，显示出其在增强泛化能力和灵活利用多样化工具方面的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19973",
            "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in\n  Digital Forensics and Incident Response",
            "url": "https://huggingface.co/papers/2505.19973",
            "abstract": "DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.",
            "score": 1,
            "issue_id": 3994,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "c4a0e825312e2a47",
            "authors": [
                "Bilel Cherif",
                "Tamas Bisztray",
                "Richard A. Dubniczky",
                "Aaesha Aldahmani",
                "Saeed Alshehhi",
                "Norbert Tihanyi"
            ],
            "affiliations": [
                "Eötvös Loránd University, Budapest, Hungary",
                "Technology Innovation Institute, Abu Dhabi, UAE",
                "University of Oslo, Oslo, Norway"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19973.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#hallucinations",
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Комплексная оценка языковых моделей для цифровой криминалистики",
                    "desc": "DFIR-Metric - это комплексный инструмент для оценки больших языковых моделей (LLM) в области цифровой криминалистики. Он включает тесты на знания, реалистичные криминалистические задачи и практические случаи анализа. Benchmark оценивает 14 LLM по точности и согласованности результатов. Авторы также вводят новую метрику - Task Understanding Score (TUS), для оценки моделей в сценариях с почти нулевой точностью."
                },
                "en": {
                    "title": "Evaluating AI in Digital Forensics: The DFIR-Metric Benchmark",
                    "desc": "The paper introduces DFIR-Metric, a benchmark designed to evaluate Large Language Models (LLMs) in the field of Digital Forensics and Incident Response (DFIR). It consists of three main components: a Knowledge Assessment with expert-reviewed questions, Realistic Forensic Challenges that test reasoning and evidence correlation, and Practical Analysis using real forensic cases. The study also presents a new metric called the Task Understanding Score (TUS) to assess model performance in low-accuracy situations. By evaluating 14 LLMs, this benchmark aims to provide a reliable framework for improving AI applications in digital forensics."
                },
                "zh": {
                    "title": "DFIR-Metric：数字取证中的语言模型评估新标准",
                    "desc": "DFIR-Metric 是一个评估大型语言模型在数字取证领域表现的基准工具。它包含三个主要部分：知识评估、现实取证挑战和实际分析案例，旨在全面测试模型的能力。通过对 14 个大型语言模型的评估，研究者分析了它们在准确性和一致性方面的表现。新引入的任务理解分数（TUS）可以更有效地评估模型在接近零准确率的场景中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17813",
            "title": "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.17813",
            "abstract": "Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive \"thinking\" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer \"thinking\" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.",
            "score": 1,
            "issue_id": 3992,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "d8d9d938b84c5ea6",
            "authors": [
                "Michael Hassid",
                "Gabriel Synnaeve",
                "Yossi Adi",
                "Roy Schwartz"
            ],
            "affiliations": [
                "FAIR Team, Meta",
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17813.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Короче мысль - быстрее вывод: оптимизация рассуждений в LLM",
                    "desc": "Статья исследует эффективность коротких цепочек рассуждений в крупных языковых моделях (LLM) для задач рассуждения. Авторы предлагают метод short-m@k, который выполняет параллельные генерации и выбирает ответ голосованием среди первых m завершенных процессов. Эксперименты показывают, что короткие цепочки часто дают более точные результаты, чем длинные, при меньших вычислительных затратах. Дополнительно, авторы обнаружили, что обучение на коротких цепочках рассуждений приводит к лучшей производительности модели."
                },
                "en": {
                    "title": "Shorter Chains, Smarter Reasoning!",
                    "desc": "This paper investigates the effectiveness of reasoning in large language models (LLMs) by comparing long and short reasoning chains. The authors find that shorter reasoning chains can yield significantly more accurate answers, with improvements of up to 34.5% compared to longer chains. They introduce a new method called short-m@k, which allows for parallel processing of multiple reasoning chains and selects the final answer based on majority voting. Their results indicate that shorter reasoning processes not only enhance performance but also reduce computational costs and inference time, challenging the traditional belief that longer reasoning leads to better outcomes."
                },
                "zh": {
                    "title": "短思维链，提升推理能力！",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在推理任务中依赖长思维链的假设。研究表明，较短的推理链在回答问题时更可能产生正确答案，准确率比最长链高出34.5%。基于此，提出了一种新的推理方法short-m@k，通过并行生成k个独立的思维过程，并在第一个m个完成后停止计算，最终答案通过多数投票选出。研究结果表明，短思维链的训练可以提高模型性能，挑战了长思维链必然带来更好推理能力的传统观念。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16901",
            "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks",
            "url": "https://huggingface.co/papers/2505.16901",
            "abstract": "Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.",
            "score": 1,
            "issue_id": 3994,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "a91f0c74f1c3c191",
            "authors": [
                "Hongyuan Tao",
                "Ying Zhang",
                "Zhenhao Tang",
                "Hongen Peng",
                "Xukun Zhu",
                "Bingchang Liu",
                "Yingguang Yang",
                "Ziyin Zhang",
                "Zhaogui Xu",
                "Haipeng Zhang",
                "Linchao Zhu",
                "Rui Wang",
                "Hang Yu",
                "Jianguo Li",
                "Peng Di"
            ],
            "affiliations": [
                "Ant Group, Hangzhou, China",
                "Shanghai Jiaotong University, Shanghai, China",
                "ShanghaiTech University, Shanghai, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16901.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#games",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Графовые модели кода: новый уровень генерации в открытых ИИ-системах",
                    "desc": "Статья представляет Code Graph Models (CGM) - новый подход к генерации кода на уровне репозитория. CGM интегрируют структуры графов кода в механизм внимания языковых моделей, улучшая понимание функций и файлов в кодовой базе. Эта техника позволяет открытым языковым моделям эффективно решать задачи на уровне репозитория без использования агентных подходов. В сочетании с безагентным фреймворком graph RAG, метод достигает высоких результатов на бенчмарке SWE-bench Lite, превосходя предыдущие открытые модели."
                },
                "en": {
                    "title": "Empowering Code Generation with Open-Source Graph Models",
                    "desc": "This paper presents a novel approach to enhance repository-level code generation tasks using open-source Code Graph Models (CGMs). By integrating code graph structures into the attention mechanisms of Large Language Models (LLMs), the authors demonstrate that these models can effectively understand the relationships and dependencies within codebases. This method eliminates the need for agent-based solutions, which often compromise data privacy and customization. The results show a significant improvement in performance, achieving a top ranking among open-source models on the SWE-bench Lite benchmark."
                },
                "zh": {
                    "title": "开源代码图模型提升代码生成性能",
                    "desc": "本论文提出了一种开源代码图模型（Code Graph Models, CGMs），旨在提升代码生成任务的性能。通过将代码图结构整合到大型语言模型（LLMs）的注意力机制中，CGMs能够更好地理解代码库中的函数和文件。与依赖代理的传统方法不同，我们的方法不需要代理，确保了数据隐私和模型定制的灵活性。实验结果表明，使用开源Qwen2.5-72B模型，我们的方法在SWE-bench Lite基准测试中达到了43.00%的解决率，表现优于其他开源模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16673",
            "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large\n  Language Models via Share-GRPO",
            "url": "https://huggingface.co/papers/2505.16673",
            "abstract": "Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.",
            "score": 1,
            "issue_id": 3994,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "e90d190149bd97ed",
            "authors": [
                "Huanjin Yao",
                "Qixiang Yin",
                "Jingyi Zhang",
                "Min Yang",
                "Yibo Wang",
                "Wenhao Wu",
                "Fei Su",
                "Li Shen",
                "Minghui Qiu",
                "Dacheng Tao",
                "Jiaxing Huang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "ByteDance",
                "Nanyang Technological University",
                "The University of Sydney",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16673.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление рассуждений MLLM через разделяемое обучение с подкреплением",
                    "desc": "Share-GRPO - это новый подход в области обучения с подкреплением, который улучшает мультимодальные большие языковые модели (MLLM). Метод расширяет пространство вопросов, позволяя исследовать разнообразные траектории рассуждений. Share-GRPO использует иерархическое вычисление преимуществ для более точной оценки относительных выгод. Эксперименты на шести широко используемых тестах показали превосходную производительность этого метода."
                },
                "en": {
                    "title": "Enhancing MLLMs with Share-GRPO: Expanding Questions and Sharing Reasoning",
                    "desc": "This paper introduces Share-GRPO, a new reinforcement learning method designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). It addresses challenges like sparse rewards and advantage vanishing by expanding the question space and sharing diverse reasoning paths. The approach encourages MLLMs to explore various reasoning trajectories and share insights across different question variants. By hierarchically computing advantages, Share-GRPO enhances the stability of policy training and demonstrates superior performance on multiple reasoning benchmarks."
                },
                "zh": {
                    "title": "Share-GRPO：提升多模态语言模型推理能力的新方法",
                    "desc": "本文提出了一种新颖的强化学习方法Share-GRPO，旨在增强多模态大型语言模型（MLLM）的推理能力。通过扩展问题空间和共享多样的推理轨迹，Share-GRPO有效地解决了强化学习中的稀疏奖励和优势消失问题。该方法首先通过数据转换技术扩展给定问题的空间，然后鼓励MLLM在扩展的问题空间中探索多样的推理轨迹，并在强化学习过程中共享这些轨迹。此外，Share-GRPO在优势计算中共享奖励信息，从而提高了相对优势的估计准确性，增强了策略训练的稳定性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21494",
            "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal\n  Alignment",
            "url": "https://huggingface.co/papers/2505.21494",
            "abstract": "Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.",
            "score": 0,
            "issue_id": 3992,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "9308405d203b4ba9",
            "authors": [
                "Xiaojun Jia",
                "Sensen Gao",
                "Simeng Qin",
                "Tianyu Pang",
                "Chao Du",
                "Yihao Huang",
                "Xinfeng Li",
                "Yiming Li",
                "Bo Li",
                "Yang Liu"
            ],
            "affiliations": [
                "MBZUAI, United Arab Emirates",
                "Nanyang Technological University, Singapore",
                "Sea AI Lab, Singapore",
                "University of Illinois Urbana-Champaign, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21494.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#security",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Усовершенствованная атака на мультимодальные языковые модели через оптимальное выравнивание признаков",
                    "desc": "Статья представляет новый метод атаки на мультимодальные языковые модели, называемый FOA-Attack. Он использует оптимальное выравнивание признаков на глобальном и локальном уровнях для улучшения переносимости состязательных примеров. Метод включает глобальное выравнивание с помощью косинусного сходства и локальное выравнивание с использованием кластеризации и оптимального транспорта. Также предлагается динамическая стратегия взвешивания ансамбля моделей для адаптивной балансировки влияния нескольких моделей при генерации состязательных примеров."
                },
                "en": {
                    "title": "Enhancing Adversarial Transferability with FOA-Attack",
                    "desc": "This paper addresses the vulnerability of multimodal large language models (MLLMs) to transferable adversarial examples. The authors introduce a new attack method called FOA-Attack, which focuses on aligning both global and local features to enhance the transferability of adversarial samples. By utilizing cosine similarity for global feature alignment and optimal transport for local feature alignment, the method improves the effectiveness of attacks on closed-source models. The proposed approach outperforms existing methods in various experiments, demonstrating its robustness in generating transferable adversarial examples."
                },
                "zh": {
                    "title": "提升对抗样本转移能力的FOA-Attack方法",
                    "desc": "多模态大型语言模型（MLLMs）容易受到可转移的对抗样本攻击。现有方法通常通过对齐全局特征来实现目标攻击，但忽视了局部信息的丰富性。为了解决这个问题，我们提出了一种基于特征最优对齐的针对性可转移对抗攻击方法，称为FOA-Attack。通过引入全局特征损失和局部聚类最优传输损失，我们显著提高了对抗样本的转移能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19377",
            "title": "Absolute Coordinates Make Motion Generation Easy",
            "url": "https://huggingface.co/papers/2505.19377",
            "abstract": "Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications.",
            "score": 0,
            "issue_id": 3994,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "d9415ccd47a86548",
            "authors": [
                "Zichong Meng",
                "Zeyu Han",
                "Xiaogang Peng",
                "Yiming Xie",
                "Huaizu Jiang"
            ],
            "affiliations": [
                "Northeastern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19377.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#games",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Глобальные координаты для улучшения генерации движений из текста",
                    "desc": "В статье предлагается новый подход к генерации движений на основе текста, использующий абсолютные координаты суставов в глобальном пространстве. Этот метод обеспечивает более высокую точность движений, улучшенное соответствие тексту и хорошую масштабируемость даже с простой архитектурой Transformer. Подход также поддерживает задачи управления движением на основе текста и редактирования без дополнительной доработки. Авторы демонстрируют возможность прямой генерации вершин меша SMPL-H в движении из текста."
                },
                "en": {
                    "title": "Revolutionizing Text-to-Motion with Global Coordinates",
                    "desc": "This paper introduces a new approach to text-to-motion generation by using absolute joint coordinates in global space instead of the traditional local-relative motion representation. This change enhances motion fidelity, improves text alignment, and allows for better scalability, even when using a simple Transformer model. The authors demonstrate that their method supports various downstream tasks without needing complex reengineering or additional guidance. Overall, this work lays a solid foundation for future advancements in motion generation and related applications."
                },
                "zh": {
                    "title": "全局绝对坐标提升文本到运动生成的效果",
                    "desc": "本文提出了一种新的文本到运动生成方法，使用全局空间中的绝对关节坐标来提高运动的真实感、文本对齐和可扩展性。传统的运动表示方法依赖于相对运动，虽然简化了训练过程，但对扩散模型的应用造成了限制。通过系统分析，我们证明了这种新的表示方法在运动真实感和文本对齐方面显著提升，且能够支持下游任务。最终，我们展示了该方法在从文本直接生成运动的SMPL-H网格顶点方面的良好泛化能力，为未来的研究和运动相关应用奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16340",
            "title": "Improving Chemical Understanding of LLMs via SMILES Parsing",
            "url": "https://huggingface.co/papers/2505.16340",
            "abstract": "Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark.",
            "score": 0,
            "issue_id": 3993,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 мая",
                "en": "May 22",
                "zh": "5月22日"
            },
            "hash": "22065ebe729018b8",
            "authors": [
                "Yunhui Jang",
                "Jaehyung Kim",
                "Sungsoo Ahn"
            ],
            "affiliations": [
                "KAIST",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16340.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#graphs",
                    "#science",
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "CLEANMOL: Улучшение понимания молекул языковыми моделями",
                    "desc": "Статья представляет CLEANMOL - новый фреймворк для улучшения понимания молекулярных структур большими языковыми моделями (LLM). CLEANMOL формулирует разбор SMILES-представлений молекул в виде набора детерминированных задач, специально разработанных для улучшения понимания графовой структуры молекул. Авторы создали датасет для предобучения с адаптивной оценкой сложности и предобучили открытые LLM на этих задачах. Результаты показывают, что CLEANMOL улучшает понимание структуры молекул и достигает лучших или сопоставимых результатов на бенчмарке Mol-Instructions."
                },
                "en": {
                    "title": "CLEANMOL: Enhancing LLMs for Molecular Understanding",
                    "desc": "This paper presents CLEANMOL, a new framework aimed at improving how large language models (LLMs) understand molecular structures represented in SMILES format. The authors identify that existing LLMs struggle with basic molecular tasks, such as counting rings in molecules. CLEANMOL addresses this by breaking down SMILES parsing into clear, structured tasks that enhance graph-level comprehension of molecular properties. The framework includes a pretraining dataset with varying difficulty levels, leading to improved performance on molecular understanding benchmarks."
                },
                "zh": {
                    "title": "CLEANMOL：提升分子结构理解的创新框架",
                    "desc": "大型语言模型（LLMs）在分子科学的科学发现中被越来越多地认可为强大的工具。为了使这些模型能够准确理解分子结构，我们提出了CLEANMOL框架，将SMILES解析转化为一系列清晰且确定的任务，以促进图级分子理解。这些任务包括子图匹配和全局图匹配，提供与分子结构特性相一致的结构化监督。我们的研究表明，CLEANMOL不仅增强了结构理解能力，还在Mol-Instructions基准测试中表现优异。"
                }
            }
        }
    ],
    "link_prev": "2025-05-27.html",
    "link_next": "2025-05-29.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "27.05",
        "en": "05/27",
        "zh": "5月27日"
    },
    "short_date_next": {
        "ru": "29.05",
        "en": "05/29",
        "zh": "5月29日"
    },
    "categories": {
        "#dataset": 9,
        "#data": 4,
        "#benchmark": 21,
        "#agents": 5,
        "#cv": 7,
        "#rl": 6,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 1,
        "#video": 9,
        "#multimodal": 14,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 13,
        "#robotics": 0,
        "#agi": 0,
        "#games": 5,
        "#interpretability": 1,
        "#reasoning": 16,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 15,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了大型语言模型和多模态语言模型的快速发展。以前主要通过增加参数数量来提高性能，但现在硬件限制使得自注意力成本成为主要瓶颈。文章提出研究重点应从模型压缩转向数据压缩，特别是令牌压缩。令牌压缩可以减少训练或推理过程中的令牌数量，从而提高AI效率。作者分析了长上下文AI的发展，提出了一个统一的数学框架，并探讨了令牌压缩的优势和挑战。",
        "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
        "pinyin": "这篇文章讨论了大型语言模型和多模态语言模型的快速发展。以前主要通过增加参数数量来提高性能，但现在硬件限制使得自注意力成本成为主要瓶颈。文章提出研究重点应从模型压缩转向数据压缩，特别是令牌压缩。令牌压缩可以减少训练或推理过程中的令牌数量，从而提高AI效率。作者分析了长上下文AI的发展，提出了一个统一的数学框架，并探讨了令牌压缩的优势和挑战。\n\nZhè piān wénzhāng tǎolùn le dàxíng yǔyán móxíng hé duō móshì yǔyán móxíng de kuàisù fāzhǎn. Yǐqián zhǔyào tōngguò zēngjiā cānshù shùliàng lái tígāo xìngnéng, dàn xiànzài yìngjiàn xiànzhì shǐdé zì zhùyìlì chéngběn chéngwéi zhǔyào píngtǐng. Wénzhāng tíchū yánjiū zhòngdiǎn yīng cóng móxíng yāsuō zhuǎnxiàng shùjù yāsuō, tèbié shì lìngpái yāsuō. Lìngpái yāsuō kěyǐ jiǎnshǎo xùnliàn huò tuīlǐ guòchéng zhōng de lìngpái shùliàng, cóng'ér tígāo AI xiàonéng. Zuòzhě fēnxi le cháng shàngxìawénfǎ AI de fāzhǎn, tíchū le yīgè tǒngyī de shùxué kuàngjià, bìng tàntǎo le lìngpái yāsuō de yōushì hé tiǎozhàn.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"大型\", \"pinyin\": \"dà xíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔ yán mó xíng\", \"trans\": \"language model\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"快速\", \"pinyin\": \"kuài sù\", \"trans\": \"rapid\"},\n    {\"word\": \"发展\", \"pinyin\": \"fā zhǎn\", \"trans\": \"development\"},\n    {\"word\": \"主要\", \"pinyin\": \"zhǔ yào\", \"trans\": \"main\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"增加\", \"pinyin\": \"zēng jiā\", \"trans\": \"increase\"},\n    {\"word\": \"参数\", \"pinyin\": \"cān shǔ\", \"trans\": \"parameter\"},\n    {\"word\": \"数量\", \"pinyin\": \"shù liàng\", \"trans\": \"quantity\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"硬件\", \"pinyin\": \"yìng jiàn\", \"trans\": \"hardware\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"自注意力\", \"pinyin\": \"zì zhù yì lì\", \"trans\": \"self-attention\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"瓶颈\", \"pinyin\": \"píng lóng\", \"trans\": \"bottleneck\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"重点\", \"pinyin\": \"zhòng diǎn\", \"trans\": \"focus\"},\n    {\"word\": \"模型压缩\", \"pinyin\": \"mó xíng yā suō\", \"trans\": \"model compression\"},\n    {\"word\": \"转向\", \"pinyin\": \"zhuǎn xiàng\", \"trans\": \"turn to\"},\n    {\"word\": \"数据压缩\", \"pinyin\": \"shù jù yā suō\", \"trans\": \"data compression\"},\n    {\"word\": \"令牌\", \"pinyin\": \"lìng pái\", \"trans\": \"token\"},\n    {\"word\": \"压缩\", \"pinyin\": \"yā suō\", \"trans\": \"compression\"},\n    {\"word\": \"减少\", \"pinyin\": \"jiǎn shǎo\", \"trans\": \"reduce\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"training\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"过程\", \"pinyin\": \"guò chéng\", \"trans\": \"process\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiào lǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"作者\", \"pinyin\": \"zuò zhě\", \"trans\": \"author\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēn xī\", \"trans\": \"analyze\"},\n    {\"word\": \"长上下文\", \"pinyin\": \"cháng shàng xià wén\", \"trans\": \"long context\"},\n    {\"word\": \"数学框架\", \"pinyin\": \"shù xué kuàng jià\", \"trans\": \"mathematical framework\"},\n    {\"word\": \"探讨\", \"pinyin\": \"tàn tǎo\", \"trans\": \"explore\"},\n    {\"word\": \"优势\", \"pinyin\": \"yōu shì\", \"trans\": \"advantage\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"}\n]",
        "trans": "This article discusses the rapid development of large language models and multimodal language models. Previously, performance was mainly improved by increasing the number of parameters, but now hardware limitations make the cost of self-attention a major bottleneck. The article proposes that the focus of research should shift from model compression to data compression, particularly token compression. Token compression can reduce the number of tokens during training or inference, thereby improving AI efficiency. The author analyzes the development of long-context AI, proposes a unified mathematical framework, and explores the advantages and challenges of token compression.",
        "update_ts": "2025-05-27 09:12"
    }
}