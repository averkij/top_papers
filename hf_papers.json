{
    "date": {
        "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 27",
        "zh": "8æœˆ27æ—¥"
    },
    "time_utc": "2025-08-27 08:15",
    "weekday": 2,
    "issue_id": 5568,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.18124",
            "title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics",
            "url": "https://huggingface.co/papers/2508.18124",
            "abstract": "CMPhysBench evaluates LLMs in condensed matter physics using calculation problems and a new SEED score for partial credit assessment, revealing significant capability gaps.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure a deep understanding of the problem-solving process,we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields a more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a significant capability gap, especially for this practical and frontier domain relative to traditional physics. The code anddataset are publicly available at https://github.com/CMPhysBench/CMPhysBench.",
            "score": 30,
            "issue_id": 5564,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "20823f1294d8217d",
            "authors": [
                "Weida Wang",
                "Dongchen Huang",
                "Jiatong Li",
                "Tengchao Yang",
                "Ziyang Zheng",
                "Di Zhang",
                "Dong Han",
                "Benteng Chen",
                "Binzhao Luo",
                "Zhiyu Liu",
                "Kunling Liu",
                "Zhiyuan Gao",
                "Shiqi Geng",
                "Wei Ma",
                "Jiaming Su",
                "Xin Li",
                "Shuchen Pu",
                "Yuhan Shui",
                "Qianjia Cheng",
                "Zhihao Dou",
                "Dongfei Cui",
                "Changyong He",
                "Jin Zeng",
                "Zeke Xie",
                "Mao Su",
                "Dongzhan Zhou",
                "Yuqiang Li",
                "Wanli Ouyang",
                "Yunqi Cai",
                "Xi Dai",
                "Shufei Zhang",
                "Lei Bai",
                "Jinguang Cheng",
                "Zhong Fang",
                "Hongming Weng"
            ],
            "affiliations": [
                "Beijing National Laboratory for Condensed Matter Physics and Institute of Physics, Chinese Academy of Sciences",
                "Condensed Matter Physics Data Center, Chinese Academy of Sciences",
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Shanghai Artificial Intelligence Laboratory",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18124.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#dataset"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜: Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ ĞºĞ¾Ğ½Ğ´ĞµĞ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ",
                    "desc": "CMPhysBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ ĞºĞ¾Ğ½Ğ´ĞµĞ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 520 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ°ÑĞ¿Ğ¸Ñ€Ğ°Ğ½Ñ‚ÑƒÑ€Ñ‹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° SEED Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Assessing LLMs in Condensed Matter Physics with CMPhysBench",
                    "desc": "CMPhysBench is a new benchmark designed to evaluate the performance of Large Language Models (LLMs) in the field of Condensed Matter Physics. It consists of over 520 carefully selected graduate-level questions that cover key topics such as magnetism and superconductivity. The benchmark focuses on calculation problems, requiring LLMs to produce detailed solutions independently. To assess their performance, a novel scoring method called the Scalable Expression Edit Distance (SEED) score is introduced, which allows for partial credit and provides a nuanced evaluation of the models' outputs, revealing significant gaps in their capabilities."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‡èšæ€ç‰©ç†ä¸­çš„èƒ½åŠ›å·®è·",
                    "desc": "CMPhysBenchæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å‡èšæ€ç‰©ç†å­¦ä¸­çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«520å¤šä¸ªç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„ç ”ç©¶ç”Ÿæ°´å¹³é—®é¢˜ï¼Œæ¶µç›–äº†å‡èšæ€ç‰©ç†çš„ä»£è¡¨æ€§å­é¢†åŸŸå’ŒåŸºç¡€ç†è®ºæ¡†æ¶ã€‚æˆ‘ä»¬ä¸“æ³¨äºè®¡ç®—é—®é¢˜ï¼Œè¦æ±‚LLMsç‹¬ç«‹ç”Ÿæˆå…¨é¢çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼•å…¥äº†å¯æ‰©å±•è¡¨è¾¾ç¼–è¾‘è·ç¦»ï¼ˆSEEDï¼‰åˆ†æ•°ï¼Œä»¥æä¾›æ›´ç²¾ç»†çš„éƒ¨åˆ†è¯„åˆ†ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹Grok-4ï¼Œåœ¨CMPhysBenchä¸Šçš„å¹³å‡SEEDåˆ†æ•°ä»…ä¸º36ï¼Œå‡†ç¡®ç‡ä¸º28%ï¼Œè¿™è¡¨æ˜åœ¨è¿™ä¸€å‰æ²¿é¢†åŸŸå­˜åœ¨æ˜¾è‘—çš„èƒ½åŠ›å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19205",
            "title": "VibeVoice Technical Report",
            "url": "https://huggingface.co/papers/2508.19205",
            "abstract": "VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models.",
            "score": 26,
            "issue_id": 5562,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "fc5dc3d2ea656b66",
            "authors": [
                "Zhiliang Peng",
                "Jianwei Yu",
                "Wenhui Wang",
                "Yaoyao Chang",
                "Yutao Sun",
                "Li Dong",
                "Yi Zhu",
                "Weijiang Xu",
                "Hangbo Bao",
                "Zehua Wang",
                "Shaohan Huang",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19205.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#long_context",
                    "#diffusion",
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "VibeVoice: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸",
                    "desc": "VibeVoice - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ€ĞµÑ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 90 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ñ€ĞµÑ‡Ğ¸ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ´Ğ¾ 4 Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Encodec, Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ VibeVoice ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 80 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ. VibeVoice Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "VibeVoice: Revolutionizing Multi-Speaker Speech Synthesis",
                    "desc": "VibeVoice is a new model that creates long speeches with multiple speakers using a technique called next-token diffusion. This method generates speech by predicting the next part of the audio in a smart way, making it efficient for continuous data. The model also introduces a special speech tokenizer that compresses data much better than existing models, allowing it to maintain high audio quality while being faster. With VibeVoice, users can generate up to 90 minutes of realistic multi-speaker conversations, outperforming other dialogue systems."
                },
                "zh": {
                    "title": "VibeVoiceï¼šé«˜æ•ˆåˆæˆå¤šè¯´è¯äººé•¿è¯­éŸ³çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "VibeVoiceæ˜¯ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œæ—¨åœ¨åˆæˆé•¿æ—¶é—´çš„å¤šè¯´è¯äººè¯­éŸ³ã€‚å®ƒé‡‡ç”¨äº†ä¸‹ä¸€æ­¥æ‰©æ•£çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªå›å½’ç”Ÿæˆæ½œåœ¨å‘é‡æ¥å»ºæ¨¡è¿ç»­æ•°æ®ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è¿ç»­è¯­éŸ³æ ‡è®°å™¨ï¼Œä¸æµè¡Œçš„Encodecæ¨¡å‹ç›¸æ¯”ï¼Œæ•°æ®å‹ç¼©æé«˜äº†80å€ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„æ€§èƒ½ã€‚VibeVoiceèƒ½å¤Ÿåˆæˆæœ€é•¿å¯è¾¾90åˆ†é’Ÿçš„è¯­éŸ³ï¼Œæ•æ‰çœŸå®çš„å¯¹è¯æ°›å›´ï¼Œè¶…è¶Šäº†å¼€æºå’Œä¸“æœ‰çš„å¯¹è¯æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17661",
            "title": "Spacer: Towards Engineered Scientific Inspiration",
            "url": "https://huggingface.co/papers/2508.17661",
            "abstract": "Spacer, a scientific discovery system, uses deliberate decontextualization to generate creative and factually grounded scientific concepts from keyword sets, achieving high accuracy and similarity to leading publications.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in LLMs have made automated scientific research the next frontline in the path to artificial superintelligence. However, these systems are bound either to tasks of narrow scope or the limited creative capabilities of LLMs. We propose Spacer, a scientific discovery system that develops creative and factually grounded concepts without external intervention. Spacer attempts to achieve this via 'deliberate decontextualization,' an approach that disassembles information into atomic units - keywords - and draws creativity from unexplored connections between them. Spacer consists of (i) Nuri, an inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline that refines these sets into elaborate scientific statements. Nuri extracts novel, high-potential keyword sets from a keyword graph built with 180,000 academic publications in biological fields. The Manifesting Pipeline finds links between keywords, analyzes their logical structure, validates their plausibility, and ultimately drafts original scientific concepts. According to our experiments, the evaluation metric of Nuri accurately classifies high-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline also successfully reconstructs core concepts from the latest top-journal articles solely from their keyword sets. An LLM-based scoring system estimates that this reconstruction was sound for over 85% of the cases. Finally, our embedding space analysis shows that outputs from Spacer are significantly more similar to leading publications compared with those from SOTA LLMs.",
            "score": 19,
            "issue_id": 5563,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "5ecb3211ea5e7d8d",
            "authors": [
                "Minhyeong Lee",
                "Suyoung Hwang",
                "Seunghyun Moon",
                "Geonho Nah",
                "Donghyun Koh",
                "Youngjun Cho",
                "Johyun Park",
                "Hojin Yoo",
                "Jiho Park",
                "Haneul Choi",
                "Sungbin Moon",
                "Taehoon Hwang",
                "Seungwon Kim",
                "Jaeyeong Kim",
                "Seongjun Kim",
                "Juneau Jung"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2508.17661.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#data",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Spacer: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ÑÑ…",
                    "desc": "Spacer - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½ÑƒÑ Ğ´ĞµĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Nuri, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ², Ğ¸ Manifesting Pipeline, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¸Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Spacer Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ»Ğ¸Ğ¶Ğµ Ğº Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Spacer: Unleashing Creativity in Scientific Discovery",
                    "desc": "Spacer is a scientific discovery system that generates innovative and accurate scientific concepts by using a method called deliberate decontextualization. This process breaks down information into basic units, or keywords, and explores new connections between them to foster creativity. The system includes Nuri, which creates keyword sets from a vast database of academic publications, and the Manifesting Pipeline, which refines these sets into coherent scientific statements. Experimental results show that Spacer's outputs are highly similar to top-tier publications, outperforming existing language models in terms of relevance and originality."
                },
                "zh": {
                    "title": "Spacerï¼šåˆ›æ–°ç§‘å­¦æ¦‚å¿µçš„å‘ç°ç³»ç»Ÿ",
                    "desc": "Spaceræ˜¯ä¸€ä¸ªç§‘å­¦å‘ç°ç³»ç»Ÿï¼Œé€šè¿‡æ•…æ„å»ä¸Šä¸‹æ–‡åŒ–çš„æ–¹æ³•ï¼Œä»å…³é”®è¯é›†åˆä¸­ç”Ÿæˆåˆ›é€ æ€§ä¸”äº‹å®åŸºç¡€çš„ç§‘å­¦æ¦‚å¿µã€‚è¯¥ç³»ç»Ÿå°†ä¿¡æ¯æ‹†è§£ä¸ºåŸå­å•ä½â€”â€”å…³é”®è¯ï¼Œå¹¶ä»å®ƒä»¬ä¹‹é—´æœªè¢«æ¢ç´¢çš„è”ç³»ä¸­æ±²å–åˆ›é€ åŠ›ã€‚SpaceråŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ï¼šNuriï¼Œä¸€ä¸ªçµæ„Ÿå¼•æ“ï¼Œç”¨äºæ„å»ºå…³é”®è¯é›†åˆï¼›ä»¥åŠManifesting Pipelineï¼Œç”¨äºå°†è¿™äº›é›†åˆç²¾ç‚¼æˆè¯¦ç»†çš„ç§‘å­¦é™ˆè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNurièƒ½å¤Ÿå‡†ç¡®åˆ†ç±»é«˜å½±å“åŠ›çš„å‡ºç‰ˆç‰©ï¼Œè€ŒManifesting Pipelineèƒ½å¤ŸæˆåŠŸé‡å»ºæœ€æ–°é¡¶çº§æœŸåˆŠæ–‡ç« çš„æ ¸å¿ƒæ¦‚å¿µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19209",
            "title": "OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive\n  Simulation",
            "url": "https://huggingface.co/papers/2508.19209",
            "abstract": "A framework using Multimodal Large Language Models and a specialized Multimodal DiT architecture generates semantically coherent and expressive character animations from multimodal inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing video avatar models can produce fluid human animations, yet they struggle to move beyond mere physical likeness to capture a character's authentic essence. Their motions typically synchronize with low-level cues like audio rhythm, lacking a deeper semantic understanding of emotion, intent, or context. To bridge this gap, we propose a framework designed to generate character animations that are not only physically plausible but also semantically coherent and expressive. Our model, OmniHuman-1.5, is built upon two key technical contributions. First, we leverage Multimodal Large Language Models to synthesize a structured textual representation of conditions that provides high-level semantic guidance. This guidance steers our motion generator beyond simplistic rhythmic synchronization, enabling the production of actions that are contextually and emotionally resonant. Second, to ensure the effective fusion of these multimodal inputs and mitigate inter-modality conflicts, we introduce a specialized Multimodal DiT architecture with a novel Pseudo Last Frame design. The synergy of these components allows our model to accurately interpret the joint semantics of audio, images, and text, thereby generating motions that are deeply coherent with the character, scene, and linguistic content. Extensive experiments demonstrate that our model achieves leading performance across a comprehensive set of metrics, including lip-sync accuracy, video quality, motion naturalness and semantic consistency with textual prompts. Furthermore, our approach shows remarkable extensibility to complex scenarios, such as those involving multi-person and non-human subjects. Homepage: https://omnihuman-lab.github.io/v1_5/",
            "score": 18,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "a23b1da7bec45638",
            "authors": [
                "Jianwen Jiang",
                "Weihong Zeng",
                "Zerong Zheng",
                "Jiaqi Yang",
                "Chao Liang",
                "Wang Liao",
                "Han Liang",
                "Yuan Zhang",
                "Mingyuan Gao"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19209.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#optimization",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OmniHuman-1.5 Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Multimodal LLM) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Multimodal DiT Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Pseudo Last Frame Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¾Ğ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°, ÑÑ†ĞµĞ½Ğ¾Ğ¹ Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ± Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Animating Characters with Emotion and Context",
                    "desc": "This paper presents a framework called OmniHuman-1.5 that enhances character animation by integrating Multimodal Large Language Models with a specialized Multimodal DiT architecture. Unlike traditional models that focus on physical likeness, this framework aims to create animations that reflect deeper emotional and contextual understanding. By synthesizing structured textual representations, the model guides motion generation to produce actions that resonate with the character's intent and the surrounding context. The results show significant improvements in lip-sync accuracy, video quality, and overall motion naturalness, making it adaptable for complex scenarios involving multiple characters or non-human entities."
                },
                "zh": {
                    "title": "ç”Ÿæˆå¯Œæœ‰è¡¨ç°åŠ›çš„è§’è‰²åŠ¨ç”»",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸“é—¨çš„å¤šæ¨¡æ€DiTæ¶æ„ï¼Œä»å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆè¯­ä¹‰è¿è´¯ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è§’è‰²åŠ¨ç”»ã€‚ç°æœ‰çš„è§†é¢‘å¤´åƒæ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆæµç•…çš„äººç±»åŠ¨ç”»ï¼Œä½†å¾€å¾€æ— æ³•æ•æ‰è§’è‰²çš„çœŸå®æœ¬è´¨ï¼ŒåŠ¨ä½œå¤šä¾èµ–äºä½çº§çº¿ç´¢å¦‚éŸ³é¢‘èŠ‚å¥ã€‚æˆ‘ä»¬çš„æ¨¡å‹OmniHuman-1.5é€šè¿‡åˆæˆç»“æ„åŒ–çš„æ–‡æœ¬è¡¨ç¤ºï¼Œæä¾›é«˜å±‚æ¬¡çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œä½¿å¾—åŠ¨ä½œç”Ÿæˆè¶…è¶Šç®€å•çš„èŠ‚å¥åŒæ­¥ï¼Œèƒ½å¤Ÿäº§ç”Ÿä¸ä¸Šä¸‹æ–‡å’Œæƒ…æ„Ÿç›¸ç¬¦çš„åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å”‡åŒæ­¥ç²¾åº¦ã€è§†é¢‘è´¨é‡ã€åŠ¨ä½œè‡ªç„¶æ€§å’Œä¸æ–‡æœ¬æç¤ºçš„è¯­ä¹‰ä¸€è‡´æ€§ç­‰å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å¤æ‚åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18756",
            "title": "UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior\n  Long-Context Learning",
            "url": "https://huggingface.co/papers/2508.18756",
            "abstract": "UltraMemV2, a redesigned memory-layer architecture, achieves performance parity with 8-expert MoE models while significantly reducing memory access costs.  \t\t\t\t\tAI-generated summary \t\t\t\t While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, they suffer from high memory access costs during inference. Memory-layer architectures offer an appealing alternative with very few memory access, but previous attempts like UltraMem have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2, a redesigned memory-layer architecture that closes this performance gap. Our approach introduces five key improvements: integrating memory layers into every transformer block, simplifying value expansion with single linear projections, adopting FFN-based value processing from PEER, implementing principled parameter initialization, and rebalancing memory-to-FFN computation ratios. Through extensive evaluation, we demonstrate that UltraMemV2 achieves performance parity with 8-expert MoE models under same computation and parameters but significantly low memory access. Notably, UltraMemV2 shows superior performance on memory-intensive tasks, with improvements of +1.6 points on long-context memorization, +6.2 points on multi-round memorization, and +7.9 points on in-context learning. We validate our approach at scale with models up to 2.5B activated parameters from 120B total parameters, and establish that activation density has greater impact on performance than total sparse parameter count. Our work brings memory-layer architectures to performance parity with state-of-the-art MoE models, presenting a compelling alternative for efficient sparse computation.",
            "score": 16,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "baef9d7d7f92b5b0",
            "authors": [
                "Zihao Huang",
                "Yu Bao",
                "Qiyang Min",
                "Siyan Chen",
                "Ran Guo",
                "Hongzhi Huang",
                "Defa Zhu",
                "Yutao Zeng",
                "Banggu Wu",
                "Xun Zhou",
                "Siyuan Qiao"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18756.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#long_context",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "UltraMemV2: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ MoE Ğ±ĞµĞ· Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ",
                    "desc": "UltraMemV2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ»Ğ¾Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 8-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture of Experts (MoE) Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². UltraMemV2 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ MoE Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "UltraMemV2: Bridging Memory Efficiency and Expert Performance",
                    "desc": "UltraMemV2 is a new memory-layer architecture designed to improve the efficiency of machine learning models by reducing memory access costs. It achieves performance similar to advanced 8-expert Mixture of Experts (MoE) models while using fewer resources. The architecture incorporates several enhancements, such as integrating memory layers into transformer blocks and optimizing value processing. This results in better performance on tasks that require extensive memory, making UltraMemV2 a strong contender for efficient sparse computation."
                },
                "zh": {
                    "title": "UltraMemV2ï¼šé«˜æ•ˆå†…å­˜å±‚æ¶æ„çš„çªç ´",
                    "desc": "UltraMemV2æ˜¯ä¸€ç§é‡æ–°è®¾è®¡çš„å†…å­˜å±‚æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨æ˜¾è‘—é™ä½å†…å­˜è®¿é—®æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°ä¸8ä¸“å®¶æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚è¯¥æ¶æ„é€šè¿‡å°†å†…å­˜å±‚é›†æˆåˆ°æ¯ä¸ªå˜æ¢å™¨å—ä¸­ã€ç®€åŒ–å€¼æ‰©å±•ã€é‡‡ç”¨åŸºäºå‰é¦ˆç¥ç»ç½‘ç»œçš„å€¼å¤„ç†ç­‰äº”ä¸ªå…³é”®æ”¹è¿›ï¼ŒæˆåŠŸç¼©å°äº†ä¸é«˜æ€§èƒ½MoEæ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒUltraMemV2åœ¨ç›¸åŒè®¡ç®—å’Œå‚æ•°ä¸‹ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜è®¿é—®ï¼ŒåŒæ—¶åœ¨å†…å­˜å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ¿€æ´»å¯†åº¦å¯¹æ€§èƒ½çš„å½±å“å¤§äºç¨€ç–å‚æ•°çš„æ€»æ•°ï¼Œå±•ç¤ºäº†å†…å­˜å±‚æ¶æ„åœ¨é«˜æ•ˆç¨€ç–è®¡ç®—ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17445",
            "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
            "url": "https://huggingface.co/papers/2508.17445",
            "abstract": "TreePO, a self-guided rollout algorithm for sequence generation, reduces computational cost and enhances exploration diversity in reinforcement learning for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\\% up to 43\\% of the sampling design for the trained models, meanwhile showing up to 40\\% reduction at trajectory-level and 35\\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO.",
            "score": 16,
            "issue_id": 5564,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 24",
                "zh": "8æœˆ24æ—¥"
            },
            "hash": "1425b1aed1182fe6",
            "authors": [
                "Yizhi Li",
                "Qingshui Gu",
                "Zhoufutu Wen",
                "Ziniu Li",
                "Tianshun Xing",
                "Shuyue Guo",
                "Tianyu Zheng",
                "Xin Zhou",
                "Xingwei Qu",
                "Wangchunshu Zhou",
                "Zheng Zhang",
                "Wei Shen",
                "Qian Liu",
                "Chenghua Lin",
                "Jian Yang",
                "Ge Zhang",
                "Wenhao Huang"
            ],
            "affiliations": [
                "ByteDance",
                "M-A-P",
                "UoM"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17445.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "TreePO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TreePO - Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. TreePO Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. TreePO Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ¾ 43%."
                },
                "en": {
                    "title": "TreePO: Efficient Exploration in Sequence Generation",
                    "desc": "TreePO is a novel self-guided rollout algorithm designed to improve sequence generation in reinforcement learning for large language models. It treats sequence generation as a tree search, allowing for better exploration of diverse reasoning paths while reducing computational costs. By utilizing dynamic tree sampling and fixed-length segment decoding, TreePO efficiently manages computation and enhances exploration through local uncertainty. The algorithm demonstrates significant efficiency gains, reducing GPU usage by up to 43% and improving performance on reasoning benchmarks."
                },
                "zh": {
                    "title": "TreePOï¼šé«˜æ•ˆçš„åºåˆ—ç”Ÿæˆä¸æ¢ç´¢å¤šæ ·æ€§",
                    "desc": "TreePOæ˜¯ä¸€ç§è‡ªæŒ‡å¯¼çš„å›æ»šç®—æ³•ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è®¡ç®—æ•ˆç‡å’Œæ¢ç´¢å¤šæ ·æ€§ã€‚å®ƒå°†åºåˆ—ç”Ÿæˆè§†ä¸ºæ ‘çŠ¶æœç´¢è¿‡ç¨‹ï¼Œé€šè¿‡åŠ¨æ€æ ‘é‡‡æ ·ç­–ç•¥å’Œå›ºå®šé•¿åº¦æ®µè§£ç æ¥å®ç°ã€‚TreePOåˆ©ç”¨å±€éƒ¨ä¸ç¡®å®šæ€§æ¥ç”Ÿæˆé¢å¤–çš„åˆ†æ”¯ï¼Œå¹¶é€šè¿‡åœ¨å…¬å…±å‰ç¼€ä¸Šåˆ†æ‘Šè®¡ç®—å’Œæå‰ä¿®å‰ªä½ä»·å€¼è·¯å¾„ï¼Œæ˜¾è‘—é™ä½æ¯æ¬¡æ›´æ–°çš„è®¡ç®—è´Ÿæ‹…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTreePOåœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒGPUè®¡ç®—æ•ˆç‡æé«˜äº†22%åˆ°43%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19247",
            "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
            "url": "https://huggingface.co/papers/2508.19247",
            "abstract": "VoxHammer is a training-free method that performs precise and coherent 3D editing in latent space, ensuring consistency in preserved regions and high-quality overall results.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.",
            "score": 9,
            "issue_id": 5562,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "1b1c0dd833778383",
            "authors": [
                "Lin Li",
                "Zehuan Huang",
                "Haoran Feng",
                "Gengxiong Zhuang",
                "Rui Chen",
                "Chunchao Guo",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "Renmin University of China",
                "Tencent Hunyuan",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19247.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#dataset",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ”¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "VoxHammer - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. VoxHammer Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "VoxHammer: Precision and Coherence in 3D Latent Space Editing",
                    "desc": "VoxHammer is a novel method for editing 3D models in latent space without the need for training. It focuses on maintaining the consistency of unedited regions while ensuring high-quality results in the edited areas. By predicting an inversion trajectory and utilizing key-value tokens, VoxHammer effectively integrates changes while preserving contextual features. This approach outperforms existing techniques, making it valuable for applications in the gaming industry and robotics."
                },
                "zh": {
                    "title": "VoxHammerï¼šæ— è®­ç»ƒçš„ç²¾ç¡®3Dç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "VoxHammeræ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç²¾ç¡®ä¸”è¿è´¯çš„3Dç¼–è¾‘ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†ä¿ç•™åŒºåŸŸçš„ä¸€è‡´æ€§å’Œæ•´ä½“ç»“æœçš„é«˜è´¨é‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒVoxHammerç›´æ¥åœ¨3Dæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç¼–è¾‘ï¼Œé¿å…äº†å¤šè§†å›¾å›¾åƒæ¸²æŸ“å’Œé‡å»ºæ¨¡å‹çš„å¤æ‚æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVoxHammeråœ¨ä¿ç•™åŒºåŸŸçš„3Dä¸€è‡´æ€§å’Œæ•´ä½“è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17437",
            "title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from\n  Pixels",
            "url": "https://huggingface.co/papers/2508.17437",
            "abstract": "PIXIE, a neural network method, predicts physical properties of 3D scenes from visual features, enabling fast and realistic physics simulation using supervised learning and pretrained visual features.  \t\t\t\t\tAI-generated summary \t\t\t\t Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/",
            "score": 9,
            "issue_id": 5563,
            "pub_date": "2025-08-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 20",
                "zh": "8æœˆ20æ—¥"
            },
            "hash": "113a80a24546f00c",
            "authors": [
                "Long Le",
                "Ryan Lucas",
                "Chen Wang",
                "Chuhao Chen",
                "Dinesh Jayaraman",
                "Eric Eaton",
                "Lingjie Liu"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17437.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#inference",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² 3D-ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹",
                    "desc": "PIXIE - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° 3D-ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. PIXIE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ° Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Fast and Realistic Physics Simulation with PIXIE",
                    "desc": "PIXIE is a neural network approach designed to predict the physical properties of 3D scenes from visual features, which enhances the speed and realism of physics simulations. Unlike traditional methods that require slow optimization for each scene, PIXIE uses supervised learning to create a generalizable model that can infer material characteristics across various environments. The model is trained on a large dataset called PIXIEVERSE, which includes 3D assets and their corresponding physical property annotations. By utilizing pretrained visual features, PIXIE can also apply its knowledge to real-world scenes, achieving significant improvements in performance and efficiency compared to existing methods."
                },
                "zh": {
                    "title": "PIXIEï¼šå¿«é€Ÿé¢„æµ‹ä¸‰ç»´åœºæ™¯ç‰©ç†å±æ€§çš„ç¥ç»ç½‘ç»œ",
                    "desc": "PIXIEæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œå¯ä»¥ä»è§†è§‰ç‰¹å¾ä¸­é¢„æµ‹ä¸‰ç»´åœºæ™¯çš„ç‰©ç†å±æ€§ï¼Œä»è€Œå®ç°å¿«é€Ÿè€ŒçœŸå®çš„ç‰©ç†æ¨¡æ‹Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡ç›‘ç£å­¦ä¹ å’Œé¢„è®­ç»ƒçš„è§†è§‰ç‰¹å¾ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ¯ä¸ªåœºæ™¯ä¸Šä¼˜åŒ–çš„æ…¢é€Ÿé™åˆ¶ã€‚PIXIEè®­ç»ƒå‡ºä¸€ä¸ªå¯æ³›åŒ–çš„ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªåœºæ™¯ä¸­é¢„æµ‹ç‰©ç†å±æ€§ï¼Œå¹¶ä¸”åœ¨æ¨ç†æ—¶é€Ÿåº¦æå¿«ã€‚é€šè¿‡ç»“åˆå­¦ä¹ åˆ°çš„é™æ€åœºæ™¯è¡¨ç¤ºï¼ŒPIXIEèƒ½å¤Ÿåœ¨å¤–åŠ›ä½œç”¨ä¸‹è¿›è¡ŒçœŸå®çš„ç‰©ç†æ¨¡æ‹Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19242",
            "title": "Autoregressive Universal Video Segmentation Model",
            "url": "https://huggingface.co/papers/2508.19242",
            "abstract": "AUSM, an autoregressive universal segmentation model, unifies prompted and unprompted video segmentation by treating it as sequential mask prediction, achieving superior performance and faster training on standard benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video foundation models such as SAM2 excel at prompted video segmentation by treating masks as a general-purpose primitive. However, many real-world settings require unprompted segmentation that aims to detect and track all objects in a video without external cues, leaving today's landscape fragmented across task-specific models and pipelines. We recast streaming video segmentation as sequential mask prediction, analogous to language modeling, and introduce the Autoregressive Universal Segmentation Model (AUSM), a single architecture that unifies both prompted and unprompted video segmentation. Built on recent state-space models, AUSM maintains a fixed-size spatial state and scales to video streams of arbitrary length. Furthermore, all components of AUSM are designed for parallel training across frames, yielding substantial speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS 2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior universal streaming video segmentation methods and achieves up to 2.5x faster training on 16-frame sequences.",
            "score": 6,
            "issue_id": 5564,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "d20ae3e4567983a7",
            "authors": [
                "Miran Heo",
                "Sukjun Hwang",
                "Min-Hung Chen",
                "Yu-Chiang Frank Wang",
                "Albert Gu",
                "Seon Joo Kim",
                "Ryo Hachiuma"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "NVIDIA",
                "National Taiwan University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19242.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#video",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑĞ¾Ğº",
                    "desc": "AUSM - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ…. ĞĞ½Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑĞ¾Ğº, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. AUSM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Unifying Video Segmentation with AUSM: Fast and Flexible!",
                    "desc": "The Autoregressive Universal Segmentation Model (AUSM) is a novel approach that combines prompted and unprompted video segmentation into a single framework. By treating video segmentation as sequential mask prediction, AUSM leverages techniques similar to language modeling, allowing it to effectively detect and track objects without needing external prompts. This model is built on state-space architectures, enabling it to handle video streams of any length while maintaining a fixed-size spatial state. AUSM also features parallel training across frames, resulting in significant speed improvements and superior performance on various standard benchmarks."
                },
                "zh": {
                    "title": "è‡ªå›å½’é€šç”¨åˆ†å‰²æ¨¡å‹ï¼šç»Ÿä¸€è§†é¢‘åˆ†å‰²çš„æœªæ¥",
                    "desc": "AUSMæ˜¯ä¸€ç§è‡ªå›å½’é€šç”¨åˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡å°†è§†é¢‘åˆ†å‰²è§†ä¸ºé¡ºåºæ©ç é¢„æµ‹ï¼Œç»Ÿä¸€äº†æœ‰æç¤ºå’Œæ— æç¤ºçš„è§†é¢‘åˆ†å‰²ã€‚è¯¥æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”è®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚AUSMåŸºäºæœ€æ–°çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„é•¿åº¦çš„è§†é¢‘æµï¼Œå¹¶ä¿æŒå›ºå®šå¤§å°çš„ç©ºé—´çŠ¶æ€ã€‚æ‰€æœ‰ç»„ä»¶éƒ½è®¾è®¡ä¸ºå¯ä»¥åœ¨å¸§ä¹‹é—´å¹¶è¡Œè®­ç»ƒï¼Œä»è€Œæ˜¾è‘—æé«˜è®­ç»ƒé€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18621",
            "title": "Wan-S2V: Audio-Driven Cinematic Video Generation",
            "url": "https://huggingface.co/papers/2508.18621",
            "abstract": "Wan-S2V, an audio-driven model built on Wan, enhances expressiveness and fidelity in cinematic character animation compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.",
            "score": 6,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "0e23abc799529ace",
            "authors": [
                "Xin Gao",
                "Li Hu",
                "Siqi Hu",
                "Mingyang Huang",
                "Chaonan Ji",
                "Dechao Meng",
                "Jinwei Qi",
                "Penchong Qiao",
                "Zhen Shen",
                "Yafei Song",
                "Ke Sun",
                "Linrui Tian",
                "Guangyuan Wang",
                "Qi Wang",
                "Zhongjian Wang",
                "Jiayu Xiao",
                "Sheng Xu",
                "Bang Zhang",
                "Peng Zhang",
                "Xindi Zhang",
                "Zhe Zhang",
                "Jingren Zhou",
                "Lian Zhuo"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18621.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#audio",
                    "#games",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Wan-S2V: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Wan-S2V, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞ»ĞµĞ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ½ÑĞ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞ»Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Wan-S2V Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Hunyuan-Avatar Ğ¸ Omnihuman. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ±."
                },
                "en": {
                    "title": "Elevating Cinematic Animation with Wan-S2V",
                    "desc": "The paper introduces Wan-S2V, an advanced audio-driven model designed to improve character animation in cinematic contexts. Unlike current state-of-the-art methods that excel mainly in speech and singing, Wan-S2V addresses the complexities of film and television, including intricate character interactions and realistic movements. Through rigorous benchmarking against leading models like Hunyuan-Avatar and Omnihuman, the results show that Wan-S2V significantly enhances expressiveness and fidelity. Furthermore, the model's versatility is highlighted through its applications in long-form video generation and accurate lip-sync editing."
                },
                "zh": {
                    "title": "æå‡ç”µå½±è§’è‰²åŠ¨ç”»çš„éŸ³é¢‘é©±åŠ¨æ¨¡å‹",
                    "desc": "Wan-S2Væ˜¯ä¸€ç§åŸºäºéŸ³é¢‘é©±åŠ¨çš„æ¨¡å‹ï¼Œæ—¨åœ¨æå‡ç”µå½±è§’è‰²åŠ¨ç”»çš„è¡¨ç°åŠ›å’ŒçœŸå®æ„Ÿã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å¤æ‚çš„å½±è§†åˆ¶ä½œä¸­è¡¨ç°æ›´ä½³ï¼Œèƒ½å¤Ÿå¤„ç†ç»†è…»çš„è§’è‰²äº’åŠ¨ã€çœŸå®çš„èº«ä½“åŠ¨ä½œå’ŒåŠ¨æ€çš„é•œå¤´è¿ä½œã€‚æˆ‘ä»¬é€šè¿‡ä¸æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜äº†Wan-S2Våœ¨åŠ¨ç”»æ•ˆæœä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å…·æœ‰åœ¨é•¿è§†é¢‘ç”Ÿæˆå’Œç²¾ç¡®è§†é¢‘å£å‹åŒæ­¥ç¼–è¾‘ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15774",
            "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
            "url": "https://huggingface.co/papers/2508.15774",
            "abstract": "CineScale is a novel inference paradigm that enables high-resolution visual generation for both images and videos without extensive fine-tuning, addressing issues of repetitive patterns and high-frequency information accumulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/.",
            "score": 6,
            "issue_id": 5563,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "520a6e202fa3979c",
            "authors": [
                "Haonan Qiu",
                "Ning Yu",
                "Ziqi Huang",
                "Paul Debevec",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Netflix Eyeline Studios",
                "Scanline VFX"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15774.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#inference",
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "CineScale: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "CineScale - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². CineScale Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 8K Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ 4K Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ LoRA."
                },
                "en": {
                    "title": "CineScale: Elevating Visual Generation to New Resolutions",
                    "desc": "CineScale is a new method for generating high-resolution images and videos without needing extensive adjustments to existing models. It addresses common problems like repetitive patterns and the buildup of high-frequency details that can occur when generating visuals at resolutions higher than those used during training. By introducing specialized versions of the model for different types of video generation, CineScale enhances the capabilities of pre-trained models, allowing for high-quality outputs. The results show that it can produce 8k images and 4k videos with minimal fine-tuning, significantly improving visual fidelity."
                },
                "zh": {
                    "title": "CineScaleï¼šé«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆçš„æ–°èŒƒå¼",
                    "desc": "CineScaleæ˜¯ä¸€ç§æ–°é¢–çš„æ¨ç†èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¤§é‡å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°é«˜åˆ†è¾¨ç‡çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚è¯¥æ–¹æ³•è§£å†³äº†ç”Ÿæˆå†…å®¹ä¸­é‡å¤æ¨¡å¼å’Œé«˜é¢‘ä¿¡æ¯ç§¯ç´¯çš„é—®é¢˜ï¼Œæå‡äº†è§†è§‰ç”Ÿæˆçš„è´¨é‡ã€‚é€šè¿‡ä¸“é—¨è®¾è®¡çš„å˜ä½“ï¼ŒCineScaleæ‰©å±•äº†é«˜åˆ†è¾¨ç‡å›¾åƒåˆ°è§†é¢‘çš„åˆæˆèƒ½åŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCineScaleåœ¨é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿç”Ÿæˆ8kå›¾åƒå’Œ4kè§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15804",
            "title": "ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks",
            "url": "https://huggingface.co/papers/2508.15804",
            "abstract": "ReportBench evaluates the content quality of research reports generated by large language models, focusing on cited literature quality and statement faithfulness, demonstrating that commercial Deep Research agents produce more comprehensive and reliable reports than standalone LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench",
            "score": 5,
            "issue_id": 5566,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 14",
                "zh": "8æœˆ14æ—¥"
            },
            "hash": "c52252190b40f5fb",
            "authors": [
                "Minghao Li",
                "Ying Zeng",
                "Zhihao Cheng",
                "Cong Ma",
                "Kai Jia"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15804.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#survey",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ReportBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "ReportBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ², ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ†Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°Ñ…. ReportBench Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ñ arXiv Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Deep Research Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹, Ñ‡ĞµĞ¼ standalone LLM, Ñ…Ğ¾Ñ‚Ñ Ğ²ÑĞµ ĞµÑ‰Ğµ ĞµÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Evaluating Research Quality in AI-Generated Reports",
                    "desc": "ReportBench is a new tool that assesses the quality of research reports created by large language models (LLMs). It focuses on two main aspects: the quality of the literature cited and the accuracy of the statements made in the reports. By using high-quality survey papers as references, ReportBench checks if the generated content is both comprehensive and trustworthy. The findings show that advanced Deep Research agents outperform standalone LLMs in producing reliable reports, but there is still a need for improvement in research coverage and factual accuracy."
                },
                "zh": {
                    "title": "è¯„ä¼°AIç”Ÿæˆç ”ç©¶æŠ¥å‘Šçš„è´¨é‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ReportBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»ŸåŒ–çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šçš„å†…å®¹è´¨é‡ã€‚è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šå¼•ç”¨æ–‡çŒ®çš„è´¨é‡å’Œç›¸å…³æ€§ï¼Œä»¥åŠç”ŸæˆæŠ¥å‘Šä¸­é™ˆè¿°çš„çœŸå®æ€§å’Œå¯é æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå•†ä¸šæ·±åº¦ç ”ç©¶ä»£ç†ï¼ˆå¦‚OpenAIå’ŒGoogleå¼€å‘çš„ï¼‰ç”Ÿæˆçš„æŠ¥å‘Šæ¯”ç‹¬ç«‹çš„è¯­è¨€æ¨¡å‹æ›´å…¨é¢å’Œå¯é ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç ”ç©¶è¦†ç›–çš„å¹¿åº¦å’Œæ·±åº¦ä»¥åŠäº‹å®ä¸€è‡´æ€§ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18773",
            "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large\n  Language Models",
            "url": "https://huggingface.co/papers/2508.18773",
            "abstract": "ThinkDial is an open-source framework that implements controllable reasoning in large language models through discrete operational modes, achieving performance while reducing computational effort.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI's gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with <10 percent performance degradation), and Low mode (75 percent token reduction with <15 percent performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks.",
            "score": 3,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "2f5e648d646cae5b",
            "authors": [
                "Qianyu He",
                "Siyu Yuan",
                "Xuefeng Li",
                "Mingxuan Wang",
                "Jiangjie Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Fudan University",
                "SIA-Lab of Tsinghua AIR",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18773.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#agi",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ThinkDial: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "ThinkDial - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€ĞµĞ¼Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼, ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼ (50% ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²) Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ (75% ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ). Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ThinkDial Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "ThinkDial: Control Your AI's Thinking Power!",
                    "desc": "ThinkDial is an innovative open-source framework designed to enhance large language models (LLMs) by enabling controllable reasoning through discrete operational modes. It allows users to switch between three reasoning modes: High, Medium, and Low, which vary in computational effort and performance. This framework incorporates budget-mode control during training, ensuring that reasoning capabilities are embedded into the model's learning process. Extensive testing shows that ThinkDial effectively balances performance and efficiency, making it a valuable tool for practical applications of LLMs."
                },
                "zh": {
                    "title": "ThinkDialï¼šå¯æ§æ¨ç†çš„æ–°å¼€æºæ¡†æ¶",
                    "desc": "ThinkDialæ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç¦»æ•£æ“ä½œæ¨¡å¼å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯æ§æ¨ç†ã€‚è¯¥ç³»ç»Ÿå…è®¸åœ¨ä¸‰ç§ä¸åŒçš„æ¨ç†æ¨¡å¼ä¹‹é—´æ— ç¼åˆ‡æ¢ï¼šé«˜æ¨¡å¼ï¼ˆå®Œå…¨æ¨ç†èƒ½åŠ›ï¼‰ã€ä¸­æ¨¡å¼ï¼ˆå‡å°‘50%çš„ä»¤ç‰Œï¼Œæ€§èƒ½ä¸‹é™ä¸åˆ°10%ï¼‰å’Œä½æ¨¡å¼ï¼ˆå‡å°‘75%çš„ä»¤ç‰Œï¼Œæ€§èƒ½ä¸‹é™ä¸åˆ°15%ï¼‰ã€‚é€šè¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹æ³•ï¼ŒThinkDialåœ¨æ•´ä¸ªæµç¨‹ä¸­é›†æˆäº†é¢„ç®—æ¨¡å¼æ§åˆ¶ï¼Œç¡®ä¿äº†æ¨ç†èƒ½åŠ›çš„å¯æ§æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒThinkDialåœ¨å‹ç¼©æ€§èƒ½æƒè¡¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨å¤„ç†è¶…å‡ºåˆ†å¸ƒçš„ä»»åŠ¡æ—¶ä¹Ÿå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18672",
            "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks",
            "url": "https://huggingface.co/papers/2508.18672",
            "abstract": "MoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity.",
            "score": 2,
            "issue_id": 5566,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "d56645cd1da5b6a9",
            "pdf_title_img": "assets/pdf/title_img/2508.18672.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² MoE: ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture-of-Experts (MoE) Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ, Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ° top-k Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ğ»Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Balancing Memorization and Reasoning in Sparse MoE Models",
                    "desc": "This paper explores how Mixture-of-Experts (MoE) models, which introduce sparsity in large language models (LLMs), impact their ability to memorize and reason. While increasing the number of parameters improves memorization performance, reasoning capabilities can actually decline despite these gains. The authors systematically analyze various configurations of MoE Transformers, focusing on how active parameters and routing strategies affect model performance. Their findings indicate that traditional hyperparameters and post-training techniques do not effectively address the reasoning deficits caused by excessive sparsity in these models."
                },
                "zh": {
                    "title": "MoEæ¨¡å‹ï¼šç¨€ç–æ€§å¯¹è®°å¿†ä¸æ¨ç†çš„åŒé‡å½±å“",
                    "desc": "MoEæ¨¡å‹å¼•å…¥äº†ç¨€ç–æ€§ï¼Œè¿™å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è®°å¿†å’Œæ¨ç†èƒ½åŠ›äº§ç”Ÿäº†ä¸åŒçš„å½±å“ã€‚å°½ç®¡å‚æ•°å¢åŠ ï¼Œæ¨ç†æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ï¼Œè€Œè®°å¿†èƒ½åŠ›åˆ™éšç€æ€»å‚æ•°çš„å¢åŠ è€ŒæŒç»­æ”¹å–„ã€‚æˆ‘ä»¬ç ”ç©¶äº†MoEç¨€ç–æ€§å¦‚ä½•å½±å“è®°å¿†å’Œæ¨ç†è¿™ä¸¤ç§èƒ½åŠ›æ¨¡å¼ï¼Œå¹¶é€šè¿‡è®­ç»ƒä¸åŒå‚æ•°é…ç½®çš„MoE Transformeræ¥åˆ†æå…¶æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œç»å…¸è¶…å‚æ•°å¦‚å­¦ä¹ ç‡å’Œåˆå§‹åŒ–å¯¹æ³›åŒ–å·®è·çš„å½±å“ä¸ç¨€ç–æ€§æ–¹å‘ä¸€è‡´ï¼Œä½†è¿‡äºç¨€ç–çš„æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18370",
            "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo",
            "url": "https://huggingface.co/papers/2508.18370",
            "abstract": "CTF-Dojo, a large-scale executable runtime with 658 CTF challenges, enables rapid training of LLM-based agents with verifiable feedback, achieving state-of-the-art performance in competitive benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.",
            "score": 2,
            "issue_id": 5563,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "cd60ce12b233220d",
            "authors": [
                "Terry Yue Zhuo",
                "Dingmin Wang",
                "Hantian Ding",
                "Varun Kumar",
                "Zijian Wang"
            ],
            "affiliations": [
                "Amazon",
                "Monash University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18370.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#games",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "CTF-Dojo: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ÑƒÑ ÑÑ€ĞµĞ´Ñƒ",
                    "desc": "CTF-Dojo - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ñ 658 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ CTF, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ CTF-Forge Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ€ĞµĞ´ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²ÑĞµĞ³Ğ¾ 486 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¸Ğ· CTF-Dojo Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ›ÑƒÑ‡ÑˆĞ°Ñ 32-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°, ÑĞ¾Ğ¿ĞµÑ€Ğ½Ğ¸Ñ‡Ğ°Ñ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "CTF-Dojo: Revolutionizing LLM Training with Executable Challenges",
                    "desc": "CTF-Dojo is a large-scale platform designed to train large language models (LLMs) using executable runtime environments. It features 658 Capture-The-Flag (CTF) challenges that provide verifiable feedback, which is crucial for improving the performance of machine learning agents. The introduction of CTF-Forge allows for the rapid creation of these training environments from publicly available resources, significantly reducing setup time. As a result, LLM-based agents trained on CTF-Dojo have achieved state-of-the-art performance on competitive benchmarks, showcasing the effectiveness of execution-grounded training methods."
                },
                "zh": {
                    "title": "CTF-Dojoï¼šé«˜æ•ˆè®­ç»ƒæ™ºèƒ½ä½“çš„æ–°å¹³å°",
                    "desc": "CTF-Dojoæ˜¯ä¸€ä¸ªå¤§å‹å¯æ‰§è¡Œè¿è¡Œç¯å¢ƒï¼ŒåŒ…å«658ä¸ªCTFæŒ‘æˆ˜ï¼Œæ—¨åœ¨å¿«é€Ÿè®­ç»ƒåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“ï¼Œå¹¶æä¾›å¯éªŒè¯çš„åé¦ˆã€‚è¯¥å¹³å°é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“CTF-Forgeï¼Œå°†å…¬å¼€å¯ç”¨çš„èµ„æºè½¬åŒ–ä¸ºå¯ç”¨çš„æ‰§è¡Œç¯å¢ƒï¼Œæ˜¾è‘—å‡å°‘äº†é…ç½®æ—¶é—´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨CTF-Dojoè®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº†æœ€é«˜11.6%çš„ç»å¯¹æå‡ã€‚CTF-Dojoå±•ç¤ºäº†æ‰§è¡ŒåŸºç¡€è®­ç»ƒä¿¡å·åœ¨æå‡é«˜æ€§èƒ½æœºå™¨å­¦ä¹ æ™ºèƒ½ä½“æ–¹é¢çš„é‡è¦æ€§ï¼Œä¸”ä¸ä¾èµ–äºæ˜‚è´µçš„ä¸“æœ‰ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16697",
            "title": "QueryBandits for Hallucination Mitigation: Exploiting Semantic Features\n  for No-Regret Rewriting",
            "url": "https://huggingface.co/papers/2508.16697",
            "abstract": "QueryBandits, a bandit framework, effectively mitigates hallucinations in LLMs by proactively rewriting queries based on linguistic features, outperforming static prompting strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. We introduce QueryBandits, a bandit framework that designs rewrite strategies to maximize a reward model, that encapsulates hallucination propensity based upon the sensitivities of 17 linguistic features of the input query-and therefore, proactively steer LLMs away from generating hallucinations. Across 13 diverse QA benchmarks and 1,050 lexically perturbed queries per dataset, our top contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a no-rewrite baseline and also outperforms zero-shot static prompting (\"paraphrase\" or \"expand\") by 42.6% and 60.3% respectively. Therefore, we empirically substantiate the effectiveness of QueryBandits in mitigating hallucination via the intervention that takes the form of a query rewrite. Interestingly, certain static prompting strategies, which constitute a considerable number of current query rewriting literature, have a higher cumulative regret than the no-rewrite baseline, signifying that static rewrites can worsen hallucination. Moreover, we discover that the converged per-arm regression feature weight vectors substantiate that there is no single rewrite strategy optimal for all queries. In this context, guided rewriting via exploiting semantic features with QueryBandits can induce significant shifts in output behavior through forward-pass mechanisms, bypassing the need for retraining or gradient-based adaptation.",
            "score": 2,
            "issue_id": 5564,
            "pub_date": "2025-08-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 22",
                "zh": "8æœˆ22æ—¥"
            },
            "hash": "33856afd1b846ac5",
            "authors": [
                "Nicole Cho",
                "William Watson",
                "Alec Koppel",
                "Sumitra Ganesh",
                "Manuela Veloso"
            ],
            "affiliations": [
                "JP Morgan AI Research, New York, NY"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.16697.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#hallucinations",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "QueryBandits - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€ÑƒĞºĞ¸Ñ… Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ½ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ QueryBandits Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² LLM."
                },
                "en": {
                    "title": "Proactive Query Rewriting to Combat Hallucinations in LLMs",
                    "desc": "The paper introduces QueryBandits, a novel framework designed to reduce hallucinations in Large Language Models (LLMs) by rewriting input queries based on linguistic features. Unlike traditional methods that filter out hallucinations after they occur, QueryBandits proactively modifies queries to minimize the likelihood of generating incorrect information. The framework employs a reward model that evaluates the potential for hallucination based on 17 linguistic characteristics, leading to improved performance across various question-answering benchmarks. Experimental results show that QueryBandits significantly outperforms static prompting techniques, demonstrating the importance of dynamic query rewriting in enhancing LLM reliability."
                },
                "zh": {
                    "title": "QueryBanditsï¼šä¸»åŠ¨é‡å†™æŸ¥è¯¢ä»¥å‡å°‘å¹»è§‰ç°è±¡",
                    "desc": "QueryBandits æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ ¹æ®è¯­è¨€ç‰¹å¾ä¸»åŠ¨é‡å†™æŸ¥è¯¢æ¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¹»è§‰ç°è±¡ã€‚ä¸ä¼ ç»Ÿçš„é™æ€æç¤ºç­–ç•¥ç›¸æ¯”ï¼ŒQueryBandits é€šè¿‡è®¾è®¡é‡å†™ç­–ç•¥æ¥æœ€å¤§åŒ–å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œæœ‰æ•ˆåœ°å¼•å¯¼ LLMs é¿å…ç”Ÿæˆå¹»è§‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQueryBandits åœ¨å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨è¯­ä¹‰ç‰¹å¾è¿›è¡Œå¼•å¯¼é‡å†™å¯ä»¥æ˜¾è‘—æ”¹å–„è¾“å‡ºè¡Œä¸ºï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19188",
            "title": "FastMesh:Efficient Artistic Mesh Generation via Component Decoupling",
            "url": "https://huggingface.co/papers/2508.19188",
            "abstract": "A framework for efficient artistic mesh generation reduces redundancy by separating vertex and face generation, using an autoregressive model for vertices and a bidirectional transformer for faces, and includes a fidelity enhancer and post-processing to improve quality and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23\\% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8times faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality.",
            "score": 1,
            "issue_id": 5563,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "31ec4c8e0ef6b3e8",
            "authors": [
                "Jeonghwan Kim",
                "Yushi Lan",
                "Armando Fortes",
                "Yongwei Chen",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19188.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑ€ÑˆĞ¸Ğ½ Ğ¸ Ğ³Ñ€Ğ°Ğ½ĞµĞ¹. Ğ”Ğ»Ñ Ğ²ĞµÑ€ÑˆĞ¸Ğ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ° Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ğ½ĞµĞ¹ - Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ĞµĞ»ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€ÑˆĞ¸Ğ½ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ñ€Ñ‘Ğ±ĞµÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ 8-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Artistic Mesh Generation: Less Redundancy, More Quality!",
                    "desc": "This paper presents a new framework for generating artistic meshes that improves efficiency by separating the processes of vertex and face generation. It uses an autoregressive model to generate vertices, which reduces the number of tokens needed by about 77% compared to traditional methods. For face generation, a bidirectional transformer is employed to quickly construct the mesh by understanding the relationships between vertices. Additionally, a fidelity enhancer and post-processing techniques are introduced to enhance the quality and speed of the generated meshes."
                },
                "zh": {
                    "title": "é«˜æ•ˆè‰ºæœ¯ç½‘æ ¼ç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è‰ºæœ¯ç½‘æ ¼ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å°†é¡¶ç‚¹å’Œé¢ç”Ÿæˆåˆ†å¼€ï¼Œå‡å°‘äº†å†—ä½™ã€‚æˆ‘ä»¬ä½¿ç”¨è‡ªå›å½’æ¨¡å‹ç”Ÿæˆé¡¶ç‚¹ï¼Œæ˜¾è‘—é™ä½äº†æ‰€éœ€çš„ä»¤ç‰Œæ•°é‡ã€‚æ¥ç€ï¼Œåˆ©ç”¨åŒå‘å˜æ¢å™¨ä¸€æ¬¡æ€§å®Œæˆç½‘æ ¼æ„å»ºï¼Œæ•æ‰é¡¶ç‚¹é—´çš„å…³ç³»ã€‚æœ€åï¼Œé€šè¿‡å¼•å…¥ä¿çœŸåº¦å¢å¼ºå™¨å’Œåå¤„ç†æ¡†æ¶ï¼Œè¿›ä¸€æ­¥æé«˜ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19026",
            "title": "MovieCORE: COgnitive REasoning in Movies",
            "url": "https://huggingface.co/papers/2508.19026",
            "abstract": "MovieCORE is a video question answering dataset that uses multiple large language models to generate deep cognitive questions, and introduces an agentic enhancement module to improve VQA model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.",
            "score": 1,
            "issue_id": 5564,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "ad2f2e8c67dd7dd8",
            "authors": [
                "Gueter Josmy Faure",
                "Min-Hung Chen",
                "Jia-Fong Yeh",
                "Ying Cheng",
                "Hung-Ting Su",
                "Yung-Hao Tang",
                "Shang-Hong Lai",
                "Winston H. Hsu"
            ],
            "affiliations": [
                "NVIDIA",
                "National Chengchi University",
                "National Taiwan University",
                "National Tsing Hua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19026.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#reasoning",
                    "#alignment",
                    "#agents",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "MovieCORE: Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "MovieCORE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ 2-Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ›Ğ›Ğœ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ACE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 25%."
                },
                "en": {
                    "title": "Deepening Movie Understanding with MovieCORE",
                    "desc": "MovieCORE is a new video question answering (VQA) dataset that focuses on deeper cognitive understanding of movies. It uses multiple large language models to create complex questions that require advanced reasoning, rather than just surface-level comprehension. The paper introduces an agentic enhancement module called Agentic Choice Enhancement (ACE) that significantly boosts the performance of VQA models by improving their reasoning abilities. This work aims to enhance AI's understanding of cinematic content and provides a framework for evaluating VQA models on more challenging questions."
                },
                "zh": {
                    "title": "æ·±åº¦è®¤çŸ¥ç”µå½±é—®ç­”çš„æ–°çªç ´",
                    "desc": "MovieCOREæ˜¯ä¸€ä¸ªæ–°çš„è§†é¢‘é—®ç­”æ•°æ®é›†ï¼Œæ—¨åœ¨æ·±å…¥æ¢è®¨ç”µå½±å†…å®¹çš„è®¤çŸ¥ç†è§£ã€‚ä¸ç°æœ‰æ•°æ®é›†ä¸åŒï¼ŒMovieCOREå¼ºè°ƒéœ€è¦ç³»ç»ŸäºŒæ€ç»´çš„é—®é¢˜ï¼Œä¸“æ³¨äºè§†é¢‘ææ–™ã€‚æˆ‘ä»¬é‡‡ç”¨å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ€ç»´ä»£ç†ï¼Œç”Ÿæˆå’Œä¼˜åŒ–é«˜è´¨é‡çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚ä¸ºäº†è¯„ä¼°æ•°æ®é›†çš„è´¨é‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—è®¤çŸ¥æµ‹è¯•ï¼Œè¯„ä¼°é—®é¢˜çš„æ·±åº¦ã€æ€ç»´æ¿€å‘æ½œåŠ›å’Œå¥æ³•å¤æ‚æ€§ï¼ŒåŒæ—¶æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ–¹æ¡ˆæ¥è¯„ä¼°VQAæ¨¡å‹åœ¨æ›´æ·±å±‚æ¬¡è®¤çŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18271",
            "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion\n  Models",
            "url": "https://huggingface.co/papers/2508.18271",
            "abstract": "ObjFiller-3D uses video editing models to achieve high-quality and consistent 3D object completion, outperforming previous methods in terms of reconstruction fidelity and practical deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D .",
            "score": 1,
            "issue_id": 5565,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "7d474b5d9c6dd1d2",
            "authors": [
                "Haitang Feng",
                "Jie Liu",
                "Jie Tang",
                "Gangshan Wu",
                "Beiqi Chen",
                "Jianhuang Lai",
                "Guangcong Wang"
            ],
            "affiliations": [
                "Great Bay University",
                "Harbin Institute of Technology",
                "Nanjing University",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18271.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğµ: Ğ¾Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼",
                    "desc": "ObjFiller-3D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² 2D-Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°. ObjFiller-3D Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ 3D Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Object Completion with Video Editing Techniques",
                    "desc": "ObjFiller-3D is a new method for completing and editing 3D objects using advanced video editing models. It addresses the common issues of blurred textures and visual artifacts that arise from traditional 2D image inpainting methods. By adapting video inpainting techniques, ObjFiller-3D achieves higher reconstruction fidelity and structural coherence in 3D object completion. Experiments show that it significantly outperforms previous methods, making it suitable for real-world 3D editing applications."
                },
                "zh": {
                    "title": "é«˜è´¨é‡3Dç‰©ä½“è¡¥å…¨çš„æ–°æ–¹æ³•",
                    "desc": "ObjFiller-3Dæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°é«˜è´¨é‡å’Œä¸€è‡´æ€§çš„3Dç‰©ä½“è¡¥å…¨ã€‚ä¸ä¼ ç»Ÿçš„2Då›¾åƒä¿®å¤æ¨¡å‹ä¸åŒï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å…ˆè¿›çš„è§†é¢‘ç¼–è¾‘æ¨¡å‹æ¥å¡«è¡¥3Dç‰©ä½“çš„é®æŒ¡åŒºåŸŸã€‚é€šè¿‡åˆ†æ3Dä¸è§†é¢‘ä¹‹é—´çš„è¡¨ç¤ºå·®è·ï¼ŒObjFiller-3Dèƒ½å¤Ÿæœ‰æ•ˆå…‹æœå¤šè§†è§’ä¿®å¤ä¸­çš„ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºç²¾åº¦å’Œå®é™…åº”ç”¨æ½œåŠ›æ–¹é¢å‡ä¼˜äºä»¥å¾€çš„æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19202",
            "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and\n  Reasoning",
            "url": "https://huggingface.co/papers/2508.19202",
            "abstract": "SciReas and SciReas-Pro benchmarks, along with KRUX framework, provide insights into the distinct roles of knowledge and reasoning in scientific tasks, highlighting critical bottlenecks and improvements for LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning.",
            "score": 0,
            "issue_id": 5567,
            "pub_date": "2025-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "fe91970689f2e015",
            "authors": [
                "Alan Li",
                "Yixin Liu",
                "Arpan Sarkar",
                "Doug Downey",
                "Arman Cohan"
            ],
            "affiliations": [
                "Allen Institute of AI",
                "Harvard University",
                "Northwestern University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19202.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#science"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¯Ğœ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ SciReas Ğ¸ SciReas-Pro Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº KRUX Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€Ğ¾Ğ»Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ·ĞºĞ¸Ğ¼ Ğ¼ĞµÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¯Ğœ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¯Ğœ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Scientific Reasoning in LLMs with SciReas and KRUX",
                    "desc": "The paper introduces SciReas and SciReas-Pro, benchmarks designed to evaluate the performance of large language models (LLMs) in scientific reasoning tasks. It highlights the importance of both knowledge retrieval and reasoning capabilities, identifying key challenges that LLMs face in these areas. The KRUX framework is proposed to analyze how knowledge and reasoning interact in scientific problem-solving. The findings suggest that improving knowledge retrieval and enhancing reasoning processes can significantly boost the effectiveness of LLMs in scientific contexts."
                },
                "zh": {
                    "title": "æ­ç¤ºç§‘å­¦æ¨ç†ä¸­çš„çŸ¥è¯†ä¸æ¨ç†è§’è‰²",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†SciReaså’ŒSciReas-ProåŸºå‡†ï¼Œä»¥åŠKRUXæ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºçŸ¥è¯†å’Œæ¨ç†åœ¨ç§‘å­¦ä»»åŠ¡ä¸­çš„ä¸åŒè§’è‰²ã€‚ç§‘å­¦é—®é¢˜è§£å†³å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ·±åšçš„é¢†åŸŸçŸ¥è¯†å’Œå¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºçš„SciReasæ˜¯ä¸€ä¸ªå¤šæ ·åŒ–çš„åŸºå‡†å¥—ä»¶ï¼Œè€ŒSciReas-Proåˆ™æ˜¯ä¸€ä¸ªéœ€è¦æ›´å¤æ‚æ¨ç†çš„å­é›†ã€‚é€šè¿‡ç»¼åˆè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†æ¨¡å‹çš„å¤–éƒ¨çŸ¥è¯†å¢å¼ºå¯¹ç§‘å­¦æ¨ç†çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18192",
            "title": "Unraveling the cognitive patterns of Large Language Models through\n  module communities",
            "url": "https://huggingface.co/papers/2508.18192",
            "abstract": "A network-based framework links cognitive skills, LLM architectures, and datasets, revealing unique emergent skill patterns in LLMs that benefit from dynamic, cross-regional interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have reshaped our world with significant advancements in science, engineering, and society through applications ranging from scientific discoveries and medical diagnostics to Chatbots. Despite their ubiquity and utility, the underlying mechanisms of LLM remain concealed within billions of parameters and complex structures, making their inner architecture and cognitive processes challenging to comprehend. We address this gap by adopting approaches to understanding emerging cognition in biology and developing a network-based framework that links cognitive skills, LLM architectures, and datasets, ushering in a paradigm shift in foundation model analysis. The skill distribution in the module communities demonstrates that while LLMs do not strictly parallel the focalized specialization observed in specific biological systems, they exhibit unique communities of modules whose emergent skill patterns partially mirror the distributed yet interconnected cognitive organization seen in avian and small mammalian brains. Our numerical results highlight a key divergence from biological systems to LLMs, where skill acquisition benefits substantially from dynamic, cross-regional interactions and neural plasticity. By integrating cognitive science principles with machine learning, our framework provides new insights into LLM interpretability and suggests that effective fine-tuning strategies should leverage distributed learning dynamics rather than rigid modular interventions.",
            "score": 0,
            "issue_id": 5563,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 25",
                "zh": "8æœˆ25æ—¥"
            },
            "hash": "6ce511ea51abb060",
            "authors": [
                "Kushal Raj Bhandari",
                "Pin-Yu Chen",
                "Jianxi Gao"
            ],
            "affiliations": [
                "IBM Research",
                "Rensselaer Polytechnic Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18192.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#science",
                    "#training",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡ĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµÑ‚ĞµĞ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ² LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ… LLM Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ, Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½ÑƒÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼ÑƒÑ Ğ² Ğ¼Ğ¾Ğ·Ğ³Ğµ Ğ¿Ñ‚Ğ¸Ñ† Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¼Ğ»ĞµĞºĞ¾Ğ¿Ğ¸Ñ‚Ğ°ÑÑ‰Ğ¸Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼ĞµĞ¶Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¾Ñ‚ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Unlocking LLMs: A Networked Approach to Cognitive Skills and Architecture",
                    "desc": "This paper presents a network-based framework that connects cognitive skills with the architectures of Large Language Models (LLMs) and the datasets they are trained on. It reveals that LLMs exhibit unique emergent skill patterns that are influenced by dynamic interactions across different regions of their architecture, similar to cognitive processes in biological systems. The study highlights that while LLMs do not strictly mimic the specialization found in biological brains, they show a distributed and interconnected organization of skills. The findings suggest that understanding these emergent skills can improve LLM interpretability and inform better fine-tuning strategies that utilize flexible learning dynamics."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥æŠ€èƒ½ä¸æ¶æ„çš„è”ç³»",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç½‘ç»œçš„æ¡†æ¶ï¼Œè¿æ¥äº†è®¤çŸ¥æŠ€èƒ½ã€è¯­è¨€æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†ï¼Œæ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ç‹¬ç‰¹çš„æŠ€èƒ½æ¨¡å¼ã€‚è¿™äº›æ¨¡å‹åœ¨åŠ¨æ€çš„è·¨åŒºåŸŸäº¤äº’ä¸­å—ç›Šï¼Œå±•ç°å‡ºä¸ç”Ÿç‰©ç³»ç»Ÿä¸åŒçš„æŠ€èƒ½è·å–æ–¹å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMsçš„æ¨¡å—ç¤¾åŒºè™½ç„¶ä¸å®Œå…¨ä¸ç‰¹å®šç”Ÿç‰©ç³»ç»Ÿçš„ä¸“ä¸šåŒ–ç›¸ä¼¼ï¼Œä½†å®ƒä»¬çš„æŠ€èƒ½æ¨¡å¼éƒ¨åˆ†åæ˜ äº†é¸Ÿç±»å’Œå°å‹å“ºä¹³åŠ¨ç‰©å¤§è„‘çš„åˆ†å¸ƒå¼è®¤çŸ¥ç»„ç»‡ã€‚é€šè¿‡å°†è®¤çŸ¥ç§‘å­¦åŸç†ä¸æœºå™¨å­¦ä¹ ç»“åˆï¼Œè¯¥æ¡†æ¶ä¸ºLLMsçš„å¯è§£é‡Šæ€§æä¾›äº†æ–°è§è§£ï¼Œå¹¶å»ºè®®æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥åº”åˆ©ç”¨åˆ†å¸ƒå¼å­¦ä¹ åŠ¨æ€ï¼Œè€ŒéåƒµåŒ–çš„æ¨¡å—å¹²é¢„ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-26.html",
    "link_next": "2025-08-28.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "26.08",
        "en": "08/26",
        "zh": "8æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "28.08",
        "en": "08/28",
        "zh": "8æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 3,
        "#cv": 1,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 4,
        "#audio": 2,
        "#video": 6,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 1,
        "#games": 5,
        "#interpretability": 3,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 4,
        "#low_resource": 0
    }
}