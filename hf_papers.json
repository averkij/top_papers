{
    "date": {
        "ru": "4 декабря",
        "en": "December 4",
        "zh": "12月4日"
    },
    "time_utc": "2024-12-04 04:13",
    "weekday": 2,
    "issue_id": 933,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.19943",
            "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability",
            "url": "https://huggingface.co/papers/2411.19943",
            "abstract": "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO.",
            "score": 10,
            "issue_id": 933,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "aaf523f6bd9412e3",
            "authors": [
                "Zicheng Lin",
                "Tian Liang",
                "Jiahao Xu",
                "Xing Wang",
                "Ruilin Luo",
                "Chufan Shi",
                "Siheng Li",
                "Yujiu Yang",
                "Zhaopeng Tu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.19943.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Повышение точности рассуждений LLM путем выявления критических токенов",
                    "desc": "Исследование посвящено влиянию отдельных токенов на результаты рассуждений в больших языковых моделях (LLM). Авторы обнаружили существование 'критических токенов', которые приводят к неправильным траекториям рассуждений. Они предложили новый метод cDPO для автоматического распознавания и обучения с учетом критических токенов в процессе выравнивания модели. Экспериментальные результаты на бенчмарках GSM8K и MATH500 с использованием моделей Llama-3 и deepseek-math продемонстрировали эффективность предложенного подхода."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs by Identifying Critical Tokens",
                    "desc": "This paper investigates how individual tokens in Large Language Models (LLMs) affect reasoning outcomes. It identifies 'critical tokens' that can lead to incorrect reasoning paths, suggesting that LLMs perform better when they focus on non-critical tokens. The authors introduce a new method called cDPO, which uses contrastive estimation to automatically detect these critical tokens during the model's alignment process. Experimental results show that cDPO improves reasoning performance on benchmark datasets by effectively managing token-level rewards."
                },
                "zh": {
                    "title": "识别关键token，提升推理准确性",
                    "desc": "大型语言模型（LLMs）在推理任务中表现出色，能够通过自回归的方式生成推理过程。本文探讨了单个token对推理任务最终结果的影响，发现存在“关键token”，这些token会导致错误的推理轨迹。我们提出了一种新方法cDPO，旨在自动识别关键token并在对齐过程中进行token级奖励。通过对比正负模型的生成可能性，我们能够识别出在错误轨迹中导致错误结果的关键token，从而提高模型的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02632",
            "title": "Scaling Image Tokenizers with Grouped Spherical Quantization",
            "url": "https://huggingface.co/papers/2412.02632",
            "abstract": "Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.",
            "score": 3,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "60eda94a31cded90",
            "authors": [
                "Jiangtao Wang",
                "Zhen Qin",
                "Yifan Zhang",
                "Vincent Tao Hu",
                "Björn Ommer",
                "Rania Briq",
                "Stefan Kesselheim"
            ],
            "affiliations": [
                "CompVis @ LMU Munich",
                "Jülich Supercomputing Centre",
                "TapTap",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02632.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#cv",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "GSQ: Эффективная токенизация изображений на сферической поверхности",
                    "desc": "В статье представлен новый метод токенизации изображений - Групповая Сферическая Квантизация (GSQ). GSQ использует сферическую инициализацию кодовой книги и регуляризацию поиска для ограничения латентного пространства кодовой книги сферической поверхностью. Авторы провели эмпирический анализ стратегий обучения токенизаторов изображений и показали, что GSQ-GAN превосходит современные методы по качеству реконструкции при меньшем количестве итераций обучения. Исследование масштабируемости GSQ выявило различное поведение при высоких и низких уровнях пространственного сжатия, подчеркивая сложности представления высокоразмерных латентных пространств."
                },
                "en": {
                    "title": "Efficient Image Processing with Grouped Spherical Quantization",
                    "desc": "This paper introduces a new method called Grouped Spherical Quantization (GSQ) for improving vision tokenizers, which are tools used to process images efficiently. GSQ uses a special technique to initialize and regularize a spherical codebook, helping to keep the data organized on a spherical surface. The authors demonstrate that GSQ-GAN, a model based on this method, can reconstruct images with high quality while requiring fewer training iterations compared to existing methods. Their analysis also reveals how different factors like latent dimensionality and codebook size affect the model's performance, particularly in handling high-dimensional data efficiently."
                },
                "zh": {
                    "title": "分组球面量化：高效的视觉标记器新方法",
                    "desc": "本文介绍了一种新的视觉标记器方法，称为分组球面量化（GSQ），旨在解决传统方法中的一些问题。GSQ通过球面代码本初始化和查找正则化，限制了代码本潜在空间在球面上的分布。我们的实证分析表明，GSQ-GAN在重建质量上优于现有的最先进方法，并且训练迭代次数更少。研究还系统地考察了GSQ在潜在维度、代码本大小和压缩比等方面的扩展行为，揭示了高维潜在空间表示的挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01292",
            "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences",
            "url": "https://huggingface.co/papers/2412.01292",
            "abstract": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.",
            "score": 3,
            "issue_id": 933,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "e8f8ddd05e13e9ef",
            "authors": [
                "Hongyan Zhi",
                "Peihao Chen",
                "Junyan Li",
                "Shuailei Ma",
                "Xinyu Sun",
                "Tianhang Xiang",
                "Yinjie Lei",
                "Mingkui Tan",
                "Chuang Gan"
            ],
            "affiliations": [
                "MIT-IBM Watson AI Lab",
                "Northeastern University",
                "Pazhou Laboratory",
                "Sichuan University",
                "South China University of Technology",
                "Tencent Robotics X",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01292.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Умное понимание 3D-сцен с LSceneLLM",
                    "desc": "Исследование 3D Vision-Language Models (3D-VLMs) важно для развития воплощенного AI в 3D-сценах, таких как визуальная навигация и ответы на вопросы. Из-за высокой плотности визуальных признаков в больших 3D-сценах сложно точно определить важную для задачи информацию. Предлагается адаптивная структура LSceneLLM, которая автоматически определяет важные области, используя визуальные предпочтения LLM для разных задач. Эксперименты показывают, что наш метод превосходит существующие методы в понимании больших сцен и улучшает результаты при интеграции в существующие 3D-VLMs."
                },
                "en": {
                    "title": "Enhancing 3D Scene Understanding with LSceneLLM",
                    "desc": "This paper introduces LSceneLLM, a novel framework designed to enhance 3D Vision-Language Models (3D-VLMs) for better understanding of large 3D scenes. It addresses the challenge of identifying task-relevant visual information amidst the dense features present in these scenes. By utilizing a dense token selector and an adaptive self-attention module, the framework effectively magnifies important details while reducing redundant information. The authors also present a new benchmark, XR-Scene, to evaluate the performance of 3D-VLMs on various large scene understanding tasks, demonstrating that their approach significantly outperforms existing methods."
                },
                "zh": {
                    "title": "自适应3D视觉语言模型，提升场景理解能力",
                    "desc": "3D视觉语言模型（3D-VLMs）的研究越来越受到关注，这对在3D场景中发展具身人工智能至关重要，如视觉导航和具身问答。由于3D场景中视觉特征的高密度，准确定位与任务相关的视觉信息变得具有挑战性。现有方法尝试对所有对象进行分割，并将其特征视为场景表示，但这些与任务无关的对象特征包含大量冗余信息和缺失的细节。为了解决这些问题，我们提出了LSceneLLM，一个自适应框架，通过利用大语言模型（LLM）对不同任务的视觉偏好，自动识别与任务相关的区域，并通过可插拔的场景放大模块捕捉聚焦区域的细粒度细节。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02611",
            "title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?",
            "url": "https://huggingface.co/papers/2412.02611",
            "abstract": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.",
            "score": 2,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "f63565048b4948b4",
            "authors": [
                "Kaixiong Gong",
                "Kaituo Feng",
                "Bohao Li",
                "Yibing Wang",
                "Mofan Cheng",
                "Shijia Yang",
                "Jiaming Han",
                "Benyou Wang",
                "Yutong Bai",
                "Zhuoran Yang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "CUHK (SZ)",
                "CUHK MMLab",
                "Stanford University",
                "UC Berkeley",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02611.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#interpretability",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "Слышат ли ИИ-модели то, что видят?",
                    "desc": "Статья представляет новый тест DeafTest и бенчмарк AV-Odyssey Bench для оценки мультимодальных больших языковых моделей (MLLM) в задачах аудио-визуального понимания. Исследователи обнаружили, что современные MLLM часто не справляются с простыми задачами определения громкости и высоты звуков. Бенчмарк включает 4,555 задач с текстом, изображениями и аудио, требующих эффективного использования обоих модальностей. Авторы провели тестирование ряда закрытых и открытых моделей, выявив ограничения текущих систем для дальнейшего улучшения сбора данных и разработки моделей."
                },
                "en": {
                    "title": "Unveiling the Limits of Multimodal Models with AV-Odyssey Bench",
                    "desc": "This paper introduces DeafTest, a benchmark that highlights the limitations of multimodal large language models (MLLMs) in understanding basic audio tasks that humans find easy. The authors present AV-Odyssey Bench, which consists of 4,555 problems that require models to analyze and integrate audio, visual, and text information. The benchmark is designed to objectively evaluate MLLM performance through multiple-choice questions, avoiding reliance on human judgment. By assessing various models, the study aims to shed light on the shortcomings of current MLLMs and guide future improvements in model training and dataset creation."
                },
                "zh": {
                    "title": "揭示多模态模型的局限性",
                    "desc": "最近，多模态大型语言模型（MLLMs）如GPT-4o、Gemini 1.5 Pro和Reka Core，扩展了其在视觉和音频方面的能力。尽管这些模型在多种音频-视觉应用中表现出色，但我们的DeafTest显示，MLLMs在一些人类认为简单的任务上常常表现不佳，例如判断两个声音哪个更响和哪个音调更高。为此，我们提出了AV-Odyssey Bench，这是一个全面的音频-视觉基准，旨在评估这些MLLMs是否真正理解音频-视觉信息。该基准包含4555个精心设计的问题，要求模型有效利用视觉和音频输入中的线索，以准确推断答案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02259",
            "title": "VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation",
            "url": "https://huggingface.co/papers/2412.02259",
            "abstract": "Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.",
            "score": 1,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "5a007f38be3e3ba7",
            "authors": [
                "Mingzhe Zheng",
                "Yongqi Xu",
                "Haojian Huang",
                "Xuran Ma",
                "Yexin Liu",
                "Wenjie Shu",
                "Yatian Pang",
                "Feilong Tang",
                "Qifeng Chen",
                "Harry Yang",
                "Ser-Nam Lim"
            ],
            "affiliations": [
                "Everlyn AI",
                "Hong Kong University of Science and Technology",
                "National University of Singapore",
                "Peking University",
                "University of Central Florida",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02259.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#story_generation",
                    "#games"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VGoT: Новый уровень в генерации многокадровых видео с сохранением логики повествования",
                    "desc": "Статья представляет новый подход к генерации многокадровых видео под названием VideoGen-of-Thought (VGoT). Эта архитектура разделяет процесс на несколько этапов: генерация сценария, создание ключевых кадров, генерация отдельных сцен и механизм сглаживания. VGoT использует встраивания для сохранения идентичности персонажей между сценами и применяет межкадровое сглаживание для обеспечения визуальной согласованности. Эксперименты показывают, что VGoT превосходит существующие методы в создании качественных и связных многокадровых видео."
                },
                "en": {
                    "title": "Revolutionizing Multi-Shot Video Generation with VGoT",
                    "desc": "The paper introduces VideoGen-of-Thought (VGoT), a novel architecture aimed at improving multi-shot video generation. Unlike traditional models that focus on single-shot outputs, VGoT employs a structured approach that includes script generation, keyframe creation, and shot-level video generation, ensuring a cohesive narrative. It emphasizes reasonable narrative design by incorporating principles from cinematic scriptwriting, which enhances character development and logical flow. Additionally, VGoT utilizes identity-preserving embeddings and a cross-shot smoothing mechanism to maintain visual consistency and smooth transitions across multiple shots."
                },
                "zh": {
                    "title": "多镜头视频生成的新突破",
                    "desc": "当前的视频生成模型在生成短片方面表现出色，但在创建多镜头、电影般的视频时仍然存在困难。现有模型通常只针对单镜头目标进行训练，因此在保持逻辑故事线和视觉一致性方面显得不足。为此，我们提出了VideoGen-of-Thought（VGoT），这是一种专门为多镜头视频生成设计的协作和无训练架构。VGoT通过脚本生成、关键帧生成和镜头级视频生成等模块化步骤，确保了合理的叙事设计和跨镜头的一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01981",
            "title": "Free Process Rewards without Process Labels",
            "url": "https://huggingface.co/papers/2412.01981",
            "abstract": "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\'a la Math-Shepherd using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible.",
            "score": 0,
            "issue_id": 933,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "13434e4f301a0d88",
            "authors": [
                "Lifan Yuan",
                "Wendi Li",
                "Huayu Chen",
                "Ganqu Cui",
                "Ning Ding",
                "Kaiyan Zhang",
                "Bowen Zhou",
                "Zhiyuan Liu",
                "Hao Peng"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01981.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#training",
                    "#reasoning",
                    "#data",
                    "#low_resource"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение PRM без пошаговой разметки",
                    "desc": "Статья представляет новый подход к обучению процессуальных моделей вознаграждения (PRM) в машинном обучении. Авторы показывают, что неявную PRM можно получить без дополнительных затрат, просто обучая модель вознаграждения за результат (ORM) на более дешевых метках уровня ответов. Эксперименты на датасете MATH демонстрируют, что предложенный метод превосходит сильный бейзлайн на основе MCTS, используя менее 1/38 обучающих данных. Исследование также выявляет, что масштабирование инструкций и ответов улучшает производительность неявной PRM."
                },
                "en": {
                    "title": "Unlocking Efficient Training for Process Reward Models",
                    "desc": "This paper introduces a novel approach to training process reward models (PRMs) that score reasoning steps individually, as opposed to outcome reward models (ORMs) which evaluate entire responses. The authors propose that an implicit PRM can be derived from an ORM trained on simpler response-level labels, thus avoiding the need for detailed step-by-step annotations. Through theoretical and empirical analysis, they demonstrate that this implicit PRM can achieve superior performance on tasks like MATH, using significantly less training data than traditional methods. The findings suggest that optimizing the outcome reward as log-likelihood ratios enhances data efficiency and model performance, even in scenarios with limited training examples."
                },
                "zh": {
                    "title": "隐式过程奖励模型：高效训练的新思路",
                    "desc": "本文提出了一种新的过程奖励模型（PRM），与传统的结果奖励模型（ORM）不同，PRM能够逐步评估推理过程，提供更细致的奖励。然而，训练PRM需要在每个中间步骤都有标注，这在数据收集上面临挑战。我们展示了通过训练ORM并使用响应级别的标签，可以在没有额外成本的情况下获得隐式PRM。实验结果表明，隐式PRM在数据效率和性能上优于传统方法，尤其是在数据稀缺的情况下。"
                }
            }
        }
    ],
    "link_prev": "2024-12-03.html",
    "link_next": "2024-12-05.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "03.12",
        "en": "12/03",
        "zh": "12月3日"
    },
    "short_date_next": {
        "ru": "05.12",
        "en": "12/05",
        "zh": "12月5日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了多模态大语言模型（MLLMs）在视觉理解和生成任务中的进展。然而，生成交错的图像-文本内容仍然是一个挑战。现有的基准测试由于数据量和多样性的限制，无法充分评估这些方法。为了解决这个问题，作者提出了GATE OpenING（OpenING），一个包含5,400个高质量人工标注实例的全面基准，涵盖了56个真实世界任务。OpenING涵盖了各种日常场景，如旅行指南、设计和头脑风暴，为挑战性的交错生成方法提供了一个强大的平台。此外，作者还介绍了IntJudge，一个用于评估开放式多模态生成方法的评估模型。",
        "title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
        "pinyin": "这篇文章介绍了多模态大语言模型（MLLMs）在视觉理解和生成任务中的进展。然而，生成交错的图像-文本内容仍然是一个挑战。现有的基准测试由于数据量和多样性的限制，无法充分评估这些方法。为了解决这个问题，作者提出了GATE OpenING（OpenING），一个包含5,400个高质量人工标注实例的全面基准，涵盖了56个真实世界任务。OpenING涵盖了各种日常场景，如旅行指南、设计和头脑风暴，为挑战性的交错生成方法提供了一个强大的平台。此外，作者还介绍了IntJudge，一个用于评估开放式多模态生成方法的评估模型。\n\nzhè piān wén zhāng jiè shào le duō mó tài dà yǔ yán mó xíng (MLLMs) zài shì jué lǐ jiě hé shēng chéng rèn wù zhōng de jìn zhàn. rán ér, shēng chéng jiāo cuò de tú xiàng-wén běn nèi róng réng shì yī gè tiǎo zhàn. xiàn yǒu de jī zhǔn cè shì yóu yú shù jù liàng hé duō yàng xìng de xiàn zhì, wú fǎ chōng fēn píng gū zhè xiē fāng fǎ. wèi le jiě jué zhè gè wèn tí, zuò zhě tí chū le GATE OpenING (OpenING), yī gè bāo hán 5,400 gè gāo zhì liàng rén gōng biāo zhù shí lì de quán miàn jī zhǔn, hán gǎi le 56 gè zhēn shí shì jiè rèn wù. OpenING hán gǎi le gè zhǒng rì cháng chǎng jīng, rú lǚ xíng zhǐ nán, shè jì hé tóu nǎo fēng bào, wèi tiǎo zhàn xìng de jiāo cuò shēng chéng fāng fǎ tí gōng le yī gè qiáng dà de píng tái. cǐ wài, zuò zhě hái jiè shào le IntJudge, yī gè yòng yú píng gū kāi fàng shì duō mó tài shēng chéng fāng fǎ de píng gū mó xíng.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"视觉理解\", \"pinyin\": \"shì jué lǐ jiě\", \"trans\": \"visual understanding\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generation\"},\n    {\"word\": \"交错\", \"pinyin\": \"jiāo cuò\", \"trans\": \"interleaved\"},\n    {\"word\": \"图像-文本\", \"pinyin\": \"tú xiàng wén běn\", \"trans\": \"image-text\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jī zhǔn cè shì\", \"trans\": \"benchmark test\"},\n    {\"word\": \"数据量\", \"pinyin\": \"shù jù liàng\", \"trans\": \"data volume\"},\n    {\"word\": \"多样性\", \"pinyin\": \"duō yàng xìng\", \"trans\": \"diversity\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluation\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"人工标注\", \"pinyin\": \"réngōng biāo zhù\", \"trans\": \"manual annotation\"},\n    {\"word\": \"实例\", \"pinyin\": \"shí lì\", \"trans\": \"instance\"},\n    {\"word\": \"真实世界\", \"pinyin\": \"zhēn shí shì jiè\", \"trans\": \"real world\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"日常场景\", \"pinyin\": \"rì cháng chǎng jǐng\", \"trans\": \"daily scenarios\"},\n    {\"word\": \"旅行指南\", \"pinyin\": \"lǚ xíng zhǐ nán\", \"trans\": \"travel guide\"},\n    {\"word\": \"设计\", \"pinyin\": \"shè jì\", \"trans\": \"design\"},\n    {\"word\": \"头脑风暴\", \"pinyin\": \"tóu nǎo fēng bào\", \"trans\": \"brainstorming\"},\n    {\"word\": \"平台\", \"pinyin\": \"píng tài\", \"trans\": \"platform\"},\n    {\"word\": \"评估模型\", \"pinyin\": \"píng gū mó xíng\", \"trans\": \"evaluation model\"},\n    {\"word\": \"开放式\", \"pinyin\": \"kāi fàng shì\", \"trans\": \"open-ended\"}\n]",
        "trans": "This article discusses the advancements of Multimodal Large Language Models (MLLMs) in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge. Existing benchmarks, due to limitations in data volume and diversity, are insufficient for evaluating these methods. To address this issue, the authors propose GATE OpenING (OpenING), a comprehensive benchmark containing 5,400 high-quality, manually annotated instances covering 56 real-world tasks. OpenING encompasses a variety of everyday scenarios, such as travel guides, design, and brainstorming, providing a robust platform for challenging interleaved generation methods. Additionally, the authors introduce IntJudge, a model for evaluating open-ended multimodal generation methods.",
        "update_ts": "2024-12-03 09:11"
    }
}