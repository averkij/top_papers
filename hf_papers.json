{
    "date": {
        "ru": "21 Ğ¸ÑĞ»Ñ",
        "en": "July 21",
        "zh": "7æœˆ21æ—¥"
    },
    "time_utc": "2025-07-21 07:20",
    "weekday": 0,
    "issue_id": 4918,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.11097",
            "title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion\n  LLMs",
            "url": "https://huggingface.co/papers/2507.11097",
            "abstract": "DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA.",
            "score": 10,
            "issue_id": 4917,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "a104a398841ff4ff",
            "authors": [
                "Zichen Wen",
                "Jiashu Qu",
                "Dongrui Liu",
                "Zhiyuan Liu",
                "Ruixi Wu",
                "Yicun Yang",
                "Xiangqi Jin",
                "Haoyun Xu",
                "Xuyang Liu",
                "Weijia Li",
                "Chaochao Lu",
                "Jing Shao",
                "Conghui He",
                "Linfeng Zhang"
            ],
            "affiliations": [
                "EPIC Lab, Shanghai Jiao Tong University",
                "Shanghai AI Laboratory",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11097.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#alignment",
                    "#security"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜",
                    "desc": "DIJA - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ÑÑ‚ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ²Ğ¾Ğ´Ğ¾Ğ¼. DIJA ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DIJA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ³Ñ€Ğ¾Ğ·."
                },
                "en": {
                    "title": "Unmasking Vulnerabilities in Diffusion-Based Language Models with DIJA",
                    "desc": "DIJA is a novel framework that identifies and exploits safety vulnerabilities in diffusion-based large language models (dLLMs) through the use of adversarial prompts. It reveals that existing alignment mechanisms are inadequate in preventing harmful outputs when faced with context-aware, masked-input prompts. By leveraging the unique features of dLLMs, such as bidirectional modeling and parallel decoding, DIJA demonstrates how these models can produce unsafe content despite alignment efforts. The framework significantly outperforms previous methods, highlighting the critical need for improved safety measures in the design of dLLMs."
                },
                "zh": {
                    "title": "æ­ç¤ºæ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨è„†å¼±æ€§",
                    "desc": "DIJAæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å®‰å…¨å¼±ç‚¹ï¼Œé€šè¿‡æ„é€ å¯¹æŠ—æ€§æç¤ºï¼Œå±•ç¤ºäº†å…¶å¯¹é½æœºåˆ¶çš„æ˜¾è‘—è„†å¼±æ€§ã€‚å°½ç®¡æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’Œæ–‡æœ¬å¡«å……æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æˆ‘ä»¬å‘ç°ç°æœ‰çš„å¯¹é½æœºåˆ¶æ— æ³•æœ‰æ•ˆé˜²æŠ¤é’ˆå¯¹ä¸Šä¸‹æ–‡çš„å¯¹æŠ—æ€§æç¤ºã€‚DIJAé€šè¿‡æ„å»ºå¯¹æŠ—æ€§äº¤é”™æ©ç æ–‡æœ¬æç¤ºï¼Œåˆ©ç”¨äº†æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆæœºåˆ¶ï¼Œå¯¼è‡´æ ‡å‡†å¯¹é½æœºåˆ¶å¤±æ•ˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDIJAåœ¨æ­ç¤ºæ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹æ¶æ„ä¸­çš„æ–°å¨èƒæ–¹é¢ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¶Šç‹±æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12566",
            "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal\n  Large Language Models",
            "url": "https://huggingface.co/papers/2507.12566",
            "abstract": "Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.",
            "score": 4,
            "issue_id": 4917,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "fdd7f0b217aa40b2",
            "authors": [
                "Gen Luo",
                "Wenhan Dou",
                "Wenhao Li",
                "Zhaokai Wang",
                "Xue Yang",
                "Changyao Tian",
                "Hao Li",
                "Weiyun Wang",
                "Wenhai Wang",
                "Xizhou Zhu",
                "Yu Qiao",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12566.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#architecture",
                    "#agi",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Mono-InternVL: ĞœĞ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Mono-InternVL - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Mono-InternVL Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ½Ğ´Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (EViP) Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ MLLM, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Visual Learning with Mono-InternVL",
                    "desc": "The paper introduces Mono-InternVL, a monolithic Multimodal Large Language Model (MLLM) that combines visual and language processing into one model. It addresses issues like unstable optimization and catastrophic forgetting by embedding a new visual parameter space into a pre-trained language model, allowing for stable learning from noisy data. The model incorporates a multimodal mixture-of-experts architecture and an innovative Endogenous Visual Pre-training (EViP) strategy to enhance visual capabilities. Additionally, Mono-InternVL-1.5 is presented as a more efficient version that reduces training costs while maintaining competitive performance across multiple benchmarks."
                },
                "zh": {
                    "title": "Mono-InternVLï¼šé«˜æ•ˆçš„å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
                    "desc": "Mono-InternVLæ˜¯ä¸€ç§å…ˆè¿›çš„å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰ä¸“å®¶å’Œæ”¹è¿›çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºè§†è§‰å­¦ä¹ å¹¶é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†æ–°çš„è§†è§‰å‚æ•°ç©ºé—´åµŒå…¥åˆ°é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œè§£å†³äº†ä¸ç¨³å®šä¼˜åŒ–å’Œç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚Mono-InternVL-1.5åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œé‡‡ç”¨äº†æ”¹è¿›çš„å†…ç”Ÿè§†è§‰é¢„è®­ç»ƒï¼ˆEViP++ï¼‰ï¼Œå¼•å…¥äº†é¢å¤–çš„è§†è§‰æ³¨æ„åŠ›ä¸“å®¶ï¼Œå¹¶é«˜æ•ˆåœ°é‡æ–°ç»„ç»‡äº†é¢„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMono-InternVLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸”åœ¨æ¨ç†æ—¶æ˜¾è‘—é™ä½äº†å»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14137",
            "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation\n  Learning",
            "url": "https://huggingface.co/papers/2507.14137",
            "abstract": "Franca, an open-source vision foundation model, achieves high performance using a transparent training pipeline and novel clustering and disentanglement techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.",
            "score": 2,
            "issue_id": 4918,
            "pub_date": "2025-07-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ»Ñ",
                "en": "July 18",
                "zh": "7æœˆ18æ—¥"
            },
            "hash": "9435de131fc3d757",
            "authors": [
                "Shashanka Venkataramanan",
                "Valentinos Pariza",
                "Mohammadreza Salehi",
                "Lukas Knobel",
                "Spyros Gidaris",
                "Elias Ramzi",
                "Andrei Bursuc",
                "Yuki M. Asano"
            ],
            "affiliations": [
                "Fundamental AI Lab, UTN",
                "VIS Lab, UvA",
                "valeo.ai, Paris"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14137.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#optimization",
                    "#dataset",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Franca - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ImageNet-21K Ğ¸ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ReLAION-2B. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€ĞµÑˆĞµÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Franca: The Open-Source Vision Model Redefining Performance Standards",
                    "desc": "Franca is an open-source vision foundation model that achieves competitive performance with leading proprietary models by utilizing a transparent training pipeline. It employs innovative clustering and disentanglement techniques to enhance the quality of feature representations. The model addresses limitations in self-supervised learning (SSL) clustering methods by introducing a multi-head clustering projector that refines features into detailed clusters efficiently. Additionally, a positional disentanglement strategy is implemented to eliminate biases, resulting in improved semantic encoding and better performance on various benchmarks."
                },
                "zh": {
                    "title": "Francaï¼šå¼€æºè§†è§‰æ¨¡å‹çš„æ–°æ ‡å‡†",
                    "desc": "Francaæ˜¯ä¸€ä¸ªå¼€æºçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨é€æ˜çš„è®­ç»ƒæµç¨‹å’Œæ–°é¢–çš„èšç±»ä¸è§£ç¼ æŠ€æœ¯ï¼Œå–å¾—äº†é«˜æ€§èƒ½ã€‚å®ƒæ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨å¼€æºçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæ€§èƒ½ä¸è®¸å¤šæœ€å…ˆè¿›çš„ä¸“æœ‰æ¨¡å‹ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šå®ƒä»¬ã€‚Francaä½¿ç”¨å…¬å¼€å¯ç”¨çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºåµŒå¥—é©¬ç‰¹ryoshkaè¡¨ç¤ºçš„å¤šå¤´èšç±»æŠ•å½±å™¨ï¼Œä»¥æé«˜èšç±»çš„ç²¾ç¡®åº¦ã€‚é€šè¿‡æ¶ˆé™¤å¯†é›†è¡¨ç¤ºä¸­çš„ä½ç½®åå·®ï¼ŒFrancaåœ¨å¤šä¸ªä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸€è‡´çš„æå‡ï¼Œå±•ç¤ºäº†æ›´æ¸…æ™°ç‰¹å¾ç©ºé—´çš„å®ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.10605",
            "title": "RedOne: Revealing Domain-specific LLM Post-Training in Social Networking\n  Services",
            "url": "https://huggingface.co/papers/2507.10605",
            "abstract": "RedOne, a domain-specific LLM, enhances performance across multiple SNS tasks through a three-stage training strategy, improving generalization and reducing harmful content exposure.  \t\t\t\t\tAI-generated summary \t\t\t\t As a primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, a domain-specific LLM designed to break the performance bottleneck of single-task baselines and establish a comprehensive foundation for the SNS. RedOne was developed through a three-stage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using a large-scale real-world dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as a robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios.",
            "score": 2,
            "issue_id": 4914,
            "pub_date": "2025-07-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ»Ñ",
                "en": "July 13",
                "zh": "7æœˆ13æ—¥"
            },
            "hash": "0f03049bcdca7ad4",
            "authors": [
                "Fei Zhao",
                "Chonggang Lu",
                "Yue Wang",
                "Zheyong Xie",
                "Ziyan Liu",
                "Haofu Qian",
                "JianZhao Huang",
                "Fangcheng Shi",
                "Zijie Meng",
                "Hongcheng Guo",
                "Mingqian He",
                "Xinze Lyu",
                "Yiming Lu",
                "Ziyang Xiang",
                "Zheyu Ye",
                "Chengqiang Lu",
                "Zhe Xu",
                "Yi Wu",
                "Yao Hu",
                "Yan Gao",
                "Jun Fan",
                "Xiaolong Jiang",
                "Weiting Liu",
                "Boyang Wang",
                "Shaosheng Cao"
            ],
            "affiliations": [
                "NLP Team, Xiaohongshu Inc., China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.10605.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#optimization",
                    "#dataset",
                    "#multilingual",
                    "#science"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "RedOne: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…",
                    "desc": "RedOne - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 14.02% Ğ¿Ğ¾ 8 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ SNS Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. RedOne Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ»Ğ¸ĞºĞ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ´Ğ°Ñ‡Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ SNS Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "RedOne: Elevating SNS Performance with Domain-Specific LLMs",
                    "desc": "RedOne is a specialized large language model (LLM) designed to improve performance on various social networking service (SNS) tasks. It employs a three-stage training strategy that includes continued pretraining, supervised fine-tuning, and preference optimization, which helps it generalize better across different tasks. The model shows significant improvements, achieving up to 14.02% better performance on major SNS tasks and reducing harmful content exposure by 11.23%. Overall, RedOne demonstrates its effectiveness as a domain-specific LLM, enhancing interaction quality and content management in real-world SNS applications."
                },
                "zh": {
                    "title": "RedOneï¼šç¤¾äº¤ç½‘ç»œæœåŠ¡çš„å¼ºå¤§è¯­è¨€æ¨¡å‹",
                    "desc": "RedOneæ˜¯ä¸€ç§ç‰¹å®šé¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé€šè¿‡ä¸‰é˜¶æ®µçš„è®­ç»ƒç­–ç•¥æå‡äº†ç¤¾äº¤ç½‘ç»œæœåŠ¡ï¼ˆSNSï¼‰ä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹çš„è®­ç»ƒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œåå¥½ä¼˜åŒ–ï¼Œä½¿ç”¨äº†å¤§è§„æ¨¡çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRedOneåœ¨8ä¸ªä¸»è¦SNSä»»åŠ¡ä¸Šå¹³å‡æå‡äº†14.02%çš„æ€§èƒ½ï¼Œå¹¶åœ¨æœ‰å®³å†…å®¹æ£€æµ‹ä¸­å‡å°‘äº†11.23%çš„æ›å…‰ç‡ã€‚è¿™äº›æˆæœè¡¨æ˜RedOneåœ¨å¤šä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå®é™…åº”ç”¨åœºæ™¯ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-18.html",
    "link_next": "2025-07-22.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "18.07",
        "en": "07/18",
        "zh": "7æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "22.07",
        "en": "07/22",
        "zh": "7æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}