{
    "date": {
        "ru": "25 марта",
        "en": "March 25",
        "zh": "3月25日"
    },
    "time_utc": "2025-03-25 05:12",
    "weekday": 1,
    "issue_id": 2877,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.17359",
            "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
            "url": "https://huggingface.co/papers/2503.17359",
            "abstract": "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.",
            "score": 35,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "0046c940a41d8637",
            "authors": [
                "Jiwen Yu",
                "Yiran Qin",
                "Haoxuan Che",
                "Quande Liu",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Kuaishou",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17359.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Революция в разработке игр: ИИ-генерируемые миры будущего",
                    "desc": "Статья предлагает концепцию Генеративных Игровых Движков (GGE), основанных на Интерактивной Генеративной Видео технологии (IGV). GGE позволяет создавать неограниченный новый контент для игр следующего поколения, используя преимущества IGV в синтезе высококачественного контента, моделировании физики мира и интерактивности. Авторы представляют комплексную структуру основных модулей GGE и иерархическую дорожную карту зрелости (L0-L4) для его развития. Это исследование открывает новые перспективы для разработки игр в эпоху искусственного интеллекта."
                },
                "en": {
                    "title": "Revolutionizing Game Development with AI-Driven Generative Engines",
                    "desc": "This paper discusses the limitations of traditional game engines that rely on fixed content, which can hinder creativity and increase costs in game development. It introduces Interactive Generative Video (IGV) as a new approach to create Generative Game Engines (GGE), which can produce endless unique game content. GGE utilizes advanced features like high-quality content synthesis, physics-aware modeling, and user interactivity to enhance the gaming experience. The authors also outline a framework and roadmap for the development of GGE, aiming to transform the future of game creation through AI technologies."
                },
                "zh": {
                    "title": "AI驱动的游戏创作新纪元",
                    "desc": "现代游戏开发面临着创造力和成本的重大挑战，传统游戏引擎的内容预设限制了创新。最近，视频生成模型的突破使得合成逼真且互动的虚拟环境成为可能，这为游戏创作带来了革命性的机会。我们提出了互动生成视频（IGV）作为生成游戏引擎（GGE）的基础，能够在下一代游戏中实现无限的新内容生成。GGE利用IGV在高质量内容合成、物理感知世界建模、用户控制互动、长期记忆能力和因果推理等方面的独特优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18942",
            "title": "Video-T1: Test-Time Scaling for Video Generation",
            "url": "https://huggingface.co/papers/2503.18942",
            "abstract": "With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: https://liuff19.github.io/Video-T1",
            "score": 29,
            "issue_id": 2876,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "1d482b72d90d6136",
            "authors": [
                "Fangfu Liu",
                "Hanyang Wang",
                "Yimo Cai",
                "Kaiyan Zhang",
                "Xiaohang Zhan",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18942.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#games",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Масштабирование времени тестирования: новый подход к улучшению генерации видео",
                    "desc": "Статья исследует применение метода масштабирования времени тестирования (TTS) для улучшения качества генерации видео по текстовому описанию. Авторы представляют этот процесс как задачу поиска лучших траекторий от гауссова шума к целевому распределению видео. Они предлагают два подхода: линейный поиск с увеличением кандидатов шума и более эффективный метод Tree-of-Frames (ToF), который адаптивно расширяет и обрезает ветви видео авторегрессивным способом. Эксперименты показывают, что увеличение вычислительных ресурсов на этапе тестирования значительно улучшает качество генерируемых видео."
                },
                "en": {
                    "title": "Unlocking Video Quality with Test-Time Scaling",
                    "desc": "This paper explores the concept of Test-Time Scaling (TTS) in video generation, which allows models to utilize additional computational resources during inference to enhance video quality. Instead of focusing solely on training larger models, the authors investigate how increasing inference-time computation can improve the generation of videos from text prompts. They propose a method called Tree-of-Frames (ToF) that efficiently manages the search for better video outputs by adaptively expanding and pruning video branches. The results show that leveraging more computational power at test time leads to significant improvements in the quality of generated videos."
                },
                "zh": {
                    "title": "测试时间扩展：提升视频生成质量的新方法",
                    "desc": "随着训练数据、模型规模和计算成本的增加，视频生成在数字创作中取得了显著成果。本文探讨了在视频生成中应用测试时间扩展（TTS）的潜力，旨在提高生成质量。我们将测试时间扩展重新解释为一个搜索问题，通过从高斯噪声空间中采样更好的轨迹来生成目标视频。实验结果表明，增加测试时间计算可以显著提升视频质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18940",
            "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
            "url": "https://huggingface.co/papers/2503.18940",
            "abstract": "Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video resolution. While existing acceleration methods often compromise output quality or necessitate costly retraining, we observe that most diffusion models are pre-trained at lower resolutions, presenting an opportunity to exploit these low-resolution priors for more efficient inference without degrading performance. In this work, we introduce Bottleneck Sampling, a training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. Bottleneck Sampling follows a high-low-high denoising workflow: it performs high-resolution denoising in the initial and final stages while operating at lower resolutions in intermediate steps. To mitigate aliasing and blurring artifacts, we further refine the resolution transition points and adaptively shift the denoising timesteps at each stage. We evaluate Bottleneck Sampling on both image and video generation tasks, where extensive experiments demonstrate that it accelerates inference by up to 3times for image generation and 2.5times for video generation, all while maintaining output quality comparable to the standard full-resolution sampling process across multiple evaluation metrics. Code is available at: https://github.com/tyfeld/Bottleneck-Sampling",
            "score": 8,
            "issue_id": 2875,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "83ffcf1c20f5d4db",
            "authors": [
                "Ye Tian",
                "Xin Xia",
                "Yuxi Ren",
                "Shanchuan Lin",
                "Xing Wang",
                "Xuefeng Xiao",
                "Yunhai Tong",
                "Ling Yang",
                "Bin Cui"
            ],
            "affiliations": [
                "Bytedance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18940.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#inference",
                    "#video"
                ],
                "emoji": "⏱️",
                "ru": {
                    "title": "Ускорение диффузионных моделей без потери качества",
                    "desc": "Статья представляет новый метод ускорения работы диффузионных моделей под названием Bottleneck Sampling. Этот подход использует предобученные низкоразрешающие модели для уменьшения вычислительных затрат без потери качества выходных данных. Метод следует схеме высокое-низкое-высокое разрешение при денойзинге, что позволяет ускорить вывод в 2.5-3 раза для задач генерации изображений и видео. Bottleneck Sampling не требует переобучения модели и сохраняет качество результатов на уровне стандартного полноразрешающего семплирования."
                },
                "en": {
                    "title": "Speeding Up Diffusion Models with Bottleneck Sampling",
                    "desc": "This paper presents Bottleneck Sampling, a new method to speed up diffusion models used for generating images and videos. Traditional diffusion models are slow because they use a complex self-attention mechanism that increases with image resolution. Bottleneck Sampling takes advantage of low-resolution training data to reduce the computational load during inference without sacrificing quality. By using a high-low-high denoising approach, it achieves significant speed improvements—up to 3 times faster for images and 2.5 times for videos—while still producing high-quality outputs."
                },
                "zh": {
                    "title": "瓶颈采样：高效的扩散模型推理",
                    "desc": "扩散模型在视觉内容生成方面表现出色，但在推理时由于计算成本高而难以部署。主要的计算负担来自于自注意力机制在图像或视频分辨率上的二次复杂性。我们提出了一种名为瓶颈采样的框架，利用低分辨率的先验知识来减少计算开销，同时保持输出质量。通过在高分辨率和低分辨率之间进行高低高的去噪工作流程，我们的实验表明，该方法在图像生成中加速推理速度可达3倍，在视频生成中可达2.5倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17489",
            "title": "Judge Anything: MLLM as a Judge Across Any Modality",
            "url": "https://huggingface.co/papers/2503.17489",
            "abstract": "Evaluating generative foundation models on open-ended multimodal understanding (MMU) and generation (MMG) tasks across diverse modalities (e.g., images, audio, video) poses significant challenges due to the complexity of cross-modal interactions. To this end, the idea of utilizing Multimodal LLMs (MLLMs) as automated judges has emerged, with encouraging results in assessing vision-language understanding tasks. Moving further, this paper extends MLLM-as-a-Judge across modalities to a unified manner by introducing two benchmarks, TaskAnything and JudgeAnything, to respectively evaluate the overall performance and judging capabilities of MLLMs across any-to-any modality tasks. Specifically, TaskAnything evaluates the MMU and MMG capabilities across 15 any-to-any modality categories, employing 1,500 queries curated from well-established benchmarks. Furthermore, JudgeAnything evaluates the judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from the perspectives of Pair Comparison and Score Evaluation, providing a standardized testbed that incorporates human judgments and detailed rubrics. Our extensive experiments reveal that while these MLLMs show promise in assessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting and 42.79% in Score Evaluation setting), they encounter significant challenges with MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and 30.05% in Score Evaluation setting), exposing cross-modality biases and hallucination issues. To address this, we present OmniArena, an automated platform for evaluating omni-models and multimodal reward models. Our work highlights the need for fairer evaluation protocols and stronger alignment with human preferences. The source code and dataset are publicly available at: https://urrealhero.github.io/judgeanythingweb/.",
            "score": 7,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "bb040618997e1b0a",
            "authors": [
                "Shu Pu",
                "Yaochen Wang",
                "Dongping Chen",
                "Yuhang Chen",
                "Guohao Wang",
                "Qi Qin",
                "Zhongyi Zhang",
                "Zhiyuan Zhang",
                "Zetong Zhou",
                "Shuang Gong",
                "Yi Gui",
                "Yao Wan",
                "Philip S. Yu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17489.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#hallucinations",
                    "#benchmark",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Универсальная оценка мультимодальных ИИ-моделей: от понимания к генерации",
                    "desc": "Статья представляет новые бенчмарки TaskAnything и JudgeAnything для оценки мультимодальных языковых моделей (MLLM) в задачах понимания и генерации контента различных модальностей. TaskAnything оценивает способности MLLM в 15 категориях задач с различными комбинациями модальностей, используя 1500 запросов. JudgeAnything оценивает способности MLLM выступать в роли судей, сравнивая их оценки с человеческими по методикам попарного сравнения и балльной оценки. Результаты показывают, что MLLM лучше справляются с задачами понимания, чем с задачами генерации, выявляя проблемы межмодальных предубеждений и галлюцинаций."
                },
                "en": {
                    "title": "Enhancing Multimodal Evaluation with MLLMs",
                    "desc": "This paper discusses the challenges of evaluating generative foundation models in tasks that involve multiple types of data, like images and audio. It introduces Multimodal LLMs (MLLMs) as automated judges to assess these models' understanding and generation capabilities across different modalities. The authors present two benchmarks, TaskAnything and JudgeAnything, to systematically evaluate MLLMs' performance and judging abilities. The findings reveal that while MLLMs perform reasonably well in understanding tasks, they struggle with generation tasks, highlighting the need for improved evaluation methods and alignment with human preferences."
                },
                "zh": {
                    "title": "多模态评估的新视角",
                    "desc": "本论文探讨了在多模态理解（MMU）和生成（MMG）任务中评估生成基础模型的挑战，尤其是跨模态交互的复杂性。我们提出了使用多模态大语言模型（MLLMs）作为自动评估者的想法，并引入了两个基准：TaskAnything和JudgeAnything，分别用于评估MLLMs在任何模态任务中的整体性能和判断能力。实验结果显示，尽管MLLMs在MMU任务中表现出一定的潜力，但在MMG任务中面临显著挑战，暴露了跨模态偏见和幻觉问题。为了解决这些问题，我们提出了OmniArena，一个用于评估多模态模型和奖励模型的自动化平台。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18892",
            "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild",
            "url": "https://huggingface.co/papers/2503.18892",
            "abstract": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.",
            "score": 6,
            "issue_id": 2876,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "4c0c4ab2292562e4",
            "authors": [
                "Weihao Zeng",
                "Yuzhen Huang",
                "Qian Liu",
                "Wei Liu",
                "Keqing He",
                "Zejun Ma",
                "Junxian He"
            ],
            "affiliations": [
                "BUPT",
                "HKUST",
                "TikTok"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18892.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#rl",
                    "#training",
                    "#small_models"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Развитие рассуждений в языковых моделях через RL с нуля",
                    "desc": "Это исследование посвящено применению обучения с подкреплением (RL) без предварительной подготовки для развития способностей к рассуждениям у различных языковых моделей. Авторы провели эксперименты на 10 разных базовых моделях, включая LLama3, Mistral, DeepSeek-Math и Qwen2.5. Используя специальные стратегии, такие как настройка формата вознаграждения и контроль сложности запросов, удалось значительно улучшить точность рассуждений и длину ответов. Наблюдения показали, что разные модели демонстрируют различные паттерны в процессе обучения, причем увеличение длины ответа не всегда коррелирует с появлением определенных когнитивных способностей."
                },
                "en": {
                    "title": "Unlocking Reasoning with Zero RL Training",
                    "desc": "The paper discusses DeepSeek-R1, which demonstrates that long chain-of-thought reasoning can be developed using a simple reinforcement learning framework with rule-based rewards, starting directly from base models, a method called zero RL training. The authors explore zero RL training across ten different base models, revealing that many of these models already possess strong instruction-following and self-reflection capabilities. They implement design strategies to enhance reasoning accuracy and response length, while also noting that training dynamics vary significantly among models. Importantly, they identify the 'aha moment' in smaller models outside the Qwen family, providing insights and open-sourcing their findings to support further research."
                },
                "zh": {
                    "title": "零强化学习训练：推理与反思的新突破",
                    "desc": "DeepSeek-R1展示了通过简单的强化学习框架和基于规则的奖励，长链思维推理可以自然出现。这种训练方法被称为零强化学习训练，允许直接从基础模型开始。我们研究了10种不同的基础模型，发现它们在推理准确性和响应长度上都有显著提升。我们还观察到，不同模型在训练过程中表现出不同的模式，特别是小模型首次出现了“恍然大悟”的现象。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17439",
            "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
            "url": "https://huggingface.co/papers/2503.17439",
            "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines.",
            "score": 6,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "946d486485fedb03",
            "authors": [
                "Zhuoshi Pan",
                "Yu Li",
                "Honglin Lin",
                "Qizhi Pei",
                "Zinan Tang",
                "Wei Wu",
                "Chenlin Ming",
                "H. Vicky Zhao",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17439.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#reasoning",
                    "#dataset",
                    "#data"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Учимся на ошибках: новый подход к улучшению математических способностей ИИ",
                    "desc": "Эта статья предлагает метод LEMMA для улучшения способности больших языковых моделей (LLM) решать математические задачи путем обучения на ошибках. LEMMA создает набор данных, состоящий из неправильных решений с ошибочными шагами и связями с правильными решениями для дообучения модели. Авторы вводят метод аугментации ошибок на основе типов ошибок для сбора разнообразных и репрезентативных ошибок. Эксперименты показывают, что LEMMA значительно улучшает производительность по сравнению с другими сильными базовыми моделями."
                },
                "en": {
                    "title": "Empowering LLMs: Learning from Errors to Enhance Reasoning",
                    "desc": "This paper introduces a novel approach called Learning from Errors for Mathematical Advancement (LEMMA) to improve the reasoning capabilities of large language models (LLMs) in solving mathematical problems. Unlike traditional methods that focus solely on enhancing correct training data, LEMMA leverages the value of error data by constructing a dataset that includes incorrect solutions paired with reflections on correct solutions. The method systematically analyzes error types and employs a mistake augmentation technique to gather diverse errors, allowing the model to learn from its mistakes. By fine-tuning on this enriched dataset, LEMMA enables LLMs to autonomously correct their errors during the generation process, leading to significant performance gains compared to existing methods."
                },
                "zh": {
                    "title": "从错误中学习，提升数学推理能力",
                    "desc": "大型语言模型（LLMs）在解决数学问题时展现了出色的推理能力。现有的方法主要关注提高正确训练数据的质量，而忽视了错误数据的价值，这可能会妨碍模型的反思能力。我们提出了一种通过学习错误来提升数学推理能力的方法，称为LEMMA。该方法通过构建包含错误步骤的错误解和与正确解的反思连接的数据集，来进行模型的微调，从而使模型能够在生成过程中自主纠正错误。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18923",
            "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
            "url": "https://huggingface.co/papers/2503.18923",
            "abstract": "Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this gap, we introduce Video SimpleQA, the first comprehensive benchmark tailored for factuality evaluation of LVLMs. Our work distinguishes from existing video benchmarks through the following key features: 1) Knowledge required: demanding integration of external knowledge beyond the explicit narrative; 2) Fact-seeking question: targeting objective, undisputed events or relationships, avoiding subjective interpretation; 3) Definitive & short-form answer: Answers are crafted as unambiguous and definitively correct in a short format, enabling automated evaluation through LLM-as-a-judge frameworks with minimal scoring variance; 4) External-source verified: All annotations undergo rigorous validation against authoritative external references to ensure the reliability; 5) Temporal reasoning required: The annotated question types encompass both static single-frame understanding and dynamic temporal reasoning, explicitly evaluating LVLMs factuality under the long-context dependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize key findings as follows: 1) Current LVLMs exhibit notable deficiencies in factual adherence, particularly for open-source models. The best-performing model Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute paradigms show insignificant performance gains, revealing fundamental constraints for enhancing factuality through post-hoc computation; 3) Retrieval-Augmented Generation demonstrates consistent improvements at the cost of additional inference time overhead, presenting a critical efficiency-performance trade-off.",
            "score": 5,
            "issue_id": 2876,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "599abe342d833dd0",
            "authors": [
                "Meng Cao",
                "Pengfei Hu",
                "Yingyao Wang",
                "Jihao Gu",
                "Haoran Tang",
                "Haoze Zhao",
                "Jiahua Dong",
                "Wangbo Yu",
                "Ge Zhang",
                "Ian Reid",
                "Xiaodan Liang"
            ],
            "affiliations": [
                "Alibaba Group",
                "M-A-P",
                "MBZUAI",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18923.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#interpretability",
                    "#reasoning",
                    "#long_context",
                    "#multimodal",
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Новый стандарт оценки фактической точности видео-языковых моделей",
                    "desc": "Статья представляет Video SimpleQA - первый комплексный бенчмарк для оценки фактической точности Больших Видео-Языковых Моделей (LVLM). Бенчмарк отличается требованием интеграции внешних знаний, объективностью вопросов и верифицируемостью ответов. Оценка 41 современной LVLM показала значительные недостатки в фактической точности, при этом лучшая модель Gemini-1.5-Pro достигла F-меры всего 54.4%. Исследование выявило ограничения улучшения фактической точности через пост-обработку и компромисс между эффективностью и производительностью при использовании Retrieval-Augmented Generation."
                },
                "en": {
                    "title": "Evaluating Factual Accuracy in Video Language Models",
                    "desc": "This paper introduces Video SimpleQA, a new benchmark designed to evaluate the factual accuracy of Large Video Language Models (LVLMs). It focuses on assessing how well these models can integrate external knowledge and answer fact-based questions about video content. The benchmark emphasizes the need for definitive answers and includes rigorous validation against authoritative sources to ensure reliability. The evaluation of 41 LVLMs reveals significant shortcomings in factual adherence, particularly among open-source models, highlighting the challenges in improving factual accuracy in multi-modal contexts."
                },
                "zh": {
                    "title": "视频语言模型的事实性评估新基准",
                    "desc": "最近，大型视频语言模型（LVLMs）的进展显示了它们在多模态理解方面的潜力，但在视频上下文中评估其事实基础仍然是一个重要的未解决挑战。为了解决这个问题，我们引入了Video SimpleQA，这是第一个专门针对LVLMs事实性评估的综合基准。该基准的特点包括：需要整合外部知识、针对客观事件的问题、明确且简短的答案，以及经过外部来源验证的注释。我们对41个最先进的LVLMs进行了广泛评估，发现当前模型在事实遵循方面存在显著不足，尤其是开源模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18013",
            "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning",
            "url": "https://huggingface.co/papers/2503.18013",
            "abstract": "Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However, constructing high-quality human-annotated preference data and developing robust reward models to mimic these preferences are both costly and challenging. Motivated by this observation, we propose Vision-R1, a novel vision-guided R1-like reinforcement learning algorithm for LVLMs that rewards models with definitive vision feedback. It only leverages curated instruction data, eliminating the need for specialized reward models and handcrafted preference datasets. We incorporate a criterion-driven reward function that further integrates multi-dimensional feedback to evaluate model completions comprehensively based on the vision task logic. Furthermore, we introduce a progressive rule refinement strategy that dynamically adjusts the reward criteria during training, enabling continuous model improvement and mitigating reward hacking. Extensive experiments on both in-distribution and out-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with Vision-R1 achieves consistent performance gains, with even up to 50% improvement and surpassing the state-of-the-art 10x size model.",
            "score": 3,
            "issue_id": 2876,
            "pub_date": "2025-03-23",
            "pub_date_card": {
                "ru": "23 марта",
                "en": "March 23",
                "zh": "3月23日"
            },
            "hash": "45029d297f1b8ac9",
            "authors": [
                "Yufei Zhan",
                "Yousong Zhu",
                "Shurong Zheng",
                "Hongyin Zhao",
                "Fan Yang",
                "Ming Tang",
                "Jinqiao Wang"
            ],
            "affiliations": [
                "Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Peng Cheng Laboratory, Shenzhen, China",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
                "Wuhan AI Research, Wuhan, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18013.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Vision-R1: Революция в обучении визуально-языковых моделей без ручной разметки",
                    "desc": "В статье представлен новый алгоритм обучения с подкреплением для крупных визуально-языковых моделей под названием Vision-R1. Этот метод использует обратную связь на основе зрения для улучшения моделей, не требуя специальных наборов данных о предпочтениях или моделей вознаграждения. Vision-R1 включает функцию вознаграждения на основе критериев и стратегию прогрессивного уточнения правил. Эксперименты показывают значительное улучшение производительности моделей, обученных с помощью Vision-R1, в некоторых случаях превосходящее модели в 10 раз большего размера."
                },
                "en": {
                    "title": "Reinforcing Vision with Vision-R1: Simplifying LVLM Training",
                    "desc": "This paper introduces Vision-R1, a new reinforcement learning algorithm designed for Large Vision-Language Models (LVLMs). Unlike traditional methods that require expensive human-annotated preference data, Vision-R1 uses curated instruction data to provide vision feedback directly to the models. The algorithm employs a criterion-driven reward function that assesses model outputs based on the logic of vision tasks, allowing for a more comprehensive evaluation. Additionally, it features a progressive rule refinement strategy that adapts reward criteria during training, leading to significant performance improvements in LVLMs without the need for complex reward models."
                },
                "zh": {
                    "title": "视觉引导的强化学习提升模型能力",
                    "desc": "大型视觉语言模型（LVLMs）通常采用两阶段训练方法：预训练和监督微调。最近，源自语言领域的偏好优化成为一种有效的后训练强化策略，用于提升LVLMs的能力。我们提出了一种新颖的视觉引导R1类强化学习算法Vision-R1，它通过明确的视觉反馈来奖励模型，避免了构建高质量人类标注的偏好数据和开发复杂的奖励模型的高成本。通过引入多维反馈的标准驱动奖励函数，Vision-R1能够全面评估模型的完成情况，并在训练过程中动态调整奖励标准，从而实现持续的模型改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18945",
            "title": "Aether: Geometric-Aware Unified World Modeling",
            "url": "https://huggingface.co/papers/2503.18945",
            "abstract": "The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video prediction, and (3) goal-conditioned visual planning. Through task-interleaved feature learning, Aether achieves synergistic knowledge sharing across reconstruction, prediction, and planning objectives. Building upon video generation models, our framework demonstrates unprecedented synthetic-to-real generalization despite never observing real-world data during training. Furthermore, our approach achieves zero-shot generalization in both action following and reconstruction tasks, thanks to its intrinsic geometric modeling. Remarkably, even without real-world data, its reconstruction performance far exceeds that of domain-specific models. Additionally, Aether leverages a geometry-informed action space to seamlessly translate predictions into actions, enabling effective autonomous trajectory planning. We hope our work inspires the community to explore new frontiers in physically-reasonable world modeling and its applications.",
            "score": 2,
            "issue_id": 2877,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "e9f70faf8bc6d0d0",
            "authors": [
                "Aether Team",
                "Haoyi Zhu",
                "Yifan Wang",
                "Jianjun Zhou",
                "Wenzheng Chang",
                "Yang Zhou",
                "Zizun Li",
                "Junyi Chen",
                "Chunhua Shen",
                "Jiangmiao Pang",
                "Tong He"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18945.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#reasoning",
                    "#agents",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Единая система для геометрического моделирования мира и планирования действий",
                    "desc": "Статья представляет Aether - унифицированную систему для геометрически-осознанного рассуждения в моделях мира. Aether объединяет три ключевые возможности: 4D динамическую реконструкцию, предсказание видео с учетом действий и визуальное планирование с учетом целей. Благодаря совместному обучению признаков для различных задач, система демонстрирует беспрецедентную генерализацию с синтетических данных на реальные. Aether также показывает способность к обобщению без дополнительного обучения в задачах следования действиям и реконструкции."
                },
                "en": {
                    "title": "Aether: Bridging Geometry and Generative Modeling for AI Spatial Reasoning",
                    "desc": "This paper introduces Aether, a framework that combines geometric reconstruction with generative modeling to enhance AI's spatial reasoning abilities. Aether optimizes three main functions: dynamic 4D reconstruction, action-based video prediction, and goal-oriented visual planning. By using task-interleaved feature learning, it allows for effective knowledge sharing among these functions, leading to improved performance. Notably, Aether achieves strong generalization to real-world scenarios without ever training on real data, showcasing its potential for autonomous trajectory planning and physical world modeling."
                },
                "zh": {
                    "title": "Aether：实现几何感知推理的统一框架",
                    "desc": "本论文提出了Aether框架，旨在解决几何重建与生成建模的整合问题，以实现类人空间推理。Aether通过联合优化四个核心能力，包括4D动态重建、基于动作的视频预测和基于目标的视觉规划，来实现几何感知推理。该框架通过任务交错特征学习，促进了重建、预测和规划目标之间的知识共享。尽管在训练过程中未观察到真实世界数据，Aether仍展现出前所未有的合成到真实的泛化能力，且在无监督情况下在动作跟随和重建任务中实现了零样本泛化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18866",
            "title": "Reasoning to Learn from Latent Thoughts",
            "url": "https://huggingface.co/papers/2503.18866",
            "abstract": "Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining.",
            "score": 2,
            "issue_id": 2876,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "ab963a9dd28b0934",
            "authors": [
                "Yangjun Ruan",
                "Neil Band",
                "Chris J. Maddison",
                "Tatsunori Hashimoto"
            ],
            "affiliations": [
                "Stanford University",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18866.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#transfer_learning",
                    "#synthetic",
                    "#data",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрытие скрытых мыслей для эффективного обучения языковых моделей",
                    "desc": "Статья предлагает метод повышения эффективности предобучения языковых моделей в условиях ограниченных данных. Авторы предлагают моделировать и выводить скрытые мысли, лежащие в основе процесса генерации текста. Этот подход рассматривает веб-текст как сжатый результат подробного мыслительного процесса человека. Эмпирические результаты показывают значительное улучшение эффективности обучения, особенно в области математики."
                },
                "en": {
                    "title": "Unlocking Data Efficiency through Latent Thought Inference",
                    "desc": "This paper addresses the challenge of limited human-written text data for training large language models (LMs). It proposes a method to model and infer the underlying thoughts that lead to text generation, which can enhance data efficiency during pretraining. By treating web text as a condensed version of human thought processes, the authors show that inferring these latent thoughts can lead to better learning outcomes, especially in data-scarce situations. Their experiments demonstrate that this approach not only improves performance on tasks like math but also allows LMs to iteratively enhance their own capabilities without relying heavily on external data."
                },
                "zh": {
                    "title": "潜在思维推断提升语言模型预训练效率",
                    "desc": "这篇论文探讨了在语言模型预训练中，数据增长速度慢于模型规模扩展的问题。作者提出通过显式建模和推断文本生成过程中的潜在思维，可以显著提高预训练的数据效率。研究表明，合成数据方法在推断潜在思维方面的应用，能够在数据受限的情况下，提升模型的学习效果。通过迭代的EM算法，模型能够自我提升性能，并在多个迭代中显著超越仅使用原始数据训练的基线模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14428",
            "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
            "url": "https://huggingface.co/papers/2503.14428",
            "abstract": "Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we propose MagicComp, a training-free method that enhances compositional T2V generation through dual-phase refinement. Specifically, (1) During the Conditioning Stage: We introduce the Semantic Anchor Disambiguation to reinforces subject-specific semantics and resolve inter-subject ambiguity by progressively injecting the directional vectors of semantic anchors into original text embedding; (2) During the Denoising Stage: We propose Dynamic Layout Fusion Attention, which integrates grounding priors and model-adaptive spatial perception to flexibly bind subjects to their spatiotemporal regions through masked attention modulation. Furthermore, MagicComp is a model-agnostic and versatile approach, which can be seamlessly integrated into existing T2V architectures. Extensive experiments on T2V-CompBench and VBench demonstrate that MagicComp outperforms state-of-the-art methods, highlighting its potential for applications such as complex prompt-based and trajectory-controllable video generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
            "score": 2,
            "issue_id": 2875,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 марта",
                "en": "March 18",
                "zh": "3月18日"
            },
            "hash": "1cd532518024f266",
            "authors": [
                "Hongyu Zhang",
                "Yufan Deng",
                "Shenghai Yuan",
                "Peng Jin",
                "Zesen Cheng",
                "Yian Zhao",
                "Chang Liu",
                "Jie Chen"
            ],
            "affiliations": [
                "Peng Cheng Laboratory, Shenzhen, China",
                "School of Electronic and Computer Engineering, Peking University, Shenzhen, China",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14428.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#architecture",
                    "#games",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MagicComp: Усовершенствованная генерация видео по тексту без дополнительного обучения",
                    "desc": "MagicComp - это метод генерации видео по тексту, не требующий дополнительного обучения. Он использует двухфазовое уточнение для улучшения композиционной генерации: семантическое разрешение неоднозначности на этапе подготовки условий и динамическое слияние макетов на этапе шумоподавления. Метод решает проблемы связывания атрибутов, определения пространственных отношений и захвата сложных взаимодействий между несколькими объектами. MagicComp может быть интегрирован в существующие архитектуры генерации видео по тексту и превосходит современные методы в экспериментах."
                },
                "en": {
                    "title": "Enhancing Text-to-Video Generation with MagicComp",
                    "desc": "This paper presents MagicComp, a novel method for improving text-to-video (T2V) generation using diffusion models. It addresses challenges in accurately linking attributes and understanding spatial relationships between subjects in videos. The method consists of two main phases: the Conditioning Stage, which clarifies subject semantics using Semantic Anchor Disambiguation, and the Denoising Stage, which employs Dynamic Layout Fusion Attention to enhance spatial binding. MagicComp is designed to be adaptable and can be integrated into existing T2V systems, showing superior performance in various benchmarks."
                },
                "zh": {
                    "title": "MagicComp：提升文本到视频生成的创新方法",
                    "desc": "本文提出了一种名为MagicComp的文本到视频生成方法，旨在解决现有方法在属性绑定、空间关系确定和复杂动作交互方面的不足。该方法通过双阶段的精炼过程来增强组合式T2V生成，首先在条件阶段引入语义锚点消歧，以强化特定主题的语义并解决主题间的歧义。其次，在去噪阶段，提出动态布局融合注意力，通过掩蔽注意力调制灵活绑定主题与其时空区域。MagicComp是一种与模型无关的通用方法，可以无缝集成到现有的T2V架构中，并在多个基准测试中表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18908",
            "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
            "url": "https://huggingface.co/papers/2503.18908",
            "abstract": "We introduce FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71X speedup in inference latency and 35X lower per-token cost while maintaining strong performance across benchmarks. Through extensive experiments on models from 49B to 253B parameters, we demonstrate that FFN Fusion becomes increasingly effective at larger scales and can complement existing optimization techniques like quantization and pruning. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.",
            "score": 1,
            "issue_id": 2877,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "77fd55a50b93d2d4",
            "authors": [
                "Akhiad Bercovich",
                "Mohammad Dabbah",
                "Omri Puny",
                "Ido Galil",
                "Amnon Geifman",
                "Yonatan Geifman",
                "Izhak Golan",
                "Ehud Karpas",
                "Itay Levy",
                "Zach Moshe",
                "Najeeb Nabwani",
                "Tomer Ronen",
                "Itamar Schen",
                "Elad Segal",
                "Ido Shahaf",
                "Oren Tropp",
                "Ran Zilberstein",
                "Ran El-Yaniv"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18908.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение больших языковых моделей через параллелизацию FFN слоев",
                    "desc": "Статья представляет технику оптимизации архитектуры под названием FFN Fusion, которая сокращает последовательные вычисления в больших языковых моделях. Метод идентифицирует последовательности слоев Feed-Forward Network (FFN), которые можно распараллелить с минимальным влиянием на точность модели. Применение этой техники к модели Llama-3.1-405B-Instruct позволило создать Llama-Nemotron-Ultra-253B-Base, достигающую ускорения в 1.71 раза при сохранении производительности. Исследования показали, что FFN Fusion особенно эффективен для крупных моделей и может сочетаться с другими методами оптимизации, такими как квантизация и прунинг."
                },
                "en": {
                    "title": "Accelerating Language Models with FFN Fusion",
                    "desc": "The paper presents FFN Fusion, a technique that optimizes large language models by enabling parallel computation of Feed-Forward Network (FFN) layers. This method identifies sequences of FFN layers that can be fused and executed in parallel, which reduces the time it takes for the model to make predictions without sacrificing accuracy. The authors demonstrate this approach on the Llama-3.1-405B-Instruct model, resulting in a new model, Llama-Nemotron-Ultra-253B-Base, that is significantly faster and cheaper to run. The findings suggest that FFN Fusion is particularly beneficial for larger models and can work alongside other optimization methods like quantization and pruning."
                },
                "zh": {
                    "title": "FFN Fusion：提升大型语言模型的推理效率",
                    "desc": "我们提出了一种名为FFN Fusion的架构优化技术，旨在通过识别和利用并行化的自然机会来减少大型语言模型中的顺序计算。我们的关键见解是，去除特定注意力层后，前馈网络（FFN）层的序列通常可以以最小的准确性影响进行并行化。我们开发了一种原则性的方法来识别和融合这些序列，将其转化为并行操作，从而显著降低推理延迟，同时保持模型行为。通过将这些技术应用于Llama-3.1-405B-Instruct，我们创建了Llama-Nemotron-Ultra-253B-Base（Ultra-253B-Base），该模型在推理延迟上实现了1.71倍的加速，并且每个token的成本降低了35倍，同时在基准测试中保持了强劲的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18769",
            "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
            "url": "https://huggingface.co/papers/2503.18769",
            "abstract": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet.",
            "score": 1,
            "issue_id": 2875,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "e92ee9df78b66019",
            "authors": [
                "Alan Dao",
                "Dinh Bach Vu",
                "Bui Quang Huy"
            ],
            "affiliations": [
                "Menlo Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18769.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#reasoning",
                    "#3d"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AlphaSpace: Прорыв в пространственном мышлении ИИ",
                    "desc": "AlphaSpace - это новая методология, разработанная для улучшения пространственного мышления больших языковых моделей (LLM) в навигации по 3D декартовому пространству. Она использует токенизацию на основе семантики, кодируя информацию о высоте через специальные семантические токены, и интегрирует преимущественно символические синтетические данные для рассуждений. Этот подход позволяет LLM точно манипулировать объектами, позиционируя их по конкретным координатам [x, y, z]. Экспериментальные результаты показывают, что AlphaSpace значительно превосходит существующие модели в подзадачах манипулирования, достигая общей точности 66,67%."
                },
                "en": {
                    "title": "Enhancing 3D Navigation in Language Models with AlphaSpace",
                    "desc": "This paper introduces AlphaSpace, a new method aimed at improving how large language models (LLMs) understand and navigate 3D spaces. It uses a unique tokenization method that incorporates height information through special semantic tokens, allowing for better spatial reasoning. By combining this with symbolic reasoning data, AlphaSpace enables LLMs to effectively manipulate objects in a 3D environment by placing them at precise coordinates. The results show that AlphaSpace achieves a notable accuracy of 66.67% in manipulation tasks, outperforming other models like GPT-4o and Claude 3.5 Sonnet."
                },
                "zh": {
                    "title": "AlphaSpace：提升语言模型的空间推理能力",
                    "desc": "本文介绍了一种新方法AlphaSpace，旨在提升大型语言模型（LLMs）在三维笛卡尔空间导航中的空间推理能力。AlphaSpace采用基于语义的标记化策略，通过专门的语义标记编码高度信息，并主要整合符号合成推理数据。该方法使得LLMs能够准确地通过特定的[x, y, z]坐标来操作物体。实验结果表明，AlphaSpace在操作子任务上显著优于现有模型，总准确率达到66.67%，而GPT-4o为37.5%，Claude 3.5 Sonnet为29.17%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18559",
            "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
            "url": "https://huggingface.co/papers/2503.18559",
            "abstract": "Text-to-Video (T2V) generation has attracted significant attention for its ability to synthesize realistic videos from textual descriptions. However, existing models struggle to balance computational efficiency and high visual quality, particularly on resource-limited devices, e.g.,iGPUs and mobile phones. Most prior work prioritizes visual fidelity while overlooking the need for smaller, more efficient models suitable for real-world deployment. To address this challenge, we propose a lightweight T2V framework, termed Hummingbird, which prunes existing models and enhances visual quality through visual feedback learning. Our approach reduces the size of the U-Net from 1.4 billion to 0.7 billion parameters, significantly improving efficiency while preserving high-quality video generation. Additionally, we introduce a novel data processing pipeline that leverages Large Language Models (LLMs) and Video Quality Assessment (VQA) models to enhance the quality of both text prompts and video data. To support user-driven training and style customization, we publicly release the full training code, including data processing and model training. Extensive experiments show that our method achieves a 31X speedup compared to state-of-the-art models such as VideoCrafter2, while also attaining the highest overall score on VBench. Moreover, our method supports the generation of videos with up to 26 frames, addressing the limitations of existing U-Net-based methods in long video generation. Notably, the entire training process requires only four GPUs, yet delivers performance competitive with existing leading methods. Hummingbird presents a practical and efficient solution for T2V generation, combining high performance, scalability, and flexibility for real-world applications.",
            "score": 1,
            "issue_id": 2877,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "f6ded1274ae1fbf4",
            "authors": [
                "Takashi Isobe",
                "He Cui",
                "Dong Zhou",
                "Mengmeng Ge",
                "Dong Li",
                "Emad Barsoum"
            ],
            "affiliations": [
                "Advanced Micro Devices, Inc.",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18559.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#video",
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#small_models",
                    "#data"
                ],
                "emoji": "🐦",
                "ru": {
                    "title": "Эффективная генерация видео по тексту для мобильных устройств",
                    "desc": "Статья представляет легковесную модель генерации видео по тексту под названием Hummingbird. Авторы уменьшили размер U-Net с 1,4 до 0,7 миллиардов параметров, сохранив при этом высокое качество генерации. В работе используются большие языковые модели (LLM) и модели оценки качества видео (VQA) для улучшения текстовых запросов и видеоданных. Модель Hummingbird показывает 31-кратное ускорение по сравнению с современными аналогами и поддерживает генерацию видео длиной до 26 кадров."
                },
                "en": {
                    "title": "Hummingbird: Efficient Text-to-Video Generation for Real-World Use",
                    "desc": "This paper introduces Hummingbird, a lightweight framework for Text-to-Video (T2V) generation that aims to improve efficiency without sacrificing visual quality. By pruning the U-Net model from 1.4 billion to 0.7 billion parameters, Hummingbird achieves a significant speedup of 31 times compared to existing models like VideoCrafter2. The framework also incorporates a novel data processing pipeline that utilizes Large Language Models and Video Quality Assessment to enhance both text prompts and video data. With the ability to generate videos with up to 26 frames and requiring only four GPUs for training, Hummingbird offers a scalable and practical solution for real-world T2V applications."
                },
                "zh": {
                    "title": "轻量级文本到视频生成的高效解决方案",
                    "desc": "本文提出了一种轻量级的文本到视频生成框架，称为Hummingbird，旨在提高视频生成的效率和视觉质量。该框架通过剪枝现有模型，将U-Net的参数从14亿减少到7亿，从而在保持高质量视频生成的同时显著提高了计算效率。我们还引入了一种新的数据处理流程，利用大型语言模型和视频质量评估模型来提升文本提示和视频数据的质量。实验结果表明，Hummingbird在速度和性能上均优于现有的最先进模型，适用于资源有限的设备。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17735",
            "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation",
            "url": "https://huggingface.co/papers/2503.17735",
            "abstract": "Recently, great progress has been made in video generation technology, attracting the widespread attention of scholars. To apply this technology to downstream applications under resource-constrained conditions, researchers usually fine-tune the pre-trained models based on parameter-efficient tuning methods such as Adapter or Lora. Although these methods can transfer the knowledge from the source domain to the target domain, fewer training parameters lead to poor fitting ability, and the knowledge from the source domain may lead to the inference process deviating from the target domain. In this paper, we argue that under constrained resources, training a smaller video generation model from scratch using only million-level samples can outperform parameter-efficient tuning on larger models in downstream applications: the core lies in the effective utilization of data and curriculum strategy. Take animated sticker generation (ASG) as a case study, we first construct a discrete frame generation network for stickers with low frame rates, ensuring that its parameters meet the requirements of model training under constrained resources. In order to provide data support for models trained from scratch, we come up with a dual-mask based data utilization strategy, which manages to improve the availability and expand the diversity of limited data. To facilitate convergence under dual-mask situation, we propose a difficulty-adaptive curriculum learning method, which decomposes the sample entropy into static and adaptive components so as to obtain samples from easy to difficult. The experiment demonstrates that our resource-efficient dual-mask training framework is quantitatively and qualitatively superior to efficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the feasibility of our method on downstream tasks under constrained resources. Code will be available.",
            "score": 1,
            "issue_id": 2876,
            "pub_date": "2025-03-22",
            "pub_date_card": {
                "ru": "22 марта",
                "en": "March 22",
                "zh": "3月22日"
            },
            "hash": "186b92c438925eb6",
            "authors": [
                "Zhiqiang Yuan",
                "Ting Zhang",
                "Ying Deng",
                "Jiapei Zhang",
                "Yeshuang Zhu",
                "Zexi Jia",
                "Jie Zhou",
                "Jinchao Zhang"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17735.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#transfer_learning"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Эффективная генерация видео с нуля вместо тонкой настройки больших моделей",
                    "desc": "Статья представляет новый подход к генерации видео в условиях ограниченных ресурсов. Авторы предлагают обучать с нуля небольшую модель на миллионах образцов вместо тонкой настройки больших предобученных моделей. Ключевые элементы подхода включают эффективное использование данных с помощью стратегии двойной маски и адаптивное обучение по учебной программе. Эксперименты на задаче генерации анимированных стикеров показывают превосходство предложенного метода над существующими подходами."
                },
                "en": {
                    "title": "Train Small, Win Big: Efficient Video Generation Under Constraints",
                    "desc": "This paper discusses advancements in video generation technology and its application in resource-limited environments. It challenges the effectiveness of parameter-efficient tuning methods like Adapter and Lora, suggesting that training smaller models from scratch can yield better results with limited data. The authors introduce a dual-mask data utilization strategy to enhance data diversity and a difficulty-adaptive curriculum learning method to improve model training. Their experiments show that this new approach outperforms existing tuning methods, demonstrating its potential for downstream applications."
                },
                "zh": {
                    "title": "资源受限下的视频生成新策略",
                    "desc": "最近，视频生成技术取得了显著进展，吸引了学者们的广泛关注。为了在资源受限的条件下应用这一技术，研究人员通常基于参数高效的调优方法对预训练模型进行微调。本文提出在资源受限的情况下，从头开始训练一个较小的视频生成模型，使用百万级样本，能够在下游应用中超越大型模型的参数高效调优。我们通过构建低帧率贴纸的离散帧生成网络和双掩码数据利用策略，结合难度自适应的课程学习方法，显著提高了模型的训练效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17422",
            "title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms",
            "url": "https://huggingface.co/papers/2503.17422",
            "abstract": "The recent exponential growth of Large Language Models (LLMs) has relied on GPU-based systems. However, CPUs are emerging as a flexible and lower-cost alternative, especially when targeting inference and reasoning workloads. RISC-V is rapidly gaining traction in this area, given its open and vendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the corresponding software ecosystem are not fully mature and streamlined, given the requirement of domain-specific tuning. This paper aims at filling this gap, focusing on optimizing LLM inference on the Sophon SG2042, the first commercially available many-core RISC-V CPU with vector processing capabilities.   On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1 Distill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s for token generation and 6.54/3.68 token/s for prompt processing, with a speed up of up 2.9x/3.0x compared to our baseline.",
            "score": 1,
            "issue_id": 2876,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "3811c1f2a2e12813",
            "authors": [
                "Javier J. Poveda Rodrigo",
                "Mohamed Amine Ahmdi",
                "Alessio Burrello",
                "Daniele Jahier Pagliari",
                "Luca Benini"
            ],
            "affiliations": [
                "DAUIN, Politecnico of Turin, Turin, Italy",
                "ETHZ, Zurich, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17422.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение языковых моделей на RISC-V процессорах",
                    "desc": "Данная статья посвящена оптимизации инференса больших языковых моделей (LLM) на процессорах RISC-V, в частности на Sophon SG2042. Авторы исследуют возможности использования CPU как альтернативы GPU для задач обработки естественного языка. В работе представлены результаты оптимизации двух современных LLM моделей - DeepSeek R1 Distill Llama 8B и DeepSeek R1 Distill QWEN 14B. Достигнуто значительное ускорение инференса по сравнению с базовой реализацией, до 2.9-3.0 раз."
                },
                "en": {
                    "title": "Unlocking LLM Potential with RISC-V CPUs",
                    "desc": "This paper discusses the potential of using CPUs, specifically RISC-V architecture, for optimizing Large Language Model (LLM) inference. It highlights the advantages of RISC-V, such as its flexibility and cost-effectiveness, especially for reasoning tasks. The authors focus on the Sophon SG2042, a many-core RISC-V CPU, and demonstrate significant performance improvements in token generation and prompt processing for two advanced LLMs. The results show up to 3x speed improvements compared to traditional systems, indicating a promising direction for LLM deployment on CPU architectures."
                },
                "zh": {
                    "title": "用RISC-V优化大型语言模型推理",
                    "desc": "近年来，大型语言模型（LLMs）的快速发展依赖于基于GPU的系统。然而，CPU作为一种灵活且成本更低的替代方案，正在逐渐崭露头角，特别是在推理和推断工作负载方面。RISC-V因其开放和中立的指令集架构（ISA）而在这一领域迅速获得关注。本文旨在优化在Sophon SG2042上进行LLM推理，展示了在两个最新的优化推理模型上实现的显著性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16924",
            "title": "Optimized Minimal 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2503.16924",
            "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for real-time, high-performance rendering, enabling a wide range of applications. However, representing 3D scenes with numerous explicit Gaussian primitives imposes significant storage and memory overhead. Recent studies have shown that high-quality rendering can be achieved with a substantially reduced number of Gaussians when represented with high-precision attributes. Nevertheless, existing 3DGS compression methods still rely on a relatively large number of Gaussians, focusing primarily on attribute compression. This is because a smaller set of Gaussians becomes increasingly sensitive to lossy attribute compression, leading to severe quality degradation. Since the number of Gaussians is directly tied to computational costs, it is essential to reduce the number of Gaussians effectively rather than only optimizing storage. In this paper, we propose Optimized Minimal Gaussians representation (OMG), which significantly reduces storage while using a minimal number of primitives. First, we determine the distinct Gaussian from the near ones, minimizing redundancy without sacrificing quality. Second, we propose a compact and precise attribute representation that efficiently captures both continuity and irregularity among primitives. Additionally, we propose a sub-vector quantization technique for improved irregularity representation, maintaining fast training with a negligible codebook size. Extensive experiments demonstrate that OMG reduces storage requirements by nearly 50% compared to the previous state-of-the-art and enables 600+ FPS rendering while maintaining high rendering quality. Our source code is available at https://maincold2.github.io/omg/.",
            "score": 0,
            "issue_id": 2877,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "280ac899f8c492d0",
            "authors": [
                "Joo Chan Lee",
                "Jong Hwan Ko",
                "Eunbyung Park"
            ],
            "affiliations": [
                "Sungkyunkwan University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16924.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#inference",
                    "#open_source"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "OMG: Оптимизация 3D Gaussian Splatting для эффективного рендеринга",
                    "desc": "В этой статье представлен метод Optimized Minimal Gaussians (OMG) для оптимизации 3D Gaussian Splatting. OMG значительно сокращает требования к хранению данных и использует минимальное количество примитивов, сохраняя при этом высокое качество рендеринга. Метод включает определение различных гауссиан среди близких и компактное представление атрибутов, эффективно capturing непрерывность и нерегулярность примитивов. Авторы также предлагают технику субвекторной квантизации для улучшенного представления нерегулярности, сохраняя быстрое обучение с незначительным размером кодовой книги."
                },
                "en": {
                    "title": "Streamlining 3D Rendering with Minimal Gaussians",
                    "desc": "This paper introduces the Optimized Minimal Gaussians (OMG) representation, which aims to enhance 3D Gaussian Splatting (3DGS) by significantly reducing the number of Gaussian primitives needed for high-quality rendering. The authors focus on minimizing redundancy among similar Gaussians while ensuring that the quality of the rendered scenes is preserved. They also present a compact attribute representation that captures both smooth and irregular features of the 3D scene, along with a sub-vector quantization method to improve efficiency. The results show that OMG can cut storage requirements by nearly 50% and achieve over 600 frames per second in rendering without compromising quality."
                },
                "zh": {
                    "title": "优化最小高斯表示，提升渲染效率！",
                    "desc": "3D高斯点云表示（3DGS）是一种用于实时高性能渲染的强大方法，但使用大量显式高斯原语会导致存储和内存开销大。本文提出了一种优化的最小高斯表示（OMG），通过减少高斯数量来显著降低存储需求，同时保持高渲染质量。我们通过识别相似高斯来减少冗余，并提出了一种紧凑的属性表示方法，以有效捕捉原语之间的连续性和不规则性。此外，我们还引入了一种子向量量化技术，以提高不规则性的表示效果。"
                }
            }
        }
    ],
    "link_prev": "2025-03-24.html",
    "link_next": "2025-03-26.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "24.03",
        "en": "03/24",
        "zh": "3月24日"
    },
    "short_date_next": {
        "ru": "26.03",
        "en": "03/26",
        "zh": "3月26日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 3,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 5,
        "#3d": 3,
        "#audio": 0,
        "#video": 8,
        "#multimodal": 4,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了多模态科学问题（MSPs）及其挑战。MSPs需要整合多种模态，如文本和图表。目前，MSPs面临两大问题：多模态推理和缺乏反思能力。作者提出了基于大七人格和苏格拉底指导的多代理框架（MAPS）来解决这些问题。实验结果显示，该框架在多个数据集上表现出色，超越了现有的最佳模型。",
        "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
        "pinyin": "这篇文章讨论了多模态科学问题（MSPs）及其挑战。\nZhè piān wénzhāng tǎolùn le duō móshuài kēxué wèntí (MSPs) jí qí tiǎozhàn.\n\nMSPs需要整合多种模态，如文本和图表。\nMSPs xūyào zhěnghé duō zhǒng móshuài, rú wénběn hé túbiǎo.\n\n目前，MSPs面临两大问题：多模态推理和缺乏反思能力。\nMùqián, MSPs miànlín liǎng dà wèntí: duō móshuài tuīlǐ hé quēfá fǎnsī nénglì.\n\n作者提出了基于大七人格和苏格拉底指导的多代理框架（MAPS）来解决这些问题。\nZuòzhě tíchū le jīyú dà qī réngé hé Sūgélādǐ zhǐdǎo de duō dàilǐ kuàngjià (MAPS) lái jiějué zhèxiē wèntí.\n\n实验结果显示，该框架在多个数据集上表现出色，超越了现有的最佳模型。\nShíyàn jiéguǒ xiǎnshì, gǎi kuàngjià zài duō gè shùjùjí shàng biǎoxiàn chūsè, chāoyuè le xiànyǒu de zuìjiā móxíng.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"科学问题\", \"pinyin\": \"kē xué wèn tí\", \"trans\": \"scientific problems\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenges\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěng hé\", \"trans\": \"integrate\"},\n    {\"word\": \"模态\", \"pinyin\": \"mó tài\", \"trans\": \"modality\"},\n    {\"word\": \"文本\", \"pinyin\": \"wén běn\", \"trans\": \"text\"},\n    {\"word\": \"图表\", \"pinyin\": \"tú biǎo\", \"trans\": \"charts\"},\n    {\"word\": \"面临\", \"pinyin\": \"miàn lín\", \"trans\": \"face\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"反思\", \"pinyin\": \"fǎn sī\", \"trans\": \"reflection\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"大七人格\", \"pinyin\": \"dà qī rén gé\", \"trans\": \"Big Five personality traits\"},\n    {\"word\": \"苏格拉底\", \"pinyin\": \"sū gé lā dǐ\", \"trans\": \"Socrates\"},\n    {\"word\": \"指导\", \"pinyin\": \"zhǐ dǎo\", \"trans\": \"guidance\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"results\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiàn yǒu\", \"trans\": \"existing\"},\n    {\"word\": \"最佳\", \"pinyin\": \"zuì jiā\", \"trans\": \"best\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"}\n]",
        "trans": "This article discusses multimodal scientific problems (MSPs) and their challenges. MSPs require the integration of multiple modalities, such as text and graphs. Currently, MSPs face two major issues: multimodal reasoning and a lack of reflective capabilities. The authors propose a multi-agent framework (MAPS) based on the Big Five personality traits and Socratic guidance to address these issues. Experimental results show that this framework performs excellently on multiple datasets, surpassing existing state-of-the-art models.",
        "update_ts": "2025-03-24 09:12"
    }
}