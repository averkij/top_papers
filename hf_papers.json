{
    "date": {
        "ru": "11 апреля",
        "en": "April 11",
        "zh": "4月11日"
    },
    "time_utc": "2025-04-11 03:28",
    "weekday": 4,
    "issue_id": 3183,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.07943",
            "title": "HoloPart: Generative 3D Part Amodal Segmentation",
            "url": "https://huggingface.co/papers/2504.07943",
            "abstract": "3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.",
            "score": 8,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "4cc1401ee10a171e",
            "authors": [
                "Yunhan Yang",
                "Yuan-Chen Guo",
                "Yukun Huang",
                "Zi-Xin Zou",
                "Zhipeng Yu",
                "Yangguang Li",
                "Yan-Pei Cao",
                "Xihui Liu"
            ],
            "affiliations": [
                "The University of Hong Kong",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07943.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#diffusion",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Революция в 3D-сегментации: видеть невидимое",
                    "desc": "Статья представляет новый подход к амодальной сегментации трехмерных частей объектов. Авторы предлагают двухэтапный метод: сначала используется существующая сегментация 3D-частей для получения начальных неполных сегментов, затем применяется новая модель HoloPart на основе диффузии для завершения этих сегментов в полные 3D-части. HoloPart использует специализированную архитектуру с локальным и глобальным вниманием для захвата геометрии частей и обеспечения общей согласованности формы. Результаты показывают, что предложенный метод значительно превосходит современные методы завершения форм и открывает новые возможности для приложений в редактировании геометрии, анимации и назначении материалов."
                },
                "en": {
                    "title": "Unlocking Hidden Shapes: HoloPart for 3D Amodal Segmentation",
                    "desc": "This paper addresses the challenge of 3D part amodal segmentation, which involves identifying complete parts of a 3D shape even when some parts are hidden. Current methods only work with visible surfaces, limiting their effectiveness. The authors propose a two-stage approach that first uses existing segmentation techniques to identify incomplete parts, followed by a novel diffusion-based model called HoloPart to complete these segments. HoloPart employs a specialized architecture to ensure both detailed part geometry and overall shape consistency, achieving superior results on new benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "突破3D分割：HoloPart模型的创新之路",
                    "desc": "本文提出了一种新的3D部分无模态分割任务，旨在将3D形状分解为完整且具有语义意义的部分，即使在被遮挡的情况下也能实现。现有的3D部分分割方法仅能识别可见的表面，限制了其应用。我们提出了一种实用的两阶段方法，首先利用现有的3D部分分割获取初步的不完整部分，然后引入HoloPart模型，通过扩散方法完成这些部分。HoloPart采用了专门的架构，结合局部注意力和全局形状一致性，显著提升了3D部分无模态分割的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07956",
            "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2504.07956",
            "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.",
            "score": 6,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "88860725e51f3629",
            "authors": [
                "Yukun Qi",
                "Yiming Zhao",
                "Yu Zeng",
                "Xikun Bao",
                "Wenxuan Huang",
                "Lin Chen",
                "Zehui Chen",
                "Jie Zhao",
                "Zhongang Qi",
                "Feng Zhao"
            ],
            "affiliations": [
                "East China Normal University",
                "Huawei Noahs Ark Lab",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07956.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VCR-Bench: новый стандарт оценки рассуждений ИИ по видео",
                    "desc": "В статье представлен новый бенчмарк VCR-Bench для оценки способностей больших визуально-языковых моделей к рассуждениям на основе цепочки мыслей при анализе видео. Бенчмарк включает 859 видео и 1034 пары вопрос-ответ с пошаговыми обоснованиями, помеченными для оценки восприятия и рассуждений. Эксперименты показали существенные ограничения современных моделей, особенно в обработке пространственно-временной информации. VCR-Bench призван стать стандартизированным инструментом для выявления недостатков в сложных задачах видеоанализа."
                },
                "en": {
                    "title": "VCR-Bench: Evaluating Video Reasoning in LVLMs",
                    "desc": "This paper introduces VCR-Bench, a new benchmark for evaluating Video Chain-of-Thought (CoT) reasoning in large vision-language models (LVLMs). It addresses the lack of rigorous evaluation frameworks for assessing how well these models can reason about video content. The benchmark includes 859 videos and 1,034 annotated question-answer pairs, each with a stepwise CoT rationale linked to perception or reasoning capabilities. Experiments reveal that current LVLMs struggle with video reasoning, particularly in processing temporal-spatial information, highlighting the need for improved models in this area."
                },
                "zh": {
                    "title": "VCR-Bench：视频推理的新标准",
                    "desc": "链式思维（CoT）推理的进步显著提升了大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的能力。然而，目前缺乏一个严格的视频CoT推理评估框架，现有的视频基准无法充分评估推理过程。为此，我们提出了VCR-Bench，这是一个新颖的基准，旨在全面评估LVLMs在视频链式思维推理方面的能力。通过859个视频和1034对高质量问答对，VCR-Bench为每个问答对提供了逐步的CoT推理依据，揭示了当前LVLMs在复杂视频推理中的关键瓶颈。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07960",
            "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
            "url": "https://huggingface.co/papers/2504.07960",
            "abstract": "Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.",
            "score": 5,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "a8b5331ac40d6a3d",
            "authors": [
                "Zhong-Yu Li",
                "Ruoyi Du",
                "Juncheng Yan",
                "Le Zhuo",
                "Zhen Li",
                "Peng Gao",
                "Zhanyu Ma",
                "Ming-Ming Cheng"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "VCIP, CS, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07960.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#transfer_learning",
                    "#diffusion",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "VisualCloze: универсальная генерация изображений с визуальным контекстным обучением",
                    "desc": "VisualCloze - это универсальная система генерации изображений, использующая визуальное обучение в контексте для идентификации задач. Она поддерживает широкий спектр задач, включая генерализацию на новые задачи и обратную генерацию. Система использует Graph200K - графовую структуру данных для создания взаимосвязанных задач и улучшения переноса знаний. VisualCloze объединяет генерацию изображений с их дополнением, используя сильные генеративные приоры предобученных моделей."
                },
                "en": {
                    "title": "VisualCloze: Bridging Tasks with Visual Learning in Image Generation",
                    "desc": "This paper introduces VisualCloze, a universal image generation framework that overcomes the limitations of task-specific models by supporting a variety of in-domain tasks and generalizing to unseen tasks. It addresses challenges such as task instruction ambiguity and sparse task distributions by utilizing visual in-context learning, which allows models to learn from visual examples rather than language instructions. The authors also present Graph200K, a dataset that enhances task density and facilitates knowledge transfer across related tasks. Additionally, they demonstrate that their unified image generation approach aligns with image infilling, enabling the use of pre-trained models for improved generative performance."
                },
                "zh": {
                    "title": "VisualCloze：通用图像生成的新框架",
                    "desc": "本论文介绍了一种名为VisualCloze的通用图像生成框架，旨在解决当前图像生成任务中存在的效率和通用性问题。与传统依赖语言指令的方法不同，VisualCloze通过视觉示例进行任务识别，从而减少了任务模糊性和提高了泛化能力。为了增强任务之间的可转移知识，我们引入了Graph200K数据集，该数据集通过图结构建立了多种相关任务。最后，我们发现我们的图像生成方法与图像填充具有一致的目标，从而能够利用预训练填充模型的强生成先验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07830",
            "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
            "url": "https://huggingface.co/papers/2504.07830",
            "abstract": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.",
            "score": 4,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "85dbaddf009300e0",
            "authors": [
                "Genglin Liu",
                "Salman Rahman",
                "Elisa Kreiss",
                "Marzyeh Ghassemi",
                "Saadia Gabriel"
            ],
            "affiliations": [
                "MIT CSAIL",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07830.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#graphs",
                    "#reasoning",
                    "#games",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Цифровое общество под микроскопом ИИ",
                    "desc": "MOSAIC - это новая система моделирования социальных сетей с открытым исходным кодом, использующая языковые модели для имитации поведения пользователей. Она сочетает агентов на основе больших языковых моделей с направленным социальным графом для анализа поведения, связанного с обманом. Система позволяет проводить многоагентные симуляции, моделирующие распространение контента и динамику взаимодействия пользователей. Исследователи оценили три стратегии модерации контента и обнаружили, что они снижают распространение недостоверной информации и повышают вовлеченность пользователей."
                },
                "en": {
                    "title": "MOSAIC: Simulating Social Networks to Combat Misinformation",
                    "desc": "The paper introduces MOSAIC, an innovative open-source framework for simulating social networks using generative language agents. These agents predict user behaviors like liking and sharing content, allowing researchers to study how deception emerges in online interactions. By creating detailed user personas, the framework facilitates large-scale simulations of content spread and user engagement. The study evaluates various content moderation strategies, revealing that they can effectively reduce misinformation while enhancing user interaction."
                },
                "zh": {
                    "title": "MOSAIC：社交网络行为模拟与内容审核新探索",
                    "desc": "我们提出了一种新颖的开源社交网络模拟框架MOSAIC，利用生成语言代理预测用户行为，如点赞、分享和标记内容。该模拟结合了大型语言模型（LLM）代理和有向社交图，分析新出现的欺骗行为，帮助理解用户如何判断在线社交内容的真实性。通过构建多样化的细粒度用户画像，我们的系统支持大规模的多代理模拟，模拟内容传播和用户参与的动态。我们评估了三种不同的内容审核策略，发现它们不仅能减缓虚假信息的传播，还能提高用户参与度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07934",
            "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
            "url": "https://huggingface.co/papers/2504.07934",
            "abstract": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.",
            "score": 1,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "419fda5d0a4bcadf",
            "authors": [
                "Xiyao Wang",
                "Zhengyuan Yang",
                "Chao Feng",
                "Hongjin Lu",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Furong Huang",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "University of Maryland, College Park",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07934.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#reasoning",
                    "#training",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Меньше данных, больше мышления: революция в визуальном рассуждении ИИ",
                    "desc": "В этой статье представлен эффективный метод улучшения визуального рассуждения с использованием значительно меньшего количества обучающих примеров, основанный на самосовершенствовании без передачи знаний. Ключевой идеей является важность сложности обучающих данных при тонкой настройке с подкреплением (RFT). Авторы предлагают новый способ использования поиска по дереву Монте-Карло (MCTS) для количественной оценки сложности примеров и эффективной фильтрации данных. Разработанная модель ThinkLite-VL, обученная на отфильтрованном наборе из 11 тысяч примеров, превосходит существующие модели визуально-языкового рассуждения уровня 7B и достигает лучших результатов на ряде бенчмарков."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Fewer Samples through Smart Data Selection",
                    "desc": "This paper introduces a novel method to improve visual reasoning in models using fewer training samples through self-improvement techniques, avoiding knowledge distillation. The authors emphasize the importance of sample difficulty during reinforcement fine-tuning (RFT), suggesting that challenging samples can enhance reasoning capabilities even with limited data. They propose a unique application of Monte Carlo Tree Search (MCTS) to quantify sample difficulty, allowing for effective data filtering. The resulting model, ThinkLite-VL, demonstrates a 7% performance increase over its predecessor using only 11k samples, achieving state-of-the-art results in various benchmarks."
                },
                "zh": {
                    "title": "用少量样本提升视觉推理能力",
                    "desc": "本文提出了一种有效的方法，通过自我改进来增强视觉推理，且所需的训练样本显著减少，不依赖知识蒸馏。我们发现，在强化微调（RFT）过程中，训练数据的难度至关重要，适当具有挑战性的样本可以显著提升推理能力。我们提出了一种新颖的方式，利用蒙特卡洛树搜索（MCTS）来量化样本的难度，从而实现有效的数据筛选。最终，我们的模型ThinkLite-VL在八个基准测试中表现出色，使用仅11k个训练样本，平均性能提升了7%。"
                }
            }
        }
    ],
    "link_prev": "2025-04-10.html",
    "link_next": "2025-04-14.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4月10日"
    },
    "short_date_next": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4月14日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 2,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "扩散变压器在生成质量上表现出色，但需要更长的训练迭代和多次推理步骤。每个去噪步骤中，扩散变压器编码噪声输入以提取低频语义成分，然后解码高频成分。这种方案导致优化困境：编码低频语义需要减少高频成分，造成语义编码和高频解码之间的矛盾。我们提出一种新的解耦扩散变压器（DDT），具有专门的条件编码器和速度解码器。实验显示，更大的编码器随着模型规模增加提高性能。在ImageNet 256x256上，DDT-XL/2达到1.31 FID（训练收敛速度近乎快4倍）；在ImageNet 512x512上，达到1.28 FID。此外，解耦架构提高推理速度，允许相邻去噪步骤间共享自我条件。我们提出一种新的统计动态规划方法来最小化性能下降。",
        "title": "DDT: Decoupled Diffusion Transformer",
        "pinyin": "扩散变压器在生成质量上表现出色，但需要更长的训练迭代和多次推理步骤。每个去噪步骤中，扩散变压器编码噪声输入以提取低频语义成分，然后解码高频成分。这种方案导致优化困境：编码低频语义需要减少高频成分，造成语义编码和高频解码之间的矛盾。我们提出一种新的解耦扩散变压器（DDT），具有专门的条件编码器和速度解码器。实验显示，更大的编码器随着模型规模增加提高性能。在ImageNet 256x256上，DDT-XL/2达到1.31 FID（训练收敛速度近乎快4倍）；在ImageNet 512x512上，达到1.28 FID。此外，解耦架构提高推理速度，允许相邻去噪步骤间共享自我条件。我们提出一种新的统计动态规划方法来最小化性能下降。\n\nkuò sàn biàn yā qì zài shēng chéng zhì liàng shàng biǎo xiàn chū sè, dàn xū yào gèng cháng de xùn liàn dié dǎi hé duō cì tuī lǐ bù zhòu. měi gè qù zào bù zhòu zhōng, kuò sàn biàn yā qì biān mǎ shēng yīn yùn tǐ yǐ tí qǔ dī pín yǔ yì chéng fēn, rán hòu jiě mǎ gāo pín chéng fēn. zhè zhǒng fāng àn dǎo zhì yòu huà kùn jìng: biān mǎ dī pín yǔ yì xū yào jiǎn shǎo gāo pín chéng fēn, zào chéng yǔ yì biān mǎ hé gāo pín jiě mǎ zhī jiān de máo dùn. wǒ men tí chū yī zhǒng xīn de jiě kǒu kuò sàn biàn yā qì (DDT), jù yǒu zhuān mén de tiáo jiàn biān mǎ qì hé sù dù jiě mǎ qì. shí yàn shì zhù, gèng dà de biān mǎ qì suí zhě mó xíng guī mó zēng jiā tí gāo xíng néng. zài ImageNet 256x256 shàng, DDT-XL/2 dá dào 1.31 FID (xùn liàn shōu liǎn sù dù jìn hū kuài 4 bèi); zài ImageNet 512x512 shàng, dá dào 1.28 FID. cǐ wài, jiě kǒu jià gòu tí gāo tuī lǐ sù dù, yǔn xǔ xiāng lín qù zào bù zhòu jiān gòng xiǎng zì wǒ tiáo jiàn. wǒ men tí chū yī zhǒng xīn de tǒng jì dòng tài guī huà fāng fǎ lái zuì shǎo huà xíng néng xià jiàng.",
        "vocab": "[{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'},\n{'word': '变压器', 'pinyin': 'biàn yā qì', 'trans': 'transformer'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '迭代', 'pinyin': 'dié dài', 'trans': 'iteration'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'},\n{'word': '去噪', 'pinyin': 'qù zào', 'trans': 'denoising'},\n{'word': '编码', 'pinyin': 'biān mǎ', 'trans': 'encode'},\n{'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'},\n{'word': '成分', 'pinyin': 'chéng fèn', 'trans': 'component'},\n{'word': '解码', 'pinyin': 'jiě mǎ', 'trans': 'decode'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'},\n{'word': '困境', 'pinyin': 'kùn jìng', 'trans': 'dilemma'},\n{'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'},\n{'word': '矛盾', 'pinyin': 'máo dùn', 'trans': 'contradiction'},\n{'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'},\n{'word': '条件', 'pinyin': 'tiāo jiàn', 'trans': 'condition'},\n{'word': '速度', 'pinyin': 'sù dù', 'trans': 'speed'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'},\n{'word': '收敛', 'pinyin': 'shōu liǎn', 'trans': 'convergence'},\n{'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'},\n{'word': '共享', 'pinyin': 'gòng xiǎng', 'trans': 'share'},\n{'word': '自我', 'pinyin': 'zì wǒ', 'trans': 'self'},\n{'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'},\n{'word': '规划', 'pinyin': 'guī huà', 'trans': 'planning'},\n{'word': '最小化', 'pinyin': 'zuì xiǎo huà', 'trans': 'minimize'},\n{'word': '下降', 'pinyin': 'xià jiàng', 'trans': 'decrease'}]",
        "trans": "The diffusion transformer performs excellently in terms of generation quality but requires longer training iterations and multiple inference steps. In each denoising step, the diffusion transformer encodes noisy input to extract low-frequency semantic components and then decodes high-frequency components. This scheme leads to an optimization dilemma: encoding low-frequency semantics requires reducing high-frequency components, creating a conflict between semantic encoding and high-frequency decoding. We propose a new decoupled diffusion transformer (DDT) with dedicated conditional encoders and velocity decoders. Experiments show that larger encoders improve performance as the model scale increases. On ImageNet 256x256, DDT-XL/2 achieves 1.31 FID (with training convergence speed nearly 4 times faster); on ImageNet 512x512, it achieves 1.28 FID. Additionally, the decoupled architecture increases inference speed, allowing adjacent denoising steps to share self-conditioning. We propose a new statistical dynamic programming method to minimize performance degradation.",
        "update_ts": "2025-04-10 09:12"
    }
}