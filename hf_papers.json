{
    "date": {
        "ru": "11 сентября",
        "en": "September 11",
        "zh": "9月11日"
    },
    "time_utc": "2025-09-11 02:19",
    "weekday": 3,
    "issue_id": 5829,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.08826",
            "title": "RewardDance: Reward Scaling in Visual Generation",
            "url": "https://huggingface.co/papers/2509.08826",
            "abstract": "RewardDance is a scalable reward modeling framework that aligns with VLM architectures, enabling effective scaling of RMs and resolving reward hacking issues in generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.",
            "score": 2,
            "issue_id": 5829,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "773c62b4801465a6",
            "authors": [
                "Jie Wu",
                "Yu Gao",
                "Zilyu Ye",
                "Ming Li",
                "Liang Li",
                "Hanzhong Guo",
                "Jie Liu",
                "Zeyue Xue",
                "Xiaoxia Hou",
                "Wei Liu",
                "Yan Zeng",
                "Weilin Huang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2509.08826.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "💃",
                "ru": {
                    "title": "RewardDance: Танцуя с вознаграждениями в масштабируемом обучении генеративных моделей",
                    "desc": "RewardDance - это масштабируемая система моделирования вознаграждений, совместимая с архитектурами VLM. Она позволяет эффективно масштабировать модели вознаграждений и решает проблемы эксплуатации вознаграждений в генеративных моделях. RewardDance переформулирует оценку вознаграждения как вероятность модели предсказать токен 'да', что позволяет масштабировать как саму модель, так и контекст. Эксперименты показывают, что RewardDance превосходит современные методы в задачах генерации изображений и видео, сохраняя высокую вариативность вознаграждений при обучении с подкреплением."
                },
                "en": {
                    "title": "RewardDance: Scaling Reward Models for Better AI Generation",
                    "desc": "RewardDance is a new framework designed to improve reward modeling in visual generation tasks by aligning with Vision-Language Models (VLMs). It addresses the limitations of existing reward models, which often struggle with architectural constraints and misalignment with next-token predictions. The framework introduces a novel generative reward paradigm that reformulates reward scoring, allowing for effective scaling of reward models up to 26 billion parameters. Importantly, RewardDance mitigates the issue of reward hacking, ensuring that models produce diverse and high-quality outputs during reinforcement learning fine-tuning."
                },
                "zh": {
                    "title": "RewardDance：解决奖励黑客的可扩展奖励建模框架",
                    "desc": "RewardDance是一个可扩展的奖励建模框架，旨在与视觉语言模型（VLM）架构对齐，从而有效地扩展奖励模型（RM）并解决生成模型中的奖励黑客问题。现有的奖励模型在视觉生成中的扩展性受到架构和输入模态的限制，而流行的Bradley-Terry损失与VLM的下一个标记预测机制不匹配，阻碍了有效扩展。通过将奖励分数重新定义为模型预测“是”标记的概率，RewardDance使奖励目标与VLM架构内在对齐，从而在模型和上下文两个维度上实现扩展。实验表明，RewardDance在文本到图像、文本到视频和图像到视频生成方面显著超越了现有的最先进方法，并有效解决了奖励黑客问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08827",
            "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
            "url": "https://huggingface.co/papers/2509.08827",
            "abstract": "Reinforcement Learning enhances Large Language Models for complex reasoning tasks, facing challenges in scalability and infrastructure as the field advances.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
            "score": 0,
            "issue_id": 5829,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "ea01091be58aef4b",
            "authors": [
                "Kaiyan Zhang",
                "Yuxin Zuo",
                "Bingxiang He",
                "Youbang Sun",
                "Runze Liu",
                "Che Jiang",
                "Yuchen Fan",
                "Kai Tian",
                "Guoli Jia",
                "Pengfei Li",
                "Yu Fu",
                "Xingtai Lv",
                "Yuchen Zhang",
                "Sihang Zeng",
                "Shang Qu",
                "Haozhan Li",
                "Shijie Wang",
                "Yuru Wang",
                "Xinwei Long",
                "Fangfu Liu",
                "Xiang Xu",
                "Jiaze Ma",
                "Xuekai Zhu",
                "Ermo Hua",
                "Yihao Liu",
                "Zonglin Li",
                "Huayu Chen",
                "Xiaoye Qu",
                "Yafu Li",
                "Weize Chen",
                "Zhenzhao Yuan",
                "Junqi Gao",
                "Dong Li",
                "Zhiyuan Ma",
                "Ganqu Cui",
                "Zhiyuan Liu",
                "Biqing Qi",
                "Ning Ding",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Huazhong University of Science and Technology",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University College London",
                "University of Science and Technology of China",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08827.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#survey",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обучение с подкреплением: ключ к улучшению рассуждений языковых моделей",
                    "desc": "Эта статья представляет обзор последних достижений в области обучения с подкреплением (RL) для улучшения способностей больших языковых моделей (LLM) к рассуждению. Авторы рассматривают успехи RL в решении сложных логических задач, таких как математика и программирование, что привело к появлению языковых моделей с улучшенными навыками рассуждения (LRM). В работе обсуждаются проблемы масштабирования RL для LRM, включая вычислительные ресурсы, алгоритмы, данные для обучения и инфраструктуру. Статья анализирует текущее состояние области и перспективы развития RL для создания моделей с более широкими возможностями рассуждения."
                },
                "en": {
                    "title": "Scaling Reinforcement Learning for Advanced Reasoning in Language Models",
                    "desc": "This paper reviews the integration of Reinforcement Learning (RL) with Large Language Models (LLMs) to improve their reasoning capabilities. It highlights the success of RL in enhancing LLMs for complex tasks like mathematics and coding, positioning RL as a key method for evolving LLMs into more advanced reasoning models (LRMs). The authors discuss the challenges of scaling RL, including the need for better computational resources, algorithm design, and training data. They aim to identify future research directions to further develop RL applications in reasoning tasks, especially in the context of achieving Artificial SuperIntelligence (ASI)."
                },
                "zh": {
                    "title": "强化学习助力大型语言模型推理能力提升",
                    "desc": "本论文调查了强化学习（RL）在大型语言模型（LLM）推理任务中的最新进展。强化学习在提升LLM能力方面取得了显著成功，尤其是在解决复杂的逻辑任务如数学和编程方面。随着该领域的快速发展，RL在大型语言模型的扩展面临着计算资源、算法设计、训练数据和基础设施等基础性挑战。我们希望通过这项综述促进未来在更广泛推理模型上应用强化学习的研究。"
                }
            }
        }
    ],
    "link_prev": "2025-09-10.html",
    "link_next": "2025-09-12.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9月10日"
    },
    "short_date_next": {
        "ru": "12.09",
        "en": "09/12",
        "zh": "9月12日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}