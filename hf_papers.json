{
    "date": {
        "ru": "25 ноября",
        "en": "November 25",
        "zh": "11月25日"
    },
    "time_utc": "2024-11-25 07:11",
    "weekday": 0,
    "issue_id": 756,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.14793",
            "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
            "url": "https://huggingface.co/papers/2411.14793",
            "abstract": "Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new \"style templates\", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation.",
            "score": 16,
            "issue_id": 752,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "03859b57f29683ab",
            "authors": [
                "Jooyoung Choi",
                "Chaehun Shin",
                "Yeongtak Oh",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14793.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#multimodal",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшение стилизации изображений с помощью оптимизации шума в диффузионных моделях",
                    "desc": "Статья представляет новый метод улучшения генерации изображений в определенном стиле с помощью диффузионных моделей. Авторы предлагают использовать Style-friendly SNR sampler, который смещает распределение соотношения сигнал-шум в сторону более высоких уровней шума при дообучении модели. Это позволяет лучше захватывать уникальные стилистические особенности и генерировать изображения с более высоким соответствием заданному стилю. Метод демонстрирует способность генерировать различные стили, включая акварельные рисунки, минималистичные мультфильмы, 3D-рендеры и мемы с текстом."
                },
                "en": {
                    "title": "Unlocking Unique Artistic Styles with Style-friendly SNR Sampler",
                    "desc": "This paper addresses the challenge of adapting large-scale diffusion models to generate personalized artistic styles. The authors introduce the Style-friendly SNR sampler, which modifies the signal-to-noise ratio (SNR) during fine-tuning to emphasize higher noise levels where stylistic features are more prominent. By doing so, the model improves its ability to capture unique styles, resulting in images that align better with the desired artistic expression. The proposed method expands the creative possibilities for generating diverse styles, including watercolor paintings and cartoons, thus enhancing personalized content creation."
                },
                "zh": {
                    "title": "提升个性化艺术风格生成的信噪比方法",
                    "desc": "最近的大规模扩散模型能够生成高质量的图像，但在学习新的个性化艺术风格方面存在困难，这限制了独特风格模板的创建。微调参考图像是最有前景的方法，但通常盲目使用预训练时的目标和噪声水平分布，导致风格对齐不理想。我们提出了风格友好的信噪比（SNR）采样器，在微调过程中积极将信噪比分布向更高的噪声水平转移，以专注于风格特征出现的噪声水平。这使得模型能够更好地捕捉独特风格，并生成具有更高风格对齐的图像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15098",
            "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
            "url": "https://huggingface.co/papers/2411.15098",
            "abstract": "In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only ~0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation.",
            "score": 4,
            "issue_id": 754,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "9cd668db99ed0902",
            "authors": [
                "Zhenxiong Tan",
                "Songhua Liu",
                "Xingyi Yang",
                "Qiaochu Xue",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15098.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#synthetic",
                    "#data",
                    "#diffusion",
                    "#dataset",
                    "#architecture",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "OminiControl: Универсальное управление генерацией изображений с минимальными затратами",
                    "desc": "OminiControl - это новая эффективная система для интеграции изображений в предобученные модели Diffusion Transformer (DiT). Она использует механизм повторного использования параметров, позволяя DiT кодировать условия изображений с помощью собственной архитектуры. OminiControl требует всего 0,1% дополнительных параметров и может решать широкий спектр задач условной генерации изображений. Система превосходит существующие модели на основе UNet и адаптированные DiT как в генерации, управляемой субъектом, так и в пространственно-согласованной условной генерации."
                },
                "en": {
                    "title": "Efficient Image Conditioning with OminiControl",
                    "desc": "OminiControl is a new framework that enhances pre-trained Diffusion Transformer (DiT) models by integrating image conditions efficiently. It uses a parameter reuse mechanism, allowing the DiT to process image conditions with minimal additional parameters, specifically around 0.1%. This framework can handle various image conditioning tasks, such as generating images based on specific subjects or aligning them with spatial features like edges and depth. OminiControl has shown superior performance compared to traditional UNet-based models and other DiT adaptations, and it comes with a large dataset, Subjects200K, to support further research."
                },
                "zh": {
                    "title": "OminiControl：高效整合图像条件的创新框架",
                    "desc": "本文介绍了OminiControl，这是一个高度灵活且参数高效的框架，能够将图像条件集成到预训练的扩散变换器（DiT）模型中。OminiControl利用参数重用机制，使DiT能够使用自身作为强大的基础，编码图像条件，并通过灵活的多模态注意力处理器进行处理。与现有方法不同，OminiControl仅需约0.1%的额外参数，就能有效地整合注入的图像条件，并以统一的方式处理多种图像条件任务。通过在DiT自身生成的图像上进行训练，OminiControl在主题驱动生成和空间对齐条件生成方面的表现优于现有的UNet和DiT适应模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12946",
            "title": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection",
            "url": "https://huggingface.co/papers/2411.12946",
            "abstract": "Large Language Models are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety.",
            "score": 3,
            "issue_id": 756,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 ноября",
                "en": "November 20",
                "zh": "11月20日"
            },
            "hash": "de5ca9118a5cab35",
            "authors": [
                "Gabriel Chua",
                "Shing Yee Chan",
                "Shaun Khoo"
            ],
            "affiliations": [
                "Government Technology Agency Singapore",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12946.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#hallucinations",
                    "#synthetic",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Гибкая защита языковых моделей без реальных данных",
                    "desc": "Статья представляет новую методологию разработки защитных механизмов для больших языковых моделей (LLM) от нецелевого использования. Авторы предлагают гибкий подход, не требующий реальных данных, основанный на генерации синтетического датасета с помощью LLM. Разработанные защитные механизмы превосходят эвристические подходы и могут обобщаться на другие категории злоупотреблений. Авторы также открывают доступ к синтетическому датасету и моделям защиты для поддержки будущих исследований в области безопасности LLM."
                },
                "en": {
                    "title": "Building Better Guardrails for Large Language Models",
                    "desc": "This paper addresses the issue of off-topic misuse in Large Language Models (LLMs) by proposing a new methodology for developing guardrails without relying on real-world data. The authors create a synthetic dataset by using LLMs to generate diverse prompts based on a well-defined problem space, which helps in training more effective off-topic guardrails. Their approach not only reduces false positives but also enhances adaptability to various misuse scenarios, such as harmful prompts and jailbreak attempts. Furthermore, the authors contribute to the community by open-sourcing their synthetic dataset and guardrail models, promoting further research in LLM safety."
                },
                "zh": {
                    "title": "构建灵活的防护措施，提升大型语言模型安全性",
                    "desc": "本论文探讨了大型语言模型在使用中可能出现的偏离主题的误用问题。我们提出了一种灵活的、无数据的防护措施开发方法，旨在解决现有方法的高误报率和适应性不足的问题。通过对问题空间的定性定义，并利用大型语言模型生成多样化的提示，我们构建了一个合成数据集，用于基准测试和训练防护措施。最后，我们开源了合成数据集和防护模型，为大型语言模型的安全性研究和开发提供了重要资源。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14762",
            "title": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction",
            "url": "https://huggingface.co/papers/2411.14762",
            "abstract": "Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled (x,y,t) coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128times128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.",
            "score": 2,
            "issue_id": 756,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "9490a40884583735",
            "authors": [
                "Huiwon Jang",
                "Sihyun Yu",
                "Jinwoo Shin",
                "Pieter Abbeel",
                "Younggyo Seo"
            ],
            "affiliations": [
                "KAIST",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14762.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "CoordTok: Эффективная токенизация длинных видео с помощью координатного представления",
                    "desc": "CoordTok - это новый токенизатор для видео, который использует координатное представление для кодирования длинных видеоклипов. Он обучается восстанавливать патчи, соответствующие случайно выбранным координатам (x,y,t), что позволяет эффективно обрабатывать длинные видео. CoordTok значительно сокращает количество токенов, необходимых для кодирования видео, по сравнению с базовыми методами. Это делает возможным эффективное по памяти обучение диффузионного трансформера для генерации длинных видео."
                },
                "en": {
                    "title": "Efficient Video Tokenization with CoordTok",
                    "desc": "This paper presents CoordTok, a novel video tokenizer designed to efficiently encode long video clips by leveraging coordinate-based representations. Unlike traditional tokenizers that reconstruct all frames simultaneously, CoordTok uses a factorized triplane representation to map (x,y,t) coordinates to video patches, significantly reducing the number of tokens needed. The approach allows for training large models on long videos without incurring high computational costs. Experimental results demonstrate that CoordTok can encode a 128-frame video into just 1280 tokens, outperforming existing methods that require thousands of tokens for similar quality."
                },
                "zh": {
                    "title": "高效视频标记化，降低训练成本！",
                    "desc": "本论文提出了一种名为CoordTok的视频标记器，旨在高效处理长视频的标记化问题。CoordTok通过学习坐标表示与输入视频补丁之间的映射，利用了视频的时间一致性。与传统方法相比，CoordTok显著减少了编码长视频所需的标记数量，从而降低了训练成本。实验表明，CoordTok能够在保持重建质量的同时，将128帧视频的标记数量减少到1280个。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15124",
            "title": "TÜLU 3: Pushing Frontiers in Open Language Model Post-Training",
            "url": "https://huggingface.co/papers/2411.15124",
            "abstract": "Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.   In addition to the T\\\"ULU 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the T\\\"ULU 3 approach to more domains.",
            "score": 2,
            "issue_id": 755,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "44809ea81d71ef97",
            "authors": [
                "Nathan Lambert",
                "Jacob Morrison",
                "Valentina Pyatkin",
                "Shengyi Huang",
                "Hamish Ivison",
                "Faeze Brahman",
                "Lester James V. Miranda",
                "Alisa Liu",
                "Nouha Dziri",
                "Shane Lyu",
                "Yuling Gu",
                "Saumya Malik",
                "Victoria Graf",
                "Jena D. Hwang",
                "Jiangjiang Yang",
                "Ronan Le Bras",
                "Oyvind Tafjord",
                "Chris Wilhelm",
                "Luca Soldaini",
                "Noah A. Smith",
                "Yizhong Wang",
                "Pradeep Dasigi",
                "Hannaneh Hajishirzi"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15124.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#data",
                    "#open_source",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Открытый рецепт пост-тренировки языковых моделей",
                    "desc": "Статья представляет T\"ULU 3 - семейство открытых моделей, обученных с помощью пост-тренировки. Авторы описывают методы обучения, включая SFT, DPO и новый метод RLVR. T\"ULU 3 превосходит многие современные модели, включая инструктированные версии Llama 3.1 и GPT-4o-mini. Исследователи предоставляют полные рецепты обучения, наборы данных и инструменты для воспроизведения результатов."
                },
                "en": {
                    "title": "Unlocking Language Model Potential with T\"ULU 3",
                    "desc": "This paper presents T\"ULU 3, a set of fully-open post-trained language models that enhance performance and capabilities beyond existing models. It emphasizes the importance of transparency in training data and methodologies, providing a comprehensive guide for implementing modern post-training techniques. The models utilize advanced training algorithms, including supervised finetuning and a novel reinforcement learning method, achieving superior results compared to both open and closed counterparts. Additionally, the paper offers extensive resources for replication and adaptation, including datasets, training code, and evaluation tools."
                },
                "zh": {
                    "title": "T\"ULU 3：开放的后训练模型新纪元",
                    "desc": "本文介绍了T\"ULU 3，这是一个完全开放的最新后训练模型系列，旨在提高语言模型的行为和技能。我们提供了模型的训练数据、代码和训练配方，填补了开放技术与专有技术之间的透明度差距。T\"ULU 3在多个基准测试中超越了现有的语言模型，包括Llama 3.1和GPT-4o-mini等。我们还引入了一种多任务评估方案，并提供了详细的报告，以便于在更多领域中复现和适应T\"ULU 3的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14521",
            "title": "MyTimeMachine: Personalized Facial Age Transformation",
            "url": "https://huggingface.co/papers/2411.14521",
            "abstract": "Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20sim40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches.",
            "score": 1,
            "issue_id": 755,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "d104e8a00be886bb",
            "authors": [
                "Luchao Qi",
                "Jiaye Wu",
                "Bang Gong",
                "Annie N. Wang",
                "David W. Jacobs",
                "Roni Sengupta"
            ],
            "affiliations": [
                "University of Maryland, College Park",
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14521.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#cv",
                    "#video"
                ],
                "emoji": "👴",
                "ru": {
                    "title": "Персонализированное старение лиц с помощью глубокого обучения",
                    "desc": "Статья представляет новый метод персонализированного старения лиц на изображениях, называемый MyTimeMachine (MyTM). Авторы комбинируют глобальную модель старения с личной коллекцией фотографий для создания индивидуализированной трансформации возраста. В работе вводится Адаптерная Сеть, объединяющая персонализированные и глобальные признаки старения для генерации состаренного изображения с помощью StyleGAN2. Метод также применим к видео, обеспечивая качественные, сохраняющие идентичность и темпорально согласованные эффекты старения."
                },
                "en": {
                    "title": "Personalized Aging: Your Face, Your Time",
                    "desc": "This paper presents MyTimeMachine (MyTM), a novel approach to facial aging that combines global aging knowledge with personalized photo collections. It addresses the challenge of accurately predicting an individual's appearance at a target age by utilizing as few as 50 personal images. The method employs an Adapter Network that integrates personalized and global aging features, generating realistic re-aged images using StyleGAN2. Additionally, the authors introduce three specialized loss functions to enhance the personalization of the aging process, resulting in high-quality, identity-preserving, and temporally consistent aging effects."
                },
                "zh": {
                    "title": "个性化老化，真实再现",
                    "desc": "面部老化是一个复杂的过程，受到性别、种族、生活方式等多种因素的影响，因此很难学习到一个通用的老化模型来准确预测个体的老化情况。现有技术虽然能够生成逼真的老化效果，但重塑的图像往往与目标年龄的外貌不符，因此需要个性化处理。我们提出了MyTimeMachine（MyTM），它结合了全球老化模型和个人照片集（仅需50张图像）来学习个性化的年龄转换。我们的创新在于引入了适配器网络和三种损失函数，使得生成的老化图像能够保持身份一致性，并在视频中实现高质量的老化效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15131",
            "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
            "url": "https://huggingface.co/papers/2411.15131",
            "abstract": "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world environments, which requires the robot to (1) have skills that generalize across object configurations; (2) be capable of long-horizon task execution in diverse environments; and (3) perform complex manipulation beyond pick-and-place. Quadruped robots with manipulators hold promise for extending the workspace and enabling robust locomotion, but existing results do not investigate such a capability. This paper proposes WildLMa with three components to address these issues: (1) adaptation of learned low-level controller for VR-enabled whole-body teleoperation and traversability; (2) WildLMa-Skill -- a library of generalizable visuomotor skills acquired via imitation learning or heuristics and (3) WildLMa-Planner -- an interface of learned skills that allow LLM planners to coordinate skills for long-horizon tasks. We demonstrate the importance of high-quality training data by achieving higher grasping success rate over existing RL baselines using only tens of demonstrations. WildLMa exploits CLIP for language-conditioned imitation learning that empirically generalizes to objects unseen in training demonstrations. Besides extensive quantitative evaluation, we qualitatively demonstrate practical robot applications, such as cleaning up trash in university hallways or outdoor terrains, operating articulated objects, and rearranging items on a bookshelf.",
            "score": 0,
            "issue_id": 755,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "c38df92e9599db09",
            "authors": [
                "Ri-Zhao Qiu",
                "Yuchen Song",
                "Xuanbin Peng",
                "Sai Aneesh Suryadevara",
                "Ge Yang",
                "Minghuan Liu",
                "Mazeyu Ji",
                "Chengzhe Jia",
                "Ruihan Yang",
                "Xueyan Zou",
                "Xiaolong Wang"
            ],
            "affiliations": [
                "MIT",
                "NVIDIA",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15131.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#robotics",
                    "#training",
                    "#agi",
                    "#transfer_learning",
                    "#long_context"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "WildLMa: Универсальный робот-манипулятор для реального мира",
                    "desc": "Статья представляет систему WildLMa для мобильной манипуляции роботов в реальных условиях. Система включает адаптивный низкоуровневый контроллер для телеоперации, библиотеку обобщаемых визуомоторных навыков и планировщик на основе языковых моделей для выполнения долгосрочных задач. WildLMa использует имитационное обучение и CLIP для генерализации на новые объекты, демонстрируя высокую точность захвата при небольшом количестве обучающих примеров. Система показала свою эффективность в практических задачах, таких как уборка мусора и перестановка предметов."
                },
                "en": {
                    "title": "Empowering Robots for Real-World Manipulation with WildLMa",
                    "desc": "This paper presents WildLMa, a framework designed for mobile manipulation robots to operate effectively in varied real-world settings. It includes a low-level controller for teleoperation, a library of generalizable visuomotor skills learned through imitation, and a planner that coordinates these skills for complex tasks. The approach emphasizes the significance of high-quality training data, achieving better performance in grasping tasks compared to existing reinforcement learning methods. Additionally, WildLMa utilizes CLIP for language-conditioned imitation learning, enabling the robot to adapt to new objects not seen during training."
                },
                "zh": {
                    "title": "WildLMa：提升机器人在真实环境中的操控能力",
                    "desc": "本论文提出了一种名为WildLMa的移动操控机器人系统，旨在解决在多样化真实环境中执行复杂任务的挑战。该系统包括三个主要组件：适应性低级控制器、通用视觉运动技能库和长时间任务规划接口。通过模仿学习和高质量训练数据，WildLMa在抓取成功率上超越了现有的强化学习基线。该研究展示了机器人在实际应用中的潜力，如清理校园走廊垃圾和操作复杂物体。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14208",
            "title": "Novel View Extrapolation with Video Diffusion Priors",
            "url": "https://huggingface.co/papers/2411.14208",
            "abstract": "The field of novel view synthesis has made significant strides thanks to the development of radiance field methods. However, most radiance field techniques are far better at novel view interpolation than novel view extrapolation where the synthesis novel views are far beyond the observed training views. We design ViewExtrapolator, a novel view synthesis approach that leverages the generative priors of Stable Video Diffusion (SVD) for realistic novel view extrapolation. By redesigning the SVD denoising process, ViewExtrapolator refines the artifact-prone views rendered by radiance fields, greatly enhancing the clarity and realism of the synthesized novel views. ViewExtrapolator is a generic novel view extrapolator that can work with different types of 3D rendering such as views rendered from point clouds when only a single view or monocular video is available. Additionally, ViewExtrapolator requires no fine-tuning of SVD, making it both data-efficient and computation-efficient. Extensive experiments demonstrate the superiority of ViewExtrapolator in novel view extrapolation. Project page: https://kunhao-liu.github.io/ViewExtrapolator/.",
            "score": 0,
            "issue_id": 755,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "dddb7a1ecc9850f3",
            "authors": [
                "Kunhao Liu",
                "Ling Shao",
                "Shijian Lu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "UCAS-Terminus AI Lab, UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14208.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#video"
                ],
                "emoji": "🔭",
                "ru": {
                    "title": "Реалистичная экстраполяция новых ракурсов с помощью генеративных моделей",
                    "desc": "ViewExtrapolator - это новый подход к синтезу видов, использующий генеративные возможности Stable Video Diffusion (SVD) для реалистичной экстраполяции новых ракурсов. Метод улучшает качество изображений, полученных с помощью радиационных полей, значительно повышая четкость и реалистичность синтезированных видов. ViewExtrapolator может работать с различными типами 3D-рендеринга, включая облака точек, и не требует дополнительного обучения SVD. Эксперименты подтверждают превосходство ViewExtrapolator в задаче экстраполяции новых ракурсов."
                },
                "en": {
                    "title": "Enhancing Novel View Extrapolation with ViewExtrapolator",
                    "desc": "This paper introduces ViewExtrapolator, a new method for synthesizing novel views that go beyond the original training views. It utilizes the generative capabilities of Stable Video Diffusion (SVD) to improve the quality of these extrapolated views. By modifying the SVD denoising process, the method reduces artifacts and enhances the realism of the generated images. ViewExtrapolator is versatile, working with various 3D rendering types and requiring no fine-tuning, making it efficient in both data and computation."
                },
                "zh": {
                    "title": "提升新视图外推的清晰度与真实感",
                    "desc": "本论文介绍了一种新的视图合成方法，称为ViewExtrapolator，旨在改善新视图外推的效果。传统的辐射场技术在新视图插值方面表现良好，但在新视图外推时效果较差。ViewExtrapolator利用稳定视频扩散（SVD）的生成先验，通过重新设计去噪过程，显著提高了合成新视图的清晰度和真实感。该方法适用于不同类型的3D渲染，并且无需对SVD进行微调，具有数据和计算效率。"
                }
            }
        }
    ],
    "link_prev": "2024-11-22.html",
    "link_next": "2024-11-26.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "22.11",
        "en": "11/22",
        "zh": "11月22日"
    },
    "short_date_next": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11月26日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了现有开源多模态大语言模型（MLLMs）的训练过程及其在分布偏移上的局限性。为了提升多模态推理能力，作者引入了偏好优化（PO）过程。他们创建了一个高质量的多模态推理偏好数据集MMPR，并开发了一种混合偏好优化（MPO）方法。结果显示，这种方法在多个基准测试中表现出色，特别是在多模态推理任务中。作者希望这项研究能激发更多进展。代码、数据和模型将公开发布。",
        "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization",
        "pinyin": "这篇文章讨论了现有开源多模态大语言模型（MLLMs）的训练过程及其在分布偏移上的局限性。\nZhè piān wénzhāng tǎolùn le xiànyǒu kāiyuán duō móshì dà yǔyán móxíng (MLLMs) de xùnliàn guòchéng jí qí zài fēnbù piānyí shàng de júxìanxìng.\n\n为了提升多模态推理能力，作者引入了偏好优化（PO）过程。\nWèile tíshēng duō móshì tuīlǐ nénglì, zuòzhě yǐnrù le piānhào yōuhuà (PO) guòchéng.\n\n他们创建了一个高质量的多模态推理偏好数据集MMPR，并开发了一种混合偏好优化（MPO）方法。\nTāmen chuàngjiàn le yīgè gāo zhìliàng de duō móshì tuīlǐ piānhào shùjùjí MMPR, bìng kāifā le yīzhǒng hùnhé piānhào yōuhuà (MPO) fāngfǎ.\n\n结果显示，这种方法在多个基准测试中表现出色，特别是在多模态推理任务中。\nJiégǔo xiǎnshì, zhè zhǒng fāngfǎ zài duō gè jīzhǔn cèshì zhōng biǎoxiàn chūsè, tèbié shì zài duō móshì tuīlǐ rènwù zhōng.\n\n作者希望这项研究能激发更多进展。\nZuòzhě xīwàng zhè xiàng yánjiū néng jīfā gèng duō jìnzhǎn.\n\n代码、数据和模型将公开发布。\nDàimǎ, shùjù hé móxíng jiāng gōngkāi fābù.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'},\n{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open-source'},\n{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'},\n{'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'},\n{'word': '局限性', 'pinyin': 'jú xiàn xìng', 'trans': 'limitations'},\n{'word': '提升', 'pinyin': 'tí shēng', 'trans': 'enhance'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'},\n{'word': '偏好', 'pinyin': 'piān hào', 'trans': 'preference'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'},\n{'word': '创建', 'pinyin': 'chuàng jiàn', 'trans': 'create'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high-quality'},\n{'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'},\n{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'develop'},\n{'word': '混合', 'pinyin': 'hùn hé', 'trans': 'hybrid'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '特别', 'pinyin': 'tè bié', 'trans': 'particularly'},\n{'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'},\n{'word': '激发', 'pinyin': 'jī fā', 'trans': 'inspire'},\n{'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'}]",
        "trans": "This article discusses the training process of existing open-source multimodal large language models (MLLMs) and their limitations in distribution shifts. To enhance multimodal reasoning capabilities, the authors introduce a preference optimization (PO) process. They created a high-quality multimodal reasoning preference dataset called MMPR and developed a hybrid preference optimization (MPO) method. The results show that this method performs excellently on multiple benchmark tests, particularly in multimodal reasoning tasks. The authors hope that this research will inspire further advancements. The code, data, and models will be publicly released.",
        "update_ts": "2024-11-24 09:32"
    }
}