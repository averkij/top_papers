{
    "date": {
        "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 18",
        "zh": "2æœˆ18æ—¥"
    },
    "time_utc": "2025-02-18 20:11",
    "weekday": 1,
    "issue_id": 2280,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.11089",
            "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
            "url": "https://huggingface.co/papers/2502.11089",
            "abstract": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.",
            "score": 51,
            "issue_id": 2271,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "aa26756957a07b01",
            "authors": [
                "Jingyang Yuan",
                "Huazuo Gao",
                "Damai Dai",
                "Junyu Luo",
                "Liang Zhao",
                "Zhengyan Zhang",
                "Zhenda Xie",
                "Y. X. Wei",
                "Lean Wang",
                "Zhiping Xiao",
                "Yuqing Wang",
                "Chong Ruan",
                "Ming Zhang",
                "Wenfeng Liang",
                "Wangding Zeng"
            ],
            "affiliations": [
                "DeepSeek-AI",
                "Key Laboratory for Multimedia Information Processing, Peking University, PKU-Anker LLM Lab",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11089.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NSA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. NSA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ NSA ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 64 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficient Long-Context Modeling with Natively Trainable Sparse Attention",
                    "desc": "This paper introduces NSA, a new Sparse Attention mechanism designed for efficient long-context modeling in language models. NSA combines coarse-grained token compression with fine-grained token selection to enhance both global context and local detail while reducing computational costs. The method features innovations that optimize performance on modern hardware and allow for end-to-end training, which cuts down on pretraining time without losing effectiveness. Experimental results demonstrate that NSA not only matches but often surpasses traditional Full Attention models in various tasks, proving its efficiency and capability in handling long sequences."
                },
                "zh": {
                    "title": "é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„æ–°çªç ´",
                    "desc": "é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡å¯¹ä¸‹ä¸€ä»£è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„é«˜è®¡ç®—æˆæœ¬å¸¦æ¥äº†æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚ç¨€ç–æ³¨æ„åŠ›æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¯ä»¥åœ¨ä¿æŒæ¨¡å‹èƒ½åŠ›çš„åŒæ—¶æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†NSAï¼Œä¸€ç§åŸç”Ÿå¯è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆäº†ç®—æ³•åˆ›æ–°å’Œç¡¬ä»¶ä¼˜åŒ–ï¼Œä»¥å®ç°é«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚NSAé‡‡ç”¨åŠ¨æ€åˆ†å±‚ç¨€ç–ç­–ç•¥ï¼Œç»“åˆç²—ç²’åº¦çš„ä»¤ç‰Œå‹ç¼©å’Œç»†ç²’åº¦çš„ä»¤ç‰Œé€‰æ‹©ï¼Œæ—¢ä¿ç•™äº†å…¨å±€ä¸Šä¸‹æ–‡æ„è¯†ï¼Œåˆä¿æŒäº†å±€éƒ¨ç²¾åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12152",
            "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
            "url": "https://huggingface.co/papers/2502.12152",
            "abstract": "Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of humanoid locomotion learning, the getting-up task involves complex contact patterns, which necessitates accurately modeling the collision geometry and sparser rewards. We address these challenges through a two-phase approach that follows a curriculum. The first stage focuses on discovering a good getting-up trajectory under minimal constraints on smoothness or speed / torque limits. The second stage then refines the discovered motions into deployable (i.e. smooth and slow) motions that are robust to variations in initial configuration and terrains. We find these innovations enable a real-world G1 humanoid robot to get up from two main situations that we considered: a) lying face up and b) lying face down, both tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass and snowfield). To the best of our knowledge, this is the first successful demonstration of learned getting-up policies for human-sized humanoid robots in the real world. Project page: https://humanoid-getup.github.io/",
            "score": 28,
            "issue_id": 2266,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "c87382b187d7b745",
            "authors": [
                "Xialin He",
                "Runpei Dong",
                "Zixuan Chen",
                "Saurabh Gupta"
            ],
            "affiliations": [
                "Simon Fraser University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12152.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#robotics",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ²ÑÑ‚Ğ°Ğ²Ğ°Ñ‚ÑŒ: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼ Ğ²ÑÑ‚Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºÑƒÑ€Ñ€Ğ¸ĞºÑƒĞ»ÑƒĞ¼Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ğ¾Ğ´ÑŠĞµĞ¼Ğ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ G1 Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞºĞ¾Ğ»ÑŒĞ·ĞºĞ¸Ğµ Ğ¸ Ğ½Ğ°ĞºĞ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ÑŠĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ."
                },
                "en": {
                    "title": "Learning to Get Up: Humanoid Robots Rise to the Challenge!",
                    "desc": "This paper presents a novel learning framework for enabling humanoid robots to recover from falls by getting up from various positions and terrains. The approach consists of a two-phase curriculum where the first phase focuses on discovering effective getting-up trajectories with minimal constraints, while the second phase refines these motions to ensure they are smooth and robust. The framework addresses the complexities of contact patterns and collision geometry, which are critical for the getting-up task. The results demonstrate successful real-world applications, allowing a humanoid robot to rise from both face-up and face-down positions on challenging surfaces."
                },
                "zh": {
                    "title": "äººå½¢æœºå™¨äººè‡ªä¸»èµ·èº«çš„åˆ›æ–°å­¦ä¹ æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å­¦ä¹ æ¡†æ¶ï¼Œä½¿äººå½¢æœºå™¨äººèƒ½å¤Ÿä»ä¸åŒçš„å§¿åŠ¿å’Œåœ°å½¢ä¸­è‡ªè¡Œç«™èµ·ã€‚ç”±äºäººå½¢æœºå™¨äººåœ¨è·Œå€’åå¯èƒ½å¤„äºå¤šç§å¤æ‚çš„å§¿åŠ¿ï¼Œè®¾è®¡æ§åˆ¶å™¨éå¸¸å›°éš¾ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œç¬¬ä¸€é˜¶æ®µå‘ç°é€‚åˆçš„èµ·èº«è½¨è¿¹ï¼Œç¬¬äºŒé˜¶æ®µåˆ™å°†è¿™äº›è½¨è¿¹ä¼˜åŒ–ä¸ºå¹³æ»‘ä¸”ç¨³å¥çš„åŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿å¾—äººå½¢æœºå™¨äººèƒ½å¤Ÿåœ¨å¤šç§çœŸå®ç¯å¢ƒä¸­æˆåŠŸç«™èµ·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12115",
            "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?",
            "url": "https://huggingface.co/papers/2502.12115",
            "abstract": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \\1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to \\$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.",
            "score": 21,
            "issue_id": 2266,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "0b2638455d4393b0",
            "authors": [
                "Samuel Miserendino",
                "Michele Wang",
                "Tejal Patwardhan",
                "Johannes Heidecke"
            ],
            "affiliations": [
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12115.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ’»",
                "ru": {
                    "title": "SWE-Lancer: Ğ˜Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ",
                    "desc": "SWE-Lancer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1400 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Upwork Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ¾Ğ². Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ğµ (Ğ¾Ñ‚ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹) Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking AI's Potential in Freelance Software Engineering",
                    "desc": "SWE-Lancer is a new benchmark that includes over 1,400 freelance software engineering tasks sourced from Upwork, with a total value of $1 million. It features both independent tasks, like bug fixes and large feature implementations, and managerial tasks that require decision-making between different technical proposals. The tasks are rigorously evaluated, with independent tasks tested by experienced engineers and managerial decisions compared to those made by original engineering managers. Despite the evaluation, current advanced models struggle to complete most tasks, highlighting the need for further research in AI's economic implications."
                },
                "zh": {
                    "title": "SWE-Lancerï¼šæ¨åŠ¨AIæ¨¡å‹ç»æµå½±å“ç ”ç©¶çš„åŸºå‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†SWE-Lancerï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1400å¤šä¸ªæ¥è‡ªUpworkçš„è‡ªç”±è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„åŸºå‡†ï¼Œä»»åŠ¡æ€»ä»·å€¼è¾¾åˆ°100ä¸‡ç¾å…ƒã€‚SWE-LanceråŒ…æ‹¬ç‹¬ç«‹çš„å·¥ç¨‹ä»»åŠ¡ï¼Œå¦‚50ä¸ªbugä¿®å¤å’Œä»·å€¼32000ç¾å…ƒçš„åŠŸèƒ½å®ç°ï¼Œä»¥åŠç®¡ç†ä»»åŠ¡ï¼Œæ¨¡å‹éœ€è¦åœ¨æŠ€æœ¯å®æ–½ææ¡ˆä¸­è¿›è¡Œé€‰æ‹©ã€‚ç‹¬ç«‹ä»»åŠ¡é€šè¿‡ç»éªŒä¸°å¯Œçš„è½¯ä»¶å·¥ç¨‹å¸ˆè¿›è¡Œä¸‰é‡éªŒè¯çš„ç«¯åˆ°ç«¯æµ‹è¯•æ¥è¯„åˆ†ï¼Œè€Œç®¡ç†å†³ç­–åˆ™ä¸åŸé›‡ä½£çš„å·¥ç¨‹ç»ç†çš„é€‰æ‹©è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬è¯„ä¼°äº†æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°å‰æ²¿æ¨¡å‹ä»ç„¶æ— æ³•è§£å†³å¤§å¤šæ•°ä»»åŠ¡ï¼ŒSWE-Lancerçš„å¼€æºå°†ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11190",
            "title": "ReLearn: Unlearning via Learning for Large Language Models",
            "url": "https://huggingface.co/papers/2502.11190",
            "abstract": "Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.",
            "score": 15,
            "issue_id": 2266,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "83a57c7c971f5060",
            "authors": [
                "Haoming Xu",
                "Ningyuan Zhao",
                "Liming Yang",
                "Sendong Zhao",
                "Shumin Deng",
                "Mengru Wang",
                "Bryan Hooi",
                "Nay Oo",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "National University of Singapore",
                "Tsinghua University",
                "Xiamen University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11190.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#hallucinations",
                    "#open_source",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ReLearn: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReLearn Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ReLearn Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: KFR, KRR Ğ¸ LS. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReLearn ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "ReLearn: Effective Unlearning Without Sacrificing Coherence",
                    "desc": "This paper introduces ReLearn, a novel approach for unlearning in large language models that avoids the pitfalls of reverse optimization, which can harm the model's ability to predict subsequent tokens. ReLearn employs a data augmentation and fine-tuning pipeline that effectively targets specific knowledge for removal while maintaining the fluency and relevance of generated text. The authors propose new evaluation metrics, including Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR), to assess the balance between forgetting and retaining knowledge, alongside a Linguistic Score (LS) for output quality. Experimental results demonstrate that ReLearn achieves its unlearning goals without sacrificing the coherence of the model's text generation capabilities."
                },
                "zh": {
                    "title": "ReLearnï¼šé«˜æ•ˆå»å­¦ä¹ ä¸è¯­è¨€ç”Ÿæˆçš„å®Œç¾ç»“åˆ",
                    "desc": "å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å»å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–äºåå‘ä¼˜åŒ–æ¥é™ä½ç›®æ ‡è¯çš„æ¦‚ç‡ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šå¹²æ‰°åç»­è¯çš„é¢„æµ‹ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å’Œè¯­è¨€è¿è´¯æ€§ä¸‹é™ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡è¿‡äºå¼ºè°ƒä¸Šä¸‹æ–‡é—å¿˜ï¼Œè€Œå¯¹å“åº”çš„æµç•…æ€§å’Œç›¸å…³æ€§è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReLearnï¼Œä¸€ä¸ªæœ‰æ•ˆçš„å»å­¦ä¹ çš„æ•°æ®å¢å¼ºå’Œå¾®è°ƒæµç¨‹ï¼Œå¹¶å¼•å…¥äº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12148",
            "title": "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2502.12148",
            "abstract": "The remarkable success of the autoregressive paradigm has made significant advancement in Multimodal Large Language Models (MLLMs), with powerful models like Show-o, Transfusion and Emu3 achieving notable progress in unified image understanding and generation. For the first time, we uncover a common phenomenon: the understanding capabilities of MLLMs are typically stronger than their generative capabilities, with a significant gap between the two. Building on this insight, we propose HermesFlow, a simple yet general framework designed to seamlessly bridge the gap between understanding and generation in MLLMs. Specifically, we take the homologous data as input to curate homologous preference data of both understanding and generation. Through Pair-DPO and self-play iterative optimization, HermesFlow effectively aligns multimodal understanding and generation using homologous preference data. Extensive experiments demonstrate the significant superiority of our approach over prior methods, particularly in narrowing the gap between multimodal understanding and generation. These findings highlight the potential of HermesFlow as a general alignment framework for next-generation multimodal foundation models. Code: https://github.com/Gen-Verse/HermesFlow",
            "score": 14,
            "issue_id": 2265,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "09e80125d90fd3df",
            "authors": [
                "Ling Yang",
                "Xinchen Zhang",
                "Ye Tian",
                "Chenming Shang",
                "Minghao Xu",
                "Wentao Zhang",
                "Bin Cui"
            ],
            "affiliations": [
                "Mila - Quebec AI Institute",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12148.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#alignment"
                ],
                "emoji": "ğŸŒ‰",
                "ru": {
                    "title": "HermesFlow: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº HermesFlow Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. HermesFlow Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ğ¾Ğ¼Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Pair-DPO Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ HermesFlow Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² MLLM."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Understanding and Generation in MLLMs with HermesFlow",
                    "desc": "This paper discusses the advancements in Multimodal Large Language Models (MLLMs) and identifies a key issue: these models often understand information better than they can generate it. The authors introduce HermesFlow, a new framework that aims to improve the balance between understanding and generation in MLLMs. By using homologous data to create preference data for both tasks, HermesFlow employs Pair-DPO and self-play optimization to align these capabilities more effectively. Experimental results show that HermesFlow significantly reduces the performance gap between understanding and generation, suggesting its potential as a foundational model for future multimodal applications."
                },
                "zh": {
                    "title": "HermesFlowï¼šç¼©å°ç†è§£ä¸ç”Ÿæˆçš„å·®è·",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è‡ªå›å½’èŒƒå¼åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åƒShow-oã€Transfusionå’ŒEmu3è¿™æ ·çš„æ¨¡å‹åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ç ”ç©¶å‘ç°ï¼ŒMLLMsçš„ç†è§£èƒ½åŠ›é€šå¸¸å¼ºäºç”Ÿæˆèƒ½åŠ›ï¼Œä¸¤è€…ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†HermesFlowæ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨åŒæºæ•°æ®æ¥ä¼˜åŒ–ç†è§£å’Œç”Ÿæˆä¹‹é—´çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHermesFlowåœ¨ç¼©å°å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·æ–¹é¢ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å¯¹é½æ¡†æ¶çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11196",
            "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training",
            "url": "https://huggingface.co/papers/2502.11196",
            "abstract": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.",
            "score": 11,
            "issue_id": 2266,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "d97eafe0888b9da4",
            "authors": [
                "Yixin Ou",
                "Yunzhi Yao",
                "Ningyu Zhang",
                "Hui Jin",
                "Jiacheng Sun",
                "Shumin Deng",
                "Zhenguo Li",
                "Huajun Chen"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "National University of Singapore, NUS-NCS Joint Lab, Singapore",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11196.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#architecture",
                    "#transfer_learning",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ†ĞµĞ¿ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² LLM",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² ÑƒÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ 'Ñ†ĞµĞ¿ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹' - Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ° Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ²Ğ¾ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¸Ñ… ÑĞ²ÑĞ·Ğ¸ Ñ ÑƒĞ¶Ğµ Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ, Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ†ĞµĞ¿ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ğ°Ğ·Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Understanding Knowledge Integration in Large Language Models",
                    "desc": "This paper explores how Large Language Models (LLMs) learn and store new knowledge within their neural networks. It introduces the concept of knowledge circuit evolution, which refers to the computational pathways that help LLMs process and retain information. The study finds that new knowledge is better integrated when it relates to what the model already knows, and that the process of knowledge circuit evolution shifts from forming new connections to optimizing existing ones. Additionally, the research reveals that this evolution tends to move from deeper to shallower layers of the model, offering insights that could improve continual pre-training methods for LLMs."
                },
                "zh": {
                    "title": "ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å†…åŒ–",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨ç†è§£å¦‚ä½•å†…åŒ–æ–°çŸ¥è¯†æ–¹é¢å­˜åœ¨é‡è¦ç¼ºå£ã€‚æœ¬æ–‡é€šè¿‡çŸ¥è¯†ç”µè·¯æ¼”åŒ–çš„è§†è§’ï¼Œè¯†åˆ«å‡ºä¿ƒè¿›çŸ¥è¯†å­˜å‚¨å’Œå¤„ç†çš„è®¡ç®—å­å›¾ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåˆ†æè¡¨æ˜ï¼Œæ–°çŸ¥è¯†çš„è·å–å—å·²æœ‰çŸ¥è¯†ç›¸å…³æ€§çš„å½±å“ï¼ŒçŸ¥è¯†ç”µè·¯çš„æ¼”åŒ–ç»å†ä»å½¢æˆåˆ°ä¼˜åŒ–çš„æ˜æ˜¾é˜¶æ®µè½¬å˜ï¼Œå¹¶ä¸”æ¼”åŒ–æ¨¡å¼å‘ˆç°ç”±æ·±åˆ°æµ…çš„ç‰¹å¾ã€‚è¿™äº›å‘ç°ä¸ä»…æ¨åŠ¨äº†æˆ‘ä»¬å¯¹LLMsä¸­æ–°çŸ¥è¯†è·å–æœºåˆ¶çš„ç†è®ºç†è§£ï¼Œè¿˜æœ‰åŠ©äºæ”¹è¿›æŒç»­é¢„è®­ç»ƒç­–ç•¥ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11167",
            "title": "SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors",
            "url": "https://huggingface.co/papers/2502.11167",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of a program without actually running it. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple open-source and proprietary LLMs on SURGE and conduct a scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at https://github.com/Imbernoulli/SURGE.",
            "score": 10,
            "issue_id": 2266,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "99e183fd31de3be0",
            "authors": [
                "Bohan Lyu",
                "Siqiao Huang",
                "Zichen Liang"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua",
                "Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11167.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#plp",
                    "#dataset",
                    "#agi",
                    "#open_source",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»Ğ¸ ĞºĞ¾Ğ´Ğ°: Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SURGE - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ±ĞµĞ· ĞµĞ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ½Ğ° SURGE Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…, Ğ½Ğ¾ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°ĞµÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Exploring LLMs as Surrogate Code Executors with SURGE",
                    "desc": "This paper explores the potential of large language models (LLMs) to act as surrogate code executors, which means predicting the output of code without actually running it. The authors introduce a benchmark called SURGE, which tests LLMs on various programming tasks, including multi-language support and analysis of complex algorithms. They evaluate different LLMs to see how well they can predict code execution results and identify common errors in their predictions. The results show that while LLMs can succeed in some scenarios, they still have significant limitations in general-purpose code execution prediction."
                },
                "zh": {
                    "title": "æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºä»£ç æ‰§è¡Œå™¨çš„æ½œåŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬èƒ½å¦ä½œä¸ºé€šç”¨çš„æ›¿ä»£ä»£ç æ‰§è¡Œå™¨æ¥é¢„æµ‹ç¨‹åºçš„è¾“å‡ºå’Œè¡Œä¸ºä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SURGEï¼Œä¸€ä¸ªæ¶µç›–å…«ä¸ªå…³é”®æ–¹é¢çš„ç»¼åˆåŸºå‡†ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€ç¼–ç¨‹ä»»åŠ¡å’Œé«˜æˆæœ¬ç§‘å­¦è®¡ç®—ç­‰ã€‚æˆ‘ä»¬å¯¹å¤šç§å¼€æºå’Œä¸“æœ‰çš„LLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ç ”ç©¶äº†æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®è§„æ¨¡å¯¹æ›¿ä»£æ‰§è¡Œå‡†ç¡®æ€§çš„å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨æŸäº›æƒ…å†µä¸‹èƒ½å¤Ÿé¢„æµ‹ä»£ç æ‰§è¡Œç»“æœï¼Œä½†åœ¨é€šç”¨æ›¿ä»£æ‰§è¡Œæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08745",
            "title": "IHEval: Evaluating Language Models on Following the Instruction Hierarchy",
            "url": "https://huggingface.co/papers/2502.08745",
            "abstract": "The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models' ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated models experience a sharp performance decline when facing conflicting instructions, compared to their original instruction-following performance. Moreover, the most competitive open-source model only achieves 48% accuracy in resolving such conflicts. Our results underscore the need for targeted optimization in the future development of LMs.",
            "score": 9,
            "issue_id": 2279,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "b61f4ac9281295ed",
            "authors": [
                "Zhihan Zhang",
                "Shiyang Li",
                "Zixuan Zhang",
                "Xin Liu",
                "Haoming Jiang",
                "Xianfeng Tang",
                "Yifan Gao",
                "Zheng Li",
                "Haodong Wang",
                "Zhaoxuan Tan",
                "Yichuan Li",
                "Qingyu Yin",
                "Bing Yin",
                "Meng Jiang"
            ],
            "affiliations": [
                "Amazon",
                "University of Notre Dame",
                "Worcester Polytechnic Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08745.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹: Ğ°Ñ…Ğ¸Ğ»Ğ»ĞµÑĞ¾Ğ²Ğ° Ğ¿ÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IHEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 3,538 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¯Ğœ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¸Ñ… Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ·ĞºĞ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»Ğ¸ÑˆÑŒ 48% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Language Models: Prioritizing Instructions for Better Performance",
                    "desc": "This paper discusses the importance of instruction hierarchy in language models (LMs), which helps prioritize system messages, user messages, and conversation history. The authors introduce IHEval, a new benchmark with 3,538 examples across nine tasks to evaluate how well LMs follow these instruction priorities. Their findings reveal that popular LMs struggle significantly when faced with conflicting instructions, showing a notable drop in performance. The results indicate a critical need for improvements in LMs to better handle instruction hierarchies and conflicts in the future."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤ç†è§£èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æŒ‡ä»¤å±‚çº§çš„é‡è¦æ€§ï¼ŒæŒ‡ä»¤å±‚çº§ä»ç³»ç»Ÿæ¶ˆæ¯åˆ°ç”¨æˆ·æ¶ˆæ¯ã€å¯¹è¯å†å²å’Œå·¥å…·è¾“å‡ºå»ºç«‹äº†ä¼˜å…ˆé¡ºåºã€‚è¿™ä¸€ä¸»é¢˜åœ¨è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ä¸­å—åˆ°çš„å…³æ³¨æœ‰é™ï¼Œç¼ºä¹å…¨é¢çš„åŸºå‡†æ¥è¯„ä¼°æ¨¡å‹éµå¾ªæŒ‡ä»¤å±‚çº§çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†IHEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼ŒåŒ…å«3,538ä¸ªç¤ºä¾‹ï¼Œæ¶µç›–äº†ä¸åŒä¼˜å…ˆçº§æŒ‡ä»¤ä¸€è‡´æˆ–å†²çªçš„ä¹ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œæµè¡Œçš„è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«æŒ‡ä»¤ä¼˜å…ˆçº§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶åœ¨é¢å¯¹å†²çªæŒ‡ä»¤æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10458",
            "title": "I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models",
            "url": "https://huggingface.co/papers/2502.10458",
            "abstract": "This paper presents ThinkDiff, a novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on pixel-level reconstruction rather than in-context reasoning, and are constrained by the complexity and limited availability of reasoning-based datasets. ThinkDiff addresses these challenges by leveraging vision-language training as a proxy task, aligning VLMs with the decoder of an encoder-decoder large language model (LLM) instead of a diffusion decoder. This proxy task builds on the observation that the LLM decoder shares the same input feature space with diffusion decoders that use the corresponding LLM encoder for prompt embedding. As a result, aligning VLMs with diffusion decoders can be simplified through alignment with the LLM decoder. Without complex training and datasets, ThinkDiff effectively unleashes understanding, reasoning, and composing capabilities in diffusion models. Experiments demonstrate that ThinkDiff significantly improves accuracy from 19.2% to 46.3% on the challenging CoBSAT benchmark for multimodal in-context reasoning generation, with only 5 hours of training on 4 A100 GPUs. Additionally, ThinkDiff demonstrates exceptional performance in composing multiple images and texts into logically coherent images. Project page: https://mizhenxing.github.io/ThinkDiff.",
            "score": 9,
            "issue_id": 2270,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "bd036baca649b696",
            "authors": [
                "Zhenxing Mi",
                "Kuan-Chieh Wang",
                "Guocheng Qian",
                "Hanrong Ye",
                "Runtao Liu",
                "Sergey Tulyakov",
                "Kfir Aberman",
                "Dan Xu"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering (CSE), The Hong Kong University of Science and Technology (HKUST)",
                "Snap Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10458.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#diffusion",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ThinkDiff: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ThinkDiff - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° (VLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºÑƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ VLM Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. ThinkDiff ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ 19.2% Ğ´Ğ¾ 46.3% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ CoBSAT Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ."
                },
                "en": {
                    "title": "Empowering Diffusion Models with Multimodal Reasoning",
                    "desc": "ThinkDiff is a new approach that enhances text-to-image diffusion models by integrating vision-language models (VLMs) for better understanding and reasoning. Unlike previous methods that focused on pixel-level accuracy, ThinkDiff emphasizes in-context reasoning, overcoming challenges related to complex datasets. It aligns VLMs with the decoder of a large language model (LLM), simplifying the training process and improving performance. Experiments show that ThinkDiff significantly boosts accuracy on multimodal reasoning tasks and excels in creating coherent images from multiple inputs."
                },
                "zh": {
                    "title": "ThinkDiffï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒçš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ThinkDiffï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¯¹é½èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ä¼˜åŠ¿ï¼Œå¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æ‰©æ•£å¾®è°ƒæ–¹æ³•ä¸»è¦å…³æ³¨åƒç´ çº§é‡å»ºï¼Œè€Œå¿½è§†äº†ä¸Šä¸‹æ–‡æ¨ç†ï¼Œå¹¶å—åˆ°æ¨ç†åŸºç¡€æ•°æ®é›†å¤æ‚æ€§å’Œæœ‰é™æ€§çš„é™åˆ¶ã€‚ThinkDiffé€šè¿‡å°†è§†è§‰-è¯­è¨€è®­ç»ƒä½œä¸ºä»£ç†ä»»åŠ¡ï¼Œè§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼Œå°†VLMä¸ç¼–ç å™¨-è§£ç å™¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§£ç å™¨å¯¹é½ï¼Œè€Œä¸æ˜¯æ‰©æ•£è§£ç å™¨ã€‚å®éªŒè¡¨æ˜ï¼ŒThinkDiffåœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ¨ç†ç”Ÿæˆçš„CoBSATåŸºå‡†æµ‹è¯•ä¸­ï¼Œå‡†ç¡®ç‡ä»19.2%æ˜¾è‘—æé«˜åˆ°46.3%ï¼Œä»…éœ€åœ¨4ä¸ªA100 GPUä¸Šè®­ç»ƒ5å°æ—¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12146",
            "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
            "url": "https://huggingface.co/papers/2502.12146",
            "abstract": "We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening",
            "score": 9,
            "issue_id": 2265,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "25a69f8cf847067e",
            "authors": [
                "Ye Tian",
                "Ling Yang",
                "Xinchen Zhang",
                "Yunhai Tong",
                "Mengdi Wang",
                "Bin Cui"
            ],
            "affiliations": [
                "Peking University",
                "Princeton University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12146.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#diffusion",
                    "#alignment",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Diffusion-Sharpening Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ğ»Ñ‹ Ğ¿Ğ¾ Ğ¿ÑƒÑ‚ÑĞ¼ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Diffusion-Sharpening Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Optimize Sampling Paths for Better Model Alignment!",
                    "desc": "The paper introduces Diffusion-Sharpening, a novel fine-tuning method that improves the alignment of machine learning models by optimizing the paths taken during sampling. Unlike traditional reinforcement learning (RL) methods that focus on individual training steps, this approach considers the entire trajectory, which enhances overall performance. By employing a path integral framework, Diffusion-Sharpening efficiently selects the best trajectories while minimizing inference costs. Experimental results show that this method not only converges faster but also achieves better performance than existing RL-based and trajectory optimization techniques across various evaluation metrics."
                },
                "zh": {
                    "title": "æ‰©æ•£é”åŒ–ï¼šé«˜æ•ˆçš„å¾®è°ƒæ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ‰©æ•£é”åŒ–ï¼ˆDiffusion-Sharpeningï¼‰çš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–é‡‡æ ·è½¨è¿¹æ¥å¢å¼ºä¸‹æ¸¸å¯¹é½ã€‚ç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸ªè®­ç»ƒæ—¶é—´æ­¥ï¼Œå¿½è§†äº†è½¨è¿¹çº§åˆ«çš„å¯¹é½ï¼Œè€Œæœ€è¿‘çš„é‡‡æ ·è½¨è¿¹ä¼˜åŒ–æ–¹æ³•åˆ™å¸¦æ¥äº†æ˜¾è‘—çš„æ¨ç†æˆæœ¬ã€‚æ‰©æ•£é”åŒ–é€šè¿‡ä½¿ç”¨è·¯å¾„ç§¯åˆ†æ¡†æ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€‰æ‹©æœ€ä½³è½¨è¿¹ï¼Œåˆ©ç”¨å¥–åŠ±åé¦ˆå¹¶æ‘Šé”€æ¨ç†æˆæœ¬ï¼Œä»è€Œå…‹æœäº†è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰©æ•£é”åŒ–åœ¨è®­ç»ƒæ•ˆç‡å’Œæ¨ç†æ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å¾®è°ƒæ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„æœªæ¥æ‰©æ•£æ¨¡å‹å¾®è°ƒè§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09061",
            "title": "CRANE: Reasoning with constrained LLM generation",
            "url": "https://huggingface.co/papers/2502.09061",
            "abstract": "Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.",
            "score": 9,
            "issue_id": 2264,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "4a44947deeb14cf4",
            "authors": [
                "Debangshu Banerjee",
                "Tarun Suresh",
                "Shubham Ugare",
                "Sasa Misailovic",
                "Gagandeep Singh"
            ],
            "affiliations": [
                "Department of Computer Science, University of Illinois Urbana-Champaign, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09061.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LLM Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ CRANE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CRANE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Correctness and Reasoning in LLM Outputs with CRANE",
                    "desc": "This paper discusses the challenges of generating outputs from large language models (LLMs) that are both correct in form and meaning, especially in tasks like code generation and symbolic reasoning. It explains that overly strict constraints on the grammar can hinder the model's reasoning abilities. The authors propose a new approach called CRANE, which enhances the output grammar with additional rules to maintain reasoning capabilities while ensuring syntactic and semantic correctness. Their experiments show that CRANE outperforms existing methods, achieving significant accuracy improvements on difficult reasoning tasks."
                },
                "zh": {
                    "title": "å¹³è¡¡æ¨ç†èƒ½åŠ›ä¸ç”Ÿæˆæ­£ç¡®æ€§çš„åˆ›æ–°è§£ç ç®—æ³•",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åœ¨ç”Ÿæˆä»£ç å’Œç¬¦å·æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­ï¼Œç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºçš„è¯­æ³•å’Œè¯­ä¹‰æ­£ç¡®æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¿‡äºä¸¥æ ¼çš„è¯­æ³•çº¦æŸä¼šé™ä½æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§£ç ç®—æ³•CRANEï¼Œé€šè¿‡å¢åŠ ç²¾å¿ƒè®¾è®¡çš„é¢å¤–è§„åˆ™ï¼Œæ—¢èƒ½ä¿æŒè¾“å‡ºçš„æ­£ç¡®æ€§ï¼Œåˆèƒ½å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCRANEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§£ç ç­–ç•¥ï¼Œæå‡äº†ç¬¦å·æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11831",
            "title": "Intuitive physics understanding emerges from self-supervised pretraining on natural videos",
            "url": "https://huggingface.co/papers/2502.11831",
            "abstract": "We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.",
            "score": 6,
            "issue_id": 2270,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "269bf0ee605c4537",
            "authors": [
                "Quentin Garrido",
                "Nicolas Ballas",
                "Mahmoud Assran",
                "Adrien Bardes",
                "Laurent Najman",
                "Michael Rabbat",
                "Emmanuel Dupoux",
                "Yann LeCun"
            ],
            "affiliations": [
                "EHESS",
                "FAIR at Meta",
                "Univ Gustave Eiffel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11831.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#architecture",
                    "#multimodal",
                    "#agi"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑÑ… Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğ¹, Ğ¾Ğ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². ĞĞ°Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼, Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğº ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Learning Intuitive Physics Through Video Prediction",
                    "desc": "This paper explores how deep neural networks can learn intuitive physics by predicting missing parts of videos. The researchers found that models trained in a learned representation space can grasp concepts like object permanence and shape consistency. In contrast, models that operate directly on pixel data or rely on text reasoning perform poorly. The study suggests that understanding intuitive physics can emerge from training on sensory input without needing pre-existing core knowledge."
                },
                "zh": {
                    "title": "é€šè¿‡é¢„æµ‹å­¦ä¹ ç›´è§‚ç‰©ç†ç†è§£",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†é€šç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨é¢„æµ‹è‡ªç„¶è§†é¢‘ä¸­è¢«é®æŒ¡åŒºåŸŸæ—¶ï¼Œå¦‚ä½•äº§ç”Ÿç›´è§‚ç‰©ç†ç†è§£ã€‚æˆ‘ä»¬å‘ç°ï¼Œç»è¿‡è®­ç»ƒçš„æ¨¡å‹åœ¨å­¦ä¹ çš„è¡¨ç¤ºç©ºé—´ä¸­é¢„æµ‹ç»“æœæ—¶ï¼Œèƒ½å¤Ÿç†è§£ç‰©ä½“çš„æŒä¹…æ€§å’Œå½¢çŠ¶ä¸€è‡´æ€§ç­‰ç›´è§‚ç‰©ç†ç‰¹æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨åƒç´ ç©ºé—´ä¸­è¿›è¡Œè§†é¢‘é¢„æµ‹çš„æ¨¡å‹å’Œé€šè¿‡æ–‡æœ¬æ¨ç†çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…¶è¡¨ç°æ¥è¿‘éšæœºæ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè”åˆå­¦ä¹ æŠ½è±¡è¡¨ç¤ºç©ºé—´å¹¶é¢„æµ‹æ„Ÿå®˜è¾“å…¥çš„ç¼ºå¤±éƒ¨åˆ†ï¼Œè¶³ä»¥è·å¾—ç›´è§‚ç‰©ç†çš„ç†è§£ï¼Œç”šè‡³åœ¨ä»…ç”¨ä¸€å‘¨ç‹¬ç‰¹è§†é¢‘è®­ç»ƒçš„æ¨¡å‹ä¹Ÿèƒ½è¡¨ç°å‡ºè¶…å‡ºéšæœºçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11438",
            "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL",
            "url": "https://huggingface.co/papers/2502.11438",
            "abstract": "Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.",
            "score": 6,
            "issue_id": 2264,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "93b545df707b1538",
            "authors": [
                "Jimin Lee",
                "Ingeol Baek",
                "Byeongjeong Kim",
                "Hwanhee Lee"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11438.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#transfer_learning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ² Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² SQL",
                    "desc": "SAFE-SQL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. SAFE-SQL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… zero-shot Ğ¸ few-shot, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ³Ğ´Ğµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ."
                },
                "en": {
                    "title": "Transforming Language to SQL with Self-Augmented Learning",
                    "desc": "This paper introduces SAFE-SQL, a new framework for converting natural language questions into SQL queries. It addresses the limitations of previous methods that rely on existing training examples, which may not be available in real-world situations. SAFE-SQL enhances SQL generation by creating and filtering self-generated examples using a large language model (LLM). The framework demonstrates improved execution accuracy, especially in challenging and unseen scenarios, outperforming traditional zero-shot and few-shot approaches."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å¢å¼ºï¼Œæå‡Text-to-SQLçš„å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶SAFE-SQLï¼Œç”¨äºå°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªæˆ‘å¢å¼ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ å’Œç»†ç²’åº¦ç¤ºä¾‹é€‰æ‹©æ¥æé«˜SQLç”Ÿæˆçš„è´¨é‡ã€‚SAFE-SQLé¦–å…ˆç”Ÿæˆå¤šä¸ªä¸æµ‹è¯•è¾“å…¥ç›¸å…³çš„Text-to-SQLç¤ºä¾‹ï¼Œç„¶åé€šè¿‡ä¸‰ç§ç›¸å…³æ€§è¯„ä¼°å¯¹è¿™äº›ç¤ºä¾‹è¿›è¡Œè¿‡æ»¤ï¼Œä»è€Œæ„å»ºé«˜è´¨é‡çš„å­¦ä¹ ç¤ºä¾‹ã€‚ä¸ä¼ ç»Ÿçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ–¹æ³•ç›¸æ¯”ï¼ŒSAFE-SQLåœ¨æ‰§è¡Œå‡†ç¡®æ€§ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨å›°éš¾å’Œæœªè§è¿‡çš„åœºæ™¯ä¸­è¡¨ç°æ›´ä½³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12135",
            "title": "MagicArticulate: Make Your 3D Models Articulation-Ready",
            "url": "https://huggingface.co/papers/2502.12135",
            "abstract": "With the explosive growth of 3D content creation, there is an increasing demand for automatically converting static 3D models into articulation-ready versions that support realistic animation. Traditional approaches rely heavily on manual annotation, which is both time-consuming and labor-intensive. Moreover, the lack of large-scale benchmarks has hindered the development of learning-based solutions. In this work, we present MagicArticulate, an effective framework that automatically transforms static 3D models into articulation-ready assets. Our key contributions are threefold. First, we introduce Articulation-XL, a large-scale benchmark containing over 33k 3D models with high-quality articulation annotations, carefully curated from Objaverse-XL. Second, we propose a novel skeleton generation method that formulates the task as a sequence modeling problem, leveraging an auto-regressive transformer to naturally handle varying numbers of bones or joints within skeletons and their inherent dependencies across different 3D models. Third, we predict skinning weights using a functional diffusion process that incorporates volumetric geodesic distance priors between vertices and joints. Extensive experiments demonstrate that MagicArticulate significantly outperforms existing methods across diverse object categories, achieving high-quality articulation that enables realistic animation. Project page: https://chaoyuesong.github.io/MagicArticulate.",
            "score": 5,
            "issue_id": 2270,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "ff42a91250856a78",
            "authors": [
                "Chaoyue Song",
                "Jianfeng Zhang",
                "Xiu Li",
                "Fan Yang",
                "Yiwen Chen",
                "Zhongcong Xu",
                "Jun Hao Liew",
                "Xiaoyang Guo",
                "Fayao Liu",
                "Jiashi Feng",
                "Guosheng Lin"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Institute for Inforcomm Research, A*STAR",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12135.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "ĞœĞ°Ğ³Ğ¸Ñ Ğ¾Ğ¶Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D: Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "MagicArticulate - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğº Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€ÑĞ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Articulation-XL - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 33 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² ÑĞºĞ¸Ğ½Ğ½Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ´ĞµĞ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµÑ€ÑˆĞ¸Ğ½Ğ°Ğ¼Ğ¸ Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming 3D Models for Realistic Animation with MagicArticulate",
                    "desc": "This paper introduces MagicArticulate, a framework designed to convert static 3D models into articulation-ready versions for realistic animation. It addresses the challenges of manual annotation by providing Articulation-XL, a large-scale benchmark with over 33,000 3D models and high-quality articulation annotations. The framework employs a novel skeleton generation method using an auto-regressive transformer to manage varying bone structures and their dependencies. Additionally, it utilizes a functional diffusion process to predict skinning weights, resulting in superior performance in generating articulated models compared to existing methods."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–3Dæ¨¡å‹å…³èŠ‚åŒ–çš„é©å‘½æ€§æ¡†æ¶",
                    "desc": "éšç€3Då†…å®¹åˆ›ä½œçš„å¿«é€Ÿå¢é•¿ï¼Œè‡ªåŠ¨å°†é™æ€3Dæ¨¡å‹è½¬æ¢ä¸ºå¯è¿›è¡ŒçœŸå®åŠ¨ç”»çš„å…³èŠ‚æ¨¡å‹çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨æ ‡æ³¨ï¼Œæ—¢è€—æ—¶åˆè´¹åŠ›ï¼Œä¸”ç¼ºä¹å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é™åˆ¶äº†åŸºäºå­¦ä¹ çš„è§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚æˆ‘ä»¬æå‡ºäº†MagicArticulateæ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†é™æ€3Dæ¨¡å‹è½¬åŒ–ä¸ºé€‚åˆå…³èŠ‚åŠ¨ç”»çš„èµ„äº§ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬å»ºç«‹äº†Articulation-XLåŸºå‡†ã€æå‡ºäº†ä¸€ç§æ–°é¢–çš„éª¨æ¶ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨åŠŸèƒ½æ‰©æ•£è¿‡ç¨‹é¢„æµ‹è’™çš®æƒé‡ï¼Œæ˜¾è‘—æå‡äº†åŠ¨ç”»è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11275",
            "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
            "url": "https://huggingface.co/papers/2502.11275",
            "abstract": "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token prediction into extraction for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, Cuckoo, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.",
            "score": 5,
            "issue_id": 2263,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "6444052efad6f8be",
            "authors": [
                "Letian Peng",
                "Zilong Wang",
                "Feng Yao",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11275.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#data",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ£",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ğ»ĞµÑ‡Ğ°Ñ… Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ²: ĞºĞ°Ğº IE Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (IE) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²' (NTE) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Cuckoo, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 102,6 Ğ¼Ğ»Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ IE Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ IE Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ LLM Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Leveraging LLMs for Enhanced Information Extraction",
                    "desc": "This paper introduces a new approach for information extraction (IE) using large language models (LLMs) as a resource. The authors propose a method called next tokens extraction (NTE), which allows IE models to leverage existing LLM data for training. They present a model named Cuckoo, which is trained on 102.6 million extractive data points derived from LLMs, showing superior performance in few-shot scenarios. Cuckoo's design enables it to adapt to various IE tasks while benefiting from ongoing improvements in LLM training without requiring extra manual data preparation."
                },
                "zh": {
                    "title": "åˆ©ç”¨LLMæå‡ä¿¡æ¯æå–æ¨¡å‹çš„æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æå‡ä¿¡æ¯æå–ï¼ˆIEï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æå–æ–¹æ³•ï¼Œç§°ä¸ºä¸‹ä¸€æ ‡è®°æå–ï¼ˆNTEï¼‰ï¼Œé€šè¿‡å°†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è½¬åŒ–ä¸ºå¯¹ä¸Šä¸‹æ–‡ä¸­å·²å­˜åœ¨æ ‡è®°çš„æå–ï¼Œä»è€Œä½¿IEæ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨LLMçš„èµ„æºã€‚æˆ‘ä»¬å¼€å‘çš„Cuckooæ¨¡å‹åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”ä¼ ç»Ÿå’Œå¤æ‚çš„æŒ‡ä»¤è·ŸéšIEä»»åŠ¡ï¼Œå¹¶ä¸”è¡¨ç°ä¼˜äºç°æœ‰çš„é¢„è®­ç»ƒIEæ¨¡å‹ã€‚Cuckooä½œä¸ºä¸€ä¸ªâ€œæ­ä¾¿è½¦è€…â€ï¼Œèƒ½å¤Ÿéšç€LLMæ•°æ®å‡†å¤‡çš„è¿›æ­¥è€Œè‡ªç„¶æ¼”å˜ï¼Œæ— éœ€é¢å¤–çš„äººå·¥åŠªåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11157",
            "title": "Dyve: Thinking Fast and Slow for Dynamic Process Verification",
            "url": "https://huggingface.co/papers/2502.11157",
            "abstract": "We present Dyve, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation System 1 for straightforward steps and comprehensive analysis System 2 for complex ones. Leveraging a novel step-wise consensus-filtered process supervision technique, combining Monte Carlo estimation with LLM based evaluation, Dyve curates high-quality supervision signals from noisy data. Experimental results on ProcessBench and the MATH dataset confirm that Dyve significantly outperforms existing process-based verifiers and boosts performance in Best-of-N settings.",
            "score": 4,
            "issue_id": 2272,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "df0803abac7073f4",
            "authors": [
                "Jianyuan Zhong",
                "Zeju Li",
                "Zhijian Xu",
                "Xiangyu Wen",
                "Qiang Xu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11157.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#training",
                    "#optimization",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Dyve: ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ˜Ğ˜-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Dyve - Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ‚ĞµĞ¾Ñ€Ğ¸ĞµĞ¹ ĞšĞ°Ğ½ĞµĞ¼Ğ°Ğ½Ğ° Ğ¾ Ğ´Ğ²ÑƒÑ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. Dyve Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ProcessBench Ğ¸ MATH Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Dyve Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Dyve: Enhancing Language Model Reasoning with Dual Thinking Strategies",
                    "desc": "Dyve is a dynamic process verifier designed to improve error detection in large language models by utilizing two types of reasoning: fast (System 1) and slow (System 2) thinking. It applies quick, token-level checks for simple tasks while employing in-depth analysis for more complex processes. The system uses a unique method of step-wise consensus filtering, which combines Monte Carlo estimation with evaluations from large language models to generate reliable supervision signals from noisy data. Experiments show that Dyve outperforms current process verifiers and enhances performance in competitive settings."
                },
                "zh": {
                    "title": "Dyveï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†çš„åŠ¨æ€éªŒè¯å™¨",
                    "desc": "Dyveæ˜¯ä¸€ç§åŠ¨æ€è¿‡ç¨‹éªŒè¯å™¨ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†é”™è¯¯æ£€æµ‹èƒ½åŠ›ã€‚å®ƒç»“åˆäº†å¿«é€Ÿæ€ç»´å’Œæ…¢é€Ÿæ€ç»´ï¼Œçµæ„Ÿæ¥æºäºå¡å°¼æ›¼çš„ç³»ç»Ÿç†è®ºã€‚Dyveæ ¹æ®ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œçµæ´»åœ°åº”ç”¨å³æ—¶çš„tokençº§ç¡®è®¤ï¼ˆç³»ç»Ÿ1ï¼‰å’Œå…¨é¢åˆ†æï¼ˆç³»ç»Ÿ2ï¼‰ã€‚é€šè¿‡ä¸€ç§æ–°é¢–çš„é€æ­¥å…±è¯†è¿‡æ»¤è¿‡ç¨‹ç›‘ç£æŠ€æœ¯ï¼ŒDyveä»å˜ˆæ‚æ•°æ®ä¸­æå–é«˜è´¨é‡çš„ç›‘ç£ä¿¡å·ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¿‡ç¨‹éªŒè¯å™¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11775",
            "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
            "url": "https://huggingface.co/papers/2502.11775",
            "abstract": "While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities.",
            "score": 4,
            "issue_id": 2265,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "ff6f52de37532ca7",
            "authors": [
                "Guangzhi Sun",
                "Yudong Yang",
                "Jimin Zhuang",
                "Changli Tang",
                "Yixuan Li",
                "Wei Li",
                "Zejun MA",
                "Chao Zhang"
            ],
            "affiliations": [
                "ByteDance",
                "Tsinghua university",
                "Univeristy of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11775.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ video-SALMONN-o1 - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (pDPO) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RivaBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. video-SALMONN-o1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Video Understanding with Enhanced Reasoning",
                    "desc": "This paper introduces video-SALMONN-o1, an innovative open-source audio-visual large language model (LLM) aimed at improving general video understanding. It addresses the gap in reasoning capabilities for video content by creating a specialized dataset with complex audio-visual questions and detailed solutions. The authors also present process direct preference optimization (pDPO), a method that enhances reward modeling for multimodal inputs through contrastive step selection. The model demonstrates significant accuracy improvements over existing benchmarks, showcasing its effectiveness in tasks like synthetic video detection without prior training."
                },
                "zh": {
                    "title": "è§†é¢‘ç†è§£çš„æ–°çªç ´ï¼švideo-SALMONN-o1",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†video-SALMONN-o1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æºçš„å¢å¼ºæ¨ç†éŸ³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¸€èˆ¬è§†é¢‘ç†è§£ä»»åŠ¡ã€‚ä¸ºäº†æå‡å…¶æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªåŒ…å«å…·æœ‰æŒ‘æˆ˜æ€§çš„éŸ³è§†é¢‘é—®é¢˜å’Œé€æ­¥è§£å†³æ–¹æ¡ˆçš„æ¨ç†å¯†é›†å‹æ•°æ®é›†ã€‚è®ºæ–‡è¿˜æå‡ºäº†è¿‡ç¨‹ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆpDPOï¼‰ï¼Œåˆ©ç”¨å¯¹æ¯”æ­¥éª¤é€‰æ‹©å®ç°é’ˆå¯¹å¤šæ¨¡æ€è¾“å…¥çš„é«˜æ•ˆæ­¥éª¤çº§å¥–åŠ±å»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒRivaBenchä½œä¸ºç¬¬ä¸€ä¸ªæ¨ç†å¯†é›†å‹è§†é¢‘ç†è§£åŸºå‡†ï¼Œæä¾›äº†è¶…è¿‡4000ä¸ªé«˜è´¨é‡çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–äº†å¤šç§åœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11901",
            "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
            "url": "https://huggingface.co/papers/2502.11901",
            "abstract": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
            "score": 4,
            "issue_id": 2263,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "9451c99877c67e4d",
            "authors": [
                "Dylan Zhang",
                "Justin Wang",
                "Tianran Sun"
            ],
            "affiliations": [
                "Shanghai Jiaotong University",
                "University of Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11901.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#data",
                    "#plp",
                    "#transfer_learning",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PoPilot, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ½Ğ° 64% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ GPT-4 Ğ½Ğ° 54% Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞµĞ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Proof-Oriented Programming with Synthetic Data Augmentation",
                    "desc": "This paper addresses the challenges faced by language models (LMs) in proof-oriented programming due to limited data availability. It introduces a novel approach of synthetic data augmentation to enhance the training of LMs for generating and repairing proofs in programming languages like F*. The method involves creating basic proof-oriented programming problems and utilizing diverse coding data to improve reasoning capabilities. The results demonstrate that the fine-tuned 14B parameter model, PoPilot, significantly outperforms existing models, including GPT-4o, in project-level proof-oriented programming tasks."
                },
                "zh": {
                    "title": "åˆæˆæ•°æ®å¢å¼ºï¼Œæå‡è¯æ˜ç¼–ç¨‹èƒ½åŠ›ï¼",
                    "desc": "ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨é¢å‘è¯æ˜çš„ç¼–ç¨‹ä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šç¼ºä¹è¶³å¤Ÿçš„é¢å‘è¯æ˜ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚F*ï¼‰çš„è¯­æ–™åº“ï¼Œä»¥åŠç¼ºå°‘å¤§è§„æ¨¡çš„é¡¹ç›®çº§è¯æ˜å®ç°ï¼Œæ— æ³•æ•™ä¼šæ¨¡å‹å¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåˆæˆæ•°æ®å¢å¼ºçš„æ–¹æ³•ï¼Œä¸“æ³¨äºé¡¹ç›®çº§çš„é¢å‘è¯æ˜ç¼–ç¨‹ï¼Œæ—¢ç”¨äºç”Ÿæˆä¹Ÿç”¨äºä¿®å¤ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆæˆåŸºæœ¬çš„é¢å‘è¯æ˜ç¼–ç¨‹é—®é¢˜æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶ç»“åˆå¤šæ ·åŒ–çš„ç¼–ç æ•°æ®ä»¥æé«˜æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨ç°æœ‰ä»£ç åº“ä¸­åˆ›å»ºæ–°çš„è¯æ˜å’Œä¿®å¤æ•°æ®ã€‚æˆ‘ä»¬çš„14Bå‚æ•°æ¨¡å‹PoPilotç»è¿‡å¾®è°ƒåï¼Œåœ¨é¡¹ç›®çº§é¢å‘è¯æ˜ç¼–ç¨‹ä¸­è¶…è¶Šäº†GPT-4oæ¨¡å‹64%çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡ä¿®å¤å…¶è¾“å‡ºæé«˜äº†54%çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11357",
            "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents",
            "url": "https://huggingface.co/papers/2502.11357",
            "abstract": "Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible.",
            "score": 3,
            "issue_id": 2277,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "26d055a62173bda2",
            "authors": [
                "Vardaan Pahuja",
                "Yadong Lu",
                "Corby Rosset",
                "Boyu Gou",
                "Arindam Mitra",
                "Spencer Whitehead",
                "Yu Su",
                "Ahmed Awadallah"
            ],
            "affiliations": [
                "Microsoft Research, Redmond",
                "The Ohio State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11357.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#synthetic",
                    "#dataset",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ±Ğ¾Ğ»ĞµĞµ 94 Ñ‚Ñ‹ÑÑÑ‡ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 49 Ñ‚Ñ‹ÑÑÑ‡ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… URL-Ğ°Ğ´Ñ€ĞµÑĞ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚ Explorer, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Web Tasks with Scalable Multimodal Datasets",
                    "desc": "This paper presents a solution to the challenge of training large multimodal models (LMMs) for web tasks by creating a vast and diverse dataset of web trajectories. The dataset includes over 94,000 successful multimodal web interactions, which were synthesized through extensive web exploration, making it cost-effective to produce. The authors introduce Explorer, a multimodal web agent trained on this dataset, which shows improved performance on various benchmarks compared to previous models. The study emphasizes the importance of data scaling in enhancing the capabilities of web agents, aiming to make advanced LMM research more accessible to the community."
                },
                "zh": {
                    "title": "åˆæˆå¤šæ ·åŒ–æ•°æ®é›†ï¼Œæå‡å¤šæ¨¡æ€ä»£ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç”¨äºåˆæˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰æ‰€éœ€çš„å¤šæ ·åŒ–è½¨è¿¹çº§æ•°æ®é›†ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡94,000ä¸ªæˆåŠŸçš„å¤šæ¨¡æ€ç½‘ç»œè½¨è¿¹çš„æ•°æ®é›†ï¼Œæ¶µç›–49,000ä¸ªç‹¬ç‰¹çš„URLå’Œ720,000ä¸ªæˆªå›¾ã€‚é€šè¿‡å¹¿æ³›çš„ç½‘ç»œæ¢ç´¢å’Œä»»åŠ¡æ„å›¾çš„ç»†åŒ–ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»¥ä½æˆæœ¬æ”¶é›†å¤šæ ·åŒ–çš„æ•°æ®ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ•°æ®è§„æ¨¡çš„æ‰©å¤§æ˜¯æå‡ç½‘ç»œä»£ç†èƒ½åŠ›çš„å…³é”®å› ç´ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11748",
            "title": "ILIAS: Instance-Level Image retrieval At Scale",
            "url": "https://huggingface.co/papers/2502.11748",
            "abstract": "This work introduces ILIAS, a new test dataset for Instance-Level Image retrieval At Scale. It is designed to evaluate the ability of current and future foundation models and retrieval techniques to recognize particular objects. The key benefits over existing datasets include large scale, domain diversity, accurate ground truth, and a performance that is far from saturated. ILIAS includes query and positive images for 1,000 object instances, manually collected to capture challenging conditions and diverse domains. Large-scale retrieval is conducted against 100 million distractor images from YFCC100M. To avoid false negatives without extra annotation effort, we include only query objects confirmed to have emerged after 2014, i.e. the compilation date of YFCC100M. An extensive benchmarking is performed with the following observations: i) models fine-tuned on specific domains, such as landmarks or products, excel in that domain but fail on ILIAS ii) learning a linear adaptation layer using multi-domain class supervision results in performance improvements, especially for vision-language models iii) local descriptors in retrieval re-ranking are still a key ingredient, especially in the presence of severe background clutter iv) the text-to-image performance of the vision-language foundation models is surprisingly close to the corresponding image-to-image case. website: https://vrg.fel.cvut.cz/ilias/",
            "score": 3,
            "issue_id": 2277,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "59967e50364f64f1",
            "authors": [
                "Giorgos Kordopatis-Zilos",
                "Vladan StojniÄ‡",
                "Anna Manko",
                "Pavel Å uma",
                "Nikolaos-Antonios Ypsilantis",
                "Nikos Efthymiadis",
                "Zakaria Laskar",
                "JiÅ™Ã­ Matas",
                "OndÅ™ej Chum",
                "Giorgos Tolias"
            ],
            "affiliations": [
                "VRG, FEE, Czech Technical University in Prague"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11748.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ILIAS: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ILIAS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. ĞĞ½ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. ILIAS Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ 1000 ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ñ„Ğ¾Ğ½Ğµ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· YFCC100M, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "ILIAS: Advancing Instance-Level Image Retrieval with Scale and Diversity",
                    "desc": "This paper presents ILIAS, a novel dataset aimed at enhancing instance-level image retrieval capabilities. It features a large-scale collection of 1,000 object instances with diverse domains and challenging conditions, evaluated against 100 million distractor images. The dataset allows for benchmarking of various models, revealing that domain-specific fine-tuning can lead to performance drops in broader contexts. Additionally, the study highlights the importance of local descriptors in retrieval tasks and notes that vision-language models perform comparably in text-to-image and image-to-image retrieval scenarios."
                },
                "zh": {
                    "title": "ILIASï¼šå¤§è§„æ¨¡å®ä¾‹çº§å›¾åƒæ£€ç´¢çš„æ–°æ ‡å‡†",
                    "desc": "æœ¬ç ”ç©¶ä»‹ç»äº†ILIASï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤§è§„æ¨¡å®ä¾‹çº§å›¾åƒæ£€ç´¢çš„æ–°æµ‹è¯•æ•°æ®é›†ã€‚å®ƒæ—¨åœ¨è¯„ä¼°å½“å‰å’Œæœªæ¥åŸºç¡€æ¨¡å‹åŠæ£€ç´¢æŠ€æœ¯è¯†åˆ«ç‰¹å®šå¯¹è±¡çš„èƒ½åŠ›ã€‚ILIASçš„ä¼˜åŠ¿åœ¨äºå…¶å¤§è§„æ¨¡ã€é¢†åŸŸå¤šæ ·æ€§ã€å‡†ç¡®çš„çœŸå®æ ‡ç­¾ï¼Œä»¥åŠå°šæœªé¥±å’Œçš„æ€§èƒ½è¡¨ç°ã€‚æ•°æ®é›†ä¸­åŒ…å«1000ä¸ªå¯¹è±¡å®ä¾‹çš„æŸ¥è¯¢å’Œæ­£æ ·æœ¬å›¾åƒï¼Œæ‰‹åŠ¨æ”¶é›†ä»¥æ•æ‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶å’Œå¤šæ ·çš„é¢†åŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10550",
            "title": "Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2502.10550",
            "abstract": "Memory is crucial for enabling agents to tackle complex tasks with temporal and spatial dependencies. While many reinforcement learning (RL) algorithms incorporate memory, the field lacks a universal benchmark to assess an agent's memory capabilities across diverse scenarios. This gap is particularly evident in tabletop robotic manipulation, where memory is essential for solving tasks with partial observability and ensuring robust performance, yet no standardized benchmarks exist. To address this, we introduce MIKASA (Memory-Intensive Skills Assessment Suite for Agents), a comprehensive benchmark for memory RL, with three key contributions: (1) we propose a comprehensive classification framework for memory-intensive RL tasks, (2) we collect MIKASA-Base - a unified benchmark that enables systematic evaluation of memory-enhanced agents across diverse scenarios, and (3) we develop MIKASA-Robo - a novel benchmark of 32 carefully designed memory-intensive tasks that assess memory capabilities in tabletop robotic manipulation. Our contributions establish a unified framework for advancing memory RL research, driving the development of more reliable systems for real-world applications. The code is available at https://sites.google.com/view/memorybenchrobots/.",
            "score": 3,
            "issue_id": 2272,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "99224cee50b48e48",
            "authors": [
                "Egor Cherepanov",
                "Nikita Kachaev",
                "Alexey K. Kovalev",
                "Aleksandr I. Panov"
            ],
            "affiliations": [
                "AIRI, Moscow, Russia",
                "MIPT, Dolgoprudny, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10550.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#benchmark",
                    "#agents",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "MIKASA: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ RL-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MIKASA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² MIKASA-Base Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ MIKASA-Robo - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 32 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "MIKASA: Advancing Memory in Reinforcement Learning for Robotics",
                    "desc": "This paper addresses the importance of memory in reinforcement learning (RL) for agents performing complex tasks that require understanding of time and space. It highlights the lack of standardized benchmarks to evaluate memory capabilities in RL, especially in tabletop robotic manipulation scenarios. To fill this gap, the authors introduce MIKASA, a benchmark suite designed to assess memory-intensive skills in agents. MIKASA includes a classification framework, a unified benchmark called MIKASA-Base, and a set of 32 specific tasks in MIKASA-Robo to systematically evaluate memory-enhanced agents."
                },
                "zh": {
                    "title": "æå‡æ™ºèƒ½ä½“è®°å¿†èƒ½åŠ›çš„ç»Ÿä¸€åŸºå‡†",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è®°å¿†åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†MIKASAï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„è®°å¿†èƒ½åŠ›ã€‚MIKASAåŒ…æ‹¬ä¸€ä¸ªåˆ†ç±»æ¡†æ¶å’Œä¸¤ä¸ªåŸºå‡†ï¼Œåˆ†åˆ«ç”¨äºç³»ç»Ÿè¯„ä¼°è®°å¿†å¢å¼ºæ™ºèƒ½ä½“å’Œè®¾è®¡32ä¸ªè®°å¿†å¯†é›†å‹ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè®°å¿†å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›äº†ç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ¨åŠ¨äº†æ›´å¯é ç³»ç»Ÿçš„å¼€å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12054",
            "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
            "url": "https://huggingface.co/papers/2502.12054",
            "abstract": "Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason.",
            "score": 3,
            "issue_id": 2269,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "4aaf92e2d2fd9766",
            "authors": [
                "Xinyu Zhang",
                "Yuxuan Dong",
                "Yanrui Wu",
                "Jiaxing Huang",
                "Chengyou Jia",
                "Basura Fernando",
                "Mike Zheng Shou",
                "Lingling Zhang",
                "Jun Liu"
            ],
            "affiliations": [
                "Institute of High-Performance Computing, A*STAR",
                "Show Lab, National University of Singapore",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12054.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "PhysReason: Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PhysReason - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1200 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 8.1 ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ”Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğ¸Ğ¶Ğµ 60% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "PhysReason: A New Benchmark for Physics-Based Reasoning in AI",
                    "desc": "This paper introduces PhysReason, a new benchmark designed to evaluate the physics-based reasoning abilities of large language models. It consists of 1,200 problems, with a mix of knowledge-based and reasoning-based tasks, categorized into three difficulty levels. The benchmark highlights the complexity of physics reasoning, requiring multiple solution steps, with hard problems demanding an average of 15.6 steps. The study also identifies key challenges in physics reasoning, such as theorem application and process understanding, providing insights into the limitations of current models."
                },
                "zh": {
                    "title": "ç‰©ç†æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç‰©ç†æ¨ç†æ–¹é¢çš„è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†PhysReasonï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1200ä¸ªé—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­75%æ˜¯æ¨ç†ç±»é—®é¢˜ï¼Œåˆ†ä¸ºç®€å•ã€ä¸­ç­‰å’Œå›°éš¾ä¸‰ä¸ªéš¾åº¦çº§åˆ«ã€‚é€šè¿‡å¼•å…¥ç‰©ç†è§£é¢˜è‡ªåŠ¨è¯„åˆ†æ¡†æ¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨ç‰©ç†å®šç†åº”ç”¨ã€è¿‡ç¨‹ç†è§£ã€è®¡ç®—å’Œæ¡ä»¶åˆ†æç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰é¡¶å°–æ¨¡å‹åœ¨ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»æœ‰å¾…æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨å›°éš¾é—®é¢˜ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11330",
            "title": "System Message Generation for User Preferences using Open-Source Models",
            "url": "https://huggingface.co/papers/2502.11330",
            "abstract": "System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communication styles. Despite such versatility, publicly available data are often lack system messages and subject to strict license constraints in the industry field. Manual labeling of publicly available data with system messages that align with user instructions demands significant resources. In view of such challenges, our work introduces SysGen, a pipeline for generating system messages with better aligned assistant responses from the supervised fine-tuning dataset without system messages. Training on SysGen data has demonstrated substantial improvements in the alignment of model responses with system messages and user instructions, as demonstrated across various open-source models on the Multifacet benchmark, while maintaining minimal impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our qualitative analysis highlights the importance of diverse system messages to ensure better adaptability across different contexts.",
            "score": 3,
            "issue_id": 2267,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "36ca5a9ceb25e7fa",
            "authors": [
                "Minbyul Jeong",
                "Jungho Cho",
                "Minsoo Khang",
                "Dawoon Jung",
                "Teakgyu Hong"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.11330.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "SysGen: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SysGen - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). SysGen ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ±ĞµĞ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SysGen Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing LLM Responses with SysGen: Better System Messages for Better Alignment",
                    "desc": "This paper presents SysGen, a new method for generating system messages that help large language models (LLMs) respond more accurately to user instructions. System messages are crucial for guiding LLMs in their interactions, but there is a lack of publicly available data that includes these messages. SysGen addresses this issue by creating a pipeline that generates system messages from existing supervised fine-tuning datasets, leading to improved alignment of model responses. The results show that training with SysGen data enhances the performance of various open-source models while keeping their effectiveness on other benchmarks largely unchanged."
                },
                "zh": {
                    "title": "SysGenï¼šæå‡è¯­è¨€æ¨¡å‹å“åº”å¯¹é½æ€§çš„ç³»ç»Ÿæ¶ˆæ¯ç”Ÿæˆ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†SysGenï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆç³»ç»Ÿæ¶ˆæ¯çš„ç®¡é“ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç”¨æˆ·æŒ‡ä»¤çš„å¯¹é½åº¦ã€‚ç³»ç»Ÿæ¶ˆæ¯åœ¨ä¸LLMsçš„äº¤äº’ä¸­èµ·ç€é‡è¦ä½œç”¨ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æŒ‡å®šè§’è‰²å’Œä»»åŠ¡ã€‚é€šè¿‡åœ¨æ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯çš„ç›‘ç£å¾®è°ƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒSysGenæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹å“åº”çš„å¯¹é½æ€§ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¤šæ ·åŒ–çš„ç³»ç»Ÿæ¶ˆæ¯å¯¹äºåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­å®ç°æ›´å¥½çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11098",
            "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
            "url": "https://huggingface.co/papers/2502.11098",
            "abstract": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose Talk Structurally, Act Hierarchically (TalkHier), a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. TalkHier surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier.",
            "score": 3,
            "issue_id": 2265,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "9aaac2e7c7b495a4",
            "authors": [
                "Zhao Wang",
                "Sota Moriyama",
                "Wei-Yao Wang",
                "Briti Gangopadhyay",
                "Shingo Takamatsu"
            ],
            "affiliations": [
                "Sony Group Corporation, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11098.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#agents",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° TalkHier Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). TalkHier Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². TalkHier Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Collaboration with TalkHier Framework",
                    "desc": "This paper introduces Talk Hierarchically, Act Structurally (TalkHier), a new framework designed to improve communication and collaboration among multi-agent systems using large language models (LLMs). It features a structured communication protocol that enhances context understanding and a hierarchical refinement system to correct errors and biases in agent outputs. The framework outperforms existing state-of-the-art models in various tasks, demonstrating its effectiveness in open-domain question answering and targeted text generation. Overall, TalkHier aims to establish a new benchmark for LLM-based multi-agent systems, promoting better teamwork and adaptability among agents."
                },
                "zh": {
                    "title": "ç»“æ„åŒ–äº¤æµï¼Œåˆ†å±‚è¡ŒåŠ¨çš„æ™ºèƒ½ä½“åä½œæ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºTalk Structurally, Act Hierarchicallyï¼ˆTalkHierï¼‰ï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„é€šä¿¡å’Œåä½œã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–çš„é€šä¿¡åè®®ï¼Œä»¥ä¾¿åœ¨å¤æ‚ä»»åŠ¡ä¸­è¿›è¡Œä¸°å¯Œçš„ä¸Šä¸‹æ–‡äº¤æµï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåˆ†å±‚çš„ç²¾ç‚¼ç³»ç»Ÿï¼Œä»¥è§£å†³é”™è¯¯è¾“å‡ºã€è™šå‡ä¿¡æ¯å’Œåè§ç­‰é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTalkHieråœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¼€æ”¾é¢†åŸŸé—®ç­”å’Œç‰¹å®šé¢†åŸŸé€‰æ‹©æ€§æé—®ç­‰ã€‚è¯¥ç ”ç©¶ä¸ºLLM-MAç³»ç»Ÿè®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œæ¨åŠ¨äº†æ›´æœ‰æ•ˆã€çµæ´»å’Œåä½œçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10454",
            "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs",
            "url": "https://huggingface.co/papers/2502.10454",
            "abstract": "Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of \"proof by counterexamples\" commonly used in human mathematics education, our work aims to enhance LLMs' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.",
            "score": 3,
            "issue_id": 2265,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "1821254437fc158d",
            "authors": [
                "Yinghui Li",
                "Jiayi Kuang",
                "Haojing Huang",
                "Zhikun Xu",
                "Xinnian Liang",
                "Yi Yu",
                "Wenlian Lu",
                "Yangning Li",
                "Xiaoyu Tan",
                "Chao Qu",
                "Ying Shen",
                "Hai-Tao Zheng",
                "Philip S. Yu"
            ],
            "affiliations": [
                "ARC Lab, Arizona State University",
                "Bytedance Inc.",
                "INFLY TECH (Shanghai) Co., Ltd.",
                "Peng Cheng Laboratory",
                "School of Mathematical Science, Fudan University",
                "Sun-Yat Sen University",
                "Tsinghua University",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10454.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ LLM Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CounterMATH Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM."
                },
                "en": {
                    "title": "Enhancing LLMs' Mathematical Proofs through Counterexamples",
                    "desc": "This paper discusses the limitations of current Large Language Models (LLMs) in generating mathematical proofs, emphasizing their dependence on prior exposure to proof processes during training. The authors introduce a new benchmark called CounterMATH, which challenges LLMs to prove mathematical statements by providing counterexamples, thereby testing their understanding of mathematical concepts. They also present a data engineering framework to enhance the training data for LLMs, aiming to improve their reasoning capabilities. The findings suggest that enhancing counterexample-driven reasoning is essential for advancing the mathematical proficiency of LLMs."
                },
                "zh": {
                    "title": "é€šè¿‡åä¾‹æå‡æ•°å­¦æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ•°å­¦è¯æ˜ç”Ÿæˆçš„èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå½“å‰LLMsçš„è¯æ˜èƒ½åŠ›ä¸»è¦ä¾èµ–äºå…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å¦æ¥è§¦è¿‡ç›¸å…³çš„è¯æ˜è¿‡ç¨‹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹æ•°å­¦å®šç†å’Œç›¸å…³æ¦‚å¿µçš„æ·±å…¥ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåä¾‹çš„è¯æ˜æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åä¾‹å¢å¼ºLLMsçš„æ•°å­¦æ¨ç†å’Œè¯æ˜èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ‰‹åŠ¨åˆ›å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°å­¦åŸºå‡†CounterMATHï¼Œä»¥è¯„ä¼°LLMsåœ¨æä¾›åä¾‹æ—¶çš„æ•°å­¦æ¦‚å¿µæŒæ¡æƒ…å†µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11085",
            "title": "Towards Data-Efficient Pretraining for Atomic Property Prediction",
            "url": "https://huggingface.co/papers/2502.11085",
            "abstract": "This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th of the computational cost. We introduce the Chemical Similarity Index (CSI), a novel metric inspired by computer vision's Fr\\'echet Inception Distance, for molecular graphs which quantifies the alignment between upstream pretraining datasets and downstream tasks. By selecting the most relevant dataset with minimal CSI distance, we show that models pretrained on a smaller, focused dataset consistently outperform those pretrained on massive, mixed datasets such as JMP, even when those larger datasets include the relevant dataset. Counterintuitively, we also find that indiscriminately adding more data can degrade model performance when the additional data poorly aligns with the task at hand. Our findings highlight that quality often outperforms quantity in pretraining for atomic property prediction.",
            "score": 2,
            "issue_id": 2270,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "de635e01a182309d",
            "authors": [
                "Yasir Ghunaim",
                "Hasan Abed Al Kader Hammoud",
                "Bernard Ghanem"
            ],
            "affiliations": [
                "King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11085.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ², ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼, Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ÑŒÑÑ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1/24 Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ - Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ¥Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¡Ñ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° (CSI), Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¤Ñ€ĞµÑˆĞµ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸, Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼, Ğ½Ğ¾ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Quality Over Quantity in Atomic Property Prediction",
                    "desc": "This paper argues that the success of predicting atomic properties does not solely depend on having large datasets and powerful computing resources. Instead, it demonstrates that pretraining on a well-chosen, relevant dataset can achieve better results while using significantly less computational power. The authors introduce the Chemical Similarity Index (CSI), a new metric that measures how well the pretraining dataset aligns with the specific task. Their research shows that using a focused dataset can lead to superior model performance compared to using larger, less relevant datasets, emphasizing that the quality of data is more important than its quantity."
                },
                "zh": {
                    "title": "è´¨é‡èƒœäºæ•°é‡ï¼šåŸå­å±æ€§é¢„æµ‹çš„æ–°è§†è§’",
                    "desc": "è¿™ç¯‡è®ºæ–‡æŒ‘æˆ˜äº†åŸå­å±æ€§é¢„æµ‹é¢†åŸŸçš„ä¼ ç»Ÿè§‚å¿µï¼Œè®¤ä¸ºè¿›æ­¥ä¸æ•°æ®é›†è§„æ¨¡å’Œè®¡ç®—èµ„æºçš„å¢åŠ æœ‰å…³ã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨ç²¾å¿ƒé€‰æ‹©çš„ã€ä¸ä»»åŠ¡ç›¸å…³çš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥åŒ¹é…ç”šè‡³è¶…è¶Šå¤§è§„æ¨¡é¢„è®­ç»ƒï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬ä»…ä¸º1/24ã€‚æˆ‘ä»¬å¼•å…¥äº†åŒ–å­¦ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆCSIï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–ä¸Šæ¸¸é¢„è®­ç»ƒæ•°æ®é›†ä¸ä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„æ•°æ®é›†å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¼ºè°ƒäº†åœ¨åŸå­å±æ€§é¢„æµ‹ä¸­ï¼Œè´¨é‡å¾€å¾€ä¼˜äºæ•°é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09509",
            "title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling",
            "url": "https://huggingface.co/papers/2502.09509",
            "abstract": "Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose EQ-VAE, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By finetuning pre-trained autoencoders with EQ-VAE, we enhance the performance of several state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT, achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning. EQ-VAE is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: https://eq-vae.github.io/.",
            "score": 1,
            "issue_id": 2280,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "d034cfa8688142a4",
            "authors": [
                "Theodoros Kouzelis",
                "Ioannis Kakogeorgiou",
                "Spyros Gidaris",
                "Nikos Komodakis"
            ],
            "affiliations": [
                "Archimedes, Athena RC, Greece",
                "IACM-Forth, Greece",
                "National Technical University of Athens, Greece",
                "University of Crete, Greece",
                "valaio.ai, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09509.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#training",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ­ĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EQ-VAE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ. EQ-VAE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ DiT, SiT, REPA Ğ¸ MaskGIT. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ ĞºĞ°Ğº Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Image Synthesis with EQ-VAE: Simplifying Latent Spaces for Better Generative Performance",
                    "desc": "This paper introduces EQ-VAE, a novel regularization technique designed to improve latent generative models used for image synthesis. The authors highlight that traditional autoencoders struggle with maintaining equivariance to transformations like scaling and rotation, which complicates the latent space and affects generative quality. By enforcing equivariance in the latent space, EQ-VAE simplifies the structure while preserving the quality of image reconstruction. The results show significant performance improvements in various state-of-the-art generative models, demonstrating EQ-VAE's effectiveness and versatility across different types of autoencoders."
                },
                "zh": {
                    "title": "EQ-VAEï¼šæå‡æ½œåœ¨ç”Ÿæˆæ¨¡å‹æ€§èƒ½çš„å…³é”®",
                    "desc": "æ½œåœ¨ç”Ÿæˆæ¨¡å‹å·²æˆä¸ºé«˜è´¨é‡å›¾åƒåˆæˆçš„ä¸»è¦æ–¹æ³•ã€‚è¿™äº›æ¨¡å‹ä½¿ç”¨è‡ªç¼–ç å™¨å°†å›¾åƒå‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œç„¶åé€šè¿‡ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ½œåœ¨åˆ†å¸ƒã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„è‡ªç¼–ç å™¨åœ¨è¯­ä¹‰ä¿æŒå˜æ¢ï¼ˆå¦‚ç¼©æ”¾å’Œæ—‹è½¬ï¼‰æ–¹é¢ç¼ºä¹ç­‰å˜æ€§ï¼Œå¯¼è‡´å¤æ‚çš„æ½œåœ¨ç©ºé—´ï¼Œå½±å“ç”Ÿæˆæ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EQ-VAEï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ½œåœ¨ç©ºé—´ä¸­å¼ºåˆ¶ç­‰å˜æ€§ï¼Œä»è€Œé™ä½å¤æ‚æ€§è€Œä¸é™ä½é‡å»ºè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09969",
            "title": "Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning",
            "url": "https://huggingface.co/papers/2502.09969",
            "abstract": "Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.",
            "score": 1,
            "issue_id": 2278,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "6530066ad48b1daf",
            "authors": [
                "Ishika Agarwal",
                "Dilek Hakkani-TÃ¼r"
            ],
            "affiliations": [
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09969.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#interpretability",
                    "#training",
                    "#small_models"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ InfluenceNetwork. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ 99%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ²ÑĞµĞ³Ğ¾ 0,0027% Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ NN-CIFT Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Efficient Influence Estimation with InfluenceNetwork",
                    "desc": "This paper introduces the InfluenceNetwork, a small neural network designed to efficiently estimate influence values in model training. Traditional methods are computationally expensive and struggle with large datasets, but the InfluenceNetwork achieves up to a 99% reduction in costs. The proposed method, NN-CIFT, allows for effective subset selection in instruction fine-tuning without sacrificing performance compared to existing influence functions. The authors also provide a detailed analysis of hyperparameters to optimize the performance of their approach."
                },
                "zh": {
                    "title": "å°å‹ç¥ç»ç½‘ç»œå®ç°é«˜æ•ˆå½±å“ä¼°è®¡",
                    "desc": "å½±å“å‡½æ•°åœ¨æ¨¡å‹è®­ç»ƒä¸­æä¾›äº†é‡è¦çš„è§è§£ï¼Œä½†ç°æœ‰æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å°å‹ç¥ç»ç½‘ç»œï¼Œç§°ä¸ºå½±å“ç½‘ç»œï¼ˆInfluenceNetworkï¼‰ï¼Œç”¨äºä¼°è®¡å½±å“å€¼ï¼Œæˆæœ¬é™ä½é«˜è¾¾99%ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆNN-CIFTï¼‰åœ¨é€‰æ‹©å­é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä¸”ä¸ä¼ ç»Ÿå½±å“å‡½æ•°ç›¸æ¯”ï¼Œæ€§èƒ½æ²¡æœ‰å¦¥åã€‚é€šè¿‡å¯¹è¶…å‚æ•°çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†å°æ¨¡å‹ä¹Ÿèƒ½æœ‰æ•ˆä¼°è®¡å½±å“å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08820",
            "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model",
            "url": "https://huggingface.co/papers/2502.08820",
            "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CALM-IT, we train three models CALM 8B, CALM 70B, and CALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks.",
            "score": 1,
            "issue_id": 2274,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "f3f1c79c06903edb",
            "authors": [
                "Emre Can Acikgoz",
                "Jeremiah Greer",
                "Akul Datta",
                "Ze Yang",
                "William Zeng",
                "Oussama Elachqar",
                "Emmanouil Koukoumidis",
                "Dilek Hakkani-TÃ¼r",
                "Gokhan Tur"
            ],
            "affiliations": [
                "Oumi",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08820.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#agi",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "CALM: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ CALM (Conversational Agentic Language Model), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CALM-IT, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ReAct Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… API. ĞĞ° ĞµĞ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CALM 8B, 70B Ğ¸ 405B, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…: MultiWOZ 2.4, BFCL V3 Ğ¸ API-Bank. CALM Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° (TOD) Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ (LA), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹."
                },
                "en": {
                    "title": "CALM: Bridging the Gap in Conversational AI",
                    "desc": "This paper discusses the development of CALM, a new model that combines the strengths of Language Agents (LA) and task-oriented dialogue (TOD) systems. Traditional TOD systems struggle with limited API training and maintaining user intent over multiple interactions, while LAs lack robust multi-turn management. The authors introduce CALM-IT, a multi-task dataset that integrates reasoning and API usage, allowing for better training of conversational agents. The results show that CALM models significantly outperform existing specialized models on key benchmarks, demonstrating the effectiveness of this unified approach."
                },
                "zh": {
                    "title": "CALMï¼šå¯¹è¯ä¸ä»£ç†èƒ½åŠ›çš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¯¹è¯ä»£ç†æ¨¡å‹CALMï¼ˆConversational Agentic Language Modelï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿï¼ˆTODï¼‰å’Œè¯­è¨€ä»£ç†ï¼ˆLAï¼‰åœ¨å¤šè½®å¯¹è¯ç®¡ç†å’ŒåŠŸèƒ½è°ƒç”¨æ–¹é¢çš„ä¸è¶³ã€‚å½“å‰çš„ç³»ç»Ÿé€šå¸¸åœ¨æœ‰é™çš„ç›®æ ‡APIä¸Šè®­ç»ƒï¼Œå¯¼è‡´åœ¨ä¸æ–°æœåŠ¡äº¤äº’æ—¶éœ€è¦æ–°çš„æ•°æ®æ¥ç»´æŒè´¨é‡ï¼Œè€Œè¯­è¨€ä»£ç†åˆ™æœªèƒ½æœ‰æ•ˆä¿æŒç”¨æˆ·æ„å›¾ã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºCALM-ITæ•°æ®é›†ï¼Œå°†å¤šè½®æ¨ç†ä¸å¤æ‚APIä½¿ç”¨ç›¸ç»“åˆï¼Œè®­ç»ƒäº†ä¸‰ç§ä¸åŒè§„æ¨¡çš„CALMæ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºè¿™äº›æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„é¢†åŸŸç‰¹å®šæ¨¡å‹ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼ŒCALMæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¯¹è¯èƒ½åŠ›å’Œä»£ç†èƒ½åŠ›ï¼Œæ¨åŠ¨å¯¹è¯ç³»ç»Ÿçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08441",
            "title": "Better Embeddings with Coupled Adam",
            "url": "https://huggingface.co/papers/2502.08441",
            "abstract": "Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.",
            "score": 1,
            "issue_id": 2271,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "4357e7dc6b15b5b7",
            "authors": [
                "Felix Stollenwerk",
                "Tobias Stollenwerk"
            ],
            "affiliations": [
                "AI Sweden",
                "Forschungszentrum JÃ¼lich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08441.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Coupled Adam",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ°Ğ½Ğ¸Ğ·Ğ¾Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… ÑĞ»Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğµ Adam ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ·Ğ¾Ñ‚Ñ€Ğ¾Ğ¿Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Coupled Adam Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Coupled Adam Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ ĞºĞ°Ğº Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Embedding Quality with Coupled Adam",
                    "desc": "This paper addresses the issue of anisotropic embeddings in large language models (LLMs), which can negatively impact their performance. The authors identify that the second moment in the Adam optimizer contributes to this anisotropy. To counteract this, they propose a new optimizer called Coupled Adam, which modifies the way embeddings are learned. Experimental results show that Coupled Adam not only enhances the quality of embeddings but also improves performance in both upstream and downstream tasks when applied to sufficiently large datasets."
                },
                "zh": {
                    "title": "æ”¹è¿›ä¼˜åŒ–å™¨ï¼Œæå‡åµŒå…¥è´¨é‡",
                    "desc": "å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å‡ºè‰²çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å­¦ä¹ çš„è¯è¡¨ç¤ºå­˜åœ¨ä¸€ç§ä¸ç†æƒ³ä¸”å°šæœªå……åˆ†ç†è§£çš„ç‰¹å¾ï¼Œå³å„å‘å¼‚æ€§ã€‚æœ¬æ–‡è®¤ä¸ºï¼ŒAdamä¼˜åŒ–å™¨ä¸­çš„äºŒé˜¶çŸ©æ˜¯å¯¼è‡´å„å‘å¼‚æ€§åµŒå…¥çš„åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºCoupled Adamçš„æ”¹è¿›ä¼˜åŒ–å™¨æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒCoupled Adamæ˜¾è‘—æé«˜äº†åµŒå…¥çš„è´¨é‡ï¼ŒåŒæ—¶åœ¨è¶³å¤Ÿå¤§çš„æ•°æ®é›†ä¸Šä¹Ÿæå‡äº†ä¸Šæ¸¸å’Œä¸‹æ¸¸çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09083",
            "title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking",
            "url": "https://huggingface.co/papers/2502.09083",
            "abstract": "The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model's reasoning path, reference specific evidence, and highlight uncertainty and information gaps.",
            "score": 1,
            "issue_id": 2270,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "ecf7018a52bbede5",
            "authors": [
                "Greta Warren",
                "Irina Shklovski",
                "Isabelle Augenstein"
            ],
            "affiliations": [
                "LinkÃ¶ping University, LinkÃ¶ping, Sweden",
                "University of Copenhagen, Copenhagen, Denmark"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09083.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#ethics",
                    "#healthcare",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ñ‡ĞµĞºĞµÑ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ĞºĞ°Ğº Ğ¾Ğ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ±Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ„Ğ°ĞºÑ‚Ñ‡ĞµĞºĞµÑ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑÑ…, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ñ… Ñ…Ğ¾Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑÑÑ‹Ğ»Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ½Ğ° Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Automated Fact-Checking with Human-Centric Explanations",
                    "desc": "This paper addresses the growing challenge of misinformation in online media by exploring the role of automated fact-checking systems. It highlights the necessity for these systems to provide clear explanations that align with how human fact-checkers evaluate evidence and make decisions. Through interviews with fact-checking professionals, the study identifies specific requirements for explanations that can enhance the integration of automated tools into fact-checking workflows. The findings reveal critical gaps in current explanation practices and suggest criteria for developing more effective automated fact-checking solutions."
                },
                "zh": {
                    "title": "æå‡è‡ªåŠ¨åŒ–äº‹å®æ ¸æŸ¥çš„è§£é‡Šèƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨åœ¨çº¿åª’ä½“ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¼ºè°ƒäº†è‡ªåŠ¨åŒ–äº‹å®æ ¸æŸ¥çš„å¿…è¦æ€§ï¼Œä»¥å¸®åŠ©æ ¸æŸ¥å‘˜åº”å¯¹æ—¥ç›Šå¢åŠ å’Œå¤æ‚åŒ–çš„è™šå‡ä¿¡æ¯ã€‚ç ”ç©¶é€šè¿‡ä¸äº‹å®æ ¸æŸ¥ä¸“ä¸šäººå£«çš„åŠç»“æ„åŒ–è®¿è°ˆï¼Œåˆ†æäº†æ ¸æŸ¥å‘˜å¦‚ä½•è¯„ä¼°è¯æ®ã€åšå‡ºå†³ç­–ä»¥åŠè§£é‡Šä»–ä»¬çš„è¿‡ç¨‹ã€‚è®ºæ–‡è¿˜è€ƒå¯Ÿäº†æ ¸æŸ¥å‘˜åœ¨å®è·µä¸­å¦‚ä½•ä½¿ç”¨è‡ªåŠ¨åŒ–å·¥å…·ï¼Œå¹¶è¯†åˆ«äº†ä»–ä»¬å¯¹è‡ªåŠ¨åŒ–äº‹å®æ ¸æŸ¥å·¥å…·çš„è§£é‡Šéœ€æ±‚ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ ¸æŸ¥å‘˜åœ¨è§£é‡Šæ–¹é¢å­˜åœ¨æœªæ»¡è¶³çš„éœ€æ±‚ï¼Œå¹¶ç¡®å®šäº†å¯å¤åˆ¶çš„äº‹å®æ ¸æŸ¥è§£é‡Šçš„é‡è¦æ ‡å‡†ï¼ŒåŒ…æ‹¬è¿½è¸ªæ¨¡å‹çš„æ¨ç†è·¯å¾„ã€å¼•ç”¨å…·ä½“è¯æ®ä»¥åŠçªå‡ºä¸ç¡®å®šæ€§å’Œä¿¡æ¯ç¼ºå£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08826",
            "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2502.08826",
            "abstract": "Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.",
            "score": 0,
            "issue_id": 2279,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "e299bbaebf315923",
            "authors": [
                "Mohammad Mahdi Abootorabi",
                "Amirhosein Zobeiri",
                "Mahdi Dehghani",
                "Mohammadali Mohammadkhani",
                "Bardia Mohammadi",
                "Omid Ghahroodi",
                "Mahdieh Soleymani Baghshah",
                "Ehsaneddin Asgari"
            ],
            "affiliations": [
                "College of Interdisciplinary Science and Technology, University of Tehran, Tehran, Iran",
                "Computer Engineering Department, K.N. Toosi University of Technology, Tehran, Iran",
                "Computer Engineering Department, Sharif University of Technology, Tehran, Iran",
                "Qatar Computing Research Institute, Doha, Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08826.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rag",
                    "#multimodal",
                    "#survey",
                    "#benchmark",
                    "#hallucinations"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ RAG: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ¸Ñ€ Ğ² Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (Multimodal RAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ RAG. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ RAG Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing AI with Multimodal Retrieval-Augmented Generation",
                    "desc": "This paper discusses the limitations of Large Language Models (LLMs) in handling hallucinations and outdated information due to their static training data. It introduces Retrieval-Augmented Generation (RAG) as a solution that incorporates external, dynamic information to improve the accuracy and relevance of generated content. The paper further explores Multimodal RAG, which combines various data types like text, images, and audio to enhance output quality, while addressing the challenges of cross-modal alignment and reasoning. It provides a comprehensive analysis of methodologies, evaluation metrics, and future research directions to advance the development of more reliable AI systems that utilize multimodal knowledge effectively."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€RAGï¼šæå‡ç”Ÿæˆèƒ½åŠ›çš„æœªæ¥ä¹‹è·¯",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¹»è§‰å’Œè¿‡æ—¶çŸ¥è¯†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºé™æ€è®­ç»ƒæ•°æ®ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡æ•´åˆå¤–éƒ¨åŠ¨æ€ä¿¡æ¯æ¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä»è€Œå¢å¼ºäº‹å®å’Œæ›´æ–°çš„åŸºç¡€ã€‚æœ€è¿‘çš„å¤šæ¨¡æ€å­¦ä¹ è¿›å±•å¯¼è‡´äº†å¤šæ¨¡æ€RAGçš„å‘å±•ï¼Œç»“åˆäº†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€ï¼Œä»¥å¢å¼ºç”Ÿæˆçš„è¾“å‡ºã€‚ç„¶è€Œï¼Œè·¨æ¨¡æ€å¯¹é½å’Œæ¨ç†ä¸ºå¤šæ¨¡æ€RAGå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œä½¿å…¶ä¸ä¼ ç»Ÿçš„å•æ¨¡æ€RAGæœ‰æ‰€ä¸åŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11177",
            "title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild",
            "url": "https://huggingface.co/papers/2502.11177",
            "abstract": "Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.",
            "score": 0,
            "issue_id": 2273,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "d72ce28b4092ab0f",
            "authors": [
                "Wanli Yang",
                "Fei Sun",
                "Jiajun Tan",
                "Xinyu Ma",
                "Qi Cao",
                "Dawei Yin",
                "Huawei Shen",
                "Xueqi Cheng"
            ],
            "affiliations": [
                "Baidu Inc.",
                "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS",
                "Huawei",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11177.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº QAEdit Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ, Ñ‡ĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°Ğ½ĞµĞµ, Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Rethinking Model Editing: Bridging Theory and Real-World Performance",
                    "desc": "This paper investigates the effectiveness of model editing techniques in question answering (QA) systems, particularly focusing on large language models (LLMs). The authors introduce QAEdit, a new benchmark and evaluation framework to rigorously assess how well these editing methods correct errors in LLMs. Their findings reveal that current editing methods perform significantly worse in real-world scenarios than previously reported, highlighting flaws in past evaluation practices. By simulating real-world conditions, they show that existing approaches struggle with even a small number of edits, prompting a reevaluation of model editing methods and their assessment."
                },
                "zh": {
                    "title": "é‡æ–°å®¡è§†æ¨¡å‹ç¼–è¾‘çš„æœ‰æ•ˆæ€§ä¸è¯„ä¼°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æ¨¡å‹ç¼–è¾‘åœ¨é—®ç­”ç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®åº”ç”¨ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†QAEditï¼Œä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç°æœ‰ç¼–è¾‘æ–¹æ³•åœ¨çº æ­£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é”™è¯¯æ—¶çš„æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„ç¼–è¾‘æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°è¿œä½äºä¹‹å‰çš„æŠ¥å‘Šï¼Œåªæœ‰38.5%çš„å‡†ç¡®ç‡ã€‚é€šè¿‡æ¨¡å—åˆ†æå’Œæ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¯„ä¼°å®è·µä¸­çš„é—®é¢˜å¯¼è‡´äº†è¿™ç§æ€§èƒ½ä¸‹é™ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ¨åŠ¨æ¨¡å‹ç¼–è¾‘ç ”ç©¶çš„å¯é æ€§å’Œå®ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11574",
            "title": "Large Language Models and Mathematical Reasoning Failures",
            "url": "https://huggingface.co/papers/2502.11574",
            "abstract": "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling.",
            "score": 0,
            "issue_id": 2268,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "478a2c4575e67b28",
            "authors": [
                "Johan Boye",
                "Birger Moell"
            ],
            "affiliations": [
                "KTH Royal Institute of Technology, Stockholm, Sweden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11574.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 50 Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑÑ‚Ğ°Ñ€ÑˆĞµĞ¹ ÑˆĞºĞ¾Ğ»Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¸ Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Evaluating Reasoning, Not Just Answers in LLMs",
                    "desc": "This paper examines how well large language models (LLMs) can solve high-school-level math word problems by focusing on their reasoning abilities. It evaluates eight advanced models, revealing that while some newer models show better accuracy, they still struggle with spatial reasoning, strategic planning, and arithmetic. The analysis identifies common reasoning failures, such as making incorrect assumptions and having difficulty with multi-step deductions. The findings stress the importance of assessing the reasoning process in addition to the final answers, highlighting the need for improvements in LLMs' structured reasoning skills."
                },
                "zh": {
                    "title": "è¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼Œè¶…è¶Šç­”æ¡ˆæ­£ç¡®æ€§",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œä½¿ç”¨äº†50ä¸ªæ–°æ„å»ºçš„é«˜ä¸­æ°´å¹³çš„æ–‡å­—é—®é¢˜ã€‚ä¸ä»¥å¾€åªå…³æ³¨ç­”æ¡ˆæ­£ç¡®æ€§çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬ä¸¥æ ¼åˆ†æäº†æœ€ç»ˆç­”æ¡ˆå’Œè§£å†³æ­¥éª¤ï¼Œä»¥è¯†åˆ«æ¨ç†å¤±è´¥ã€‚è¯„ä¼°äº†åŒ…æ‹¬Mixtralã€Llamaã€Geminiã€GPT-4oå’ŒOpenAIçš„o1å˜ä½“åœ¨å†…çš„å…«ä¸ªæœ€å…ˆè¿›æ¨¡å‹ï¼Œå‘ç°å°½ç®¡æ–°æ¨¡å‹ï¼ˆå¦‚o3-miniã€deepseek-r1ï¼‰åœ¨å‡†ç¡®æ€§ä¸Šæ›´é«˜ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨ç©ºé—´æ¨ç†ã€æˆ˜ç•¥è§„åˆ’å’Œç®—æœ¯æ–¹é¢éƒ½å­˜åœ¨é”™è¯¯ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„é‡è¦æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯ç­”æ¡ˆï¼Œå¹¶è­¦å‘Šä¸è¦é«˜ä¼°LLMsçš„è§£å†³é—®é¢˜èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11578",
            "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
            "url": "https://huggingface.co/papers/2502.11578",
            "abstract": "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.",
            "score": 0,
            "issue_id": 2268,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "039b4ffb618ff3b1",
            "authors": [
                "Birger Moell",
                "Johan Boye"
            ],
            "affiliations": [
                "KTH Royal Institute of Technology, Stockholm, Sweden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11578.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#interpretability",
                    "#science"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ° ĞºĞ°Ğº Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ‡Ğ¸Ñ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LIX Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ (ADD). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° ÑˆĞ²ĞµĞ´ÑĞºĞ¸Ñ… ÑÑÑĞµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑÑ‚Ğ°Ñ€ÑˆĞµĞ¹ ÑˆĞºĞ¾Ğ»Ñ‹ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ChatGPT-o1-mini Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ÑĞ¸Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ LIX Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMLU, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ° ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM."
                },
                "en": {
                    "title": "Evaluating Language Complexity as a Proxy for LLM Performance",
                    "desc": "This paper explores how well large language models (LLMs) can measure language complexity using specific metrics like the LIX readability score and Average Dependency Distance (ADD). The authors tested these models on Swedish essays from high school and university students to see how accurately they could compute LIX scores and perform dependency parsing. The results showed that while all models had some success, ChatGPT-o1-mini was the most reliable, achieving the best accuracy in both tasks. Furthermore, a strong correlation was found between the models' LIX computation accuracy and their performance on the MMLU benchmark, indicating that language complexity measurements can be useful for evaluating LLM capabilities without needing extensive datasets."
                },
                "zh": {
                    "title": "è¯­è¨€å¤æ‚æ€§æµ‹é‡ï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€å¤æ‚æ€§æµ‹é‡ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯è®¡ç®—LIXå¯è¯»æ€§æŒ‡æ ‡å’Œå¹³å‡ä¾èµ–è·ç¦»ï¼ˆADDï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨ç‘å…¸é«˜ä¸­å’Œå¤§å­¦çš„è®ºæ–‡æ¥è¯„ä¼°æ¨¡å‹è®¡ç®—LIXåˆ†æ•°å’Œè¿›è¡Œä¾èµ–è§£æçš„èƒ½åŠ›ï¼Œå¹¶å°†ç»“æœä¸å·²å»ºç«‹çš„åŸºå‡†è¿›è¡Œæ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æ‰€æœ‰æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šéƒ½æœ‰ä¸€å®šèƒ½åŠ›ï¼Œä½†ChatGPT-o1-miniåœ¨LIXè®¡ç®—å’Œä¾èµ–è§£æä¸­è¡¨ç°æœ€ä¸ºä¸€è‡´ï¼Œå‡†ç¡®ç‡æœ€é«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹åœ¨è®¡ç®—LIXæ—¶çš„å‡†ç¡®æ€§ä¸å…¶åœ¨å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰åŸºå‡†ä¸Šçš„æ•´ä½“è¡¨ç°ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è´Ÿç›¸å…³å…³ç³»ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-17.html",
    "link_next": "2025-02-19.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.02",
        "en": "02/19",
        "zh": "2æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 20,
        "#data": 8,
        "#benchmark": 19,
        "#agents": 4,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 2,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 10,
        "#math": 4,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 1,
        "#training": 22,
        "#robotics": 2,
        "#agi": 3,
        "#games": 1,
        "#interpretability": 4,
        "#reasoning": 12,
        "#transfer_learning": 5,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 22,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 6,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†äººå½¢æœºå™¨äººè‡ªåŠ¨è·Œå€’æ¢å¤çš„é‡è¦æ€§ã€‚è®¾è®¡æ§åˆ¶å™¨è®©æœºå™¨äººç«™èµ·æ¥å¾ˆéš¾ï¼Œå› ä¸ºè·Œå€’åæœºå™¨äººå¯èƒ½å¤„äºå„ç§å§¿æ€ï¼Œä¸”åœ°å½¢å¤æ‚ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªå­¦ä¹ æ¡†æ¶ï¼Œè®©æœºå™¨äººåœ¨ä¸åŒå§¿æ€å’Œåœ°å½¢ä¸‹ç«™èµ·æ¥ã€‚æ¡†æ¶åˆ†ä¸¤é˜¶æ®µï¼Œå…ˆæ‰¾åˆ°ç«™èµ·æ¥çš„è·¯å¾„ï¼Œå†ä¼˜åŒ–åŠ¨ä½œä½¿å…¶å¹³ç¨³å¯é ã€‚å®éªŒä¸­ï¼Œæœºå™¨äººåœ¨ä¸åŒåœ°é¢å’Œå§¿æ€ä¸‹æˆåŠŸç«™èµ·æ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨çœŸå®ç¯å¢ƒä¸­æˆåŠŸåº”ç”¨çš„å­¦ä¹ æ–¹æ³•ã€‚",
        "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†äººå½¢æœºå™¨äººè‡ªåŠ¨è·Œå€’æ¢å¤çš„é‡è¦æ€§ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le rÃ©nxÃ­ng jÄ«qÃ¬rÃ©n zÃ¬dÃ²ng diÄ“dÇo huÄ«fÃ¹ de zhÃ²ngyÃ oxÃ¬ng.\n\nè®¾è®¡æ§åˆ¶å™¨è®©æœºå™¨äººç«™èµ·æ¥å¾ˆéš¾ï¼Œå› ä¸ºè·Œå€’åæœºå™¨äººå¯èƒ½å¤„äºå„ç§å§¿æ€ï¼Œä¸”åœ°å½¢å¤æ‚ã€‚\nShÃ¨jÃ¬ kÃ²ngzhÃ¬qÃ¬ rÃ ng jÄ«qÃ¬rÃ©n zhÃ n qÇlÃ¡i hÄ›n nÃ¡n, yÄ«nwÃ¨i diÄ“dÇo hÃ²u jÄ«qÃ¬rÃ©n kÄ›nÃ©ng chÇ”yÃº gÃ¨zhÇ’ng zÄ«tÃ i, qiÄ› dÃ¬xÃ­ng fÃ¹zÃ¡.\n\næ–‡ç« æå‡ºäº†ä¸€ä¸ªå­¦ä¹ æ¡†æ¶ï¼Œè®©æœºå™¨äººåœ¨ä¸åŒå§¿æ€å’Œåœ°å½¢ä¸‹ç«™èµ·æ¥ã€‚\nWÃ©nzhÄng tÃ­chÅ« le yÄ«gÃ¨ xuÃ©xÃ­ kuÃ ngjiÃ , rÃ ng jÄ«qÃ¬rÃ©n zÃ i bÃ¹tÃ³ng zÄ«tÃ i hÃ© dÃ¬xÃ­ng xiÃ  zhÃ n qÇlÃ¡i.\n\næ¡†æ¶åˆ†ä¸¤é˜¶æ®µï¼Œå…ˆæ‰¾åˆ°ç«™èµ·æ¥çš„è·¯å¾„ï¼Œå†ä¼˜åŒ–åŠ¨ä½œä½¿å…¶å¹³ç¨³å¯é ã€‚\nKuÃ ngjiÃ  fÄ“n liÇng jiÄ“duÃ n, xiÄn zhÇo dÃ o zhÃ n qÇlÃ¡i de lÃ¹jÃ¬ng, zÃ i yÅuhuÃ  dÃ²ngzuÃ² shÇ qÃ­ pÃ­ngwÄ›n kÄ›kÃ o.\n\nå®éªŒä¸­ï¼Œæœºå™¨äººåœ¨ä¸åŒåœ°é¢å’Œå§¿æ€ä¸‹æˆåŠŸç«™èµ·æ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨çœŸå®ç¯å¢ƒä¸­æˆåŠŸåº”ç”¨çš„å­¦ä¹ æ–¹æ³•ã€‚\nShÃ­yÃ n zhÅng, jÄ«qÃ¬rÃ©n zÃ i bÃ¹tÃ³ng dÃ¬miÃ n hÃ© zÄ«tÃ i xiÃ  chÃ©nggÅng zhÃ n qÇlÃ¡i, zhÃ¨ shÃ¬ shÇ’ucÃ¬ zÃ i zhÄ“nshÃ­ huÃ¡njÃ¬ng zhÅng chÃ©nggÅng yÃ¬ngyÃ²ng de xuÃ©xÃ­ fÄngfÇ.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'äººå½¢æœºå™¨äºº', 'pinyin': 'rÃ©n xÃ­ng jÄ« qÃ¬ rÃ©n', 'trans': 'humanoid robot'},\n{'word': 'è‡ªåŠ¨', 'pinyin': 'zÃ¬ dÃ²ng', 'trans': 'automatic'},\n{'word': 'è·Œå€’', 'pinyin': 'diÄ“ dÇo', 'trans': 'fall down'},\n{'word': 'æ¢å¤', 'pinyin': 'huÄ« fÃ¹', 'trans': 'recover'},\n{'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'},\n{'word': 'è®¾è®¡', 'pinyin': 'shÃ¨ jÃ¬', 'trans': 'design'},\n{'word': 'æ§åˆ¶å™¨', 'pinyin': 'kÃ²ng zhÃ¬ qÃ¬', 'trans': 'controller'},\n{'word': 'ç«™èµ·æ¥', 'pinyin': 'zhÃ n qÇ lÃ¡i', 'trans': 'stand up'},\n{'word': 'å§¿æ€', 'pinyin': 'zÄ« tÃ i', 'trans': 'posture'},\n{'word': 'åœ°å½¢', 'pinyin': 'dÃ¬ xÃ­ng', 'trans': 'terrain'},\n{'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'å­¦ä¹ ', 'pinyin': 'xuÃ© xÃ­', 'trans': 'learn'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'é˜¶æ®µ', 'pinyin': 'jiÄ“ duÃ n', 'trans': 'stage'},\n{'word': 'è·¯å¾„', 'pinyin': 'lÃ¹ jÃ¬ng', 'trans': 'path'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimize'},\n{'word': 'åŠ¨ä½œ', 'pinyin': 'dÃ²ng zuÃ²', 'trans': 'action'},\n{'word': 'å¹³ç¨³', 'pinyin': 'pÃ­ng wÄ›n', 'trans': 'stable'},\n{'word': 'å¯é ', 'pinyin': 'kÄ› kÃ o', 'trans': 'reliable'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'åœ°é¢', 'pinyin': 'dÃ¬ miÃ n', 'trans': 'ground'},\n{'word': 'æˆåŠŸ', 'pinyin': 'chÃ©ng gÅng', 'trans': 'success'},\n{'word': 'é¦–æ¬¡', 'pinyin': 'shÇ’u cÃ¬', 'trans': 'first time'},\n{'word': 'çœŸå®', 'pinyin': 'zhÄ“n shÃ­', 'trans': 'real'},\n{'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡n jÃ¬ng', 'trans': 'environment'},\n{'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ng yÃ²ng', 'trans': 'apply'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}]",
        "trans": "This article discusses the importance of automatic fall recovery for humanoid robots. Designing a controller to make a robot stand up after a fall is challenging because the robot may be in various postures and the terrain can be complex. The article proposes a learning framework that enables the robot to stand up from different postures and terrains. The framework consists of two stages: first, finding a path to stand up, and then optimizing the actions to make them smooth and reliable. In experiments, the robot successfully stood up from various surfaces and postures, marking the first successful application of a learning method in a real-world environment.",
        "update_ts": "2025-02-18 09:11"
    }
}