{
    "date": {
        "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 6",
        "zh": "3æœˆ6æ—¥"
    },
    "time_utc": "2025-03-06 15:11",
    "weekday": 3,
    "issue_id": 2567,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.00865",
            "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
            "url": "https://huggingface.co/papers/2503.00865",
            "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.",
            "score": 37,
            "issue_id": 2555,
            "pub_date": "2025-03-02",
            "pub_date_card": {
                "ru": "2 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 2",
                "zh": "3æœˆ2æ—¥"
            },
            "hash": "bc2424e709a2dd78",
            "authors": [
                "Yiran Zhao",
                "Chaoqun Liu",
                "Yue Deng",
                "Jiahao Ying",
                "Mahani Aljunied",
                "Zhaodonghui Li",
                "Lidong Bing",
                "Hou Pong Chan",
                "Yu Rong",
                "Deli Zhao",
                "Wenxuan Zhang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00865.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#architecture",
                    "#open_source",
                    "#training",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Babel: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Babel, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ 25 ÑĞ°Ğ¼Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ¸Ñ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸: Babel-9B Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ Babel-83B, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ±Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Babel: Bridging the Language Gap with Open Multilingual LLMs",
                    "desc": "This paper presents Babel, an innovative open-source multilingual large language model (LLM) that addresses the lack of coverage for under-resourced languages in existing models. Babel supports the top 25 languages spoken globally, reaching over 90% of the world's population, and includes many languages that are often neglected. The model employs a unique layer extension technique to increase its parameter count, enhancing its performance beyond traditional pretraining methods. With two variants, Babel-9B and Babel-83B, the model demonstrates superior performance on multilingual tasks, outperforming other open LLMs of similar size and achieving results comparable to commercial models."
                },
                "zh": {
                    "title": "Babelï¼šæ‰“ç ´è¯­è¨€å£å’çš„å¤šè¯­è¨€æ¨¡å‹",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†å¼€æºçš„å¤šè¯­è¨€LLMsä»ç„¶ç¨€ç¼ºï¼Œç°æœ‰æ¨¡å‹é€šå¸¸åœ¨è¯­è¨€è¦†ç›–ä¸Šæœ‰é™ã€‚è®¸å¤šæ¨¡å‹ä¼˜å…ˆè€ƒè™‘èµ„æºä¸°å¯Œçš„è¯­è¨€ï¼Œè€Œå¹¿æ³›ä½¿ç”¨ä½†èµ„æºä¸è¶³çš„è¯­è¨€å¸¸å¸¸è¢«å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Babelï¼Œä¸€ä¸ªå¼€æ”¾çš„å¤šè¯­è¨€LLMï¼Œè¦†ç›–å…¨çƒå‰25ç§è¯­è¨€ï¼Œæ”¯æŒè¶…è¿‡90%çš„äººå£ï¼Œå¹¶åŒ…æ‹¬è®¸å¤šå…¶ä»–å¼€æºå¤šè¯­è¨€LLMså¿½è§†çš„è¯­è¨€ã€‚Babelé€šè¿‡å±‚æ‰©å±•æŠ€æœ¯å¢åŠ å‚æ•°æ•°é‡ï¼Œæå‡äº†æ€§èƒ½ï¼Œå¹¶æ¨å‡ºäº†ä¸¤ä¸ªå˜ä½“ï¼šBabel-9Bå’ŒBabel-83Bï¼Œåè€…åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.00329",
            "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
            "url": "https://huggingface.co/papers/2503.00329",
            "abstract": "Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page.",
            "score": 12,
            "issue_id": 2555,
            "pub_date": "2025-03-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 1",
                "zh": "3æœˆ1æ—¥"
            },
            "hash": "0483c542c8885777",
            "authors": [
                "Benjamin Schneider",
                "Florian Kerschbaum",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Cheriton School of Computer Science, University of Waterloo",
                "Vector Institute, Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00329.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ABC: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ABC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ABC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ABC Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "ABC: Unifying Vision and Language for Enhanced Multimodal Understanding",
                    "desc": "This paper presents ABC, a new multimodal embedding model that integrates visual and natural language inputs more effectively than existing CLIP-based methods. Unlike previous models that treat images and text separately, ABC combines these modalities deeply, allowing for better interaction and user control. The model excels in zero-shot tasks, particularly in image-to-text retrieval and classification, outperforming others in the Massive Multimodal Embedding Benchmark. To assess its capabilities, the authors introduce CtrlBench, a benchmark designed to evaluate the model's performance in handling complex visual retrieval tasks with natural language instructions."
                },
                "zh": {
                    "title": "ABCï¼šå¤šæ¨¡æ€åµŒå…¥çš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºABCçš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è§†è§‰æ£€ç´¢å’Œåˆ†ç±»ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚ä¸ç°æœ‰çš„CLIPæ–¹æ³•ä¸åŒï¼ŒABCé€šè¿‡æ·±åº¦æ•´åˆå›¾åƒç‰¹å¾å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæä¾›æ›´å¼ºçš„æ¨¡æ€äº¤äº’ã€‚ABCåœ¨MSCOCOå›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨åˆ†ç±»å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚é€šè¿‡è®¾è®¡CtrlBenchåŸºå‡†ï¼Œè¯„ä¼°äº†ABCåœ¨å¤„ç†å¤æ‚è§†è§‰æ£€ç´¢é—®é¢˜æ—¶çš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶é«˜è´¨é‡çš„è¡¨ç¤ºå’Œçµæ´»çš„è‡ªç„¶è¯­è¨€æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03751",
            "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
            "url": "https://huggingface.co/papers/2503.03751",
            "abstract": "We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
            "score": 10,
            "issue_id": 2555,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 5",
                "zh": "3æœˆ5æ—¥"
            },
            "hash": "8f5f2ad910a260c0",
            "authors": [
                "Xuanchi Ren",
                "Tianchang Shen",
                "Jiahui Huang",
                "Huan Ling",
                "Yifan Lu",
                "Merlin Nimier-David",
                "Thomas MÃ¼ller",
                "Alexander Keller",
                "Sanja Fidler",
                "Jun Gao"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03751.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "GEN3C - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D-ĞºÑÑˆ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ½ĞµĞµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸÑ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² GEN3C Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° 2D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ 3D-ĞºÑÑˆĞ° Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, Ğ½Ğµ Ñ‚Ñ€Ğ°Ñ‚Ñ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "GEN3C: Mastering Video Generation with 3D Precision and Camera Control",
                    "desc": "GEN3C is a generative video model that enhances video generation by incorporating precise camera control and maintaining temporal 3D consistency. Unlike previous models that often lack 3D information, GEN3C utilizes a 3D cache of point clouds derived from depth predictions, allowing for more coherent object presence in videos. The model is conditioned on 2D renderings from this cache, enabling it to generate new frames without needing to remember past outputs or infer scene structure from camera angles. This approach results in superior camera control and state-of-the-art performance in generating novel views, particularly in complex scenarios like driving scenes."
                },
                "zh": {
                    "title": "GEN3Cï¼šç²¾ç¡®ç›¸æœºæ§åˆ¶ä¸æ—¶é—´ä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹",
                    "desc": "æˆ‘ä»¬æå‡ºäº†GEN3Cï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ç²¾ç¡®ç›¸æœºæ§åˆ¶å’Œæ—¶é—´ä¸€è‡´æ€§çš„ç”Ÿæˆè§†é¢‘æ¨¡å‹ã€‚ä»¥å¾€çš„è§†é¢‘æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œä½†å¾€å¾€ç¼ºä¹3Dä¿¡æ¯ï¼Œå¯¼è‡´ç‰©ä½“å‡ºç°å’Œæ¶ˆå¤±çš„ä¸ä¸€è‡´æ€§ã€‚GEN3Cé€šè¿‡3Dç¼“å­˜æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨ä»ç§å­å›¾åƒæˆ–å…ˆå‰ç”Ÿæˆå¸§ä¸­é¢„æµ‹çš„åƒç´ æ·±åº¦è·å¾—çš„ç‚¹äº‘ã€‚è¿™æ ·ï¼ŒGEN3Cèƒ½å¤Ÿåœ¨ç”¨æˆ·æä¾›çš„æ–°ç›¸æœºè½¨è¿¹ä¸‹ï¼Œä¸“æ³¨äºç”Ÿæˆæœªè§‚å¯Ÿåˆ°çš„åŒºåŸŸï¼Œå¹¶æœ‰æ•ˆæ¨è¿›åœºæ™¯çŠ¶æ€åˆ°ä¸‹ä¸€ä¸ªå¸§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02951",
            "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
            "url": "https://huggingface.co/papers/2503.02951",
            "abstract": "We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.",
            "score": 10,
            "issue_id": 2555,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 4",
                "zh": "3æœˆ4æ—¥"
            },
            "hash": "6c344ba0bf71ac84",
            "authors": [
                "Zhangchen Xu",
                "Yang Liu",
                "Yueqin Yin",
                "Mingyuan Zhou",
                "Radha Poovendran"
            ],
            "affiliations": [
                "Microsoft",
                "The University of Texas at Austin",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02951.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ§‘â€ğŸ’»",
                "ru": {
                    "title": "KodCode: Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "KodCode - ÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ-Ñ‚ĞµÑÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ KodCode Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° KodCode, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "KodCode: Elevating Coding Models with Verified Data",
                    "desc": "KodCode is a synthetic dataset designed to improve the training of Large Language Models (LLMs) for coding tasks by providing high-quality, verifiable data. It includes question-solution-test triplets that are validated through a self-verification process, ensuring both correctness and a wide range of coding difficulties. The dataset is generated using a systematic pipeline that synthesizes coding questions, creates solutions, and develops test cases, particularly focusing on challenging problems. Fine-tuning experiments show that models trained on KodCode outperform existing models on various coding benchmarks, demonstrating its effectiveness in enhancing LLM performance."
                },
                "zh": {
                    "title": "KodCodeï¼šé«˜è´¨é‡ç¼–ç æ•°æ®é›†çš„è§£å†³æ–¹æ¡ˆ",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†KodCodeï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è·å–é«˜è´¨é‡ã€å¯éªŒè¯çš„è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ï¼Œä»¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ã€‚ç°æœ‰çš„ä»£ç èµ„æºé€šå¸¸æ— æ³•ç¡®ä¿è¦†ç›–èŒƒå›´å¹¿æ³›æˆ–æ­£ç¡®æ€§å¯éªŒè¯ã€‚KodCodeç”±é—®é¢˜-è§£å†³æ–¹æ¡ˆ-æµ‹è¯•ä¸‰å…ƒç»„ç»„æˆï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯ç¨‹åºç³»ç»Ÿåœ°éªŒè¯ã€‚æˆ‘ä»¬çš„æµç¨‹åŒ…æ‹¬åˆæˆå„ç§ç¼–ç é—®é¢˜ï¼Œç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶åœ¨åæœŸé€šè¿‡é‡å†™é—®é¢˜å’Œç”Ÿæˆå“åº”æ¥è¿›è¡Œæ•°æ®åˆæˆï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªå¤§è§„æ¨¡ã€å¼ºå¤§ä¸”å¤šæ ·åŒ–çš„ç¼–ç æ•°æ®é›†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03746",
            "title": "Process-based Self-Rewarding Language Models",
            "url": "https://huggingface.co/papers/2503.03746",
            "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities.",
            "score": 9,
            "issue_id": 2564,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 5",
                "zh": "3æœˆ5æ—¥"
            },
            "hash": "808bee960390ec29",
            "authors": [
                "Shimao Zhang",
                "Xiao Liu",
                "Xin Zhang",
                "Junxiao Liu",
                "Zheheng Luo",
                "Shujian Huang",
                "Yeyun Gong"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "The University of Manchester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03746.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#alignment",
                    "#rlhf",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ: ÑˆĞ°Ğ³ Ğ·Ğ° ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº ÑĞ²ĞµÑ€Ñ…Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Process-based Self-Rewarding, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering LLMs with Process-based Self-Rewarding for Superior Reasoning",
                    "desc": "This paper discusses the limitations of current self-rewarding methods used to train Large Language Models (LLMs), particularly in mathematical reasoning tasks. The authors introduce a new approach called Process-based Self-Rewarding, which incorporates long-thought reasoning and a step-wise evaluation process. By allowing LLMs to act as judges of their own outputs, this method optimizes the training process iteratively. The results show significant improvements in LLM performance on mathematical reasoning benchmarks, suggesting that self-rewarding can enhance reasoning capabilities beyond human levels."
                },
                "zh": {
                    "title": "åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±ï¼šè¶…è¶Šäººç±»çš„æ¨ç†èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå¤šä¸ªåœºæ™¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ï¼Œç ”ç©¶è€…ä½¿ç”¨äººç±»æ ‡æ³¨çš„åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†è¿™å—åˆ°äººç±»è¡¨ç°ä¸Šé™çš„é™åˆ¶ã€‚å› æ­¤ï¼Œæå‡ºäº†è‡ªæˆ‘å¥–åŠ±çš„æ–¹æ³•ï¼Œè®©è¯­è¨€æ¨¡å‹é€šè¿‡å¥–åŠ±è‡ªå·±çš„è¾“å‡ºç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªæˆ‘å¥–åŠ±æ–¹æ³•åœ¨æ•°å­¦æ¨ç†åœºæ™¯ä¸­æ•ˆæœä¸ä½³ï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±ç®¡é“ï¼Œé€šè¿‡å¼•å…¥é•¿æ—¶é—´æ€è€ƒæ¨ç†ã€é€æ­¥çš„è¯­è¨€æ¨¡å‹è¯„åˆ¤å’Œé€æ­¥çš„åå¥½ä¼˜åŒ–ï¼ŒæˆåŠŸæå‡äº†è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œå±•ç¤ºäº†è‡ªæˆ‘å¥–åŠ±åœ¨è¶…è¶Šäººç±»èƒ½åŠ›çš„æ¨ç†ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03278",
            "title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions",
            "url": "https://huggingface.co/papers/2503.03278",
            "abstract": "Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities.",
            "score": 9,
            "issue_id": 2560,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 5",
                "zh": "3æœˆ5æ—¥"
            },
            "hash": "6103dbe5d60b5f3f",
            "authors": [
                "Jun Li",
                "Che Liu",
                "Wenjia Bai",
                "Rossella Arcucci",
                "Cosmin I. Bercea",
                "Julia A. Schnabel"
            ],
            "affiliations": [
                "Helmholtz AI and Helmholtz Munich, Germany",
                "Imperial College London, UK",
                "Kings College London, UK",
                "Munich Center for Machine Learning, Germany",
                "Technical University of Munich, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03278.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#healthcare",
                    "#alignment",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ VLM Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹. Ğ­Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Florence-2 Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ VLM, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Medical VLMs through Decomposed Knowledge",
                    "desc": "This paper presents a new method to improve Visual Language Models (VLMs) for detecting and locating abnormalities in medical images. The authors address the challenge of complex medical terminology by breaking down medical concepts into simpler attributes and common visual patterns. This approach enhances the alignment between text descriptions and visual features, leading to better performance in recognizing and localizing abnormalities. The proposed method shows competitive results with larger models while using significantly less training data, indicating its efficiency and strong generalization capabilities."
                },
                "zh": {
                    "title": "åˆ†è§£åŒ»å­¦çŸ¥è¯†ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸï¼Œå°¤å…¶æ˜¯åŒ»å­¦å›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹å’Œå®šä½æ–¹é¢ï¼Œä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚åŒ»å­¦æœ¯è¯­çš„å¤æ‚æ€§ä½¿å¾—å°†ç—…ç†å¼‚å¸¸æœ¯è¯­ä¸ç›¸åº”çš„è§†è§‰ç‰¹å¾ç›´æ¥å…³è”å˜å¾—å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ†è§£åŒ»å­¦çŸ¥è¯†æ¥å¢å¼ºVLMåœ¨åŒ»å­¦å¼‚å¸¸æ£€æµ‹å’Œå®šä½ä¸­çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†åŒ»å­¦æ¦‚å¿µåˆ†è§£ä¸ºåŸºæœ¬å±æ€§å’Œå¸¸è§è§†è§‰æ¨¡å¼ï¼Œä¿ƒè¿›äº†æ–‡æœ¬æè¿°ä¸è§†è§‰ç‰¹å¾ä¹‹é—´çš„æ›´å¼ºå¯¹é½ï¼Œä»è€Œæé«˜äº†åŒ»å­¦å›¾åƒä¸­å¼‚å¸¸çš„è¯†åˆ«å’Œå®šä½èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01836",
            "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom",
            "url": "https://huggingface.co/papers/2503.01836",
            "abstract": "Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores, model perplexity), they fail to capture the complexity of instruction-following across diverse fields. Therefore, we investigate more diverse signals to capture comprehensive instruction-response pair characteristics and propose three foundational metrics that leverage Multi-LLM wisdom, informed by (1) diverse LLM responses and (2) reward model assessment. Building upon base metrics, we propose CrowdSelect, an integrated metric incorporating a clustering-based approach to maintain response diversity. Our comprehensive experiments demonstrate that our foundation metrics consistently improve performance across 4 base models on MT-bench and Arena-Hard. CrowdSelect, efficiently incorporating all metrics, achieves state-of-the-art performance in both Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and 11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring valuable insights for future research in this direction. Code are available at https://github.com/listentm/crowdselect.",
            "score": 7,
            "issue_id": 2560,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "d59d65fb3b60c043",
            "authors": [
                "Yisen Li",
                "Lingfeng Yang",
                "Wenxuan Shen",
                "Pan Zhou",
                "Yao Wan",
                "Weiwei Lin",
                "Dongping Chen"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01836.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#synthetic",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CrowdSelect: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ CrowdSelect. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸x ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. CrowdSelect Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MT-bench Ğ¸ Arena-Hard. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ĞµÑĞµÑ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Model Training with Diverse Instruction Metrics",
                    "desc": "This paper focuses on improving the training of smaller models by distilling the instruction-following abilities of larger language models (LLMs). It critiques existing methods that use simple metrics for selecting synthetic instruction data, which do not adequately reflect the complexity of instruction-following tasks. The authors propose new metrics that utilize diverse responses from multiple LLMs and a reward model to better assess instruction-response pairs. Their method, CrowdSelect, combines these metrics with a clustering approach to enhance response diversity, leading to significant performance improvements in various model evaluations."
                },
                "zh": {
                    "title": "æå‡å°æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›æç‚¼åˆ°æ›´å°çš„æ¨¡å‹ä¸­ã€‚ç°æœ‰çš„åˆæˆæŒ‡ä»¤æ•°æ®é€‰æ‹©ç­–ç•¥ä¸»è¦ä¾èµ–å•ä¸€ç»´åº¦çš„ä¿¡å·ï¼Œæœªèƒ½å…¨é¢æ•æ‰æŒ‡ä»¤è·Ÿéšçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§åŸºç¡€æŒ‡æ ‡ï¼Œåˆ©ç”¨å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºæ…§ï¼Œç»“åˆå¤šæ ·çš„å“åº”å’Œå¥–åŠ±æ¨¡å‹è¯„ä¼°ã€‚é€šè¿‡ç»¼åˆå®éªŒï¼Œæˆ‘ä»¬çš„CrowdSelectæŒ‡æ ‡åœ¨å¤šä¸ªåŸºæ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†æœªæ¥ç ”ç©¶çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03044",
            "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
            "url": "https://huggingface.co/papers/2503.03044",
            "abstract": "Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. Our QE4PE study investigates the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated by behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.",
            "score": 4,
            "issue_id": 2560,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 4",
                "zh": "3æœˆ4æ—¥"
            },
            "hash": "e4d3d7db506b6e1c",
            "authors": [
                "Gabriele Sarti",
                "VilÃ©m Zouhar",
                "Grzegorz ChrupaÅ‚a",
                "Ana Guerberof-Arenas",
                "Malvina Nissim",
                "Arianna Bisazza"
            ],
            "affiliations": [
                "CLCG, University of Groningen",
                "CSAI, Tilburg University",
                "ETH ZÃ¼rich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03044.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#data",
                    "#multilingual",
                    "#healthcare"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ² (word-level QE) Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾ÑÑ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ»Ğ¸ 42 Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ²ÑˆĞ¸Ñ… Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°, ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ QE Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹."
                },
                "en": {
                    "title": "Enhancing Post-Editing Efficiency with Word-Level Quality Estimation",
                    "desc": "This paper explores how word-level quality estimation (QE) can help improve the efficiency of human post-editing in machine translation (MT). It examines the effectiveness of different methods for highlighting potential translation errors, comparing supervised and uncertainty-based approaches. The study involves 42 professional post-editors and assesses their editing speed and quality improvements through detailed behavioral logs and human annotations. The findings reveal that factors like domain, language, and editor speed significantly influence the effectiveness of error highlights, indicating a need to bridge the gap between the accuracy of QE systems and their practical usability in real-world editing tasks."
                },
                "zh": {
                    "title": "æå‡æœºå™¨ç¿»è¯‘åç¼–è¾‘æ•ˆç‡çš„å…³é”®",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†è¯çº§è´¨é‡è¯„ä¼°ï¼ˆQEï¼‰åœ¨æœºå™¨ç¿»è¯‘åç¼–è¾‘ä¸­çš„å½±å“ã€‚æˆ‘ä»¬åˆ†æäº†42åä¸“ä¸šåç¼–è¾‘åœ¨ä¸¤ç§ç¿»è¯‘æ–¹å‘ä¸‹çš„è¡¨ç°ï¼Œæ¯”è¾ƒäº†å››ç§é”™è¯¯èŒƒå›´é«˜äº®æ–¹å¼ï¼ŒåŒ…æ‹¬ç›‘ç£å’ŒåŸºäºä¸ç¡®å®šæ€§çš„è¯çº§QEæ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œé¢†åŸŸã€è¯­è¨€å’Œç¼–è¾‘é€Ÿåº¦æ˜¯å½±å“é«˜äº®æ•ˆæœçš„å…³é”®å› ç´ ã€‚ç»“æœè¡¨æ˜ï¼Œäººå·¥å’Œè‡ªåŠ¨QEé«˜äº®ä¹‹é—´å­˜åœ¨é€‚åº¦å·®å¼‚ï¼Œçªæ˜¾äº†ä¸“ä¸šå·¥ä½œæµç¨‹ä¸­å‡†ç¡®æ€§ä¸å¯ç”¨æ€§ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01933",
            "title": "Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective",
            "url": "https://huggingface.co/papers/2503.01933",
            "abstract": "Deploying large scale language models on edge devices faces inherent challenges such as high computational demands, energy consumption, and potential data privacy risks. This paper introduces the Shakti Small Language Models (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these constraints headon. By combining efficient architectures, quantization techniques, and responsible AI principles, the Shakti series enables on-device intelligence for smartphones, smart appliances, IoT systems, and beyond. We provide comprehensive insights into their design philosophy, training pipelines, and benchmark performance on both general tasks (e.g., MMLU, Hellaswag) and specialized domains (healthcare, finance, and legal). Our findings illustrate that compact models, when carefully engineered and fine-tuned, can meet and often exceed expectations in real-world edge-AI scenarios.",
            "score": 3,
            "issue_id": 2563,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "6fa74210552cc49f",
            "authors": [
                "Rakshit Aralimatti",
                "Syed Abdul Gaffar Shakhadri",
                "Kruthika KR",
                "Kartik Basavaraj Angadi"
            ],
            "affiliations": [
                "SandLogic Technologies Pvt Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01933.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#benchmark",
                    "#training",
                    "#ethics",
                    "#inference",
                    "#small_models",
                    "#optimization"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞœĞ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° ĞºÑ€Ğ°Ñ ÑĞµÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Shakti, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿ĞµÑ€Ğ¸Ñ„ĞµÑ€Ğ¸Ğ¹Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Shakti-100M, Shakti-250M Ğ¸ Shakti-500M Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜, ÑĞµÑ€Ğ¸Ñ Shakti Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ², ÑƒĞ¼Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ IoT-ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ, Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿ĞµÑ€Ğ¸Ñ„ĞµÑ€Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Empowering Edge Devices with Efficient Language Models",
                    "desc": "This paper presents the Shakti Small Language Models (SLMs) designed to operate efficiently on edge devices while addressing challenges like high computational needs and energy consumption. The Shakti models, including Shakti-100M, Shakti-250M, and Shakti-500M, utilize advanced architectures and quantization techniques to optimize performance without compromising data privacy. The authors detail the design philosophy, training processes, and benchmark results across various tasks and specialized fields such as healthcare and finance. The results demonstrate that well-engineered compact models can perform effectively in real-world applications, showcasing the potential of on-device AI."
                },
                "zh": {
                    "title": "å°å‹è¯­è¨€æ¨¡å‹ï¼Œæ™ºèƒ½è¾¹ç¼˜è®¡ç®—çš„æœªæ¥",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†Shaktiå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ï¼ŒåŒ…æ‹¬Shakti-100Mã€Shakti-250Må’ŒShakti-500Mï¼Œæ—¨åœ¨è§£å†³åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´çš„é«˜è®¡ç®—éœ€æ±‚å’Œèƒ½è€—é—®é¢˜ã€‚é€šè¿‡ç»“åˆé«˜æ•ˆæ¶æ„ã€é‡åŒ–æŠ€æœ¯å’Œè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½åŸåˆ™ï¼ŒShaktiç³»åˆ—å®ç°äº†æ™ºèƒ½æ‰‹æœºã€æ™ºèƒ½å®¶ç”µå’Œç‰©è”ç½‘ç³»ç»Ÿçš„æœ¬åœ°æ™ºèƒ½ã€‚æˆ‘ä»¬æä¾›äº†å…³äºå…¶è®¾è®¡ç†å¿µã€è®­ç»ƒæµç¨‹å’Œåœ¨ä¸€èˆ¬ä»»åŠ¡ï¼ˆå¦‚MMLUã€Hellaswagï¼‰åŠä¸“ä¸šé¢†åŸŸï¼ˆåŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ï¼‰ä¸Šçš„åŸºå‡†æ€§èƒ½çš„å…¨é¢è§è§£ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒè®¾è®¡å’Œå¾®è°ƒçš„ç´§å‡‘æ¨¡å‹èƒ½å¤Ÿåœ¨å®é™…è¾¹ç¼˜äººå·¥æ™ºèƒ½åœºæ™¯ä¸­æ»¡è¶³å¹¶è¶…è¶Šé¢„æœŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20317",
            "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
            "url": "https://huggingface.co/papers/2502.20317",
            "abstract": "Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.",
            "score": 3,
            "issue_id": 2561,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "686b2ff85600f281",
            "authors": [
                "Yongjia Lei",
                "Haoyu Han",
                "Ryan A. Rossi",
                "Franck Dernoncourt",
                "Nedim Lipka",
                "Mahantesh M Halappanavar",
                "Jiliang Tang",
                "Yu Wang"
            ],
            "affiliations": [
                "Adobe Research",
                "Michigan State University",
                "Pacific Northwest National Laboratory",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20317.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#graphs",
                    "#dataset",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ°Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MoR (Mixture of Structural-and-Textual Retrieval) Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. MoR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ñ…Ğ¾Ğ´ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ½ĞµÑ†, Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MoR Ğ² Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Harmonizing Text and Structure for Better Knowledge Retrieval",
                    "desc": "This paper introduces a new method called Mixture of Structural-and-Textual Retrieval (MoR) for improving the retrieval of knowledge from Text-rich Graph Knowledge Bases (TG-KBs). Unlike traditional methods that treat textual and structural knowledge separately, MoR integrates both types through a three-stage framework: Planning, Reasoning, and Organizing. In the Planning stage, it creates graphs that outline the logic for query responses, while the Reasoning stage combines structural traversal with textual matching to gather relevant candidates. Finally, the Organizing stage enhances the selection process by reranking candidates based on their structural paths, leading to better retrieval performance across various query types."
                },
                "zh": {
                    "title": "ç»“æ„ä¸æ–‡æœ¬çŸ¥è¯†çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆæ£€ç´¢æ–¹æ³•ï¼Œç§°ä¸ºç»“æ„ä¸æ–‡æœ¬æ£€ç´¢çš„æ··åˆä½“ï¼ˆMoRï¼‰ï¼Œæ—¨åœ¨åŒæ—¶åˆ©ç”¨æ–‡æœ¬å’Œç»“æ„çŸ¥è¯†æ¥å›ç­”æŸ¥è¯¢ã€‚MoRé€šè¿‡è§„åˆ’-æ¨ç†-ç»„ç»‡çš„æ¡†æ¶æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œåœ¨è§„åˆ’é˜¶æ®µç”Ÿæˆæ–‡æœ¬è§„åˆ’å›¾ï¼Œæ˜ç¡®å›ç­”æŸ¥è¯¢çš„é€»è¾‘ã€‚æ¥ç€ï¼Œåœ¨æ¨ç†é˜¶æ®µï¼ŒMoRç»“åˆç»“æ„éå†å’Œæ–‡æœ¬åŒ¹é…ï¼Œä»æ–‡æœ¬ä¸°å¯Œçš„å›¾çŸ¥è¯†åº“ä¸­è·å–å€™é€‰ç­”æ¡ˆã€‚æœ€åï¼Œåœ¨ç»„ç»‡é˜¶æ®µï¼ŒMoRæ ¹æ®ç»“æ„è½¨è¿¹å¯¹è·å–çš„å€™é€‰ç­”æ¡ˆè¿›è¡Œé‡æ–°æ’åºï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç»“æ„å’Œæ–‡æœ¬æ£€ç´¢çš„åè°ƒæ€§ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18860",
            "title": "Exploring Rewriting Approaches for Different Conversational Tasks",
            "url": "https://huggingface.co/papers/2502.18860",
            "abstract": "Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best.",
            "score": 3,
            "issue_id": 2561,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "1f018cc4f38149bc",
            "authors": [
                "Md Mehrab Tanjim",
                "Ryan A. Rossi",
                "Mike Rimer",
                "Xiang Chen",
                "Sungchul Kim",
                "Vaishnavi Muppala",
                "Tong Yu",
                "Zhengmian Hu",
                "Ritwik Sinha",
                "Wei Zhang",
                "Iftikhar Ahamath Burhanuddin",
                "Franck Dernoncourt"
            ],
            "affiliations": [
                "Adobe Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18860.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ˜Ğ˜: Ğ¾Ğ´Ğ¸Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ñ…: Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ñ‚ĞµĞºÑÑ‚-Ğ²-Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ”Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ° Ğ´Ğ»Ñ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Tailoring Response Strategies for Conversational Assistants",
                    "desc": "This paper explores how conversational assistants can improve their responses by using two methods: rewriting and fusion. The rewriting method modifies user questions to enhance accuracy, while the fusion method combines information from past interactions to generate answers. The study shows that the effectiveness of these methods varies based on the task; rewriting is better for text-based question answering, while fusion excels in generating visualizations from data analysis tasks. The findings highlight the importance of tailoring the approach to the specific use case for optimal performance."
                },
                "zh": {
                    "title": "å¯¹è¯åŠ©æ‰‹ä¸­çš„é—®é¢˜é‡å†™ä¸èåˆæ–¹æ³•çš„æœ€ä½³é€‰æ‹©",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¯¹è¯åŠ©æ‰‹ä¸­é—®é¢˜é‡å†™ç®—æ³•çš„ä¸¤ç§ä¸åŒæ–¹æ³•ï¼šé‡å†™å’Œèåˆã€‚è¿™ä¸¤ç§æ–¹æ³•åœ¨æ–‡æœ¬ç”Ÿæˆå’Œå¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¸åŒï¼Œå…·ä½“å–å†³äºåº”ç”¨åœºæ™¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹äºå¯¹è¯é—®ç­”åŠ©æ‰‹ï¼ŒæŸ¥è¯¢é‡å†™æ–¹æ³•æ•ˆæœæœ€ä½³ï¼›è€Œå¯¹äºç”Ÿæˆå¯è§†åŒ–å’Œæ•°æ®è¡¨çš„æ•°æ®åˆ†æåŠ©æ‰‹ï¼ŒæŸ¥è¯¢èåˆæ–¹æ³•æ›´ä¸ºæœ‰æ•ˆã€‚æˆ‘ä»¬è¿˜åˆ†æäº†çŸ­å¯¹è¯å’Œé•¿å¯¹è¯çš„æ•°æ®é›†ï¼Œç»“æœè¡¨æ˜æŸ¥è¯¢èåˆåœ¨æ•°æ®åˆ†æä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°æ›´å¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01763",
            "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
            "url": "https://huggingface.co/papers/2503.01763",
            "abstract": "Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.",
            "score": 3,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "e6a23582f741dc5b",
            "authors": [
                "Zhengliang Shi",
                "Yuhan Wang",
                "Lingyong Yan",
                "Pengjie Ren",
                "Shuaiqiang Wang",
                "Dawei Yin",
                "Zhaochun Ren"
            ],
            "affiliations": [
                "Baidu Inc., Beijing, China",
                "Leiden University, Leiden, The Netherlands",
                "Shandong University, Qingdao, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01763.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ToolRet: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ˜Ğ˜",
                    "desc": "ToolRet - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 7,6 Ñ‚Ñ‹ÑÑÑ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 43 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ToolRet. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 200 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Tool Retrieval for Language Models with ToolRet",
                    "desc": "This paper introduces ToolRet, a benchmark designed to evaluate the effectiveness of information retrieval (IR) models in selecting tools for large language models (LLMs) in practical tasks. The authors highlight that existing benchmarks often rely on a limited set of pre-annotated tools, which does not reflect real-world complexities. Their findings reveal that even high-performing IR models struggle with tool retrieval in this new context, leading to lower task success rates for LLMs. To address this issue, they provide a large-scale training dataset that significantly enhances the tool retrieval capabilities of IR models."
                },
                "zh": {
                    "title": "å·¥å…·æ£€ç´¢ï¼šæå‡LLMsçš„å®ç”¨èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å·¥å…·å­¦ä¹ å¦‚ä½•å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿä½œä¸ºä»£ç†è§£å†³å®é™…ä»»åŠ¡ã€‚ç”±äºå·¥å…·ä½¿ç”¨çš„LLMså…·æœ‰æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå› æ­¤é‡‡ç”¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ¨¡å‹ä»å¤§é‡å·¥å…·é›†ä¸­é€‰æ‹©æœ‰ç”¨å·¥å…·æ˜¯å…³é”®çš„åˆæ­¥æ­¥éª¤ã€‚æˆ‘ä»¬æå‡ºäº†ToolRetï¼Œä¸€ä¸ªåŒ…å«7.6kå¤šæ ·åŒ–æ£€ç´¢ä»»åŠ¡å’Œ43kå·¥å…·çš„å¼‚æ„å·¥å…·æ£€ç´¢åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°IRæ¨¡å‹åœ¨å·¥å…·æ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨ä¼ ç»ŸIRåŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹ï¼Œåœ¨ToolRetä¸Šçš„è¡¨ç°å´å¾ˆå·®ï¼Œè¿™é™ä½äº†å·¥å…·ä½¿ç”¨LLMsçš„ä»»åŠ¡é€šè¿‡ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01729",
            "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
            "url": "https://huggingface.co/papers/2503.01729",
            "abstract": "Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. While federated learning enables decentralized, privacy-preserving training, its application to robotic manipulation remains largely unexplored. We introduce FLAME (Federated Learning Across Manipulation Environments), the first benchmark designed for federated learning in robotic manipulation. FLAME consists of: (i) a set of large-scale datasets of over 160,000 expert demonstrations of multiple manipulation tasks, collected across a wide range of simulated environments; (ii) a training and evaluation framework for robotic policy learning in a federated setting. We evaluate standard federated learning algorithms in FLAME, showing their potential for distributed policy learning and highlighting key challenges. Our benchmark establishes a foundation for scalable, adaptive, and privacy-aware robotic learning.",
            "score": 2,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "893358a382c79250",
            "authors": [
                "Santiago Bou Betran",
                "Alberta Longhini",
                "Miguel Vasco",
                "Yuchong Zhang",
                "Danica Kragic"
            ],
            "affiliations": [
                "KTH Royal Institute of Technology, Stockholm, Sweden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01729.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¤ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FLAME - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. FLAME Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 160 000 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° FLAME, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Empowering Robots with Federated Learning for Privacy and Scalability",
                    "desc": "This paper presents FLAME, a benchmark for applying federated learning to robotic manipulation tasks. It addresses the limitations of centralized training by allowing robots to learn from diverse datasets while preserving data privacy. FLAME includes over 160,000 expert demonstrations from various simulated environments, facilitating decentralized training. The study evaluates existing federated learning algorithms, demonstrating their effectiveness and identifying challenges in distributed policy learning for robotics."
                },
                "zh": {
                    "title": "è”é‚¦å­¦ä¹ åŠ©åŠ›æœºå™¨äººæ“æ§çš„æœªæ¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†FLAMEï¼ˆè·¨æ“æ§ç¯å¢ƒçš„è”é‚¦å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæœºå™¨äººæ“æ§è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚FLAMEåŒ…å«è¶…è¿‡160,000ä¸ªä¸“å®¶æ¼”ç¤ºçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§æ“æ§ä»»åŠ¡ï¼Œæ”¶é›†è‡ªå¤šç§æ¨¡æ‹Ÿç¯å¢ƒã€‚é€šè¿‡åœ¨FLAMEä¸­è¯„ä¼°æ ‡å‡†çš„è”é‚¦å­¦ä¹ ç®—æ³•ï¼Œè®ºæ–‡å±•ç¤ºäº†åˆ†å¸ƒå¼ç­–ç•¥å­¦ä¹ çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†å…³é”®æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†ä¸ºå¯æ‰©å±•ã€é€‚åº”æ€§å¼ºä¸”æ³¨é‡éšç§çš„æœºå™¨äººå­¦ä¹ å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01449",
            "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
            "url": "https://huggingface.co/papers/2503.01449",
            "abstract": "Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.",
            "score": 2,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "1b4593bb9d78ec53",
            "authors": [
                "Ting Zhang",
                "Chengran Yang",
                "Yindu Su",
                "Martin Weyssow",
                "Hung Nguyen",
                "Tan Bui",
                "Hong Jin Kang",
                "Yikun Li",
                "Eng Lieh Ouh",
                "Lwin Khin Shar",
                "David Lo"
            ],
            "affiliations": [
                "School of Computer Science, University of Sydney, Australia",
                "School of Computing and Information Systems, Singapore Management University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01449.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#plp",
                    "#training",
                    "#security",
                    "#benchmark",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "LLM Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ (SVD). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Python, Java Ğ¸ JavaScript, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² SVD, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SVD Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ LLM, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking LLMs for Software Vulnerability Detection",
                    "desc": "This paper investigates the effectiveness of large language models (LLMs) in detecting software vulnerabilities, an important area for software security. It highlights the lack of comprehensive studies on LLMs' capabilities across various programming languages, as most existing research focuses on C/C++ datasets. The authors present an empirical study using a dataset of over 44,000 vulnerable functions from Python, Java, and JavaScript, evaluating five open-source LLMs with different strategies like prompt engineering and instruction tuning. The findings reveal that while LLMs show promise, software vulnerability detection remains a challenging task, providing valuable insights for future improvements in this field."
                },
                "zh": {
                    "title": "æå‡è½¯ä»¶å®‰å…¨ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¼æ´æ£€æµ‹ä¸­çš„åº”ç”¨",
                    "desc": "æœ€è¿‘ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„è¿›å±•ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œè§£å†³äº†è®¸å¤šé•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹å¯¹LLMsåœ¨è½¯ä»¶æ¼æ´æ£€æµ‹ï¼ˆSVDï¼‰èƒ½åŠ›çš„å…¨é¢ç ”ç©¶ï¼Œè¿™å¯¹è½¯ä»¶å®‰å…¨è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨C/C++æ•°æ®é›†è¯„ä¼°LLMsï¼Œé€šå¸¸åªæ¢è®¨äº†æç¤ºå·¥ç¨‹ã€æŒ‡ä»¤è°ƒä¼˜å’Œåºåˆ—åˆ†ç±»å¾®è°ƒä¸­çš„ä¸€ä¸¤ç§ç­–ç•¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œè¯„ä¼°LLMsåœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¸­æ£€æµ‹æ¼æ´çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01378",
            "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
            "url": "https://huggingface.co/papers/2503.01378",
            "abstract": "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io",
            "score": 2,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "8a4aab69ce92453d",
            "authors": [
                "Artem Lykov",
                "Valerii Serpiva",
                "Muhammad Haris Khan",
                "Oleg Sautenkov",
                "Artyom Myshlyaev",
                "Grik Tadevosyan",
                "Yasheerah Yaqoot",
                "Dzmitry Tsetserukou"
            ],
            "affiliations": [
                "Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01378.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸš",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ´Ñ€Ğ¾Ğ½Ñ‹: ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ‘ĞŸĞ›Ğ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CognitiveDrone - Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ—Ñ€ĞµĞ½Ğ¸Ğµ-Ğ¯Ğ·Ñ‹Ğº-Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ (VLA) Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ»ĞµÑ‚Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ² (Ğ‘ĞŸĞ›Ğ). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 8000 ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ CognitiveDrone-R1 Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ—Ñ€ĞµĞ½Ğ¸Ñ-Ğ¯Ğ·Ñ‹ĞºĞ° (VLM) Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ CognitiveDrone-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² 77.2%, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° 30% Ğ»ÑƒÑ‡ÑˆĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "CognitiveDrone: Elevating UAV Intelligence with Vision-Language-Action!",
                    "desc": "This paper presents CognitiveDrone, a new Vision-Language-Action (VLA) model designed for complex tasks performed by Unmanned Aerial Vehicles (UAVs). It is trained on a dataset of over 8,000 simulated flight paths focusing on Human Recognition, Symbol Understanding, and Reasoning. The model can generate real-time 4D action commands from visual inputs and text instructions, with an enhanced version, CognitiveDrone-R1, that includes a Vision-Language Model (VLM) reasoning module for better task management. Experimental results show significant performance improvements, with CognitiveDrone-R1 achieving a 77.2% success rate, highlighting the importance of advanced reasoning in UAV operations."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ— äººæœºçš„è®¤çŸ¥é£è¡Œæ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCognitiveDroneçš„æ–°å‹è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ï¼Œä¸“ä¸ºå¤æ‚çš„æ— äººæœºä»»åŠ¡è®¾è®¡ï¼Œå…·å¤‡é«˜çº§è®¤çŸ¥èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡8000æ¡æ¨¡æ‹Ÿé£è¡Œè½¨è¿¹çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–äººç±»è¯†åˆ«ã€ç¬¦å·ç†è§£å’Œæ¨ç†ä¸‰ä¸ªå…³é”®ç±»åˆ«ã€‚CognitiveDrone-R1é€šè¿‡é›†æˆé¢å¤–çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨ç†æ¨¡å—ï¼Œè¿›ä¸€æ­¥æå‡åœ¨å¤æ‚åœºæ™¯ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCognitiveDroneæ¨¡å‹çš„æˆåŠŸç‡è¾¾åˆ°59.6%ï¼Œè€ŒCognitiveDrone-R1çš„æˆåŠŸç‡æ›´æ˜¯é«˜è¾¾77.2%ï¼Œè¯æ˜äº†å°†é«˜çº§æ¨ç†èƒ½åŠ›èå…¥æ— äººæœºæ§åˆ¶ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.00502",
            "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions",
            "url": "https://huggingface.co/papers/2503.00502",
            "abstract": "Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, but the conflict between slow inference speed and the need for real-time decision-making challenges practical deployment. To address these issues, this paper introduces a parallel Actor-Reasoner framework designed to enable explicit bidirectional AV-HV interactions across multiple scenarios. First, by facilitating interactions between the LLM-driven Reasoner and heterogeneous simulated HVs during training, an interaction memory database, referred to as the Actor, is established. Then, by introducing the memory partition module and the two-layer memory retrieval module, the Actor's ability to handle heterogeneous HVs is significantly enhanced. Ablation studies and comparisons with other decision-making methods demonstrate that the proposed Actor-Reasoner framework significantly improves safety and efficiency. Finally, with the combination of the external Human-Machine Interface (eHMI) information derived from Reasoner's reasoning and the feasible action solutions retrieved from the Actor, the effectiveness of the proposed Actor-Reasoner is confirmed in multi-scenario field interactions. Our code is available at https://github.com/FanGShiYuu/Actor-Reasoner.",
            "score": 2,
            "issue_id": 2555,
            "pub_date": "2025-03-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 1",
                "zh": "3æœˆ1æ—¥"
            },
            "hash": "d184a5cae68093d5",
            "authors": [
                "Shiyu Fang",
                "Jiaqi Liu",
                "Chengkai Xu",
                "Chen Lv",
                "Peng Hang",
                "Jian Sun"
            ],
            "affiliations": [
                "College of Transportation, Tongji University, Shanghai 201804, China",
                "Nanyang Technological University, 639798, Singapore",
                "State Key Lab of Intelligent Transportation System, Beijing 100088, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00502.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#robotics",
                    "#inference",
                    "#optimization",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Actor-Reasoner Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Actor-Reasoner Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing AV-HV Interactions with the Actor-Reasoner Framework",
                    "desc": "This paper presents a new framework called the Actor-Reasoner to improve interactions between Autonomous Vehicles (AVs) and Human-driven Vehicles (HVs). It leverages large language models (LLMs) to facilitate real-time communication and decision-making, addressing the challenge of slow inference speeds. The framework includes an interaction memory database, which enhances the AV's ability to understand and respond to various HV behaviors. Experimental results show that this approach significantly boosts both safety and efficiency in multi-scenario driving situations."
                },
                "zh": {
                    "title": "æå‡è‡ªåŠ¨é©¾é©¶ä¸äººç±»é©¾é©¶äº’åŠ¨çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¹¶è¡Œæ¼”å‘˜-æ¨ç†å™¨æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ˆAVï¼‰ä¸äººç±»é©¾é©¶æ±½è½¦ï¼ˆHVï¼‰ä¹‹é—´çš„äº’åŠ¨ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿ƒè¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ¨ç†å™¨ä¸ä¸åŒç±»å‹çš„æ¨¡æ‹ŸHVä¹‹é—´çš„äº’åŠ¨ï¼Œå»ºç«‹äº†ä¸€ä¸ªäº’åŠ¨è®°å¿†æ•°æ®åº“ã€‚å¼•å…¥è®°å¿†åˆ†åŒºæ¨¡å—å’ŒåŒå±‚è®°å¿†æ£€ç´¢æ¨¡å—åï¼Œæ¼”å‘˜çš„å¤„ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šåœºæ™¯äº¤äº’ä¸­æ˜¾è‘—æé«˜äº†å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01372",
            "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
            "url": "https://huggingface.co/papers/2503.01372",
            "abstract": "In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.",
            "score": 1,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "3de5be81537fa0fd",
            "authors": [
                "Joel Niklaus",
                "Jakob Merane",
                "Luka Nenadic",
                "Sina Ahmadi",
                "Yingqiang Gao",
                "Cyrill A. H. Chevalley",
                "Claude Humbel",
                "Christophe GÃ¶sken",
                "Lorenzo Tanzi",
                "Thomas LÃ¼thi",
                "Stefan Palombo",
                "Spencer Poff",
                "Boling Yang",
                "Nan Wu",
                "Matthew Guillod",
                "Robin MamiÃ©",
                "Daniel Brunner",
                "Julio Pereyra",
                "Niko Grupen"
            ],
            "affiliations": [
                "Canton of Solothurn",
                "ETH Zurich",
                "Max Planck Institute for Research on Collective Goods",
                "Swiss Federal Supreme Court",
                "University of Basel",
                "University of Geneva",
                "University of Lausanne",
                "University of Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01372.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#machine_translation"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ: Ğ˜Ğ˜ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¨Ğ²ĞµĞ¹Ñ†Ğ°Ñ€Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SwiLTra-Bench - Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ¨Ğ²ĞµĞ¹Ñ†Ğ°Ñ€Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° SwiLTra-Judge Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Legal Translation with SwiLTra-Bench and LLMs",
                    "desc": "This paper addresses the challenges of legal translation in Switzerland, where multiple languages complicate the process. It introduces SwiLTra-Bench, a benchmark dataset with over 180,000 aligned legal translation pairs to evaluate large language model (LLM) translation systems. The findings show that while advanced models perform well across various document types, specialized systems are better for translating laws but struggle with headnotes. The study also highlights the effectiveness of fine-tuning open-source language models, although they still do not match the performance of top zero-shot models like Claude-3.5-Sonnet."
                },
                "zh": {
                    "title": "ç‘å£«æ³•å¾‹ç¿»è¯‘çš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ",
                    "desc": "åœ¨ç‘å£«ï¼Œç”±äºæœ‰å››ç§å®˜æ–¹è¯­è¨€ï¼Œæ³•å¾‹ç¿»è¯‘æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹ä¾èµ–äºæ—¢æ˜¯æ³•å¾‹ä¸“å®¶åˆæ˜¯ç¿»è¯‘é«˜æ‰‹çš„ä¸“ä¸šäººå£«ï¼Œå¯¼è‡´äº†ç“¶é¢ˆï¼Œå½±å“äº†å…¬æ­£çš„æœ‰æ•ˆè·å–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SwiLTra-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡18ä¸‡å¯¹ç‘å£«æ³•å¾‹ç¿»è¯‘çš„å¤šè¯­è¨€åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¿»è¯‘ç³»ç»Ÿã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå‰æ²¿æ¨¡å‹åœ¨æ‰€æœ‰æ–‡æ¡£ç±»å‹çš„ç¿»è¯‘è¡¨ç°ä¸Šä¼˜äºå…¶ä»–ç³»ç»Ÿï¼Œè€Œä¸“é—¨çš„ç¿»è¯‘ç³»ç»Ÿåœ¨æ³•å¾‹æ–‡æœ¬ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤´æ³¨æ–¹é¢è¡¨ç°ä¸ä½³ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-05.html",
    "link_next": "2025-03-07.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "05.03",
        "en": "03/05",
        "zh": "3æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "07.03",
        "en": "03/07",
        "zh": "3æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 4,
        "#benchmark": 8,
        "#agents": 1,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 3,
        "#architecture": 1,
        "#healthcare": 3,
        "#training": 7,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 2,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼Œä½†å¼€æºçš„å¤šè¯­è¨€LLMsä»ç„¶ç¨€ç¼ºï¼Œç°æœ‰æ¨¡å‹é€šå¸¸è¯­è¨€è¦†ç›–æœ‰é™ã€‚è¿™äº›æ¨¡å‹é€šå¸¸ä¼˜å…ˆè€ƒè™‘èµ„æºä¸°å¯Œçš„è¯­è¨€ï¼Œè€Œå¿½ç•¥äº†å¹¿æ³›ä½¿ç”¨ä½†èµ„æºåŒ®ä¹çš„è¯­è¨€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬ä»‹ç»äº†Babelï¼Œä¸€ä¸ªå¼€æ”¾çš„å¤šè¯­è¨€LLMï¼Œæ¶µç›–äº†æŒ‰ä½¿ç”¨äººæ•°æ’åå‰25çš„è¯­è¨€ï¼Œæ”¯æŒè¶…è¿‡90%çš„å…¨çƒäººå£ï¼Œå¹¶åŒ…æ‹¬è®¸å¤šå…¶ä»–å¼€æ”¾å¤šè¯­è¨€LLMså¿½ç•¥çš„è¯­è¨€ã€‚ä¸ä¼ ç»Ÿçš„ç»§ç»­é¢„è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒBabelé€šè¿‡ä¸€ç§å±‚æ‰©å±•æŠ€æœ¯å¢åŠ å…¶å‚æ•°æ•°é‡ï¼Œä»è€Œæé«˜äº†Babelçš„æ€§èƒ½ä¸Šé™ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå˜ä½“ï¼šBabel-9Bï¼Œç”¨äºé«˜æ•ˆæ¨ç†å’Œå¾®è°ƒï¼Œä»¥åŠBabel-83Bï¼Œä¸ºå¼€æ”¾å¤šè¯­è¨€LLMsè®¾å®šäº†æ–°æ ‡å‡†ã€‚å¹¿æ³›çš„å¤šè¯­è¨€ä»»åŠ¡è¯„ä¼°è¯æ˜äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¼€æºç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼ŒBabelå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼ŒBabel-9B-Chatåœ¨10Bå¤§å°çš„LLMsä¸­é¢†å…ˆï¼ŒBabel-83B-Chatåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è®¾å®šäº†æ–°æ ‡å‡†ï¼Œè¾¾åˆ°äº†å•†ä¸šæ¨¡å‹çš„æ°´å¹³ã€‚",
        "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
        "pinyin": "DÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) chÃ¨dÇ gÇibiÃ nle zÃ¬rÃ¡n yÇ”yÃ¡n chÇ”lÇ (NLP), dÃ n kÄiyuÃ¡n de duÅyÇ”yÃ¡n LLMs rÃ©ngrÃ¡n xÄ«quÄ“, xiÃ nyÇ’u mÃ³xÃ­ng tÅngchÃ¡ng yÇ”yÃ¡n fÃºgÃ i yÇ’uxiÃ n. ZhÃ¨xiÄ“ mÃ³xÃ­ng tÅngchÃ¡ng yÅuxiÄn kÇolÇœ zÄ«yuÃ¡n fÄ“ngfÃ¹ de yÇ”yÃ¡n, Ã©r hÅ«lÃ¼e le guÇngfÃ n shÇyÃ²ng dÃ n zÄ«yuÃ¡n kuÃ¬fÃ¡ de yÇ”yÃ¡n. WÃ¨ile jiÄ›juÃ© zhÃ¨ yÄ« chÄjÃ¹, wÇ’men jiÃ¨shÃ o le Babel, yÄ«gÃ¨ kÄifÃ ng de duÅyÇ”yÃ¡n LLM, hÃ nhuÃ²le Ã n shÇyÃ²ng rÃ©nshÃ¹ pÃ¡imÃ­ng qiÃ¡n 25 de yÇ”yÃ¡n, zhÄ«chÃ­ chÄoguÃ² 90% de quÃ¡nqiÃº rÃ©nkÇ’u, bÃ¬ng bÄokuÃ² xÇ”duÅ qÃ­tÄ kÄifÃ ng duÅyÇ”yÃ¡n LLMs hÅ«lÃ¼e de yÇ”yÃ¡n. YÇ” chuÃ¡ntÇ’ng de jÃ¬xÃ¹ yÃ¹xÃ¹n fÄngfÇ bÃ¹tÃ³ng, Babel tÅngguÃ² yÄ«zhÇ’ng cÃ©ng kuÃ²zhÇn jÃ¬shÃ¹ zÄ“ngjiÄ qÃ­ cÄnshÃ¹ shÃ¹liÃ ng, dÃ ngrÃ¡n tÃ­gÄole Babel de xÃ¬ngnÃ©ng shÃ ngxiÃ n. WÇ’men yÇnrÃ¹le liÇnggÃ¨ biÃ ntÇ: Babel-9B, yÃ²ngyÃº gÄoxiÃ o tuÄ«lÇ hÃ© wÄ“itiÃ¡o, yÇjiÇ Babel-83B, wÃ¨i kÄifÃ ng duÅyÇ”yÃ¡n LLMs shÃ¨dÃ¬ngle xÄ«n biÄozhÇ”n. GuÇngfÃ n de duÅyÇ”yÃ¡n rÃ¨nwÃ¹ pÃ­nggÅ« zhÃ¨ngmÃ­ngle qÃ­ yÅubiÃ¨ de xÃ¬ngnÃ©ng. CÇwÃ i, shÇyÃ²ng kÄiyuÃ¡n jiÃ nshÇ wÄ“itiÃ¡o shÃ¹jÃºjÃ­, Babel quÃ¨dÃ©le xiÇnzhÃ¹ de xÃ¬ngnÃ©ng, Babel-9B-Chat zÃ i 10B dÃ xÃ¬ng de LLMs zhÅng lÇngxiÄn, Babel-83B-Chat zÃ i duÅyÇ”yÃ¡n rÃ¨nwÃ¹ zhÅng shÃ¨dÃ¬ngle xÄ«n biÄozhÇ”n, dÃ¡le shÄngyÃ¨ mÃ³xÃ­ng de shuÇpÃ­ng.",
        "vocab": "[\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"å½»åº•\", \"pinyin\": \"chÃ¨dÇ\", \"trans\": \"thoroughly\"},\n    {\"word\": \"è‡ªç„¶è¯­è¨€å¤„ç†\", \"pinyin\": \"zÃ¬rÃ¡n yÇ”yÃ¡n chÇ”lÇ\", \"trans\": \"Natural Language Processing\"},\n    {\"word\": \"ç¨€ç¼º\", \"pinyin\": \"xÄ«quÄ“\", \"trans\": \"scarce\"},\n    {\"word\": \"è¦†ç›–\", \"pinyin\": \"fÃ¹gÃ i\", \"trans\": \"cover\"},\n    {\"word\": \"æœ‰é™\", \"pinyin\": \"yÇ’uxiÃ n\", \"trans\": \"limited\"},\n    {\"word\": \"ä¼˜å…ˆ\", \"pinyin\": \"yÅuxiÄn\", \"trans\": \"prioritize\"},\n    {\"word\": \"èµ„æº\", \"pinyin\": \"zÄ«yuÃ¡n\", \"trans\": \"resources\"},\n    {\"word\": \"ä¸°å¯Œ\", \"pinyin\": \"fÄ“ngfÃ¹\", \"trans\": \"abundant\"},\n    {\"word\": \"åŒ®ä¹\", \"pinyin\": \"kuÃ¬fÃ¡\", \"trans\": \"scarce\"},\n    {\"word\": \"å·®è·\", \"pinyin\": \"chÄjÃ¹\", \"trans\": \"gap\"},\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"æ¶µç›–\", \"pinyin\": \"hÃ¡ngÃ i\", \"trans\": \"cover\"},\n    {\"word\": \"æŒ‰\", \"pinyin\": \"Ã n\", \"trans\": \"according to\"},\n    {\"word\": \"æ’å\", \"pinyin\": \"pÃ¡imÃ­ng\", \"trans\": \"ranking\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ«chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"å…¨çƒ\", \"pinyin\": \"quÃ¡nqiÃº\", \"trans\": \"global\"},\n    {\"word\": \"äººå£\", \"pinyin\": \"rÃ©nkÇ’u\", \"trans\": \"population\"},\n    {\"word\": \"ç»§ç»­\", \"pinyin\": \"jÃ¬xÃ¹\", \"trans\": \"continue\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹nliÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"å±‚\", \"pinyin\": \"cÃ©ng\", \"trans\": \"layer\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ²zhÇn\", \"trans\": \"expand\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬shÃ¹\", \"trans\": \"technology\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄnshÇ”\", \"trans\": \"parameters\"},\n    {\"word\": \"æ•°é‡\", \"pinyin\": \"shÃ¹liÃ ng\", \"trans\": \"quantity\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ngnÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ä¸Šé™\", \"pinyin\": \"shÃ ngxiÃ n\", \"trans\": \"upper limit\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇnrÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"å˜ä½“\", \"pinyin\": \"biÃ ntÇ\", \"trans\": \"variants\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄoxiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“itiÃ¡o\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"è®¾å®š\", \"pinyin\": \"shÃ¨dÃ¬ng\", \"trans\": \"set\"},\n    {\"word\": \"æ ‡å‡†\", \"pinyin\": \"biÄozhÇ”n\", \"trans\": \"standard\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"è¯æ˜\", \"pinyin\": \"zhÃ¨ngmÃ­ng\", \"trans\": \"prove\"},\n    {\"word\": \"ä¼˜è¶Š\", \"pinyin\": \"yÅuyuÃ¨\", \"trans\": \"superior\"},\n    {\"word\": \"ç›‘ç£\", \"pinyin\": \"jiÃ ndÅ«\", \"trans\": \"supervised\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇnzhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"é¢†å…ˆ\", \"pinyin\": \"lÇngxiÄn\", \"trans\": \"lead\"},\n    {\"word\": \"å•†ä¸š\", \"pinyin\": \"shÄngyÃ¨\", \"trans\": \"commercial\"},\n    {\"word\": \"æ°´å¹³\", \"pinyin\": \"shuÇpÃ­ng\", \"trans\": \"level\"}\n]",
        "trans": "Large language models (LLMs) have revolutionized natural language processing (NLP), but open-source multilingual LLMs remain scarce, with existing models often having limited language coverage. These models typically prioritize resource-rich languages while neglecting widely used but resource-scarce languages. To address this gap, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supporting over 90% of the global population and including many languages overlooked by other open multilingual LLMs. Unlike traditional continued pre-training methods, Babel enhances its parameter count through a layer expansion technique, raising Babel's performance ceiling. We introduce two variants: Babel-9B for efficient inference and fine-tuning, and Babel-83B, setting a new standard for open multilingual LLMs. Extensive multilingual task evaluations demonstrate its superior performance. Additionally, using open-source supervised fine-tuning datasets, Babel achieves significant performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting new standards in multilingual tasks, reaching the level of commercial models.",
        "update_ts": "2025-03-06 09:11"
    }
}