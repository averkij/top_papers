{
    "date": {
        "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 28",
        "zh": "10æœˆ28æ—¥"
    },
    "time_utc": "2024-10-28 05:14",
    "weekday": 0,
    "issue_id": 303,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.17856",
            "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
            "url": "https://huggingface.co/papers/2410.17856",
            "abstract": "Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planning. A common approach to address this problem is through the use of hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language and imagined observations. However, language often fails to effectively convey spatial information, while generating future images with sufficient accuracy remains challenging. To address these limitations, we propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from both past and present observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, with real-time object tracking provided by SAM-2. Our method unlocks the full potential of VLMs visual-language reasoning abilities, enabling them to solve complex creative tasks, especially those heavily reliant on spatial understanding. Experiments in Minecraft demonstrate that our approach allows agents to accomplish previously unattainable tasks, highlighting the effectiveness of visual-temporal context prompting in embodied decision-making. Codes and demos will be available on the project page: https://craftjarvis.github.io/ROCKET-1.",
            "score": 10,
            "issue_id": 302,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "0a477d46d035f1d4",
            "data": {
                "categories": [
                    "#agents",
                    "#cv",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ VLM Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ VLM Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑÑ€ĞµĞ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Minecraft Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ¸Ğ¶Ğ¸Ğ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "è§£å†³LLMå¹»è§‰çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å¹»è§‰çš„ä¸€ä¸ªåŠ é‡å› ç´ æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„çŸ¥è¯†ä¸ä¸€è‡´ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰çš„å¾®è°ƒæ•°æ®æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºPrereq-Tuneï¼Œæ—¨åœ¨å‡å°‘è¿™ç§çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚Prereq-Tuneé€šè¿‡å¼•å…¥é¢å¤–çš„å…ˆå†³å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹åœ¨å¾®è°ƒä¹‹å‰å…ˆå­¦ä¹ å¿…è¦çš„çŸ¥è¯†ï¼Œä»è€Œä¸“æ³¨äºä»»åŠ¡æŠ€èƒ½çš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒPrereq-Tuneåœ¨æé«˜LLMçš„äº‹å®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºçŸ¥è¯†æ§åˆ¶ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19008",
            "title": "Teach Multimodal LLMs to Comprehend Electrocardiographic Images",
            "url": "https://huggingface.co/papers/2410.19008",
            "abstract": "The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may not be readily available in resource-limited settings where only printed or digital ECG images are accessible. Recent advancements in multimodal large language models (MLLMs) present promising opportunities for addressing these challenges. However, the application of MLLMs to ECG image interpretation remains challenging due to the lack of instruction tuning datasets and well-established ECG image benchmarks for quantitative evaluation. To address these challenges, we introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset of over one million samples, covering a wide range of ECG-related tasks from diverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for ECG image comprehension. In addition, we curate ECGBench, a new evaluation benchmark covering four key ECG image interpretation tasks across nine different datasets. Our experiments show that PULSE sets a new state-of-the-art, outperforming general MLLMs with an average accuracy improvement of 15% to 30%. This work highlights the potential of PULSE to enhance ECG interpretation in clinical practice.",
            "score": 7,
            "issue_id": 300,
            "pub_date": "2024-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "85936de603f8cc7a",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#medicine"
                ],
                "emoji": "â¤ï¸",
                "ru": {
                    "title": "PULSE: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ­ĞšĞ“ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ­ĞšĞ“ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ECGInstruct - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ­ĞšĞ“ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ PULSE - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ MLLM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ­ĞšĞ“. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ECGBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ­ĞšĞ“. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PULSE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ MLLM Ğ½Ğ° 15-30% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ­ĞšĞ“ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "è§£å†³LLMå¹»è§‰çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å¹»è§‰çš„ä¸€ä¸ªåŠ é‡å› ç´ æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„çŸ¥è¯†ä¸ä¸€è‡´ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰çš„å¾®è°ƒæ•°æ®æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºPrereq-Tuneï¼Œæ—¨åœ¨å‡å°‘è¿™ç§çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚Prereq-Tuneé€šè¿‡å¼•å…¥é¢å¤–çš„å…ˆå†³å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹åœ¨å¾®è°ƒä¹‹å‰å…ˆå­¦ä¹ å¿…è¦çš„çŸ¥è¯†ï¼Œä»è€Œä¸“æ³¨äºä»»åŠ¡æŠ€èƒ½çš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒPrereq-Tuneåœ¨æé«˜LLMçš„äº‹å®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºçŸ¥è¯†æ§åˆ¶ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19168",
            "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark",
            "url": "https://huggingface.co/papers/2410.19168",
            "abstract": "The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.",
            "score": 3,
            "issue_id": 300,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "7a747d9a9b0717cc",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "MMAU: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "MMAU - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 10 Ñ‚Ñ‹ÑÑÑ‡ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑ‡ÑŒ, Ğ·Ğ²ÑƒĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ. MMAU Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ 27 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ”Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Gemini Pro v1.5 Ğ¸ Qwen2-Audio, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ Ğ¾ĞºĞ¾Ğ»Ğ¾ 53%, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "è§£å†³LLMå¹»è§‰çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å¹»è§‰çš„ä¸€ä¸ªåŠ é‡å› ç´ æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„çŸ¥è¯†ä¸ä¸€è‡´ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰çš„å¾®è°ƒæ•°æ®æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºPrereq-Tuneï¼Œæ—¨åœ¨å‡å°‘è¿™ç§çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚Prereq-Tuneé€šè¿‡å¼•å…¥é¢å¤–çš„å…ˆå†³å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹åœ¨å¾®è°ƒä¹‹å‰å…ˆå­¦ä¹ å¿…è¦çš„çŸ¥è¯†ï¼Œä»è€Œä¸“æ³¨äºä»»åŠ¡æŠ€èƒ½çš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒPrereq-Tuneåœ¨æé«˜LLMçš„äº‹å®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºçŸ¥è¯†æ§åˆ¶ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19355",
            "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality",
            "url": "https://huggingface.co/papers/2410.19355",
            "abstract": "In this paper, we present \\textit{FasterCache}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to the loss of subtle variations. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\\eg 1.67times speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality.",
            "score": 2,
            "issue_id": 301,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 25",
                "zh": "10æœˆ25æ—¥"
            },
            "hash": "2de0b0700140bc7e",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "FasterCache: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "FasterCache - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºÑÑˆ-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². FasterCache Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ CFG-ĞºÑÑˆ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ² 1,67 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Vchitect-2.0) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "è§£å†³LLMå¹»è§‰çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å¹»è§‰çš„ä¸€ä¸ªåŠ é‡å› ç´ æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„çŸ¥è¯†ä¸ä¸€è‡´ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰çš„å¾®è°ƒæ•°æ®æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºPrereq-Tuneï¼Œæ—¨åœ¨å‡å°‘è¿™ç§çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚Prereq-Tuneé€šè¿‡å¼•å…¥é¢å¤–çš„å…ˆå†³å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹åœ¨å¾®è°ƒä¹‹å‰å…ˆå­¦ä¹ å¿…è¦çš„çŸ¥è¯†ï¼Œä»è€Œä¸“æ³¨äºä»»åŠ¡æŠ€èƒ½çš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒPrereq-Tuneåœ¨æé«˜LLMçš„äº‹å®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºçŸ¥è¯†æ§åˆ¶ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19290",
            "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning",
            "url": "https://huggingface.co/papers/2410.19290",
            "abstract": "Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called Prereq-Tune to address this knowledge inconsistency and reduce hallucinations. Fundamentally, Prereq-Tune disentangles the learning of skills and knowledge, so the model learns only the task skills without being impacted by the knowledge inconsistency. To achieve this, Prereq-Tune introduces an additional prerequisite learning stage to learn the necessary knowledge for SFT, allowing subsequent SFT to focus only on task skills. Prereq-Tune can also be combined with fictitious synthetic data to enhance the grounding of LLM outputs to their internal knowledge. Experiments show that Prereq-Tune outperforms existing baselines in improving LLM's factuality across short QA and long-form generation tasks. It also opens new possibilities for knowledge-controlled generation in LLMs. Our code is available at https://github.com/UCSB-NLP-Chang/Prereq_tune.git.",
            "score": 1,
            "issue_id": 302,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 25",
                "zh": "10æœˆ25æ—¥"
            },
            "hash": "a11e3c6587db25fc",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Prereq-Tune: Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Prereq-Tune, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼, Ğ²Ğ²Ğ¾Ğ´Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. Prereq-Tune Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞµĞµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Prereq-Tune Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "è§£å†³LLMå¹»è§‰çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å¹»è§‰çš„ä¸€ä¸ªåŠ é‡å› ç´ æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„çŸ¥è¯†ä¸ä¸€è‡´ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰çš„å¾®è°ƒæ•°æ®æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºPrereq-Tuneï¼Œæ—¨åœ¨å‡å°‘è¿™ç§çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚Prereq-Tuneé€šè¿‡å¼•å…¥é¢å¤–çš„å…ˆå†³å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹åœ¨å¾®è°ƒä¹‹å‰å…ˆå­¦ä¹ å¿…è¦çš„çŸ¥è¯†ï¼Œä»è€Œä¸“æ³¨äºä»»åŠ¡æŠ€èƒ½çš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒPrereq-Tuneåœ¨æé«˜LLMçš„äº‹å®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºçŸ¥è¯†æ§åˆ¶ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18076",
            "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
            "url": "https://huggingface.co/papers/2410.18076",
            "abstract": "Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.",
            "score": 1,
            "issue_id": 300,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "4c364a800e98d62f",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² RL Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SUPE Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. SUPE ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° (VAE), Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ„Ñ„-Ğ¿Ğ¾Ğ»Ğ¸ÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. SUPE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "è§£å†³LLMå¹»è§‰çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å¹»è§‰çš„ä¸€ä¸ªåŠ é‡å› ç´ æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„çŸ¥è¯†ä¸ä¸€è‡´ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰çš„å¾®è°ƒæ•°æ®æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºPrereq-Tuneï¼Œæ—¨åœ¨å‡å°‘è¿™ç§çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚Prereq-Tuneé€šè¿‡å¼•å…¥é¢å¤–çš„å…ˆå†³å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹åœ¨å¾®è°ƒä¹‹å‰å…ˆå­¦ä¹ å¿…è¦çš„çŸ¥è¯†ï¼Œä»è€Œä¸“æ³¨äºä»»åŠ¡æŠ€èƒ½çš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒPrereq-Tuneåœ¨æé«˜LLMçš„äº‹å®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºçŸ¥è¯†æ§åˆ¶ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18558",
            "title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data",
            "url": "https://huggingface.co/papers/2410.18558",
            "abstract": "Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through rigorous quality filtering and deduplication. We also propose a synthetic instruction generation method based on open-source VLMs, using detailed image annotations and diverse question generation. Using this data, we trained a 2-billion-parameter VLM, Aquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of similar scale. This demonstrates that expanding instruction data and generating synthetic data can significantly improve the performance of open-source models.",
            "score": 1,
            "issue_id": 300,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "18e760a965f56e6d",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Infinity-MM - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 40 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ 2-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Aquila-VL-2B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆÑƒÑ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "è§£å†³LLMå¹»è§‰çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å¹»è§‰çš„ä¸€ä¸ªåŠ é‡å› ç´ æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„çŸ¥è¯†ä¸ä¸€è‡´ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰çš„å¾®è°ƒæ•°æ®æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºPrereq-Tuneï¼Œæ—¨åœ¨å‡å°‘è¿™ç§çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚Prereq-Tuneé€šè¿‡å¼•å…¥é¢å¤–çš„å…ˆå†³å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹åœ¨å¾®è°ƒä¹‹å‰å…ˆå­¦ä¹ å¿…è¦çš„çŸ¥è¯†ï¼Œä»è€Œä¸“æ³¨äºä»»åŠ¡æŠ€èƒ½çš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒPrereq-Tuneåœ¨æé«˜LLMçš„äº‹å®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºçŸ¥è¯†æ§åˆ¶ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19133",
            "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback",
            "url": "https://huggingface.co/papers/2410.19133",
            "abstract": "Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations as they are more consistent, cheaper, and scale better than human annotation; however, they are also prone to biases and errors. In this work, we introduce a routing framework that combines inputs from humans and LMs to achieve better annotation quality, while reducing the total cost of human annotation. The crux of our approach is to identify preference instances that will benefit from human annotations. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we train a performance prediction model to predict a reward model's performance on an arbitrary combination of human and LM annotations and employ a routing strategy that selects a combination that maximizes predicted performance. We train the performance prediction model on MultiPref, a new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of LM and direct human preferences using our routing framework achieves better reward model performance compared to using either one exclusively. We simulate selective human preference collection on three other datasets and show that our method generalizes well to all three. We analyze features from the routing model to identify characteristics of instances that can benefit from human feedback, e.g., prompts with a moderate safety concern or moderate intent complexity. We release the dataset, annotation platform, and source code used in this study to foster more efficient and accurate preference collection in the future.",
            "score": 0,
            "issue_id": 301,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "f6e6f7e5b9e467fe",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#data",
                    "#optimization"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "è§£å†³LLMå¹»è§‰çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å¹»è§‰çš„ä¸€ä¸ªåŠ é‡å› ç´ æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„çŸ¥è¯†ä¸ä¸€è‡´ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰çš„å¾®è°ƒæ•°æ®æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºPrereq-Tuneï¼Œæ—¨åœ¨å‡å°‘è¿™ç§çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚Prereq-Tuneé€šè¿‡å¼•å…¥é¢å¤–çš„å…ˆå†³å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹åœ¨å¾®è°ƒä¹‹å‰å…ˆå­¦ä¹ å¿…è¦çš„çŸ¥è¯†ï¼Œä»è€Œä¸“æ³¨äºä»»åŠ¡æŠ€èƒ½çš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒPrereq-Tuneåœ¨æé«˜LLMçš„äº‹å®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºçŸ¥è¯†æ§åˆ¶ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-10-25.html",
    "link_next": "2024-10-29.html",
    "short_date_prev": {
        "ru": "25.10",
        "en": "10/25",
        "zh": "10æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "29.10",
        "en": "10/29",
        "zh": "10æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 1,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å¯¹æ¯”æŸå¤±çš„è®¡ç®—ç­–ç•¥ã€‚ä¼ ç»Ÿæ–¹æ³•å—é™äºGPUå†…å­˜æ¶ˆè€—çš„å¢åŠ ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå—çš„è®¡ç®—ç­–ç•¥ï¼Œé¿å…äº†ç›¸ä¼¼çŸ©é˜µçš„å®Œå…¨å®ä¾‹åŒ–ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜å¼•å…¥äº†å¤šçº§å—ç­–ç•¥ï¼Œåˆ©ç”¨åˆ†å¸ƒå¼ç³»ç»Ÿçš„å±‚æ¬¡ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—æ‰©å¤§æ‰¹é‡å¤§å°ï¼Œå¹¶ä¿æŒå‡†ç¡®æ€§ã€‚",
        "title": "Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng duÃ¬bÇ sÇ”nshÄ« de jÃ¬suÃ n cÃ¨lÃ¼Ã¨. ChuÃ¡ntÇ’ng fÄngfÇ shÃ²uxiÃ n yÃº GPU nÃ¨icÃ¹n xiÄohÃ o de zÄ“ngjiÄ. ZuÃ²zhÄ› tÃ­chÅ« le yÄ« zhÇ’ng jÄ«yÃº kuÃ i de jÃ¬suÃ n cÃ¨lÃ¼Ã¨, bÃ¬miÇn le xiÄngsÃ¬ jÇ”zhÃ¨n de wÃ¡nquÃ¡n shÃ­lÃ¬huÃ . CÇwÃ i, tÄmen hÃ¡i yÇnrÃ¹ le duÅ jÃ­ kuÃ i cÃ¨lÃ¼Ã¨, lÃ¬yÃ²ng fÄ“nbÃ¹shÃ¬ xÃ¬tÇ’ng de cÃ©ngcÃ¬ jiÃ©gÃ²u. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, gÄi fÄngfÇ kÄ›yÇ xiÇnzhÃ¹ kuÃ²dÃ  pÄ«liÃ ng dÃ xiao, bÃ¬ng bÇochÃ­ zhÇ”nquÃ¨xÃ¬ng.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"å¯¹æ¯”æŸå¤±\", \"pinyin\": \"duÃ¬bÇ sÇ”nshÄ«\", \"trans\": \"contrastive loss\"},\n    {\"word\": \"è®¡ç®—ç­–ç•¥\", \"pinyin\": \"jÃ¬suÃ n cÃ¨lÃ¼Ã¨\", \"trans\": \"computational strategy\"},\n    {\"word\": \"ä¼ ç»Ÿæ–¹æ³•\", \"pinyin\": \"chuÃ¡ntÇ’ng fÄngfÇ\", \"trans\": \"traditional method\"},\n    {\"word\": \"å—é™äº\", \"pinyin\": \"shÃ²u xiÃ n yÃº\", \"trans\": \"limited by\"},\n    {\"word\": \"GPUå†…å­˜æ¶ˆè€—\", \"pinyin\": \"GPU nÃ¨icÃºn xiÄohÃ o\", \"trans\": \"GPU memory consumption\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"åŸºäºå—çš„è®¡ç®—ç­–ç•¥\", \"pinyin\": \"jÄ«yÃº kuÃ i de jÃ¬suÃ n cÃ¨lÃ¼Ã¨\", \"trans\": \"block-based computational strategy\"},\n    {\"word\": \"é¿å…\", \"pinyin\": \"bÃ¬miÇn\", \"trans\": \"avoid\"},\n    {\"word\": \"ç›¸ä¼¼çŸ©é˜µ\", \"pinyin\": \"xiÄngsÃ¬ jÇ”zhÃ¨n\", \"trans\": \"similarity matrix\"},\n    {\"word\": \"å®Œå…¨å®ä¾‹åŒ–\", \"pinyin\": \"wÃ¡nquÃ¡n shÃ­lÃ¬huÃ \", \"trans\": \"fully instantiate\"},\n    {\"word\": \"å¤šçº§å—ç­–ç•¥\", \"pinyin\": \"duÅjÃ­ kuÃ i cÃ¨lÃ¼Ã¨\", \"trans\": \"multi-level block strategy\"},\n    {\"word\": \"åˆ©ç”¨\", \"pinyin\": \"lÃ¬yÃ²ng\", \"trans\": \"utilize\"},\n    {\"word\": \"åˆ†å¸ƒå¼ç³»ç»Ÿ\", \"pinyin\": \"fÄ“nbÃ¹shÃ¬ xÃ¬tÇ’ng\", \"trans\": \"distributed system\"},\n    {\"word\": \"å±‚æ¬¡ç»“æ„\", \"pinyin\": \"cÃ©ngcÃ¬ jiÃ©gÃ²u\", \"trans\": \"hierarchical structure\"},\n    {\"word\": \"å®éªŒç»“æœ\", \"pinyin\": \"shÃ­yÃ n jiÃ©guÇ’\", \"trans\": \"experimental results\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇnzhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æ‰©å¤§\", \"pinyin\": \"kuÃ²dÃ \", \"trans\": \"expand\"},\n    {\"word\": \"æ‰¹é‡å¤§å°\", \"pinyin\": \"pÄ«liÃ ng dÃ xiaÇ’\", \"trans\": \"batch size\"},\n    {\"word\": \"å‡†ç¡®æ€§\", \"pinyin\": \"zhÇ”nquÃ¨xÃ¬ng\", \"trans\": \"accuracy\"}\n]",
        "trans": "This article introduces a strategy for computing contrastive loss. Traditional methods are limited by the increase in GPU memory consumption. The authors propose a block-based computing strategy that avoids the complete instantiation of similarity matrices. Additionally, they introduce a multi-level block strategy that leverages the hierarchical structure of distributed systems. Experimental results demonstrate that this method can significantly increase batch size while maintaining accuracy.",
        "update_ts": "2024-10-27 10:11"
    }
}