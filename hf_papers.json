{
    "date": {
        "ru": "11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 11",
        "zh": "8æœˆ11æ—¥"
    },
    "time_utc": "2025-08-11 12:24",
    "weekday": 0,
    "issue_id": 5281,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.06471",
            "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
            "url": "https://huggingface.co/papers/2508.06471",
            "abstract": "GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.",
            "score": 48,
            "issue_id": 5273,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 8",
                "zh": "8æœˆ8æ—¥"
            },
            "hash": "fdf71e495fcd6efa",
            "authors": [
                "GLM-4. 5 Team",
                ":",
                "Aohan Zeng",
                "Xin Lv",
                "Qinkai Zheng",
                "Zhenyu Hou",
                "Bin Chen",
                "Chengxing Xie",
                "Cunxiang Wang",
                "Da Yin",
                "Hao Zeng",
                "Jiajie Zhang",
                "Kedong Wang",
                "Lucen Zhong",
                "Mingdao Liu",
                "Rui Lu",
                "Shulin Cao",
                "Xiaohan Zhang",
                "Xuancheng Huang",
                "Yao Wei",
                "Yean Cheng",
                "Yifan An",
                "Yilin Niu",
                "Yuanhao Wen",
                "Yushi Bai",
                "Zhengxiao Du",
                "Zihan Wang",
                "Zilin Zhu",
                "Bohan Zhang",
                "Bosi Wen",
                "Bowen Wu",
                "Bowen Xu",
                "Can Huang",
                "Casey Zhao",
                "Changpeng Cai",
                "Chao Yu",
                "Chen Li",
                "Chendi Ge",
                "Chenghua Huang",
                "Chenhui Zhang",
                "Chenxi Xu",
                "Chenzheng Zhu",
                "Chuang Li",
                "Congfeng Yin",
                "Daoyan Lin",
                "Dayong Yang",
                "Dazhi Jiang",
                "Ding Ai",
                "Erle Zhu",
                "Fei Wang",
                "Gengzheng Pan",
                "Guo Wang",
                "Hailong Sun",
                "Haitao Li",
                "Haiyang Li",
                "Haiyi Hu",
                "Hanyu Zhang",
                "Hao Peng",
                "Hao Tai",
                "Haoke Zhang",
                "Haoran Wang",
                "Haoyu Yang",
                "He Liu",
                "He Zhao",
                "Hongwei Liu",
                "Hongxi Yan",
                "Huan Liu",
                "Huilong Chen",
                "Ji Li",
                "Jiajing Zhao",
                "Jiamin Ren",
                "Jian Jiao",
                "Jiani Zhao",
                "Jianyang Yan",
                "Jiaqi Wang",
                "Jiayi Gui",
                "Jiayue Zhao",
                "Jie Liu",
                "Jijie Li",
                "Jing Li",
                "Jing Lu",
                "Jingsen Wang",
                "Jingwei Yuan",
                "Jingxuan Li",
                "Jingzhao Du",
                "Jinhua Du",
                "Jinxin Liu",
                "Junkai Zhi",
                "Junli Gao",
                "Ke Wang",
                "Lekang Yang",
                "Liang Xu",
                "Lin Fan",
                "Lindong Wu",
                "Lintao Ding",
                "Lu Wang",
                "Man Zhang",
                "Minghao Li",
                "Minghuan Xu",
                "Mingming Zhao",
                "Mingshu Zhai",
                "Pengfan Du",
                "Qian Dong",
                "Shangde Lei",
                "Shangqing Tu",
                "Shangtong Yang",
                "Shaoyou Lu",
                "Shijie Li",
                "Shuang Li",
                "Shuang-Li",
                "Shuxun Yang",
                "Sibo Yi",
                "Tianshu Yu",
                "Wei Tian",
                "Weihan Wang",
                "Wenbo Yu",
                "Weng Lam Tam",
                "Wenjie Liang",
                "Wentao Liu",
                "Xiao Wang",
                "Xiaohan Jia",
                "Xiaotao Gu",
                "Xiaoying Ling",
                "Xin Wang",
                "Xing Fan",
                "Xingru Pan",
                "Xinyuan Zhang",
                "Xinze Zhang",
                "Xiuqing Fu",
                "Xunkai Zhang",
                "Yabo Xu",
                "Yandong Wu",
                "Yida Lu",
                "Yidong Wang",
                "Yilin Zhou",
                "Yiming Pan",
                "Ying Zhang",
                "Yingli Wang",
                "Yingru Li",
                "Yinpei Su",
                "Yipeng Geng",
                "Yitong Zhu",
                "Yongkun Yang",
                "Yuhang Li",
                "Yuhao Wu",
                "Yujiang Li",
                "Yunan Liu",
                "Yunqing Wang",
                "Yuntao Li",
                "Yuxuan Zhang",
                "Zezhen Liu",
                "Zhen Yang",
                "Zhengda Zhou",
                "Zhongpei Qiao",
                "Zhuoer Feng",
                "Zhuorui Liu",
                "Zichen Zhang",
                "Zihan Wang",
                "Zijun Yao",
                "Zikang Wang",
                "Ziqiang Liu",
                "Ziwei Chai",
                "Zixuan Li",
                "Zuodong Zhao",
                "Wenguang Chen",
                "Jidong Zhai",
                "Bin Xu",
                "Minlie Huang",
                "Hongning Wang",
                "Juanzi Li",
                "Yuxiao Dong",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06471.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#open_source",
                    "#reasoning",
                    "#architecture",
                    "#agi",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GLM-4.5: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "GLM-4.5 - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 355 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 23 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. GLM-4.5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ GLM-4.5, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ GLM-4.5-Air Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "GLM-4.5: Powerful Reasoning with Fewer Parameters",
                    "desc": "GLM-4.5 is a large language model that uses a Mixture-of-Experts (MoE) architecture with 355 billion parameters, of which 32 billion are activated during operation. It employs a hybrid reasoning approach that allows it to perform tasks in both thinking and direct response modes. The model has undergone extensive multi-stage training on 23 trillion tokens and has been fine-tuned using reinforcement learning, resulting in impressive performance on various benchmarks. Notably, GLM-4.5 ranks highly among its peers, demonstrating strong capabilities in agentic, reasoning, and coding tasks while being more parameter-efficient than many competitors."
                },
                "zh": {
                    "title": "GLM-4.5ï¼šå¼ºå¤§çš„æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹",
                    "desc": "GLM-4.5æ˜¯ä¸€ç§å…·æœ‰3550äº¿å‚æ•°çš„æ··åˆä¸“å®¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç†ã€æ¨ç†å’Œç¼–ç ä»»åŠ¡ä¸Šã€‚å®ƒé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œç»è¿‡23ä¸‡äº¿ä¸ªæ ‡è®°çš„è®­ç»ƒï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚GLM-4.5åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼Œå°¤å…¶åœ¨ä»£ç†åŸºå‡†ä¸­æ’åç¬¬äºŒã€‚è¯¥æ¨¡å‹çš„å¼€æºç‰ˆæœ¬å’Œç´§å‡‘ç‰ˆï¼ˆGLM-4.5-Airï¼‰éƒ½å·²å‘å¸ƒï¼Œæ—¨åœ¨æ¨åŠ¨æ¨ç†å’Œä»£ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.04825",
            "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off",
            "url": "https://huggingface.co/papers/2508.04825",
            "abstract": "Voost, a unified diffusion transformer framework, jointly learns virtual try-on and try-off, enhancing garment-body correspondence and achieving state-of-the-art results across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.",
            "score": 23,
            "issue_id": 5272,
            "pub_date": "2025-08-06",
            "pub_date_card": {
                "ru": "6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 6",
                "zh": "8æœˆ6æ—¥"
            },
            "hash": "085e45094380ed54",
            "authors": [
                "Seungyong Lee",
                "Jeong-gi Kwak"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2508.04825.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#optimization",
                    "#inference",
                    "#architecture",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ‘š",
                "ru": {
                    "title": "Voost: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞµ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹",
                    "desc": "Voost - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞµ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ ĞµĞµ ÑĞ½ÑÑ‚Ğ¸Ñ. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ´ĞµĞ¶Ğ´Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞ»Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Voost Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµÑ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ."
                },
                "en": {
                    "title": "Voost: Revolutionizing Virtual Try-On with Unified Learning",
                    "desc": "Voost is a new framework that uses a diffusion transformer to improve virtual try-on and try-off tasks in fashion technology. It learns to create realistic images of people wearing clothes by understanding how garments fit the body, even when poses and appearances change. By training both tasks together, Voost enhances the relationship between garments and bodies without needing extra networks or labels. The framework also includes innovative techniques to improve performance during image generation, leading to top results in accuracy and visual quality across various benchmarks."
                },
                "zh": {
                    "title": "Voostï¼šè™šæ‹Ÿè¯•ç©¿ä¸è¯•è„±çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "Voostæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å­¦ä¹ è™šæ‹Ÿè¯•ç©¿å’Œè¯•è„±ã€‚å®ƒé€šè¿‡è”åˆå»ºæ¨¡è¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œå¢å¼ºäº†æœè£…ä¸èº«ä½“ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œè§£å†³äº†åœ¨å§¿åŠ¿å’Œå¤–è§‚å˜åŒ–ä¸‹çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ”¯æŒçµæ´»çš„ç”Ÿæˆæ–¹å‘å’Œæœè£…ç±»åˆ«ï¼Œé¿å…äº†ç‰¹å®šä»»åŠ¡çš„ç½‘ç»œå’Œé¢å¤–æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVooståœ¨è¯•ç©¿å’Œè¯•è„±çš„åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05731",
            "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy\n  Optimization",
            "url": "https://huggingface.co/papers/2508.05731",
            "abstract": "Adaptive Exploration Policy Optimization (AEPO) enhances semantic alignment in Multimodal Large Language Models (MLLMs) for GUI interaction, improving performance on benchmarks by up to 9.0% compared to RLVR.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.",
            "score": 13,
            "issue_id": 5272,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "31f17f5fb142d3e8",
            "authors": [
                "Yuhang Liu",
                "Zeyu Liu",
                "Shuanghe Zhu",
                "Pengxiang Li",
                "Congkai Xie",
                "Jiasheng Wang",
                "Xueyu Hu",
                "Xiaotian Han",
                "Jianbo Yuan",
                "Xinyao Wang",
                "Shengyu Zhang",
                "Hongxia Yang",
                "Fei Wu"
            ],
            "affiliations": [
                "Amazon",
                "InfiX.ai",
                "The Hong Kong Polytechnic University",
                "The University of Chicago",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05731.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#training",
                    "#rl",
                    "#benchmark",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "AEPO: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (AEPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM) Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. AEPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (AER). ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AEPO, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ 9.0% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ RLVR Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing GUI Interaction with Adaptive Exploration in MLLMs",
                    "desc": "Adaptive Exploration Policy Optimization (AEPO) is a novel framework designed to enhance semantic alignment in Multimodal Large Language Models (MLLMs) for effective interaction with Graphical User Interfaces (GUIs). The paper identifies that while existing methods like Reinforcement Learning with Verifiable Rewards (RLVR) improve spatial alignment, they struggle with semantic alignment due to inefficient exploration strategies. AEPO addresses this by implementing a multi-answer generation approach that encourages broader exploration, guided by an Adaptive Exploration Reward (AER) function. As a result, models trained with AEPO demonstrate significant performance improvements, achieving state-of-the-art results on various GUI grounding benchmarks."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”æ¢ç´¢ï¼Œæå‡è¯­ä¹‰å¯¹é½ï¼",
                    "desc": "è‡ªé€‚åº”æ¢ç´¢ç­–ç•¥ä¼˜åŒ–ï¼ˆAEPOï¼‰é€šè¿‡å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰äº¤äº’ä¸­çš„è¯­ä¹‰å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸UIå…ƒç´ ä¹‹é—´çš„è¯­ä¹‰å¯¹é½é—®é¢˜ï¼Œå…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¢ç´¢æ•ˆç‡ä¸Šçš„ç“¶é¢ˆã€‚AEPOé‡‡ç”¨å¤šç­”æ¡ˆç”Ÿæˆç­–ç•¥ï¼Œç»“åˆç†è®ºåŸºç¡€çš„è‡ªé€‚åº”æ¢ç´¢å¥–åŠ±å‡½æ•°ï¼Œä¿ƒè¿›äº†æ›´å¹¿æ³›çš„æ¢ç´¢ã€‚ç»è¿‡AEPOè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€é«˜9.0%çš„ç›¸å¯¹æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨è¯­ä¹‰ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06433",
            "title": "Memp: Exploring Agent Procedural Memory",
            "url": "https://huggingface.co/papers/2508.06433",
            "abstract": "Agents equipped with a learnable, updatable procedural memory system, Memp, achieve improved performance and efficiency across tasks by distilling past experiences into detailed instructions and higher-level abstractions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.",
            "score": 12,
            "issue_id": 5279,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 8",
                "zh": "8æœˆ8æ—¥"
            },
            "hash": "28c51ac0d694bc54",
            "authors": [
                "Runnan Fang",
                "Yuan Liang",
                "Xiaobin Wang",
                "Jialong Wu",
                "Shuofei Qiao",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06433.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agi",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Memp: ÑƒĞ¼Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Memp - Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Memp Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚, Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞ²Ğ¾ĞµĞ¼ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Memp Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Empowering Agents with Evolving Procedural Memory",
                    "desc": "This paper introduces Memp, a learnable and updatable procedural memory system for agents that enhances their performance and efficiency in various tasks. Memp allows agents to distill their past experiences into detailed instructions and higher-level abstractions, addressing the limitations of traditional static procedural memory. The authors explore different strategies for building, retrieving, and updating this memory, ensuring it evolves with new experiences. Empirical results demonstrate that agents using Memp show improved success rates and efficiency, even when transferring memory from a stronger model to a weaker one."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä½“çš„å¯å­¦ä¹ ç¨‹åºè®°å¿†æå‡ä»»åŠ¡è¡¨ç°",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯å­¦ä¹ å’Œå¯æ›´æ–°çš„ç¨‹åºè®°å¿†ç³»ç»ŸMempï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ä½“åœ¨ä»»åŠ¡ä¸­çš„è¡¨ç°å’Œæ•ˆç‡ã€‚Mempé€šè¿‡å°†è¿‡å»çš„ç»éªŒæç‚¼ä¸ºè¯¦ç»†çš„æŒ‡ä»¤å’Œæ›´é«˜å±‚æ¬¡çš„æŠ½è±¡ï¼Œå¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°æ‰§è¡Œä»»åŠ¡ã€‚ç ”ç©¶æ¢è®¨äº†ç¨‹åºè®°å¿†çš„æ„å»ºã€æ£€ç´¢å’Œæ›´æ–°ç­–ç•¥ï¼Œå¹¶å±•ç¤ºäº†åŠ¨æ€æ›´æ–°æœºåˆ¶å¦‚ä½•ä½¿è®°å¿†åº“éšç€æ–°ç»éªŒä¸æ–­æ¼”å˜ã€‚å®éªŒè¯æ˜ï¼Œéšç€è®°å¿†åº“çš„ä¼˜åŒ–ï¼Œæ™ºèƒ½ä½“åœ¨ç±»ä¼¼ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡å’Œæ•ˆç‡æ˜¾è‘—æé«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05988",
            "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token\n  Surprisal",
            "url": "https://huggingface.co/papers/2508.05988",
            "abstract": "ASAP, a novel coarse-to-fine framework, compresses Chain-of-Thought in code reasoning by preserving core structure and essential steps, reducing costs and improving efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables a logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving a competitive accuracy of 36.19% in Pass@1. Our results highlight a promising direction for building powerful and efficient LRMs.",
            "score": 12,
            "issue_id": 5272,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 8",
                "zh": "8æœˆ8æ—¥"
            },
            "hash": "82ad3c225d1615aa",
            "authors": [
                "Wenhao Zeng",
                "Yaoning Wang",
                "Chao Hu",
                "Yuling Shi",
                "Chengcheng Wan",
                "Hongyu Zhang",
                "Xiaodong Gu"
            ],
            "affiliations": [
                "Chongqing University",
                "East China Normal University",
                "Fudan University",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05988.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#architecture",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼",
                    "desc": "ASAP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ ĞºĞ¾Ğ´Ğµ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ASAP Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ASAP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´."
                },
                "en": {
                    "title": "Efficient Code Reasoning with ASAP: Smart Compression for Better Performance",
                    "desc": "The paper introduces ASAP, a new framework designed to compress Chain-of-Thought (CoT) in code reasoning while maintaining essential logical structures. It addresses the challenges posed by long reasoning traces, which can increase training costs and slow down inference. ASAP uses anchor-guided pruning to focus on core reasoning elements and applies a novel first-token surprisal metric to identify critical reasoning steps. The results demonstrate that ASAP not only improves efficiency by reducing token generation and inference latency but also maintains high accuracy in code generation tasks."
                },
                "zh": {
                    "title": "ASAPï¼šé«˜æ•ˆçš„ä»£ç æ¨ç†æ€ç»´é“¾å‹ç¼©æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºASAPçš„æ–°çš„ç²—åˆ°ç»†æ¡†æ¶ï¼Œç”¨äºå‹ç¼©ä»£ç æ¨ç†ä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œæ—¨åœ¨ä¿ç•™æ ¸å¿ƒç»“æ„å’Œå…³é”®æ­¥éª¤ï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚ASAPé¦–å…ˆé€šè¿‡é”šç‚¹å¼•å¯¼ä¿®å‰ªæ¥ä¿ç•™æ ¸å¿ƒæ¨ç†ç»“æ„ï¼Œå‡å°‘åç»­å¤„ç†çš„æœç´¢ç©ºé—´ã€‚æ¥ç€ï¼Œå®ƒåŸºäºæ–°é¢–çš„é¦–ä¸ªä»¤ç‰ŒæƒŠè®¶åº¦æŒ‡æ ‡ï¼Œé€‰æ‹©é€»è¾‘ä¸Šé‡è¦çš„æ¨ç†æ­¥éª¤è¿›è¡Œé€»è¾‘æ„ŸçŸ¥ä¿®å‰ªã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒASAPåœ¨å¤šä¸ªä»£ç ç”ŸæˆåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02831",
            "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
            "url": "https://huggingface.co/papers/2508.02831",
            "abstract": "GENIE combines NeRF's photorealistic rendering with Gaussian Splatting's editable and structured representation, enabling real-time, locality-aware editing and integration with physics-based simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie)",
            "score": 6,
            "issue_id": 5276,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "95f1bf82d6941705",
            "authors": [
                "MikoÅ‚aj ZieliÅ„ski",
                "Krzysztof Byrski",
                "Tomasz Szczepanik",
                "PrzemysÅ‚aw Spurek"
            ],
            "affiliations": [
                "IDEAS Research Institute",
                "Jagiellonian University, Faculty of Mathematics and Computer Science, Åojasiewicza 6, 30-348, Krakow, Poland",
                "Poznan University of Technology, Institute of Robotics and Machine Intelligence, ul. Piotrowo 3A, Poznan 60-965, Poland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02831.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "GENIE: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ NeRF Ğ¸ Gaussian Splatting Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½",
                    "desc": "GENIE - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ NeRF Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Gaussian Splatting. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñƒ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµÑ‚Ğ¸ NeRF Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ k Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğº ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ÑĞ¾ ÑÑ†ĞµĞ½Ğ¾Ğ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "GENIE: Real-Time 3D Scene Editing with Photorealism and Intuition",
                    "desc": "GENIE is a new model that merges the strengths of Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) for 3D scene rendering and editing. It allows for photorealistic images while also enabling real-time, interactive editing through an explicit representation of scenes using Gaussian primitives. By introducing a method called Ray-Traced Gaussian Proximity Search (RT-GPS), GENIE efficiently finds the nearest Gaussians for each point, making editing faster and more intuitive. This combination supports dynamic interactions and physical simulations, making it easier to manipulate 3D scenes in a realistic way."
                },
                "zh": {
                    "title": "GENIEï¼šå®æ—¶å¯ç¼–è¾‘çš„3Dåœºæ™¯æ¸²æŸ“æ–°æ–¹æ³•",
                    "desc": "GENIEæ˜¯ä¸€ç§ç»“åˆäº†NeRFçš„é«˜è´¨é‡å…‰çº¿æ¸²æŸ“å’ŒGaussian Splattingçš„å¯ç¼–è¾‘ç»“æ„è¡¨ç¤ºçš„æ··åˆæ¨¡å‹ã€‚å®ƒé€šè¿‡ä¸ºæ¯ä¸ªé«˜æ–¯åˆ†å¸ƒåˆ†é…å¯è®­ç»ƒçš„ç‰¹å¾åµŒå…¥ï¼Œæ¥å®ç°å®æ—¶çš„å±€éƒ¨æ„ŸçŸ¥ç¼–è¾‘ã€‚GENIEè¿˜å¼•å…¥äº†åŸºäºå…‰çº¿è¿½è¸ªçš„é«˜æ–¯é‚»è¿‘æœç´¢ï¼ˆRT-GPSï¼‰ï¼Œæé«˜äº†é«˜æ–¯æœç´¢çš„æ•ˆç‡ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼ŒGENIEæ”¯æŒç›´è§‚çš„åœºæ™¯æ“ä½œå’Œä¸ç‰©ç†æ¨¡æ‹Ÿçš„å…¼å®¹æ€§ï¼Œå¼¥åˆäº†å‡ ä½•ç¼–è¾‘ä¸ç¥ç»æ¸²æŸ“ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05547",
            "title": "Adapting Vision-Language Models Without Labels: A Comprehensive Survey",
            "url": "https://huggingface.co/papers/2508.05547",
            "abstract": "A comprehensive survey of unsupervised adaptation methods for Vision-Language Models (VLMs) categorizes approaches based on the availability of unlabeled visual data and discusses methodologies, benchmarks, and future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.",
            "score": 3,
            "issue_id": 5274,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "9c2f43e8f72ea22d",
            "authors": [
                "Hao Dong",
                "Lijun Sheng",
                "Jian Liang",
                "Ran He",
                "Eleni Chatzi",
                "Olga Fink"
            ],
            "affiliations": [
                "EPFL, Switzerland",
                "ETH Zurich, Switzerland",
                "Institute of Automation, Chinese Academy of Sciences",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05547.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#survey",
                    "#transfer_learning",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ·Ñ€ĞµĞ½Ğ¸Ğµ + ÑĞ·Ñ‹Ğº) Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğµ Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Vision-Language Models: A Guide to Unsupervised Adaptation",
                    "desc": "This paper provides a detailed survey of unsupervised adaptation methods for Vision-Language Models (VLMs), which are designed to improve model performance without the need for labeled data. It categorizes these methods into four main types based on the availability of unlabeled visual data: Data-Free Transfer, Unsupervised Domain Transfer, Episodic Test-Time Adaptation, and Online Test-Time Adaptation. The authors analyze various methodologies and strategies within this framework, aiming to create a clearer understanding of how to effectively adapt VLMs to specific tasks. Furthermore, the paper discusses existing benchmarks and identifies future research opportunities in the field of unsupervised VLM adaptation."
                },
                "zh": {
                    "title": "æ— ç›‘ç£é€‚åº”ï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ½œåŠ›",
                    "desc": "æœ¬æ–‡å¯¹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ— ç›‘ç£é€‚åº”æ–¹æ³•æ–¹é¢è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ã€‚ç ”ç©¶å°†ç°æœ‰æ–¹æ³•æ ¹æ®æ— æ ‡ç­¾è§†è§‰æ•°æ®çš„å¯ç”¨æ€§è¿›è¡Œåˆ†ç±»ï¼Œæå‡ºäº†å››ç§ä¸»è¦èŒƒå¼ï¼šæ— æ•°æ®è¿ç§»ã€æ— ç›‘ç£é¢†åŸŸè¿ç§»ã€æƒ…æ™¯æµ‹è¯•æ—¶é€‚åº”å’Œåœ¨çº¿æµ‹è¯•æ—¶é€‚åº”ã€‚æ–‡ç« åˆ†æäº†æ¯ç§èŒƒå¼çš„æ ¸å¿ƒæ–¹æ³•å’Œé€‚åº”ç­–ç•¥ï¼Œå¹¶å›é¡¾äº†ä¸åŒåº”ç”¨ä¸­çš„ä»£è¡¨æ€§åŸºå‡†ã€‚æœ€åï¼ŒæŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„å¼€æ”¾æŒ‘æˆ˜å’Œæœ‰å‰æ™¯çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05502",
            "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for\n  Low-Resource Language MLLMs",
            "url": "https://huggingface.co/papers/2508.05502",
            "abstract": "MELLA, a multimodal, multilingual dataset, enhances MLLMs in low-resource languages by improving linguistic capability and cultural groundedness through native web alt-text and MLLM-generated captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce \"thin descriptions\", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing \"thick descriptions\". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.",
            "score": 3,
            "issue_id": 5272,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 7",
                "zh": "8æœˆ7æ—¥"
            },
            "hash": "b7b633c31ed6e35e",
            "authors": [
                "Yufei Gao",
                "Jiaying Fei",
                "Nuo Chen",
                "Ruirui Chen",
                "Guohang Yan",
                "Yunshi Lan",
                "Botian Shi"
            ],
            "affiliations": [
                "East China Normal University",
                "Institute of High Performance Computing, A*STAR",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05502.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#multilingual",
                    "#training",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "MELLA: Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ² MLLM Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²",
                    "desc": "Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ MELLA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². MELLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ MLLM, Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… MLLM Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° MELLA. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ ĞºĞ°Ğº Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing MLLMs for Low-Resource Languages with MELLA",
                    "desc": "This paper introduces MELLA, a new dataset designed to improve Multimodal Large Language Models (MLLMs) for low-resource languages. It focuses on enhancing both linguistic capabilities and cultural groundedness by using native web alt-text and MLLM-generated captions. The study highlights the limitations of existing methods that rely solely on text or machine translation, which often fail to provide rich, informative content. By fine-tuning MLLMs on MELLA, the results show significant performance improvements across multiple languages, enabling models to generate more detailed and culturally aware descriptions."
                },
                "zh": {
                    "title": "æå‡ä½èµ„æºè¯­è¨€çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹",
                    "desc": "MELLAæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šè¯­è¨€çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡ä½èµ„æºè¯­è¨€ä¸­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¯­è¨€èƒ½åŠ›å’Œæ–‡åŒ–åŸºç¡€ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŒæºç­–ç•¥ï¼Œé€šè¿‡æ”¶é›†æœ¬åœ°ç½‘é¡µçš„æ›¿ä»£æ–‡æœ¬å’ŒMLLMç”Ÿæˆçš„æ ‡é¢˜ï¼Œæ¥å®ç°è¯­è¨€èƒ½åŠ›å’Œæ–‡åŒ–åŸºç¡€çš„åŒé‡ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨MELLAä¸Šè¿›è¡Œå¾®è°ƒåï¼Œå…«ç§è¯­è¨€çš„æ¨¡å‹åœ¨å¤šç§MLLMåŸºç¡€ä¸Šæ™®éæé«˜äº†æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´ä¸°å¯Œçš„æè¿°ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¼ºè°ƒäº†æ–‡åŒ–çŸ¥è¯†å’Œè¯­è¨€èƒ½åŠ›çš„å¢å¼ºï¼Œé€‚ç”¨äºä½èµ„æºè¯­è¨€ç”¨æˆ·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01242",
            "title": "MeshLLM: Empowering Large Language Models to Progressively Understand\n  and Generate 3D Mesh",
            "url": "https://huggingface.co/papers/2508.01242",
            "abstract": "MeshLLM uses large language models to generate and understand text-serialized 3D meshes by decomposing them into meaningful subunits and training with local mesh assembly strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MeshLLM, a novel framework that leverages large language models (LLMs) to understand and generate text-serialized 3D meshes. Our approach addresses key limitations in existing methods, including the limited dataset scale when catering to LLMs' token length and the loss of 3D structural information during mesh serialization. We introduce a Primitive-Mesh decomposition strategy, which divides 3D meshes into structurally meaningful subunits. This enables the creation of a large-scale dataset with 1500k+ samples, almost 50 times larger than previous methods, which aligns better with the LLM scaling law principles. Furthermore, we propose inferring face connectivity from vertices and local mesh assembly training strategies, significantly enhancing the LLMs' ability to capture mesh topology and spatial structures. Experiments show that MeshLLM outperforms the state-of-the-art LLaMA-Mesh in both mesh generation quality and shape understanding, highlighting its great potential in processing text-serialized 3D meshes.",
            "score": 3,
            "issue_id": 5274,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 2",
                "zh": "8æœˆ2æ—¥"
            },
            "hash": "425226c54ca88a3a",
            "authors": [
                "Shuangkang Fang",
                "I-Chao Shen",
                "Yufeng Wang",
                "Yi-Hsuan Tsai",
                "Yi Yang",
                "Shuchang Zhou",
                "Wenrui Ding",
                "Takeo Igarashi",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "Atmanity Inc.",
                "Beihang University",
                "StepFun Inc.",
                "The University of Tokyo",
                "UC Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01242.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#dataset",
                    "#optimization",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "MeshLLM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "MeshLLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. MeshLLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 1500000+ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ğ½ĞµĞ¹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Mesh Generation with Language Models",
                    "desc": "MeshLLM is a new framework that uses large language models to generate and interpret 3D meshes represented as text. It improves upon previous methods by introducing a Primitive-Mesh decomposition strategy, which breaks down 3D meshes into meaningful parts, allowing for a much larger dataset of over 1.5 million samples. This approach helps maintain important 3D structural information that is often lost in serialization. Additionally, MeshLLM enhances the model's understanding of mesh topology and spatial relationships through innovative training techniques, leading to superior performance in generating and understanding 3D shapes compared to existing models."
                },
                "zh": {
                    "title": "MeshLLMï¼šé‡å¡‘3Dç½‘æ ¼ç”Ÿæˆä¸ç†è§£çš„æœªæ¥",
                    "desc": "MeshLLMæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç†è§£å’Œç”Ÿæˆæ–‡æœ¬åºåˆ—åŒ–çš„3Dç½‘æ ¼ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åŸå§‹ç½‘æ ¼åˆ†è§£ç­–ç•¥ï¼Œå°†3Dç½‘æ ¼åˆ†è§£ä¸ºç»“æ„ä¸Šæœ‰æ„ä¹‰çš„å­å•å…ƒï¼Œä»è€Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ•°æ®é›†è§„æ¨¡å’Œ3Dç»“æ„ä¿¡æ¯æŸå¤±æ–¹é¢çš„å…³é”®é™åˆ¶ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè¶…è¿‡150ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå‡ ä¹æ˜¯ä¹‹å‰æ–¹æ³•çš„50å€ï¼Œæ›´å¥½åœ°ç¬¦åˆLLMçš„æ‰©å±•æ³•åˆ™ã€‚æ­¤å¤–ï¼ŒMeshLLMé€šè¿‡æ¨æ–­é¡¶ç‚¹ä¹‹é—´çš„é¢è¿æ¥æ€§å’Œå±€éƒ¨ç½‘æ ¼ç»„è£…è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†LLMæ•æ‰ç½‘æ ¼æ‹“æ‰‘å’Œç©ºé—´ç»“æ„çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22025",
            "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding",
            "url": "https://huggingface.co/papers/2507.22025",
            "abstract": "UI-AGILE enhances GUI agents through improved training with a Continuous Reward function, Simple Thinking reward, and Cropping-based Resampling, and inference with Decomposed Grounding with Selection, achieving state-of-the-art performance on GUI benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a \"Simple Thinking\" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro.",
            "score": 2,
            "issue_id": 5274,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "f7ff8c2c5517f5de",
            "authors": [
                "Shuquan Lian",
                "Yuhang Wu",
                "Jia Ma",
                "Zihan Song",
                "Bingqi Chen",
                "Xiawu Zheng",
                "Hui Li"
            ],
            "affiliations": [
                "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22025.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#cv",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "UI-AGILE: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² GUI",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° UI-AGILE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² GUI Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²ÑˆĞµÑÑ‚Ğ²Ğ°: Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€ĞµÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸. Ğ”Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UI-AGILE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ScreenSpot-Pro Ğ¸ ScreenSpot-v2."
                },
                "en": {
                    "title": "Revolutionizing GUI Agents with UI-AGILE",
                    "desc": "UI-AGILE is a framework designed to improve the performance of Graphical User Interface (GUI) agents by enhancing their training and inference processes. It introduces a Continuous Reward function to encourage precise grounding, a Simple Thinking reward to optimize the balance between planning speed and accuracy, and a Cropping-based Resampling technique to address sparse rewards in complex tasks. For inference, it employs Decomposed Grounding with Selection, which enhances grounding accuracy by breaking down images into smaller sections for better processing. The framework has demonstrated state-of-the-art results on GUI benchmarks, significantly improving grounding accuracy compared to existing methods."
                },
                "zh": {
                    "title": "UI-AGILEï¼šæå‡GUIä»£ç†çš„æ™ºèƒ½è®­ç»ƒä¸æ¨ç†",
                    "desc": "UI-AGILEæ˜¯ä¸€ä¸ªå¢å¼ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„æ¡†æ¶ï¼Œé€šè¿‡æ”¹è¿›è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹æ¥æå‡å…¶æ€§èƒ½ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼ŒUI-AGILEå¼•å…¥äº†è¿ç»­å¥–åŠ±å‡½æ•°ã€ç®€å•æ€ç»´å¥–åŠ±å’ŒåŸºäºè£å‰ªçš„é‡é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜é«˜ç²¾åº¦çš„åŸºç¡€å®šä½èƒ½åŠ›å’Œå­¦ä¹ å¤æ‚ä»»åŠ¡çš„æ•ˆæœã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé‡‡ç”¨äº†åˆ†è§£åŸºç¡€å®šä½ä¸é€‰æ‹©çš„æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†é«˜åˆ†è¾¨ç‡æ˜¾ç¤ºå™¨ä¸Šçš„åŸºç¡€å®šä½å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUI-AGILEåœ¨ScreenSpot-Proå’ŒScreenSpot-v2åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06494",
            "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion",
            "url": "https://huggingface.co/papers/2508.06494",
            "abstract": "Lightswitch, a material-relighting diffusion framework, enhances 3D relighting by integrating multi-view and material cues, achieving superior quality and efficiency compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent approaches for 3D relighting have shown promise in integrating 2D image relighting generative priors to alter the appearance of a 3D representation while preserving the underlying structure. Nevertheless, generative priors used for 2D relighting that directly relight from an input image do not take advantage of intrinsic properties of the subject that can be inferred or cannot consider multi-view data at scale, leading to subpar relighting. In this paper, we propose Lightswitch, a novel finetuned material-relighting diffusion framework that efficiently relights an arbitrary number of input images to a target lighting condition while incorporating cues from inferred intrinsic properties. By using multi-view and material information cues together with a scalable denoising scheme, our method consistently and efficiently relights dense multi-view data of objects with diverse material compositions. We show that our 2D relighting prediction quality exceeds previous state-of-the-art relighting priors that directly relight from images. We further demonstrate that LightSwitch matches or outperforms state-of-the-art diffusion inverse rendering methods in relighting synthetic and real objects in as little as 2 minutes.",
            "score": 1,
            "issue_id": 5279,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 8",
                "zh": "8æœˆ8æ—¥"
            },
            "hash": "f69d154601aca623",
            "authors": [
                "Yehonathan Litman",
                "Fernando De la Torre",
                "Shubham Tulsiani"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06494.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞ»Ğ°Ğ¹Ñ‚Ğ¸Ğ½Ğ³Ğµ: Lightswitch Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾",
                    "desc": "Lightswitch - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ 3D-Ñ€ĞµĞ»Ğ°Ğ¹Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². ĞĞ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ€Ğ¸ÑĞ¾Ğ²Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ñ…. Lightswitch Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ»Ğ°Ğ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Relighting with Lightswitch",
                    "desc": "Lightswitch is a new framework designed for improving 3D relighting by using both multi-view and material information. It addresses the limitations of previous methods that relied solely on 2D image relighting, which often failed to utilize the intrinsic properties of objects. By integrating these cues into a diffusion model, Lightswitch can efficiently relight multiple images to match a desired lighting condition. The results show that it not only enhances the quality of relighting but also does so faster than existing techniques."
                },
                "zh": {
                    "title": "Lightswitchï¼šé«˜æ•ˆçš„3Dé‡å…‰æ–°æ–¹æ³•",
                    "desc": "Lightswitchæ˜¯ä¸€ç§æ–°å‹çš„ææ–™é‡å…‰æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå¤šè§†è§’å’Œææ–™çº¿ç´¢ï¼Œæ˜¾è‘—æå‡äº†3Dé‡å…‰çš„è´¨é‡å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†ä»»æ„æ•°é‡çš„è¾“å…¥å›¾åƒé‡å…‰åˆ°ç›®æ ‡å…‰ç…§æ¡ä»¶ï¼ŒåŒæ—¶è€ƒè™‘åˆ°ç‰©ä½“çš„å†…åœ¨ç‰¹æ€§ã€‚ä¸ä¼ ç»Ÿçš„2Då›¾åƒé‡å…‰æ–¹æ³•ç›¸æ¯”ï¼ŒLightswitchåœ¨å¤„ç†å¤šè§†è§’æ•°æ®æ—¶è¡¨ç°å‡ºæ›´å¥½çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLightswitchåœ¨åˆæˆå’ŒçœŸå®ç‰©ä½“çš„é‡å…‰ä»»åŠ¡ä¸­ï¼Œé€Ÿåº¦å¿«ä¸”è´¨é‡é«˜ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-08.html",
    "link_next": "2025-08-12.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "08.08",
        "en": "08/08",
        "zh": "8æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "12.08",
        "en": "08/12",
        "zh": "8æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 4,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 2,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}