{
    "date": {
        "ru": "18 февраля",
        "en": "February 18",
        "zh": "2月18日"
    },
    "time_utc": "2025-02-18 05:10",
    "weekday": 1,
    "issue_id": 2265,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.12146",
            "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
            "url": "https://huggingface.co/papers/2502.12146",
            "abstract": "We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening",
            "score": 5,
            "issue_id": 2265,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "25a69f8cf847067e",
            "authors": [
                "Ye Tian",
                "Ling Yang",
                "Xinchen Zhang",
                "Yunhai Tong",
                "Mengdi Wang",
                "Bin Cui"
            ],
            "affiliations": [
                "Peking University",
                "Princeton University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12146.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#diffusion",
                    "#alignment",
                    "#rl"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Оптимизация траекторий для повышения точности диффузионных моделей",
                    "desc": "Авторы предлагают метод Diffusion-Sharpening для улучшения точности генеративных диффузионных моделей. Этот подход оптимизирует траектории сэмплирования во время обучения, используя интегралы по путям и обратную связь по вознаграждению. Diffusion-Sharpening демонстрирует превосходную эффективность обучения и вывода по сравнению с существующими методами. Эксперименты показывают, что предложенный метод превосходит другие подходы по различным метрикам, включая согласованность текста и предпочтения людей."
                },
                "en": {
                    "title": "Optimize Sampling Paths for Better Model Alignment!",
                    "desc": "The paper introduces Diffusion-Sharpening, a novel fine-tuning method that improves the alignment of machine learning models by optimizing the paths taken during sampling. Unlike traditional reinforcement learning (RL) methods that focus on individual training steps, this approach considers the entire trajectory, which enhances overall performance. By employing a path integral framework, Diffusion-Sharpening efficiently selects the best trajectories while minimizing inference costs. Experimental results show that this method not only converges faster but also achieves better performance than existing RL-based and trajectory optimization techniques across various evaluation metrics."
                },
                "zh": {
                    "title": "扩散锐化：高效的微调新方法",
                    "desc": "我们提出了一种名为扩散锐化（Diffusion-Sharpening）的微调方法，通过优化采样轨迹来增强下游对齐。现有的基于强化学习的微调方法主要关注单个训练时间步，忽视了轨迹级别的对齐，而最近的采样轨迹优化方法则带来了显著的推理成本。扩散锐化通过使用路径积分框架在训练过程中选择最佳轨迹，利用奖励反馈并摊销推理成本，从而克服了这些问题。我们的实验表明，扩散锐化在训练效率和推理效率上均优于现有的微调方法，提供了一种可扩展且高效的未来扩散模型微调解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12148",
            "title": "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2502.12148",
            "abstract": "The remarkable success of the autoregressive paradigm has made significant advancement in Multimodal Large Language Models (MLLMs), with powerful models like Show-o, Transfusion and Emu3 achieving notable progress in unified image understanding and generation. For the first time, we uncover a common phenomenon: the understanding capabilities of MLLMs are typically stronger than their generative capabilities, with a significant gap between the two. Building on this insight, we propose HermesFlow, a simple yet general framework designed to seamlessly bridge the gap between understanding and generation in MLLMs. Specifically, we take the homologous data as input to curate homologous preference data of both understanding and generation. Through Pair-DPO and self-play iterative optimization, HermesFlow effectively aligns multimodal understanding and generation using homologous preference data. Extensive experiments demonstrate the significant superiority of our approach over prior methods, particularly in narrowing the gap between multimodal understanding and generation. These findings highlight the potential of HermesFlow as a general alignment framework for next-generation multimodal foundation models. Code: https://github.com/Gen-Verse/HermesFlow",
            "score": 5,
            "issue_id": 2265,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "09e80125d90fd3df",
            "authors": [
                "Ling Yang",
                "Xinchen Zhang",
                "Ye Tian",
                "Chenming Shang",
                "Minghao Xu",
                "Wentao Zhang",
                "Bin Cui"
            ],
            "affiliations": [
                "Mila - Quebec AI Institute",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12148.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#alignment"
                ],
                "emoji": "🌉",
                "ru": {
                    "title": "HermesFlow: мост между пониманием и генерацией в мультимодальных ИИ",
                    "desc": "Статья представляет новый фреймворк HermesFlow для мультимодальных больших языковых моделей (MLLM). Авторы обнаружили, что способности MLLM к пониманию обычно превосходят их генеративные возможности. HermesFlow призван устранить этот разрыв, используя гомологичные данные предпочтений для обучения. Фреймворк применяет методы Pair-DPO и итеративной оптимизации для эффективного выравнивания мультимодального понимания и генерации. Эксперименты показывают превосходство HermesFlow над существующими методами в сокращении разрыва между пониманием и генерацией в MLLM."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Understanding and Generation in MLLMs with HermesFlow",
                    "desc": "This paper discusses the advancements in Multimodal Large Language Models (MLLMs) and identifies a key issue: these models often understand information better than they can generate it. The authors introduce HermesFlow, a new framework that aims to improve the balance between understanding and generation in MLLMs. By using homologous data to create preference data for both tasks, HermesFlow employs Pair-DPO and self-play optimization to align these capabilities more effectively. Experimental results show that HermesFlow significantly reduces the performance gap between understanding and generation, suggesting its potential as a foundational model for future multimodal applications."
                },
                "zh": {
                    "title": "HermesFlow：缩小理解与生成的差距",
                    "desc": "这篇论文探讨了自回归范式在多模态大型语言模型（MLLMs）中的成功，特别是像Show-o、Transfusion和Emu3这样的模型在图像理解和生成方面的进展。研究发现，MLLMs的理解能力通常强于生成能力，两者之间存在显著差距。为了解决这个问题，论文提出了HermesFlow框架，通过使用同源数据来优化理解和生成之间的对齐。实验结果表明，HermesFlow在缩小多模态理解与生成之间的差距方面优于之前的方法，展示了其作为下一代多模态基础模型对齐框架的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11438",
            "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL",
            "url": "https://huggingface.co/papers/2502.11438",
            "abstract": "Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.",
            "score": 4,
            "issue_id": 2264,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "93b545df707b1538",
            "authors": [
                "Jimin Lee",
                "Ingeol Baek",
                "Byeongjeong Kim",
                "Hwanhee Lee"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11438.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#transfer_learning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Самоусиление ИИ в преобразовании текста в SQL",
                    "desc": "SAFE-SQL - это новый подход к преобразованию естественного языка в SQL-запросы. Он использует большие языковые модели для генерации и фильтрации релевантных примеров, улучшая процесс обучения в контексте. SAFE-SQL превосходит предыдущие методы в задачах zero-shot и few-shot, достигая более высокой точности выполнения запросов. Особенно эффективен в сложных и ранее не встречавшихся сценариях, где традиционные методы часто не справляются."
                },
                "en": {
                    "title": "Transforming Language to SQL with Self-Augmented Learning",
                    "desc": "This paper introduces SAFE-SQL, a new framework for converting natural language questions into SQL queries. It addresses the limitations of previous methods that rely on existing training examples, which may not be available in real-world situations. SAFE-SQL enhances SQL generation by creating and filtering self-generated examples using a large language model (LLM). The framework demonstrates improved execution accuracy, especially in challenging and unseen scenarios, outperforming traditional zero-shot and few-shot approaches."
                },
                "zh": {
                    "title": "自我增强，提升Text-to-SQL的准确性",
                    "desc": "本文提出了一种新的框架SAFE-SQL，用于将自然语言问题转换为可执行的SQL查询。该框架通过自我增强的上下文学习和细粒度示例选择来提高SQL生成的质量。SAFE-SQL首先生成多个与测试输入相关的Text-to-SQL示例，然后通过三种相关性评估对这些示例进行过滤，从而构建高质量的学习示例。与传统的零样本和少样本方法相比，SAFE-SQL在执行准确性上取得了显著提升，尤其在困难和未见过的场景中表现更佳。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11275",
            "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
            "url": "https://huggingface.co/papers/2502.11275",
            "abstract": "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token prediction into extraction for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, Cuckoo, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.",
            "score": 3,
            "issue_id": 2263,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "6444052efad6f8be",
            "authors": [
                "Letian Peng",
                "Zilong Wang",
                "Feng Yao",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11275.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#data",
                    "#transfer_learning"
                ],
                "emoji": "🐣",
                "ru": {
                    "title": "Извлечение информации на плечах гигантов: как IE модели могут использовать ресурсы LLM",
                    "desc": "Исследователи представили новый подход к извлечению информации (IE) с использованием ресурсов больших языковых моделей (LLM). Метод под названием 'извлечение следующих токенов' (NTE) позволяет переформулировать задачу предсказания следующего токена в задачу извлечения уже присутствующих в контексте токенов. Модель Cuckoo, обученная на 102,6 млн примеров извлекательных данных, показывает лучшие результаты в условиях малого количества обучающих примеров по сравнению с существующими предобученными IE моделями. Этот подход позволяет IE моделям автоматически развиваться вместе с улучшениями в подготовке данных для LLM без дополнительных ручных усилий."
                },
                "en": {
                    "title": "Leveraging LLMs for Enhanced Information Extraction",
                    "desc": "This paper introduces a new approach for information extraction (IE) using large language models (LLMs) as a resource. The authors propose a method called next tokens extraction (NTE), which allows IE models to leverage existing LLM data for training. They present a model named Cuckoo, which is trained on 102.6 million extractive data points derived from LLMs, showing superior performance in few-shot scenarios. Cuckoo's design enables it to adapt to various IE tasks while benefiting from ongoing improvements in LLM training without requiring extra manual data preparation."
                },
                "zh": {
                    "title": "利用LLM提升信息提取模型的性能",
                    "desc": "本文探讨了如何利用大型语言模型（LLM）来提升信息提取（IE）模型的性能。我们提出了一种新的提取方法，称为下一标记提取（NTE），通过将下一个标记预测转化为对上下文中已存在标记的提取，从而使IE模型能够利用LLM的资源。我们开发的Cuckoo模型在少量样本的情况下，能够有效适应传统和复杂的指令跟随IE任务，并且表现优于现有的预训练IE模型。Cuckoo作为一个“搭便车者”，能够随着LLM数据准备的进步而自然演变，无需额外的人工努力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09061",
            "title": "CRANE: Reasoning with constrained LLM generation",
            "url": "https://huggingface.co/papers/2502.09061",
            "abstract": "Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.",
            "score": 2,
            "issue_id": 2264,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "4a44947deeb14cf4",
            "authors": [
                "Debangshu Banerjee",
                "Tarun Suresh",
                "Shubham Ugare",
                "Sasa Misailovic",
                "Gagandeep Singh"
            ],
            "affiliations": [
                "Department of Computer Science, University of Illinois Urbana-Champaign, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09061.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Баланс между ограничениями и рассуждением в языковых моделях",
                    "desc": "Эта статья исследует проблему генерации синтаксически и семантически корректных выходных данных языковыми моделями (LLM) для задач, требующих формального рассуждения. Авторы предлагают теоретическое объяснение, почему строгое ограничение выходных данных LLM может снижать их способности к рассуждению. Они представляют алгоритм CRANE, который балансирует между корректностью ограниченной генерации и гибкостью неограниченной генерации. Эксперименты показывают, что CRANE значительно превосходит существующие методы ограниченного и неограниченного декодирования на сложных задачах символьного рассуждения."
                },
                "en": {
                    "title": "Balancing Correctness and Reasoning in LLM Outputs with CRANE",
                    "desc": "This paper discusses the challenges of generating outputs from large language models (LLMs) that are both correct in form and meaning, especially in tasks like code generation and symbolic reasoning. It explains that overly strict constraints on the grammar can hinder the model's reasoning abilities. The authors propose a new approach called CRANE, which enhances the output grammar with additional rules to maintain reasoning capabilities while ensuring syntactic and semantic correctness. Their experiments show that CRANE outperforms existing methods, achieving significant accuracy improvements on difficult reasoning tasks."
                },
                "zh": {
                    "title": "平衡推理能力与生成正确性的创新解码算法",
                    "desc": "本研究探讨了如何在生成代码和符号数学推理等任务中，确保大型语言模型（LLM）输出的语法和语义正确性。我们发现，过于严格的语法约束会降低模型的推理能力。为了解决这个问题，我们提出了一种新的解码算法CRANE，通过增加精心设计的额外规则，既能保持输出的正确性，又能增强推理能力。实验结果表明，CRANE在多个基准测试中显著优于现有的解码策略，提升了符号推理任务的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11901",
            "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
            "url": "https://huggingface.co/papers/2502.11901",
            "abstract": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
            "score": 2,
            "issue_id": 2263,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "9451c99877c67e4d",
            "authors": [
                "Dylan Zhang",
                "Justin Wang",
                "Tianran Sun"
            ],
            "affiliations": [
                "Shanghai Jiaotong University",
                "University of Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11901.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#data",
                    "#plp",
                    "#transfer_learning",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Синтетические данные открывают новые горизонты в доказательном программировании",
                    "desc": "Статья посвящена проблеме обучения языковых моделей программированию, ориентированному на доказательства. Авторы предлагают метод синтетического расширения данных для решения проблемы нехватки корпусов на языках доказательного программирования. Они создают модель PoPilot, которая превосходит GPT-4 на 64% в задачах проектного уровня. Метод также позволяет улучшить результаты GPT-4 на 54% путем исправления его выходных данных."
                },
                "en": {
                    "title": "Enhancing Proof-Oriented Programming with Synthetic Data Augmentation",
                    "desc": "This paper addresses the challenges faced by language models (LMs) in proof-oriented programming due to limited data availability. It introduces a novel approach of synthetic data augmentation to enhance the training of LMs for generating and repairing proofs in programming languages like F*. The method involves creating basic proof-oriented programming problems and utilizing diverse coding data to improve reasoning capabilities. The results demonstrate that the fine-tuned 14B parameter model, PoPilot, significantly outperforms existing models, including GPT-4o, in project-level proof-oriented programming tasks."
                },
                "zh": {
                    "title": "合成数据增强，提升证明编程能力！",
                    "desc": "现有的语言模型在面向证明的编程中面临数据稀缺的问题，主要体现在两个方面：缺乏足够的面向证明编程语言（如F*）的语料库，以及缺少大规模的项目级证明实现，无法教会模型复杂的推理过程。我们提出了一种基于合成数据增强的方法，专注于项目级的面向证明编程，既用于生成也用于修复。该方法通过合成基本的面向证明编程问题来解决数据稀缺问题，并结合多样化的编码数据以提高推理能力，同时在现有代码库中创建新的证明和修复数据。我们的14B参数模型PoPilot经过微调后，在项目级面向证明编程中超越了GPT-4o模型64%的性能，并通过修复其输出提高了54%的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11775",
            "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
            "url": "https://huggingface.co/papers/2502.11775",
            "abstract": "While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities.",
            "score": 0,
            "issue_id": 2265,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "ff6f52de37532ca7",
            "authors": [
                "Guangzhi Sun",
                "Yudong Yang",
                "Jimin Zhuang",
                "Changli Tang",
                "Yixuan Li",
                "Wei Li",
                "Zejun MA",
                "Chao Zhang"
            ],
            "affiliations": [
                "ByteDance",
                "Tsinghua university",
                "Univeristy of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11775.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшение рассуждений в мультимодальных языковых моделях для понимания видео",
                    "desc": "Статья представляет video-SALMONN-o1 - первую открытую аудиовизуальную языковую модель с улучшенными способностями рассуждения для общего понимания видео. Авторы разработали набор данных с интенсивными рассуждениями и метод оптимизации предпочтений процесса (pDPO) для эффективного обучения на многомодальных входных данных. Также был создан бенчмарк RivaBench для оценки способностей моделей к рассуждению при анализе видео. video-SALMONN-o1 демонстрирует значительные улучшения точности по сравнению с базовыми моделями на различных задачах понимания видео."
                },
                "en": {
                    "title": "Revolutionizing Video Understanding with Enhanced Reasoning",
                    "desc": "This paper introduces video-SALMONN-o1, an innovative open-source audio-visual large language model (LLM) aimed at improving general video understanding. It addresses the gap in reasoning capabilities for video content by creating a specialized dataset with complex audio-visual questions and detailed solutions. The authors also present process direct preference optimization (pDPO), a method that enhances reward modeling for multimodal inputs through contrastive step selection. The model demonstrates significant accuracy improvements over existing benchmarks, showcasing its effectiveness in tasks like synthetic video detection without prior training."
                },
                "zh": {
                    "title": "视频理解的新突破：video-SALMONN-o1",
                    "desc": "这篇论文提出了video-SALMONN-o1，这是第一个开源的增强推理音视频大语言模型，旨在解决一般视频理解任务。为了提升其推理能力，研究团队开发了一个包含具有挑战性的音视频问题和逐步解决方案的推理密集型数据集。论文还提出了过程直接偏好优化（pDPO），利用对比步骤选择实现针对多模态输入的高效步骤级奖励建模。此外，RivaBench作为第一个推理密集型视频理解基准，提供了超过4000个高质量的问题-答案对，涵盖了多种场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11098",
            "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
            "url": "https://huggingface.co/papers/2502.11098",
            "abstract": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose Talk Structurally, Act Hierarchically (TalkHier), a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. TalkHier surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier.",
            "score": 0,
            "issue_id": 2265,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "9aaac2e7c7b495a4",
            "authors": [
                "Zhao Wang",
                "Sota Moriyama",
                "Wei-Yao Wang",
                "Briti Gangopadhyay",
                "Shingo Takamatsu"
            ],
            "affiliations": [
                "Sony Group Corporation, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11098.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#agents",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Структурированное общение и иерархические действия для эффективного взаимодействия ИИ-агентов",
                    "desc": "В статье представлена новая система TalkHier для улучшения взаимодействия между агентами на основе больших языковых моделей (LLM). TalkHier вводит структурированный протокол коммуникации и иерархическую систему уточнения для решения проблем неверных выводов и предвзятости. Система превосходит современные методы в различных задачах, включая ответы на вопросы и генерацию рекламных текстов. TalkHier демонстрирует потенциал для установления нового стандарта в многоагентных системах на основе LLM."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Collaboration with TalkHier Framework",
                    "desc": "This paper introduces Talk Hierarchically, Act Structurally (TalkHier), a new framework designed to improve communication and collaboration among multi-agent systems using large language models (LLMs). It features a structured communication protocol that enhances context understanding and a hierarchical refinement system to correct errors and biases in agent outputs. The framework outperforms existing state-of-the-art models in various tasks, demonstrating its effectiveness in open-domain question answering and targeted text generation. Overall, TalkHier aims to establish a new benchmark for LLM-based multi-agent systems, promoting better teamwork and adaptability among agents."
                },
                "zh": {
                    "title": "结构化交流，分层行动的智能体协作新标准",
                    "desc": "本文提出了一种新的框架，称为Talk Structurally, Act Hierarchically（TalkHier），旨在改善大语言模型（LLM）多智能体系统中的通信和协作。该框架引入了一种结构化的通信协议，以便在复杂任务中进行丰富的上下文交流，并建立了一个分层的精炼系统，以解决错误输出、虚假信息和偏见等问题。实验结果表明，TalkHier在多个任务上超越了现有的最先进技术，包括开放领域问答和特定领域选择性提问等。该研究为LLM-MA系统设定了新的标准，推动了更有效、灵活和协作的多智能体框架的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10454",
            "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs",
            "url": "https://huggingface.co/papers/2502.10454",
            "abstract": "Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of \"proof by counterexamples\" commonly used in human mathematics education, our work aims to enhance LLMs' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.",
            "score": 0,
            "issue_id": 2265,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "1821254437fc158d",
            "authors": [
                "Yinghui Li",
                "Jiayi Kuang",
                "Haojing Huang",
                "Zhikun Xu",
                "Xinnian Liang",
                "Yi Yu",
                "Wenlian Lu",
                "Yangning Li",
                "Xiaoyu Tan",
                "Chao Qu",
                "Ying Shen",
                "Hai-Tao Zheng",
                "Philip S. Yu"
            ],
            "affiliations": [
                "ARC Lab, Arizona State University",
                "Bytedance Inc.",
                "INFLY TECH (Shanghai) Co., Ltd.",
                "Peng Cheng Laboratory",
                "School of Mathematical Science, Fudan University",
                "Sun-Yat Sen University",
                "Tsinghua University",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10454.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Контрпримеры как ключ к улучшению математических способностей ИИ",
                    "desc": "Статья исследует способность больших языковых моделей (LLM) генерировать математические доказательства. Авторы утверждают, что текущие LLM ограничены в глубоком понимании теорем и предлагают метод обучения на контрпримерах. Они создали бенчмарк CounterMATH для оценки способности LLM доказывать утверждения через контрпримеры. Эксперименты показывают, что улучшение навыков рассуждения на основе контрпримеров критически важно для повышения общих математических способностей LLM."
                },
                "en": {
                    "title": "Enhancing LLMs' Mathematical Proofs through Counterexamples",
                    "desc": "This paper discusses the limitations of current Large Language Models (LLMs) in generating mathematical proofs, emphasizing their dependence on prior exposure to proof processes during training. The authors introduce a new benchmark called CounterMATH, which challenges LLMs to prove mathematical statements by providing counterexamples, thereby testing their understanding of mathematical concepts. They also present a data engineering framework to enhance the training data for LLMs, aiming to improve their reasoning capabilities. The findings suggest that enhancing counterexample-driven reasoning is essential for advancing the mathematical proficiency of LLMs."
                },
                "zh": {
                    "title": "通过反例提升数学推理能力",
                    "desc": "本论文探讨了利用大型语言模型（LLMs）进行数学证明生成的能力。我们认为，当前LLMs的证明能力主要依赖于其在训练过程中是否接触过相关的证明过程，这限制了它们对数学定理和相关概念的深入理解。我们提出了一种基于反例的证明方法，旨在通过反例增强LLMs的数学推理和证明能力。为此，我们手动创建了一个高质量的数学基准CounterMATH，以评估LLMs在提供反例时的数学概念掌握情况。"
                }
            }
        }
    ],
    "link_prev": "2025-02-17.html",
    "link_next": "2025-02-19.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2月17日"
    },
    "short_date_next": {
        "ru": "19.02",
        "en": "02/19",
        "zh": "2月19日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 3,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。",
        "title": "Region-Adaptive Sampling for Diffusion Transformers",
        "pinyin": "扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。\n\nkuò sàn mó xíng (DMs) zài gè zhǒng lǐng yù de shēng chéng rèn wù zhōng chéng wéi shǒu xuǎn. rán ér, tā men yī lài duō cì shùn xù qián xiāng chuán dì, xiǎn zhù xiàn zhì le shí shí xìng néng. yǐ qián de jiā sù fāng fǎ zhǔ yào jī zhōng zài jiǎn shǎo cǎi yàng bù zhòu huò chóng yòng zhōng jiān jié guǒ, wèi néng lì yòng tú xiàng nèi bù kōng jiān qū yù de biàn huà. tōng guò lì yòng kuò sàn biàn shū zhǔ (DiTs) chǔ lǐ kě biàn shù liàng de biāo jì de líng huó xìng, wǒ men yǐn rù le RAS, yī zhǒng xīn de wú xū xùn liàn de cǎi yàng cè lüè, gēn jù DiT mó xíng de guān zhù diǎn dòng tài fēn pèi tú xiàng nèi bù tōng qū yù de cǎi yàng bǐ lǜ. wǒ men de guǎn jiàn guān chá shì, zài měi gè cǎi yàng bù zhòu zhōng, mó xíng jí zhōng zài yǔ yì shàng yǒu yì yì de qū yù, zhè xiē guān zhù qū yù zài lián xù bù zhòu zhōng biǎo xiàn chū qiáng dà de lián xù xìng. lì yòng zhè yī dòng chá, RAS jǐn gēng xīn shǐ dāng qián guān zhù de qū yù, ér qí tā qū yù shǐ yòng shàng yī bù de huǎn cùn zào shēng gēng xīn. mó xíng de guān zhù diǎn gēn jù qián yī bù de shū chū què dìng, lì yòng le wǒ men guān chá dào de shí jiān yī zhì xìng. wǒ men zài Stable Diffusion 3 hé Lumina-Next-T2I shàng píng guǎ RAS, fēn bié shí xiàn le zuì gāo 2.36 bèi hé 2.51 bèi de jiā sù, shēng chéng zhì liàng jǐn qīng wēi xià jiàng. cǐ wài, yòng hù yán jiū biǎo míng, RAS zài rén lèi píng jià xià tí gōng le xiāng sì de zhì liàng, tóng shí shí xiàn le 1.6 bèi de jiā sù. wǒ men de fāng fǎ zài gèng gāo xiào de kuò sàn biàn shū zhǔ fāng miàn zhǔ dé dào le zhòng yào jìn zhǎn, zēng qiáng le tā men zài shí shí yìng yòng zhōng de qián lì.",
        "vocab": "[\n    {\"word\": \"扩散模型\", \"pinyin\": \"kuò sàn mó xíng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"首选\", \"pinyin\": \"shǒu xuǎn\", \"trans\": \"preferred choice\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"顺序\", \"pinyin\": \"shùn xù\", \"trans\": \"sequential\"},\n    {\"word\": \"前向传递\", \"pinyin\": \"qián xiàng chuán dì\", \"trans\": \"forward pass\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"实时性能\", \"pinyin\": \"shí shí xìng néng\", \"trans\": \"real-time performance\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"accelerate\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"集中\", \"pinyin\": \"jí zhōng\", \"trans\": \"focus on\"},\n    {\"word\": \"减少\", \"pinyin\": \"jiǎn shǎo\", \"trans\": \"reduce\"},\n    {\"word\": \"采样步骤\", \"pinyin\": \"cǎi yàng bù zhòu\", \"trans\": \"sampling steps\"},\n    {\"word\": \"重用\", \"pinyin\": \"chóng yòng\", \"trans\": \"reuse\"},\n    {\"word\": \"中间结果\", \"pinyin\": \"zhōng jiān jié guǒ\", \"trans\": \"intermediate results\"},\n    {\"word\": \"利用\", \"pinyin\": \"lì yòng\", \"trans\": \"utilize\"},\n    {\"word\": \"图像\", \"pinyin\": \"tú xiàng\", \"trans\": \"image\"},\n    {\"word\": \"内部空间区域\", \"pinyin\": \"nèi bù kōng jiān qū yù\", \"trans\": \"internal spatial regions\"},\n    {\"word\": \"变化\", \"pinyin\": \"biàn huà\", \"trans\": \"change\"},\n    {\"word\": \"扩散变压器\", \"pinyin\": \"kuò sàn biàn yā qì\", \"trans\": \"diffusion transformer\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"RAS\", \"pinyin\": \"RAS\", \"trans\": \"RAS\"},\n    {\"word\": \"采样策略\", \"pinyin\": \"cǎi yàng cè lüè\", \"trans\": \"sampling strategy\"},\n    {\"word\": \"动态分配\", \"pinyin\": \"dòng tài fēn pèi\", \"trans\": \"dynamic allocation\"},\n    {\"word\": \"关注点\", \"pinyin\": \"guān zhù diǎn\", \"trans\": \"focus points\"},\n    {\"word\": \"关键观察\", \"pinyin\": \"guǎn jiàn guān chá\", \"trans\": \"key observation\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantic\"},\n    {\"word\": \"有意义\", \"pinyin\": \"yǒu yì yì\", \"trans\": \"meaningful\"},\n    {\"word\": \"连续步骤\", \"pinyin\": \"lián xù bù zhòu\", \"trans\": \"continuous steps\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"连续性\", \"pinyin\": \"lián xù xìng\", \"trans\": \"continuity\"},\n    {\"word\": \"洞察\", \"pinyin\": \"dòng chá\", \"trans\": \"insight\"},\n    {\"word\": \"更新\", \"pinyin\": \"gēng xīn\", \"trans\": \"update\"},\n    {\"word\": \"缓存噪声\", \"pinyin\": \"huǎn cún zào shēng\", \"trans\": \"cached noise\"},\n    {\"word\": \"确定\", \"pinyin\": \"què dìng\", \"trans\": \"determine\"},\n    {\"word\": \"时间一致性\", \"pinyin\": \"shí jiān yī zhì xìng\", \"trans\": \"temporal consistency\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"Stable Diffusion 3\", \"pinyin\": \"Stable Diffusion 3\", \"trans\": \"Stable Diffusion 3\"},\n    {\"word\": \"Lumina-Next-T2I\", \"pinyin\": \"Lumina-Next-T2I\", \"trans\": \"Lumina-Next-T2I\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"acceleration\"},\n    {\"word\": \"生成质量\", \"pinyin\": \"shēng chéng zhì liàng\", \"trans\": \"generation quality\"},\n    {\"word\": \"轻微下降\", \"pinyin\": \"qīng wēi xià jiàng\", \"trans\": \"slight decrease\"},\n    {\"word\": \"用户研究\", \"pinyin\": \"yòng hù yán jiū\", \"trans\": \"user study\"},\n    {\"word\": \"人类评估\", \"pinyin\": \"rén lèi píng gū\", \"trans\": \"human evaluation\"},\n    {\"word\": \"相似\", \"pinyin\": \"xiāng sì\", \"trans\": \"similar\"},\n    {\"word\": \"潜力\", \"pinyin\": \"qián lì\", \"trans\": \"potential\"},\n    {\"word\": \"重要进展\", \"pinyin\": \"zhòng yào jìn zhǎn\", \"trans\": \"significant progress\"}\n]",
        "trans": "Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications.",
        "update_ts": "2025-02-17 09:12"
    }
}