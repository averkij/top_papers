{
    "date": {
        "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 10",
        "zh": "2æœˆ10æ—¥"
    },
    "time_utc": "2025-02-10 16:12",
    "weekday": 0,
    "issue_id": 2130,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.05173",
            "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
            "url": "https://huggingface.co/papers/2502.05173",
            "abstract": "While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.",
            "score": 44,
            "issue_id": 2118,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "ba284ed1a62b3c2c",
            "authors": [
                "Xilin Wei",
                "Xiaoran Liu",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Cao",
                "Jian Tong",
                "Haodong Duan",
                "Qipeng Guo",
                "Jiaqi Wang",
                "Xipeng Qiu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Fudan University, Shanghai, China",
                "Shanghai AI Laboratory, Shanghai, China",
                "Shanghai Innovation Institute, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05173.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#3d",
                    "#architecture",
                    "#video",
                    "#long_context"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VideoRoPE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoRoPE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Rotary Position Embedding. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ 4 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ RoPE Ğº Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ V-NIAH-D Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² RoPE. VideoRoPE Ğ¸Ğ¼ĞµĞµÑ‚ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ RoPE Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Understanding with VideoRoPE",
                    "desc": "This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data."
                },
                "zh": {
                    "title": "VideoRoPEï¼šè§†é¢‘ä¸­çš„æ—‹è½¬ä½ç½®åµŒå…¥æ–°çªç ´",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•å°†æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰æœ‰æ•ˆåœ°æ‰©å±•åˆ°è§†é¢‘æ•°æ®ä¸­ã€‚ç ”ç©¶åˆ†æäº†å››ä¸ªå…³é”®ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§å¯¹äºRoPEåœ¨è§†é¢‘ä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡V-NIAH-Dï¼Œå±•ç¤ºäº†ç°æœ‰RoPEå˜ä½“åœ¨å¤„ç†è§†é¢‘æ—¶å®¹æ˜“å—åˆ°å¹²æ‰°çš„ç¼ºé™·ã€‚åŸºäºè¿™äº›åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†VideoRoPEï¼Œå®ƒé€šè¿‡3Dç»“æ„æ¥ä¿æŒæ—¶ç©ºå…³ç³»ï¼Œå¹¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¹‹å‰çš„RoPEå˜ä½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04507",
            "title": "Fast Video Generation with Sliding Tile Attention",
            "url": "https://huggingface.co/papers/2502.04507",
            "abstract": "Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.",
            "score": 34,
            "issue_id": 2120,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "dcbf1070dac1b391",
            "authors": [
                "Peiyuan Zhang",
                "Yongqi Chen",
                "Runlong Su",
                "Hangliang Ding",
                "Ion Stoica",
                "Zhenghong Liu",
                "Hao Zhang"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence",
                "Tsinghua University",
                "University of California, Berkeley",
                "University of California, San Diego",
                "University of Michigan, Ann Arbor"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04507.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ¸Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ¸Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (STA) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². STA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ¾ĞºĞ½Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. STA ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² 2.8-17 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FlashAttention-2 Ğ¸ Ğ² 1.6-10 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FlashAttention-3, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Efficient Video Generation with Sliding Tile Attention",
                    "desc": "This paper presents a new method called sliding tile attention (STA) to improve the efficiency of video generation using Diffusion Transformers (DiTs). Traditional full attention mechanisms are computationally expensive, especially for generating high-resolution videos, leading to long inference times. STA reduces this cost by focusing on localized 3D windows, allowing for faster processing without sacrificing the quality of the generated videos. The implementation of STA achieves significant speedups in attention computation, making it a promising solution for real-time video generation tasks."
                },
                "zh": {
                    "title": "æ»‘åŠ¨ç“¦ç‰‡æ³¨æ„åŠ›ï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ»‘åŠ¨ç“¦ç‰‡æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSTAï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡ã€‚ä¼ ç»Ÿçš„æ‰©æ•£å˜æ¢å™¨åœ¨ç”Ÿæˆè§†é¢‘æ—¶è®¡ç®—æˆæœ¬é«˜ï¼Œè€ŒSTAé€šè¿‡å…³æ³¨å±€éƒ¨çš„æ—¶ç©ºåŒºåŸŸæ¥å‡å°‘å†—ä½™è®¡ç®—ã€‚ä¸ä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸åŒï¼ŒSTAé‡‡ç”¨äº†ç¡¬ä»¶å‹å¥½çš„è®¾è®¡ï¼Œé€å—å¤„ç†ï¼Œä¿æŒäº†è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚ç»è¿‡ä¼˜åŒ–ï¼ŒSTAåœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—åŠ é€Ÿäº†æ³¨æ„åŠ›è®¡ç®—ï¼Œé™ä½äº†å»¶è¿Ÿï¼ŒåŒæ—¶ä¸å½±å“ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05003",
            "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
            "url": "https://huggingface.co/papers/2502.05003",
            "abstract": "One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the \"optimal\" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations.   We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the \"true\" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.",
            "score": 24,
            "issue_id": 2122,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "c011c3548ad7a5dd",
            "authors": [
                "Andrei Panferov",
                "Jiale Chen",
                "Soroush Tabesh",
                "Roberto L. Castro",
                "Mahdi Nikdan",
                "Dan Alistarh"
            ],
            "affiliations": [
                "ISTA",
                "Red Hat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05003.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ QuEST Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. QuEST Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² 4 Ğ±Ğ¸Ñ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FP16. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ QuEST Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "QuEST: Revolutionizing Quantization for Efficient Language Models",
                    "desc": "This paper introduces QuEST, a novel method for training large language models (LLMs) using quantization-aware training (QAT) with significantly reduced bit-widths. QuEST achieves competitive accuracy with traditional FP16 precision while utilizing weights and activations as low as 4 bits, and even supports stable training with 1-bit representations. The method enhances QAT by employing Hadamard normalization for better quantization and a new trust gradient estimator to minimize errors in gradient calculations. Experiments demonstrate that QuEST maintains stable performance across various hardware-supported precisions and can be efficiently executed with provided GPU kernel support."
                },
                "zh": {
                    "title": "QuESTï¼šä½ç²¾åº¦é«˜æ•ˆè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºQuESTçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚QuESTèƒ½å¤Ÿåœ¨4ä½æˆ–æ›´ä½çš„ç²¾åº¦ä¸‹è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒä¸FP16ç²¾åº¦ç›¸å½“çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ”¹è¿›æƒé‡å’Œæ¿€æ´»çš„é‡åŒ–è¿‡ç¨‹ï¼Œä»¥åŠå¼•å…¥æ–°çš„ä¿¡ä»»æ¢¯åº¦ä¼°è®¡å™¨ï¼Œæ¥å®ç°æ›´ç¨³å®šçš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuESTåœ¨å„ç§ç¡¬ä»¶æ”¯æŒçš„ç²¾åº¦èŒƒå›´å†…éƒ½èƒ½å®ç°ç¨³å®šçš„æ‰©å±•æ€§ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆæ‰§è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04896",
            "title": "Goku: Flow Based Video Generative Foundation Models",
            "url": "https://huggingface.co/papers/2502.04896",
            "abstract": "This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.",
            "score": 23,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "ad6ef6eed233cc90",
            "authors": [
                "Shoufa Chen",
                "Chongjian Ge",
                "Yuqi Zhang",
                "Yida Zhang",
                "Fengda Zhu",
                "Hao Yang",
                "Hongxiang Hao",
                "Hui Wu",
                "Zhichao Lai",
                "Yifei Hu",
                "Ting-Che Lin",
                "Shilong Zhang",
                "Fu Li",
                "Chuan Li",
                "Xing Wang",
                "Yanghua Peng",
                "Peize Sun",
                "Ping Luo",
                "Yi Jiang",
                "Zehuan Yuan",
                "Bingyue Peng",
                "Xiaobing Liu"
            ],
            "affiliations": [
                "Bytedance Inc",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04896.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#video",
                    "#architecture",
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ‰",
                "ru": {
                    "title": "Goku: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Goku Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Goku Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Goku: Revolutionizing Image and Video Generation with Transformers",
                    "desc": "This paper presents Goku, a cutting-edge model for generating images and videos using rectified flow Transformers. It discusses key components that contribute to its high-quality output, such as the data curation process and the design of the model architecture. Goku sets new performance records in various tasks, achieving impressive scores on benchmarks for both text-to-image and text-to-video generation. The authors aim to offer insights and advancements that will benefit researchers working on similar generation models."
                },
                "zh": {
                    "title": "Gokuï¼šå›¾åƒä¸è§†é¢‘ç”Ÿæˆçš„æ–°æ ‡æ†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Gokuï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„è”åˆå›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨äº†ä¿®æ­£æµTransformerä»¥å®ç°è¡Œä¸šé¢†å…ˆçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¯¦ç»†é˜è¿°äº†é«˜è´¨é‡è§†è§‰ç”Ÿæˆçš„åŸºç¡€è¦ç´ ï¼ŒåŒ…æ‹¬æ•°æ®æ•´ç†æµç¨‹ã€æ¨¡å‹æ¶æ„è®¾è®¡ã€æµçš„å…¬å¼åŒ–ä»¥åŠé«˜æ•ˆç¨³å¥çš„å¤§è§„æ¨¡è®­ç»ƒåŸºç¡€è®¾æ–½ã€‚Gokuæ¨¡å‹åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä¸ºä¸»è¦ä»»åŠ¡è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚å…·ä½“è€Œè¨€ï¼ŒGokuåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­è¾¾åˆ°äº†0.76çš„GenEvalå’Œ83.65çš„DPG-Benchï¼Œåœ¨æ–‡æœ¬åˆ°è§†é¢‘ä»»åŠ¡ä¸­è¾¾åˆ°äº†84.85çš„VBenchã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05176",
            "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360Â° Unbounded Scene Inpainting",
            "url": "https://huggingface.co/papers/2502.05176",
            "abstract": "Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.",
            "score": 22,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "9b52f2788f53c3c0",
            "authors": [
                "Chung-Ho Wu",
                "Yang-Jung Chen",
                "Ying-Huan Chen",
                "Jie-Ying Lee",
                "Bo-Hsu Ke",
                "Chun-Wei Tuan Mu",
                "Yi-Chuan Huang",
                "Chin-Yang Lin",
                "Min-Hung Chen",
                "Yen-Yu Lin",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "NVIDIA",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05176.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸: AuraFusion360 Ğ´Ğ»Ñ Ğ±ĞµĞ·ÑƒĞ¿Ñ€ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½",
                    "desc": "AuraFusion360 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Gaussian Splatting. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SDEdit Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 360-USID Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ¾Ğ¼ 360 Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Inpainting with AuraFusion360",
                    "desc": "AuraFusion360 is a new method for filling in missing parts of 3D scenes, which is important for applications like virtual reality. It uses Gaussian Splatting to represent scenes and introduces techniques for better identifying what should be filled in, ensuring that the filled areas look realistic from different angles. The method also places points accurately without needing extra training and enhances details to keep the views consistent. Additionally, it comes with a new dataset for testing these 3D inpainting methods, showing that AuraFusion360 performs better than previous techniques in both quality and accuracy."
                },
                "zh": {
                    "title": "AuraFusion360ï¼šä¸‰ç»´åœºæ™¯ä¿®å¤çš„æ–°çªç ´",
                    "desc": "ä¸‰ç»´åœºæ™¯ä¿®å¤åœ¨è™šæ‹Ÿç°å®å’Œå»ºç­‘å¯è§†åŒ–ç­‰åº”ç”¨ä¸­éå¸¸é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨360åº¦æ— ç•Œåœºæ™¯ä¸­é¢ä¸´è§†å›¾ä¸€è‡´æ€§å’Œå‡ ä½•ç²¾åº¦çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†AuraFusion360ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºå‚è€ƒçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é«˜è´¨é‡çš„3Dåœºæ™¯ä¸­è¿›è¡Œç‰©ä½“ç§»é™¤å’Œå­”å¡«å……ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ·±åº¦æ„ŸçŸ¥çš„æœªè§æ©ç ç”Ÿæˆã€é€‚åº”æ€§å¼•å¯¼æ·±åº¦æ‰©æ•£å’ŒåŸºäºSDEditçš„ç»†èŠ‚å¢å¼ºï¼Œç¡®ä¿å¤šè§†å›¾çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒAuraFusion360åœ¨æ„ŸçŸ¥è´¨é‡å’Œå‡ ä½•ç²¾åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‰§çƒˆè§†è§’å˜åŒ–ä¸­ä¿æŒé«˜è´¨é‡çš„ä¿®å¤æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05171",
            "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
            "url": "https://huggingface.co/papers/2502.05171",
            "abstract": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.",
            "score": 20,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "4386159312d9856b",
            "authors": [
                "Jonas Geiping",
                "Sean McLeish",
                "Neel Jain",
                "John Kirchenbauer",
                "Siddharth Singh",
                "Brian R. Bartoldson",
                "Bhavya Kailkhura",
                "Abhinav Bhatele",
                "Tom Goldstein"
            ],
            "affiliations": [
                "ELLIS Institute TÃ¼bingen, Max-Planck Institute for Intelligent Systems, TÃ¼bingen AI Center",
                "Lawrence Livermore National Laboratory",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05171.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ°, Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ÑÑÑŒ Ğ´Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ½Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ 3,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ 800 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Scaling Reasoning with Latent Space Computation",
                    "desc": "This paper presents a new language model architecture that enhances reasoning capabilities by performing computations in a latent space during test-time. Instead of generating more tokens to scale reasoning, the model uses a recurrent block that allows it to unroll computations to any depth. This method does not rely on specialized training data and can effectively handle small context windows, enabling it to capture complex reasoning patterns. The authors demonstrate that their model, with 3.5 billion parameters, can achieve significant improvements on reasoning tasks, comparable to models with much larger parameter counts."
                },
                "zh": {
                    "title": "éšå¼æ¨ç†ï¼Œæå‡è¯­è¨€æ¨¡å‹çš„è®¡ç®—èƒ½åŠ›",
                    "desc": "æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§æ–°é¢–çš„è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­éšå¼æ¨ç†æ¥æ‰©å±•æµ‹è¯•æ—¶çš„è®¡ç®—èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡è¿­ä»£é€’å½’å—å·¥ä½œï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶å¯ä»¥å±•å¼€åˆ°ä»»æ„æ·±åº¦ã€‚è¿™ä¸ä¸»æµæ¨ç†æ¨¡å‹ä¸åŒï¼Œåè€…é€šè¿‡ç”Ÿæˆæ›´å¤šçš„æ ‡è®°æ¥å¢åŠ è®¡ç®—é‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸éœ€è¦ç‰¹æ®Šçš„è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿå¤„ç†å°çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ•æ‰ä¸æ˜“ç”¨è¯­è¨€è¡¨ç¤ºçš„æ¨ç†ç±»å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05163",
            "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails",
            "url": "https://huggingface.co/papers/2502.05163",
            "abstract": "The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \\ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard.",
            "score": 14,
            "issue_id": 2120,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "ae863d89ab71ab51",
            "authors": [
                "Yihe Deng",
                "Yu Yang",
                "Junkai Zhang",
                "Wei Wang",
                "Bo Li"
            ],
            "affiliations": [
                "University of California, Los Angeles",
                "University of Illinois at Urbana-Champaign",
                "VirtueAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05163.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#inference",
                    "#synthetic",
                    "#dataset",
                    "#open_source",
                    "#multilingual",
                    "#rl"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ framework Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾ ĞºĞ°Ğº Ğ¸Ğ³Ñ€Ğ° Ğ´Ğ²ÑƒÑ… Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ² Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğº Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸Ñ ĞÑÑˆĞ°. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multilingual Safety in LLMs with Synthetic Data Generation",
                    "desc": "This paper introduces a new method for creating guardrail models that help ensure the safe use of large language models (LLMs) by detecting harmful content in multiple languages. The authors propose a two-player Reinforcement Learning framework where a generator and a guardrail model work together in a competitive manner to create high-quality synthetic safety data. They demonstrate that this approach not only improves performance on English safety benchmarks but also significantly enhances the model's ability to handle lower-resource languages. The results show that their method is faster and more efficient, making it a promising solution for developing multilingual safety measures in LLMs."
                },
                "zh": {
                    "title": "å¤šè¯­è¨€æŠ¤æ æ¨¡å‹çš„åˆ›æ–°è¿›å±•",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç¡®ä¿å…¶è´Ÿè´£ä»»ä½¿ç”¨çš„æŠ¤æ æ¨¡å‹éœ€æ±‚å¢åŠ ï¼Œå°¤å…¶æ˜¯åœ¨æ£€æµ‹ä¸å®‰å…¨å’Œéæ³•å†…å®¹æ–¹é¢ã€‚è™½ç„¶è‹±è¯­çš„å®‰å…¨æ•°æ®ç›¸å¯¹ä¸°å¯Œï¼Œä½†ç”±äºå…¶ä»–è¯­è¨€å¼€æ”¾æºä»£ç å®‰å…¨æ•°æ®çš„ç¨€ç¼ºï¼Œå¤šè¯­è¨€æŠ¤æ å»ºæ¨¡ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒç©å®¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…¶ä¸­ç”Ÿæˆå™¨å’ŒæŠ¤æ æ¨¡å‹å¯¹æŠ—æ€§åœ°å…±åŒè¿›åŒ–ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæ•°æ®ç”¨äºå¤šè¯­è¨€æŠ¤æ è®­ç»ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šè¯­è¨€å®‰å…¨ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä½èµ„æºè¯­è¨€çš„ä¸å¹³è¡¡é—®é¢˜ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04403",
            "title": "Agency Is Frame-Dependent",
            "url": "https://huggingface.co/papers/2502.04403",
            "abstract": "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.",
            "score": 11,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "32ceb8df4d77794a",
            "authors": [
                "David Abel",
                "AndrÃ© Barreto",
                "Michael Bowling",
                "Will Dabney",
                "Shi Dong",
                "Steven Hansen",
                "Anna Harutyunyan",
                "Khimya Khetarpal",
                "Clare Lyle",
                "Razvan Pascanu",
                "Georgios Piliouras",
                "Doina Precup",
                "Jonathan Richens",
                "Mark Rowland",
                "Tom Schaul",
                "Satinder Singh"
            ],
            "affiliations": [
                "Amii, University of Alberta",
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04403.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agi",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ²ÑĞµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ‚ÑÑ‡ĞµÑ‚Ğ°. ĞĞ½Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ¾Ñ‚ Ñ‚ĞµĞ·Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡ĞµÑ‚Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ‚ÑÑ‡ĞµÑ‚Ğ° Ğ² Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Agency in Reinforcement Learning: A Frame-Dependent Perspective",
                    "desc": "This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning."
                },
                "zh": {
                    "title": "èƒ½åŠ¨æ€§ï¼šä¾èµ–äºæ¡†æ¶çš„ç³»ç»Ÿèƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ç³»ç»Ÿçš„èƒ½åŠ¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ çš„èƒŒæ™¯ä¸‹ã€‚èƒ½åŠ¨æ€§æ˜¯æŒ‡ç³»ç»Ÿæœç€ç›®æ ‡å¼•å¯¼ç»“æœçš„èƒ½åŠ›ï¼Œä½†åˆ¤æ–­ä¸€ä¸ªç³»ç»Ÿæ˜¯å¦å…·å¤‡èƒ½åŠ¨æ€§æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œèƒ½åŠ¨æ€§æ˜¯ä¾èµ–äºå‚è€ƒæ¡†æ¶çš„ï¼Œä»»ä½•å¯¹ç³»ç»Ÿèƒ½åŠ¨æ€§çš„æµ‹é‡éƒ½å¿…é¡»ç›¸å¯¹äºæŸä¸ªå‚è€ƒæ¡†æ¶è¿›è¡Œã€‚é€šè¿‡å“²å­¦è®ºè¯ï¼Œæˆ‘ä»¬æ”¯æŒè¿™ä¸€è§‚ç‚¹ï¼Œå¹¶è®¨è®ºäº†è¿™ä¸€ç»“è®ºå¯¹å¼ºåŒ–å­¦ä¹ çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05179",
            "title": "FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation",
            "url": "https://huggingface.co/papers/2502.05179",
            "abstract": "DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .",
            "score": 8,
            "issue_id": 2120,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "c3147244e03af4a6",
            "authors": [
                "Shilong Zhang",
                "Wenbo Li",
                "Shoufa Chen",
                "Chongjian Ge",
                "Peize Sun",
                "Yida Zhang",
                "Yi Jiang",
                "Zehuan Yuan",
                "Binyue Peng",
                "Ping Luo"
            ],
            "affiliations": [
                "ByteDance",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05179.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FlashVideo. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸."
                },
                "en": {
                    "title": "FlashVideo: Efficient Text-to-Video Generation with Two-Stage Framework",
                    "desc": "The paper introduces FlashVideo, a two-stage framework for text-to-video generation that improves efficiency and quality. In the first stage, it focuses on generating low-resolution videos with high fidelity to text prompts, using large model parameters and sufficient function evaluations. The second stage enhances the video by matching flow between low and high resolutions, adding fine details while minimizing computational demands. This approach not only achieves high-resolution outputs with better efficiency but also allows users to preview results before full generation, making it more practical for commercial use."
                },
                "zh": {
                    "title": "FlashVideoï¼šé«˜æ•ˆç”Ÿæˆé«˜åˆ†è¾¨ç‡è§†é¢‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "DiTæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†é«˜å†…å®¹å’Œè¿åŠ¨ä¿çœŸåº¦é€šå¸¸éœ€è¦å¤§é‡æ¨¡å‹å‚æ•°å’Œå‡½æ•°è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™äº›è®¡ç®—éœ€æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæ¡†æ¶FlashVideoï¼Œæ—¨åœ¨å¹³è¡¡ç”Ÿæˆçš„ä¿çœŸåº¦å’Œè´¨é‡ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡ä½åˆ†è¾¨ç‡ç”Ÿæˆè¿‡ç¨‹ä¼˜å…ˆè€ƒè™‘æç¤ºä¿çœŸåº¦ï¼Œåˆ©ç”¨å¤§å‚æ•°å’Œè¶³å¤Ÿçš„å‡½æ•°è¯„ä¼°æé«˜è®¡ç®—æ•ˆç‡ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åœ¨ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡ä¹‹é—´å»ºç«‹æµåŒ¹é…ï¼Œæœ‰æ•ˆç”Ÿæˆç»†èŠ‚ï¼Œä¸”æ‰€éœ€çš„å‡½æ•°è¯„ä¼°æœ€å°åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04728",
            "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
            "url": "https://huggingface.co/papers/2502.04728",
            "abstract": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.",
            "score": 8,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "cd97668cd0eee0ee",
            "authors": [
                "Zhouliang Yu",
                "Yuhuan Yuan",
                "Tim Z. Xiao",
                "Fuxiang Frank Xia",
                "Jie Fu",
                "Ge Zhang",
                "Ge Lin",
                "Weiyang Liu"
            ],
            "affiliations": [
                "Environmental Systems Research Institute, Inc.",
                "Max Planck Institute for Intelligent Systems, TÃ¼bingen",
                "SEED, Bytedance",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04728.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ PDDL Ğ¸ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (PDDL) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Best-of-N Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² PDDL Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ."
                },
                "en": {
                    "title": "Enhancing LLMs for Optimal Planning with PDDL",
                    "desc": "This paper addresses the challenge of using Large Language Models (LLMs) for complex planning problems by introducing a method to generate Planning Domain Definition Language (PDDL) domains. PDDL serves as a formal language that helps in creating precise state descriptions, which is crucial for avoiding rule violations and ensuring optimal planning. The authors propose a novel algorithm that enhances LLMs' reasoning capabilities through a Best-of-N sampling approach, followed by fine-grained refinement using verbalized machine learning techniques. Their approach significantly improves the generation of PDDL domains, achieving over 50% success in generating high-quality plans from natural language descriptions without additional training."
                },
                "zh": {
                    "title": "åˆ©ç”¨PDDLæå‡è§„åˆ’é—®é¢˜è§£å†³èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³å¤æ‚çš„è§„åˆ’é—®é¢˜ã€‚ä¸ºäº†é¿å…è§„åˆ™è¿åå’Œç¡®ä¿æœ€ä¼˜æ€§ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰ï¼Œä½œä¸ºä¸€ç§ç²¾ç¡®çš„çŠ¶æ€æè¿°å·¥å…·ã€‚é€šè¿‡PDDLï¼Œå¯ä»¥ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶åº”ç”¨ç»å…¸æœç´¢ç®—æ³•ï¼ˆå¦‚A*ï¼‰æ¥å¯»æ‰¾æœ€ä¼˜è®¡åˆ’ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç®—æ³•ï¼Œé€šè¿‡Best-of-Né‡‡æ ·å’Œç»†è‡´çš„æœºå™¨å­¦ä¹ ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†PDDLé¢†åŸŸçš„ç”Ÿæˆè´¨é‡ï¼ŒæˆåŠŸç‡è¶…è¿‡50%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04959",
            "title": "No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces",
            "url": "https://huggingface.co/papers/2502.04959",
            "abstract": "Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .",
            "score": 6,
            "issue_id": 2127,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "a73679e79ff14b7c",
            "authors": [
                "Daniel Marczak",
                "Simone Magistri",
                "Sebastian Cygert",
                "BartÅ‚omiej Twardowski",
                "Andrew D. Bagdanov",
                "Joost van de Weijer"
            ],
            "affiliations": [
                "Computer Vision Center, Barcelona, Spain",
                "Department of Computer Science, Universitat Autonoma de Barcelona, Spain",
                "Department of Information Engineering, University of Florence, Italy",
                "Gdansk University of Technology, Poland",
                "IDEAS NCBR, Warsaw, Poland",
                "Warsaw University of Technology, Poland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04959.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ² Ğ¾Ğ´Ğ½Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ·Ğ¾Ñ‚Ñ€Ğ¾Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Performance Gap in Model Merging",
                    "desc": "This paper focuses on model merging, which combines the weights of different models designed for specific tasks into one model that can handle multiple tasks. The authors identify that there is often a performance gap when comparing the merged model to single-task models. They explore the importance of task matrices, which are weight update matrices, and find that better alignment between the components of these matrices leads to improved performance. To address the performance gap, they introduce an isotropic merging framework that optimizes the alignment of task matrices and incorporates both shared and unique features of tasks, achieving top performance across various scenarios."
                },
                "zh": {
                    "title": "æœ‰æ•ˆçš„æ¨¡å‹åˆå¹¶æ–¹æ³•æå‡å¤šä»»åŠ¡æ€§èƒ½",
                    "desc": "æ¨¡å‹åˆå¹¶æ˜¯å°†å¤šä¸ªç‰¹å®šä»»åŠ¡æ¨¡å‹çš„æƒé‡æ•´åˆä¸ºä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹çš„è¿‡ç¨‹ã€‚å°½ç®¡è¿™ä¸€é—®é¢˜å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œä½†åˆå¹¶æ¨¡å‹ä¸å•ä»»åŠ¡æ¨¡å‹ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æœ¬æ–‡ç ”ç©¶äº†ä»»åŠ¡çŸ©é˜µçš„å…³é”®ç‰¹æ€§ï¼Œè¿™äº›çŸ©é˜µæ˜¯åº”ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡æ›´æ–°çŸ©é˜µï¼Œå‘ç°ä»»åŠ¡ç‰¹å®šçŸ©é˜µä¸åˆå¹¶çŸ©é˜µä¹‹é—´çš„å¯¹é½ç¨‹åº¦ä¸æ€§èƒ½æå‡å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å„å‘åŒæ€§åˆå¹¶æ¡†æ¶ï¼Œé€šè¿‡å¹³å¦åŒ–ä»»åŠ¡çŸ©é˜µçš„å¥‡å¼‚å€¼è°±ï¼Œå¢å¼ºå¯¹é½æ€§ï¼Œä»è€Œç¼©å°æ€§èƒ½å·®è·ï¼Œå¹¶åœ¨å¤šä¸ªåœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04520",
            "title": "Linear Correlation in LM's Compositional Generalization and Hallucination",
            "url": "https://huggingface.co/papers/2502.04520",
            "abstract": "The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., \"X lives in the city of\" rightarrow \"X lives in the country of\" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM's generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.",
            "score": 6,
            "issue_id": 2119,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "41ef9027d1533f06",
            "authors": [
                "Letian Peng",
                "Chenyang An",
                "Shibo Hao",
                "Chengyu Dong",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04520.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#architecture",
                    "#agi",
                    "#data",
                    "#hallucinations"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğº Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼Ñƒ. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ."
                },
                "en": {
                    "title": "Unlocking Language Models: The Power of Linear Correlations in Knowledge Composition",
                    "desc": "This paper explores how language models (LMs) handle knowledge composition, revealing that they exhibit linear correlations when predicting the next token. It shows that there is a linear transformation that connects related knowledge, allowing LMs to generalize information effectively, similar to how humans relate concepts. The study finds that while these linear transformations can adapt to new information through fine-tuning, they can also lead to incorrect outputs, or hallucinations, when the relationships are not aligned with reality. Overall, the research suggests that understanding these linear correlations can help identify how well LMs generalize knowledge."
                },
                "zh": {
                    "title": "è¯­è¨€æ¨¡å‹çš„çº¿æ€§ç›¸å…³æ€§ä¸çŸ¥è¯†ç»„åˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨çŸ¥è¯†ç»„åˆä¸­çš„çº¿æ€§ç›¸å…³ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼ŒæŸäº›ç›¸å…³çŸ¥è¯†ä¹‹é—´å­˜åœ¨çº¿æ€§å˜æ¢ï¼Œè¿™ç§å˜æ¢å¯ä»¥å°†ä¸€ä¸ªæç¤ºçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä»ä¸€ä¸ªæ˜ å°„åˆ°å¦ä¸€ä¸ªã€‚æ¯”å¦‚ï¼Œä»\"X ä½åœ¨åŸå¸‚\"å¯ä»¥è½¬å˜ä¸º\"X ä½åœ¨å›½å®¶\"ã€‚ç»“æœè¡¨æ˜ï¼Œçº¿æ€§å˜æ¢åœ¨å¤§è§„æ¨¡å¾®è°ƒä¸­å…·æœ‰éŸ§æ€§ï¼Œå¹¶ä¸”å½“ä¸ç°å®ä¸–ç•Œå…³ç³»ä¸€è‡´æ—¶èƒ½å¤Ÿæ¨å¹¿æ›´æ–°çš„çŸ¥è¯†ï¼Œä½†å½“åç¦»æ—¶åˆ™ä¼šå¯¼è‡´å¹»è§‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04416",
            "title": "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
            "url": "https://huggingface.co/papers/2502.04416",
            "abstract": "Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/JarvisPei/CMoE.",
            "score": 5,
            "issue_id": 2125,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "0b4520b0860c835c",
            "authors": [
                "Zehua Pei",
                "Lancheng Zou",
                "Hui-Ling Zhen",
                "Xianzhi Yu",
                "Wulong Liu",
                "Sinno Jialin Pan",
                "Mingxuan Yuan",
                "Bei Yu"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04416.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… LLM Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ MoE Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ CMoE (Carved MoE) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts Ğ¸Ğ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). CMoE Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸. CMoE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Efficiently Carving Mixture-of-Experts for Large Language Models",
                    "desc": "This paper introduces CMoE (Carved Mixture-of-Experts), a new framework designed to enhance the efficiency of large language models (LLMs) by utilizing a mixture-of-experts architecture. CMoE takes advantage of the high activation sparsity found in feed-forward networks by activating only a subset of parameters, which reduces inference overhead. The framework groups neurons into shared and routed experts based on their activation rates and employs a differentiable routing mechanism that avoids the need for extensive retraining. Remarkably, CMoE can create a functional MoE model from a dense model in just five minutes using limited data, achieving high performance with minimal fine-tuning."
                },
                "zh": {
                    "title": "é«˜æ•ˆé›•åˆ»æ··åˆä¸“å®¶æ¨¡å‹çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¢åŠ æ¨¡å‹å‚æ•°å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†è¿™ä¹Ÿå¸¦æ¥äº†æ˜¾è‘—çš„æ¨ç†å¼€é”€ã€‚å‰é¦ˆç½‘ç»œï¼ˆFFNsï¼‰åœ¨LLMå‚æ•°ä¸­å ä¸»å¯¼åœ°ä½ï¼Œéšè—ç¥ç»å…ƒçš„æ¿€æ´»ç¨€ç–æ€§å¾ˆé«˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œä»…æ¿€æ´»ä¸€éƒ¨åˆ†å‚æ•°ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œèµ„æºï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†CMoEï¼ˆCarved MoEï¼‰ï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥é«˜æ•ˆåœ°ä»ç¨ å¯†æ¨¡å‹ä¸­é›•åˆ»å‡ºMoEæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03738",
            "title": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
            "url": "https://huggingface.co/papers/2502.03738",
            "abstract": "Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch_Scaling.",
            "score": 5,
            "issue_id": 2122,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "aa76478090a36c04",
            "authors": [
                "Feng Wang",
                "Yaodong Yu",
                "Guoyizhe Wei",
                "Wei Shao",
                "Yuyin Zhou",
                "Alan Yuille",
                "Cihang Xie"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "UC Berkeley",
                "UC Santa Cruz",
                "University of Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03738.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¸ĞºÑĞµĞ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ Ğ² vision-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ² Vision Transformer (ViT) Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ´Ğ¾ 1x1 Ğ¿Ğ¸ĞºÑĞµĞ»Ñ (Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ViT Ğ¸ Mamba. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ñ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 50 176 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 84,6% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet-1k."
                },
                "en": {
                    "title": "Unlocking Visual Potential: The Power of Smaller Patches in Vision Transformers",
                    "desc": "This paper investigates the impact of patchification, a method of dividing images into smaller sections, on the performance of Vision Transformers (ViT) in visual tasks. The authors find that reducing the size of these patches leads to better predictive performance, with the optimal size being 1x1 pixels, which represents pixel tokenization. They conduct experiments that demonstrate this scaling law across various architectures and tasks, revealing that smaller patches diminish the need for complex decoder heads in dense prediction tasks. The study achieves a significant milestone by processing a visual sequence of 50,176 tokens, achieving a competitive accuracy on the ImageNet-1k benchmark, and aims to inform future developments in non-compressive vision models."
                },
                "zh": {
                    "title": "å°å—æ›´ä¼˜ï¼Œè§†è§‰ç†è§£æ›´å¼ºï¼",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰ä¸­å›¾åƒåˆ†å—ï¼ˆpatchificationï¼‰å¯¹ä¿¡æ¯æŸå¤±çš„å½±å“ã€‚é€šè¿‡ç¼©å°å›¾åƒçš„ç©ºé—´å¤§å°ï¼Œåˆ†å—æ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡å°‘ä»¤ç‰Œåºåˆ—çš„é•¿åº¦ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œéšç€åˆ†å—å¤§å°çš„å‡å°ï¼Œæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½æŒç»­æé«˜ï¼Œç›´åˆ°è¾¾åˆ°æœ€å°çš„1x1åƒç´ åˆ†å—ã€‚è¯¥ç ”ç©¶ç»“æœé€‚ç”¨äºå¤šç§è§†è§‰ä»»åŠ¡å’Œä¸åŒçš„æ¨¡å‹æ¶æ„ï¼Œä¸ºæœªæ¥æ„å»ºéå‹ç¼©è§†è§‰æ¨¡å‹æä¾›äº†ç†è®ºåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04363",
            "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
            "url": "https://huggingface.co/papers/2502.04363",
            "abstract": "We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.",
            "score": 5,
            "issue_id": 2119,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "339b45dee733174c",
            "authors": [
                "Bosung Kim",
                "Kyuhwan Lee",
                "Isu Jeong",
                "Jungmin Cheon",
                "Yeojin Lee",
                "Seulki Lee"
            ],
            "affiliations": [
                "Ulsan National Institute of Science and Technology South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04363.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#low_resource"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ½Ğ° Ğ²Ğ°ÑˆĞµĞ¼ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğµ",
                    "desc": "On-device Sora Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ĞµĞµ Ğ½Ğ° ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Linear Proportional Leap (LPL) Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°, Temporal Dimension Token Merging (TDTM) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ»Ğ¾ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸ Concurrent Inference with Dynamic Loading (CI-DL) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° iPhone 15 Pro Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ On-device Sora ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Open-Sora Ğ½Ğ° Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… GPU."
                },
                "en": {
                    "title": "Empowering Video Creation on Your Smartphone!",
                    "desc": "On-device Sora is a groundbreaking solution for generating videos from text using diffusion models directly on smartphones. It introduces three innovative techniques: Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to optimize token processing in attention layers, and Concurrent Inference with Dynamic Loading (CI-DL) to manage memory efficiently. These advancements allow the system to produce high-quality videos comparable to those generated by powerful GPUs, all while operating within the constraints of mobile devices. This work not only enhances accessibility to advanced generative technologies but also prioritizes user privacy and reduces reliance on cloud services."
                },
                "zh": {
                    "title": "ç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆæ–°çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†On-device Soraï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ç§»åŠ¨è®¾å¤‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨æ™ºèƒ½æ‰‹æœºä¸Šè¿è¡Œã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†ä¸‰ç§æ–°æŠ€æœ¯æ¥è§£å†³ç§»åŠ¨è®¾å¤‡åœ¨è®¡ç®—å’Œå†…å­˜æ–¹é¢çš„é™åˆ¶ã€‚é¦–å…ˆï¼Œçº¿æ€§æ¯”ä¾‹è·³è·ƒï¼ˆLPLï¼‰é€šè¿‡é«˜æ•ˆçš„è·³è·ƒæ–¹æ³•å‡å°‘äº†è§†é¢‘æ‰©æ•£ä¸­æ‰€éœ€çš„å»å™ªæ­¥éª¤ã€‚å…¶æ¬¡ï¼Œæ—¶é—´ç»´åº¦ä»¤ç‰Œåˆå¹¶ï¼ˆTDTMï¼‰é€šè¿‡æ²¿æ—¶é—´ç»´åº¦åˆå¹¶è¿ç»­ä»¤ç‰Œï¼Œé™ä½äº†æ³¨æ„åŠ›å±‚ä¸­å¯†é›†çš„ä»¤ç‰Œå¤„ç†è®¡ç®—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04404",
            "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
            "url": "https://huggingface.co/papers/2502.04404",
            "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.",
            "score": 5,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "a7bd201755c7ea1d",
            "authors": [
                "Xiao-Wen Yang",
                "Xuan-Yi Zhu",
                "Wen-Da Wei",
                "Ding-Chu Zhang",
                "Jie-Jing Shao",
                "Zhi Zhou",
                "Lan-Zhe Guo",
                "Yu-Feng Li"
            ],
            "affiliations": [
                "National Key Laboratory for Novel Software Technology, Nanjing University, China",
                "School of Artificial Intelligence, Nanjing University, China",
                "School of Intelligence Science and Technology, Nanjing University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04404.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#training",
                    "#inference",
                    "#agents",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ° (self-backtracking) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ Ğ³Ğ´Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒÑÑ Ğ½Ğ°Ğ·Ğ°Ğ´ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Backtracking for Enhanced Reasoning",
                    "desc": "This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å›æº¯æœºåˆ¶ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "å°†æ…¢æ€è€ƒæœºåˆ¶æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œä¸ºå®ç°äºŒçº§AGIæ¨ç†å™¨æä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ã€‚å½“å‰çš„æŒ‘æˆ˜åŒ…æ‹¬ä½æ•ˆçš„è¿‡åº¦æ€è€ƒå’Œå¯¹è¾…åŠ©å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¾èµ–ã€‚æˆ‘ä»¬æŒ‡å‡ºï¼Œè¿™äº›é™åˆ¶æºäºLLMsæ— æ³•å†…åŒ–æœç´¢è¿‡ç¨‹ï¼Œè€Œæœç´¢è¿‡ç¨‹æ˜¯æœ‰æ•ˆæ¨ç†çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘å›æº¯æœºåˆ¶ï¼Œä½¿LLMsèƒ½å¤Ÿåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•å›æº¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05092",
            "title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs",
            "url": "https://huggingface.co/papers/2502.05092",
            "abstract": "Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) ClockQA, which comprises various types of clock styles-standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks-paired with time related questions; and 2) CalendarQA, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.",
            "score": 4,
            "issue_id": 2128,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "ea9f14d34d4cbb60",
            "authors": [
                "Rohit Saxena",
                "Aryo Pradipta Gema",
                "Pasquale Minervini"
            ],
            "affiliations": [
                "ILCC, School of Informatics, University of Edinburgh",
                "Miniml.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05092.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#cv",
                    "#interpretability"
                ],
                "emoji": "â°",
                "ru": {
                    "title": "Ğ’Ñ€ĞµĞ¼Ñ Ğ±Ñ€Ğ¾ÑĞ°ĞµÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ´Ğ°Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ‡Ğ°ÑÑ‹ Ğ¸ ĞºĞ°Ğ»ĞµĞ½Ğ´Ğ°Ñ€Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ClockQA Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ÑĞ¼Ğ¸ Ñ‡Ğ°ÑĞ¾Ğ² Ğ¸ CalendarQA Ñ ĞºĞ°Ğ»ĞµĞ½Ğ´Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ¦ĞµĞ»ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, ĞºĞ°Ğº MLLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ»Ñ MLLM, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Time: Challenges for Multimodal Language Models",
                    "desc": "This paper explores how multimodal large language models (MLLMs) understand time through visual aids like clocks and calendars. The authors created a dataset with two parts: ClockQA, which includes different clock styles and related questions, and CalendarQA, featuring yearly calendars with various date-related queries. The study assesses the models' abilities in visual recognition, numerical reasoning, and temporal inference when interpreting these time-related images. Despite progress in MLLMs, the findings indicate that accurately grasping time concepts continues to be a major hurdle."
                },
                "zh": {
                    "title": "ç†è§£æ—¶é—´çš„æŒ‘æˆ˜ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å±€é™æ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£æ—¶é—´å’Œæ—¥æœŸæ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ¨¡æ‹Ÿæ—¶é’Ÿå’Œå¹´åº¦æ—¥å†çš„è§†è§‰è¡¨ç¤ºã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼šClockQAï¼ŒåŒ…å«ä¸åŒé£æ ¼çš„æ—¶é’ŸåŠç›¸å…³æ—¶é—´é—®é¢˜ï¼›CalendarQAï¼ŒåŒ…å«å¹´åº¦æ—¥å†å›¾åƒåŠå¸¸è§æ—¥æœŸé—®é¢˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ†æMLLMsåœ¨å¤„ç†ä¸æ—¶é—´ç›¸å…³çš„è§†è§‰æ•°æ®æ—¶çš„è§†è§‰è¯†åˆ«ã€æ•°å€¼æ¨ç†å’Œæ—¶é—´æ¨æ–­èƒ½åŠ›ã€‚å°½ç®¡æœ€è¿‘å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†MLLMsåœ¨å¯é ç†è§£æ—¶é—´æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05178",
            "title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2502.05178",
            "abstract": "We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.",
            "score": 4,
            "issue_id": 2121,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "bbc1f1a0b7f5423f",
            "authors": [
                "Yue Zhao",
                "Fuzhao Xue",
                "Scott Reed",
                "Linxi Fan",
                "Yuke Zhu",
                "Jan Kautz",
                "Zhiding Yu",
                "Philipp KrÃ¤henbÃ¼hl",
                "De-An Huang"
            ],
            "affiliations": [
                "NVIDIA",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05178.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "QLIP: Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "QLIP - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾-ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ±Ğ°Ñ‚Ñ‡Ğ°Ğ¼ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. QLIP Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "QLIP: Bridging Visual and Language Understanding with Efficient Tokenization",
                    "desc": "The paper presents Quantized Language-Image Pretraining (QLIP), a novel method for visual tokenization that achieves high-quality image reconstruction while enhancing zero-shot image understanding. QLIP employs a binary-spherical-quantization-based autoencoder, integrating both reconstruction and language-image alignment objectives in a harmonious manner. By dynamically balancing these objectives during training, the method effectively addresses the challenges of large-batch image-language pre-training and memory constraints. The results show that QLIP can replace existing visual encoders and image tokenizers in models like LLaVA and LlamaGen, improving performance in multimodal understanding and text-conditioned image generation."
                },
                "zh": {
                    "title": "é‡åŒ–è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼šå¤šæ¨¡æ€ç†è§£çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é‡åŒ–è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ–¹æ³•ï¼ˆQLIPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§†è§‰æ ‡è®°åŒ–æ–¹æ³•ï¼Œç»“åˆäº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œé›¶-shotå›¾åƒç†è§£èƒ½åŠ›ã€‚QLIPä½¿ç”¨åŸºäºäºŒå…ƒçƒé¢é‡åŒ–çš„è‡ªç¼–ç å™¨è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶ä¼˜åŒ–é‡å»ºå’Œè¯­è¨€-å›¾åƒå¯¹é½ç›®æ ‡ã€‚æˆ‘ä»¬é¦–æ¬¡è¯æ˜è¿™ä¸¤ä¸ªç›®æ ‡å¯ä»¥åŠ¨æ€å¹³è¡¡ï¼Œè€Œä¸æ˜¯ç›¸äº’å¯¹ç«‹ã€‚QLIPåœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯ä»¥ä½œä¸ºLLaVAçš„è§†è§‰ç¼–ç å™¨å’ŒLlamaGençš„å›¾åƒæ ‡è®°å™¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ€§èƒ½ç›¸å½“æˆ–æ›´å¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04350",
            "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
            "url": "https://huggingface.co/papers/2502.04350",
            "abstract": "Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.",
            "score": 4,
            "issue_id": 2118,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "ad7829c09c28de41",
            "authors": [
                "Yongchao Chen",
                "Yilun Hao",
                "Yueying Liu",
                "Yang Zhang",
                "Chuchu Fan"
            ],
            "affiliations": [
                "Harvard University, Boston, MA, USA",
                "MIT-IBM Watson AI Lab, Boston, MA, USA",
                "Massachusetts Institute of Technology, Boston, MA, USA",
                "University of Illinois Urbana-Champaign, Urbana, IL, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04350.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#rlhf",
                    "#open_source",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "CodeSteer: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° LLM Ğ² ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ…",
                    "desc": "CodeSteer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SymBench Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Llama-3-8B Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CodeSteerLLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "CodeSteer: Guiding LLMs to Master Code and Reasoning!",
                    "desc": "This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks."
                },
                "zh": {
                    "title": "CodeSteerï¼šå¼•å¯¼LLMå®ç°ç¬¦å·è®¡ç®—çš„çªç ´",
                    "desc": "ç°æœ‰çš„æ–¹æ³•æ— æ³•æœ‰æ•ˆå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬æ¨ç†å’Œä»£ç ç”Ÿæˆä¹‹é—´åˆ‡æ¢ï¼Œå¯¼è‡´ç¬¦å·è®¡ç®—èƒ½åŠ›æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCodeSteerçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼LLMçš„ä»£ç å’Œæ–‡æœ¬ç”Ÿæˆã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†SymBenchï¼ŒåŒ…å«37ä¸ªå…·æœ‰å¯è°ƒå¤æ‚åº¦çš„ç¬¦å·ä»»åŠ¡ï¼Œå¹¶åˆæˆäº†åŒ…å«1.2ä¸‡å¤šè½®æŒ‡å¯¼/ç”Ÿæˆè½¨è¿¹å’Œ5500å¯¹æŒ‡å¯¼æ¯”è¾ƒçš„æ•°æ®é›†ã€‚é€šè¿‡å¯¹Llama-3-8Bæ¨¡å‹è¿›è¡Œå¤šè½®ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°çš„CodeSteerLLMæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ›´å¤§æ¨¡å‹çš„ä»£ç /æ–‡æœ¬ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03512",
            "title": "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment",
            "url": "https://huggingface.co/papers/2502.03512",
            "abstract": "Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that generated visuals not only accurately encapsulate user intents but also conform to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini fiasco, where misaligned outputs triggered significant public backlash, underscore the critical need for robust alignment mechanisms. In contrast, Large Language Models (LLMs) have achieved notable success in alignment. Building on these advancements, researchers are eager to apply similar alignment techniques, such as Direct Preference Optimization (DPO), to T2I systems to enhance image generation fidelity and reliability.   We present YinYangAlign, an advanced benchmarking framework that systematically quantifies the alignment fidelity of T2I systems, addressing six fundamental and inherently contradictory design objectives. Each pair represents fundamental tensions in image generation, such as balancing adherence to user prompts with creative modifications or maintaining diversity alongside visual coherence. YinYangAlign includes detailed axiom datasets featuring human prompts, aligned (chosen) responses, misaligned (rejected) AI-generated outputs, and explanations of the underlying contradictions.",
            "score": 3,
            "issue_id": 2122,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "ebf6482c46f8fe2f",
            "authors": [
                "Amitava Das",
                "Yaswanth Narsupalli",
                "Gurpreet Singh",
                "Vinija Jain",
                "Vasu Sharma",
                "Suranjana Trivedy",
                "Aman Chadha",
                "Amit Sheth"
            ],
            "affiliations": [
                "Amazon AI, USA",
                "Artificial Intelligence Institute, University of South Carolina, USA",
                "Meta AI, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03512.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rag",
                    "#rlhf",
                    "#ethics",
                    "#alignment"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: YinYangAlign Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ YinYangAlign - Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… (T2I) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ… Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€ÑĞ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. YinYangAlign Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ˜Ğ˜, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Image Generation Fidelity with YinYangAlign",
                    "desc": "This paper discusses the importance of precise alignment in Text-to-Image (T2I) systems to ensure that generated images meet user expectations and ethical standards. It highlights past failures, like the Google Gemini incident, which demonstrate the need for better alignment strategies. The authors propose YinYangAlign, a new benchmarking framework that evaluates T2I systems based on six conflicting design goals, such as user prompt adherence versus creative freedom. By using detailed datasets that include human prompts and AI outputs, YinYangAlign aims to improve the fidelity and reliability of image generation."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿçš„å¯¹é½ç²¾åº¦",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç³»ç»Ÿä¸­ç²¾ç¡®å¯¹é½çš„é‡è¦æ€§ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒæ—¢èƒ½å‡†ç¡®åæ˜ ç”¨æˆ·æ„å›¾ï¼Œåˆç¬¦åˆä¼¦ç†å’Œç¾å­¦æ ‡å‡†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåƒGoogle Geminiè¿™æ ·çš„äº‹ä»¶å‡¸æ˜¾äº†å¼ºå¤§å¯¹é½æœºåˆ¶çš„å¿…è¦æ€§ã€‚ä¸æ­¤ç›¸æ¯”ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹é½æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œç ”ç©¶äººå‘˜å¸Œæœ›å°†ç±»ä¼¼çš„å¯¹é½æŠ€æœ¯åº”ç”¨äºT2Iç³»ç»Ÿï¼Œä»¥æé«˜å›¾åƒç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬æå‡ºäº†YinYangAlignï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„åŸºå‡†æ¡†æ¶ï¼Œç³»ç»Ÿåœ°é‡åŒ–T2Iç³»ç»Ÿçš„å¯¹é½ä¿çœŸåº¦ï¼Œè§£å†³äº†å…­ä¸ªåŸºæœ¬ä¸”å†…åœ¨çŸ›ç›¾çš„è®¾è®¡ç›®æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04689",
            "title": "ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning",
            "url": "https://huggingface.co/papers/2502.04689",
            "abstract": "Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance (\"think step by step\"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.",
            "score": 2,
            "issue_id": 2123,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "a052e3be1fe147cd",
            "authors": [
                "Yuwei Yin",
                "Giuseppe Carenini"
            ],
            "affiliations": [
                "University of British Columbia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04689.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ARR: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ARR. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. ARR Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Chain-of-Thought Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ ARR Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "ARR: A Structured Approach to Boost LLM Reasoning",
                    "desc": "This paper presents a new prompting method called ARR for large language models (LLMs) that enhances their performance on multiple-choice question-answering tasks. ARR stands for Analyze, Retrieve, and Reason, which are the three critical steps it incorporates to improve reasoning capabilities. The method outperforms traditional zero-shot Chain-of-Thought prompting by providing clearer guidance on how to approach questions. Experiments show that each component of ARR contributes positively to the overall effectiveness, especially the analysis of the question's intent."
                },
                "zh": {
                    "title": "ARRï¼šæå‡é—®ç­”æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„é›¶-shotæç¤ºæ–¹æ³•ARRï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šé¡¹é€‰æ‹©é—®ç­”ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ARRæ˜ç¡®åŒ…å«ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šåˆ†æé—®é¢˜æ„å›¾ã€æ£€ç´¢ç›¸å…³ä¿¡æ¯å’Œé€æ­¥æ¨ç†ã€‚é€šè¿‡åœ¨å¤šç§å¤æ‚é—®ç­”ä»»åŠ¡ä¸Šçš„å®éªŒï¼ŒARR consistently outperformäº†ä¼ ç»Ÿçš„åŸºçº¿æ–¹æ³•å’ŒChain-of-Thoughtï¼ˆCoTï¼‰æç¤ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ„å›¾åˆ†æåœ¨ARRä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªç»„æˆéƒ¨åˆ†çš„ç§¯æè´¡çŒ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04376",
            "title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf",
            "url": "https://huggingface.co/papers/2502.04376",
            "abstract": "In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.",
            "score": 1,
            "issue_id": 2121,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "049ab6200a9d2eae",
            "authors": [
                "Lingxiang Hu",
                "Shurun Yuan",
                "Xiaoting Qin",
                "Jue Zhang",
                "Qingwei Lin",
                "Dongmei Zhang",
                "Saravan Rajmohan",
                "Qi Zhang"
            ],
            "affiliations": [
                "Microsoft",
                "Northeastern University, China",
                "Peking University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04376.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#agi",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµĞ»ĞµĞ³Ğ°Ñ‚Ñ‹: Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾Ğ²ĞµÑ‰Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´ĞµĞ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ğ² ÑĞ¾Ğ²ĞµÑ‰Ğ°Ğ½Ğ¸ÑÑ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚ĞµĞ½Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ ÑĞ¾Ğ²ĞµÑ‰Ğ°Ğ½Ğ¸Ğ¹, Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ´ĞµĞ»ĞµĞ³Ğ°Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾ĞºĞ¾Ğ»Ğ¾ 60% Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ³Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ğ¾Ğ´Ğ¸Ğ½ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´ĞµĞ»ĞµĞ³Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¾Ğ²ĞµÑ‰Ğ°Ğ½Ğ¸ÑÑ…, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Meetings with LLMs: Balancing Engagement and Efficiency",
                    "desc": "This paper investigates the use of Large Language Models (LLMs) as meeting delegates to improve the efficiency of workplace meetings. A prototype system was developed and evaluated using real meeting transcripts, revealing that different LLMs exhibit varying engagement strategies during discussions. The results indicate that while some models effectively address key points, there is a need for improvement in handling irrelevant content and transcription errors. The study highlights both the potential benefits and challenges of integrating LLMs into meeting processes, providing insights for future applications."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–ä¼šè®®å‚ä¸",
                    "desc": "åœ¨ç°ä»£å·¥ä½œåœºæ‰€ï¼Œä¼šè®®æ˜¯äº¤æµæ€æƒ³å’Œç¡®ä¿å›¢é˜Ÿä¸€è‡´æ€§çš„é‡è¦ç¯èŠ‚ï¼Œä½†å¸¸å¸¸é¢ä¸´æ—¶é—´æ¶ˆè€—ã€æ—¥ç¨‹å†²çªå’Œå‚ä¸æ•ˆç‡ä½ä¸‹ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼šè®®ä¸­æœ‰æ•ˆåˆ†é…å‚ä¸è€…çš„èƒ½åŠ›ï¼Œå¼€å‘äº†ä¸€ä¸ªåŸºäºLLMçš„ä¼šè®®ä»£ç†ç³»ç»ŸåŸå‹ï¼Œå¹¶ä½¿ç”¨çœŸå®ä¼šè®®è®°å½•åˆ›å»ºäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒGPT-4/4oåœ¨ç§¯æå’Œè°¨æ…çš„å‚ä¸ç­–ç•¥ä¹‹é—´ä¿æŒäº†å¹³è¡¡ï¼Œè€ŒGemini 1.5 Proåˆ™æ›´å€¾å‘äºè°¨æ…ï¼ŒGemini 1.5 Flashå’ŒLlama3-8B/70Båˆ™è¡¨ç°å‡ºæ›´ç§¯æçš„å€¾å‘ã€‚å°½ç®¡çº¦60%çš„å›åº”æ¶µç›–äº†è‡³å°‘ä¸€ä¸ªå…³é”®ç‚¹ï¼Œä½†ä»éœ€æ”¹è¿›ä»¥å‡å°‘æ— å…³æˆ–é‡å¤å†…å®¹ï¼Œå¹¶æé«˜å¯¹çœŸå®åœºæ™¯ä¸­è½¬å½•é”™è¯¯çš„å®¹å¿åº¦ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-07.html",
    "link_next": "2025-02-11.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "07.02",
        "en": "02/07",
        "zh": "2æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 8,
        "#agents": 1,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 0,
        "#inference": 6,
        "#3d": 2,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 14,
        "#healthcare": 0,
        "#training": 13,
        "#robotics": 0,
        "#agi": 4,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 8,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 2
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºæ»‘åŠ¨ç£è´´æ³¨æ„åŠ›ï¼ˆSTAï¼‰ï¼Œä»¥è§£å†³æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¼ ç»Ÿçš„å…¨æ³¨æ„åŠ›æœºåˆ¶åœ¨ç”ŸæˆçŸ­è§†é¢‘æ—¶è€—æ—¶æé•¿ã€‚STAé€šè¿‡åœ¨å±€éƒ¨3Dçª—å£å†…æ»‘åŠ¨å’Œå…³æ³¨ï¼Œæ¶ˆé™¤äº†å…¨æ³¨æ„åŠ›ä¸­çš„å†—ä½™ã€‚ä¸ä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸åŒï¼ŒSTAé‡‡ç”¨ç¡¬ä»¶æ„ŸçŸ¥çš„è®¾è®¡ï¼Œæé«˜äº†æ•ˆç‡ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSTAåœ¨ä¸é™ä½è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†è§†é¢‘ç”Ÿæˆçš„å»¶è¿Ÿã€‚",
        "title": "Fast Video Generation with Sliding Tile Attention",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºæ»‘åŠ¨ç£è´´æ³¨æ„åŠ›ï¼ˆSTAï¼‰ï¼Œä»¥è§£å†³æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¼ ç»Ÿçš„å…¨æ³¨æ„åŠ›æœºåˆ¶åœ¨ç”ŸæˆçŸ­è§†é¢‘æ—¶è€—æ—¶æé•¿ã€‚STAé€šè¿‡åœ¨å±€éƒ¨3Dçª—å£å†…æ»‘åŠ¨å’Œå…³æ³¨ï¼Œæ¶ˆé™¤äº†å…¨æ³¨æ„åŠ›ä¸­çš„å†—ä½™ã€‚ä¸ä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸åŒï¼ŒSTAé‡‡ç”¨ç¡¬ä»¶æ„ŸçŸ¥çš„è®¾è®¡ï¼Œæé«˜äº†æ•ˆç‡ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSTAåœ¨ä¸é™ä½è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†è§†é¢‘ç”Ÿæˆçš„å»¶è¿Ÿã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de zhÃ¹ yÃ¬ lÃ¬ jÄ« zhÃ¬, chÄ“ng wÃ©i huÃ¡ dÃ²ng cÃ­ tiÄ“ zhÃ¹ yÃ¬ lÃ¬ (STA), yÇ jiÄ› juÃ© kuÃ² sÃ n biÃ n shÅ« qÃ¬ (DiTs) zÃ i shÃ¬ pÃ­n shÄ“ng chÃ©ng zhÅng de jÃ¬ suÃ n chÃ©ng bÄ›n wÃ¨n tÃ­. chuÃ¡n tÇ’ng de quÃ¡n zhÃ¹ yÃ¬ lÃ¬ jÄ« zhÃ¬ zÃ i shÄ“ng chÃ©ng duÇn shÃ¬ pÃ­n shÃ­ hÃ o shÃ­ jÃ­ chÃ¡ng. STA tÅng guÃ² zÃ i jÃº bÃ¹ 3D chuÄng kÇ’u nÃ¨i huÃ¡ dÃ²ng hÃ© guÄn zhÃ¹, xiÄo chÃº le quÃ¡n zhÃ¹ yÃ¬ lÃ¬ zhÅng de rÃ³ng yÃ¹. yÇ” chuÃ¡n tÇ’ng de huÃ¡ dÃ²ng chuÄng kÇ’u zhÃ¹ yÃ¬ lÃ¬ bÃ¹ tÃ³ng, STA cÇi yÃ²ng yÃ¬ng jiÃ n gÇn zhÄ« de shÃ¨ jÃ¬, tÃ­ gÄo le xiÃ o lÇœ. shÃ­ yÃ n xiÇn shÃ¬, STA zÃ i bÃ¹ jiÃ ng dÄ« zhÃ¬ liÃ ng de qÃ­ng kuÃ ng xiÃ , xiÇn zhÃ¹ jiÇn shÇo le shÃ¬ pÃ­n shÄ“ng chÃ©ng de yÃ¡n chÃ­.",
        "vocab": "[{'word': 'æ³¨æ„åŠ›', 'pinyin': 'zhÃ¹yÃ¬lÃ¬', 'trans': 'attention'},\n{'word': 'æœºåˆ¶', 'pinyin': 'jÄ«zhÃ¬', 'trans': 'mechanism'},\n{'word': 'æ»‘åŠ¨', 'pinyin': 'huÃ¡dÃ²ng', 'trans': 'sliding'},\n{'word': 'ç£è´´', 'pinyin': 'cÃ­tiÄ“', 'trans': 'magnetic tile'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'å˜å‹å™¨', 'pinyin': 'biÃ nyÄqÃ¬', 'trans': 'transformer'},\n{'word': 'è®¡ç®—', 'pinyin': 'jÃ¬suÃ n', 'trans': 'computation'},\n{'word': 'æˆæœ¬', 'pinyin': 'chÃ©ngbÄ›n', 'trans': 'cost'},\n{'word': 'ä¼ ç»Ÿ', 'pinyin': 'chuÃ¡ntÇ’ng', 'trans': 'traditional'},\n{'word': 'å…¨', 'pinyin': 'quÃ¡n', 'trans': 'full'},\n{'word': 'è€—æ—¶', 'pinyin': 'hÃ oshÃ­', 'trans': 'time-consuming'},\n{'word': 'æé•¿', 'pinyin': 'jÃ­chÃ¡ng', 'trans': 'extremely long'},\n{'word': 'å±€éƒ¨', 'pinyin': 'jÃºbÃ¹', 'trans': 'local'},\n{'word': '3D', 'pinyin': '', 'trans': '3D'},\n{'word': 'çª—å£', 'pinyin': 'chuÄngkÇ’u', 'trans': 'window'},\n{'word': 'å†—ä½™', 'pinyin': 'rÇ’ngyÃº', 'trans': 'redundancy'},\n{'word': 'ç¡¬ä»¶', 'pinyin': 'yÃ¬ngjiÃ n', 'trans': 'hardware'},\n{'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇnzhÄ«', 'trans': 'perception'},\n{'word': 'è®¾è®¡', 'pinyin': 'shÃ¨jÃ¬', 'trans': 'design'},\n{'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇnshÃ¬', 'trans': 'display'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'å‡å°‘', 'pinyin': 'jiÇnshÇo', 'trans': 'reduce'},\n{'word': 'å»¶è¿Ÿ', 'pinyin': 'yÃ¡nchÃ­', 'trans': 'latency'}]",
        "trans": "This article introduces a new attention mechanism called Sliding Tile Attention (STA) to address the computational cost issues of Diffusion Transformers (DiTs) in video generation. Traditional full attention mechanisms take an extremely long time to generate short videos. STA eliminates redundancy in full attention by sliding and focusing within local 3D windows. Unlike traditional sliding window attention, STA employs a hardware-aware design to enhance efficiency. Experiments show that STA significantly reduces the latency of video generation without compromising quality.",
        "update_ts": "2025-02-10 09:11"
    }
}