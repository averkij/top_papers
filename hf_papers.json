{
    "date": {
        "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 6",
        "zh": "3æœˆ6æ—¥"
    },
    "time_utc": "2025-03-06 05:11",
    "weekday": 3,
    "issue_id": 2557,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.00865",
            "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
            "url": "https://huggingface.co/papers/2503.00865",
            "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.",
            "score": 19,
            "issue_id": 2555,
            "pub_date": "2025-03-02",
            "pub_date_card": {
                "ru": "2 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 2",
                "zh": "3æœˆ2æ—¥"
            },
            "hash": "bc2424e709a2dd78",
            "authors": [
                "Yiran Zhao",
                "Chaoqun Liu",
                "Yue Deng",
                "Jiahao Ying",
                "Mahani Aljunied",
                "Zhaodonghui Li",
                "Lidong Bing",
                "Hou Pong Chan",
                "Yu Rong",
                "Deli Zhao",
                "Wenxuan Zhang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00865.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#architecture",
                    "#open_source",
                    "#training",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Babel: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Babel, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ 25 ÑĞ°Ğ¼Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ¸Ñ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸: Babel-9B Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ Babel-83B, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ±Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Babel: Bridging the Language Gap with Open Multilingual LLMs",
                    "desc": "This paper presents Babel, an innovative open-source multilingual large language model (LLM) that addresses the lack of coverage for under-resourced languages in existing models. Babel supports the top 25 languages spoken globally, reaching over 90% of the world's population, and includes many languages that are often neglected. The model employs a unique layer extension technique to increase its parameter count, enhancing its performance beyond traditional pretraining methods. With two variants, Babel-9B and Babel-83B, the model demonstrates superior performance on multilingual tasks, outperforming other open LLMs of similar size and achieving results comparable to commercial models."
                },
                "zh": {
                    "title": "Babelï¼šæ‰“ç ´è¯­è¨€å£å’çš„å¤šè¯­è¨€æ¨¡å‹",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†å¼€æºçš„å¤šè¯­è¨€LLMsä»ç„¶ç¨€ç¼ºï¼Œç°æœ‰æ¨¡å‹é€šå¸¸åœ¨è¯­è¨€è¦†ç›–ä¸Šæœ‰é™ã€‚è®¸å¤šæ¨¡å‹ä¼˜å…ˆè€ƒè™‘èµ„æºä¸°å¯Œçš„è¯­è¨€ï¼Œè€Œå¹¿æ³›ä½¿ç”¨ä½†èµ„æºä¸è¶³çš„è¯­è¨€å¸¸å¸¸è¢«å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Babelï¼Œä¸€ä¸ªå¼€æ”¾çš„å¤šè¯­è¨€LLMï¼Œè¦†ç›–å…¨çƒå‰25ç§è¯­è¨€ï¼Œæ”¯æŒè¶…è¿‡90%çš„äººå£ï¼Œå¹¶åŒ…æ‹¬è®¸å¤šå…¶ä»–å¼€æºå¤šè¯­è¨€LLMså¿½è§†çš„è¯­è¨€ã€‚Babelé€šè¿‡å±‚æ‰©å±•æŠ€æœ¯å¢åŠ å‚æ•°æ•°é‡ï¼Œæå‡äº†æ€§èƒ½ï¼Œå¹¶æ¨å‡ºäº†ä¸¤ä¸ªå˜ä½“ï¼šBabel-9Bå’ŒBabel-83Bï¼Œåè€…åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.00329",
            "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
            "url": "https://huggingface.co/papers/2503.00329",
            "abstract": "Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page.",
            "score": 6,
            "issue_id": 2555,
            "pub_date": "2025-03-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 1",
                "zh": "3æœˆ1æ—¥"
            },
            "hash": "0483c542c8885777",
            "authors": [
                "Benjamin Schneider",
                "Florian Kerschbaum",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Cheriton School of Computer Science, University of Waterloo",
                "Vector Institute, Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00329.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ABC: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ABC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ABC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ABC Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "ABC: Unifying Vision and Language for Enhanced Multimodal Understanding",
                    "desc": "This paper presents ABC, a new multimodal embedding model that integrates visual and natural language inputs more effectively than existing CLIP-based methods. Unlike previous models that treat images and text separately, ABC combines these modalities deeply, allowing for better interaction and user control. The model excels in zero-shot tasks, particularly in image-to-text retrieval and classification, outperforming others in the Massive Multimodal Embedding Benchmark. To assess its capabilities, the authors introduce CtrlBench, a benchmark designed to evaluate the model's performance in handling complex visual retrieval tasks with natural language instructions."
                },
                "zh": {
                    "title": "ABCï¼šå¤šæ¨¡æ€åµŒå…¥çš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºABCçš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è§†è§‰æ£€ç´¢å’Œåˆ†ç±»ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚ä¸ç°æœ‰çš„CLIPæ–¹æ³•ä¸åŒï¼ŒABCé€šè¿‡æ·±åº¦æ•´åˆå›¾åƒç‰¹å¾å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæä¾›æ›´å¼ºçš„æ¨¡æ€äº¤äº’ã€‚ABCåœ¨MSCOCOå›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨åˆ†ç±»å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚é€šè¿‡è®¾è®¡CtrlBenchåŸºå‡†ï¼Œè¯„ä¼°äº†ABCåœ¨å¤„ç†å¤æ‚è§†è§‰æ£€ç´¢é—®é¢˜æ—¶çš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶é«˜è´¨é‡çš„è¡¨ç¤ºå’Œçµæ´»çš„è‡ªç„¶è¯­è¨€æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02951",
            "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
            "url": "https://huggingface.co/papers/2503.02951",
            "abstract": "We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.",
            "score": 5,
            "issue_id": 2555,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 4",
                "zh": "3æœˆ4æ—¥"
            },
            "hash": "6c344ba0bf71ac84",
            "authors": [
                "Zhangchen Xu",
                "Yang Liu",
                "Yueqin Yin",
                "Mingyuan Zhou",
                "Radha Poovendran"
            ],
            "affiliations": [
                "Microsoft",
                "The University of Texas at Austin",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02951.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ§‘â€ğŸ’»",
                "ru": {
                    "title": "KodCode: Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "KodCode - ÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ-Ñ‚ĞµÑÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ KodCode Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° KodCode, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "KodCode: Elevating Coding Models with Verified Data",
                    "desc": "KodCode is a synthetic dataset designed to improve the training of Large Language Models (LLMs) for coding tasks by providing high-quality, verifiable data. It includes question-solution-test triplets that are validated through a self-verification process, ensuring both correctness and a wide range of coding difficulties. The dataset is generated using a systematic pipeline that synthesizes coding questions, creates solutions, and develops test cases, particularly focusing on challenging problems. Fine-tuning experiments show that models trained on KodCode outperform existing models on various coding benchmarks, demonstrating its effectiveness in enhancing LLM performance."
                },
                "zh": {
                    "title": "KodCodeï¼šé«˜è´¨é‡ç¼–ç æ•°æ®é›†çš„è§£å†³æ–¹æ¡ˆ",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†KodCodeï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è·å–é«˜è´¨é‡ã€å¯éªŒè¯çš„è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ï¼Œä»¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ã€‚ç°æœ‰çš„ä»£ç èµ„æºé€šå¸¸æ— æ³•ç¡®ä¿è¦†ç›–èŒƒå›´å¹¿æ³›æˆ–æ­£ç¡®æ€§å¯éªŒè¯ã€‚KodCodeç”±é—®é¢˜-è§£å†³æ–¹æ¡ˆ-æµ‹è¯•ä¸‰å…ƒç»„ç»„æˆï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯ç¨‹åºç³»ç»Ÿåœ°éªŒè¯ã€‚æˆ‘ä»¬çš„æµç¨‹åŒ…æ‹¬åˆæˆå„ç§ç¼–ç é—®é¢˜ï¼Œç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶åœ¨åæœŸé€šè¿‡é‡å†™é—®é¢˜å’Œç”Ÿæˆå“åº”æ¥è¿›è¡Œæ•°æ®åˆæˆï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªå¤§è§„æ¨¡ã€å¼ºå¤§ä¸”å¤šæ ·åŒ–çš„ç¼–ç æ•°æ®é›†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03751",
            "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
            "url": "https://huggingface.co/papers/2503.03751",
            "abstract": "We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
            "score": 3,
            "issue_id": 2555,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 5",
                "zh": "3æœˆ5æ—¥"
            },
            "hash": "8f5f2ad910a260c0",
            "authors": [
                "Xuanchi Ren",
                "Tianchang Shen",
                "Jiahui Huang",
                "Huan Ling",
                "Yifan Lu",
                "Merlin Nimier-David",
                "Thomas MÃ¼ller",
                "Alexander Keller",
                "Sanja Fidler",
                "Jun Gao"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03751.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "GEN3C - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D-ĞºÑÑˆ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ½ĞµĞµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸÑ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² GEN3C Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° 2D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ 3D-ĞºÑÑˆĞ° Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, Ğ½Ğµ Ñ‚Ñ€Ğ°Ñ‚Ñ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "GEN3C: Mastering Video Generation with 3D Precision and Camera Control",
                    "desc": "GEN3C is a generative video model that enhances video generation by incorporating precise camera control and maintaining temporal 3D consistency. Unlike previous models that often lack 3D information, GEN3C utilizes a 3D cache of point clouds derived from depth predictions, allowing for more coherent object presence in videos. The model is conditioned on 2D renderings from this cache, enabling it to generate new frames without needing to remember past outputs or infer scene structure from camera angles. This approach results in superior camera control and state-of-the-art performance in generating novel views, particularly in complex scenarios like driving scenes."
                },
                "zh": {
                    "title": "GEN3Cï¼šç²¾ç¡®ç›¸æœºæ§åˆ¶ä¸æ—¶é—´ä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹",
                    "desc": "æˆ‘ä»¬æå‡ºäº†GEN3Cï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ç²¾ç¡®ç›¸æœºæ§åˆ¶å’Œæ—¶é—´ä¸€è‡´æ€§çš„ç”Ÿæˆè§†é¢‘æ¨¡å‹ã€‚ä»¥å¾€çš„è§†é¢‘æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œä½†å¾€å¾€ç¼ºä¹3Dä¿¡æ¯ï¼Œå¯¼è‡´ç‰©ä½“å‡ºç°å’Œæ¶ˆå¤±çš„ä¸ä¸€è‡´æ€§ã€‚GEN3Cé€šè¿‡3Dç¼“å­˜æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨ä»ç§å­å›¾åƒæˆ–å…ˆå‰ç”Ÿæˆå¸§ä¸­é¢„æµ‹çš„åƒç´ æ·±åº¦è·å¾—çš„ç‚¹äº‘ã€‚è¿™æ ·ï¼ŒGEN3Cèƒ½å¤Ÿåœ¨ç”¨æˆ·æä¾›çš„æ–°ç›¸æœºè½¨è¿¹ä¸‹ï¼Œä¸“æ³¨äºç”Ÿæˆæœªè§‚å¯Ÿåˆ°çš„åŒºåŸŸï¼Œå¹¶æœ‰æ•ˆæ¨è¿›åœºæ™¯çŠ¶æ€åˆ°ä¸‹ä¸€ä¸ªå¸§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01449",
            "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
            "url": "https://huggingface.co/papers/2503.01449",
            "abstract": "Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.",
            "score": 0,
            "issue_id": 2557,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "1b4593bb9d78ec53",
            "authors": [
                "Ting Zhang",
                "Chengran Yang",
                "Yindu Su",
                "Martin Weyssow",
                "Hung Nguyen",
                "Tan Bui",
                "Hong Jin Kang",
                "Yikun Li",
                "Eng Lieh Ouh",
                "Lwin Khin Shar",
                "David Lo"
            ],
            "affiliations": [
                "School of Computer Science, University of Sydney, Australia",
                "School of Computing and Information Systems, Singapore Management University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01449.jpg",
            "data": {
                "error": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01378",
            "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
            "url": "https://huggingface.co/papers/2503.01378",
            "abstract": "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io",
            "score": 0,
            "issue_id": 2557,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "8a4aab69ce92453d",
            "authors": [
                "Artem Lykov",
                "Valerii Serpiva",
                "Muhammad Haris Khan",
                "Oleg Sautenkov",
                "Artyom Myshlyaev",
                "Grik Tadevosyan",
                "Yasheerah Yaqoot",
                "Dzmitry Tsetserukou"
            ],
            "affiliations": [
                "Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01378.jpg",
            "data": {
                "error": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01372",
            "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
            "url": "https://huggingface.co/papers/2503.01372",
            "abstract": "In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.",
            "score": 0,
            "issue_id": 2557,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "3de5be81537fa0fd",
            "authors": [
                "Joel Niklaus",
                "Jakob Merane",
                "Luka Nenadic",
                "Sina Ahmadi",
                "Yingqiang Gao",
                "Cyrill A. H. Chevalley",
                "Claude Humbel",
                "Christophe GÃ¶sken",
                "Lorenzo Tanzi",
                "Thomas LÃ¼thi",
                "Stefan Palombo",
                "Spencer Poff",
                "Boling Yang",
                "Nan Wu",
                "Matthew Guillod",
                "Robin MamiÃ©",
                "Daniel Brunner",
                "Julio Pereyra",
                "Niko Grupen"
            ],
            "affiliations": [
                "Canton of Solothurn",
                "ETH Zurich",
                "Max Planck Institute for Research on Collective Goods",
                "Swiss Federal Supreme Court",
                "University of Basel",
                "University of Geneva",
                "University of Lausanne",
                "University of Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01372.jpg",
            "data": {
                "error": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.00502",
            "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions",
            "url": "https://huggingface.co/papers/2503.00502",
            "abstract": "Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, but the conflict between slow inference speed and the need for real-time decision-making challenges practical deployment. To address these issues, this paper introduces a parallel Actor-Reasoner framework designed to enable explicit bidirectional AV-HV interactions across multiple scenarios. First, by facilitating interactions between the LLM-driven Reasoner and heterogeneous simulated HVs during training, an interaction memory database, referred to as the Actor, is established. Then, by introducing the memory partition module and the two-layer memory retrieval module, the Actor's ability to handle heterogeneous HVs is significantly enhanced. Ablation studies and comparisons with other decision-making methods demonstrate that the proposed Actor-Reasoner framework significantly improves safety and efficiency. Finally, with the combination of the external Human-Machine Interface (eHMI) information derived from Reasoner's reasoning and the feasible action solutions retrieved from the Actor, the effectiveness of the proposed Actor-Reasoner is confirmed in multi-scenario field interactions. Our code is available at https://github.com/FanGShiYuu/Actor-Reasoner.",
            "score": 0,
            "issue_id": 2555,
            "pub_date": "2025-03-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 1",
                "zh": "3æœˆ1æ—¥"
            },
            "hash": "d184a5cae68093d5",
            "authors": [
                "Shiyu Fang",
                "Jiaqi Liu",
                "Chengkai Xu",
                "Chen Lv",
                "Peng Hang",
                "Jian Sun"
            ],
            "affiliations": [
                "College of Transportation, Tongji University, Shanghai 201804, China",
                "Nanyang Technological University, 639798, Singapore",
                "State Key Lab of Intelligent Transportation System, Beijing 100088, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00502.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#robotics",
                    "#inference",
                    "#optimization",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Actor-Reasoner Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Actor-Reasoner Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing AV-HV Interactions with the Actor-Reasoner Framework",
                    "desc": "This paper presents a new framework called the Actor-Reasoner to improve interactions between Autonomous Vehicles (AVs) and Human-driven Vehicles (HVs). It leverages large language models (LLMs) to facilitate real-time communication and decision-making, addressing the challenge of slow inference speeds. The framework includes an interaction memory database, which enhances the AV's ability to understand and respond to various HV behaviors. Experimental results show that this approach significantly boosts both safety and efficiency in multi-scenario driving situations."
                },
                "zh": {
                    "title": "æå‡è‡ªåŠ¨é©¾é©¶ä¸äººç±»é©¾é©¶äº’åŠ¨çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¹¶è¡Œæ¼”å‘˜-æ¨ç†å™¨æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ˆAVï¼‰ä¸äººç±»é©¾é©¶æ±½è½¦ï¼ˆHVï¼‰ä¹‹é—´çš„äº’åŠ¨ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿ƒè¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ¨ç†å™¨ä¸ä¸åŒç±»å‹çš„æ¨¡æ‹ŸHVä¹‹é—´çš„äº’åŠ¨ï¼Œå»ºç«‹äº†ä¸€ä¸ªäº’åŠ¨è®°å¿†æ•°æ®åº“ã€‚å¼•å…¥è®°å¿†åˆ†åŒºæ¨¡å—å’ŒåŒå±‚è®°å¿†æ£€ç´¢æ¨¡å—åï¼Œæ¼”å‘˜çš„å¤„ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šåœºæ™¯äº¤äº’ä¸­æ˜¾è‘—æé«˜äº†å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-05.html",
    "link_next": "2025-03-07.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "05.03",
        "en": "03/05",
        "zh": "3æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "07.03",
        "en": "03/07",
        "zh": "3æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "æœ€è¿‘çš„å¤§è¯­è¨€æ¨¡å‹è¿›å±•ä½¿å¾—åŸºäºLLMçš„ä»£ç†èƒ½å¤ŸæˆåŠŸå¤„ç†äº’åŠ¨å¼è§„åˆ’ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å—åˆ°è§„åˆ’å¹»è§‰çš„å›°æ‰°ï¼Œå¹¶ä¸”æ¯ä¸ªæ–°ä»£ç†éƒ½éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…ƒè§„åˆ’ä¼˜åŒ–ï¼ˆMPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥å¼•å…¥æ˜¾å¼æŒ‡å¯¼æ¥å¢å¼ºä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚MPOåˆ©ç”¨å…ƒè§„åˆ’æä¾›é«˜å±‚æ¬¡çš„é€šç”¨æŒ‡å¯¼ï¼Œå¸®åŠ©ä»£ç†è§„åˆ’ï¼Œå¹¶æ ¹æ®ä»»åŠ¡æ‰§è¡Œåé¦ˆæŒç»­ä¼˜åŒ–å…ƒè§„åˆ’ã€‚å®éªŒè¡¨æ˜ï¼ŒMPOåœ¨ä¸¤é¡¹ä»£è¡¨æ€§ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œå¹¶æé«˜äº†ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚",
        "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
        "pinyin": "æœ€è¿‘çš„å¤§è¯­è¨€æ¨¡å‹è¿›å±•ä½¿å¾—åŸºäºLLMçš„ä»£ç†èƒ½å¤ŸæˆåŠŸå¤„ç†äº’åŠ¨å¼è§„åˆ’ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å—åˆ°è§„åˆ’å¹»è§‰çš„å›°æ‰°ï¼Œå¹¶ä¸”æ¯ä¸ªæ–°ä»£ç†éƒ½éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…ƒè§„åˆ’ä¼˜åŒ–ï¼ˆMPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥å¼•å…¥æ˜¾å¼æŒ‡å¯¼æ¥å¢å¼ºä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚MPOåˆ©ç”¨å…ƒè§„åˆ’æä¾›é«˜å±‚æ¬¡çš„é€šç”¨æŒ‡å¯¼ï¼Œå¸®åŠ©ä»£ç†è§„åˆ’ï¼Œå¹¶æ ¹æ®ä»»åŠ¡æ‰§è¡Œåé¦ˆæŒç»­ä¼˜åŒ–å…ƒè§„åˆ’ã€‚å®éªŒè¡¨æ˜ï¼ŒMPOåœ¨ä¸¤é¡¹ä»£è¡¨æ€§ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œå¹¶æé«˜äº†ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n\nZuÃ¬jÃ¬n de dÃ  yÇ”yÃ¡n mÃ³xÃ­ng jÃ¬nzhÇn shÇdÃ© jÄ«yÃº LLM de dÃ ilÇ nÃ©nggÃ²u chÃ©nggÅng chÇ”lÇ hÃ¹dÃ²ngshÃ¬ guÄ«huÃ  rÃ¨nwÃ¹. RÃ¡n'Ã©r, xiÃ nyÇ’u fÄngfÇ chÃ¡ngchÃ¡ng shÃ²udÃ o guÄ«huÃ  huÃ njuÃ© de kÃ¹nhuÃ², bÃ¬ngqiÄ› mÄ›i gÃ¨ xÄ«n dÃ ilÇ dÅu xÅ«yÃ o chÃ³ngxÄ«n xÃ¹nliÃ n. WÃ¨i jiÄ›juÃ© zhÃ¨xiÄ“ tiÇozhÃ n, wÇ’men tÃ­chÅ«le yuÃ¡n guÄ«huÃ  yÅuhuÃ  (MPO) kuÃ ngjiÃ , tÅngguÃ² zhÃ­jiÄ“ yÇnrÃ¹ xiÇnshÃ¬ zhÇdÇo lÃ¡i zÄ“ngqiÃ¡ng dÃ ilÇ de guÄ«huÃ  nÃ©nglÃ¬. MPO lÃ¬yÃ²ng yuÃ¡n guÄ«huÃ  tÃ­gÅng gÄo cÃ©ngcÃ¬ de tÅngyÃ²ng zhÇdÇo, bÄngzhÃ¹ dÃ ilÇ guÄ«huÃ , bÃ¬nggÄ“njÃ¹ rÃ¨nwÃ¹ zhÃ­xÃ­ng fÇnkuÃ¬ chÃ­xÃ¹ yÅuhuÃ  yuÃ¡n guÄ«huÃ . ShÃ­yÃ n biÇomÃ­ng, MPO zÃ i liÇng xiÃ ng dÃ ibiÇoxÃ¬ng rÃ¨nwÃ¹ zhÅng xiÇnzhÃ¹ yÅuhuÃ n xiÃ nzhÃ¹n bÇzhÇ”n, bÃ¬ng tÃ­gÄole rÃ¨nwÃ¹ wÃ¡nchÃ©ng xiÃ olÇœ hÃ© fÃ nhuÃ  nÃ©nglÃ¬.",
        "vocab": "[{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'large language model'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'ä»£ç†', 'pinyin': 'dÃ ilÇ', 'trans': 'agent'},\n{'word': 'äº’åŠ¨å¼', 'pinyin': 'hÃ¹dÃ²ngshÃ¬', 'trans': 'interactive'},\n{'word': 'è§„åˆ’', 'pinyin': 'guÄ«huÃ ', 'trans': 'planning'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'},\n{'word': 'å¹»è§‰', 'pinyin': 'huÃ njuÃ©', 'trans': 'hallucination'},\n{'word': 'å›°æ‰°', 'pinyin': 'kÃ¹nrÇo', 'trans': 'trouble'},\n{'word': 'é‡æ–°', 'pinyin': 'chÃ³ngxÄ«n', 'trans': 'renew'},\n{'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹nliÃ n', 'trans': 'training'},\n{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇozhÃ n', 'trans': 'challenge'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ˜¾å¼', 'pinyin': 'xiÇnshÃ¬', 'trans': 'explicit'},\n{'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇdÇo', 'trans': 'guidance'},\n{'word': 'å¢å¼º', 'pinyin': 'zÄ“ngqiÃ¡ng', 'trans': 'enhance'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'ability'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬yÃ²ng', 'trans': 'utilize'},\n{'word': 'æä¾›', 'pinyin': 'tÃ­gÅng', 'trans': 'provide'},\n{'word': 'é«˜å±‚æ¬¡', 'pinyin': 'gÄo cÃ©ngcÃ¬', 'trans': 'high-level'},\n{'word': 'é€šç”¨', 'pinyin': 'tÅngyÃ²ng', 'trans': 'general'},\n{'word': 'å¸®åŠ©', 'pinyin': 'bÄngzhÃ¹', 'trans': 'help'},\n{'word': 'æ‰§è¡Œ', 'pinyin': 'zhÃ­xÃ­ng', 'trans': 'execute'},\n{'word': 'åé¦ˆ', 'pinyin': 'fÇnkuÃ¬', 'trans': 'feedback'},\n{'word': 'æŒç»­', 'pinyin': 'chÃ­xÃ¹', 'trans': 'continuous'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅuhuÃ ', 'trans': 'optimize'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'è¡¨æ˜', 'pinyin': 'biÇomÃ­ng', 'trans': 'indicate'},\n{'word': 'ä»£è¡¨æ€§', 'pinyin': 'dÃ ibiÇoxÃ¬ng', 'trans': 'representative'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅuyÃº', 'trans': 'superior to'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ nyÇ’u', 'trans': 'existing'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ«zhÇ”n', 'trans': 'benchmark'},\n{'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'},\n{'word': 'å®Œæˆ', 'pinyin': 'wÃ¡nchÃ©ng', 'trans': 'complete'},\n{'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'},\n{'word': 'æ³›åŒ–', 'pinyin': 'fÃ nhuÃ ', 'trans': 'generalize'}]",
        "trans": "Recent advancements in large language models have enabled LLM-based agents to successfully handle interactive planning tasks. However, existing methods often suffer from planning hallucinations, and each new agent requires retraining. To address these challenges, we propose the Meta-Planning Optimization (MPO) framework, which enhances the agent's planning capability by directly introducing explicit guidance. MPO leverages meta-planning to provide high-level, general guidance to assist the agent in planning and continuously optimizes the meta-planning based on task execution feedback. Experiments demonstrate that MPO significantly outperforms existing benchmarks in two representative tasks and improves task completion efficiency and generalization ability.",
        "update_ts": "2025-03-05 09:11"
    }
}