{
    "date": {
        "ru": "17 –∞–ø—Ä–µ–ª—è",
        "en": "April 17",
        "zh": "4Êúà17Êó•"
    },
    "time_utc": "2025-04-17 08:15",
    "weekday": 3,
    "issue_id": 3286,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.10514",
            "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
            "url": "https://huggingface.co/papers/2504.10514",
            "abstract": "Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.",
            "score": 16,
            "issue_id": 3281,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 –∞–ø—Ä–µ–ª—è",
                "en": "April 10",
                "zh": "4Êúà10Êó•"
            },
            "hash": "c786e69f24be2f9e",
            "authors": [
                "Yijun Liang",
                "Ming Li",
                "Chenrui Fan",
                "Ziyue Li",
                "Dang Nguyen",
                "Kwesi Cobbina",
                "Shweta Bhardwaj",
                "Jiuhai Chen",
                "Fuxiao Liu",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10514.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "üåà",
                "ru": {
                    "title": "ColorBench: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ü–≤–µ—Ç–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ColorBench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å –∏ –ø–æ–Ω–∏–º–∞—Ç—å —Ü–≤–µ—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 32 –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º —Ü–≤–µ—Ç–∞, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ö–æ—Ç—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –ª—É—á—à–µ, —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–±–æ–ª—å—à–æ–π, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ü–≤–µ—Ç–∞ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö. ColorBench –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ü–≤–µ—Ç–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–µ–ª–æ–≤–µ–∫–∞."
                },
                "en": {
                    "title": "Enhancing AI's Color Comprehension with ColorBench",
                    "desc": "This paper presents ColorBench, a benchmark designed to evaluate how vision-language models (VLMs) understand and utilize color in visual reasoning. It assesses various aspects of color perception, reasoning, and robustness through a series of real-world scenarios. The study finds that while larger models generally perform better, the existing VLMs show limited capabilities in color understanding, indicating a gap in their training. Additionally, the research highlights that while VLMs can use color cues effectively, they can also be misled by them, emphasizing the need for improved color comprehension in AI models."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢úËâ≤ÁêÜËß£ËÉΩÂäõ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜColorBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®ËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®È¢úËâ≤ÁêÜËß£ÊñπÈù¢ËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°Êõ¥Â§ßÁöÑÊ®°ÂûãÂú®ColorBench‰∏äË°®Áé∞Êõ¥Â•ΩÔºå‰ΩÜËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ΩúÁî®ÊØîËßÜËßâÁºñÁ†ÅÂô®Êõ¥‰∏∫ÈáçË¶Å„ÄÇÁé∞ÊúâÁöÑVLMsÂú®È¢úËâ≤ÁêÜËß£ÊñπÈù¢ÁöÑË°®Áé∞Â∑ÆË∑ùËæÉÂ∞èÔºåË°®ÊòéËøô‰∏ÄÈ¢ÜÂüüÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÈáçËßÜ„ÄÇÊ≠§Â§ñÔºåÂ∞ΩÁÆ°VLMsËÉΩÂ§üÂà©Áî®È¢úËâ≤Á∫øÁ¥¢Ôºå‰ΩÜÂú®Êüê‰∫õ‰ªªÂä°‰∏≠‰πüÂèØËÉΩ‰ºöÂèóÂà∞ËØØÂØº„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12240",
            "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
            "url": "https://huggingface.co/papers/2504.12240",
            "abstract": "The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.",
            "score": 15,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 –∞–ø—Ä–µ–ª—è",
                "en": "April 16",
                "zh": "4Êúà16Êó•"
            },
            "hash": "a237e12792a9a0c8",
            "authors": [
                "Junhao Zhuang",
                "Lingen Li",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Chun Yuan",
                "Ying Shan"
            ],
            "affiliations": [
                "Tencent ARC Lab, China",
                "The Chinese University of Hong Kong, China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12240.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#cv",
                    "#open_source",
                    "#architecture",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "Cobra: –ë—ã—Å—Ç—Ä–∞—è –∏ —Ç–æ—á–Ω–∞—è –∫–æ–ª–æ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–º–∏–∫—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Cobra –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∫–æ–ª–æ—Ä–∏–∑–∞—Ü–∏–∏ –∫–æ–º–∏–∫—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Causal Sparse DiT —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –∏ —Ç–æ—á–Ω–æ —Ä–∞—Å–∫—Ä–∞—à–∏–≤–∞—Ç—å –ª–∏–Ω–µ–π–Ω—ã–µ —Ä–∏—Å—É–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º –±–æ–ª–µ–µ 200 —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤. Cobra –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –æ—Ç–≤–µ—á–∞—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –∫–æ–º–∏–∫—Å–æ–≤."
                },
                "en": {
                    "title": "Cobra: Revolutionizing Line Art Colorization with Contextual Efficiency",
                    "desc": "This paper presents Cobra, a novel method for line art colorization in the comic production industry, which requires high accuracy and efficiency. Cobra utilizes a Causal Sparse DiT architecture that incorporates advanced techniques like causal sparse attention and positional encodings to handle over 200 reference images effectively. The method addresses challenges such as slow inference times and the need for flexible control, ensuring color identity consistency across diverse characters and backgrounds. Experimental results show that Cobra significantly improves the quality and speed of line art colorization, making it a valuable tool for artists."
                },
                "zh": {
                    "title": "CobraÔºöÈ´òÊïàÁÅµÊ¥ªÁöÑÁ∫øÊù°Ëâ∫ÊúØ‰∏äËâ≤Ëß£ÂÜ≥ÊñπÊ°à",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫CobraÁöÑÈ´òÊïàÁ∫øÊù°Ëâ∫ÊúØ‰∏äËâ≤ÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Êº´ÁîªÂà∂‰ΩúË°å‰∏ö‰∏≠ÂØπÈ´òÂáÜÁ°ÆÊÄßÂíåÁÅµÊ¥ªÊéßÂà∂ÁöÑÈúÄÊ±Ç„ÄÇCobraËÉΩÂ§üÂ§ÑÁêÜË∂ÖËøá200Âº†ÂèÇËÄÉÂõæÂÉèÔºåÂπ∂‰øùÊåÅ‰ΩéÂª∂ËøüÔºåÈÄÇÂ∫îÂ§çÊùÇÁöÑËßíËâ≤ÂíåËÉåÊôØ„ÄÇËØ•ÊñπÊ≥ïÈááÁî®‰∫ÜÂõ†ÊûúÁ®ÄÁñèDiTÊû∂ÊûÑÔºåÂà©Áî®ÁâπÊÆäËÆæËÆ°ÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÂíåÂõ†ÊûúÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊúâÊïàÁÆ°ÁêÜÈïø‰∏ä‰∏ãÊñáÂèÇËÄÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCobraÂú®‰∏äËâ≤Ë¥®ÈáèÂíåÊé®ÁêÜÈÄüÂ∫¶‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåÊª°Ë∂≥‰∫ÜÂ∑•‰∏öÁïåÁöÑÂÖ≥ÈîÆÈúÄÊ±Ç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12285",
            "title": "BitNet b1.58 2B4T Technical Report",
            "url": "https://huggingface.co/papers/2504.12285",
            "abstract": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.",
            "score": 9,
            "issue_id": 3281,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 –∞–ø—Ä–µ–ª—è",
                "en": "April 16",
                "zh": "4Êúà16Êó•"
            },
            "hash": "cf67f70d9f122792",
            "authors": [
                "Shuming Ma",
                "Hongyu Wang",
                "Shaohan Huang",
                "Xingxing Zhang",
                "Ying Hu",
                "Ting Song",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12285.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#science",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: 1-–±–∏—Ç–Ω–∞—è LLM –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ª–Ω–æ—Ç–æ—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω BitNet b1.58 2B4T - –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è 1-–±–∏—Ç–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (LLM) —Å 2 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –∏–∑ 4 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ—Ü–µ–Ω–µ–Ω–∞ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º, –≤–∫–ª—é—á–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ. BitNet b1.58 2B4T –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤–µ–¥—É—â–∏—Ö –ø–æ–ª–Ω–æ—Ç–æ—á–Ω—ã—Ö LLM –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –Ω–æ —Å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞–º–∏ –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ Hugging Face –≤–º–µ—Å—Ç–µ —Å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è GPU –∏ CPU."
                },
                "en": {
                    "title": "Efficient Language Understanding with BitNet: The 1-Bit Revolution",
                    "desc": "BitNet b1.58 2B4T is a groundbreaking 1-bit Large Language Model (LLM) with 2 billion parameters, making it the first of its kind to be open-source. It has been trained on an extensive dataset of 4 trillion tokens and evaluated on various benchmarks, showcasing its capabilities in language understanding, mathematical reasoning, coding, and conversation. Remarkably, BitNet achieves performance comparable to other leading full-precision LLMs while being more efficient in terms of memory usage, energy consumption, and decoding speed. The model's weights and inference implementations are made available on Hugging Face, promoting further research and practical applications."
                },
                "zh": {
                    "title": "ÂºÄÊ∫êÈ´òÊïàÁöÑ1‰ΩçÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜBitNet b1.58 2B4TÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ„ÄÅÂéüÁîüÁöÑ1‰ΩçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂèÇÊï∞ËßÑÊ®°ËææÂà∞20‰∫ø„ÄÇËØ•Ê®°ÂûãÂú®4‰∏á‰∫ø‰∏™Ê†áËÆ∞ÁöÑËØ≠ÊñôÂ∫ì‰∏äËøõË°åËÆ≠ÁªÉÔºåÂπ∂Âú®ËØ≠Ë®ÄÁêÜËß£„ÄÅÊï∞Â≠¶Êé®ÁêÜ„ÄÅÁºñÁ®ãËÉΩÂäõÂíåÂØπËØùËÉΩÂäõÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠ËøõË°å‰∫Ü‰∏•Ê†ºËØÑ‰º∞„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåBitNet b1.58 2B4TÂú®ÊÄßËÉΩ‰∏ä‰∏éÂêåÁ±ªËßÑÊ®°ÁöÑÈ¢ÜÂÖàÂºÄÊ∫êÂÖ®Á≤æÂ∫¶Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁõ∏ÂΩìÔºåÂêåÊó∂Âú®ËÆ°ÁÆóÊïàÁéá‰∏äÂÖ∑ÊúâÊòæËëó‰ºòÂäøÔºåÂåÖÊã¨ÊòæËëóÂáèÂ∞ëÁöÑÂÜÖÂ≠òÂç†Áî®„ÄÅËÉΩËÄóÂíåËß£Á†ÅÂª∂Ëøü„ÄÇ‰∏∫‰∫Ü‰øÉËøõËøõ‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂ÂíåÂ∫îÁî®ÔºåËØ•Ê®°ÂûãÁöÑÊùÉÈáçÈÄöËøáHugging FaceÂèëÂ∏ÉÔºåÂπ∂Êèê‰æõ‰∫ÜÈÄÇÁî®‰∫éGPUÂíåCPUÊû∂ÊûÑÁöÑÂºÄÊ∫êÊé®ÁêÜÂÆûÁé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10326",
            "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
            "url": "https://huggingface.co/papers/2504.10326",
            "abstract": "AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks.",
            "score": 7,
            "issue_id": 3286,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 –∞–ø—Ä–µ–ª—è",
                "en": "April 14",
                "zh": "4Êúà14Êó•"
            },
            "hash": "030a9ba5449806fb",
            "authors": [
                "Yangshen Deng",
                "Zhengxin You",
                "Long Xiang",
                "Qilong Li",
                "Peiqi Yuan",
                "Zhaoyang Hong",
                "Yitao Zheng",
                "Wanting Li",
                "Runzhong Li",
                "Haotian Liu",
                "Kyriakos Mouratidis",
                "Man Lung Yiu",
                "Huan Li",
                "Qiaomu Shen",
                "Rui Mao",
                "Bo Tang"
            ],
            "affiliations": [
                "AlayaDB AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10326.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#long_context",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "AlayaDB: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "AlayaDB - —ç—Ç–æ –ø–µ—Ä–µ–¥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ –æ—Ç–¥–µ–ª—è–µ—Ç KV-–∫—ç—à –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –æ—Ç —Å–∏—Å—Ç–µ–º –≤—ã–≤–æ–¥–∞ LLM, –∏–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É—è –∏—Ö –≤ –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö. AlayaDB –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç –º–µ–Ω—å—à–µ –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –Ω–∞–≥—Ä—É–∑–æ–∫ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏. –ö–ª—é—á–µ–≤—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º AlayaDB —è–≤–ª—è–µ—Ç—Å—è –∞–±—Å—Ç—Ä–∞–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—ç—à–µ–º –¥–ª—è –≤—ã–≤–æ–¥–∞ LLM –≤ –ø—Ä–æ—Ü–µ–¥—É—Ä—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –Ω–∞—Ç–∏–≤–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤."
                },
                "en": {
                    "title": "Revolutionizing LLM Inference with AlayaDB",
                    "desc": "AlayaDB is an innovative vector database designed to enhance long-context inference for Large Language Models (LLMs). It separates key-value (KV) caching and attention computation from the LLM inference process, streamlining these functions into a dedicated database system. This architecture allows Model as a Service (MaaS) providers to use fewer hardware resources while achieving superior generation quality across various workloads. The system's core feature is its ability to treat attention computation and cache management as a query processing task, which is further optimized by a specialized query optimizer."
                },
                "zh": {
                    "title": "AlayaDBÔºöÈ´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜËß£ÂÜ≥ÊñπÊ°à",
                    "desc": "AlayaDBÊòØ‰∏ÄÁßçÂÖàËøõÁöÑÂêëÈáèÊï∞ÊçÆÂ∫ìÁ≥ªÁªüÔºå‰∏ì‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÆæËÆ°ÔºåÊó®Âú®ÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÁöÑÊïàÁéáÂíåÊïàÊûú„ÄÇÂÆÉÂ∞ÜÈîÆÂÄºÁºìÂ≠òÂíåÊ≥®ÊÑèÂäõËÆ°ÁÆó‰ªéLLMÊé®ÁêÜÁ≥ªÁªü‰∏≠Ëß£ËÄ¶ÔºåÂπ∂Â∞ÜÂÖ∂Â∞ÅË£ÖÂà∞‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂêëÈáèÊï∞ÊçÆÂ∫ìÁ≥ªÁªü‰∏≠„ÄÇ‰∏éÁé∞ÊúâËß£ÂÜ≥ÊñπÊ°àÁõ∏ÊØîÔºåAlayaDBÂú®Á°¨‰ª∂ËµÑÊ∫êÊ∂àËÄóÂíåÁîüÊàêË¥®ÈáèÊñπÈù¢Ë°®Áé∞Êõ¥‰Ω≥ÔºåÈÄÇÁî®‰∫é‰∏çÂêåÊúçÂä°Ê∞¥Âπ≥ÁõÆÊ†áÔºàSLOÔºâÁöÑÂ§öÁßçÂ∑•‰ΩúË¥üËΩΩ„ÄÇËØ•Á≥ªÁªüÈÄöËøáÂ∞ÜÊ≥®ÊÑèÂäõËÆ°ÁÆóÂíåÁºìÂ≠òÁÆ°ÁêÜÊäΩË±°‰∏∫Êü•ËØ¢Â§ÑÁêÜËøáÁ®ãÔºåÂπ∂ÈÄöËøáÊú¨Âú∞Êü•ËØ¢‰ºòÂåñÂô®‰ºòÂåñÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11952",
            "title": "Robust and Fine-Grained Detection of AI Generated Texts",
            "url": "https://huggingface.co/papers/2504.11952",
            "abstract": "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.",
            "score": 2,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 –∞–ø—Ä–µ–ª—è",
                "en": "April 16",
                "zh": "4Êúà16Êó•"
            },
            "hash": "bdea465fe17b9401",
            "authors": [
                "Ram Mohan Rao Kadiyala",
                "Siddartha Pullakhandam",
                "Kanwal Mehreen",
                "Drishti Sharma",
                "Siddhant Gupta",
                "Jebish Purbey",
                "Ashay Srivastava",
                "Subhasya TippaReddy",
                "Arvind Reddy Bobbili",
                "Suraj Telugara Chandrashekhar",
                "Modabbir Adeeb",
                "Srinadh Vura",
                "Hamza Farooq"
            ],
            "affiliations": [
                "Cohere for AI Community",
                "IISc Bangalore",
                "IIT Roorkee",
                "M2ai.in",
                "Pulchowk Campus",
                "Stanford University",
                "Traversaal.ai",
                "University of California, Los Angeles",
                "University of Houston",
                "University of Maryland, College Park",
                "University of South Florida",
                "Vantager"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11952.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#hallucinations",
                    "#dataset",
                    "#benchmark",
                    "#multilingual",
                    "#security",
                    "#data"
                ],
                "emoji": "üïµÔ∏è",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –ò–ò-—Ç–µ–∫—Å—Ç–æ–≤: –æ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ —è–∑—ã–∫–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ–±—à–∏—Ä–Ω–æ–º –Ω–∞–±–æ—Ä–µ —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Å–æ–≤–º–µ—Å—Ç–Ω–æ —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –ò–ò. –ú–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö –∏–∑ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π, –æ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏ –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö —Å —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 2,4 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ 23 —è–∑—ã–∫–∞—Ö, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –≤ —Å–æ–∞–≤—Ç–æ—Ä—Å—Ç–≤–µ —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏."
                },
                "en": {
                    "title": "Advancing Detection of Human-LLM Co-Authored Texts",
                    "desc": "This paper addresses the challenge of detecting machine-generated content, particularly in cases where texts are co-authored by humans and language models (LLMs). The authors developed a set of token classification models trained on a large dataset of 2.4 million co-authored texts, which allows for better detection across various domains and generators. The models demonstrated strong performance even with adversarial inputs and texts from non-native speakers. Additionally, the paper provides insights into how the models perform based on text length and characteristics compared to purely human-authored content."
                },
                "zh": {
                    "title": "ÊûÑÂª∫È´òÊïàÁöÑÊú∫Âô®ÁîüÊàêÂÜÖÂÆπÊ£ÄÊµãÁ≥ªÁªü",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁêÜÊÉ≥ÁöÑÊ£ÄÊµãÁ≥ªÁªüÔºåÊó®Âú®ÊúâÊïàËØÜÂà´Êú∫Âô®ÁîüÊàêÁöÑÂÜÖÂÆπÔºåÂ∞§ÂÖ∂ÊòØÂú®Áü≠ÊñáÊú¨‰∏≠„ÄÇÁé∞ÊúâÁ≥ªÁªüÂú®ËØÜÂà´AIÁîüÊàêÂÜÖÂÆπÊó∂Â∏∏Â∏∏Èù¢‰∏¥ÊåëÊàòÔºåÂõ†Ê≠§Êàë‰ª¨‰∏ìÊ≥®‰∫é‰∫∫Á±ª‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂÖ±ÂêåÂàõ‰ΩúÁöÑÊñáÊú¨„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏ÄÁ≥ªÂàóÁî®‰∫éÊ†áËÆ∞ÂàÜÁ±ªÁöÑÊ®°ÂûãÔºåËøô‰∫õÊ®°ÂûãÂú®Â§ßÈáè‰∫∫Êú∫ÂÖ±ÂàõÊñáÊú¨‰∏äËøõË°åËÆ≠ÁªÉÔºåÂπ∂Âú®Êú™ËßÅÈ¢ÜÂüüÂíåÁîüÊàêÂô®ÁöÑÊñáÊú¨‰∏äË°®Áé∞ËâØÂ•Ω„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂåÖÂê´240‰∏áÊù°ÊñáÊú¨ÁöÑÊñ∞Êï∞ÊçÆÈõÜÔºå‰∏ªË¶ÅÁî±Â§öÁßçÊµÅË°åÁöÑ‰∏ìÊúâLLMÂÖ±ÂêåÂàõ‰ΩúÔºåÊ∂µÁõñ23ÁßçËØ≠Ë®Ä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10483",
            "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
            "url": "https://huggingface.co/papers/2504.10483",
            "abstract": "In this paper we tackle a fundamental question: \"Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?\" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io.",
            "score": 2,
            "issue_id": 3285,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 –∞–ø—Ä–µ–ª—è",
                "en": "April 14",
                "zh": "4Êúà14Êó•"
            },
            "hash": "f46935088154b083",
            "authors": [
                "Xingjian Leng",
                "Jaskirat Singh",
                "Yunzhong Hou",
                "Zhenchang Xing",
                "Saining Xie",
                "Liang Zheng"
            ],
            "affiliations": [
                "Australian National University",
                "Data61 CSIRO",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10483.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: —Å–æ–≤–º–µ—Å—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å VAE",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ (VAE) –≤ —Å–∫–≤–æ–∑–Ω–æ–º —Ä–µ–∂–∏–º–µ. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è —Ç–∞–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π (REPA). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ (REPA-E) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —É–ª—É—á—à–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ VAE. –ü–æ–¥—Ö–æ–¥ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–µ FID –Ω–∞ ImageNet 256x256."
                },
                "en": {
                    "title": "Unlocking End-to-End Training with REPA for Latent Diffusion Models",
                    "desc": "This paper explores the possibility of training latent diffusion models alongside a variational auto-encoder (VAE) in an end-to-end fashion. It highlights that traditional methods using standard diffusion loss are ineffective and can even harm performance. The authors introduce a new loss function called representation-alignment (REPA) loss, which enables effective joint training of the VAE and diffusion model. Their proposed training method, REPA-E, significantly accelerates training and enhances the quality of generated outputs, achieving state-of-the-art results on ImageNet."
                },
                "zh": {
                    "title": "Á´ØÂà∞Á´ØËÆ≠ÁªÉÁöÑÁ™ÅÁ†¥ÔºöREPAÊçüÂ§±ÁöÑÂäõÈáè",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏Ä‰∏™Âü∫Êú¨ÈóÆÈ¢òÔºöËÉΩÂê¶Â∞ÜÊΩúÂú®Êâ©Êï£Ê®°Âûã‰∏éÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÊ†áËÆ∞Âô®‰∏ÄËµ∑ËøõË°åÁ´ØÂà∞Á´ØËÆ≠ÁªÉÔºü‰º†ÁªüÁöÑÊ∑±Â∫¶Â≠¶‰π†ÁêÜËÆ∫ËÆ§‰∏∫ÔºåÂ∞ΩÂèØËÉΩËøõË°åÁ´ØÂà∞Á´ØËÆ≠ÁªÉÊòØÊõ¥‰ºòÁöÑÈÄâÊã©„ÄÇÁÑ∂ËÄåÔºåÂØπ‰∫éÊΩúÂú®Êâ©Êï£ÂèòÊç¢Âô®Ôºå‰ΩøÁî®Ê†áÂáÜÊâ©Êï£ÊçüÂ§±ËøõË°åÁ´ØÂà∞Á´ØËÆ≠ÁªÉ‰ºöÂØºËá¥ÊïàÊûú‰∏ç‰Ω≥ÔºåÁîöËá≥Èôç‰ΩéÊúÄÁªàÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÈÄöËøáË°®Á§∫ÂØπÈΩêÊçüÂ§±ÔºàREPAÊçüÂ§±ÔºâÊù•Ëß£ÈîÅÁ´ØÂà∞Á´ØËÆ≠ÁªÉÔºå‰ΩøÂæóVAEÂíåÊâ©Êï£Ê®°ÂûãËÉΩÂ§üÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂÖ±ÂêåË∞ÉÊï¥ÔºåÊúÄÁªàÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-04-16.html",
    "link_next": "2025-04-18.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4Êúà16Êó•"
    },
    "short_date_next": {
        "ru": "18.04",
        "en": "04/18",
        "zh": "4Êúà18Êó•"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜËÉΩÂäõÁöÑÂÖ¥Ë∂£„ÄÇÁõÆÂâçÁöÑÊñπÊ≥ï‰æùËµñÁõëÁù£‰ø°Âè∑ÔºåÂ≠òÂú®ÂèØÊâ©Â±ïÊÄßÂíåÈ´òÊ†áÊ≥®ÊàêÊú¨ÈóÆÈ¢ò„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÁõëÁù£ÁöÑËá™ËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂêç‰∏∫Genius„ÄÇGeniusÈÄöËøáÊ≠•ËøõÂºèÈ¢ÑÊµãÈáçÈááÊ†∑Á≠ñÁï•Âíå‰ºòÂäøÊ†°ÂáÜ‰ºòÂåñÔºàACOÔºâÊçüÂ§±ÂáΩÊï∞ÔºåÂÆûÁé∞‰∫ÜÊó†ÈúÄÂ§ñÈÉ®ÁõëÁù£ÁöÑLLMÊé®ÁêÜËÉΩÂäõÊèêÂçá„ÄÇ‰ª£Á†ÅÂ∞ÜÂú®https://github.com/xufangzhi/GeniusÂèëÂ∏É„ÄÇ",
        "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
        "pinyin": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜËÉΩÂäõÁöÑÂÖ¥Ë∂£„ÄÇ\nZh√® piƒÅn w√©nzhƒÅng t«éol√πn le t√≠shƒìng d√†x√≠ng y«îy√°n m√≥x√≠ng (LLM) tuƒ´l«ê n√©ngl√¨ de x√¨ngq√π.\n\nÁõÆÂâçÁöÑÊñπÊ≥ï‰æùËµñÁõëÁù£‰ø°Âè∑ÔºåÂ≠òÂú®ÂèØÊâ©Â±ïÊÄßÂíåÈ´òÊ†áÊ≥®ÊàêÊú¨ÈóÆÈ¢ò„ÄÇ\nM√πqi√°n de fƒÅngf«é yƒ´l√†i ji√†nd≈´ x√¨nh√†",
        "vocab": "[\n    {\"word\": \"ËÆ®ËÆ∫\", \"pinyin\": \"t«éo l√πn\", \"trans\": \"discuss\"},\n    {\"word\": \"ÊèêÂçá\", \"pinyin\": \"t√≠ shƒìng\", \"trans\": \"improve\"},\n    {\"word\": \"Â§ßÂûã\", \"pinyin\": \"d√† x√≠ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"ËØ≠Ë®ÄÊ®°Âûã\", \"pinyin\": \"y«î y√°n m√≥ x√≠ng\", \"trans\": \"language model\"},\n    {\"word\": \"Êé®ÁêÜ\", \"pinyin\": \"tuƒ´ l«ê\", \"trans\": \"reasoning\"},\n    {\"word\": \"ÂÖ¥Ë∂£\", \"pinyin\": \"x√¨ng q√π\", \"trans\": \"interest\"},\n    {\"word\": \"‰æùËµñ\", \"pinyin\": \"yƒ´ l√†i\", \"trans\": \"depend on\"},\n    {\"word\": \"ÁõëÁù£\", \"pinyin\": \"ji√†n d≈´\", \"trans\": \"supervised\"},\n    {\"word\": \"‰ø°Âè∑\", \"pinyin\": \"x√¨n h√†o\", \"trans\": \"signal\"},\n    {\"word\": \"ÂèØÊâ©Â±ïÊÄß\", \"pinyin\": \"kƒõ ku√≤ zh«én x√¨ng\", \"trans\": \"scalability\"},\n    {\"word\": \"È´ò\", \"pinyin\": \"gƒÅo\", \"trans\": \"high\"},\n    {\"word\": \"Ê†áÊ≥®\", \"pinyin\": \"biƒÅo zh√π\", \"trans\": \"annotation\"},\n    {\"word\": \"ÊàêÊú¨\", \"pinyin\": \"ch√©ng bƒõn\", \"trans\": \"cost\"},\n    {\"word\": \"ÊèêÂá∫\", \"pinyin\": \"t√≠ ch≈´\", \"trans\": \"propose\"},\n    {\"word\": \"Êó†ÁõëÁù£\", \"pinyin\": \"w√∫ ji√†n d≈´\", \"trans\": \"unsupervised\"},\n    {\"word\": \"Ëá™ËÆ≠ÁªÉ\", \"pinyin\": \"z√¨ x√πn li√†n\", \"trans\": \"self-training\"},\n    {\"word\": \"Ê°ÜÊû∂\", \"pinyin\": \"ku√†ng ji√†\", \"trans\": \"framework\"},\n    {\"word\": \"Âêç‰∏∫\", \"pinyin\": \"m√≠ng w√©i\", \"trans\": \"named\"},\n    {\"word\": \"Ê≠•ËøõÂºè\", \"pinyin\": \"b√π j√¨n sh√¨\", \"trans\": \"stepwise\"},\n    {\"word\": \"È¢ÑÊµã\", \"pinyin\": \"y√π c√®\", \"trans\": \"prediction\"},\n    {\"word\": \"ÈáçÈááÊ†∑\", \"pinyin\": \"ch√≥ng c«éi y√†ng\", \"trans\": \"resampling\"},\n    {\"word\": \"Á≠ñÁï•\", \"pinyin\": \"c√® l√º√®\", \"trans\": \"strategy\"},\n    {\"word\": \"‰ºòÂäø\", \"pinyin\": \"y≈çu sh√¨\", \"trans\": \"advantage\"},\n    {\"word\": \"Ê†°ÂáÜ\", \"pinyin\": \"ji√†o zh«în\", \"trans\": \"calibration\"},\n    {\"word\": \"‰ºòÂåñ\", \"pinyin\": \"y≈çu hu√†\", \"trans\": \"optimization\"},\n    {\"word\": \"ÊçüÂ§±ÂáΩÊï∞\", \"pinyin\": \"s«în shƒ´ h√°n sh√π\", \"trans\": \"loss function\"},\n    {\"word\": \"ÂÆûÁé∞\", \"pinyin\": \"sh√≠ xi√†n\", \"trans\": \"achieve\"},\n    {\"word\": \"Â§ñÈÉ®\", \"pinyin\": \"w√†i b√π\", \"trans\": \"external\"},\n    {\"word\": \"ÂèëÂ∏É\", \"pinyin\": \"fƒÅ b√π\", \"trans\": \"release\"}\n]",
        "trans": "This article discusses the interest in enhancing the reasoning capabilities of large language models (LLMs). Current methods rely on supervised signals, which present issues with scalability and high annotation costs. The authors",
        "update_ts": "2025-04-16 09:12"
    }
}