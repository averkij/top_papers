{
    "date": {
        "ru": "27 января",
        "en": "January 27",
        "zh": "1月27日"
    },
    "time_utc": "2025-01-27 11:08",
    "weekday": 0,
    "issue_id": 1880,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.14249",
            "title": "Humanity's Last Exam",
            "url": "https://huggingface.co/papers/2501.14249",
            "abstract": "Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 3,000 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.",
            "score": 17,
            "issue_id": 1873,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 января",
                "en": "January 24",
                "zh": "1月24日"
            },
            "hash": "4d614974221756d3",
            "authors": [
                "Long Phan",
                "Alice Gatti",
                "Ziwen Han",
                "Nathaniel Li",
                "Josephina Hu",
                "Hugh Zhang",
                "Sean Shi",
                "Michael Choi",
                "Anish Agrawal",
                "Arnav Chopra",
                "Adam Khoja",
                "Ryan Kim",
                "Jason Hausenloy",
                "Oliver Zhang",
                "Mantas Mazeika",
                "Daron Anderson",
                "Tung Nguyen",
                "Mobeen Mahmood",
                "Fiona Feng",
                "Steven Y. Feng",
                "Haoran Zhao",
                "Michael Yu",
                "Varun Gangal",
                "Chelsea Zou",
                "Zihan Wang",
                "Jessica P. Wang",
                "Pawan Kumar",
                "Oleksandr Pokutnyi",
                "Robert Gerbicz",
                "Serguei Popov",
                "John-Clark Levin",
                "Mstyslav Kazakov",
                "Johannes Schmitt",
                "Geoff Galgon",
                "Alvaro Sanchez",
                "Yongki Lee",
                "Will Yeadon",
                "Scott Sauers",
                "Marc Roth",
                "Chidozie Agu",
                "Søren Riis",
                "Fabian Giska",
                "Saiteja Utpala",
                "Zachary Giboney",
                "Gashaw M. Goshu",
                "Joan of Arc Xavier",
                "Sarah-Jane Crowson",
                "Mohinder Maheshbhai Naiya",
                "Noah Burns",
                "Lennart Finke",
                "Zerui Cheng",
                "Hyunwoo Park",
                "Francesco Fournier-Facio",
                "John Wydallis",
                "Mark Nandor",
                "Ankit Singh",
                "Tim Gehrunger",
                "Jiaqi Cai",
                "Ben McCarty",
                "Darling Duclosel",
                "Jungbae Nam",
                "Jennifer Zampese",
                "Ryan G. Hoerr",
                "Aras Bacho",
                "Gautier Abou Loume",
                "Abdallah Galal",
                "Hangrui Cao",
                "Alexis C Garretson",
                "Damien Sileo",
                "Qiuyu Ren",
                "Doru Cojoc",
                "Pavel Arkhipov",
                "Usman Qazi",
                "Lianghui Li",
                "Sumeet Motwani",
                "Christian Schroeder de Witt",
                "Edwin Taylor",
                "Johannes Veith",
                "Eric Singer",
                "Taylor D. Hartman",
                "Paolo Rissone",
                "Jaehyeok Jin",
                "Jack Wei Lun Shi",
                "Chris G. Willcocks",
                "Joshua Robinson",
                "Aleksandar Mikov",
                "Ameya Prabhu",
                "Longke Tang",
                "Xavier Alapont",
                "Justine Leon Uro",
                "Kevin Zhou",
                "Emily de Oliveira Santos",
                "Andrey Pupasov Maksimov",
                "Edward Vendrow",
                "Kengo Zenitani",
                "Julien Guillod",
                "Yuqi Li",
                "Joshua Vendrow",
                "Vladyslav Kuchkin",
                "Ng Ze-An",
                "Pierre Marion",
                "Denis Efremov",
                "Jayson Lynch",
                "Kaiqu Liang",
                "Andrew Gritsevskiy",
                "Dakotah Martinez",
                "Ben Pageler",
                "Nick Crispino",
                "Dimitri Zvonkine",
                "Natanael Wildner Fraga",
                "Saeed Soori",
                "Ori Press",
                "Henry Tang",
                "Julian Salazar",
                "Sean R. Green",
                "Lina Brüssel",
                "Moon Twayana",
                "Aymeric Dieuleveut",
                "T. Ryan Rogers",
                "Wenjin Zhang",
                "Bikun Li",
                "Jinzhou Yang",
                "Arun Rao",
                "Gabriel Loiseau",
                "Mikhail Kalinin",
                "Marco Lukas",
                "Ciprian Manolescu",
                "Subrata Mishra",
                "Ariel Ghislain Kemogne Kamdoum",
                "Tobias Kreiman",
                "Tad Hogg",
                "Alvin Jin",
                "Carlo Bosio",
                "Gongbo Sun",
                "Brian P Coppola",
                "Tim Tarver",
                "Haline Heidinger",
                "Rafael Sayous",
                "Stefan Ivanov",
                "Joseph M Cavanagh",
                "Jiawei Shen",
                "Joseph Marvin Imperial",
                "Philippe Schwaller",
                "Shaipranesh Senthilkuma",
                "Andres M Bran",
                "Ali Dehghan",
                "Andres Algaba",
                "Brecht Verbeken",
                "David Noever",
                "Ragavendran P V",
                "Lisa Schut",
                "Ilia Sucholutsky",
                "Evgenii Zheltonozhskii",
                "Derek Lim",
                "Richard Stanley",
                "Shankar Sivarajan",
                "Tong Yang",
                "John Maar",
                "Julian Wykowski",
                "Martí Oller",
                "Jennifer Sandlin",
                "Anmol Sahu",
                "Yuzheng Hu",
                "Sara Fish",
                "Nasser Heydari",
                "Archimedes Apronti",
                "Kaivalya Rawal",
                "Tobias Garcia Vilchis",
                "Yuexuan Zu",
                "Martin Lackner",
                "James Koppel",
                "Jeremy Nguyen",
                "Daniil S. Antonenko",
                "Steffi Chern",
                "Bingchen Zhao",
                "Pierrot Arsene",
                "Alan Goldfarb",
                "Sergey Ivanov",
                "Rafał Poświata",
                "Chenguang Wang",
                "Daofeng Li",
                "Donato Crisostomi",
                "Andrea Achilleos",
                "Benjamin Myklebust",
                "Archan Sen",
                "David Perrella",
                "Nurdin Kaparov",
                "Mark H Inlow",
                "Allen Zang",
                "Elliott Thornley",
                "Daniil Orel",
                "Vladislav Poritski",
                "Shalev Ben-David",
                "Zachary Berger",
                "Parker Whitfill",
                "Michael Foster",
                "Daniel Munro",
                "Linh Ho",
                "Dan Bar Hava",
                "Aleksey Kuchkin",
                "Robert Lauff",
                "David Holmes",
                "Frank Sommerhage",
                "Keith Schneider",
                "Zakayo Kazibwe",
                "Nate Stambaugh",
                "Mukhwinder Singh",
                "Ilias Magoulas",
                "Don Clarke",
                "Dae Hyun Kim",
                "Felipe Meneguitti Dias",
                "Veit Elser",
                "Kanu Priya Agarwal",
                "Victor Efren Guadarrama Vilchis",
                "Immo Klose",
                "Christoph Demian",
                "Ujjwala Anantheswaran",
                "Adam Zweiger",
                "Guglielmo Albani",
                "Jeffery Li",
                "Nicolas Daans",
                "Maksim Radionov",
                "Václav Rozhoň",
                "Ziqiao Ma",
                "Christian Stump",
                "Mohammed Berkani",
                "Jacob Platnick",
                "Volodymyr Nevirkovets",
                "Luke Basler",
                "Marco Piccardo",
                "Ferenc Jeanplong",
                "Niv Cohen",
                "Josef Tkadlec",
                "Paul Rosu",
                "Piotr Padlewski",
                "Stanislaw Barzowski",
                "Kyle Montgomery",
                "Aline Menezes",
                "Arkil Patel",
                "Zixuan Wang",
                "Jamie Tucker-Foltz",
                "Jack Stade",
                "Tom Goertzen",
                "Fereshteh Kazemi",
                "Jeremiah Milbauer",
                "John Arnold Ambay",
                "Abhishek Shukla",
                "Yan Carlos Leyva Labrador",
                "Alan Givré",
                "Hew Wolff",
                "Vivien Rossbach",
                "Muhammad Fayez Aziz",
                "Younesse Kaddar",
                "Yanxu Chen",
                "Robin Zhang",
                "Jiayi Pan",
                "Antonio Terpin",
                "Niklas Muennighoff",
                "Hailey Schoelkopf",
                "Eric Zheng",
                "Avishy Carmi",
                "Adam Jones",
                "Jainam Shah",
                "Ethan D. L. Brown",
                "Kelin Zhu",
                "Max Bartolo",
                "Richard Wheeler",
                "Andrew Ho",
                "Shaul Barkan",
                "Jiaqi Wang",
                "Martin Stehberger",
                "Egor Kretov",
                "Kaustubh Sridhar",
                "Zienab EL-Wasif",
                "Anji Zhang",
                "Daniel Pyda",
                "Joanna Tam",
                "David M. Cunningham",
                "Vladimir Goryachev",
                "Demosthenes Patramanis",
                "Michael Krause",
                "Andrew Redenti",
                "Daniel Bugas",
                "David Aldous",
                "Jesyin Lai",
                "Shannon Coleman",
                "Mohsen Bahaloo",
                "Jiangnan Xu",
                "Sangwon Lee",
                "Sandy Zhao",
                "Ning Tang",
                "Michael K. Cohen",
                "Micah Carroll",
                "Orr Paradise",
                "Jan Hendrik Kirchner",
                "Stefan Steinerberger",
                "Maksym Ovchynnikov",
                "Jason O. Matos",
                "Adithya Shenoy",
                "Benedito Alves de Oliveira Junior",
                "Michael Wang",
                "Yuzhou Nie",
                "Paolo Giordano",
                "Philipp Petersen",
                "Anna Sztyber-Betley",
                "Priti Shukla",
                "Jonathan Crozier",
                "Antonella Pinto",
                "Shreyas Verma",
                "Prashant Joshi",
                "Zheng-Xin Yong",
                "Allison Tee",
                "Jérémy Andréoletti",
                "Orion Weller",
                "Raghav Singhal",
                "Gang Zhang",
                "Alexander Ivanov",
                "Seri Khoury",
                "Hamid Mostaghimi",
                "Kunvar Thaman",
                "Qijia Chen",
                "Tran Quoc Khánh",
                "Jacob Loader",
                "Stefano Cavalleri",
                "Hannah Szlyk",
                "Zachary Brown",
                "Jonathan Roberts",
                "William Alley",
                "Kunyang Sun",
                "Ryan Stendall",
                "Max Lamparth",
                "Anka Reuel",
                "Ting Wang",
                "Hanmeng Xu",
                "Sreenivas Goud Raparthi",
                "Pablo Hernández-Cámara",
                "Freddie Martin",
                "Dmitry Malishev",
                "Thomas Preu",
                "Tomek Korbak",
                "Marcus Abramovitch",
                "Dominic Williamson",
                "Ziye Chen",
                "Biró Bálint",
                "M Saiful Bari",
                "Peyman Kassani",
                "Zihao Wang",
                "Behzad Ansarinejad",
                "Laxman Prasad Goswami",
                "Yewen Sun",
                "Hossam Elgnainy",
                "Daniel Tordera",
                "George Balabanian",
                "Earth Anderson",
                "Lynna Kvistad",
                "Alejandro José Moyano",
                "Rajat Maheshwari",
                "Ahmad Sakor",
                "Murat Eron",
                "Isaac C. McAlister",
                "Javier Gimenez",
                "Innocent Enyekwe",
                "Andrew Favre D. O.",
                "Shailesh Shah",
                "Xiaoxiang Zhou",
                "Firuz Kamalov",
                "Ronald Clark",
                "Sherwin Abdoli",
                "Tim Santens",
                "Khalida Meer",
                "Harrison K Wang",
                "Kalyan Ramakrishnan",
                "Evan Chen",
                "Alessandro Tomasiello",
                "G. Bruno De Luca",
                "Shi-Zhuo Looi",
                "Vinh-Kha Le",
                "Noam Kolt",
                "Niels Mündler",
                "Avi Semler",
                "Emma Rodman",
                "Jacob Drori",
                "Carl J Fossum",
                "Milind Jagota",
                "Ronak Pradeep",
                "Honglu Fan",
                "Tej Shah",
                "Jonathan Eicher",
                "Michael Chen",
                "Kushal Thaman",
                "William Merrill",
                "Carter Harris",
                "Jason Gross",
                "Ilya Gusev",
                "Asankhaya Sharma",
                "Shashank Agnihotri",
                "Pavel Zhelnov",
                "Siranut Usawasutsakorn",
                "Mohammadreza Mofayezi",
                "Sergei Bogdanov",
                "Alexander Piperski",
                "Marc Carauleanu",
                "David K. Zhang",
                "Dylan Ler",
                "Roman Leventov",
                "Ignat Soroko",
                "Thorben Jansen",
                "Pascal Lauer",
                "Joshua Duersch",
                "Vage Taamazyan",
                "Wiktor Morak",
                "Wenjie Ma",
                "William Held",
                "Tran Đuc Huy",
                "Ruicheng Xian",
                "Armel Randy Zebaze",
                "Mohanad Mohamed",
                "Julian Noah Leser",
                "Michelle X Yuan",
                "Laila Yacar",
                "Johannes Lengler",
                "Hossein Shahrtash",
                "Edson Oliveira",
                "Joseph W. Jackson",
                "Daniel Espinosa Gonzalez",
                "Andy Zou",
                "Muthu Chidambaram",
                "Timothy Manik",
                "Hector Haffenden",
                "Dashiell Stander",
                "Ali Dasouqi",
                "Alexander Shen",
                "Emilien Duc",
                "Bita Golshani",
                "David Stap",
                "Mikalai Uzhou",
                "Alina Borisovna Zhidkovskaya",
                "Lukas Lewark",
                "Mátyás Vincze",
                "Dustin Wehr",
                "Colin Tang",
                "Zaki Hossain",
                "Shaun Phillips",
                "Jiang Muzhen",
                "Fredrik Ekström",
                "Angela Hammon",
                "Oam Patel",
                "Nicolas Remy",
                "Faraz Farhidi",
                "George Medley",
                "Forough Mohammadzadeh",
                "Madellene Peñaflor",
                "Haile Kassahun",
                "Alena Friedrich",
                "Claire Sparrow",
                "Taom Sakal",
                "Omkar Dhamane",
                "Ali Khajegili Mirabadi",
                "Eric Hallman",
                "Mike Battaglia",
                "Mohammad Maghsoudimehrabani",
                "Hieu Hoang",
                "Alon Amit",
                "Dave Hulbert",
                "Roberto Pereira",
                "Simon Weber",
                "Stephen Mensah",
                "Nathan Andre",
                "Anton Peristyy",
                "Chris Harjadi",
                "Himanshu Gupta",
                "Stephen Malina",
                "Samuel Albanie",
                "Will Cai",
                "Mustafa Mehkary",
                "Frank Reidegeld",
                "Anna-Katharina Dick",
                "Cary Friday",
                "Jasdeep Sidhu",
                "Wanyoung Kim",
                "Mariana Costa",
                "Hubeyb Gurdogan",
                "Brian Weber",
                "Harsh Kumar",
                "Tong Jiang",
                "Arunim Agarwal",
                "Chiara Ceconello",
                "Warren S. Vaz",
                "Chao Zhuang",
                "Haon Park",
                "Andrew R. Tawfeek",
                "Daattavya Aggarwal",
                "Michael Kirchhof",
                "Linjie Dai",
                "Evan Kim",
                "Johan Ferret",
                "Yuzhou Wang",
                "Minghao Yan",
                "Krzysztof Burdzy",
                "Lixin Zhang",
                "Antonio Franca",
                "Diana T. Pham",
                "Kang Yong Loh",
                "Joshua Robinson",
                "Shreen Gul",
                "Gunjan Chhablani",
                "Zhehang Du",
                "Adrian Cosma",
                "Colin White",
                "Robin Riblet",
                "Prajvi Saxena",
                "Jacob Votava",
                "Vladimir Vinnikov",
                "Ethan Delaney",
                "Shiv Halasyamani",
                "Syed M. Shahid",
                "Jean-Christophe Mourrat",
                "Lavr Vetoshkin",
                "Renas Bacho",
                "Vincent Ginis",
                "Aleksandr Maksapetyan",
                "Florencia de la Rosa",
                "Xiuyu Li",
                "Guillaume Malod",
                "Leon Lang",
                "Julien Laurendeau",
                "Fatimah Adesanya",
                "Julien Portier",
                "Lawrence Hollom",
                "Victor Souza",
                "Yuchen Anna Zhou",
                "Yiğit Yalın",
                "Gbenga Daniel Obikoya",
                "Luca Arnaboldi",
                "Rai",
                "Filippo Bigi",
                "Kaniuar Bacho",
                "Pierre Clavier",
                "Gabriel Recchia",
                "Mara Popescu",
                "Nikita Shulga",
                "Ngefor Mildred Tanwie",
                "Thomas C. H. Lux",
                "Ben Rank",
                "Colin Ni",
                "Alesia Yakimchyk",
                "Huanxu",
                "Liu",
                "Olle Häggström",
                "Emil Verkama",
                "Himanshu Narayan",
                "Hans Gundlach",
                "Leonor Brito-Santana",
                "Brian Amaro",
                "Vivek Vajipey",
                "Rynaa Grover",
                "Yiyang Fan",
                "Gabriel Poesia Reis e Silva",
                "Linwei Xin",
                "Yosi Kratish",
                "Jakub Łucki",
                "Wen-Ding Li",
                "Justin Xu",
                "Kevin Joseph Scaria",
                "Freddie Vargus",
                "Farzad Habibi",
                "Long",
                "Lian",
                "Emanuele Rodolà",
                "Jules Robins",
                "Vincent Cheng",
                "Declan Grabb",
                "Ida Bosio",
                "Tony Fruhauff",
                "Ido Akov",
                "Eve J. Y. Lo",
                "Hao Qi",
                "Xi Jiang",
                "Ben Segev",
                "Jingxuan Fan",
                "Sarah Martinson",
                "Erik Y. Wang",
                "Kaylie Hausknecht",
                "Michael P. Brenner",
                "Mao Mao",
                "Yibo Jiang",
                "Xinyu Zhang",
                "David Avagian",
                "Eshawn Jessica Scipio",
                "Muhammad Rehan Siddiqi",
                "Alon Ragoler",
                "Justin Tan",
                "Deepakkumar Patil",
                "Rebeka Plecnik",
                "Aaron Kirtland",
                "Roselynn Grace Montecillo",
                "Stephane Durand",
                "Omer Faruk Bodur",
                "Zahra Adoul",
                "Mohamed Zekry",
                "Guillaume Douville",
                "Ali Karakoc",
                "Tania C. B. Santos",
                "Samir Shamseldeen",
                "Loukmane Karim",
                "Anna Liakhovitskaia",
                "Nate Resman",
                "Nicholas Farina",
                "Juan Carlos Gonzalez",
                "Gabe Maayan",
                "Sarah Hoback",
                "Rodrigo De Oliveira Pena",
                "Glen Sherman",
                "Hodjat Mariji",
                "Rasoul Pouriamanesh",
                "Wentao Wu",
                "Gözdenur Demir",
                "Sandra Mendoza",
                "Ismail Alarab",
                "Joshua Cole",
                "Danyelle Ferreira",
                "Bryan Johnson",
                "Hsiaoyun Milliron",
                "Mohammad Safdari",
                "Liangti Dai",
                "Siriphan Arthornthurasuk",
                "Alexey Pronin",
                "Jing Fan",
                "Angel Ramirez-Trinidad",
                "Ashley Cartwright",
                "Daphiny Pottmaier",
                "Omid Taheri",
                "David Outevsky",
                "Stanley Stepanic",
                "Samuel Perry",
                "Luke Askew",
                "Raúl Adrián Huerta Rodríguez",
                "Abdelkader Dendane",
                "Sam Ali",
                "Ricardo Lorena",
                "Krishnamurthy Iyer",
                "Sk Md Salauddin",
                "Murat Islam",
                "Juan Gonzalez",
                "Josh Ducey",
                "Russell Campbell",
                "Maja Somrak",
                "Vasilios Mavroudis",
                "Eric Vergo",
                "Juehang Qin",
                "Benjámin Borbás",
                "Eric Chu",
                "Jack Lindsey",
                "Anil Radhakrishnan",
                "Antoine Jallon",
                "I. M. J. McInnis",
                "Alex Hoover",
                "Sören Möller",
                "Song Bian",
                "John Lai",
                "Tejal Patwardhan",
                "Summer Yue",
                "Alexandr Wang",
                "Dan Hendrycks"
            ],
            "affiliations": [
                "Center for AI Safety",
                "Scale AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14249.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый рубеж для искусственного интеллекта: тест на пределе человеческих знаний",
                    "desc": "Статья представляет новый многомодальный бенчмарк для оценки возможностей больших языковых моделей (LLM) под названием 'Последний экзамен человечества' (HLE). HLE состоит из 3000 вопросов по различным предметам, разработанных экспертами со всего мира. Бенчмарк создан для преодоления ограничений существующих тестов, на которых современные LLM достигают точности более 90%. Результаты показывают, что современные LLM демонстрируют низкую точность на HLE, что указывает на значительный разрыв между их возможностями и экспертными знаниями человека."
                },
                "en": {
                    "title": "Raising the Bar: Humanity's Last Exam for LLMs",
                    "desc": "This paper introduces a new benchmark called Humanity's Last Exam (HLE) to evaluate the capabilities of large language models (LLMs). HLE consists of 3,000 questions across various subjects, including mathematics and humanities, designed to be challenging for LLMs. Unlike existing benchmarks, HLE questions cannot be easily answered through internet searches, making them a better measure of true understanding. The results show that current state-of-the-art LLMs struggle with HLE, indicating a significant gap between their performance and that of expert humans."
                },
                "zh": {
                    "title": "人类的最后考试：挑战LLM的极限",
                    "desc": "基准测试是跟踪大型语言模型（LLM）能力快速发展的重要工具。然而，现有的基准测试难度未能与LLM的进步相匹配，导致LLM在流行基准测试（如MMLU）上达到90%以上的准确率。为此，我们推出了人类的最后考试（HLE），这是一个涵盖广泛学科的多模态基准，旨在成为此类学术基准的最终版本。HLE包含3000个问题，涉及数学、人文学科和自然科学，旨在揭示当前LLM能力与专家人类水平之间的显著差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13953",
            "title": "Redundancy Principles for MLLMs Benchmarks",
            "url": "https://huggingface.co/papers/2501.13953",
            "abstract": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant redundancy among benchmarks. Therefore, it is crucial to take a step back and critically assess the current state of redundancy and propose targeted principles for constructing effective MLLM benchmarks. In this paper, we focus on redundancy from three key perspectives: 1) Redundancy of benchmark capability dimensions, 2) Redundancy in the number of test questions, and 3) Cross-benchmark redundancy within specific domains. Through the comprehensive analysis over hundreds of MLLMs' performance across more than 20 benchmarks, we aim to quantitatively measure the level of redundancy lies in existing MLLM evaluations, provide valuable insights to guide the future development of MLLM benchmarks, and offer strategies to refine and address redundancy issues effectively.",
            "score": 13,
            "issue_id": 1877,
            "pub_date": "2025-01-20",
            "pub_date_card": {
                "ru": "20 января",
                "en": "January 20",
                "zh": "1月20日"
            },
            "hash": "f504e124f29e4140",
            "authors": [
                "Zicheng Zhang",
                "Xiangyu Zhao",
                "Xinyu Fang",
                "Chunyi Li",
                "Xiaohong Liu",
                "Xiongkuo Min",
                "Haodong Duan",
                "Kai Chen",
                "Guangtao Zhai"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13953.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Борьба с избыточностью: оптимизация бенчмарков для мультимодальных языковых моделей",
                    "desc": "Статья посвящена проблеме избыточности в бенчмарках для мультимодальных больших языковых моделей (MLLM). Авторы анализируют избыточность с трех ключевых аспектов: измерения возможностей, количество тестовых вопросов и пересечение между бенчмарками в конкретных областях. На основе анализа производительности MLLM на более чем 20 бенчмарках, исследователи предлагают количественно оценить уровень избыточности и дать рекомендации по улучшению бенчмарков. Цель работы - предоставить ценные идеи для будущего развития оценки MLLM и стратегии по устранению проблем избыточности."
                },
                "en": {
                    "title": "Streamlining MLLM Benchmarks: Tackling Redundancy for Better Evaluation",
                    "desc": "This paper examines the growing issue of redundancy in benchmarks for Multi-modality Large Language Models (MLLMs). It identifies three main types of redundancy: in the capabilities being tested, the number of test questions, and across different benchmarks within the same domain. By analyzing the performance of numerous MLLMs across over 20 benchmarks, the authors quantitatively measure the extent of this redundancy. The findings aim to inform the development of more effective benchmarks and provide strategies to reduce redundancy in future evaluations."
                },
                "zh": {
                    "title": "优化多模态大型语言模型基准测试，减少冗余",
                    "desc": "随着多模态大型语言模型（MLLMs）的快速发展，年度基准测试的数量激增，导致基准测试之间的冗余现象显著增加。本文从三个关键角度分析冗余问题：基准能力维度的冗余、测试问题数量的冗余以及特定领域内的跨基准冗余。通过对数百个MLLM在20多个基准测试中的表现进行综合分析，我们定量测量现有MLLM评估中的冗余水平。我们的目标是为未来MLLM基准的开发提供有价值的见解，并提出有效解决冗余问题的策略。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14342",
            "title": "Chain-of-Retrieval Augmented Generation",
            "url": "https://huggingface.co/papers/2501.14342",
            "abstract": "This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.",
            "score": 9,
            "issue_id": 1873,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 января",
                "en": "January 24",
                "zh": "1月24日"
            },
            "hash": "cd489ba1638c5496",
            "authors": [
                "Liang Wang",
                "Haonan Chen",
                "Nan Yang",
                "Xiaolong Huang",
                "Zhicheng Dou",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Corporation",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14342.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#rag",
                    "#reasoning"
                ],
                "emoji": "🔗",
                "ru": {
                    "title": "CoRAG: Пошаговый поиск для улучшения генерации ответов",
                    "desc": "Статья представляет новый подход к обучению моделей извлечения и генерации (RAG), позволяющий выполнять пошаговый поиск и рассуждение перед генерацией окончательного ответа. Метод CoRAG (Chain-of-Retrieval Augmented Generation) позволяет модели динамически переформулировать запрос на основе развивающегося состояния. Для обучения CoRAG используется отбор с отклонением для автоматической генерации промежуточных цепочек поиска. Экспериментальные результаты показывают значительное улучшение производительности на различных бенчмарках, особенно в задачах многоэтапного ответа на вопросы."
                },
                "en": {
                    "title": "CoRAG: Enhancing RAG with Dynamic Retrieval for Complex Queries",
                    "desc": "This paper presents CoRAG, a novel approach for training retrieval-augmented generation (RAG) models that enhances their ability to handle complex queries. Unlike traditional RAG methods that rely on a single retrieval step, CoRAG employs a dynamic query reformulation process, allowing the model to retrieve information iteratively. The training process utilizes rejection sampling to create intermediate retrieval chains, enriching the dataset beyond just the final answers. Experimental results demonstrate that CoRAG significantly improves performance in multi-hop question answering tasks, achieving state-of-the-art results on the KILT benchmark."
                },
                "zh": {
                    "title": "动态检索，提升问答能力！",
                    "desc": "本文介绍了一种训练类似o1的RAG模型的新方法，该方法在生成最终答案之前逐步检索和推理相关信息。传统的RAG方法通常在生成过程之前只进行一次检索，这限制了它们在处理复杂查询时的有效性。我们提出的方法CoRAG（链式检索增强生成）允许模型根据不断变化的状态动态重构查询。通过使用拒绝采样自动生成中间检索链，我们有效地增强了现有的RAG数据集，从而在多跳问答任务中显著提高了模型的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14492",
            "title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques",
            "url": "https://huggingface.co/papers/2501.14492",
            "abstract": "Critiques are important for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique capabilities of LLMs presents a significant challenge due to the open-ended nature of the task. In this work, we introduce a new benchmark designed to assess the critique capabilities of LLMs. Unlike existing benchmarks, which typically function in an open-loop fashion, our approach employs a closed-loop methodology that evaluates the quality of corrections generated from critiques. Moreover, the benchmark incorporates features such as self-critique, cross-critique, and iterative critique, which are crucial for distinguishing the abilities of advanced reasoning models from more classical ones. We implement this benchmark using eight challenging reasoning tasks. We have several interesting findings. First, despite demonstrating comparable performance in direct chain-of-thought generation, classical LLMs significantly lag behind the advanced reasoning-based model o1-mini across all critique scenarios. Second, in self-critique and iterative critique settings, classical LLMs may even underperform relative to their baseline capabilities. We hope that this benchmark will serve as a valuable resource to guide future advancements. The code and data are available at https://github.com/tangzhy/RealCritic.",
            "score": 8,
            "issue_id": 1873,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 января",
                "en": "January 24",
                "zh": "1月24日"
            },
            "hash": "683923c8fb1958c2",
            "authors": [
                "Zhengyang Tang",
                "Ziniu Li",
                "Zhenyang Xiao",
                "Tian Ding",
                "Ruoyu Sun",
                "Benyou Wang",
                "Dayiheng Liu",
                "Fei Huang",
                "Tianyu Liu",
                "Bowen Yu",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Inc.",
                "Shenzhen Research Institute of Big Data",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14492.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый бенчмарк раскрывает истинный потенциал LLM в критическом мышлении",
                    "desc": "Эта статья представляет новый бенчмарк для оценки способностей больших языковых моделей (LLM) к критике. В отличие от существующих бенчмарков, этот подход использует замкнутую методологию, оценивающую качество исправлений, сгенерированных на основе критики. Бенчмарк включает в себя самокритику, перекрестную критику и итеративную критику, что важно для различения способностей продвинутых моделей рассуждения от классических. Исследование показало, что классические LLM значительно отстают от продвинутых моделей рассуждения во всех сценариях критики, несмотря на сопоставимую производительность в прямой генерации цепочки рассуждений."
                },
                "en": {
                    "title": "Enhancing LLMs Through Effective Critique Evaluation",
                    "desc": "This paper focuses on improving Large Language Models (LLMs) by evaluating their critique capabilities, which are essential for self-improvement and providing feedback. The authors introduce a new benchmark that uses a closed-loop methodology to assess how well LLMs can generate corrections based on critiques. This benchmark includes features like self-critique, cross-critique, and iterative critique, allowing for a more nuanced evaluation of reasoning abilities. The findings reveal that advanced reasoning models outperform classical LLMs in critique scenarios, highlighting the need for better evaluation methods in machine learning."
                },
                "zh": {
                    "title": "提升LLMs性能的新基准评估批评能力",
                    "desc": "本文探讨了大型语言模型（LLMs）在批评能力方面的评估。我们提出了一种新的基准，采用闭环方法来评估批评生成的修正质量。该基准包括自我批评、交叉批评和迭代批评等特性，以区分高级推理模型与传统模型的能力。研究发现，尽管传统LLMs在直接思维生成方面表现相似，但在所有批评场景中，它们的表现明显落后于基于高级推理的模型o1-mini。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14726",
            "title": "Relightable Full-Body Gaussian Codec Avatars",
            "url": "https://huggingface.co/papers/2501.14726",
            "abstract": "We propose Relightable Full-Body Gaussian Codec Avatars, a new approach for modeling relightable full-body avatars with fine-grained details including face and hands. The unique challenge for relighting full-body avatars lies in the large deformations caused by body articulation and the resulting impact on appearance caused by light transport. Changes in body pose can dramatically change the orientation of body surfaces with respect to lights, resulting in both local appearance changes due to changes in local light transport functions, as well as non-local changes due to occlusion between body parts. To address this, we decompose the light transport into local and non-local effects. Local appearance changes are modeled using learnable zonal harmonics for diffuse radiance transfer. Unlike spherical harmonics, zonal harmonics are highly efficient to rotate under articulation. This allows us to learn diffuse radiance transfer in a local coordinate frame, which disentangles the local radiance transfer from the articulation of the body. To account for non-local appearance changes, we introduce a shadow network that predicts shadows given precomputed incoming irradiance on a base mesh. This facilitates the learning of non-local shadowing between the body parts. Finally, we use a deferred shading approach to model specular radiance transfer and better capture reflections and highlights such as eye glints. We demonstrate that our approach successfully models both the local and non-local light transport required for relightable full-body avatars, with a superior generalization ability under novel illumination conditions and unseen poses.",
            "score": 4,
            "issue_id": 1873,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 января",
                "en": "January 24",
                "zh": "1月24日"
            },
            "hash": "0072ce1869c715b7",
            "authors": [
                "Shaofei Wang",
                "Tomas Simon",
                "Igor Santesteban",
                "Timur Bagautdinov",
                "Junxuan Li",
                "Vasu Agrawal",
                "Fabian Prada",
                "Shoou-I Yu",
                "Pace Nalbone",
                "Matt Gramlich",
                "Roman Lubachersky",
                "Chenglei Wu",
                "Javier Romero",
                "Jason Saragih",
                "Michael Zollhoefer",
                "Andreas Geiger",
                "Siyu Tang",
                "Shunsuke Saito"
            ],
            "affiliations": [
                "Codec Avatars Lab, Meta, USA",
                "ETH Zürich, Switzerland",
                "University of Tübingen, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14726.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d"
                ],
                "emoji": "🕴️",
                "ru": {
                    "title": "Реалистичное освещение для полноразмерных цифровых аватаров",
                    "desc": "Статья представляет новый подход к моделированию полноразмерных аватаров с возможностью изменения освещения, включая детализацию лица и рук. Авторы предлагают декомпозицию световых эффектов на локальные и нелокальные, используя обучаемые зональные гармоники для диффузного переноса освещения и специальную нейронную сеть для предсказания теней. Метод также включает отложенный шейдинг для моделирования зеркального переноса освещения. Результаты демонстрируют успешное моделирование как локального, так и нелокального переноса света для полноразмерных аватаров с улучшенной способностью к обобщению в новых условиях освещения и позах."
                },
                "en": {
                    "title": "Realistic Relightable Avatars Through Advanced Light Transport Modeling",
                    "desc": "This paper presents a novel method for creating relightable full-body avatars that capture intricate details like facial features and hands. The authors tackle the challenge of how body movements affect lighting and appearance by separating light transport into local and non-local effects. They utilize learnable zonal harmonics to efficiently model local changes in appearance due to body articulation, while a shadow network predicts non-local shadowing effects between body parts. The proposed approach enhances the realism of avatars under varying lighting conditions and poses, demonstrating improved generalization capabilities."
                },
                "zh": {
                    "title": "可重光照的全身头像建模新方法",
                    "desc": "我们提出了一种新的方法，称为可重光照的全身高斯编码头像，旨在建模具有细致面部和手部特征的全身头像。该方法解决了由于身体关节运动引起的大变形对外观的影响，特别是光传输的变化。我们将光传输分解为局部和非局部效应，使用可学习的区域谐波来建模局部外观变化，并引入阴影网络来预测身体部位之间的阴影。最终，我们采用延迟着色方法来建模镜面反射，以更好地捕捉反射和高光效果。"
                }
            }
        }
    ],
    "link_prev": "2025-01-24.html",
    "link_next": "2025-01-28.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "24.01",
        "en": "01/24",
        "zh": "1月24日"
    },
    "short_date_next": {
        "ru": "28.01",
        "en": "01/28",
        "zh": "1月28日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了大型语言模型（LLM）能力评估的重要性。现有的基准测试已经无法跟上LLM的进步，因为LLM在流行的基准测试上已经达到了90%以上的准确率。为了解决这个问题，作者引入了“人类最后的考试”（HLE），这是一个跨学科的多模态基准测试。HLE包含3000个问题，涵盖数学、人文学科和自然科学等多个领域。HLE由全球的专家开发，问题具有明确的答案，但无法通过互联网快速找到。",
        "title": "Humanity's Last Exam",
        "pinyin": "这篇文章讨论了大型语言模型（LLM）能力评估的重要性。现有的基准测试已经无法跟上LLM的进步，因为LLM在流行的基准测试上已经达到了90%以上的准确率。为了解决这个问题，作者引入了“人类最后的考试”（HLE），这是一个跨学科的多模态基准测试。HLE包含3000个问题，涵盖数学、人文学科和自然科学等多个领域。HLE由全球的专家开发，问题具有明确的答案，但无法通过互联网快速找到。\n\nZhè piān wénzhāng tǎolùn le dàxíng yǔyán móxíng (LLM) nénglì pínggū de zhòngyàoxìng. Xiànyǒu de jīzhǔn cèshì yǐjīng wúfǎ gēnshàng LLM de jìnbù, yīnwèi LLM zài liúxíng de jīzhǔn cèshì shàng yǐjīng dádào le 90% yǐshàng de zhǔnquèlǜ. Wèile jiějué zhègè wèntí, zuòzhě yǐnrù le “rénlèi zhòuhòu de kǎoshì” (HLE), zhè shì yīgè kuà xuékē de duō móshì jīzhǔn cèshì. HLE bāohán 3000 gè wèntí, hànjiē shùxué, rénwén xuékē hé zìrán kēxué děng duō gè lǐngyù. HLE yóu quánqiú de zhuānjiā kāifā, wèntí jùyǒu míngquè de dá'àn, dàn wúfǎ tōngguò yǐnxiàng kuàisù zhǎo dào.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'},\n{'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'},\n{'word': '基准测试', 'pinyin': 'jī zhǔn cè shì', 'trans': 'benchmark test'},\n{'word': '无法', 'pinyin': 'wú fǎ', 'trans': 'unable'},\n{'word': '跟上', 'pinyin': 'gēn shàng', 'trans': 'keep up with'},\n{'word': '进步', 'pinyin': 'jìn bù', 'trans': 'progress'},\n{'word': '流行', 'pinyin': 'liú xíng', 'trans': 'popular'},\n{'word': '准确率', 'pinyin': 'zhǔn què lǜ', 'trans': 'accuracy rate'},\n{'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'},\n{'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'},\n{'word': '人类', 'pinyin': 'rén lèi', 'trans': 'human'},\n{'word': '最后', 'pinyin': 'zuì hòu', 'trans': 'final'},\n{'word': '考试', 'pinyin': 'kǎo shì', 'trans': 'exam'},\n{'word': '跨学科', 'pinyin': 'kuà xué kē', 'trans': 'interdisciplinary'},\n{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '包含', 'pinyin': 'bāo hán', 'trans': 'contain'},\n{'word': '数学', 'pinyin': 'shù xué', 'trans': 'mathematics'},\n{'word': '人文学科', 'pinyin': 'rén wén xué kē', 'trans': 'humanities'},\n{'word': '自然科学', 'pinyin': 'zì rán kē xué', 'trans': 'natural sciences'},\n{'word': '领域', 'pinyin': 'lǐng yù', 'trans': 'field'},\n{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'develop'},\n{'word': '专家', 'pinyin': 'zhuān jiā', 'trans': 'expert'},\n{'word': '明确', 'pinyin': 'míng què', 'trans': 'clear'},\n{'word': '互联网', 'pinyin': 'hù lián wǎng', 'trans': 'internet'}]",
        "trans": "This article discusses the importance of evaluating the capabilities of Large Language Models (LLMs). Existing benchmark tests have failed to keep up with the advancements in LLMs, as LLMs have already achieved over 90% accuracy on popular benchmark tests. To address this issue, the authors introduce the \"Human's Last Exam\" (HLE), a cross-disciplinary, multimodal benchmark test. The HLE consists of 3,000 questions covering various fields such as mathematics, humanities, and natural sciences. Developed by global experts, the questions in the HLE have clear answers but cannot be quickly found on the internet.",
        "update_ts": "2025-01-27 09:11"
    }
}