{
    "date": {
        "ru": "30 Ğ¼Ğ°Ñ",
        "en": "May 30",
        "zh": "5æœˆ30æ—¥"
    },
    "time_utc": "2025-05-30 02:29",
    "weekday": 4,
    "issue_id": 4035,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.23693",
            "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
            "url": "https://huggingface.co/papers/2505.23693",
            "abstract": "A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.",
            "score": 9,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "1a301b9c565967e9",
            "authors": [
                "Tingyu Song",
                "Tongyan Hu",
                "Guo Gan",
                "Yilun Zhao"
            ],
            "affiliations": [
                "National University of Singapore",
                "School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences",
                "Yale University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23693.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VF-Eval: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VF-Eval Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4.1, ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MLLM Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Evaluating MLLMs: Bridging AI-Generated Videos and Human Feedback",
                    "desc": "The paper introduces VF-Eval, a new benchmark designed to assess the performance of Multimodal Language Models (MLLMs) in interpreting AI-generated content videos. It focuses on four specific tasks: coherence validation, error awareness, error type detection, and reasoning evaluation, which are crucial for understanding synthetic videos. The study evaluates 13 advanced MLLMs, revealing that even the top model, GPT-4.1, faces challenges in consistently performing well across these tasks. Furthermore, the paper explores how aligning MLLMs with human feedback can enhance the quality of video generation, showcasing the practical implications of the benchmark."
                },
                "zh": {
                    "title": "è¯„ä¼°AIç”Ÿæˆè§†é¢‘çš„èƒ½åŠ›æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•VF-Evalï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§£è¯»AIç”Ÿæˆå†…å®¹è§†é¢‘æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«å››ä¸ªä»»åŠ¡ï¼šè¿è´¯æ€§éªŒè¯ã€é”™è¯¯æ„è¯†ã€é”™è¯¯ç±»å‹æ£€æµ‹å’Œæ¨ç†è¯„ä¼°ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsåœ¨å¤„ç†åˆæˆè§†é¢‘æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹GPT-4.1ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ä¹Ÿéš¾ä»¥ä¿æŒä¸€è‡´çš„è‰¯å¥½è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºåŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å®éªŒRePromptï¼Œç ”ç©¶è¡¨æ˜å°†MLLMsä¸äººç±»åé¦ˆæ›´ç´§å¯†å¯¹é½å¯ä»¥æ”¹å–„è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22653",
            "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
            "url": "https://huggingface.co/papers/2505.22653",
            "abstract": "LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
            "score": 9,
            "issue_id": 4035,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "7d2f43b6997c2647",
            "authors": [
                "Ang Lv",
                "Ruobing Xie",
                "Xingwu Sun",
                "Zhanhui Kang",
                "Rui Yan"
            ],
            "affiliations": [
                "GSAI, Renmin University of China",
                "Large Language Model Department, Tencent",
                "School of Computer Science, Wuhan University",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22653.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LLM ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğº ÑˆÑƒĞ¼Ñƒ: Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑˆÑƒĞ¼Ñƒ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RPR) Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen-2.5-7B ÑĞ¼Ğ¾Ğ³Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ 5% Ğ´Ğ¾ 72%, Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ³Ğ´Ğ° 40% Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Robust LLMs: Thriving Amid Reward Noise with Reasoning Patterns",
                    "desc": "This paper explores how large language models (LLMs) can effectively handle noise in reward signals during post-training, particularly in reinforcement learning scenarios. The authors demonstrate that even with significant reward noise, such as flipping 40% of reward outputs, LLMs like Qwen-2.5-7B can still achieve impressive performance improvements on math tasks. They introduce a novel approach called reasoning pattern rewards (RPR), which focuses on rewarding the presence of key reasoning phrases rather than the correctness of answers, leading to high accuracy. The study emphasizes the importance of enhancing foundational skills during pre-training and offers insights for refining post-training methods in real-world applications."
                },
                "zh": {
                    "title": "æå‡æ¨¡å‹é²æ£’æ€§ï¼Œå…‹æœå¥–åŠ±å™ªå£°",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨åè®­ç»ƒé˜¶æ®µï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹å¥–åŠ±å™ªå£°çš„é²æ£’æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿åœ¨å¥–åŠ±å‡½æ•°è¾“å‡ºä¸­æ‰‹åŠ¨ç¿»è½¬40%çš„ç»“æœï¼Œæ¨¡å‹ä»èƒ½å¿«é€Ÿæ”¶æ•›ï¼Œæ•°å­¦ä»»åŠ¡çš„å‡†ç¡®ç‡ä»5%æå‡è‡³72%ã€‚é€šè¿‡ä»…å¥–åŠ±å…³é”®æ¨ç†çŸ­è¯­çš„å‡ºç°ï¼ˆå³æ¨ç†æ¨¡å¼å¥–åŠ±RPRï¼‰ï¼Œæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¾¾åˆ°äº†è¶…è¿‡70%çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¸ä¸¥æ ¼éªŒè¯çš„æ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†åœ¨é¢„è®­ç»ƒé˜¶æ®µæå‡æ¨¡å‹åŸºç¡€èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºåè®­ç»ƒæŠ€æœ¯çš„è¿›æ­¥æä¾›äº†æ–°æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23604",
            "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering",
            "url": "https://huggingface.co/papers/2505.23604",
            "abstract": "EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced.",
            "score": 7,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "3c092222b4cf7a3d",
            "authors": [
                "Guangtao Zeng",
                "Maohao Shen",
                "Delin Chen",
                "Zhenting Qi",
                "Subhro Das",
                "Dan Gutfreund",
                "David Cox",
                "Gregory Wornell",
                "Wei Lu",
                "Zhang-Wei Hong",
                "Chuang Gan"
            ],
            "affiliations": [
                "Department of EECS, MIT",
                "Harvard",
                "MIT-IBM Watson AI Lab, IBM Research",
                "Singapore University of Technology and Design",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23604.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#rl",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "EvoScale - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¼ĞµÑ‰Ğ°Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. EvoScale Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ 32B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ²Ğ¾Ğ¸Ñ… ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "EvoScale: Evolving Small Models for Big Performance",
                    "desc": "EvoScale is a novel method that combines evolutionary strategies and reinforcement learning to enhance the performance of small language models on real-world software engineering tasks. It addresses the limitations of existing approaches that rely heavily on supervised fine-tuning and expensive data curation. By treating the output generation as an evolutionary process, EvoScale iteratively refines model outputs through selection and mutation, significantly reducing the number of samples needed for effective solutions. This approach allows smaller models to achieve performance levels comparable to much larger models, making them more practical for real-world applications."
                },
                "zh": {
                    "title": "è¿›åŒ–æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼šå°æ¨¡å‹çš„å¼ºå¤§æå‡",
                    "desc": "EvoScaleæ˜¯ä¸€ç§åŸºäºè¿›åŒ–å’Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å°å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£æ”¹è¿›å’Œä¼˜åŒ–è¾“å‡ºï¼Œå‡å°‘äº†å¯»æ‰¾æ­£ç¡®è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒEvoScaleé‡‡ç”¨è‡ªæˆ‘è¿›åŒ–çš„æ–¹å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è‡ªæˆ‘æå‡ç”Ÿæˆç»“æœçš„è¯„åˆ†ã€‚ç»è¿‡è¯„ä¼°ï¼ŒEvoScaleä½¿å¾—32Bå‚æ•°çš„æ¨¡å‹Satori-SWE-32Båœ¨æ€§èƒ½ä¸Šèƒ½å¤ŸåŒ¹æ•Œæˆ–è¶…è¿‡100Bå‚æ•°çš„æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡æ›´å°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23762",
            "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
            "url": "https://huggingface.co/papers/2505.23762",
            "abstract": "ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI.",
            "score": 6,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "d159a7e52c608567",
            "authors": [
                "Chenyu Yang",
                "Shiqian Su",
                "Shi Liu",
                "Xuan Dong",
                "Yue Yu",
                "Weijie Su",
                "Xuehui Wang",
                "Zhaoyang Liu",
                "Jinguo Zhu",
                "Hao Li",
                "Wenhai Wang",
                "Yu Qiao",
                "Xizhou Zhu",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23762.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#games",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "ZeroGUI - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ZeroGUI Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ“ĞŸĞ˜-ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering GUI Agents with Zero Human Cost",
                    "desc": "ZeroGUI is an innovative online learning framework that leverages Vision-Language Models (VLMs) to enhance the training of GUI Agents with minimal human input. It addresses the challenges of traditional offline learning methods, which require extensive manual annotations and struggle to adapt to changing environments. By utilizing VLMs for automatic task generation and reward estimation, ZeroGUI enables continuous learning and interaction with GUI systems. Experiments show that this approach significantly improves the performance of advanced GUI Agents in various environments."
                },
                "zh": {
                    "title": "ZeroGUIï¼šæ— äººå·¥æˆæœ¬çš„GUIä»£ç†è®­ç»ƒæ¡†æ¶",
                    "desc": "ZeroGUIæ˜¯ä¸€ä¸ªåœ¨çº¿å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œä»»åŠ¡ç”Ÿæˆå’Œå¥–åŠ±è¯„ä¼°ï¼Œä»è€Œåœ¨æœ€å°äººåŠ›å¹²é¢„ä¸‹æå‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»Ÿç¦»çº¿å­¦ä¹ æ–¹æ³•çš„ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šå¯¹é«˜è´¨é‡æ‰‹åŠ¨æ ‡æ³¨çš„ä¾èµ–å’Œå¯¹åŠ¨æ€äº¤äº’ç¯å¢ƒçš„é€‚åº”æ€§ä¸è¶³ã€‚ZeroGUIé€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„è®­ç»ƒç›®æ ‡å’Œè‡ªåŠ¨è¯„ä¼°ä»»åŠ¡æˆåŠŸç‡ï¼Œæ¥å®ç°æ— äººå·¥æˆæœ¬çš„GUIä»£ç†è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZeroGUIåœ¨OSWorldå’ŒAndroidLabç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†ä¸¤ç§å…ˆè¿›GUIä»£ç†çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23559",
            "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
            "url": "https://huggingface.co/papers/2505.23559",
            "abstract": "SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce SafeScientist, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose SciSafetyBench, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. red{Warning: this paper contains example data that may be offensive or harmful.}",
            "score": 6,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "47b1655c578567ee",
            "authors": [
                "Kunlun Zhu",
                "Jiaxun Zhang",
                "Ziheng Qi",
                "Nuoxing Shang",
                "Zijia Liu",
                "Peixuan Han",
                "Yue Su",
                "Haofei Yu",
                "Jiaxuan You"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23559.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#healthcare",
                    "#ethics",
                    "#science",
                    "#benchmark",
                    "#open_source",
                    "#security"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-ÑƒÑ‡ĞµĞ½Ñ‹Ğ¹: ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "SafeScientist - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ². ĞĞ½ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ°ĞºÑ†ĞµĞ½Ñ‚ Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. SafeScientist Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° SciSafetyBench."
                },
                "en": {
                    "title": "Enhancing Safety in AI-Driven Science with SafeScientist",
                    "desc": "SafeScientist is an AI framework designed to improve safety in AI-driven scientific research by implementing various defensive mechanisms. It proactively avoids engaging in ethically questionable or high-risk tasks, ensuring a focus on safety throughout the research process. The framework includes features like prompt monitoring and agent collaboration oversight, which help maintain ethical standards. Additionally, SafeScientist is validated using the SciSafetyBench benchmark, demonstrating a 35% improvement in safety performance compared to traditional AI frameworks while maintaining high-quality scientific output."
                },
                "zh": {
                    "title": "å®‰å…¨ç§‘å­¦å®¶ï¼šæå‡AIç§‘å­¦ç ”ç©¶çš„å®‰å…¨æ€§",
                    "desc": "SafeScientistæ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤šç§é˜²å¾¡æœºåˆ¶æé«˜AIé©±åŠ¨ç§‘å­¦ç ”ç©¶çš„å®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä¸»åŠ¨æ‹’ç»ä¸é“å¾·æˆ–é«˜é£é™©çš„ä»»åŠ¡ï¼Œå¹¶åœ¨æ•´ä¸ªç ”ç©¶è¿‡ç¨‹ä¸­å¼ºè°ƒå®‰å…¨æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†SciSafetyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°ç§‘å­¦é¢†åŸŸä¸­AIçš„å®‰å…¨æ€§ï¼Œæ¶µç›–240ä¸ªé«˜é£é™©ç§‘å­¦ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeScientiståœ¨å®‰å…¨æ€§èƒ½ä¸Šæ¯”ä¼ ç»ŸAIç§‘å­¦å®¶æ¡†æ¶æé«˜äº†35%ï¼ŒåŒæ—¶ä¸å½±å“ç§‘å­¦è¾“å‡ºçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22961",
            "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
            "url": "https://huggingface.co/papers/2505.22961",
            "abstract": "ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP.",
            "score": 6,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "13c778698d292f73",
            "authors": [
                "Peixuan Han",
                "Zijia Liu",
                "Jiaxuan You"
            ],
            "affiliations": [
                "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22961.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ToMAP: Ğ˜Ğ˜-ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ToMAP - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ°. ToMAP ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜-ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ Ğ¼Ñ‹ÑĞ»ÑÑ… Ğ¸ Ğ¼Ğ½ĞµĞ½Ğ¸ÑÑ… Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ToMAP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµÑĞµĞ´ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Persuasion with Theory of Mind",
                    "desc": "This paper presents Theory of Mind Augmented Persuader (ToMAP), a new method that enhances large language models (LLMs) for persuasive tasks by integrating Theory of Mind (ToM) modules. These modules allow the persuader to better understand and anticipate the thoughts and objections of opponents, leading to improved argument quality and diversity. By employing a reinforcement learning approach, ToMAP trains the persuader to analyze opponent-related information effectively, resulting in more logical and nuanced arguments. Experimental results demonstrate that ToMAP significantly outperforms larger models, showcasing its potential for creating advanced persuasive agents."
                },
                "zh": {
                    "title": "æå‡è¯´æœåŠ›çš„ç†è®ºå¿ƒæ™ºå¢å¼ºæ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºToMAPçš„ç†è®ºå¿ƒæ™ºå¢å¼ºè¯´æœè€…æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯´æœè¿‡ç¨‹ä¸­çš„å¯¹æ‰‹æ„è¯†å’Œè®ºè¯è´¨é‡ã€‚é€šè¿‡å¼•å…¥ä¸¤ä¸ªç†è®ºå¿ƒæ™ºæ¨¡å—ï¼ŒToMAPèƒ½å¤Ÿæ›´å¥½åœ°åˆ†æå¯¹æ‰‹çš„å¿ƒç†çŠ¶æ€ï¼Œä»è€Œç”Ÿæˆæ›´æœ‰æ•ˆçš„è®ºç‚¹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ToMAPçš„å‚æ•°é‡ä»…ä¸º30äº¿ï¼Œä½†åœ¨å¤šä¸ªè¯´æœæ¨¡å‹å’Œä¸åŒè¯­æ–™åº“ä¸Šï¼Œå…¶è¡¨ç°è¶…è¿‡äº†æ›´å¤§è§„æ¨¡çš„åŸºçº¿æ¨¡å‹ï¼Œå¦‚GPT-4oï¼Œæå‡å¹…åº¦è¾¾åˆ°39.4%ã€‚æ­¤å¤–ï¼ŒToMAPåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å±•ç°å‡ºå¤æ‚çš„æ¨ç†é“¾å’Œå‡å°‘é‡å¤çš„èƒ½åŠ›ï¼Œä½¿å…¶åœ¨é•¿æ—¶é—´å¯¹è¯ä¸­æ›´å…·é€»è¾‘æ€§å’Œçµæ´»æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23735",
            "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
            "url": "https://huggingface.co/papers/2505.23735",
            "abstract": "Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\\% accuracy in 10M context length of BABILong benchmark.",
            "score": 5,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "e9763dde29798ac1",
            "authors": [
                "Ali Behrouz",
                "Zeman Li",
                "Praneeth Kacham",
                "Majid Daliri",
                "Yuan Deng",
                "Peilin Zhong",
                "Meisam Razaviyayn",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23735.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ATLAS: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ATLAS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ATLAS Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¸ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ±Ğ°Ğ·Ğµ ATLAS Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ DeepTransformers, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Transformer. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ATLAS Ğ½Ğ°Ğ´ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "ATLAS: Revolutionizing Long-Term Memory in Transformers",
                    "desc": "This paper introduces ATLAS, a long-term memory module designed to enhance the performance of sequence modeling tasks. It addresses the limitations of traditional architectures, particularly in handling long contexts, by optimizing memory based on both current and past inputs. The authors propose DeepTransformers, a new family of architectures that generalize the original Transformer model while improving memory capacity and management. Experimental results demonstrate that ATLAS outperforms existing models, achieving significant accuracy improvements in tasks requiring long-context understanding."
                },
                "zh": {
                    "title": "ATLASï¼šè¶…è¶Šä¼ ç»Ÿçš„é•¿æ—¶è®°å¿†æ¨¡å—",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„é•¿æ—¶è®°å¿†æ¨¡å—ATLASï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„å±€é™æ€§ã€‚ATLASé€šè¿‡ä¼˜åŒ–è®°å¿†ç»“æ„ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å­˜å‚¨å’Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿé•¿æ—¶è®°å¿†æ¨¡å‹çš„åœ¨çº¿æ›´æ–°é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒATLASåœ¨è¯­è¨€å»ºæ¨¡ã€å¸¸è¯†æ¨ç†å’Œé•¿ä¸Šä¸‹æ–‡ç†è§£ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„Transformerå’Œçº¿æ€§é€’å½’æ¨¡å‹ã€‚æœ€ç»ˆï¼ŒATLASåœ¨BABILongåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†10Mä¸Šä¸‹æ–‡é•¿åº¦çš„+80\\%å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23585",
            "title": "On-Policy RL with Optimal Reward Baseline",
            "url": "https://huggingface.co/papers/2505.23585",
            "abstract": "Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo.",
            "score": 4,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "8a27563b11999352",
            "authors": [
                "Yaru Hao",
                "Li Dong",
                "Xun Wu",
                "Shaohan Huang",
                "Zewen Chi",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23585.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "OPO: Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ OPO (On-Policy RL with Optimal reward baseline). OPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ OPO - Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ on-policy Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ»Ğ¸Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ OPO Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Stabilizing Reinforcement Learning for Language Models with OPO",
                    "desc": "This paper introduces a new reinforcement learning algorithm called On-Policy RL with Optimal reward baseline (OPO) to improve the training of large language models. OPO focuses on exact on-policy training, which helps stabilize the learning process and encourages better exploration of solutions. The algorithm also incorporates an optimal reward baseline that reduces gradient variance, leading to more reliable updates during training. Evaluations show that OPO outperforms existing methods in terms of stability and diversity of responses, making it a strong candidate for aligning language models with human preferences."
                },
                "zh": {
                    "title": "OPOï¼šç¨³å®šé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹å‘",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç§°ä¸ºæœ€ä¼˜å¥–åŠ±åŸºçº¿çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOPOï¼‰ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç®—æ³•åœ¨è®­ç»ƒä¸ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„é—®é¢˜ã€‚OPOå¼ºè°ƒç²¾ç¡®çš„åœ¨çº¿è®­ç»ƒï¼Œè¿™åœ¨å®è·µä¸­ç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹å¹¶å¢å¼ºäº†æ¢ç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒOPOå¼•å…¥äº†æœ€ä¼˜å¥–åŠ±åŸºçº¿ï¼Œç†è®ºä¸Šå¯ä»¥æœ€å°åŒ–æ¢¯åº¦æ–¹å·®ã€‚é€šè¿‡åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„è¯„ä¼°ï¼ŒOPOå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œä¸”æ— éœ€é¢å¤–çš„æ¨¡å‹æˆ–æ­£åˆ™åŒ–é¡¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23747",
            "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
            "url": "https://huggingface.co/papers/2505.23747",
            "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.",
            "score": 3,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "5ee23f045f054465",
            "authors": [
                "Diankun Wu",
                "Fangfu Liu",
                "Yi-Hsin Hung",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23747.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#multimodal",
                    "#architecture",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ¸Ğ· 2D Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Spatial-MLLM - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 2D Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… 3D Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Spatial-MLLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Spatial-MLLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking 3D Spatial Reasoning from 2D Inputs",
                    "desc": "This paper introduces Spatial-MLLM, a new framework designed to enhance spatial reasoning using only 2D inputs like images and videos. Unlike traditional 3D models that require additional 3D data, Spatial-MLLM utilizes a dual-encoder architecture that combines a pretrained 2D visual encoder for semantic features with a spatial encoder for 3D structure features. The model integrates these features into unified visual tokens, improving its ability to understand spatial relationships. Additionally, a novel space-aware frame sampling strategy is employed to prioritize important frames during inference, leading to superior performance in various spatial reasoning tasks."
                },
                "zh": {
                    "title": "ä»2Dåˆ°3Dçš„ç©ºé—´æ¨ç†æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºSpatial-MLLMï¼Œæ—¨åœ¨ä»çº¯2Dè§‚å¯Ÿä¸­è¿›è¡Œè§†è§‰åŸºç¡€çš„ç©ºé—´æ¨ç†ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºCLIPè§†è§‰ç¼–ç å™¨çš„è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨äº†è§†è§‰å‡ ä½•åŸºç¡€æ¨¡å‹çš„å¼ºç»“æ„å…ˆéªŒã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒç¼–ç å™¨æ¶æ„ï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„2Dè§†è§‰ç¼–ç å™¨å’Œç©ºé—´ç¼–ç å™¨ï¼Œä»¥æå–è¯­ä¹‰ç‰¹å¾å’Œ3Dç»“æ„ç‰¹å¾ã€‚é€šè¿‡åœ¨æ¨ç†æ—¶é‡‡ç”¨ç©ºé—´æ„ŸçŸ¥çš„å¸§é‡‡æ ·ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç§è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23745",
            "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
            "url": "https://huggingface.co/papers/2505.23745",
            "abstract": "TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.",
            "score": 1,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "55ac97c649fb77b4",
            "authors": [
                "Hao Dong",
                "Moru Liu",
                "Jian Liang",
                "Eleni Chatzi",
                "Olga Fink"
            ],
            "affiliations": [
                "EPFL",
                "ETH ZÃ¼rich",
                "NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences",
                "Technical University of Munich",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23745.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#interpretability",
                    "#benchmark",
                    "#architecture",
                    "#security"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "TrustVLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞµĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. TrustVLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° 17 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Trust Your Vision-Language Model with TrustVLM!",
                    "desc": "TrustVLM is a framework that enhances the reliability of Vision-Language Models (VLMs) by estimating the trustworthiness of their predictions without the need for retraining. It addresses the issue of misclassification in VLMs, which can produce confident but incorrect outputs, especially in critical applications. By introducing a novel confidence-scoring function that utilizes the image embedding space, TrustVLM improves the detection of misclassifications. The framework has been rigorously tested across multiple datasets and architectures, showing significant performance improvements in reliability metrics."
                },
                "zh": {
                    "title": "æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦",
                    "desc": "TrustVLM æ˜¯ä¸€ç§å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¯é æ€§çš„æ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹è¯„ä¼°é¢„æµ‹çš„å¯ä¿¡åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„ç½®ä¿¡è¯„åˆ†å‡½æ•°ï¼Œåˆ©ç”¨å›¾åƒåµŒå…¥ç©ºé—´æ¥æ”¹å–„é”™è¯¯åˆ†ç±»çš„æ£€æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒTrustVLM åœ¨ 17 ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šç§æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„å¯é æ€§ï¼ŒTrustVLM ä¸º VLM åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23742",
            "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
            "url": "https://huggingface.co/papers/2505.23742",
            "abstract": "Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF",
            "score": 1,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "7234edb0d22029a4",
            "authors": [
                "Yufan Deng",
                "Xun Guo",
                "Yuanyang Yin",
                "Jacob Zhiyuan Fang",
                "Yiding Yang",
                "Yizhi Wang",
                "Shenghai Yuan",
                "Angtian Wang",
                "Bo Liu",
                "Haibin Huang",
                "Chongyang Ma"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.23742.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "MAGREF: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MAGREF - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ„Ğ¾Ğ½Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. MAGREF Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "MAGREF: Mastering Multi-Subject Video Generation with Flexibility and Precision",
                    "desc": "This paper presents MAGREF, a new framework for generating videos that can include multiple subjects from various reference images and text prompts. It introduces a masked guidance technique that allows the model to adaptively focus on different subjects like people and objects without needing to change its structure. Additionally, it employs a pixel-wise channel concatenation method to maintain the visual features of the subjects during generation. The results show that MAGREF achieves superior video quality and consistency compared to existing methods, making it a significant advancement in multi-subject video synthesis."
                },
                "zh": {
                    "title": "MAGREFï¼šé«˜è´¨é‡å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMAGREFçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºåŸºäºä»»æ„å‚è€ƒå›¾åƒçš„è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ©è”½å¼•å¯¼æœºåˆ¶ï¼Œä»¥å®ç°å¤šä¸»ä½“è§†é¢‘åˆæˆï¼Œç¡®ä¿ç”Ÿæˆçš„ä¸€è‡´æ€§å’Œé«˜è´¨é‡ã€‚æˆ‘ä»¬æå‡ºçš„åŠ¨æ€æ©è”½æœºåˆ¶èƒ½å¤Ÿçµæ´»å¤„ç†ä¸åŒçš„ä¸»ä½“æ¨æ–­ï¼Œè€Œåƒç´ çº§é€šé“è¿æ¥æœºåˆ¶åˆ™æ›´å¥½åœ°ä¿ç•™å¤–è§‚ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGREFåœ¨å¤šä¸»ä½“è§†é¢‘åˆæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œå•†ä¸šåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23419",
            "title": "SWE-bench Goes Live!",
            "url": "https://huggingface.co/papers/2505.23419",
            "abstract": "The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present SWE-bench-Live, a live-updatable benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \\method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings.",
            "score": 1,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "56fc88c3eb857590",
            "authors": [
                "Linghao Zhang",
                "Shilin He",
                "Chaoyun Zhang",
                "Yu Kang",
                "Bowen Li",
                "Chengxing Xie",
                "Junhao Wang",
                "Maoquan Wang",
                "Yufan Huang",
                "Shengyu Fu",
                "Elsie Nallipogu",
                "Qingwei Lin",
                "Yingnong Dang",
                "Saravan Rajmohan",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Microsoft",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23419.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "SWE-bench-Live: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SWE-bench-Live - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², SWE-bench-Live Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1319 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· 93 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ², ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Docker-Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° SWE-bench-Live Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "SWE-bench-Live: A Dynamic Benchmark for Evaluating LLMs in Bug Fixing",
                    "desc": "This paper introduces SWE-bench-Live, a new benchmark for evaluating large language models (LLMs) in the task of generating patches for real-world software bugs. Unlike previous benchmarks like SWE-bench, SWE-bench-Live is continuously updated and includes a broader range of tasks derived from recent GitHub issues, ensuring relevance and diversity. The benchmark features an automated curation pipeline that simplifies the process of creating tasks and setting up environments, reducing manual effort and enhancing scalability. The authors demonstrate that LLMs perform significantly better on SWE-bench-Live compared to static benchmarks, highlighting the importance of using dynamic and up-to-date datasets for evaluation."
                },
                "zh": {
                    "title": "å®æ—¶æ›´æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæå‡æ¨¡å‹è¯„ä¼°èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†SWE-bench-Liveï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å®æ—¶æ›´æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ã€‚ç°æœ‰çš„SWE-benchåŠå…¶å˜ä½“æœªèƒ½åŠæ—¶æ›´æ–°ï¼Œä¸”ä¾èµ–äºæ‰‹åŠ¨æ„å»ºå®ä¾‹å’Œç¯å¢ƒè®¾ç½®ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚SWE-bench-LiveåŒ…å«æ¥è‡ª2024å¹´åçœŸå®GitHubé—®é¢˜çš„1,319ä¸ªä»»åŠ¡ï¼Œå¹¶æä¾›ä¸“ç”¨çš„Dockeré•œåƒä»¥ç¡®ä¿å¯é‡å¤æ‰§è¡Œã€‚é€šè¿‡å¯¹å¤šç§å…ˆè¿›çš„ä»£ç†æ¡†æ¶å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°SWE-bench-Liveåœ¨åŠ¨æ€è½¯ä»¶å¼€å‘ç¯å¢ƒä¸­æä¾›äº†æ›´ä¸ºä¸¥è°¨å’ŒæŠ—æ±¡æŸ“çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22943",
            "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of\n  Pre-trained Multimodal Representation via Text Updates",
            "url": "https://huggingface.co/papers/2505.22943",
            "abstract": "A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.",
            "score": 1,
            "issue_id": 4035,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "0e150bd219867ad7",
            "authors": [
                "Jaewoo Ahn",
                "Heeseung Yun",
                "Dayoon Ko",
                "Gunhee Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22943.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#multimodal",
                    "#benchmark",
                    "#security",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¼ĞµÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Multimodal Adversarial Compositionality (MAC) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…. MAC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing AI Resilience Against Deceptive Text in Multimodal Models",
                    "desc": "This paper introduces a benchmark called Multimodal Adversarial Compositionality (MAC) to assess the weaknesses in multimodal representations, such as those used in AI models like CLIP. It highlights how these models can make incorrect judgments when faced with deceptive text samples generated by large language models. The authors propose a self-training method that improves zero-shot learning by enhancing the success of attacks and increasing the diversity of the samples used. Their approach, which utilizes smaller language models, shows better performance in identifying these vulnerabilities across different types of media, including images, videos, and audio."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤šæ¨¡æ€è¡¨ç¤ºçš„ç»„åˆè„†å¼±æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨æ¬ºéª—æ€§æ–‡æœ¬æ ·æœ¬æ¥è¯„ä¼°å¤šæ¨¡æ€è¡¨ç¤ºä¸­çš„ç»„åˆè„†å¼±æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€å¯¹æŠ—ç»„åˆæ€§ï¼ˆMACï¼‰ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ¬ºéª—æ€§æ–‡æœ¬æ ·æœ¬ï¼Œåˆ©ç”¨è¿™äº›è„†å¼±æ€§è¿›è¡Œè¯„ä¼°ã€‚ä¸ºäº†æ”¹å–„é›¶æ ·æœ¬æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘è®­ç»ƒçš„æ–¹æ³•ï¼Œç»“åˆæ‹’ç»é‡‡æ ·å¾®è°ƒå’Œå¤šæ ·æ€§ä¿ƒè¿›è¿‡æ»¤ï¼Œä»è€Œæé«˜æ”»å‡»æˆåŠŸç‡å’Œæ ·æœ¬å¤šæ ·æ€§ã€‚ä½¿ç”¨è¾ƒå°çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚Llama-3.1-8Bï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ­ç¤ºå„ç§å¤šæ¨¡æ€è¡¨ç¤ºï¼ˆåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ï¼‰çš„ç»„åˆè„†å¼±æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18087",
            "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays",
            "url": "https://huggingface.co/papers/2505.18087",
            "abstract": "CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench",
            "score": 0,
            "issue_id": 4035,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "af9bc7f06b3e9889",
            "authors": [
                "Hyungyung Lee",
                "Geon Choi",
                "Jung-Oh Lee",
                "Hangyul Yoon",
                "Hyuk Gi Hong",
                "Edward Choi"
            ],
            "affiliations": [
                "KAIST",
                "Seoul Medical Center",
                "Seoul National University Hospital"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18087.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#dataset",
                    "#science",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "ğŸ©»",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ",
                    "desc": "CheXStruct Ğ¸ CXReasonBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MIMIC-CXR-JPG Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. CheXStruct Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸. CXReasonBench Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Clinical Diagnosis with Structured Reasoning in AI",
                    "desc": "The paper introduces CheXStruct and CXReasonBench, benchmarks designed to evaluate Large Vision-Language Models (LVLMs) in clinical diagnosis using the MIMIC-CXR-JPG dataset. These tools focus on assessing structured reasoning, visual grounding, and the models' ability to generalize across various diagnostic tasks. CheXStruct automates the extraction of intermediate reasoning steps from chest X-rays, while CXReasonBench tests the models' capacity to perform clinically valid reasoning and learn from structured guidance. The findings reveal that even the best LVLMs struggle with structured reasoning and connecting abstract knowledge to visual data, highlighting the need for improved model capabilities in medical contexts."
                },
                "zh": {
                    "title": "è¯„ä¼°ä¸´åºŠè¯Šæ–­ä¸­çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›",
                    "desc": "CheXStructå’ŒCXReasonBenchæ˜¯ç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„å·¥å…·ã€‚å®ƒä»¬é€šè¿‡åˆ†æèƒ¸éƒ¨Xå…‰å›¾åƒï¼Œè‡ªåŠ¨ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¸®åŠ©æ¨¡å‹è¿›è¡Œç»“æ„åŒ–æ¨ç†ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«18,988ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–12ä¸ªè¯Šæ–­ä»»åŠ¡ï¼Œæ”¯æŒå¤šè·¯å¾„å’Œå¤šé˜¶æ®µè¯„ä¼°ã€‚å°½ç®¡æœ‰10ä¸ªæ¨¡å‹å‚ä¸è¯„ä¼°ï¼Œä½†å®ƒä»¬åœ¨ç»“æ„åŒ–æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-29.html",
    "link_next": "2025-06-02.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "29.05",
        "en": "05/29",
        "zh": "5æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "02.06",
        "en": "06/02",
        "zh": "6æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 8,
        "#agents": 3,
        "#cv": 1,
        "#rl": 5,
        "#rlhf": 4,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 2,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 3,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 3,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« æ—¨åœ¨è§£å†³ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ¨ç†æ—¶çš„ä¸€ä¸ªä¸»è¦éšœç¢ï¼Œå³ç­–ç•¥ç†µçš„å´©æºƒã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æ²¡æœ‰ç†µå¹²é¢„çš„æƒ…å†µä¸‹ï¼Œç­–ç•¥ç†µåœ¨è®­ç»ƒæ—©æœŸé˜¶æ®µä¼šæ€¥å‰§ä¸‹é™ï¼Œå¯¼è‡´æ¢ç´¢èƒ½åŠ›å‡å¼±ï¼Œç­–ç•¥æ€§èƒ½åœæ»ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªå…³äºç†µå’Œä¸‹æ¸¸æ€§èƒ½ä¹‹é—´çš„è½¬æ¢æ–¹ç¨‹ï¼Œå¹¶é€šè¿‡ç†è®ºå’Œå®è¯ç ”ç©¶åˆ†æäº†ç†µåŠ¨æ€ã€‚æœ€ç»ˆï¼Œæ–‡ç« æå‡ºäº†ä¸¤ç§ç®€å•æœ‰æ•ˆçš„æŠ€æœ¯æ¥æ§åˆ¶ç†µï¼Œå®éªŒè¡¨æ˜è¿™äº›æ–¹æ³•èƒ½å¤Ÿä¿ƒè¿›æ¢ç´¢ï¼Œå¸®åŠ©ç­–ç•¥é¿å…ç†µå´©æºƒå¹¶å®ç°æ›´å¥½çš„ä¸‹æ¸¸æ€§èƒ½ã€‚",
        "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
        "pinyin": "è¿™ç¯‡æ–‡ç« æ—¨åœ¨è§£å†³ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ¨ç†æ—¶çš„ä¸€ä¸ªä¸»è¦éšœç¢ï¼Œå³ç­–ç•¥ç†µçš„å´©æºƒã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æ²¡æœ‰ç†µå¹²é¢„çš„æƒ…å†µä¸‹ï¼Œç­–ç•¥ç†µåœ¨è®­ç»ƒæ—©æœŸé˜¶æ®µä¼šæ€¥å‰§ä¸‹é™ï¼Œå¯¼è‡´æ¢ç´¢èƒ½åŠ›å‡å¼±ï¼Œç­–ç•¥æ€§èƒ½åœæ»ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªå…³äºç†µå’Œä¸‹æ¸¸æ€§èƒ½ä¹‹é—´çš„è½¬æ¢æ–¹ç¨‹ï¼Œå¹¶é€šè¿‡ç†è®ºå’Œå®è¯ç ”ç©¶åˆ†æäº†ç†µåŠ¨æ€ã€‚æœ€ç»ˆï¼Œæ–‡ç« æå‡ºäº†ä¸¤ç§ç®€å•æœ‰æ•ˆçš„æŠ€æœ¯æ¥æ§åˆ¶ç†µï¼Œå®éªŒè¡¨æ˜è¿™äº›æ–¹æ³•èƒ½å¤Ÿä¿ƒè¿›æ¢ç´¢ï¼Œå¸®åŠ©ç­–ç•¥é¿å…ç†µå´©æºƒå¹¶å®ç°æ›´å¥½çš„ä¸‹æ¸¸æ€§èƒ½ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng zhÇ yÃº jiÄ› juÃ© shÇ yÃ²ng qiÃ¡ng huÃ  xuÃ© xÃ­ (RL) hÃ© dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs) jÃ¬n xÃ­ng tuÄ« lÇ shÃ­ de yÄ« gÃ¨ zhÇ” yÃ o zhÃ ng Ã i, jÃ­ cÃ¨ lÃ¼Ã¨ shÄng de bÄ“ng kuÃ¬. yÃ¡n jiÅ« fÄ xiÃ n, zÃ i mÃ©i yÇ’u shÄng gÇn rÃ¹ de qÃ­ng kuÃ ng xiÃ , cÃ¨ lÃ¼Ã¨ shÄng zÃ i xÃ¹n liÃ n zÇo qÄ« jiÄ“ duÃ n huÃ¬ jÃ­ jÃ­ xiÃ  jiÃ ng, dÇo zhÃ¬ tÃ n suÇ’ nÃ©ng lÃ¬ jiÇn ruÃ², cÃ¨ lÃ¼Ã¨ xÃ­ng nÃ©ng tÃ­ng zhÃ¬. wÃ©n zhÄng tÃ­ chÅ« le yÄ« gÃ¨ guÄn yÃº shÄng hÃ© xiÃ  yÃº xÃ­ng nÃ©ng zhÄ« jiÄn de zhuÇn huÃ  fÄng chÃ©ng, bÃ¬ng tÅng guÃ² lÇ lÃ¹n hÃ© shÃ­ zhÃ¨ng yÃ¡n jiÅ« fÄ“n xÄ« le shÄng dÃ²ng tÃ i. zuÃ¬ zhÅng, wÃ©n zhÄng tÃ­ chÅ« le liÇng zhÇ’ng jiÇn dÄn yÇ’u xiÃ o de jÃ¬ shÃ¹ lÃ¡i kÃ²ng zhÃ¬ shÄng, shÃ­ yÃ n biÇo mÃ¬ng zhÃ¨ xiÄ“ fÄng fÇ nÃ©ng gÃ²u cÃ¹ jÃ¬n, bÄng zhÃ¹ cÃ¨ lÃ¼Ã¨ bÃ¬ miÇn shÄng bÄ“ng kuÃ¬ bÃ¬ng shÃ­ xiÃ n gÃ¨ng hÇo de xiÃ  yÃº xÃ­ng nÃ©ng.",
        "vocab": "[\n    {\"word\": \"æ—¨åœ¨\", \"pinyin\": \"zhÇ zÃ i\", \"trans\": \"aim to\"},\n    {\"word\": \"è§£å†³\", \"pinyin\": \"jiÄ› juÃ©\", \"trans\": \"solve\"},\n    {\"word\": \"å¼ºåŒ–å­¦ä¹ \", \"pinyin\": \"qiÃ¡ng huÃ  xuÃ© xÃ­\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"ä¸»è¦\", \"pinyin\": \"zhÇ” yÃ o\", \"trans\": \"major\"},\n    {\"word\": \"éšœç¢\", \"pinyin\": \"zhÃ ng Ã i\", \"trans\": \"obstacle\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"ç†µ\", \"pinyin\": \"shÄng\", \"trans\": \"entropy\"},\n    {\"word\": \"å´©æºƒ\", \"pinyin\": \"bÄ“ng kuÃ¬\", \"trans\": \"collapse\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"å‘ç°\", \"pinyin\": \"fÄ xiÃ n\", \"trans\": \"discover\"},\n    {\"word\": \"å¹²é¢„\", \"pinyin\": \"gÄn yÃ¹\", \"trans\": \"intervention\"},\n    {\"word\": \"æƒ…å†µ\", \"pinyin\": \"qÃ­ng kuÃ ng\", \"trans\": \"situation\"},\n    {\"word\": \"é˜¶æ®µ\", \"pinyin\": \"jiÄ“ duÃ n\", \"trans\": \"stage\"},\n    {\"word\": \"æ€¥å‰§\", \"pinyin\": \"jÃ­ jÃ¹\", \"trans\": \"drastic\"},\n    {\"word\": \"ä¸‹é™\", \"pinyin\": \"xiÃ  jiÃ ng\", \"trans\": \"decline\"},\n    {\"word\": \"å¯¼è‡´\", \"pinyin\": \"dÇo zhÃ¬\", \"trans\": \"lead to\"},\n    {\"word\": \"æ¢ç´¢\", \"pinyin\": \"tÃ n suÇ’\", \"trans\": \"exploration\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"å‡å¼±\", \"pinyin\": \"jiÇn ruÃ²\", \"trans\": \"weaken\"},\n    {\"word\": \"åœæ»\", \"pinyin\": \"tÃ­ng zhÃ¬\", \"trans\": \"stagnate\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"è½¬æ¢\", \"pinyin\": \"zhuÇn huÃ n\", \"trans\": \"conversion\"},\n    {\"word\": \"æ–¹ç¨‹\", \"pinyin\": \"fÄng chÃ©ng\", \"trans\": \"equation\"},\n    {\"word\": \"ä¸‹æ¸¸\", \"pinyin\": \"xiÃ  yÃ³u\", \"trans\": \"downstream\"},\n    {\"word\": \"ç†è®º\", \"pinyin\": \"lÇ lÃ¹n\", \"trans\": \"theory\"},\n    {\"word\": \"å®è¯\", \"pinyin\": \"shÃ­ zhÃ¨ng\", \"trans\": \"empirical\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“n xÄ«\", \"trans\": \"analysis\"},\n    {\"word\": \"åŠ¨æ€\", \"pinyin\": \"dÃ²ng tÃ i\", \"trans\": \"dynamics\"},\n    {\"word\": \"æœ€ç»ˆ\", \"pinyin\": \"zuÃ¬ zhÅng\", \"trans\": \"ultimately\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬ shÃ¹\", \"trans\": \"technique\"},\n    {\"word\": \"æ§åˆ¶\", \"pinyin\": \"kÃ²ng zhÃ¬\", \"trans\": \"control\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"è¡¨æ˜\", \"pinyin\": \"biÇo mÃ­ng\", \"trans\": \"indicate\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"ä¿ƒè¿›\", \"pinyin\": \"cÃ¹ jÃ¬n\", \"trans\": \"promote\"},\n    {\"word\": \"é¿å…\", \"pinyin\": \"bÃ¬ miÇn\", \"trans\": \"avoid\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"}\n]",
        "trans": "This article aims to address a major obstacle in using Reinforcement Learning (RL) and Large Language Models (LLMs) for reasoning: the collapse of policy entropy. Research has found that without entropy intervention, policy entropy drops sharply in the early stages of training, leading to reduced exploration capabilities and stagnation in policy performance. The article proposes a transformation equation between entropy and downstream performance and analyzes entropy dynamics through theoretical and empirical research. Ultimately, the article introduces two simple and effective techniques to control entropy. Experiments show that these methods promote exploration, help policies avoid entropy collapse, and achieve better downstream performance.",
        "update_ts": "2025-05-29 10:13"
    }
}