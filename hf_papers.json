{
    "date": {
        "ru": "16 Ğ¸ÑĞ½Ñ",
        "en": "June 16",
        "zh": "6æœˆ16æ—¥"
    },
    "time_utc": "2025-06-16 22:11",
    "weekday": 0,
    "issue_id": 4321,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.09600",
            "title": "Effective Red-Teaming of Policy-Adherent Agents",
            "url": "https://huggingface.co/papers/2506.09600",
            "abstract": "CRAFT, a multi-agent system using policy-aware persuasive strategies, challenges policy-adherent LLM-based agents in customer service to assess and improve their robustness against adversarial attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks",
            "score": 32,
            "issue_id": 4307,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "3de0c796f8d5a171",
            "authors": [
                "Itay Nakash",
                "George Kour",
                "Koren Lazar",
                "Matan Vetzler",
                "Guy Uziel",
                "Ateret Anaby-Tavor"
            ],
            "affiliations": [
                "IBM"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09600.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ›Ğ›Ğœ-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CRAFT - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ›Ğ›Ğœ-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ„ĞµÑ€Ğµ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ³Ñ€Ğ¾Ğ·, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰ÑƒÑÑÑ Ğ½Ğ° Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ°Ñ…, Ğ¿Ñ‹Ñ‚Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ…. CRAFT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DAN-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº tau-break Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Strengthening Policy-Adherent Agents Against Adversarial Manipulation",
                    "desc": "The paper introduces CRAFT, a multi-agent system designed to test and enhance the resilience of policy-adherent language model (LLM) agents in customer service against adversarial attacks. It highlights the challenge of ensuring these agents follow strict policies while still providing helpful interactions. The authors propose a new threat model that focuses on adversarial users who attempt to exploit these agents for personal gain. Additionally, they present tau-break, a benchmark for evaluating agent robustness, and discuss various defense strategies, revealing the need for more robust protections against manipulation."
                },
                "zh": {
                    "title": "CRAFTï¼šæå‡æ”¿ç­–éµå¾ªä»£ç†çš„é²æ£’æ€§",
                    "desc": "CRAFTæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä½¿ç”¨æ”¿ç­–æ„è¯†çš„åŠè¯´ç­–ç•¥ï¼Œæ—¨åœ¨æŒ‘æˆ˜éµå¾ªæ”¿ç­–çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å®¢æˆ·æœåŠ¡ä»£ç†ï¼Œä»¥è¯„ä¼°å’Œæé«˜å…¶å¯¹å¯¹æŠ—æ€§æ”»å‡»çš„é²æ£’æ€§ã€‚éšç€ä»»åŠ¡å¯¼å‘çš„LLMä»£ç†åœ¨ä¸¥æ ¼æ”¿ç­–é¢†åŸŸçš„åº”ç”¨å¢åŠ ï¼Œç¡®ä¿ä»£ç†å§‹ç»ˆéµå¾ªè¿™äº›è§„åˆ™å¹¶é€‚å½“åœ°æ‹’ç»è¿è§„è¯·æ±‚å˜å¾—è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¨èƒæ¨¡å‹ï¼Œä¸“æ³¨äºåˆ©ç”¨éµå¾ªæ”¿ç­–çš„ä»£ç†è¿›è¡Œä¸ªäººåˆ©ç›Šçš„å¯¹æŠ—æ€§ç”¨æˆ·ã€‚CRAFTé€šè¿‡åˆ©ç”¨æ”¿ç­–æ„è¯†çš„åŠè¯´ç­–ç•¥ï¼Œåœ¨å®¢æˆ·æœåŠ¡åœºæ™¯ä¸­æœ‰æ•ˆåœ°å‰Šå¼±äº†éµå¾ªæ”¿ç­–çš„ä»£ç†ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è¶Šç‹±æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11924",
            "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation",
            "url": "https://huggingface.co/papers/2506.11924",
            "abstract": "A diffusion-based framework generates aligned novel views of images and geometry using warping-and-inpainting with cross-modal attention distillation and proximity-based mesh conditioning, achieving high-fidelity synthesis and 3D completion.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.",
            "score": 27,
            "issue_id": 4305,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "bf8d340f29d7ad95",
            "authors": [
                "Min-Seop Kwak",
                "Junho Kim",
                "Sangdoo Yun",
                "Dongyoon Han",
                "Taekyoung Kim",
                "Seungryong Kim",
                "Jin-Hwa Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "NAVER AI Lab",
                "SNU AIIS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11924.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ»Ğ¸Ğ·Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "High-Fidelity 3D View Synthesis through Diffusion and Attention",
                    "desc": "This paper presents a diffusion-based framework for generating new views of images and their corresponding 3D geometry. It uses a technique called warping-and-inpainting, which allows for the synthesis of images and geometry without needing a lot of pre-existing data. The method incorporates cross-modal attention distillation to ensure that the generated images and geometries are well-aligned, enhancing the quality of the output. Additionally, it employs proximity-based mesh conditioning to improve the accuracy of the generated 3D structures, resulting in high-fidelity synthesis and completion of 3D scenes."
                },
                "zh": {
                    "title": "åŸºäºæ‰©æ•£çš„é«˜ä¿çœŸå›¾åƒä¸å‡ ä½•ä½“ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œé€šè¿‡æ‰­æ›²å’Œä¿®å¤çš„æ–¹æ³•ç”Ÿæˆå¯¹é½çš„æ–°è§†å›¾å›¾åƒå’Œå‡ ä½•ä½“ã€‚ä¸ä»¥å¾€éœ€è¦å¯†é›†å§¿æ€å›¾åƒæˆ–é™åˆ¶äºç‰¹å®šé¢†åŸŸè§†å›¾çš„ç”Ÿæˆæ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç°æˆçš„å‡ ä½•é¢„æµ‹å™¨æ¥é¢„æµ‹å‚è€ƒå›¾åƒçš„éƒ¨åˆ†å‡ ä½•ä½“ï¼Œå¹¶å°†æ–°è§†å›¾åˆæˆè§†ä¸ºå›¾åƒå’Œå‡ ä½•ä½“çš„ä¿®å¤ä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„å›¾åƒå’Œå‡ ä½•ä½“ä¹‹é—´çš„å‡†ç¡®å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€æ³¨æ„åŠ›è’¸é¦ï¼Œå°†å›¾åƒæ‰©æ•£åˆ†æ”¯çš„æ³¨æ„åŠ›å›¾æ³¨å…¥åˆ°å¹¶è¡Œçš„å‡ ä½•æ‰©æ•£åˆ†æ”¯ä¸­ã€‚é€šè¿‡è¿™ç§å¤šä»»åŠ¡æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†å‡ ä½•ç¨³å¥çš„å›¾åƒåˆæˆå’Œæ¸…æ™°çš„å‡ ä½•é¢„æµ‹ï¼Œæœ€ç»ˆåœ¨æœªè§åœºæ™¯ä¸­å®ç°äº†é«˜ä¿çœŸåº¦çš„è§†å›¾åˆæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10892",
            "title": "The Diffusion Duality",
            "url": "https://huggingface.co/papers/2506.10892",
            "abstract": "Duo improves uniform-state discrete diffusion models by transferring techniques from Gaussian diffusion, enhancing training speed and enabling fast few-step text generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: http://s-sahoo.github.io/duo",
            "score": 23,
            "issue_id": 4305,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "974b708b2e781af0",
            "authors": [
                "Subham Sekhar Sahoo",
                "Justin Deschenaux",
                "Aaron Gokaslan",
                "Guanghan Wang",
                "Justin Chiu",
                "Volodymyr Kuleshov"
            ],
            "affiliations": [
                "Computer and Information Science, Cornell Tech, NYC, USA",
                "School of Computer and Communication Sciences, EPFL Lausanne, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10892.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Duo: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Duo ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ° ÑÑ‡ĞµÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸. Duo Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½Ğ° Ğ´Ğ²Ğ° Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°."
                },
                "en": {
                    "title": "Duo: Accelerating Diffusion Models for Fast Text Generation",
                    "desc": "This paper presents Duo, a method that enhances uniform-state discrete diffusion models by incorporating techniques from Gaussian diffusion. The authors introduce a curriculum learning strategy that accelerates training speed by reducing variance, allowing models to outperform autoregressive models in zero-shot perplexity on several benchmarks. Additionally, they propose Discrete Consistency Distillation, which enables faster few-step text generation by adapting consistency distillation for discrete settings. Overall, Duo significantly improves the efficiency and performance of diffusion language models."
                },
                "zh": {
                    "title": "Duoï¼šåŠ é€Ÿæ–‡æœ¬ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDuoçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°†é«˜æ–¯æ‰©æ•£çš„æŠ€æœ¯è½¬ç§»åˆ°å‡åŒ€çŠ¶æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œæé«˜è®­ç»ƒé€Ÿåº¦å’Œå¿«é€Ÿæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚å‡åŒ€çŠ¶æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹å…·æœ‰è‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ï¼Œä½†é€šå¸¸åœ¨æ€§èƒ½ä¸Šä¸åŠè‡ªå›å½’æ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹ã€‚Duoé€šè¿‡å¼•å…¥åŸºäºé«˜æ–¯è¿‡ç¨‹çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†è‡ªå›å½’æ¨¡å‹ã€‚è¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†ç¦»æ•£ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œä½¿å¾—æ‰©æ•£è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå®ç°å¿«é€Ÿçš„å°‘æ­¥ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10128",
            "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual\n  Perception in VLMs",
            "url": "https://huggingface.co/papers/2506.10128",
            "abstract": "ViCrit, an RL task for fine-tuning VLMs, improves visual perception by training models to detect subtle hallucinations in image captions, with gains transferable to various visual domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in vision-language models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from a 200-word captions, we inject a single, subtle visual description error-altering a few words on objects, attributes, counts, or spatial relations-and task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing a binary, exact-match reward that is easy to compute and unambiguous. Models trained with the ViCrit Task exhibit substantial gains across a variety of VL benchmarks. Crucially, the improvements transfer beyond natural-image training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce ViCrit-Bench, a category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs.",
            "score": 13,
            "issue_id": 4309,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "5045b62235ae5509",
            "authors": [
                "Xiyao Wang",
                "Zhengyuan Yang",
                "Chao Feng",
                "Yongyuan Liang",
                "Yuhang Zhou",
                "Xiaoyu Liu",
                "Ziyi Zang",
                "Ming Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Linjie Li",
                "Furong Huang",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Cardiff University",
                "Microsoft",
                "University of Maryland, College Park",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10128.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#hallucinations",
                    "#cv",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ViCrit: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ViCrit - Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ViCrit Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ… Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ViCrit, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Enhancing Visual Perception in VLMs with ViCrit",
                    "desc": "ViCrit is a reinforcement learning task designed to enhance the visual perception capabilities of vision-language models (VLMs) by training them to identify subtle hallucinations in image captions. The task involves injecting minor visual description errors into human-written captions and challenging the model to locate these errors based on the corresponding images. This approach not only maintains the complexity of visual perception but also provides a clear and straightforward reward system for the model's performance. The results show that models trained with ViCrit achieve significant improvements across various visual benchmarks, indicating that this method fosters a deeper understanding of visual content rather than mere memorization."
                },
                "zh": {
                    "title": "é€šè¿‡ViCritæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "ViCritæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œæ—¨åœ¨å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹æ£€æµ‹å›¾åƒæ ‡é¢˜ä¸­çš„ç»†å¾®å¹»è§‰æ¥æé«˜è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨äººå·¥æ’°å†™çš„å›¾åƒæ ‡é¢˜ä¸­æ³¨å…¥è½»å¾®çš„è§†è§‰æè¿°é”™è¯¯ï¼Œè¦æ±‚æ¨¡å‹è¯†åˆ«è¿™äº›é”™è¯¯ï¼Œä»è€Œä¿æŒæ„ŸçŸ¥çš„éš¾åº¦ã€‚ç»è¿‡ViCritä»»åŠ¡è®­ç»ƒçš„æ¨¡å‹åœ¨å„ç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œä¸”è¿™äº›æ”¹è¿›ä¸ä»…é™äºè‡ªç„¶å›¾åƒæ•°æ®ï¼Œè¿˜èƒ½è¿ç§»åˆ°æŠ½è±¡å›¾åƒæ¨ç†å’Œè§†è§‰æ•°å­¦ç­‰é¢†åŸŸã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç»†è‡´çš„å¹»è§‰æ‰¹è¯„æ˜¯ä¸€ç§æœ‰æ•ˆä¸”å¯æ¨å¹¿çš„ç›®æ ‡ï¼Œæœ‰åŠ©äºå¢å¼ºVLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11928",
            "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
            "url": "https://huggingface.co/papers/2506.11928",
            "abstract": "LLMs perform well on implementation-heavy competitive programming problems but struggle with nuanced algorithmic reasoning, as highlighted by LiveCodeBench Pro.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.",
            "score": 11,
            "issue_id": 4305,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "4d3f2213d58dd8dd",
            "authors": [
                "Zihan Zheng",
                "Zerui Cheng",
                "Zeyu Shen",
                "Shang Zhou",
                "Kaiyuan Liu",
                "Hansen He",
                "Dongruixuan Li",
                "Stanley Wei",
                "Hangyi Hao",
                "Jianzhu Yao",
                "Peiyao Sheng",
                "Zixuan Wang",
                "Wenhao Chai",
                "Aleksandra Korolova",
                "Peter Henderson",
                "Sanjeev Arora",
                "Pramod Viswanath",
                "Jingbo Shang",
                "Saining Xie"
            ],
            "affiliations": [
                "Canyon Crest Academy",
                "McGill University",
                "New York University",
                "Princeton University",
                "Sentient Foundation",
                "University of California San Diego",
                "University of Washington",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11928.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LLM Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: ÑĞ¸Ğ»Ğ° Ğ² Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ÑŒ Ğ² Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LiveCodeBench Pro, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· Codeforces, ICPC Ğ¸ IOI. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 53% pass@1 Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ 0% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ LLM Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-Ğ³Ñ€Ğ¾ÑÑĞ¼ĞµĞ¹ÑÑ‚ĞµÑ€Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs vs. Human Algorithmic Mastery",
                    "desc": "This paper evaluates the performance of large language models (LLMs) in competitive programming using a new benchmark called LiveCodeBench Pro. It reveals that while LLMs excel in implementation-heavy tasks, they struggle with complex algorithmic reasoning and nuanced problem-solving. The study shows that even the best LLMs achieve only 53% success on medium-difficulty problems and none on hard problems, indicating a significant gap compared to human experts. The findings suggest that LLMs rely more on implementation accuracy and external tools rather than advanced reasoning skills, highlighting areas for future improvement in AI-driven coding solutions."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç®—æ³•æ¨ç†ä¸­çš„å±€é™æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç«äº‰ç¼–ç¨‹ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å®ç°å¯†é›†å‹é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚ç®—æ³•æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç ”ç©¶å¼•å…¥äº†LiveCodeBench Proï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºCodeforcesã€ICPCå’ŒIOIçš„é—®é¢˜åŸºå‡†ï¼Œæ—¨åœ¨å‡å°‘æ•°æ®æ±¡æŸ“çš„å¯èƒ½æ€§ã€‚é€šè¿‡å¯¹æ¨¡å‹ç”Ÿæˆçš„æäº¤è¿›è¡Œé€è¡Œåˆ†æï¼Œå‘ç°å½“å‰çš„å‰æ²¿æ¨¡å‹åœ¨ä¸­ç­‰éš¾åº¦é—®é¢˜ä¸Šçš„é€šè¿‡ç‡ä»…ä¸º53%ï¼Œè€Œåœ¨å›°éš¾é—®é¢˜ä¸Šåˆ™ä¸º0%ã€‚è¿™è¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨å®ç°ç²¾åº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚çš„ç®—æ³•æ¨ç†å’Œæ¡ˆä¾‹åˆ†æä¸­ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08989",
            "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement\n  Learning for LLM Reasoning",
            "url": "https://huggingface.co/papers/2506.08989",
            "abstract": "A self-aware problem synthesis framework that leverages model weaknesses enhances reinforcement learning with verifiable rewards, improving large language model performance on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.",
            "score": 8,
            "issue_id": 4311,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "332357cc97416117",
            "authors": [
                "Xiao Liang",
                "Zhong-Zhi Li",
                "Yeyun Gong",
                "Yang Wang",
                "Hengyuan Zhang",
                "Yelong Shen",
                "Ying Nian Wu",
                "Weizhu Chen"
            ],
            "affiliations": [
                "Microsoft",
                "School of Artificial Intelligence, Chinese Academy of Sciences",
                "Tsinghua University",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08989.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Self-aware Weakness-driven problem Synthesis (SwS). Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. SwS Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B Ğ¸ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Models by Learning from Their Weaknesses",
                    "desc": "This paper presents a Self-aware Weakness-driven problem Synthesis framework (SwS) that enhances reinforcement learning for large language models (LLMs) by focusing on their weaknesses. The framework identifies specific areas where the model struggles and generates new problems to help the model improve in those areas. By systematically augmenting the training set with these tailored problems, the model can better learn and generalize its reasoning capabilities. The results show significant performance improvements on reasoning tasks, demonstrating the effectiveness of leveraging model weaknesses for training."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æ„è¯†é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜åˆæˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘æ„è¯†çš„å¼±ç‚¹é©±åŠ¨é—®é¢˜åˆæˆæ¡†æ¶ï¼ˆSwSï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«æ¨¡å‹çš„ä¸è¶³æ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„å¯éªŒè¯å¥–åŠ±ã€‚è¯¥æ¡†æ¶ç³»ç»Ÿåœ°åˆ†ææ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åå¤å¤±è´¥çš„é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è¿™äº›å¤±è´¥æ¡ˆä¾‹æç‚¼æ ¸å¿ƒæ¦‚å¿µï¼Œåˆæˆæ–°çš„é—®é¢˜ä»¥åŠ å¼ºæ¨¡å‹çš„è–„å¼±ç¯èŠ‚ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨åç»­çš„å¢å¼ºè®­ç»ƒä¸­é›†ä¸­ç²¾åŠ›å…‹æœè‡ªèº«çš„å¼±ç‚¹ï¼Œè€Œæ— éœ€ä¾èµ–å¤–éƒ¨çŸ¥è¯†è’¸é¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹³å‡æå‡è¾¾10.0%å’Œ7.7%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11930",
            "title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback",
            "url": "https://huggingface.co/papers/2506.11930",
            "abstract": "LLMs show resistance to feedback, termed feedback friction, even under ideal conditions, and sampling-based strategies only partially mitigate this issue.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform a rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement.",
            "score": 7,
            "issue_id": 4314,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "0fc67d5ee77483c7",
            "authors": [
                "Dongwei Jiang",
                "Alvin Zhang",
                "Andrew Wang",
                "Nicholas Andrews",
                "Daniel Khashabi"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.11930.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#hallucinations",
                    "#alignment",
                    "#reasoning",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ±Ğ¾Ñ€ÑŒĞ±Ğ° Ñ 'Ñ‚Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸' Ğ² LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¾Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ 'Ñ‚Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸', Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Claude 3.7. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ»Ğ¸ÑˆÑŒ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑĞ¼ÑĞ³Ñ‡Ğ°ÑÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Unpacking Feedback Friction in LLMs",
                    "desc": "This paper investigates the phenomenon of feedback friction in large language models (LLMs), where these models struggle to effectively incorporate external feedback even in ideal conditions. The authors conduct controlled experiments where a solver model receives targeted feedback from a feedback generator based on near-complete ground-truth answers. Despite this optimal setup, the models consistently show resistance to changing their incorrect responses, indicating a significant limitation in their learning process. The study also explores various strategies to mitigate this issue, such as sampling-based methods, but finds that these approaches only partially improve performance, highlighting the need for further research into enhancing LLMs' self-improvement capabilities."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„åé¦ˆæ‘©æ“¦é—®é¢˜",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¥æ”¶å¤–éƒ¨åé¦ˆæ—¶çš„è¡¨ç°ï¼Œå‘ç°å®ƒä»¬å­˜åœ¨ä¸€ç§ç§°ä¸ºåé¦ˆæ‘©æ“¦ï¼ˆfeedback frictionï¼‰çš„ç°è±¡ï¼Œå³ä½¿åœ¨ç†æƒ³æ¡ä»¶ä¸‹ä¹Ÿéš¾ä»¥æœ‰æ•ˆæ•´åˆåé¦ˆã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ§åˆ¶å®éªŒç¯å¢ƒï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ã€çŸ¥è¯†æ¨ç†å’Œç§‘å­¦æ¨ç†ç­‰å¤šç§ä»»åŠ¡ä¸­çš„åé¦ˆæ•´åˆèƒ½åŠ›ã€‚å°½ç®¡æä¾›äº†æ¥è¿‘å®Œç¾çš„åé¦ˆï¼Œæ¨¡å‹ä»ç„¶è¡¨ç°å‡ºå¯¹åé¦ˆçš„æŠµæŠ—ï¼Œæœªèƒ½æ˜¾è‘—æ”¹å–„å…¶é”™è¯¯ç­”æ¡ˆã€‚é€šè¿‡å®éªŒä¸åŒçš„é‡‡æ ·ç­–ç•¥ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›ç­–ç•¥è™½ç„¶æœ‰æ‰€æ”¹å–„ï¼Œä½†ä»æœªèƒ½ä½¿æ¨¡å‹è¾¾åˆ°é¢„æœŸçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11886",
            "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
            "url": "https://huggingface.co/papers/2506.11886",
            "abstract": "FourierAttention is a training-free framework that enhances memory efficiency in Large Language Models by compressing long-context-insensitive transformer head dimensions using orthogonal Fourier bases, while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.",
            "score": 6,
            "issue_id": 4310,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "6e5e8424bfbe53cc",
            "authors": [
                "Xiaoran Liu",
                "Siyang He",
                "Qiqi Wang",
                "Ruixiao Li",
                "Yuerong Song",
                "Zhigeng Liu",
                "Linlin Li",
                "Qun Liu",
                "Zengfeng Huang",
                "Qipeng Guo",
                "Ziwei He",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "School of Computer Science, Fudan University",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11886.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#inference",
                    "#long_context",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "FourierAttention - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ½ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¼Ğ°Ğ»Ğ¾Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¸ÑÑ‹ Ğ¤ÑƒÑ€ÑŒĞµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. FourierAttention Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°: Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ° Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLaMA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FourierAttention Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LongBench Ğ¸ Needle-In-A-Haystack."
                },
                "en": {
                    "title": "Enhancing Memory Efficiency in LLMs with FourierAttention",
                    "desc": "FourierAttention is a novel framework designed to improve memory efficiency in Large Language Models (LLMs) without the need for training. It addresses the challenge of increasing memory demands from the Key-Value (KV) cache as context lengths grow. By using orthogonal Fourier bases, it compresses transformer head dimensions, allowing lower dimensions to focus on local context while higher dimensions capture long-range dependencies. Evaluations demonstrate that FourierAttention enhances long-context accuracy and includes a custom kernel, FlashFourierAttention, for optimized memory operations during deployment."
                },
                "zh": {
                    "title": "FourierAttentionï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…å­˜æ•ˆç‡",
                    "desc": "FourierAttentionæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…å­˜æ•ˆç‡ã€‚å®ƒé€šè¿‡ä½¿ç”¨æ­£äº¤å‚…é‡Œå¶åŸºæ¥å‹ç¼©é•¿ä¸Šä¸‹æ–‡æ— å…³çš„å˜æ¢å¤´ç»´åº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜æ¢å¤´ç»´åº¦çš„å¼‚è´¨æ€§ï¼Œä½ç»´åº¦ä¼˜å…ˆå¤„ç†å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œè€Œé«˜ç»´åº¦åˆ™æ•æ‰é•¿ç¨‹ä¾èµ–ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFourierAttentionåœ¨é•¿ä¸Šä¸‹æ–‡å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”é€šè¿‡å®šåˆ¶çš„Tritonå†…æ ¸ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œç¡®ä¿é«˜æ•ˆéƒ¨ç½²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07464",
            "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO",
            "url": "https://huggingface.co/papers/2506.07464",
            "abstract": "DeepVideo-R1 enhances video reasoning performance using Reg-GRPO, a regression-based GRPO approach, and difficulty-aware data augmentation for video large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing a PPO-style reinforcement algorithm with group-based normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as a regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks.",
            "score": 5,
            "issue_id": 4308,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "f8c207e9d26fe89e",
            "authors": [
                "Jinyoung Park",
                "Jeehye Na",
                "Jinyoung Kim",
                "Hyunwoo J. Kim"
            ],
            "affiliations": [
                "KAIST",
                "Korea University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07464.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#video",
                    "#benchmark",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "DeepVideo-R1: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ GRPO",
                    "desc": "DeepVideo-R1 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ GRPO (Reg-GRPO) Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ GRPO Ğº Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¯Ğ‘Ğœ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°. Reg-GRPO Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ GRPO ĞºĞ°Ğº Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ, Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Ñ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Reasoning with Reg-GRPO and Smart Data Augmentation",
                    "desc": "DeepVideo-R1 is a novel approach that enhances video reasoning in large language models by utilizing a regression-based method called Reg-GRPO. This method reformulates the Group Relative Policy Optimization (GRPO) objective into a regression task, allowing for more direct policy guidance without the need for complex safeguards. Additionally, the paper introduces a difficulty-aware data augmentation strategy that adjusts training samples based on their solvable difficulty, which helps in generating diverse and informative reward signals. The results demonstrate that DeepVideo-R1 significantly boosts performance on various video reasoning benchmarks, showcasing its effectiveness in the field."
                },
                "zh": {
                    "title": "DeepVideo-R1ï¼šæå‡è§†é¢‘æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "DeepVideo-R1 æ˜¯ä¸€ç§å¢å¼ºè§†é¢‘æ¨ç†æ€§èƒ½çš„æ¨¡å‹ï¼Œé‡‡ç”¨äº†å›å½’å‹çš„ GRPO æ–¹æ³•å’Œéš¾åº¦æ„ŸçŸ¥çš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚è¯¥ç ”ç©¶æ¢è®¨äº† GRPO åœ¨è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå¹¶è¯†åˆ«å‡ºå½±å“æœ‰æ•ˆå­¦ä¹ çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¾èµ–ä¿æŠ¤æªæ–½å’Œä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒDeepVideo-R1 é€šè¿‡å›å½’ GRPO é‡æ–°æ„å»ºäº† GRPO ç›®æ ‡ï¼Œç›´æ¥é¢„æµ‹ä¼˜åŠ¿å€¼ï¼Œä»è€Œç®€åŒ–äº†æ”¿ç­–æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepVideo-R1 åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†è§†é¢‘æ¨ç†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11997",
            "title": "pLSTM: parallelizable Linear Source Transition Mark networks",
            "url": "https://huggingface.co/papers/2506.11997",
            "abstract": "pLSTMs are parallelizable linear RNNs designed for DAGs, demonstrating superior performance on long-range tasks and benchmarks compared to Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern recurrent architectures, such as xLSTM and Mamba, have recently challenged the Transformer in language modeling. However, their structure constrains their applicability to sequences only or requires processing multi-dimensional data structures, such as images or molecular graphs, in a pre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are well suited for data with a higher level structure, like 2D grids, trees, and directed acyclic graphs (DAGs). In this work, we extend the notion of multi-dimensionality to linear RNNs. We introduce parallelizable Linear Source Transition Mark networks (pLSTMs) using Source, Transition, and Mark gates that act on the line graph of a general DAG. This enables parallelization in analogy to parallel associative scans and the chunkwise-recurrent form of sequential linear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this scheme can be efficiently implemented using einsum operations, concatenations, and padding in logarithmic time. pLSTMs tackle the vanishing/exploding activation/gradient problem for long distances in DAGs via two distinct modes: a directed propagation mode (P-mode) and a diffusive distribution mode (D-mode). To showcase the long-range capabilities of pLSTM, we introduce arrow-pointing extrapolation as a synthetic computer vision task that contains long-distance directional information. We demonstrate that pLSTMs generalize well to larger image sizes, whereas Transformers struggle to extrapolate. On established molecular graph and computer vision benchmarks, pLSTMs also show strong performance. Code and Datasets are available at: https://github.com/ml-jku/plstm_experiments.",
            "score": 4,
            "issue_id": 4308,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "6dff119551b986fc",
            "authors": [
                "Korbinian PÃ¶ppel",
                "Richard Freinschlag",
                "Thomas Schmied",
                "Wei Lin",
                "Sepp Hochreiter"
            ],
            "affiliations": [
                "Johannes Kepler University Linz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11997.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#optimization",
                    "#graphs",
                    "#architecture",
                    "#cv",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "pLSTM: ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ RNN Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ pLSTM (parallelizable Linear Source Transition Mark), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² (DAG). pLSTM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². pLSTM Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‰Ğ¸Ñ…/Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² DAG Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ²: Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "pLSTMs: Revolutionizing Long-Range Learning in DAGs",
                    "desc": "The paper introduces parallelizable Linear Source Transition Mark networks (pLSTMs), a new type of linear recurrent neural network (RNN) designed for processing data structured as directed acyclic graphs (DAGs). Unlike traditional RNNs and Transformers, pLSTMs can efficiently handle multi-dimensional data without being limited to sequential processing. They address the vanishing and exploding gradient problems through two modes of operation, allowing for effective long-range dependencies in data. The authors demonstrate that pLSTMs outperform Transformers on various benchmarks, particularly in tasks requiring long-distance extrapolation, such as computer vision and molecular graph analysis."
                },
                "zh": {
                    "title": "pLSTMsï¼šè¶…è¶Šå˜æ¢å™¨çš„é•¿è·ç¦»å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "pLSTMsæ˜¯ä¸€ç§å¯å¹¶è¡ŒåŒ–çš„çº¿æ€§é€’å½’ç¥ç»ç½‘ç»œï¼Œä¸“ä¸ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨é•¿è·ç¦»ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå˜æ¢å™¨ï¼ˆTransformersï¼‰ã€‚ä¸ç°ä»£é€’å½’æ¶æ„ç›¸æ¯”ï¼ŒpLSTMsé€šè¿‡æºã€è½¬ç§»å’Œæ ‡è®°é—¨çš„è®¾è®¡ï¼Œè§£å†³äº†é•¿è·ç¦»ä¼ æ’­ä¸­çš„æ¶ˆå¤±å’Œçˆ†ç‚¸æ¢¯åº¦é—®é¢˜ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºæ›´é«˜ç»“æ„çš„æ•°æ®ï¼Œå¦‚äºŒç»´ç½‘æ ¼å’Œæ ‘å½¢ç»“æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å›¾åƒç­‰å¤šç»´æ•°æ®ã€‚é€šè¿‡å¼•å…¥ç®­å¤´æŒ‡å‘å¤–æ¨çš„åˆæˆè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ŒpLSTMså±•ç¤ºäº†å…¶åœ¨é•¿è·ç¦»æ¨æ–­ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09427",
            "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved\n  Image-Text Generation",
            "url": "https://huggingface.co/papers/2506.09427",
            "abstract": "InterSyn, a large-scale dataset with tightly interleaved image-text outputs and automated quality refinement, improves multimodal understanding and generation through the SEIR method and SynJudge, an automatic evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy.   Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement.   Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn's utility for advancing multimodal systems.",
            "score": 4,
            "issue_id": 4305,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "5c1dd5f02a121213",
            "authors": [
                "Yukang Feng",
                "Jianwen Sun",
                "Chuanhao Li",
                "Zizhen Li",
                "Jiaxin Ai",
                "Fanrui Zhang",
                "Yifan Chang",
                "Sizhuo Zhou",
                "Shenglin Zhang",
                "Yu Dai",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Nankai University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09427.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "InterSyn: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ InterSyn - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. InterSyn ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸ĞµĞ¼ (SEIR) Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ñ Ñ‚ĞµÑĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ SynJudge - Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° InterSyn ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Multimodal AI with InterSyn and SEIR",
                    "desc": "The paper introduces InterSyn, a large-scale dataset designed to enhance multimodal understanding and generation in AI models. It utilizes the Self-Evaluation with Iterative Refinement (SEIR) method to create high-quality, tightly interleaved image-text outputs through multi-turn dialogues. Additionally, the paper presents SynJudge, an automatic evaluation tool that assesses multimodal outputs based on text content, image quality, and their synergy. Experimental results demonstrate that models trained on InterSyn show significant performance improvements across various evaluation metrics, highlighting its effectiveness for next-generation instruction-following models."
                },
                "zh": {
                    "title": "InterSynï¼šæå‡å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„å…³é”®æ•°æ®é›†",
                    "desc": "InterSynæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚å®ƒé€šè¿‡è‡ªæˆ‘è¯„ä¼°ä¸è¿­ä»£ç²¾ç‚¼ï¼ˆSEIRï¼‰æ–¹æ³•æ„å»ºï¼ŒåŒ…å«å¤šè½®æŒ‡ä»¤é©±åŠ¨çš„å¯¹è¯å’Œç´§å¯†äº¤ç»‡çš„å›¾åƒ-æ–‡æœ¬è¾“å‡ºã€‚ä¸ºäº†è¯„ä¼°è¿™äº›è¾“å‡ºçš„è´¨é‡ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†SynJudgeï¼Œä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°å·¥å…·ï¼Œå¯ä»¥ä»æ–‡æœ¬å†…å®¹ã€å›¾åƒå†…å®¹ã€å›¾åƒè´¨é‡å’Œå›¾åƒ-æ–‡æœ¬ååŒå››ä¸ªç»´åº¦è¿›è¡Œé‡åŒ–è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SEIRæ–¹æ³•æ„å»ºçš„æ•°æ®é›†è´¨é‡æ˜¾è‘—æé«˜ï¼Œè®­ç»ƒåœ¨InterSynä¸Šçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09366",
            "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation\n  via Skill Blending",
            "url": "https://huggingface.co/papers/2506.09366",
            "abstract": "SkillBlender is a hierarchical reinforcement learning framework that uses pretrained primitive skills to efficiently solve diverse loco-manipulation tasks for humanoid robots.  \t\t\t\t\tAI-generated summary \t\t\t\t Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and loco-manipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, a parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by a set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/.",
            "score": 4,
            "issue_id": 4305,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "411c39c85d7cabe0",
            "authors": [
                "Yuxuan Kuang",
                "Haoran Geng",
                "Amine Elhafsi",
                "Tan-Dzung Do",
                "Pieter Abbeel",
                "Jitendra Malik",
                "Marco Pavone",
                "Yue Wang"
            ],
            "affiliations": [
                "Peking University",
                "Stanford University",
                "University of California, Berkeley",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09366.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#robotics",
                    "#rl",
                    "#open_source",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "SkillBlender: ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "SkillBlender - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ»Ğ¾ĞºĞ¾Ğ¼Ğ¾Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ SkillBench - Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SkillBlender Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Empowering Humanoid Robots with SkillBlender: Efficient Loco-Manipulation through Skill Blending",
                    "desc": "SkillBlender is a hierarchical reinforcement learning framework designed to enhance the performance of humanoid robots in loco-manipulation tasks. It utilizes pretrained primitive skills that are goal-conditioned and task-agnostic, allowing for efficient blending of these skills to tackle complex tasks without extensive reward tuning. This approach not only improves the versatility of the robots but also ensures that their movements are accurate and feasible in real-world scenarios. Additionally, SkillBench provides a comprehensive benchmark for evaluating the performance of these skills across different robot embodiments and tasks, promoting further research in the field."
                },
                "zh": {
                    "title": "SkillBlenderï¼šé«˜æ•ˆçš„äººå½¢æœºå™¨äººè¿åŠ¨æ“æ§æ¡†æ¶",
                    "desc": "SkillBlender æ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„åŸºæœ¬æŠ€èƒ½é«˜æ•ˆè§£å†³äººå½¢æœºå™¨äººåœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„è¿åŠ¨æ“æ§ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é¦–å…ˆé¢„è®­ç»ƒä¸ä»»åŠ¡æ— å…³çš„ç›®æ ‡å¯¼å‘åŸºæœ¬æŠ€èƒ½ï¼Œç„¶ååŠ¨æ€èåˆè¿™äº›æŠ€èƒ½ï¼Œä»¥æœ€å°çš„ä»»åŠ¡ç‰¹å®šå¥–åŠ±è®¾è®¡å®Œæˆå¤æ‚çš„è¿åŠ¨æ“æ§ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥ SkillBenchï¼Œä¸€ä¸ªåŒ…å«å¤šç§æ¨¡æ‹Ÿç¯å¢ƒå’ŒæŒ‘æˆ˜æ€§ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼ŒSkillBlender æä¾›äº†ç§‘å­¦çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹³è¡¡äº†å‡†ç¡®æ€§å’Œå¯è¡Œæ€§ã€‚å¤§é‡çš„æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼ŒSkillBlender æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªç„¶åœ°è§„èŒƒè¡Œä¸ºï¼Œé¿å…å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œä»è€Œå®ç°æ›´å‡†ç¡®å’Œå¯è¡Œçš„è¿åŠ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11474",
            "title": "Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified\n  Process Rewards",
            "url": "https://huggingface.co/papers/2506.11474",
            "abstract": "Med-PRM enhances clinical decision making by verifying reasoning steps against medical knowledge bases, achieving state-of-the-art performance in medical QA benchmarks with improved accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/",
            "score": 3,
            "issue_id": 4314,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "123eb749b81758d8",
            "authors": [
                "Jaehoon Yun",
                "Jiwoong Sohn",
                "Jungwoo Park",
                "Hyunjae Kim",
                "Xiangru Tang",
                "Yanjun Shao",
                "Yonghoe Koo",
                "Minhyeok Ko",
                "Qingyu Chen",
                "Mark Gerstein",
                "Michael Moor",
                "Jaewoo Kang"
            ],
            "affiliations": [
                "AIGEN Sciences",
                "ETH ZÃ¼rich",
                "Hanyang University College of Medicine",
                "Korea University",
                "University of Ulsan College of Medicine",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11474.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#dataset",
                    "#training",
                    "#rag",
                    "#reasoning",
                    "#benchmark",
                    "#science",
                    "#small_models"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼: Med-PRM Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³",
                    "desc": "Med-PRM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ñ€Ğ°Ñ‡Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ ĞµĞ³Ğ¾ Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Med-PRM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 13.5%. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Meerkat, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 80% Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ MedQA."
                },
                "en": {
                    "title": "Enhancing Medical Decision Making with Verified Reasoning Steps",
                    "desc": "Med-PRM is a new framework designed to improve clinical decision making by verifying reasoning steps against established medical knowledge bases. It addresses the challenge of error localization in large language models, which is crucial for accurate medical diagnoses. By using retrieval-augmented generation, Med-PRM can assess the quality of reasoning in a detailed manner, leading to enhanced accuracy in medical question answering tasks. The framework has shown significant performance improvements, achieving state-of-the-art results on multiple benchmarks and demonstrating its versatility with various policy models."
                },
                "zh": {
                    "title": "Med-PRMï¼šæå‡åŒ»ç–—å†³ç­–çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "Med-PRMæ˜¯ä¸€ç§è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹åŒ»ç–—çŸ¥è¯†åº“çš„éªŒè¯æ¥å¢å¼ºä¸´åºŠå†³ç­–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œé€æ­¥éªŒè¯æ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥ï¼Œç¡®ä¿æ¨ç†çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹ä¸­é—´æ¨ç†æ­¥éª¤è¿›è¡ŒéªŒè¯ï¼ŒMed-PRMèƒ½å¤Ÿç»†è‡´åœ°è¯„ä¼°æ¨ç†è´¨é‡ï¼Œä»è€Œæé«˜åŒ»ç–—é—®ç­”åŸºå‡†çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMed-PRMåœ¨å¤šä¸ªåŒ»ç–—QAåŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11274",
            "title": "Learning a Continue-Thinking Token for Enhanced Test-Time Scaling",
            "url": "https://huggingface.co/papers/2506.11274",
            "abstract": "A continuous thinking token learned via reinforcement learning improves language model accuracy more effectively than a fixed token during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing \"</think>\" with \"Wait\") can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment a distilled version of DeepSeek-R1 with a single learned \"<|continue-thinking|>\" token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., \"Wait\") for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model's accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing.",
            "score": 3,
            "issue_id": 4313,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "770628ec96646ceb",
            "authors": [
                "Liran Ringel",
                "Elad Tolochinsky",
                "Yaniv Romano"
            ],
            "affiliations": [
                "Department of Computer Science, Technion Israel Institute of Technology",
                "Department of Electrical and Computer Engineering, Technion Israel Institute of Technology",
                "Independent Researcher"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11274.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#math",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GSM8K Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 4.2% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."
                },
                "en": {
                    "title": "Learned Thinking Token Boosts Language Model Accuracy!",
                    "desc": "This paper presents a novel approach to enhance language model performance by introducing a learned continuous thinking token through reinforcement learning. Unlike fixed tokens that merely extend reasoning time, the learned token adapts dynamically to improve accuracy during inference. The authors demonstrate that this method outperforms both the baseline model and traditional fixed-token strategies on standard math benchmarks. Notably, their approach achieves a significant accuracy boost, particularly on the GSM8K benchmark, showcasing the effectiveness of learning over static methods."
                },
                "zh": {
                    "title": "å­¦ä¹ è¿ç»­æ€è€ƒæ ‡è®°ï¼Œæå‡è¯­è¨€æ¨¡å‹å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ å­¦ä¹ çš„è¿ç»­æ€è€ƒæ ‡è®°ï¼Œå¦‚ä½•åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯”å›ºå®šæ ‡è®°æ›´æœ‰æ•ˆåœ°æé«˜è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ä¸“é—¨çš„è¿ç»­æ€è€ƒæ ‡è®°å¯ä»¥è§¦å‘æ›´é•¿çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨DeepSeek-R1çš„ç²¾ç®€ç‰ˆæœ¬ä¸­åŠ å…¥äº†ä¸€ä¸ªå­¦ä¹ åˆ°çš„\"<|continue-thinking|>\"æ ‡è®°ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå…¶åµŒå…¥ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æƒé‡ä¸å˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå­¦ä¹ åˆ°çš„æ ‡è®°åœ¨æ ‡å‡†æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸æ¯”äºåŸºçº¿æ¨¡å‹å’Œä½¿ç”¨å›ºå®šæ ‡è®°çš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08592",
            "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity\n  Dilemma of Embeddings",
            "url": "https://huggingface.co/papers/2506.08592",
            "abstract": "A new dataset named CapRetrieval is introduced to evaluate the ability of text encoders to recognize fine-grained entities and events, highlighting challenges in dense retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.",
            "score": 3,
            "issue_id": 4307,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "64e56d52fd4bf03d",
            "authors": [
                "Liyan Xu",
                "Zhenlin Su",
                "Mo Yu",
                "Jiangnan Li",
                "Fandong Meng",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08592.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#training",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑÑ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CapRetrieval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ»ĞºĞ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° CapRetrieval. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° 'Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼Ñ‹ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸' - ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ² ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Fine-Grained Entity Recognition in Text Encoders",
                    "desc": "This paper introduces a new dataset called CapRetrieval, designed to test how well text encoders can identify detailed entities and events in text. The authors highlight a common problem where these encoders struggle with fine-grained retrieval, even in straightforward scenarios. Through zero-shot evaluation, they demonstrate that existing models often fail to match fine details, regardless of their size or training data. To improve performance, they propose data generation strategies for fine-tuning encoders, addressing the challenge of balancing detailed recognition with overall semantic understanding."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬ç¼–ç å™¨çš„ç»†ç²’åº¦è¯†åˆ«èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†CapRetrievalï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬ç¼–ç å™¨è¯†åˆ«ç»†ç²’åº¦å®ä½“å’Œäº‹ä»¶çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„æ–‡æœ¬ç¼–ç å™¨åœ¨å¯†é›†æ£€ç´¢ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆè¯†åˆ«è¯­ä¹‰ä¸­çš„ç»†ç²’åº¦ä¿¡æ¯ã€‚é€šè¿‡é›¶æ ·æœ¬è¯„ä¼°ï¼Œå‘ç°æ— è®ºæ¨¡å‹å¤§å°æˆ–è®­ç»ƒæ¥æºï¼Œç¼–ç å™¨åœ¨ç»†ç²’åº¦åŒ¹é…ä¸Šéƒ½å¯èƒ½å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ•°æ®ç”Ÿæˆç­–ç•¥æ¥å¾®è°ƒç¼–ç å™¨ï¼Œä»è€Œåœ¨CapRetrievalä¸Šè·å¾—æœ€ä½³æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08477",
            "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT\n  Reasoning",
            "url": "https://huggingface.co/papers/2506.08477",
            "abstract": "U-CoT+ is a novel framework for detecting harmful memes by converting them into textual descriptions and using human-crafted guidelines with zero-shot CoT prompting to achieve high flexibility and explainability with small-scale LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, a novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resource-efficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. Codes and data are available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.",
            "score": 3,
            "issue_id": 4306,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "4b240f248019e671",
            "authors": [
                "Fengjun Pan",
                "Anh Tuan Luu",
                "Xiaobao Wu"
            ],
            "affiliations": [
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08477.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#low_resource",
                    "#ethics",
                    "#interpretability",
                    "#small_models",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ğ² Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ¾Ğ²",
                    "desc": "U-CoT+ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¼Ğ¾Ğ² Ğ¾Ñ‚ Ğ¸Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming Memes into Text for Smarter Detection",
                    "desc": "U-CoT+ is a new framework designed to detect harmful memes by transforming them into textual descriptions. This approach uses a meme-to-text pipeline that preserves details, allowing for better interpretation without needing complex visual analysis. By applying human-crafted guidelines with zero-shot Chain of Thought (CoT) prompting, the framework enhances flexibility and explainability in the detection process. The effectiveness of U-CoT+ is demonstrated through extensive experiments on various benchmark datasets, showcasing its potential for efficient and interpretable meme moderation using small-scale large language models (LLMs)."
                },
                "zh": {
                    "title": "U-CoT+: é«˜æ•ˆå¯è§£é‡Šçš„æœ‰å®³è¿·å› æ£€æµ‹æ¡†æ¶",
                    "desc": "U-CoT+æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹æœ‰å®³çš„ç½‘ç»œè¿·å› ã€‚å®ƒé€šè¿‡å°†è¿·å› è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œå¹¶ä½¿ç”¨äººç±»è®¾è®¡çš„æŒ‡å¯¼åŸåˆ™ï¼Œç»“åˆé›¶-shoté“¾å¼æ¨ç†ï¼Œæ¥å®ç°é«˜çµæ´»æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶é¿å…äº†å¯¹å¤æ‚è§†è§‰å†…å®¹çš„ç›´æ¥æ¨ç†ï¼Œä»è€Œæé«˜äº†èµ„æºæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒU-CoT+åœ¨å°è§„æ¨¡å¤§è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æœ‰æ•ˆçš„æœ‰å®³è¿·å› æ£€æµ‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11136",
            "title": "JAFAR: Jack up Any Feature at Any Resolution",
            "url": "https://huggingface.co/papers/2506.11136",
            "abstract": "JAFAR is a lightweight feature upsampler using an attention-based module with Spatial Feature Transform modulation, enabling high-resolution features from Foundation Vision Encoders without high-resolution supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io",
            "score": 2,
            "issue_id": 4311,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "ba6fb6c9e607162e",
            "authors": [
                "Paul Couairon",
                "Loick Chambon",
                "Louis Serrano",
                "Jean-Emmanuel Haugeard",
                "Matthieu Cord",
                "Nicolas Thome"
            ],
            "affiliations": [
                "Sorbonne UniversitÃ©, CNRS, ISIR, F-75005 Paris, France",
                "Thales, TSGF, cortAIx Labs, France",
                "Valeo.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11136.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "JAFAR: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "JAFAR - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Spatial Feature Transform. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. JAFAR ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñ‹."
                },
                "en": {
                    "title": "JAFAR: Elevating Vision Features with Attention and Modulation",
                    "desc": "JAFAR is a novel feature upsampler that enhances the spatial resolution of visual features from Foundation Vision Encoders without requiring high-resolution supervision. It utilizes an attention-based module with Spatial Feature Transform modulation to align high-resolution queries with low-resolution keys, improving semantic coherence. The model is lightweight and flexible, allowing it to upscale features to any desired resolution effectively. Experimental results indicate that JAFAR excels in recovering fine details and outperforms existing methods in various dense vision tasks."
                },
                "zh": {
                    "title": "JAFARï¼šè½»é‡çº§ç‰¹å¾ä¸Šé‡‡æ ·çš„æ–°é€‰æ‹©",
                    "desc": "JAFARæ˜¯ä¸€ç§è½»é‡çº§çš„ç‰¹å¾ä¸Šé‡‡æ ·å™¨ï¼Œåˆ©ç”¨åŸºäºæ³¨æ„åŠ›çš„æ¨¡å—å’Œç©ºé—´ç‰¹å¾å˜æ¢è°ƒåˆ¶ï¼Œèƒ½å¤Ÿä»åŸºç¡€è§†è§‰ç¼–ç å™¨ä¸­ç”Ÿæˆé«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼Œè€Œæ— éœ€é«˜åˆ†è¾¨ç‡çš„ç›‘ç£ã€‚è¯¥æ–¹æ³•é€šè¿‡å¢å¼ºä½çº§å›¾åƒç‰¹å¾ä¸è¯­ä¹‰ä¸°å¯Œçš„ä½åˆ†è¾¨ç‡é”®ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œæ¥æå‡è§†è§‰ç‰¹å¾çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚å°½ç®¡æ²¡æœ‰é«˜åˆ†è¾¨ç‡çš„ç›‘ç£ï¼ŒJAFARåœ¨ä½ä¸Šé‡‡æ ·æ¯”ç‡å’Œåˆ†è¾¨ç‡ä¸‹çš„å­¦ä¹ è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°æ¨å¹¿åˆ°æ›´é«˜çš„è¾“å‡ºå°ºåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJAFARåœ¨æ¢å¤ç»†ç²’åº¦ç©ºé—´ç»†èŠ‚æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„ç‰¹å¾ä¸Šé‡‡æ ·æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11702",
            "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
            "url": "https://huggingface.co/papers/2506.11702",
            "abstract": "Configurable Preference Tuning enables language models to dynamically adjust their behavior based on human-interprettable directives, using rubric-guided preference data for fine-tuning and inference-time modulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning",
            "score": 1,
            "issue_id": 4305,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "7a7eb1af4ef17eef",
            "authors": [
                "VÃ­ctor Gallego"
            ],
            "affiliations": [
                "Komorebi AI Technologies, Madrid, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11702.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#synthetic",
                    "#training",
                    "#dataset",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Configurable Preference Tuning (CPT) Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. CPT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¸Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ñ…, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰Ğ¸Ñ… Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½ÑĞ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Dynamic Adaptation of Language Models with Configurable Preference Tuning",
                    "desc": "This paper introduces Configurable Preference Tuning (CPT), a new method that allows language models to adapt their responses based on clear, human-understandable instructions. Unlike traditional models that rely on a fixed set of preferences, CPT uses dynamically generated preference data to fine-tune the model's behavior. By employing structured rubrics that specify desired traits, the model can adjust its outputs in real-time without needing to be retrained. This innovation enhances the model's ability to respond to complex and varied human feedback, making it more flexible and context-aware."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´è¯­è¨€æ¨¡å‹è¡Œä¸ºçš„å¯é…ç½®åå¥½è°ƒä¼˜",
                    "desc": "å¯é…ç½®åå¥½è°ƒä¼˜ï¼ˆCPTï¼‰æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ ¹æ®äººç±»å¯ç†è§£çš„æŒ‡ä»¤åŠ¨æ€è°ƒæ•´å…¶è¡Œä¸ºã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ä¸åŒï¼ŒCPTå…è®¸æ¨¡å‹ä½¿ç”¨åˆæˆç”Ÿæˆçš„åå¥½æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»è€Œåœ¨æ¨ç†æ—¶æ ¹æ®ç³»ç»Ÿæç¤ºè°ƒèŠ‚è¾“å‡ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå“åº”ä¸åŒçš„ä¸Šä¸‹æ–‡å’Œéœ€æ±‚ã€‚è¯¥æ–¹æ³•ä¸ä»…æä¾›äº†æ›´ç»†è‡´çš„æ§åˆ¶ï¼Œè¿˜èƒ½æ›´å¥½åœ°æ¨¡æ‹Ÿå¤æ‚å’Œä¾èµ–ä¸Šä¸‹æ–‡çš„äººç±»åé¦ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10082",
            "title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware\n  LoRA Fine-Tuning",
            "url": "https://huggingface.co/papers/2506.10082",
            "abstract": "A mask-based LoRA tuning method for video editing adapts pretrained Image-to-Video models for flexible and high-quality video editing, using spatial masks and reference images for context-specific adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods.",
            "score": 1,
            "issue_id": 4313,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "14126cd5898da5a7",
            "authors": [
                "Chenjian Gao",
                "Lihe Ding",
                "Xin Cai",
                "Zhanpeng Huang",
                "Zibin Wang",
                "Tianfan Xue"
            ],
            "affiliations": [
                "SenseTime Research",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10082.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#diffusion",
                    "#video",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞ¾Ğº Ğ¸ LoRA",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LoRA Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Image-to-Video. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑÑ‹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ ÑÑ†ĞµĞ½Ñ‹. ĞœĞ°ÑĞºĞ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Flexible Video Editing with Mask-Based LoRA Tuning",
                    "desc": "This paper presents a novel mask-based Low-Rank Adaptation (LoRA) tuning method for enhancing video editing capabilities using pretrained Image-to-Video models. The approach allows for flexible and high-quality edits by utilizing spatial masks and reference images, which provide context-specific guidance during the editing process. By preserving background regions and enabling controlled propagation of edits, the method improves upon existing techniques that often lack adaptability for subsequent frames. Experimental results demonstrate that this method outperforms current state-of-the-art video editing approaches, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "çµæ´»é«˜æ•ˆçš„è§†é¢‘ç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ©ç çš„LoRAè°ƒä¼˜æ–¹æ³•ï¼Œç”¨äºè§†é¢‘ç¼–è¾‘ï¼Œæ—¨åœ¨çµæ´»åœ°é€‚åº”é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡ç©ºé—´æ©ç å’Œå‚è€ƒå›¾åƒè¿›è¡Œä¸Šä¸‹æ–‡ç‰¹å®šçš„è°ƒæ•´ï¼Œä¿æŒèƒŒæ™¯åŒºåŸŸçš„åŒæ—¶å®ç°å¯æ§çš„ç¼–è¾‘ä¼ æ’­ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹ï¼Œæä¾›äº†é«˜æ•ˆä¸”é€‚åº”æ€§å¼ºçš„è§†é¢‘ç¼–è¾‘èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ç¼–è¾‘æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10056",
            "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for\n  Throughput",
            "url": "https://huggingface.co/papers/2506.10056",
            "abstract": "The standard paradigm for solving coding tasks via large language models (LLMs) is to generate-then-rank programs, where the latter step uses a verifier in the ranking process. The growing consensus is that a comprehensive verifier (e.g., a full test suite) should be prioritized over an outcome reward model (ORM) whenever possible, with little consideration given to the trade-offs involved. We aim to challenge this assumption by systematically exploring the tradeoff between speed and accuracy. We find that ORMs play a crucial role in scaling verification through trading accuracy for speed, even when a comprehensive verifier is available. Their value becomes especially apparent when used in a generate-prune-then-rank approach, where a faster but less accurate verifier removes incorrect solutions prior to ranking -- leading to a system that is 11.65x faster while only being 8.33% less accurate than the full test suite. We analyze the generate-prune-then-rank approach and show that it works by filtering out incorrect but highly ranked solutions. These findings enable the design of scalable and accurate program ranking systems.",
            "score": 1,
            "issue_id": 4318,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "1ee14b5fd87f1f86",
            "authors": [
                "Gabriel Orlanski",
                "Nicholas Roberts",
                "Aws Albarghouthi",
                "Frederic Sala"
            ],
            "affiliations": [
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10056.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾ÑĞ¿Ğ°Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¾Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ°Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (ORM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ORM Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ-Ğ¾Ñ‚ÑĞµĞ²-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ' Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ORM Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ² 11.65 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 8.33% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Balancing Speed and Accuracy in Program Verification with ORMs",
                    "desc": "This paper investigates the effectiveness of using outcome reward models (ORMs) in the coding task solution process with large language models (LLMs). It challenges the common belief that comprehensive verifiers should always be prioritized, highlighting the trade-off between speed and accuracy. The authors demonstrate that ORMs can significantly enhance the efficiency of program verification by allowing a faster, less accurate verifier to filter out incorrect solutions before ranking. Their findings suggest that a generate-prune-then-rank strategy can achieve a balance, resulting in a system that is much faster while maintaining acceptable accuracy levels."
                },
                "zh": {
                    "title": "é€Ÿåº¦ä¸å‡†ç¡®æ€§çš„æƒè¡¡ï¼šä¼˜åŒ–ç¼–ç ä»»åŠ¡çš„éªŒè¯æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³ç¼–ç ä»»åŠ¡çš„æ ‡å‡†æ–¹æ³•ï¼Œå³ç”Ÿæˆ-ç„¶å-æ’åçš„ç¨‹åºã€‚ç ”ç©¶è¡¨æ˜ï¼Œå…¨é¢çš„éªŒè¯å™¨ï¼ˆå¦‚å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼‰é€šå¸¸è¢«è®¤ä¸ºä¼˜äºç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰ï¼Œä½†ä½œè€…æŒ‘æˆ˜äº†è¿™ä¸€å‡è®¾ã€‚é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶é€Ÿåº¦ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå‘ç°ORMåœ¨æé«˜éªŒè¯é€Ÿåº¦æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œå³ä½¿åœ¨æœ‰å…¨é¢éªŒè¯å™¨çš„æƒ…å†µä¸‹ã€‚æœ€ç»ˆï¼Œç”Ÿæˆ-ä¿®å‰ª-ç„¶å-æ’åçš„æ–¹æ³•ä½¿å¾—ç³»ç»Ÿé€Ÿåº¦æé«˜äº†11.65å€ï¼Œå‡†ç¡®æ€§ä»…ä¸‹é™äº†8.33%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11130",
            "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data",
            "url": "https://huggingface.co/papers/2506.11130",
            "abstract": "A self-refining framework enhances ASR performance using unlabeled datasets by integrating pseudo-labeling, TTS, and synthesized speech to create a specialized model.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The process starts with an existing ASR model generating pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs are bootstrapped into the original ASR system, completing the closed-loop self-improvement cycle. We demonstrated the effectiveness of the framework on Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a moderate amount of text data, and synthetic content from the AI models, we adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper. Results highlight the framework as a compelling alternative to pseudo-labeling self-distillation approaches and provides a practical pathway for improving ASR performance in low-resource or domain-specific settings.",
            "score": 1,
            "issue_id": 4308,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "743ff411fbd34247",
            "authors": [
                "Cheng Kang Chou",
                "Chan-Jan Hsu",
                "Ho-Lam Chung",
                "Liang-Hsuan Tseng",
                "Hsi-Chun Cheng",
                "Yu-Kuan Fu",
                "Kuan Po Huang",
                "Hung-Yi Lee"
            ],
            "affiliations": [
                "MediaTek Research",
                "National Taiwan University",
                "Nvidia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11130.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#transfer_learning",
                    "#low_resource",
                    "#audio",
                    "#synthetic"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ĞĞ¡Ğ : Ğ¾Ñ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ ĞĞ¡Ğ , ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ñ€ĞµÑ‡ÑŒ-Ñ‚ĞµĞºÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞĞ¡Ğ , Ğ·Ğ°Ğ¼Ñ‹ĞºĞ°Ñ Ñ†Ğ¸ĞºĞ» ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ‚Ğ°Ğ¹Ğ²Ğ°Ğ½ÑŒÑĞºĞ¾Ğ¼ Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğµ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Twister, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ· Whisper-large-v2, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."
                },
                "en": {
                    "title": "Enhancing ASR with Unlabeled Data: The Twister Framework",
                    "desc": "This paper presents a self-refining framework designed to improve Automatic Speech Recognition (ASR) performance using only unlabeled datasets. The framework begins with an existing ASR model that generates pseudo-labels from unannotated speech data, which are then utilized to train a high-fidelity Text-to-Speech (TTS) system. Synthesized speech and text pairs are incorporated back into the original ASR model, creating a closed-loop system that enhances its accuracy. The proposed method, tested on Taiwanese Mandarin, shows significant error rate reductions, demonstrating its effectiveness in low-resource environments."
                },
                "zh": {
                    "title": "è‡ªæˆ‘ä¼˜åŒ–æ¡†æ¶æå‡ASRæ€§èƒ½çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨æœªæ ‡æ³¨çš„æ•°æ®é›†æ¥æå‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨ç°æœ‰çš„ASRæ¨¡å‹ä¸ºæœªæ ‡æ³¨çš„è¯­éŸ³ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªé«˜ä¿çœŸçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚æ¥ç€ï¼Œå°†åˆæˆçš„è¯­éŸ³æ–‡æœ¬å¯¹å¼•å…¥åŸå§‹çš„ASRç³»ç»Ÿï¼Œå½¢æˆä¸€ä¸ªé—­ç¯çš„è‡ªæˆ‘æ”¹è¿›å¾ªç¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å°æ¹¾æ™®é€šè¯è¯­éŸ³ä¸Šæœ‰æ•ˆï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½é”™è¯¯ç‡ï¼Œå°¤å…¶åœ¨ä½èµ„æºæˆ–ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ä¸­å…·æœ‰å®é™…æ„ä¹‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08915",
            "title": "Inherently Faithful Attention Maps for Vision Transformers",
            "url": "https://huggingface.co/papers/2506.08915",
            "abstract": "An attention-based method using learned binary masks improves robustness in object perception by focusing on relevant image regions while filtering out spurious information.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds.",
            "score": 1,
            "issue_id": 4309,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "e080cfdc03932dd9",
            "authors": [
                "Ananthu Aniraj",
                "Cassio F. Dantas",
                "Dino Ienco",
                "Diego Marcos"
            ],
            "affiliations": [
                "Inrae",
                "Inria",
                "University of Montpellier"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08915.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#optimization",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ¤Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ¼: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ±Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ°Ğ¿Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ½ĞµÑ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ„Ğ¾Ğ½Ğ°Ğ¼."
                },
                "en": {
                    "title": "Focusing Attention for Robust Object Perception",
                    "desc": "This paper presents an attention-based method that utilizes learned binary masks to enhance object perception in images. By focusing on relevant regions and filtering out irrelevant information, the method improves the robustness of predictions, especially in challenging contexts. The proposed two-stage framework first identifies object parts and task-relevant areas, then restricts analysis to these regions using attention masking. Experimental results show that this approach effectively mitigates the impact of spurious correlations and out-of-distribution backgrounds on object recognition tasks."
                },
                "zh": {
                    "title": "åŸºäºæ³¨æ„åŠ›çš„é²æ£’ç‰©ä½“æ„ŸçŸ¥æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„ç®—æ³•ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„äºŒè¿›åˆ¶æ³¨æ„åŠ›æ©ç æ¥æé«˜ç‰©ä½“æ„ŸçŸ¥çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å…³æ³¨ç›¸å…³çš„å›¾åƒåŒºåŸŸï¼Œè¿‡æ»¤æ‰æ— å…³çš„ä¿¡æ¯ï¼Œä»è€Œç¡®ä¿åªæœ‰è¢«å…³æ³¨çš„åŒºåŸŸå½±å“é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µå¤„ç†å®Œæ•´å›¾åƒä»¥å‘ç°ç‰©ä½“éƒ¨åˆ†å¹¶è¯†åˆ«ä»»åŠ¡ç›¸å…³åŒºåŸŸï¼Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨è¾“å…¥çš„æ³¨æ„åŠ›æ©ç é™åˆ¶æ„Ÿå—é‡ï¼Œä»è€Œè¿›è¡Œæ›´ä¸“æ³¨çš„åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹è™šå‡ç›¸å…³æ€§å’Œåˆ†å¸ƒå¤–èƒŒæ™¯æ–¹é¢æ˜¾è‘—æé«˜äº†é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11116",
            "title": "Infinity Instruct: Scaling Instruction Selection and Synthesis to\n  Enhance Language Models",
            "url": "https://huggingface.co/papers/2506.11116",
            "abstract": "Infinity-Instruct, a comprehensive instruction dataset, enhances both foundational and chat capabilities of large language models through curation and synthesis, achieving superior performance compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our datasethttps://huggingface.co/datasets/BAAI/Infinity-Instruct and codeshttps://gitee.com/li-touch/infinity-instruct have been publicly released.",
            "score": 1,
            "issue_id": 4313,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "4de7c4de7a621e11",
            "authors": [
                "Jijie Li",
                "Li Du",
                "Hanyu Zhao",
                "Bo-wen Zhang",
                "Liangdong Wang",
                "Boyan Gao",
                "Guang Liu",
                "Yonghua Lin"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11116.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#transfer_learning",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Infinity-Instruct: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Infinity-Instruct - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹: 7,4 Ğ¼Ğ»Ğ½ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ 1,5 Ğ¼Ğ»Ğ½ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Infinity-Instruct, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, InfInstruct-LLaMA3.1-70B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4-0314 Ğ½Ğ° 8,6% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Infinity-Instruct",
                    "desc": "Infinity-Instruct is a new instruction dataset that improves the performance of large language models (LLMs) in both foundational and chat tasks. It consists of 7.4 million curated foundational instructions and 1.5 million synthesized chat instructions, created through advanced data selection and filtering techniques. The dataset has been tested on various open-source models, showing significant performance improvements over existing instruction datasets. This research highlights the importance of combining foundational and chat training to enhance the overall capabilities of LLMs."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤èƒ½åŠ›",
                    "desc": "Infinity-Instructæ˜¯ä¸€ä¸ªå…¨é¢çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›å’Œå¯¹è¯èƒ½åŠ›ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„å¤„ç†æµç¨‹ï¼Œæˆ‘ä»¬ä»è¶…è¿‡1äº¿ä¸ªæ ·æœ¬ä¸­ç­›é€‰å‡º740ä¸‡æ¡é«˜è´¨é‡çš„åŸºç¡€æŒ‡ä»¤ï¼Œå¹¶åˆæˆ150ä¸‡æ¡é«˜è´¨é‡çš„å¯¹è¯æŒ‡ä»¤ã€‚ç»è¿‡å®è¯è¯„ä¼°ï¼Œä½¿ç”¨Infinity-Instructå¾®è°ƒçš„å¤šä¸ªå¼€æºæ¨¡å‹åœ¨åŸºç¡€å’ŒæŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºç¡€è®­ç»ƒå’Œå¯¹è¯è®­ç»ƒä¹‹é—´çš„ååŒä½œç”¨å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03857",
            "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for\n  LLM-driven Data Annotation",
            "url": "https://huggingface.co/papers/2506.03857",
            "abstract": "A novel candidate annotation paradigm using a teacher-student framework improves data quality forä¸‹æ¸¸ applications by encouraging large language models to output multiple labels when uncertain.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine a single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose a novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop a teacher-student framework CanDist that distills candidate annotations with a Small Language Model (SLM). We further provide a rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CanDist.",
            "score": 1,
            "issue_id": 4314,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "9efc9f12a990b701",
            "authors": [
                "Mingxuan Xia",
                "Haobo Wang",
                "Yixuan Li",
                "Zewei Yu",
                "Jindong Wang",
                "Junbo Zhao",
                "Runze Wu"
            ],
            "affiliations": [
                "NetEase Fuxi AI Lab",
                "University of Wisconsin Madison",
                "William & Mary",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03857.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "ğŸ·ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ LLM Ğ²Ñ‹Ğ´Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ğ»ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (SLM). Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering Uncertainty: Multi-Label Annotation for Better Data Quality",
                    "desc": "This paper introduces a new approach to data annotation using a teacher-student framework that enhances the quality of labels produced by large language models (LLMs). Instead of forcing LLMs to choose a single label, the method encourages them to generate multiple potential labels when they are uncertain. This strategy helps to mitigate the risk of incorrect labeling, which can degrade the quality of data for subsequent applications. The proposed framework, called CanDist, uses a Small Language Model (SLM) to refine these candidate annotations, leading to better performance in text classification tasks."
                },
                "zh": {
                    "title": "åˆ©ç”¨æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶æå‡æ•°æ®æ ‡æ³¨è´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å€™é€‰æ ‡æ³¨èŒƒå¼ï¼Œåˆ©ç”¨æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶æ¥æé«˜æ•°æ®è´¨é‡ã€‚è¯¥æ–¹æ³•é¼“åŠ±å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶è¾“å‡ºå¤šä¸ªæ ‡ç­¾ï¼Œä»è€Œå‡å°‘é”™è¯¯æ ‡æ³¨çš„é£é™©ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚æ ·æœ¬ï¼Œç¡®ä¿ä¸‹æ¸¸åº”ç”¨çš„æ•°æ®è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…­ä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10387",
            "title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal\n  Skills",
            "url": "https://huggingface.co/papers/2506.10387",
            "abstract": "Hierarchical Multimodal Skills and Skill-Augmented Monte Carlo Tree Search improve multimodal GUI agent performance in long-horizon tasks by abstracting knowledge and bridging the offline-online domain gap.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI agents have yielded promising outcomes. However, these agents still struggle with long-horizon tasks in online environments, primarily due to insufficient knowledge and the inherent gap between offline and online domains. In this paper, inspired by how humans generalize knowledge in open-ended environments, we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of insufficient knowledge. It progressively abstracts trajectories into execution skills, core skills, and ultimately meta-skills, providing a hierarchical knowledge structure for long-horizon task planning. To bridge the domain gap, we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm, which efficiently leverages skills acquired in offline environments to reduce the action search space during online tree exploration. Building on HMS, we propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To validate the performance of Mirage-1 in real-world long-horizon scenarios, we constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1 outperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld, MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page: https://cybertronagent.github.io/Mirage-1.github.io/",
            "score": 0,
            "issue_id": 4315,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "9c47a3d6d929dc16",
            "authors": [
                "Yuquan Xie",
                "Zaijing Li",
                "Rui Shao",
                "Gongwei Chen",
                "Kaiwen Zhou",
                "Yinchuan Li",
                "Dongmei Jiang",
                "Liqiang Nie"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Shenzhen",
                "Huawei Noahs Ark Lab",
                "Peng Cheng Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10387.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#agents",
                    "#benchmark",
                    "#long_context",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¸ ĞœĞšĞ¢ĞŸ: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞĞ°Ğ²Ñ‹ĞºĞ¾Ğ² (HMS) Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ”ĞµÑ€ĞµĞ²Ğ° ĞŸĞ¾Ğ¸ÑĞºĞ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ñ Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞĞ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ (SA-MCTS) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ HMS Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ°Ğ³ĞµĞ½Ñ‚ Mirage-1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AndroidLH."
                },
                "en": {
                    "title": "Empowering GUI Agents with Hierarchical Skills for Long-Horizon Success",
                    "desc": "This paper introduces a new approach to improve the performance of multimodal GUI agents in long-horizon tasks by using Hierarchical Multimodal Skills (HMS) and Skill-Augmented Monte Carlo Tree Search (SA-MCTS). HMS organizes knowledge into a hierarchy of execution skills, core skills, and meta-skills, allowing agents to better generalize and plan tasks. SA-MCTS enhances the search process by utilizing skills learned in offline settings to streamline decision-making in online environments. The proposed agent, Mirage-1, demonstrates significant performance improvements over existing agents in various benchmarks, showcasing its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "å±‚æ¬¡åŒ–æŠ€èƒ½ä¸å¢å¼ºæœç´¢æå‡GUIä»£ç†è¡¨ç°",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–å¤šæ¨¡æ€æŠ€èƒ½æ¨¡å—ï¼ˆHMSï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„çŸ¥è¯†ä¸è¶³é—®é¢˜ã€‚HMSé€šè¿‡å°†è½¨è¿¹é€æ­¥æŠ½è±¡ä¸ºæ‰§è¡ŒæŠ€èƒ½ã€æ ¸å¿ƒæŠ€èƒ½å’Œå…ƒæŠ€èƒ½ï¼Œæ„å»ºäº†ä¸€ä¸ªå±‚æ¬¡åŒ–çš„çŸ¥è¯†ç»“æ„ï¼Œä»¥æ”¯æŒé•¿æ—¶é—´ä»»åŠ¡çš„è§„åˆ’ã€‚ä¸ºäº†å¼¥è¡¥ç¦»çº¿å’Œåœ¨çº¿é¢†åŸŸä¹‹é—´çš„å·®è·ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†æŠ€èƒ½å¢å¼ºçš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•ï¼ˆSA-MCTSï¼‰ï¼Œè¯¥ç®—æ³•æœ‰æ•ˆåˆ©ç”¨ç¦»çº¿ç¯å¢ƒä¸­è·å¾—çš„æŠ€èƒ½ï¼Œå‡å°‘åœ¨çº¿æ ‘æœç´¢ä¸­çš„åŠ¨ä½œæœç´¢ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMirage-1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„ä»£ç†ï¼Œæå‡äº†æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-13.html",
    "link_next": "2025-06-17.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "13.06",
        "en": "06/13",
        "zh": "6æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "17.06",
        "en": "06/17",
        "zh": "6æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 10,
        "#data": 4,
        "#benchmark": 14,
        "#agents": 2,
        "#cv": 5,
        "#rl": 5,
        "#rlhf": 4,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 12,
        "#robotics": 1,
        "#agi": 0,
        "#games": 5,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 5,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 12,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 3,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 2
    }
}