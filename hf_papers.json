{
    "date": {
        "ru": "17 июля",
        "en": "July 17",
        "zh": "7月17日"
    },
    "time_utc": "2025-07-17 06:18",
    "weekday": 3,
    "issue_id": 4864,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.12465",
            "title": "PhysX: Physical-Grounded 3D Asset Generation",
            "url": "https://huggingface.co/papers/2507.12465",
            "abstract": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose PhysX, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose PhysXGen, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.",
            "score": 16,
            "issue_id": 4861,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 июля",
                "en": "July 16",
                "zh": "7月16日"
            },
            "hash": "ece62f7e4ecd0487",
            "authors": [
                "Ziang Cao",
                "Zhaoxi Chen",
                "Linag Pan",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12465.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#games",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🧱",
                "ru": {
                    "title": "Физически достоверная генерация 3D-объектов",
                    "desc": "PhysX представляет собой новый подход к генерации 3D-объектов с учетом их физических свойств. Авторы создали датасет PhysXNet с аннотациями физических характеристик 3D-моделей и разработали фреймворк PhysXGen для интеграции физических знаний в процесс генерации. PhysXGen использует двухветвевую архитектуру для моделирования связей между 3D-структурами и физическими свойствами. Эксперименты показали превосходную производительность и способность к обобщению предложенного метода."
                },
                "en": {
                    "title": "Bridging 3D Generation with Real-World Physics",
                    "desc": "PhysX introduces a new approach to 3D asset generation by incorporating physical properties into the modeling process. It presents PhysXNet, a unique dataset that annotates 3D models with essential physical attributes like scale, material, and function. Additionally, PhysXGen is a framework that uses this dataset to generate 3D assets that not only look good but also behave realistically in physical simulations. This work aims to enhance the applicability of AI-generated 3D models in real-world scenarios, such as robotics and virtual simulations."
                },
                "zh": {
                    "title": "物理驱动的3D资产生成新方法",
                    "desc": "PhysX提出了一种新的方法来解决3D生成模型中缺乏物理属性的问题。它引入了PhysXNet，这是一个物理注释的数据集，系统地标注了五个基础维度，包括绝对尺度、材料、可用性、运动学和功能描述。通过PhysXGen框架，物理知识被整合到3D资产生成中，利用双分支架构建模3D结构与物理属性之间的潜在关联。实验结果表明，该框架在生成具有可信物理预测的3D资产方面表现优越，具有良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09477",
            "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
            "url": "https://huggingface.co/papers/2507.09477",
            "abstract": "This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
            "score": 9,
            "issue_id": 4862,
            "pub_date": "2025-07-13",
            "pub_date_card": {
                "ru": "13 июля",
                "en": "July 13",
                "zh": "7月13日"
            },
            "hash": "da4aa711048f0a7f",
            "authors": [
                "Yangning Li",
                "Weizhi Zhang",
                "Yuyao Yang",
                "Wei-Chieh Huang",
                "Yaozu Wu",
                "Junyu Luo",
                "Yuanchen Bei",
                "Henry Peng Zou",
                "Xiao Luo",
                "Yusheng Zhao",
                "Chunkit Chan",
                "Yankai Chen",
                "Zhongfen Deng",
                "Yinghui Li",
                "Hai-Tao Zheng",
                "Dongyuan Li",
                "Renhe Jiang",
                "Ming Zhang",
                "Yangqiu Song",
                "Philip S. Yu"
            ],
            "affiliations": [
                "HKUST",
                "Peking University",
                "The University of Tokyo",
                "Tsinghua University",
                "University of California, Los Angeles",
                "University of Illinois Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09477.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#survey",
                    "#rag"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Объединение извлечения информации и рассуждений для создания более мощных языковых моделей",
                    "desc": "Это обзор интегрирует рассуждения и извлечение информации в больших языковых моделях (LLM) для улучшения фактической точности и многоступенчатого вывода. В статье рассматриваются подходы, сочетающие Retrieval-Augmented Generation (RAG) и методы рассуждений, что позволяет преодолеть ограничения каждого из них по отдельности. Авторы выделяют синергетические фреймворки RAG-Reasoning, демонстрирующие передовые результаты на задачах, требующих обширных знаний. Обзор также очерчивает направления будущих исследований в этой области."
                },
                "en": {
                    "title": "Enhancing LLMs with Synergized Retrieval and Reasoning",
                    "desc": "This paper discusses how to improve the accuracy and reasoning abilities of Large Language Models (LLMs) by combining retrieval and reasoning techniques. It introduces the concept of Retrieval-Augmented Generation (RAG), which enhances LLMs by providing them with external knowledge, but notes that RAG struggles with complex, multi-step reasoning tasks. The authors propose a unified framework that integrates advanced reasoning into RAG, allowing LLMs to better utilize retrieved information for deeper inference. They also highlight future research directions to create more effective and trustworthy systems that can adapt to various types of knowledge and user needs."
                },
                "zh": {
                    "title": "推理与检索的协同提升大型语言模型的能力",
                    "desc": "这篇论文调查了大型语言模型中的推理与检索的结合，以提高事实准确性和多步推理能力。检索增强生成（RAG）通过引入外部知识来提升大型语言模型的事实性，但在需要多步推理的问题上表现不足。论文提出了统一的推理-检索视角，展示了如何通过先进的推理优化RAG的每个阶段，并强调了新兴的协同RAG-推理框架。最后，论文分类了方法、数据集和开放挑战，并概述了未来研究方向，以构建更有效、适应多模态、可信赖和以人为本的RAG-推理系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12463",
            "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
            "url": "https://huggingface.co/papers/2507.12463",
            "abstract": "A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behaviorx2014such as motion, trajectories, and intentionx2014a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose MMHU, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasksx2014ranging from motion prediction to motion generation and human behavior question answeringx2014thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io.",
            "score": 8,
            "issue_id": 4864,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 июля",
                "en": "July 16",
                "zh": "7月16日"
            },
            "hash": "c5061caaead150e7",
            "authors": [
                "Renjie Li",
                "Ruijie Ye",
                "Mingyang Wu",
                "Hao Frank Yang",
                "Zhiwen Fan",
                "Hezhen Hu",
                "Zhengzhong Tu"
            ],
            "affiliations": [
                "Brown University",
                "Johns Hopkins University",
                "Texas A&M University",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12463.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "MMHU: Комплексный анализ поведения человека для безопасного автономного вождения",
                    "desc": "Представлен крупномасштабный бенчмарк MMHU для анализа поведения человека в контексте автономного вождения. Он включает богатые аннотации и разнообразные источники данных, охватывающие 57 тысяч клипов с движениями людей и 1,73 миллиона кадров. MMHU позволяет оценивать различные задачи, включая прогнозирование движения и ответы на вопросы о поведении. Бенчмарк предоставляет комплексный набор для оценки понимания человеческого поведения в системах автономного вождения."
                },
                "en": {
                    "title": "MMHU: Advancing Human Behavior Analysis for Safer Autonomous Driving",
                    "desc": "The paper introduces MMHU, a comprehensive benchmark designed for analyzing human behavior in the context of autonomous driving. It includes extensive annotations on human motion, trajectories, intentions, and safety-related behaviors, making it a valuable resource for researchers. The dataset consists of 57,000 motion clips and 1.73 million frames sourced from various platforms, including established driving datasets and real-world videos. By benchmarking multiple tasks such as motion prediction and behavior question answering, MMHU aims to enhance the understanding of human behavior in driving scenarios."
                },
                "zh": {
                    "title": "MMHU：自动驾驶人类行为分析的新基准",
                    "desc": "本文提出了一个名为MMHU的大规模基准，用于分析自动驾驶中的人类行为。该基准包含丰富的注释和多样的数据来源，涵盖了人类运动、轨迹、意图等多个方面。数据集包括57,000个运动片段和173万帧，来源于知名的驾驶数据集和YouTube等平台。我们还开发了一个人机协作的注释流程，以生成详细的行为描述，并对多个任务进行了基准测试，包括运动预测和行为问答。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11527",
            "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
            "url": "https://huggingface.co/papers/2507.11527",
            "abstract": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.",
            "score": 8,
            "issue_id": 4861,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 июля",
                "en": "July 15",
                "zh": "7月15日"
            },
            "hash": "e6f20729b2c748f9",
            "authors": [
                "Yinsheng Li",
                "Zhen Dong",
                "Yi Shao"
            ],
            "affiliations": [
                "Department of Civil Engineering McGill University",
                "NVIDIA",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11527.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#agents"
                ],
                "emoji": "📐",
                "ru": {
                    "title": "DrafterBench: Комплексная оценка LLM-агентов в инженерном проектировании",
                    "desc": "DrafterBench - это открытый бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в задаче проверки технических чертежей. Он оценивает способности агентов в понимании структурированных данных, выполнении функций, следовании инструкциям и критическом мышлении. Бенчмарк содержит 12 типов задач, основанных на реальных чертежах, с 46 специальными функциями и инструментами, всего 1920 заданий. DrafterBench предлагает детальный анализ точности выполнения задач и статистики ошибок, чтобы глубже понять возможности агентов и определить цели для улучшения интеграции LLM в инженерные приложения."
                },
                "en": {
                    "title": "DrafterBench: Evaluating LLMs for Technical Drawing Mastery",
                    "desc": "DrafterBench is an open-source benchmark designed to evaluate Large Language Model (LLM) agents specifically in the area of technical drawing revision. It includes twelve task types derived from real-world drawing files, featuring 46 customized functions and a total of 1920 tasks. The benchmark assesses LLM agents on their abilities in structured data comprehension, function execution, instruction following, and critical reasoning. By providing detailed analysis of task accuracy and error statistics, DrafterBench aims to enhance the understanding of LLM capabilities and identify areas for improvement in engineering applications."
                },
                "zh": {
                    "title": "DrafterBench：评估LLM代理的技术图纸修订能力",
                    "desc": "DrafterBench是一个开源基准，用于评估大型语言模型（LLM）代理在技术图纸修订中的能力。它涵盖了结构化数据理解、功能执行、指令遵循和批判性推理等多个方面。该基准包含来自真实绘图文件的十二种任务，提供了46种定制功能和1920个任务。DrafterBench旨在深入分析代理的能力，帮助识别在工程应用中整合LLM的改进目标。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11949",
            "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
            "url": "https://huggingface.co/papers/2507.11949",
            "abstract": "A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.",
            "score": 7,
            "issue_id": 4862,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 июля",
                "en": "July 16",
                "zh": "7月16日"
            },
            "hash": "60ad5621a3842ae5",
            "authors": [
                "Shuyang Xu",
                "Zhiyang Dou",
                "Mingyi Shi",
                "Liang Pan",
                "Leo Ho",
                "Jingbo Wang",
                "Yuan Liu",
                "Cheng Lin",
                "Yuexin Ma",
                "Wenping Wang",
                "Taku Komura"
            ],
            "affiliations": [
                "Macau University of Science and Technology",
                "Shanghai AI Lab",
                "ShanghaiTech University",
                "Texas A&M University",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11949.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#benchmark",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "Реалистичная анимация движений под пространственное аудио",
                    "desc": "Исследователи представили MOSPA - генеративную модель на основе диффузии для моделирования движений человека в ответ на пространственное аудио. Для обучения модели был создан новый набор данных SAM, содержащий разнообразные пространственные аудио и соответствующие им движения. MOSPA использует эффективный механизм слияния для захвата взаимосвязи между движениями тела и пространственным звуком. Модель достигла лучших результатов в этой задаче по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Bridging Sound and Motion: MOSPA for Realistic Human Animation",
                    "desc": "The paper presents MOSPA, a diffusion-based generative framework designed to model human motion in response to spatial audio. It introduces the SAM dataset, which is the first of its kind, containing high-quality spatial audio and corresponding human motion data. The framework effectively captures the relationship between body movements and spatial audio through a novel fusion mechanism. By training on this dataset, MOSPA can generate diverse and realistic human motions that respond dynamically to different auditory stimuli, achieving state-of-the-art results in this area."
                },
                "zh": {
                    "title": "空间音频驱动的人类运动生成新突破",
                    "desc": "本文介绍了一种基于扩散的生成框架MOSPA，用于建模人类在空间音频刺激下的运动。我们创建了首个综合性的空间音频驱动人类运动（SAM）数据集，包含多样化和高质量的空间音频与运动数据。MOSPA通过有效的融合机制，准确捕捉身体运动与空间音频之间的关系，能够生成多样且真实的人类运动。经过广泛的实验验证，我们的方法在这一任务上达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12415",
            "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
            "url": "https://huggingface.co/papers/2507.12415",
            "abstract": "SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  \t\t\t\t\tAI-generated summary \t\t\t\t Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field.",
            "score": 6,
            "issue_id": 4864,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 июля",
                "en": "July 16",
                "zh": "7月16日"
            },
            "hash": "6be5b41deae78198",
            "authors": [
                "Xinyi He",
                "Qian Liu",
                "Mingzhe Du",
                "Lin Yan",
                "Zhijie Fan",
                "Yiming Huang",
                "Zejian Yuan",
                "Zejun Ma"
            ],
            "affiliations": [
                "National University of Singapore",
                "TikTok",
                "University of California San Diego",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12415.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "SWE-Perf: Новый рубеж в оценке оптимизации кода с помощью LLM",
                    "desc": "SWE-Perf - это бенчмарк для оценки способностей больших языковых моделей (LLM) в оптимизации производительности кода на основе реальных репозиториев. Он содержит 140 тщательно отобранных примеров из популярных GitHub-репозиториев, включающих кодовую базу, целевые функции, тесты производительности и экспертные патчи. Оценка существующих методов показала значительный разрыв между возможностями LLM и экспертным уровнем оптимизации. SWE-Perf открывает новые направления исследований в области применения LLM для повышения производительности кода."
                },
                "en": {
                    "title": "Unlocking Code Efficiency: Evaluating LLMs with SWE-Perf",
                    "desc": "SWE-Perf is a new benchmark designed to evaluate how well Large Language Models (LLMs) can optimize code performance using real-world data from software repositories. It focuses on the important task of improving code efficiency, which is essential for high-quality software systems. The benchmark includes 140 instances based on actual performance-enhancing pull requests from GitHub, providing a realistic testing environment. The study reveals that current LLMs significantly lag behind expert-level optimization, indicating a need for further research in this area."
                },
                "zh": {
                    "title": "SWE-Perf：评估代码性能优化的新基准",
                    "desc": "SWE-Perf是一个基准测试，用于评估大型语言模型在代码性能优化方面的表现。该基准测试使用真实的代码库数据，专注于软件工程中的代码性能优化。虽然大型语言模型在代码生成和错误修复方面表现出色，但它们在提升代码性能方面的能力尚未得到充分探索。通过对140个精心挑选的实例进行评估，SWE-Perf揭示了现有大型语言模型与专家级优化性能之间的显著差距，指出了这一新兴领域中的重要研究机会。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09025",
            "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
            "url": "https://huggingface.co/papers/2507.09025",
            "abstract": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.",
            "score": 2,
            "issue_id": 4861,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 июля",
                "en": "July 11",
                "zh": "7月11日"
            },
            "hash": "3490901c2a32da3d",
            "authors": [
                "Chien Van Nguyen",
                "Ruiyi Zhang",
                "Hanieh Deilamsalehy",
                "Puneet Mathur",
                "Viet Dac Lai",
                "Haoliang Wang",
                "Jayakumar Subramanian",
                "Ryan A. Rossi",
                "Trung Bui",
                "Nikos Vlassis",
                "Franck Dernoncourt",
                "Thien Huu Nguyen"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09025.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🦎",
                "ru": {
                    "title": "Lizard: эффективные языковые модели с бесконечным контекстом",
                    "desc": "Lizard - это фреймворк линеаризации, который преобразует трансформерные языковые модели в субквадратичные архитектуры для эффективной генерации с бесконечным контекстом. Он использует гибридный механизм внимания, сочетающий линейное внимание с гейтингом и оконное внимание с мета-памятью. Lizard позволяет адаптивно управлять памятью, поддерживает вывод с постоянным объемом памяти и обеспечивает сильную обобщаемость по длине. Эксперименты показывают, что Lizard почти без потерь восстанавливает производительность исходной модели на стандартных задачах языкового моделирования."
                },
                "en": {
                    "title": "Lizard: Efficient Infinite-Context Generation for Transformers",
                    "desc": "Lizard is a framework designed to improve the efficiency of Transformer-based Large Language Models (LLMs) by transforming them into subquadratic architectures. It addresses the challenges of memory and computation that arise with longer context lengths by implementing a hybrid attention mechanism that approximates softmax attention while maintaining output quality. The framework incorporates a gating module for adaptive memory control, allowing for constant-memory inference and enhanced model flexibility. Experimental results demonstrate that Lizard not only preserves the performance of traditional models but also significantly enhances their capabilities on various language tasks."
                },
                "zh": {
                    "title": "Lizard：高效无限上下文生成的新框架",
                    "desc": "Lizard是一个线性化框架，旨在将基于Transformer的大型语言模型（LLMs）转变为灵活的亚二次架构，以实现高效的无限上下文生成。该框架通过引入一种亚二次注意力机制，克服了传统softmax注意力在上下文长度增加时的内存和计算瓶颈，同时保持输出质量。Lizard还结合了门控模块，支持自适应内存控制和常量内存推理，增强了模型设计的灵活性。通过混合门控线性注意力和滑动窗口注意力，Lizard能够有效捕捉长距离依赖和细粒度的局部交互，显著提升了模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02857",
            "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
            "url": "https://huggingface.co/papers/2507.02857",
            "abstract": "AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.",
            "score": 2,
            "issue_id": 4864,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "5a06d615e806a525",
            "authors": [
                "Ziye Li",
                "Hao Luo",
                "Xincheng Shuai",
                "Henghui Ding"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba group",
                "Fudan University",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02857.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Универсальная анимация изображений с контролем движения",
                    "desc": "AnyI2V - это фреймворк для анимации условных изображений с пользовательскими траекториями движения без необходимости обучения. Он поддерживает различные типы данных, включая сетки и облака точек, что позволяет создавать более гибкие и универсальные видео. AnyI2V превосходит существующие методы текст-в-видео и изображение-в-видео, предоставляя точный контроль над пространственным расположением и движением генерируемого контента. Фреймворк также поддерживает смешанные условные входные данные и позволяет выполнять перенос стиля и редактирование с помощью LoRA и текстовых подсказок."
                },
                "en": {
                    "title": "AnyI2V: Freedom in Motion-Controlled Video Generation",
                    "desc": "AnyI2V is a novel framework designed for animating images based on user-defined motion paths without the need for extensive training. It addresses the limitations of existing text-to-video and image-to-video methods by allowing for greater control over spatial layouts and dynamic motion signals. This framework supports various data types, including meshes and point clouds, which enhances its versatility in video generation. Through extensive testing, AnyI2V has shown to outperform previous methods, offering a fresh approach to generating videos with precise motion and spatial control."
                },
                "zh": {
                    "title": "AnyI2V：无训练的灵活视频生成框架",
                    "desc": "AnyI2V是一个无需训练的框架，可以根据用户定义的运动轨迹为条件图像添加动画，支持多种数据类型，从而实现灵活的视频生成。该方法解决了现有文本到视频（T2V）和图像到视频（I2V）合成中的动态运动信号和空间约束整合问题。与传统方法不同，AnyI2V不依赖于真实图像，允许更高的可编辑性，并支持混合条件输入和风格转移。实验结果表明，AnyI2V在空间和运动控制的视频生成方面表现优越，提供了新的视角。"
                }
            }
        }
    ],
    "link_prev": "2025-07-16.html",
    "link_next": "2025-07-18.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "16.07",
        "en": "07/16",
        "zh": "7月16日"
    },
    "short_date_next": {
        "ru": "18.07",
        "en": "07/18",
        "zh": "7月18日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}