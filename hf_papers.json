{
    "date": {
        "ru": "2 июля",
        "en": "July 2",
        "zh": "7月2日"
    },
    "time_utc": "2025-07-02 02:44",
    "weekday": 2,
    "issue_id": 4592,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.23115",
            "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings",
            "url": "https://huggingface.co/papers/2506.23115",
            "abstract": "MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.",
            "score": 12,
            "issue_id": 4592,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 июня",
                "en": "June 29",
                "zh": "6月29日"
            },
            "hash": "d7fecdae218ccf8e",
            "authors": [
                "Haonan Chen",
                "Hong Liu",
                "Yuping Luo",
                "Liang Wang",
                "Nan Yang",
                "Furu Wei",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Microsoft Corporation",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23115.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MoCa: революция в мультимодальном встраивании",
                    "desc": "MoCa - это двухэтапная система для улучшения предобученных причинно-следственных визуально-языковых моделей для мультимодального встраивания. Она вводит двунаправленное внимание, масштабирование с помощью неразмеченных данных и разнообразные цели обучения. Первый этап включает совместную реконструкцию для улучшения двунаправленных рассуждений с учетом контекста. Второй этап использует разнообразные семантически богатые мультимодальные данные для улучшения обобщения и выравнивания."
                },
                "en": {
                    "title": "MoCa: Enhancing Multimodal Embedding with Bidirectional Attention",
                    "desc": "MoCa is a two-stage framework designed to improve pre-trained causal vision-language models for better multimodal embedding. It addresses limitations in current models, such as the inefficiency of causal attention and the need for high-quality labeled data. The first stage focuses on modality-aware continual pre-training, which enhances understanding by denoising both text and image inputs. The second stage employs heterogeneous contrastive fine-tuning, using diverse multimodal data to improve model generalization and alignment, leading to state-of-the-art performance in various benchmarks."
                },
                "zh": {
                    "title": "MoCa：双向多模态嵌入的创新框架",
                    "desc": "MoCa是一个两阶段框架，旨在增强预训练的因果视觉语言模型在多模态嵌入中的表现。它通过引入双向注意力机制、利用未标记数据进行扩展以及多样化的训练目标来解决现有方法的局限性。第一阶段通过联合重建目标来提高文本和图像输入的去噪能力，增强双向上下文感知推理。第二阶段则利用丰富的多模态数据进行异构对比微调，从而提高模型的泛化能力和对齐效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.00432",
            "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
            "url": "https://huggingface.co/papers/2507.00432",
            "abstract": "Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.",
            "score": 7,
            "issue_id": 4592,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 июля",
                "en": "July 1",
                "zh": "7月1日"
            },
            "hash": "c4a7e4dd11865858",
            "authors": [
                "Maggie Huan",
                "Yuetai Li",
                "Tuney Zheng",
                "Xiaoyu Xu",
                "Seungone Kim",
                "Minxin Du",
                "Radha Poovendran",
                "Graham Neubig",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "M-A-P",
                "The Hong Kong Polytechnic University",
                "University of Pennsylvania",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.00432.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#transfer_learning",
                    "#optimization",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обучение с подкреплением превосходит обучение с учителем в обобщении навыков решения задач",
                    "desc": "Исследование показывает, что модели, обученные с помощью обучения с подкреплением (RL), лучше обобщают навыки решения математических задач на другие области, чем модели, настроенные с помощью обучения с учителем (SFT). Анализ латентного пространства и распределения токенов выявил, что SFT вызывает значительный сдвиг в представлении и выводе, в то время как RL сохраняет общую структуру. Эксперименты проводились на моделях Qwen3-14B с использованием только математических данных, но разных методов обучения. Результаты указывают на необходимость пересмотра стандартных подходов к обучению моделей рассуждения, особенно в отношении использования данных, полученных с помощью SFT."
                },
                "en": {
                    "title": "Rethinking Training: Reinforcement Learning for Better Generalization",
                    "desc": "This paper investigates the effectiveness of different training methods for reasoning models, particularly in the context of mathematical problem-solving. It finds that models trained with reinforcement learning (RL) outperform those fine-tuned with supervised learning (SFT) when applied to a variety of tasks beyond mathematics. The study reveals that while SFT models excel in math, they struggle to generalize their skills to other domains due to significant representation drift. In contrast, RL-tuned models maintain their general problem-solving abilities, suggesting a need to reconsider current training approaches for reasoning models."
                },
                "zh": {
                    "title": "重新思考推理模型的训练方法",
                    "desc": "这篇论文探讨了强化学习（RL）调优模型在数学问题解决能力上的表现，发现其在其他领域的泛化能力优于监督微调（SFT）模型。研究表明，虽然许多模型在数学任务上表现出色，但它们在其他任务上的迁移能力却较差。通过对Qwen3-14B模型的实验，发现RL调优模型能够在多个领域中保持良好的泛化能力，而SFT模型则容易遗忘其通用能力。结果提示我们需要重新审视现有的训练方法，尤其是对SFT数据的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21277",
            "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
            "url": "https://huggingface.co/papers/2506.21277",
            "abstract": "A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.",
            "score": 3,
            "issue_id": 4592,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 июня",
                "en": "June 26",
                "zh": "6月26日"
            },
            "hash": "15a38ef84e7820fa",
            "authors": [
                "Qize Yang",
                "Shimin Yao",
                "Weixuan Chen",
                "Shenghao Fu",
                "Detao Bai",
                "Jiaxing Zhao",
                "Boyuan Sun",
                "Bowen Yin",
                "Xihan Wei",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21277.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#games",
                    "#rl",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление мультимодального рассуждения с помощью обучения с подкреплением",
                    "desc": "Предложен подход на основе обучения с подкреплением для улучшения мультимодального рассуждения в больших языковых моделях. Метод решает проблемы понимания контекста и упрощения, используя контекстные, форматные, точностные и логические награды. Авторы вводят новый бенчмарк IntentBench для оценки понимания сложных человеческих намерений и эмоций. Предложенный метод показывает превосходную производительность на IntentBench по сравнению с другими открытыми мультимодальными моделями."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Reinforcement Learning",
                    "desc": "This paper presents a reinforcement learning approach to improve multimodal reasoning in large language models. It addresses two main challenges: insufficient understanding of global context and the shortcut problem, where models fail to consider important multimodal cues. By implementing context, format, accuracy, and logical rewards, the model enhances its reasoning capabilities and interprets multimodal inputs more effectively. The proposed method outperforms existing models on the IntentBench benchmark, showcasing its ability to understand complex human intentions and emotions."
                },
                "zh": {
                    "title": "强化学习提升多模态推理能力",
                    "desc": "本论文提出了一种基于强化学习的方法，以增强多模态推理能力，解决上下文理解和捷径问题。我们发现现有多模态推理模型存在全球上下文理解不足和捷径问题，这会导致模型错误解读多模态信息。为了解决这些问题，我们强调模型需要在多模态输入中清晰理解全球上下文，并通过上下文奖励、格式奖励和准确性奖励来确保对多模态信息的准确解读。我们的研究在IntentBench基准测试中表现优异，展示了在理解复杂人类意图和情感方面的先进性能。"
                }
            }
        }
    ],
    "link_prev": "2025-07-01.html",
    "link_next": "2025-07-03.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "01.07",
        "en": "07/01",
        "zh": "7月1日"
    },
    "short_date_next": {
        "ru": "03.07",
        "en": "07/03",
        "zh": "7月3日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}