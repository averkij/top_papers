{
    "date": {
        "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 11",
        "zh": "12æœˆ11æ—¥"
    },
    "time_utc": "2024-12-11 12:19",
    "weekday": 2,
    "issue_id": 1066,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.05210",
            "title": "Evaluating and Aligning CodeLLMs on Human Preference",
            "url": "https://huggingface.co/papers/2412.05210",
            "abstract": "Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\\url{https://codearenaeval.github.io/ }",
            "score": 38,
            "issue_id": 1060,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "0232aabe01d37826",
            "authors": [
                "Jian Yang",
                "Jiaxi Yang",
                "Ke Jin",
                "Yibo Miao",
                "Lei Zhang",
                "Liqun Yang",
                "Zeyu Cui",
                "Yichang Zhang",
                "Binyuan Hui",
                "Junyang Lin"
            ],
            "affiliations": [
                "Alibaba Group",
                "Shanghai Jiao Tong University",
                "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05210.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#alignment",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "CodeArena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CodeArena Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 397 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° 44 ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ SynCode-Instruct Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ¾ĞºĞ¾Ğ»Ğ¾ 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Bridging Code Generation and Human Preferences with CodeArena",
                    "desc": "This paper discusses the advancements in code large language models (codeLLMs) for code generation tasks. It highlights the limitations of existing benchmarks that primarily focus on code correctness without considering human preferences in real-world applications. To address this, the authors introduce CodeArena, a human-curated benchmark that includes diverse coding tasks across multiple programming languages. Additionally, they present SynCode-Instruct, a large synthetic instruction dataset that enhances the training of codeLLMs, revealing significant performance differences between various models when evaluated against human-aligned benchmarks."
                },
                "zh": {
                    "title": "æå‡ä»£ç ç”Ÿæˆæ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·CodeArenaï¼Œç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆcode LLMsï¼‰çš„æ€§èƒ½ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ä»£ç ç‰‡æ®µçš„æ­£ç¡®æ€§ï¼Œè€Œå¿½è§†äº†ä¸äººç±»åå¥½çš„å¯¹é½ã€‚CodeArenaé€šè¿‡æä¾›397ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæ¶µç›–40ä¸ªç±»åˆ«å’Œ44ç§ç¼–ç¨‹è¯­è¨€ï¼Œæ¨¡æ‹ŸçœŸå®ç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„åˆæˆæŒ‡ä»¤è¯­æ–™åº“SynCode-Instructï¼Œä»¥éªŒè¯å¤§è§„æ¨¡åˆæˆæŒ‡ä»¤å¾®è°ƒçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºå¼€æºä»£ç LLMsä¸ä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07589",
            "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation",
            "url": "https://huggingface.co/papers/2412.07589",
            "abstract": "Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limitations, we propose a new task: customized manga generation and introduce DiffSensei, an innovative framework specifically designed for generating manga with dynamic multi-character control. DiffSensei integrates a diffusion-based image generator with a multimodal large language model (MLLM) that acts as a text-compatible identity adapter. Our approach employs masked cross-attention to seamlessly incorporate character features, enabling precise layout control without direct pixel transfer. Additionally, the MLLM-based adapter adjusts character features to align with panel-specific text cues, allowing flexible adjustments in character expressions, poses, and actions. We also introduce MangaZero, a large-scale dataset tailored to this task, containing 43,264 manga pages and 427,147 annotated panels, supporting the visualization of varied character interactions and movements across sequential frames. Extensive experiments demonstrate that DiffSensei outperforms existing models, marking a significant advancement in manga generation by enabling text-adaptable character customization. The project page is https://jianzongwu.github.io/projects/diffsensei/.",
            "score": 22,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "8a1bb8ed9ae040f2",
            "authors": [
                "Jianzong Wu",
                "Chao Tang",
                "Jingbo Wang",
                "Yanhong Zeng",
                "Xiangtai Li",
                "Yunhai Tong"
            ],
            "affiliations": [
                "Bytedance Seed Project",
                "Nanyang Technological University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07589.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#story_generation",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "DiffSensei: Ğ˜Ğ˜-Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸Ğº Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiffSensei - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. DiffSensei Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MangaZero Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Dynamic Manga Generation with Character Control",
                    "desc": "This paper presents DiffSensei, a novel framework for generating manga that allows for dynamic control of multiple characters based on textual descriptions. It combines a diffusion-based image generator with a multimodal large language model (MLLM) to effectively manage character identities and interactions in complex scenes. The framework utilizes masked cross-attention to integrate character features, enabling precise layout control and adjustments in expressions, poses, and actions according to panel-specific text cues. Additionally, the authors introduce MangaZero, a comprehensive dataset designed for this task, which significantly enhances the training and evaluation of manga generation models."
                },
                "zh": {
                    "title": "å®šåˆ¶æ¼«ç”»ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼šå®šåˆ¶æ¼«ç”»ç”Ÿæˆï¼Œå¹¶ä»‹ç»äº†DiffSenseiæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°åŠ¨æ€å¤šè§’è‰²æ§åˆ¶çš„æ¼«ç”»ç”Ÿæˆã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆå™¨å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡æ©è”½äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆæ•´åˆè§’è‰²ç‰¹å¾ã€‚DiffSenseièƒ½å¤Ÿæ ¹æ®é¢æ¿ç‰¹å®šçš„æ–‡æœ¬æç¤ºçµæ´»è°ƒæ•´è§’è‰²çš„è¡¨æƒ…ã€å§¿åŠ¿å’ŒåŠ¨ä½œï¼Œä»è€Œå®ç°ç²¾ç¡®çš„å¸ƒå±€æ§åˆ¶ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MangaZeroæ•°æ®é›†ï¼ŒåŒ…å«43,264é¡µæ¼«ç”»å’Œ427,147ä¸ªæ³¨é‡Šé¢æ¿ï¼Œæ”¯æŒå¤šæ ·åŒ–è§’è‰²äº¤äº’å’ŒåŠ¨ä½œçš„å¯è§†åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07730",
            "title": "STIV: Scalable Text and Image Conditioned Video Generation",
            "url": "https://huggingface.co/papers/2412.07730",
            "abstract": "The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in a simple and scalable text-image-conditioned video generation method, named STIV. Our framework integrates image condition into a Diffusion Transformer (DiT) through frame replacement, while incorporating text conditioning via a joint image-text conditional classifier-free guidance. This design enables STIV to perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks simultaneously. Additionally, STIV can be easily extended to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation, etc. With comprehensive ablation studies on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple design. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V, surpassing both leading open and closed-source models like CogVideoX-5B, Pika, Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result of 90.1 on VBench I2V task at 512 resolution. By providing a transparent and extensible recipe for building cutting-edge video generation models, we aim to empower future research and accelerate progress toward more versatile and reliable video generation solutions.",
            "score": 18,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "a43bca3bdca1a7ba",
            "authors": [
                "Zongyu Lin",
                "Wei Liu",
                "Chen Chen",
                "Jiasen Lu",
                "Wenze Hu",
                "Tsu-Jui Fu",
                "Jesse Allardice",
                "Zhengfeng Lai",
                "Liangchen Song",
                "Bowen Zhang",
                "Cha Chen",
                "Yiran Fei",
                "Yifan Jiang",
                "Lezhi Li",
                "Yizhou Sun",
                "Kai-Wei Chang",
                "Yinfei Yang"
            ],
            "affiliations": [
                "Apple",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07730.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#games",
                    "#diffusion",
                    "#training",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "STIV: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ STIV. STIV Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Diffusion Transformer Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ±ĞµĞ·ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ STIV Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ (T2V), Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ (TI2V). ĞœĞ¾Ğ´ĞµĞ»ÑŒ STIV Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 8.7B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… T2V Ğ¸ I2V, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "STIV: Simplifying Video Generation with Text and Image Conditioning",
                    "desc": "This paper introduces STIV, a novel framework for video generation that effectively combines text and image inputs to create videos. It utilizes a Diffusion Transformer architecture, enhancing its capabilities through a method called frame replacement and a joint image-text conditional classifier-free guidance. STIV is designed to handle both text-to-video (T2V) and text-image-to-video (TI2V) tasks, making it versatile for various applications like video prediction and frame interpolation. The model demonstrates impressive performance metrics, outperforming existing models while maintaining a straightforward design, thus providing a clear pathway for future advancements in video generation."
                },
                "zh": {
                    "title": "è§†é¢‘ç”Ÿæˆçš„ç®€å•ä¸å¼ºå¤§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œæ•°æ®ç­–åˆ’ç­–ç•¥ä¹‹é—´çš„å…³ç³»ï¼Œæå‡ºäº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•STIVã€‚STIVé€šè¿‡å¸§æ›¿æ¢å°†å›¾åƒæ¡ä»¶é›†æˆåˆ°æ‰©æ•£å˜æ¢å™¨ä¸­ï¼ŒåŒæ—¶åˆ©ç”¨æ— æ¡ä»¶åˆ†ç±»å™¨å¼•å¯¼å®ç°æ–‡æœ¬æ¡ä»¶ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŒæ—¶æ‰§è¡Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œæ–‡æœ¬-å›¾åƒåˆ°è§†é¢‘ï¼ˆTI2Vï¼‰ä»»åŠ¡ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°è§†é¢‘é¢„æµ‹ã€å¸§æ’å€¼ç­‰å¤šç§åº”ç”¨ã€‚é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼ŒSTIVåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ€§èƒ½å’Œç®€å•çš„è®¾è®¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04653",
            "title": "Hidden in the Noise: Two-Stage Robust Watermarking for Images",
            "url": "https://huggingface.co/papers/2412.04653",
            "abstract": "As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks.",
            "score": 13,
            "issue_id": 1060,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "8032f89319a70b88",
            "authors": [
                "Kasra Arabi",
                "Benjamin Feuer",
                "R. Teal Witter",
                "Chinmay Hegde",
                "Niv Cohen"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04653.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#security",
                    "#rag",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸ Ğ½ĞµÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğµ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ AI-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¤ÑƒÑ€ÑŒĞµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ±ĞµĞ· Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ĞºĞ¸ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Robust Watermarking for AI-Generated Images",
                    "desc": "This paper presents a novel approach to image watermarking that aims to enhance the security of AI-generated content against forgery and removal attacks. The authors introduce a distortion-free watermarking method that utilizes the initial noise from a diffusion model, which helps to avoid revealing watermarking techniques. They propose a two-stage framework that first augments the initial noise with Fourier patterns to embed watermark information, and then efficiently detects the watermark by comparing it to a group of initial noises. This method demonstrates significant robustness against various attacks, setting a new standard in the field of image watermarking."
                },
                "zh": {
                    "title": "æ— å¤±çœŸæ°´å°ï¼Œä¿æŠ¤AIç”Ÿæˆå†…å®¹çš„æœªæ¥",
                    "desc": "éšç€å›¾åƒç”ŸæˆæŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæ·±åº¦ä¼ªé€ æˆä¸ºç¤¾ä¼šè®¨è®ºçš„çƒ­ç‚¹ã€‚å›¾åƒæ°´å°æŠ€æœ¯å¯ä»¥å¸®åŠ©æ¨¡å‹æ‹¥æœ‰è€…æ£€æµ‹å’Œæ ‡è®°ä»–ä»¬ç”Ÿæˆçš„å†…å®¹ï¼Œä»è€Œå‡å°‘æ½œåœ¨çš„å±å®³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ°´å°æ–¹æ³•åœ¨é¢å¯¹ä¼ªé€ å’Œå»é™¤æ”»å‡»æ—¶ä»ç„¶å­˜åœ¨è„†å¼±æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— å¤±çœŸæ°´å°æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µæ¡†æ¶æé«˜äº†æ°´å°çš„æ£€æµ‹æ•ˆç‡ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹å„ç§æ”»å‡»çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07774",
            "title": "UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics",
            "url": "https://huggingface.co/papers/2412.07774",
            "abstract": "We introduce UniReal, a unified framework designed to address various image generation and editing tasks. Existing solutions often vary by tasks, yet share fundamental principles: preserving consistency between inputs and outputs while capturing visual variations. Inspired by recent video generation models that effectively balance consistency and variation across frames, we propose a unifying approach that treats image-level tasks as discontinuous video generation. Specifically, we treat varying numbers of input and output images as frames, enabling seamless support for tasks such as image generation, editing, customization, composition, etc. Although designed for image-level tasks, we leverage videos as a scalable source for universal supervision. UniReal learns world dynamics from large-scale videos, demonstrating advanced capability in handling shadows, reflections, pose variation, and object interaction, while also exhibiting emergent capability for novel applications.",
            "score": 13,
            "issue_id": 1058,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "64e24bea8dffc31d",
            "authors": [
                "Xi Chen",
                "Zhifei Zhang",
                "He Zhang",
                "Yuqian Zhou",
                "Soo Ye Kim",
                "Qing Liu",
                "Yijun Li",
                "Jianming Zhang",
                "Nanxuan Zhao",
                "Yilin Wang",
                "Hui Ding",
                "Zhe Lin",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "Adobe Research",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07774.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "UniReal - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ¸ÑÑ‚ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞ½Ğ¸, Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ· Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². UniReal Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "UniReal: Unifying Image Generation and Editing through Video Dynamics",
                    "desc": "UniReal is a comprehensive framework that simplifies various image generation and editing tasks by treating them as a form of video generation. It focuses on maintaining consistency between input and output images while allowing for visual variations, similar to how video frames work. By using videos as a source of universal supervision, UniReal learns complex world dynamics, enabling it to manage challenges like shadows, reflections, and object interactions effectively. This approach not only enhances traditional image tasks but also opens up new possibilities for innovative applications in image processing."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¡†æ¶ï¼Œå›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æœªæ¥",
                    "desc": "UniRealæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å„ç§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å›¾åƒä»»åŠ¡è§†ä¸ºä¸è¿ç»­çš„è§†é¢‘ç”Ÿæˆï¼Œæ¥ä¿æŒè¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶æ•æ‰è§†è§‰å˜åŒ–ã€‚UniRealåˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘ä½œä¸ºé€šç”¨ç›‘ç£æºï¼Œå­¦ä¹ ä¸–ç•ŒåŠ¨æ€ï¼Œä»è€Œåœ¨å¤„ç†é˜´å½±ã€åå°„ã€å§¿æ€å˜åŒ–å’Œç‰©ä½“äº¤äº’æ–¹é¢å±•ç°å‡ºå…ˆè¿›çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ä»…é€‚ç”¨äºå›¾åƒä»»åŠ¡ï¼Œè¿˜å±•ç°å‡ºå¯¹æ–°åº”ç”¨çš„æ½œåœ¨èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07626",
            "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
            "url": "https://huggingface.co/papers/2412.07626",
            "abstract": "Document content extraction is crucial in computer vision, especially for meeting the high-quality data needs of large language models (LLMs) and retrieval-augmented generation (RAG) technologies. However, current document parsing methods suffer from significant limitations in terms of diversity and comprehensive evaluation. To address these challenges, we introduce OmniDocBench, a novel multi-source benchmark designed to advance automated document content extraction. OmniDocBench includes a meticulously curated and annotated high-quality evaluation dataset comprising nine diverse document types, such as academic papers, textbooks, slides, among others. Our benchmark provides a flexible and comprehensive evaluation framework with 19 layout category labels and 14 attribute labels, enabling multi-level assessments across entire datasets, individual modules, or specific data types. Using OmniDocBench, we perform an exhaustive comparative analysis of existing modular pipelines and multimodal end-to-end methods, highlighting their limitations in handling document diversity and ensuring fair evaluation. OmniDocBench establishes a robust, diverse, and fair evaluation standard for the document content extraction field, offering crucial insights for future advancements and fostering the development of document parsing technologies. The codes and dataset is available in https://github.com/opendatalab/OmniDocBench.",
            "score": 12,
            "issue_id": 1065,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "4bba9d3934addbcc",
            "authors": [
                "Linke Ouyang",
                "Yuan Qu",
                "Hongbin Zhou",
                "Jiawei Zhu",
                "Rui Zhang",
                "Qunshu Lin",
                "Bin Wang",
                "Zhiyuan Zhao",
                "Man Jiang",
                "Xiaomeng Zhao",
                "Jin Shi",
                "Fan Wu",
                "Pei Chu",
                "Minghao Liu",
                "Zhenxiang Li",
                "Chao Xu",
                "Bo Zhang",
                "Botian Shi",
                "Zhongying Tu",
                "Conghui He"
            ],
            "affiliations": [
                "Abaka AI",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07626.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "OmniDocBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ 19 Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¸ 14 Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ OmniDocBench, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… end-to-end Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "OmniDocBench: Elevating Document Content Extraction Standards",
                    "desc": "This paper introduces OmniDocBench, a new benchmark aimed at improving document content extraction in computer vision. It addresses the limitations of current parsing methods by providing a diverse and comprehensive evaluation framework with a curated dataset of nine document types. The benchmark includes 19 layout category labels and 14 attribute labels, allowing for detailed assessments of various extraction methods. By conducting a thorough analysis of existing techniques, OmniDocBench sets a new standard for evaluating document parsing technologies, promoting advancements in the field."
                },
                "zh": {
                    "title": "OmniDocBenchï¼šæ–‡æ¡£æå–çš„æ–°æ ‡å‡†",
                    "desc": "æ–‡æ¡£å†…å®¹æå–åœ¨è®¡ç®—æœºè§†è§‰ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯æ»¡è¶³å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¢å¼ºæ£€ç´¢ç”ŸæˆæŠ€æœ¯çš„é«˜è´¨é‡æ•°æ®éœ€æ±‚ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ–‡æ¡£è§£ææ–¹æ³•åœ¨å¤šæ ·æ€§å’Œå…¨é¢è¯„ä¼°æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OmniDocBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæºåŸºå‡†ï¼Œæ—¨åœ¨æ¨åŠ¨è‡ªåŠ¨åŒ–æ–‡æ¡£å†…å®¹æå–çš„å‘å±•ã€‚OmniDocBenchæä¾›äº†ä¸€ä¸ªçµæ´»ä¸”å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿå¯¹æ–‡æ¡£å¤šæ ·æ€§è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶ä¸ºæœªæ¥çš„æ–‡æ¡£è§£ææŠ€æœ¯å‘å±•æä¾›é‡è¦è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07674",
            "title": "FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models",
            "url": "https://huggingface.co/papers/2412.07674",
            "abstract": "Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable attributes from the source images. Current methods attempt to distill identity and style from source images. However, \"style\" is a broad concept that includes texture, color, and artistic elements, but does not cover other important attributes such as lighting and dynamics. Additionally, a simplified \"style\" adaptation prevents combining multiple attributes from different sources into one generated image. In this work, we formulate a more effective approach to decompose the aesthetics of a picture into specific visual attributes, allowing users to apply characteristics such as lighting, texture, and dynamics from different images. To achieve this goal, we constructed the first fine-grained visual attributes dataset (FiVA) to the best of our knowledge. This FiVA dataset features a well-organized taxonomy for visual attributes and includes around 1 M high-quality generated images with visual attribute annotations. Leveraging this dataset, we propose a fine-grained visual attribute adaptation framework (FiVA-Adapter), which decouples and adapts visual attributes from one or more source images into a generated one. This approach enhances user-friendly customization, allowing users to selectively apply desired attributes to create images that meet their unique preferences and specific content requirements.",
            "score": 12,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "339e4e8d1664472e",
            "authors": [
                "Tong Wu",
                "Yinghao Xu",
                "Ryan Po",
                "Mengchen Zhang",
                "Guandao Yang",
                "Jiaqi Wang",
                "Ziwei Liu",
                "Dahua Lin",
                "Gordon Wetzstein"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "S-Lab, NTU",
                "Shanghai Artificial Intelligence Laboratory",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07674.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#synthetic"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ FiVA Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº FiVA-Adapter Ğ´Ğ»Ñ Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ°Ğº Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ° Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Customize Your Images with Fine-Grained Visual Attributes!",
                    "desc": "This paper presents a new method for text-to-image generation that allows users to customize images by selecting specific visual attributes from different source images. The authors introduce the FiVA dataset, which contains around 1 million high-quality images annotated with fine-grained visual attributes like lighting, texture, and dynamics. Their proposed framework, FiVA-Adapter, effectively decouples these attributes, enabling users to combine them in a single generated image. This approach improves the flexibility and user-friendliness of image generation, making it easier for non-experts to achieve their desired visual outcomes."
                },
                "zh": {
                    "title": "ç»†ç²’åº¦è§†è§‰å±æ€§é€‚é…ï¼Œå®šåˆ¶ä½ çš„å›¾åƒï¼",
                    "desc": "æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿåˆ›å»ºé«˜è´¨é‡çš„å›¾åƒï¼Œåº”ç”¨å¹¿æ³›ã€‚ç„¶è€Œï¼Œå‡†ç¡®æè¿°æ‰€éœ€çš„è§†è§‰å±æ€§å¯¹éä¸“ä¸šäººå£«æ¥è¯´å¯èƒ½å¾ˆå›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†å›¾åƒçš„ç¾å­¦åˆ†è§£ä¸ºå…·ä½“çš„è§†è§‰å±æ€§ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä»ä¸åŒçš„å›¾åƒä¸­åº”ç”¨ç‰¹å¾ï¼Œå¦‚å…‰ç…§ã€çº¹ç†å’ŒåŠ¨æ€ã€‚æˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªç»†ç²’åº¦è§†è§‰å±æ€§æ•°æ®é›†ï¼ˆFiVAï¼‰ï¼Œå¹¶æå‡ºäº†FiVA-Adapteræ¡†æ¶ï¼Œä»¥ä¾¿ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®ä¸ªäººåå¥½å®šåˆ¶ç”Ÿæˆçš„å›¾åƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03548",
            "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
            "url": "https://huggingface.co/papers/2412.03548",
            "abstract": "Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection. Yet, MLMs can not produce intermediate depth or boxes to reason over. Finetuning MLMs on relevant data doesn't generalize well and outsourcing computation to specialized vision tools is too compute-intensive and memory-inefficient. To address this, we introduce Perception Tokens, intrinsic image representations designed to assist reasoning tasks where language is insufficient. Perception tokens act as auxiliary reasoning tokens, akin to chain-of-thought prompts in language models. For example, in a depth-related task, an MLM augmented with perception tokens can reason by generating a depth map as tokens, enabling it to solve the problem effectively. We propose AURORA, a training method that augments MLMs with perception tokens for improved reasoning over visual inputs. AURORA leverages a VQVAE to transform intermediate image representations, such as depth maps into a tokenized format and bounding box tokens, which is then used in a multi-task training framework. AURORA achieves notable improvements across counting benchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench, outperforming finetuning approaches in generalization across datasets. It also improves on relative depth: over +6% on BLINK. With perception tokens, AURORA expands the scope of MLMs beyond language-based reasoning, paving the way for more effective visual reasoning capabilities.",
            "score": 9,
            "issue_id": 1060,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "b1047666846bb684",
            "authors": [
                "Mahtab Bigverdi",
                "Zelun Luo",
                "Cheng-Yu Hsieh",
                "Ethan Shen",
                "Dongping Chen",
                "Linda G. Shapiro",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Google Research",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03548.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¢Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ' - Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼ Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AURORA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ VQVAE Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning in MLMs with Perception Tokens",
                    "desc": "This paper introduces Perception Tokens to enhance multimodal language models (MLMs) in visual reasoning tasks. Traditional MLMs struggle with tasks like depth estimation and object detection because they cannot generate necessary intermediate representations. The proposed AURORA method integrates these perception tokens into MLMs, allowing them to produce depth maps and bounding boxes as tokens for better reasoning. AURORA shows significant performance improvements on various benchmarks, demonstrating its effectiveness in expanding the reasoning capabilities of MLMs beyond just language."
                },
                "zh": {
                    "title": "æ„ŸçŸ¥æ ‡è®°ï¼šæå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›",
                    "desc": "å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰åœ¨åŸºæœ¬è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸“é—¨æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ„ŸçŸ¥æ ‡è®°ï¼ˆPerception Tokensï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å†…åœ¨çš„å›¾åƒè¡¨ç¤ºï¼Œæ—¨åœ¨è¾…åŠ©è¯­è¨€ä¸è¶³çš„æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥æ„ŸçŸ¥æ ‡è®°ï¼ŒMLMsèƒ½å¤Ÿç”Ÿæˆæ·±åº¦å›¾ç­‰ä¸­é—´è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆè§£å†³ä¸æ·±åº¦ç›¸å…³çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºçš„AURORAè®­ç»ƒæ–¹æ³•é€šè¿‡å°†æ„ŸçŸ¥æ ‡è®°ä¸MLMsç»“åˆï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰è¾“å…¥çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶åœ¨è®¡æ•°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07334",
            "title": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation",
            "url": "https://huggingface.co/papers/2412.07334",
            "abstract": "Interpretability is a key challenge in fostering trust for Large Language Models (LLMs), which stems from the complexity of extracting reasoning from model's parameters. We present the Frame Representation Hypothesis, a theoretically robust framework grounded in the Linear Representation Hypothesis (LRH) to interpret and control LLMs by modeling multi-token words. Prior research explored LRH to connect LLM representations with linguistic concepts, but was limited to single token analysis. As most words are composed of several tokens, we extend LRH to multi-token words, thereby enabling usage on any textual data with thousands of concepts. To this end, we propose words can be interpreted as frames, ordered sequences of vectors that better capture token-word relationships. Then, concepts can be represented as the average of word frames sharing a common concept. We showcase these tools through Top-k Concept-Guided Decoding, which can intuitively steer text generation using concepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3 families, demonstrating gender and language biases, exposing harmful content, but also potential to remediate them, leading to safer and more transparent LLMs. Code is available at https://github.com/phvv-me/frame-representation-hypothesis.git",
            "score": 8,
            "issue_id": 1062,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "ed0e99c50709fbaf",
            "authors": [
                "Pedro H. V. Valois",
                "Lincon S. Souza",
                "Erica K. Shimomoto",
                "Kazuhiro Fukui"
            ],
            "affiliations": [
                "National Institute of Advanced Industrial Science and Technology (AIST)",
                "University of Tsukuba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07334.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#ethics",
                    "#data",
                    "#interpretability",
                    "#multimodal",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²: ĞºĞ»ÑÑ‡ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… ĞºĞ°Ğº ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½Ğ´ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Unlocking LLMs: Interpreting Words as Frames for Safer AI",
                    "desc": "This paper addresses the challenge of interpreting Large Language Models (LLMs) by introducing the Frame Representation Hypothesis, which builds on the Linear Representation Hypothesis (LRH). It extends the analysis from single-token to multi-token words, allowing for a more comprehensive understanding of how words are represented in LLMs. By modeling words as framesâ€”ordered sequences of vectorsâ€”the authors provide a method to capture the relationships between tokens and concepts more effectively. The proposed Top-k Concept-Guided Decoding technique enables controlled text generation based on selected concepts, revealing biases and harmful content while also offering pathways for remediation."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§çš„æ¡†æ¶è¡¨ç¤ºå‡è®¾",
                    "desc": "æœ¬æ–‡æå‡ºäº†æ¡†æ¶è¡¨ç¤ºå‡è®¾ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚æˆ‘ä»¬æ‰©å±•äº†çº¿æ€§è¡¨ç¤ºå‡è®¾ï¼ˆLRHï¼‰ï¼Œå°†å…¶åº”ç”¨äºå¤šæ ‡è®°è¯çš„åˆ†æï¼Œä»¥æ›´å¥½åœ°ç†è§£å’Œæ§åˆ¶æ¨¡å‹çš„è¾“å‡ºã€‚é€šè¿‡å°†è¯è¯­è§†ä¸ºæ¡†æ¶ï¼Œåˆ©ç”¨å‘é‡åºåˆ—æ•æ‰æ ‡è®°ä¸è¯ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ–‡æœ¬ç”Ÿæˆä¸­å¼•å…¥æ¦‚å¿µæŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨å¤šä¸ªæ¨¡å‹ä¸ŠéªŒè¯äº†è¿™äº›æ–¹æ³•ï¼Œæ­ç¤ºäº†æ€§åˆ«å’Œè¯­è¨€åè§ï¼ŒåŒæ—¶å±•ç¤ºäº†æ”¹å–„è¿™äº›é—®é¢˜çš„æ½œåŠ›ï¼Œä»è€Œæ¨åŠ¨æ›´å®‰å…¨å’Œé€æ˜çš„LLMsã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07776",
            "title": "Video Motion Transfer with Diffusion Transformers",
            "url": "https://huggingface.co/papers/2412.07776",
            "abstract": "We propose DiTFlow, a method for transferring the motion of a reference video to a newly synthesized one, designed specifically for Diffusion Transformers (DiT). We first process the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal called the Attention Motion Flow (AMF). We guide the latent denoising process in an optimization-based, training-free, manner by optimizing latents with our AMF loss to generate videos reproducing the motion of the reference one. We also apply our optimization strategy to transformer positional embeddings, granting us a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow against recently published methods, outperforming all across multiple metrics and human evaluation.",
            "score": 8,
            "issue_id": 1062,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "57a229123b5b2c38",
            "authors": [
                "Alexander Pondaven",
                "Aliaksandr Siarohin",
                "Sergey Tulyakov",
                "Philip Torr",
                "Fabio Pizzati"
            ],
            "affiliations": [
                "MBZUAI",
                "Snap Inc.",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07776.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#transfer_learning",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "DiTFlow: ĞŸĞµÑ€ĞµĞ½Ğ¾Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "DiTFlow - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT). ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ DiT Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Attention Motion Flow (AMF). DiTFlow Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ AMF, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Seamlessly Transfer Motion with DiTFlow!",
                    "desc": "DiTFlow is a novel approach for transferring motion from a reference video to a newly created video using Diffusion Transformers (DiT). The method involves analyzing the reference video to extract Attention Motion Flow (AMF), which captures motion signals across frames. By optimizing the latent space with the AMF loss, DiTFlow effectively guides the video generation process without requiring additional training. The technique also enhances zero-shot motion transfer by optimizing transformer positional embeddings, demonstrating superior performance compared to existing methods in various evaluations."
                },
                "zh": {
                    "title": "DiTFlowï¼šé«˜æ•ˆçš„è§†é¢‘è¿åŠ¨è½¬ç§»æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºDiTFlowçš„æ–¹æ³•ï¼Œç”¨äºå°†å‚è€ƒè§†é¢‘çš„è¿åŠ¨è½¬ç§»åˆ°æ–°åˆæˆçš„è§†é¢‘ä¸Šï¼Œç‰¹åˆ«ä¸ºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰è®¾è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„DiTå¤„ç†å‚è€ƒè§†é¢‘ï¼Œåˆ†æè·¨å¸§æ³¨æ„åŠ›å›¾ï¼Œå¹¶æå–ç§°ä¸ºæ³¨æ„åŠ›è¿åŠ¨æµï¼ˆAMFï¼‰çš„è¡¥ä¸çº§è¿åŠ¨ä¿¡å·ã€‚é€šè¿‡ä¼˜åŒ–AMFæŸå¤±ï¼Œæˆ‘ä»¬ä»¥æ— è®­ç»ƒçš„ä¼˜åŒ–æ–¹å¼å¼•å¯¼æ½œåœ¨å»å™ªè¿‡ç¨‹ï¼Œä»è€Œç”Ÿæˆå†ç°å‚è€ƒè§†é¢‘è¿åŠ¨çš„è§†é¢‘ã€‚æˆ‘ä»¬è¿˜å°†ä¼˜åŒ–ç­–ç•¥åº”ç”¨äºå˜æ¢å™¨çš„ä½ç½®åµŒå…¥ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬è¿åŠ¨è½¬ç§»çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06674",
            "title": "EMOv2: Pushing 5M Vision Model Frontier",
            "url": "https://huggingface.co/papers/2412.06674",
            "abstract": "This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the infrastructure for lightweight CNNs, but no counterparts have been recognized by attention-based design. Our work rethinks the lightweight infrastructure of efficient IRB and practical components in Transformer from a unified perspective, extending CNN-based IRB to attention-based models and abstracting a one-residual Meta Mobile Block (MMBlock) for lightweight model design. Following neat but effective design criterion, we deduce a modern Improved Inverted Residual Mobile Block (i2RMB) and improve a hierarchical Efficient MOdel (EMOv2) with no elaborate complex structures. Considering the imperceptible latency for mobile users when downloading models under 4G/5G bandwidth and ensuring model performance, we investigate the performance upper limit of lightweight models with a magnitude of 5M. Extensive experiments on various vision recognition, dense prediction, and image generation tasks demonstrate the superiority of our EMOv2 over state-of-the-art methods, e.g., EMOv2-1M/2M/5M achieve 72.3, 75.8, and 79.4 Top-1 that surpass equal-order CNN-/Attention-based models significantly. At the same time, EMOv2-5M equipped RetinaNet achieves 41.5 mAP for object detection tasks that surpasses the previous EMO-5M by +2.6. When employing the more robust training recipe, our EMOv2-5M eventually achieves 82.9 Top-1 accuracy, which elevates the performance of 5M magnitude models to a new level. Code is available at https://github.com/zhangzjn/EMOv2.",
            "score": 8,
            "issue_id": 1061,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "a890d1e09015a7a6",
            "authors": [
                "Jiangning Zhang",
                "Teng Hu",
                "Haoyang He",
                "Zhucun Xue",
                "Yabiao Wang",
                "Chengjie Wang",
                "Yong Liu",
                "Xiangtai Li",
                "Dacheng Tao"
            ],
            "affiliations": [
                "Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",
                "Nanyang Technological University, Singapore",
                "Shanghai Jiao Tong University, Shanghai, China",
                "Youtu Lab, Tencent, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06674.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "EMOv2: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², FLOP Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº i2RMB, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ±Ğ»Ğ¾ĞºĞµ (IRB) Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ñ… Transformer. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EMOv2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EMOv2 Ñ 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 82.9% Top-1 Ğ½Ğ° ImageNet, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞºĞ¾Ñ€Ğ´ Ğ´Ğ»Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Lightweight Models, Heavyweight Performance!",
                    "desc": "This paper presents a new approach to creating lightweight models for dense prediction tasks in machine learning, focusing on efficiency in terms of parameters and computational cost (FLOPs). The authors introduce a novel architecture called the Improved Inverted Residual Mobile Block (i2RMB) that extends the Inverted Residual Block (IRB) concept to attention-based models. They demonstrate that their Efficient Model version 2 (EMOv2) outperforms existing models in various tasks, achieving impressive accuracy with only 5 million parameters. The results indicate that lightweight models can achieve high performance without complex structures, making them suitable for mobile applications."
                },
                "zh": {
                    "title": "è½»é‡çº§æ¨¡å‹çš„æ–°çªç ´ï¼š5Mé‡çº§çš„é«˜æ•ˆè®¾è®¡",
                    "desc": "æœ¬ç ”ç©¶è‡´åŠ›äºå¼€å‘å‚æ•°é«˜æ•ˆä¸”è½»é‡çº§çš„æ¨¡å‹ï¼Œä»¥å®ç°å¯†é›†é¢„æµ‹ï¼ŒåŒæ—¶åœ¨å‚æ•°ã€FLOPså’Œæ€§èƒ½ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ‘ä»¬æ—¨åœ¨ä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡å»ºç«‹5Mé‡çº§è½»é‡çº§æ¨¡å‹çš„æ–°å‰æ²¿ã€‚æˆ‘ä»¬é‡æ–°æ€è€ƒäº†é«˜æ•ˆçš„åå‘æ®‹å·®å—ï¼ˆIRBï¼‰å’ŒTransformerä¸­çš„å®ç”¨ç»„ä»¶ï¼Œä»ç»Ÿä¸€çš„è§’åº¦æ‰©å±•äº†åŸºäºCNNçš„IRBåˆ°åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œå¹¶æŠ½è±¡å‡ºä¸€ç§è½»é‡çº§æ¨¡å‹è®¾è®¡çš„å…ƒæ®‹å·®ç§»åŠ¨å—ï¼ˆMMBlockï¼‰ã€‚é€šè¿‡ç®€æ´è€Œæœ‰æ•ˆçš„è®¾è®¡æ ‡å‡†ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºä¸€ç§ç°ä»£åŒ–çš„æ”¹è¿›åå‘æ®‹å·®ç§»åŠ¨å—ï¼ˆi2RMBï¼‰ï¼Œå¹¶åœ¨æ²¡æœ‰å¤æ‚ç»“æ„çš„æƒ…å†µä¸‹æ”¹è¿›äº†åˆ†å±‚é«˜æ•ˆæ¨¡å‹ï¼ˆEMOv2ï¼‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07759",
            "title": "3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation",
            "url": "https://huggingface.co/papers/2412.07759",
            "abstract": "This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster",
            "score": 7,
            "issue_id": 1064,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "0e2f96c50d396f1d",
            "authors": [
                "Xiao Fu",
                "Xian Liu",
                "Xintao Wang",
                "Sida Peng",
                "Menghan Xia",
                "Xiaoyu Shi",
                "Ziyang Yuan",
                "Pengfei Wan",
                "Di Zhang",
                "Dahua Lin"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07759.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 3DTrajMaster - ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ - Ğ¸Ğ½Ğ¶ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 3D-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ğ° Ğ¿Ñ€Ğ¸ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ 360-Motion Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Mastering 3D Motion Control in Video Generation",
                    "desc": "This paper presents 3DTrajMaster, a novel approach for controlling multi-entity motions in 3D video generation. Unlike previous methods that rely on 2D control signals, 3DTrajMaster utilizes 6DoF pose sequences to accurately represent the 3D dynamics of objects. The method incorporates a gated self-attention mechanism to integrate multiple entities with their 3D trajectories, enhancing the realism of generated videos. Additionally, the authors introduce a 360-Motion Dataset to improve training data quality, leading to superior performance in both accuracy and generalization for 3D motion control."
                },
                "zh": {
                    "title": "æŒæ§ä¸‰ç»´è¿åŠ¨ï¼Œé‡å¡‘è§†é¢‘ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æ—¨åœ¨æ“æ§è§†é¢‘ç”Ÿæˆä¸­çš„å¤šå®ä½“ä¸‰ç»´è¿åŠ¨ã€‚ä»¥å¾€çš„å¯æ§è§†é¢‘ç”Ÿæˆæ–¹æ³•ä¸»è¦ä¾èµ–äºŒç»´æ§åˆ¶ä¿¡å·æ¥æ“æ§ç‰©ä½“è¿åŠ¨ï¼Œä½†äºŒç»´ä¿¡å·åœ¨è¡¨è¾¾ä¸‰ç»´è¿åŠ¨ç‰¹æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†3DTrajMasterï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„æ§åˆ¶å™¨ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·æœŸæœ›çš„å…­è‡ªç”±åº¦å§¿æ€åºåˆ—è°ƒèŠ‚ä¸‰ç»´ç©ºé—´ä¸­çš„å¤šå®ä½“åŠ¨æ€ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡ä¸€ä¸ªæ’ä»¶å¼çš„ä¸‰ç»´è¿åŠ¨åŸºç¡€ç‰©ä½“æ³¨å…¥å™¨ï¼Œç»“åˆå¤šä¸ªè¾“å…¥å®ä½“åŠå…¶ç›¸åº”çš„ä¸‰ç»´è½¨è¿¹ï¼Œåˆ©ç”¨é—¨æ§è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07724",
            "title": "Granite Guardian",
            "url": "https://huggingface.co/papers/2412.07724",
            "abstract": "We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, and hallucination-related risks such as context relevance, groundedness, and answer relevance for retrieval-augmented generation (RAG). Trained on a unique dataset combining human annotations from diverse sources and synthetic data, Granite Guardian models address risks typically overlooked by traditional risk detection models, such as jailbreaks and RAG-specific issues. With AUC scores of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks respectively, Granite Guardian is the most generalizable and competitive model available in the space. Released as open-source, Granite Guardian aims to promote responsible AI development across the community.   https://github.com/ibm-granite/granite-guardian",
            "score": 7,
            "issue_id": 1058,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "9d3367b124b8b792",
            "authors": [
                "Inkit Padhi",
                "Manish Nagireddy",
                "Giandomenico Cornacchia",
                "Subhajit Chaudhury",
                "Tejaswini Pedapati",
                "Pierre Dognin",
                "Keerthiram Murugesan",
                "Erik Miehling",
                "MartÃ­n SantillÃ¡n Cooper",
                "Kieran Fraser",
                "Giulio Zizzo",
                "Muhammad Zaid Hameed",
                "Mark Purcell",
                "Michael Desmond",
                "Qian Pan",
                "Inge Vejsbjerg",
                "Elizabeth M. Daly",
                "Michael Hind",
                "Werner Geyer",
                "Ambrish Rawat",
                "Kush R. Varshney",
                "Prasanna Sattigeri"
            ],
            "affiliations": [
                "IBM Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07724.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#hallucinations",
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#ethics",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ğ¸Ğº Ğ˜Ğ˜: Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Granite Guardian - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ñ€Ğ¸ÑĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½ĞµĞ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ»ĞµĞºÑĞ¸ĞºÑƒ, Ğ½Ğ°ÑĞ¸Ğ»Ğ¸Ğµ, ÑĞµĞºÑÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ğ½ĞµÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ¸ÑĞºĞ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Granite Guardian Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ². ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Granite Guardian: Safeguarding AI with Comprehensive Risk Detection",
                    "desc": "The Granite Guardian models are designed to enhance the safety of large language models (LLMs) by detecting various risks in prompts and responses. They cover a wide range of risk factors, including social bias, profanity, and hallucination-related issues, which are often missed by traditional models. These models are trained on a unique dataset that combines human annotations and synthetic data, making them effective at identifying risks like jailbreaking and retrieval-augmented generation (RAG) problems. With high AUC scores, Granite Guardian represents a significant advancement in responsible AI development and is available as open-source for community use."
                },
                "zh": {
                    "title": "Granite Guardianï¼šå®‰å…¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å®ˆæŠ¤è€…",
                    "desc": "Granite Guardianæ¨¡å‹æ˜¯ä¸€å¥—æ—¨åœ¨æä¾›é£é™©æ£€æµ‹çš„å®‰å…¨ä¿éšœå·¥å…·ï¼Œé€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨å’Œè´Ÿè´£ä»»ä½¿ç”¨ã€‚è¿™äº›æ¨¡å‹è¦†ç›–å¤šä¸ªé£é™©ç»´åº¦ï¼ŒåŒ…æ‹¬ç¤¾ä¼šåè§ã€ç²—ä¿—è¯­è¨€ã€æš´åŠ›ã€æ€§å†…å®¹ã€ä¸é“å¾·è¡Œä¸ºã€è¶Šç‹±å’Œå¹»è§‰ç›¸å…³é£é™©ã€‚Granite Guardianæ¨¡å‹é€šè¿‡ç»“åˆæ¥è‡ªå¤šç§æ¥æºçš„äººç±»æ³¨é‡Šå’Œåˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè§£å†³äº†ä¼ ç»Ÿé£é™©æ£€æµ‹æ¨¡å‹é€šå¸¸å¿½è§†çš„é£é™©ã€‚ä½œä¸ºå¼€æºé¡¹ç›®ï¼ŒGranite Guardianæ—¨åœ¨ä¿ƒè¿›ç¤¾åŒºå†…è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06673",
            "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
            "url": "https://huggingface.co/papers/2412.06673",
            "abstract": "In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining -- over four times fewer than what is typically needed -- while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing.",
            "score": 7,
            "issue_id": 1058,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "a8071141959ac48a",
            "authors": [
                "Chunwei Wang",
                "Guansong Lu",
                "Junwei Yang",
                "Runhui Huang",
                "Jianhua Han",
                "Lu Hou",
                "Wei Zhang",
                "Hang Xu"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06673.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#dataset",
                    "#benchmark",
                    "#interpretability",
                    "#training",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ILLUME: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "ILLUME - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ° ÑÑ…ĞµĞ¼Ğ° ÑĞ°Ğ¼Ğ¾ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ILLUME Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "ILLUME: Efficient Multimodal Mastery in One Model",
                    "desc": "The paper presents ILLUME, a unified multimodal large language model (MLLM) that combines understanding and generation of both text and images. It introduces a vision tokenizer that uses semantic information to improve data efficiency, allowing for effective training with a smaller dataset of only 15 million samples. The model employs a self-enhancing multimodal alignment scheme to ensure that the generated images accurately reflect the text descriptions, reducing errors in image generation. Through extensive testing, ILLUME demonstrates competitive performance against existing models in multimodal tasks."
                },
                "zh": {
                    "title": "ILLUMEï¼šå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ILLUMEï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒé€šè¿‡ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å…¬å¼ï¼Œå°†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ— ç¼é›†æˆåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ã€‚ä¸ºäº†åº”å¯¹é€šå¸¸éœ€è¦çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è§†è§‰æ ‡è®°å™¨ï¼Œç»“åˆäº†è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼å¤šé˜¶æ®µè®­ç»ƒç¨‹åºï¼Œä»è€Œå°†é¢„è®­ç»ƒæ‰€éœ€çš„æ•°æ®é›†å¤§å°å‡å°‘åˆ°ä»…1500ä¸‡ï¼Œè¿œä½äºé€šå¸¸æ‰€éœ€çš„æ•°é‡ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä¸ç°æœ‰çš„ç»Ÿä¸€MLLMï¼ˆå¦‚Janusï¼‰ç«äº‰æˆ–æ›´ä¼˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæˆ‘å¢å¼ºå¤šæ¨¡æ€å¯¹é½æ–¹æ¡ˆï¼Œç›‘ç£æ¨¡å‹è‡ªæˆ‘è¯„ä¼°æ–‡æœ¬æè¿°ä¸è‡ªç”Ÿæˆå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜å›¾åƒç†è§£çš„å‡†ç¡®æ€§ï¼Œé¿å…å› ç”Ÿæˆå›¾åƒä¸ä¸€è‡´è€Œå¯¼è‡´çš„ä¸ç°å®å’Œé”™è¯¯é¢„æµ‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒILLUMEåœ¨å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„å„ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºï¼Œèƒ½å¤Ÿä¸æœ€å…ˆè¿›çš„ç»Ÿä¸€MLLMå’Œä¸“ä¸šæ¨¡å‹ç«äº‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07721",
            "title": "ObjCtrl-2.5D: Training-free Object Control with Camera Poses",
            "url": "https://huggingface.co/papers/2412.07721",
            "abstract": "This study aims to achieve more precise and versatile object control in image-to-video (I2V) generation. Current methods typically represent the spatial movement of target objects with 2D trajectories, which often fail to capture user intention and frequently produce unnatural results. To enhance control, we present ObjCtrl-2.5D, a training-free object control approach that uses a 3D trajectory, extended from a 2D trajectory with depth information, as a control signal. By modeling object movement as camera movement, ObjCtrl-2.5D represents the 3D trajectory as a sequence of camera poses, enabling object motion control using an existing camera motion control I2V generation model (CMC-I2V) without training. To adapt the CMC-I2V model originally designed for global motion control to handle local object motion, we introduce a module to isolate the target object from the background, enabling independent local control. In addition, we devise an effective way to achieve more accurate object control by sharing low-frequency warped latent within the object's region across frames. Extensive experiments demonstrate that ObjCtrl-2.5D significantly improves object control accuracy compared to training-free methods and offers more diverse control capabilities than training-based approaches using 2D trajectories, enabling complex effects like object rotation. Code and results are available at https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/.",
            "score": 6,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "de32794ee8780d29",
            "authors": [
                "Zhouxia Wang",
                "Yushi Lan",
                "Shangchen Zhou",
                "Chen Change Loy"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07721.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "3D Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ ObjCtrl-2.5D. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 2D Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ObjCtrl-2.5D Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° ĞºĞ°Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¾Ñ‚ Ñ„Ğ¾Ğ½Ğ° Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ."
                },
                "en": {
                    "title": "Revolutionizing Object Control in I2V with 3D Trajectories",
                    "desc": "This paper introduces ObjCtrl-2.5D, a novel approach for enhancing object control in image-to-video (I2V) generation. Unlike traditional methods that rely on 2D trajectories, ObjCtrl-2.5D utilizes 3D trajectories, incorporating depth information to better reflect user intentions. By treating object movement as camera movement, it allows for precise control without the need for additional training. The method also includes a module for isolating the target object from the background, enabling more accurate and diverse control over object motion, including complex effects like rotation."
                },
                "zh": {
                    "title": "æå‡å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ä½“æ§åˆ¶ç²¾åº¦ä¸å¤šæ ·æ€§",
                    "desc": "æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ä½“æ§åˆ¶ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨äºŒç»´è½¨è¿¹è¡¨ç¤ºç›®æ ‡ç‰©ä½“çš„ç©ºé—´è¿åŠ¨ï¼Œéš¾ä»¥æ•æ‰ç”¨æˆ·æ„å›¾ï¼Œä¸”å¸¸å¸¸äº§ç”Ÿä¸è‡ªç„¶çš„ç»“æœã€‚æˆ‘ä»¬æå‡ºäº†ObjCtrl-2.5Dï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„ç‰©ä½“æ§åˆ¶æ–¹æ³•ï¼Œåˆ©ç”¨å¸¦æ·±åº¦ä¿¡æ¯çš„ä¸‰ç»´è½¨è¿¹ä½œä¸ºæ§åˆ¶ä¿¡å·ã€‚é€šè¿‡å°†ç‰©ä½“è¿åŠ¨å»ºæ¨¡ä¸ºç›¸æœºè¿åŠ¨ï¼ŒObjCtrl-2.5Dèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨ç°æœ‰çš„ç›¸æœºè¿åŠ¨æ§åˆ¶æ¨¡å‹å®ç°ç‰©ä½“è¿åŠ¨æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05148",
            "title": "LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation",
            "url": "https://huggingface.co/papers/2412.05148",
            "abstract": "Recent advancements in image generation models have enabled personalized image creation with both user-defined subjects (content) and styles. Prior works achieved personalization by merging corresponding low-rank adaptation parameters (LoRAs) through optimization-based methods, which are computationally demanding and unsuitable for real-time use on resource-constrained devices like smartphones. To address this, we introduce LoRA.rar, a method that not only improves image quality but also achieves a remarkable speedup of over 4000times in the merging process. LoRA.rar pre-trains a hypernetwork on a diverse set of content-style LoRA pairs, learning an efficient merging strategy that generalizes to new, unseen content-style pairs, enabling fast, high-quality personalization. Moreover, we identify limitations in existing evaluation metrics for content-style quality and propose a new protocol using multimodal large language models (MLLM) for more accurate assessment. Our method significantly outperforms the current state of the art in both content and style fidelity, as validated by MLLM assessments and human evaluations.",
            "score": 4,
            "issue_id": 1066,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "ef1bd7ea8522423b",
            "authors": [
                "Donald Shenaj",
                "Ondrej Bohdal",
                "Mete Ozay",
                "Pietro Zanuttigh",
                "Umberto Michieli"
            ],
            "affiliations": [
                "Samsung R&D Institute (SRUK)",
                "University of Padova"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05148.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#synthetic",
                    "#benchmark",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "LoRA.rar: Ğ¡Ğ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ LoRA.rar Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² LoRA Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LoRA.rar Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ."
                },
                "en": {
                    "title": "Revolutionizing Image Personalization with Lightning Speed",
                    "desc": "This paper presents LoRA.rar, a novel approach for personalized image generation that significantly enhances both speed and quality. Unlike previous methods that relied on computationally intensive optimization to merge low-rank adaptation parameters (LoRAs), LoRA.rar achieves over 4000 times faster merging by utilizing a pre-trained hypernetwork. This hypernetwork learns to efficiently combine content and style from diverse LoRA pairs, allowing for quick adaptation to new combinations. Additionally, the authors propose a new evaluation protocol using multimodal large language models to better assess the quality of generated images, demonstrating that their method surpasses existing techniques in both content and style fidelity."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„å¿«é€Ÿè§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ€è¿‘ï¼Œå›¾åƒç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ä½¿å¾—ä¸ªæ€§åŒ–å›¾åƒåˆ›å»ºæˆä¸ºå¯èƒ½ï¼Œç”¨æˆ·å¯ä»¥å®šä¹‰å›¾åƒçš„å†…å®¹å’Œé£æ ¼ã€‚ä»¥å¾€çš„ä¸ªæ€§åŒ–æ–¹æ³•é€šè¿‡ä¼˜åŒ–åˆå¹¶ä½ç§©é€‚åº”å‚æ•°ï¼ˆLoRAsï¼‰ï¼Œä½†è¿™ç§æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œä¸é€‚åˆåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šå®æ—¶ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LoRA.rarï¼Œè¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å›¾åƒè´¨é‡ï¼Œè¿˜åœ¨åˆå¹¶è¿‡ç¨‹ä¸­å®ç°äº†è¶…è¿‡4000å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°åè®®ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥æ›´å‡†ç¡®åœ°è¯„ä¼°å†…å®¹å’Œé£æ ¼çš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06845",
            "title": "Fully Open Source Moxin-7B Technical Report",
            "url": "https://huggingface.co/papers/2412.06845",
            "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be \"open-source,\" which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of \"open science\" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation.",
            "score": 4,
            "issue_id": 1058,
            "pub_date": "2024-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "410471f06d9e6883",
            "authors": [
                "Pu Zhao",
                "Xuan Shen",
                "Zhenglun Kong",
                "Yixin Shen",
                "Sung-En Chang",
                "Timothy Rupprecht",
                "Lei Lu",
                "Enfu Nan",
                "Changdi Yang",
                "Yumei He",
                "Xingchen Xu",
                "Yu Huang",
                "Wei Wang",
                "Yue Chen",
                "Yong He",
                "Yanzhi Wang"
            ],
            "affiliations": [
                "AIBAO LLC",
                "Cornell University",
                "Futurewei Technologies",
                "Harvard University",
                "Northeastern University",
                "Roboraction.ai",
                "Tulane University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06845.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#open_source",
                    "#ethics",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ”“",
                "ru": {
                    "title": "Moxin 7B: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ LLM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Moxin 7B - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ ĞœĞ¾Ğ´ĞµĞ»ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ÑÑ‚Ğ¸ (MOF). Moxin 7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑÑˆĞµĞ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ MOF Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ 7B-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² zero-shot Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ² few-shot Ğ¾Ñ†ĞµĞ½ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Moxin 7B: Leading the Way in Open-Source Language Models",
                    "desc": "This paper discusses the evolution of Large Language Models (LLMs), highlighting the contrast between proprietary models like GPT-4 and open-source alternatives such as LLaMA. It emphasizes the importance of transparency and reproducibility in AI, noting that many open-source models do not fully disclose their training processes or data. To address these issues, the authors introduce Moxin 7B, an open-source LLM that adheres to the Model Openness Framework (MOF), ensuring comprehensive access to its training code and datasets. The results demonstrate that Moxin 7B outperforms other 7B models in zero-shot tasks and remains competitive in few-shot scenarios, showcasing the potential of fully open-source LLMs."
                },
                "zh": {
                    "title": "Moxin 7Bï¼šå¼€æºè¯­è¨€æ¨¡å‹çš„æ–°æ ‡æ†",
                    "desc": "æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»å†äº†æ˜¾è‘—çš„å˜é©ï¼Œå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ä»¥GPT-4å’ŒGPT-o1ä¸ºä»£è¡¨çš„ä¸“æœ‰LLMså±•ç°äº†å“è¶Šçš„æ€§èƒ½å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶å¼€æºLLMså¦‚LLaMAå’ŒMistralä¹Ÿå› å…¶æ˜“äºå®šåˆ¶å’Œéƒ¨ç½²è€Œå—åˆ°é’çã€‚å°½ç®¡å¼€æºLLMsä¸ºåˆ›æ–°å’Œç ”ç©¶æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šï¼Œä½†å…¶å•†ä¸šåŒ–å¸¦æ¥äº†é€æ˜æ€§ã€å¯é‡å¤æ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Moxin 7Bï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„LLMï¼Œéµå¾ªæ¨¡å‹å¼€æ”¾æ¡†æ¶ï¼ˆMOFï¼‰ï¼Œå¹¶åœ¨é€æ˜æ€§å’Œå¼€æ”¾æ€§æ–¹é¢è¾¾åˆ°äº†æœ€é«˜æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07338",
            "title": "Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation",
            "url": "https://huggingface.co/papers/2412.07338",
            "abstract": "AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.",
            "score": 2,
            "issue_id": 1066,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "8b3e1c2da99f56d4",
            "authors": [
                "Lorenzo Cima",
                "Alessio Miaschi",
                "Amaury Trujillo",
                "Marco Avvenuti",
                "Felice Dell'Orletta",
                "Stefano Cresci"
            ],
            "affiliations": [
                "IIT-CNR",
                "ILC-CNR",
                "University of Pisa"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07338.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#ethics",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "ğŸ—¨ï¸",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑÑ‚Ğ¸ĞºĞµÑ‚Ğ°: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaMA2-13B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑĞ»Ğ°Ğ±ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Tailored AI Counterspeech: A New Era in Online Discourse",
                    "desc": "This paper discusses a new approach to generating counterspeech using AI to combat online toxicity. The authors focus on creating personalized and context-aware responses rather than generic replies, which can be more effective in promoting civil discourse. They utilize the LLaMA2-13B model and test various configurations to enhance the persuasiveness of the generated counterspeech. The study finds that tailored responses significantly outperform standard methods, highlighting the need for better evaluation techniques that consider both quantitative metrics and human feedback."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–åè¨€è®ºï¼šæå‡åœ¨çº¿äº¤æµçš„æœ‰æ•ˆæ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„åè¨€è®ºç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡ç›´æ¥å›å¤æ¥å‡å°‘åœ¨çº¿æ¯’æ€§ã€‚å½“å‰çš„åè¨€è®ºæ–¹æ³•ç¼ºä¹é’ˆå¯¹æ€§ï¼Œæ— æ³•é€‚åº”ä¸åŒçš„ç®¡ç†ç¯å¢ƒå’Œç”¨æˆ·éœ€æ±‚ã€‚æˆ‘ä»¬ä½¿ç”¨LLaMA2-13Bæ¨¡å‹ç”Ÿæˆä¸ªæ€§åŒ–çš„åè¨€è®ºï¼Œå¹¶é€šè¿‡å¤šç§é…ç½®è¿›è¡Œå®éªŒï¼Œä»¥è¯„ä¼°å…¶æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹ç‰¹å®šä¸Šä¸‹æ–‡çš„åè¨€è®ºåœ¨é€‚å½“æ€§å’Œè¯´æœåŠ›ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é€šç”¨åè¨€è®ºï¼Œå¼ºè°ƒäº†äººæœºåä½œåœ¨å†…å®¹ç®¡ç†ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04835",
            "title": "Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment",
            "url": "https://huggingface.co/papers/2412.04835",
            "abstract": "Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment.",
            "score": 2,
            "issue_id": 1064,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "1fe129433c4c71a5",
            "authors": [
                "Ran Tian",
                "Yilin Wu",
                "Chenfeng Xu",
                "Masayoshi Tomizuka",
                "Jitendra Malik",
                "Andrea Bajcsy"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04835.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ RAPL Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF), RAPL Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. RAPL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² 5 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Efficiently Aligning Robot Policies with Minimal Human Feedback",
                    "desc": "This paper introduces Representation-Aligned Preference-based Learning (RAPL), a novel approach to align visuomotor robot policies with human preferences using minimal feedback. RAPL improves upon traditional reinforcement learning from human feedback (RLHF) by focusing on fine-tuning pre-trained vision encoders, allowing for the construction of dense visual rewards through feature matching. The method is validated through simulations and hardware experiments, showing that it can effectively learn rewards that align with human preferences while requiring significantly less human input. Overall, RAPL represents a significant step towards efficient alignment of robot policies in various manipulation tasks."
                },
                "zh": {
                    "title": "å‡å°‘äººç±»åé¦ˆï¼Œå®ç°æœºå™¨äººç­–ç•¥å¯¹é½çš„çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºè¡¨ç¤ºå¯¹é½çš„åå¥½å­¦ä¹ ï¼ˆRAPLï¼‰ï¼Œæ—¨åœ¨å‡å°‘å¯¹äººç±»åé¦ˆçš„éœ€æ±‚ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¯¹é½è§†è§‰è¿åŠ¨æœºå™¨äººç­–ç•¥ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡çš„äººç±»åé¦ˆæ¥å­¦ä¹ è§†è§‰å¥–åŠ±å‡½æ•°ï¼Œè€ŒRAPLé€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç‰¹å¾åŒ¹é…åœ¨å¯¹é½çš„è¡¨ç¤ºç©ºé—´ä¸­æ„å»ºå¯†é›†çš„è§†è§‰å¥–åŠ±ï¼Œä»è€Œæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAPLèƒ½å¤Ÿåœ¨å‡å°‘äººç±»åå¥½æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°å¯¹é½æœºå™¨äººç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05983",
            "title": "Chimera: Improving Generalist Model with Domain-Specific Experts",
            "url": "https://huggingface.co/papers/2412.05983",
            "abstract": "Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. To address these challenges, we introduce Chimera, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs.",
            "score": 2,
            "issue_id": 1064,
            "pub_date": "2024-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "991103a0d9d85ad4",
            "authors": [
                "Tianshuo Peng",
                "Mingsheng Li",
                "Hongbin Zhou",
                "Renqiu Xia",
                "Renrui Zhang",
                "Lei Bai",
                "Song Mao",
                "Bin Wang",
                "Conghui He",
                "Aojun Zhou",
                "Botian Shi",
                "Tao Chen",
                "Bo Zhang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "Fudan University",
                "MMLab, The Chinese University of Hong Kong",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05983.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#cv",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Chimera: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ LMM Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Chimera - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ LMM. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (GSCM). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Chimera: Bridging Generalist and Specialist Models for Enhanced Multi-modal Learning",
                    "desc": "This paper presents Chimera, a new approach to enhance Large Multi-modal Models (LMMs) by integrating domain-specific expert models. The authors highlight that while LMMs perform well on general tasks, they struggle with specialized tasks due to their training on broad datasets. To improve this, Chimera employs a progressive training strategy that incorporates expert features into the generalist model's input. Additionally, the Generalist-Specialist Collaboration Masking (GSCM) mechanism is introduced to balance optimization between the generalist and specialist models, leading to superior performance in multi-modal reasoning and visual content extraction tasks."
                },
                "zh": {
                    "title": "Chimeraï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„é¢†åŸŸä¸“é•¿",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºChimeraçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸçš„èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥é¢†åŸŸä¸“å®¶æ¨¡å‹çš„ç‰¹å¾ï¼ŒChimeraé‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œè§£å†³äº†é€šç”¨æ¨¡å‹ä¸ä¸“å®¶æ¨¡å‹ä¹‹é—´çš„è¡¨ç¤ºå·®è·å’Œä¼˜åŒ–ä¸å¹³è¡¡é—®é¢˜ã€‚ç‰¹åˆ«åœ°ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨-ä¸“å®¶åä½œæ©ç æœºåˆ¶ï¼ˆGSCMï¼‰ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚æœ€ç»ˆï¼ŒChimeraåœ¨å›¾è¡¨ã€è¡¨æ ¼ã€æ•°å­¦å’Œæ–‡æ¡£ç­‰é¢†åŸŸçš„å¤šæ¨¡æ€æ¨ç†å’Œè§†è§‰å†…å®¹æå–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07187",
            "title": "A New Federated Learning Framework Against Gradient Inversion Attacks",
            "url": "https://huggingface.co/papers/2412.07187",
            "abstract": "Federated Learning (FL) aims to protect data privacy by enabling clients to collectively train machine learning models without sharing their raw data. However, recent studies demonstrate that information exchanged during FL is subject to Gradient Inversion Attacks (GIA) and, consequently, a variety of privacy-preserving methods have been integrated into FL to thwart such attacks, such as Secure Multi-party Computing (SMC), Homomorphic Encryption (HE), and Differential Privacy (DP). Despite their ability to protect data privacy, these approaches inherently involve substantial privacy-utility trade-offs. By revisiting the key to privacy exposure in FL under GIA, which lies in the frequent sharing of model gradients that contain private data, we take a new perspective by designing a novel privacy preserve FL framework that effectively ``breaks the direct connection'' between the shared parameters and the local private data to defend against GIA. Specifically, we propose a Hypernetwork Federated Learning (HyperFL) framework that utilizes hypernetworks to generate the parameters of the local model and only the hypernetwork parameters are uploaded to the server for aggregation. Theoretical analyses demonstrate the convergence rate of the proposed HyperFL, while extensive experimental results show the privacy-preserving capability and comparable performance of HyperFL. Code is available at https://github.com/Pengxin-Guo/HyperFL.",
            "score": 1,
            "issue_id": 1061,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "772d62408694e82b",
            "authors": [
                "Pengxin Guo",
                "Shuang Zeng",
                "Wenhao Chen",
                "Xiaodan Zhang",
                "Weihong Ren",
                "Yuyin Zhou",
                "Liangqiong Qu"
            ],
            "affiliations": [
                "College of Computer Science, Beijing University of Technology",
                "Department of Computer Science and Engineering, UC Santa Cruz",
                "Department of Mathematics, The University of Hong Kong",
                "School of Computing and Data Science, The University of Hong Kong",
                "School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07187.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#healthcare",
                    "#ethics",
                    "#security",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (FL), Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° (GIA). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº HyperFL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ° ÑĞµÑ€Ğ²ĞµÑ€ Ğ´Ğ»Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° HyperFL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ HyperFL ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Privacy in Federated Learning with Hypernetworks",
                    "desc": "Federated Learning (FL) allows multiple clients to train machine learning models while keeping their data private. However, it is vulnerable to Gradient Inversion Attacks (GIA), which can expose sensitive information through shared model gradients. To address this, the paper introduces a new framework called Hypernetwork Federated Learning (HyperFL), which uses hypernetworks to generate model parameters, ensuring that only hypernetwork parameters are shared. This approach effectively reduces the risk of privacy breaches while maintaining strong performance and convergence rates in training."
                },
                "zh": {
                    "title": "è¶…ç½‘ç»œè”é‚¦å­¦ä¹ ï¼šä¿æŠ¤éšç§çš„æ–°æ–¹æ³•",
                    "desc": "è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ—¨åœ¨é€šè¿‡è®©å®¢æˆ·ç«¯å…±åŒè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹è€Œä¸å…±äº«åŸå§‹æ•°æ®æ¥ä¿æŠ¤æ•°æ®éšç§ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨FLè¿‡ç¨‹ä¸­äº¤æ¢çš„ä¿¡æ¯å¯èƒ½ä¼šå—åˆ°æ¢¯åº¦åæ¼”æ”»å‡»ï¼ˆGIAï¼‰çš„å¨èƒï¼Œå› æ­¤è®¸å¤šéšç§ä¿æŠ¤æ–¹æ³•è¢«æ•´åˆè¿›FLä¸­ä»¥æŠµå¾¡è¿™äº›æ”»å‡»ã€‚è¿™äº›æ–¹æ³•å¦‚å®‰å…¨å¤šæ–¹è®¡ç®—ï¼ˆSMCï¼‰ã€åŒæ€åŠ å¯†ï¼ˆHEï¼‰å’Œå·®åˆ†éšç§ï¼ˆDPï¼‰è™½ç„¶èƒ½ä¿æŠ¤æ•°æ®éšç§ï¼Œä½†é€šå¸¸ä¼šæ¶‰åŠæ˜¾è‘—çš„éšç§ä¸æ•ˆç”¨ä¹‹é—´çš„æƒè¡¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„éšç§ä¿æŠ¤FLæ¡†æ¶â€”â€”è¶…ç½‘ç»œè”é‚¦å­¦ä¹ ï¼ˆHyperFLï¼‰ï¼Œé€šè¿‡è®¾è®¡è¶…ç½‘ç»œç”Ÿæˆæœ¬åœ°æ¨¡å‹å‚æ•°ï¼Œä»è€Œæœ‰æ•ˆâ€œæ‰“ç ´â€å…±äº«å‚æ•°ä¸æœ¬åœ°ç§æœ‰æ•°æ®ä¹‹é—´çš„ç›´æ¥è”ç³»ï¼Œä»¥æŠµå¾¡GIAã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-10.html",
    "link_next": "2024-12-12.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.12",
        "en": "12/12",
        "zh": "12æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 8,
        "#rl": 0,
        "#rlhf": 2,
        "#rag": 2,
        "#plp": 1,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 12,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 12,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 5,
        "#security": 2,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 4,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆcodeLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ä»¥å‰çš„ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ç‰‡æ®µï¼Œä½†å¿½ç•¥äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCodeArenaçš„ä¸¥æ ¼äººå·¥ç¼–åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œä½œè€…å‘ç°åœ¨å¼€æºä»£ç LLMså’Œä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†äººç±»åå¥½ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚",
        "title": "Evaluating and Aligning CodeLLMs on Human Preference",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆcodeLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ä»¥å‰çš„ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ç‰‡æ®µï¼Œä½†å¿½ç•¥äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCodeArenaçš„ä¸¥æ ¼äººå·¥ç¼–åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œä½œè€…å‘ç°åœ¨å¼€æºä»£ç LLMså’Œä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†äººç±»åå¥½ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ i mÇ dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (codeLLMs) zÃ i dÃ i mÇ shÄ“ng chÃ©ng fÄng miÃ n de jÃ¬n zhÃ n. yÇ qiÃ¡n de dÃ i mÇ xiÄng guÄn jÄ« zhÇ”n cÃ¨ shÃ¬ zhÇ” yÃ o guÄn zhÃ¹ shÄ“ng chÃ©ng zhÃ¨ng quÃ¨ de dÃ i mÇ piÃ n duÃ n, dÃ n hÅ« lÃ¼Ã¨ le yÇ” rÃ©n lÃ¨i piÄn hÃ o de yÄ« zhÃ¬ xÃ¬ng. wÃ¨i le mÃ­ bÇ” zhÃ¨ yÄ« chÄ jÃ¹, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« gÃ¨ mÃ­ng wÃ¨i CodeArena de yÃ¡n gÃ© rÃ©n gÅng biÄn zhÃ¬ jÄ« zhÇ”n cÃ¨ shÃ¬, mÃ³ nÇ zhÄ“n shÃ­ shÃ¬ jiÃ¨ biÄn mÇ rÃ¨n wÃ¹ de fÃº zÃ  xÃ¬ng hÃ© duÅ yÃ ng xÃ¬ng. tÅng guÃ² xÃ¬ tÇ’ng shÃ­ yÃ n, zuÃ² zhÄ› fÄ xiÃ n zÃ i kÄi yuÃ¡n dÃ i mÇ LLMs hÃ© zhuÄn yÇ’u LLMs zhÄ« jiÄn cÃºn zÃ i xiÇn zhÃ¹ de xÃ¬ng nÃ©ng chÄ jÃ¹, qiÃ¡ng diÃ o le rÃ©n lÃ¨i piÄn hÃ o yÄ« zhÃ¬ xÃ¬ng de zhÃ²ng yÃ o xÃ¬ng.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'},\n{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},\n{'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'},\n{'word': 'ä»¥å‰', 'pinyin': 'yÇ qiÃ¡n', 'trans': 'before'},\n{'word': 'ç›¸å…³', 'pinyin': 'xiÄng guÄn', 'trans': 'related'},\n{'word': 'åŸºå‡†æµ‹è¯•', 'pinyin': 'jÄ« zhÇ”n cÃ¨ shÃ¬', 'trans': 'benchmark test'},\n{'word': 'ä¸»è¦', 'pinyin': 'zhÇ” yÃ o', 'trans': 'main'},\n{'word': 'å…³æ³¨', 'pinyin': 'guÄn zhÃ¹', 'trans': 'focus on'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'},\n{'word': 'æ­£ç¡®', 'pinyin': 'zhÃ¨ng quÃ¨', 'trans': 'correct'},\n{'word': 'ç‰‡æ®µ', 'pinyin': 'piÃ n duÃ n', 'trans': 'segment'},\n{'word': 'å¿½ç•¥', 'pinyin': 'hÅ« lÃ¼Ã¨', 'trans': 'ignore'},\n{'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'},\n{'word': 'å¼¥è¡¥', 'pinyin': 'mÃ­ bÇ”', 'trans': 'make up for'},\n{'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'ä¸¥æ ¼', 'pinyin': 'yÃ¡n gÃ©', 'trans': 'strict'},\n{'word': 'äººå·¥ç¼–åˆ¶', 'pinyin': 'rÃ©n gÅng biÄn zhÃ¬', 'trans': 'artificially compiled'},\n{'word': 'æ¨¡æ‹Ÿ', 'pinyin': 'mÃ³ nÇ', 'trans': 'simulate'},\n{'word': 'çœŸå®ä¸–ç•Œ', 'pinyin': 'zhÄ“n shÃ­ shÃ¬ jiÃ¨', 'trans': 'real world'},\n{'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'},\n{'word': 'å¤šæ ·æ€§', 'pinyin': 'duÅ yÃ ng xÃ¬ng', 'trans': 'diversity'},\n{'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'å¼€æº', 'pinyin': 'kÄi yuÃ¡n', 'trans': 'open source'},\n{'word': 'ä¸“æœ‰', 'pinyin': 'zhuÄn yÇ’u', 'trans': 'proprietary'},\n{'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'å¼ºè°ƒ', 'pinyin': 'qiÃ¡ng diÃ o', 'trans': 'emphasize'},\n{'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'}]",
        "trans": "This article discusses the advancements in code generation by code large language models (codeLLMs). Previous code-related benchmark tests primarily focused on generating correct code snippets but overlooked consistency with human preferences. To address this gap, the authors propose a rigorous, human-crafted benchmark test called CodeArena, which simulates the complexity and diversity of real-world coding tasks. Through systematic experiments, the authors found a significant performance gap between open-source codeLLMs and proprietary LLMs, highlighting the importance of consistency with human preferences.",
        "update_ts": "2024-12-11 09:11"
    }
}