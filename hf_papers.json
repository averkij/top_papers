{
    "date": {
        "ru": "12 Ğ¼Ğ°Ñ",
        "en": "May 12",
        "zh": "5æœˆ12æ—¥"
    },
    "time_utc": "2025-05-12 07:12",
    "weekday": 0,
    "issue_id": 3705,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.05026",
            "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
            "url": "https://huggingface.co/papers/2505.05026",
            "abstract": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, a principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasiveness-the key factor in optimizing user interactions. To address this, we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly.",
            "score": 3,
            "issue_id": 3705,
            "pub_date": "2025-05-08",
            "pub_date_card": {
                "ru": "8 Ğ¼Ğ°Ñ",
                "en": "May 8",
                "zh": "5æœˆ8æ—¥"
            },
            "hash": "41e61eccd430ea55",
            "authors": [
                "Jaehyun Jeon",
                "Jang Han Yoon",
                "Min Soo Kim",
                "Sumin Shim",
                "Yejin Choi",
                "Hanbin Kim",
                "Youngjae Yu"
            ],
            "affiliations": [
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.05026.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ UI Ğ±ĞµĞ· A/B-Ñ‚ĞµÑÑ‚Ğ¾Ğ²",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WiserUI-Bench Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 300 Ğ¿Ğ°Ñ€ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… UI-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ A/B-Ñ‚ĞµÑÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ G-FOCUS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ G-FOCUS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing UI Evaluation with G-FOCUS and WiserUI-Bench",
                    "desc": "This paper discusses the importance of evaluating user interface (UI) design not just for its visual appeal but for its ability to influence user behavior, a concept known as Design Persuasiveness. The authors highlight the limitations of traditional A/B testing, which is often expensive and slow, and propose a new benchmark called WiserUI-Bench for assessing UI design effectiveness through pairwise comparisons. They introduce G-FOCUS, an innovative reasoning strategy that improves the accuracy of Vision-Language Models (VLMs) in evaluating UI persuasiveness by minimizing biases. The results demonstrate that G-FOCUS outperforms existing methods, paving the way for more efficient and scalable UI design optimization."
                },
                "zh": {
                    "title": "æå‡ç”¨æˆ·ç•Œé¢è®¾è®¡çš„è¯´æœåŠ›è¯„ä¼°",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰è®¾è®¡çš„æœ‰æ•ˆæ€§è¯„ä¼°ï¼Œå¼ºè°ƒè®¾è®¡çš„è¯´æœåŠ›å¯¹ç”¨æˆ·è¡Œä¸ºçš„å½±å“ã€‚ä¼ ç»Ÿçš„A/Bæµ‹è¯•æ–¹æ³•è™½ç„¶å¸¸ç”¨ï¼Œä½†æˆæœ¬é«˜ä¸”è€—æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†WiserUI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæˆå¯¹UIè®¾è®¡è¯´æœåŠ›è¯„ä¼°çš„åŸºå‡†ï¼ŒåŒ…å«300å¯¹çœŸå®çš„UIå›¾åƒåŠå…¶A/Bæµ‹è¯•ç»“æœå’Œä¸“å®¶ç†ç”±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†G-FOCUSï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¨ç†ç­–ç•¥ï¼Œèƒ½å¤Ÿæé«˜åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯´æœåŠ›è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.06111",
            "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
            "url": "https://huggingface.co/papers/2505.06111",
            "abstract": "A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.",
            "score": 1,
            "issue_id": 3704,
            "pub_date": "2025-05-09",
            "pub_date_card": {
                "ru": "9 Ğ¼Ğ°Ñ",
                "en": "May 9",
                "zh": "5æœˆ9æ—¥"
            },
            "hash": "bf19981dd100b8fb",
            "authors": [
                "Qingwen Bu",
                "Yanting Yang",
                "Jisong Cai",
                "Shenyuan Gao",
                "Guanghui Ren",
                "Maoqing Yao",
                "Ping Luo",
                "Hongyang Li"
            ],
            "affiliations": [
                "AgiBot",
                "OpenDriveLab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06111.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#training",
                    "#benchmark",
                    "#agents",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹Ğº",
                    "desc": "UniVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. UniVLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ OpenVLA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "UniVLA: Empowering Robots with Cross-Embodiment Learning",
                    "desc": "The paper introduces UniVLA, a framework designed to enhance the capabilities of generalist robots by learning cross-embodiment vision-language-action (VLA) policies. It addresses the limitations of existing methods that depend on large amounts of action-annotated data and are restricted to specific physical forms. By utilizing a latent action model derived from videos, UniVLA can leverage diverse data sources and improve knowledge transfer across different robot embodiments and environments. The framework demonstrates state-of-the-art performance in various tasks while requiring significantly less computational resources and data compared to previous approaches."
                },
                "zh": {
                    "title": "UniVLAï¼šæå‡é€šç”¨æœºå™¨äººå­¦ä¹ æ•ˆç‡çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶UniVLAï¼Œç”¨äºå­¦ä¹ è·¨ä½“ç°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥ï¼Œä»¥æé«˜é€šç”¨æœºå™¨äººåœ¨ä¸åŒç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬é€šè¿‡è§†é¢‘ä¸­çš„æ½œåœ¨åŠ¨ä½œæ¨¡å‹æå–ä»¥ä»»åŠ¡ä¸ºä¸­å¿ƒçš„åŠ¨ä½œè¡¨ç¤ºï¼Œä»è€Œåˆ©ç”¨å¹¿æ³›çš„å¤šæ ·åŒ–æ•°æ®ã€‚ä¸ºäº†å‡å°‘ä¸ä»»åŠ¡æ— å…³çš„åŠ¨æ€å½±å“ï¼Œæˆ‘ä»¬ç»“åˆäº†è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶åœ¨DINOç‰¹å¾ç©ºé—´ä¸­å»ºç«‹äº†æ½œåœ¨åŠ¨ä½œæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniVLAåœ¨å¤šä¸ªæ“ä½œå’Œå¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨é¢„è®­ç»ƒè®¡ç®—å’Œä¸‹æ¸¸æ•°æ®æ–¹é¢çš„éœ€æ±‚æ˜¾è‘—ä½äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-09.html",
    "link_next": "2025-05-13.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "09.05",
        "en": "05/09",
        "zh": "5æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "13.05",
        "en": "05/13",
        "zh": "5æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ¨ç†åœ¨äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾ã€ä¸ç¡®å®šå’Œå¤šæ¨¡æ€ç¯å¢ƒä¸­ã€‚å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ˆLMRMsï¼‰ç»“åˆäº†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰æ¨¡æ€ï¼Œæ”¯æŒå¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ä»æ¨¡å—åŒ–ã€æ„ŸçŸ¥é©±åŠ¨çš„æµæ°´çº¿å‘å±•åˆ°ç»Ÿä¸€çš„ã€ä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„æ¡†æ¶ã€‚æ–‡ç« å›é¡¾äº†æ—©æœŸä»»åŠ¡ç‰¹å®šæ¨¡å—çš„åŠªåŠ›ï¼Œå¹¶æ£€æŸ¥äº†æœ€è¿‘å°†æ¨ç†ç»Ÿä¸€åˆ°å¤šæ¨¡æ€LLMsçš„æ–¹æ³•ï¼Œæœ€åè®¨è®ºäº†æœªæ¥çš„æ–¹å‘ã€‚",
        "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ¨ç†åœ¨äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾ã€ä¸ç¡®å®šå’Œå¤šæ¨¡æ€ç¯å¢ƒä¸­ã€‚å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ˆLMRMsï¼‰ç»“åˆäº†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰æ¨¡æ€ï¼Œæ”¯æŒå¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ä»æ¨¡å—åŒ–ã€æ„ŸçŸ¥é©±åŠ¨çš„æµæ°´çº¿å‘å±•åˆ°ç»Ÿä¸€çš„ã€ä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„æ¡†æ¶ã€‚æ–‡ç« å›é¡¾äº†æ—©æœŸä»»åŠ¡ç‰¹å®šæ¨¡å—çš„åŠªåŠ›ï¼Œå¹¶æ£€æŸ¥äº†æœ€è¿‘å°†æ¨ç†ç»Ÿä¸€åˆ°å¤šæ¨¡æ€LLMsçš„æ–¹æ³•ï¼Œæœ€åè®¨è®ºäº†æœªæ¥çš„æ–¹å‘ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le tuÄ«lÇ zÃ i rÃ©ngÅng zhÃ¬nÃ©ng zhÅng de zhÃ²ngyÃ oxÃ¬ng, tÃ¨biÃ© shÃ¬ zÃ i kÄifÃ ng, bÃ¹ quÃ¨dÃ¬ng hÃ© duÅ mÃ³shÃ¬ huÃ¡njÃ¬ng zhÅng. DÃ xÃ­ng duÅ mÃ³shÃ¬ tuÄ«lÇ mÃ³xÃ­ng (LMRMs) jiÃ©hÃ© le wÃ©nbÄ›n, tÃºxiÃ ng, yÄ«npiÃ n hÃ© shÃ¬pÃ­n dÄ›ng mÃ³shÃ¬, zhÄ«chÃ­ fÃ¹zÃ¡ tuÄ«lÇ nÃ©nglÃ¬. YÃ¡njiÅ« cÃ³ng mÃ³kÃ¹huÃ , gÇnjuÃ© qÅ«dÃ²ng de liÃºshuÇxiÃ n fÄzhÇn dÃ o tÇ’ngyÄ« de, yÇ yÇ”yÃ¡n wÃ©i zhÅngxÄ«n de kuÃ ngjiÃ . WÃ©nzhÄng huÃ­gÃ¹ le zÇoqÄ« rÃ¨nwÃ¹ tÃ¨dÃ¬ng mÃ³kÃ¹ de nÇ”lÃ¬, bÃ¬ng jiÇnchÃ¡ le zuÃ¬jÃ¬n jiÄng tuÄ«lÇ tÇ’ngyÄ« dÃ o duÅ mÃ³shÃ¬ LLMs de fÄngfÇ, zuÃ¬hÃ²u tÇolÃ¹n le wÃ¨ilÃ¡i de fÄngxiÃ ng.",
        "vocab": "[\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"äººå·¥æ™ºèƒ½\", \"pinyin\": \"rÃ©n gÅng zhÃ¬ nÃ©ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"ä¸ç¡®å®š\", \"pinyin\": \"bÃ¹ quÃ¨ dÃ¬ng\", \"trans\": \"uncertain\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¨¡æ€\", \"pinyin\": \"mÃ³ tÃ i\", \"trans\": \"modality\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ© hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ« chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"å¤æ‚\", \"pinyin\": \"fÃ¹ zÃ¡\", \"trans\": \"complex\"},\n    {\"word\": \"æ¨¡å—åŒ–\", \"pinyin\": \"mÃ³ kuÃ i huÃ \", \"trans\": \"modularization\"},\n    {\"word\": \"æ„ŸçŸ¥\", \"pinyin\": \"gÇn zhÄ«\", \"trans\": \"perception\"},\n    {\"word\": \"é©±åŠ¨\", \"pinyin\": \"qÅ« dÃ²ng\", \"trans\": \"drive\"},\n    {\"word\": \"æµæ°´çº¿\", \"pinyin\": \"liÃº shuÇ xiÃ n\", \"trans\": \"pipeline\"},\n    {\"word\": \"ç»Ÿä¸€\", \"pinyin\": \"tÇ’ng yÄ«\", \"trans\": \"unified\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"å›é¡¾\", \"pinyin\": \"huÃ­ gÃ¹\", \"trans\": \"review\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"ç‰¹å®š\", \"pinyin\": \"tÃ¨ dÃ¬ng\", \"trans\": \"specific\"},\n    {\"word\": \"æ£€æŸ¥\", \"pinyin\": \"jiÇn chÃ¡\", \"trans\": \"examine\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"æ–¹å‘\", \"pinyin\": \"fÄng xiÃ ng\", \"trans\": \"direction\"}\n]",
        "trans": "This article discusses the importance of reasoning in artificial intelligence, particularly in open, uncertain, and multimodal environments. Large Multimodal Reasoning Models (LMRMs) integrate modalities such as text, images, audio, and video, supporting complex reasoning capabilities. Research has evolved from modular, perception-driven pipelines to unified, language-centric frameworks. The article reviews early efforts with task-specific modules and examines recent methods that unify reasoning into multimodal LLMs, concluding with a discussion on future directions.",
        "update_ts": "2025-05-11 18:30"
    }
}