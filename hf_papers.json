{
    "date": {
        "ru": "12 декабря",
        "en": "December 12",
        "zh": "12月12日"
    },
    "time_utc": "2024-12-12 05:11",
    "weekday": 3,
    "issue_id": 1082,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.07760",
            "title": "SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints",
            "url": "https://huggingface.co/papers/2412.07760",
            "abstract": "Recent advancements in video diffusion models have shown exceptional abilities in simulating real-world dynamics and maintaining 3D consistency. This progress inspires us to investigate the potential of these models to ensure dynamic consistency across various viewpoints, a highly desirable feature for applications such as virtual filming. Unlike existing methods focused on multi-view generation of single objects for 4D reconstruction, our interest lies in generating open-world videos from arbitrary viewpoints, incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. Specifically, we introduce a multi-view synchronization module to maintain appearance and geometry consistency across these viewpoints. Given the scarcity of high-quality training data, we design a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. Furthermore, our method enables intriguing extensions, such as re-rendering a video from novel viewpoints. We also release a multi-view synchronized video dataset, named SynCamVideo-Dataset. Project page: https://jianhongbai.github.io/SynCamMaster/.",
            "score": 17,
            "issue_id": 1081,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "5ac69027d8ae0669",
            "authors": [
                "Jianhong Bai",
                "Menghan Xia",
                "Xintao Wang",
                "Ziyang Yuan",
                "Xiao Fu",
                "Zuozhu Liu",
                "Haoji Hu",
                "Pengfei Wan",
                "Di Zhang"
            ],
            "affiliations": [
                "CUHK",
                "Kuaishou Technology",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07760.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#3d",
                    "#open_source",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Согласованная генерация видео с множества ракурсов",
                    "desc": "Статья представляет новый подход к генерации мультиракурсных видео с использованием диффузионных моделей. Авторы разработали модуль, который улучшает предобученную модель text-to-video для создания согласованного контента с разных точек обзора. Они применяют гибридную схему обучения, используя мультиракурсные изображения и монокулярные видео в дополнение к рендерам из Unreal Engine. Метод также позволяет перерендерить видео с новых ракурсов и включает выпуск нового датасета SynCamVideo-Dataset."
                },
                "en": {
                    "title": "Dynamic Consistency in Multi-View Video Generation",
                    "desc": "This paper explores the use of video diffusion models to create videos that maintain dynamic consistency from multiple viewpoints, which is important for applications like virtual filming. The authors propose a new module that enhances existing text-to-video models, allowing them to generate videos that are consistent in appearance and geometry across different camera angles. They introduce a multi-view synchronization module to ensure that the content remains coherent, even when viewed from various perspectives. Additionally, they present a hybrid training approach that combines different types of video data to improve the model's performance and release a new dataset for multi-view synchronized videos."
                },
                "zh": {
                    "title": "实现多视角视频的一致性",
                    "desc": "最近视频扩散模型的进展显示出在模拟现实世界动态和保持三维一致性方面的卓越能力。我们研究这些模型在不同视角下确保动态一致性的潜力，这对于虚拟拍摄等应用非常重要。与现有方法不同，我们关注的是从任意视角生成开放世界视频，并引入六自由度相机姿态。为此，我们提出了一个可插拔模块，增强了预训练的文本到视频模型，以实现多相机视频生成，并确保不同视角下内容的一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08580",
            "title": "LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations",
            "url": "https://huggingface.co/papers/2412.08580",
            "abstract": "Recent advances in text-to-image (T2I) generation have shown remarkable success in producing high-quality images from text. However, existing T2I models show decayed performance in compositional image generation involving multiple objects and intricate relationships. We attribute this problem to limitations in existing datasets of image-text pairs, which lack precise inter-object relationship annotations with prompts only. To address this problem, we construct LAION-SG, a large-scale dataset with high-quality structural annotations of scene graphs (SG), which precisely describe attributes and relationships of multiple objects, effectively representing the semantic structure in complex scenes. Based on LAION-SG, we train a new foundation model SDXL-SG to incorporate structural annotation information into the generation process. Extensive experiments show advanced models trained on our LAION-SG boast significant performance improvements in complex scene generation over models on existing datasets. We also introduce CompSG-Bench, a benchmark that evaluates models on compositional image generation, establishing a new standard for this domain.",
            "score": 7,
            "issue_id": 1081,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 декабря",
                "en": "December 11",
                "zh": "12月11日"
            },
            "hash": "07b05e5ae44a52c7",
            "authors": [
                "Zejian Li",
                "Chenye Meng",
                "Yize Li",
                "Ling Yang",
                "Shengyuan Zhang",
                "Jiarui Ma",
                "Jiayi Li",
                "Guang Yang",
                "Changyuan Yang",
                "Zhiyuan Yang",
                "Jinxiong Chang",
                "Lingyun Sun"
            ],
            "affiliations": [
                "Alibaba Group",
                "Ant Group",
                "Jiangnan University",
                "Peking University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08580.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#synthetic",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Новый уровень генерации сложных сцен с помощью графов",
                    "desc": "Исследователи представили новый подход к генерации изображений по тексту, который улучшает композиционную генерацию сложных сцен с несколькими объектами. Они создали датасет LAION-SG с высококачественными структурными аннотациями в виде графов сцен. На основе этого датасета была обучена новая фундаментальная модель SDXL-SG, которая демонстрирует значительное улучшение в генерации сложных сцен по сравнению с существующими моделями. Также авторы представили бенчмарк CompSG-Bench для оценки моделей в задаче композиционной генерации изображений."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Generation with Structured Scene Graphs",
                    "desc": "This paper addresses the challenges faced by text-to-image (T2I) models in generating complex images with multiple objects and their relationships. The authors identify that existing datasets lack detailed annotations for inter-object relationships, which hampers model performance. To overcome this, they introduce LAION-SG, a new dataset that includes comprehensive scene graph annotations, enhancing the understanding of object attributes and relationships. They also present a new model, SDXL-SG, trained on this dataset, which shows significant improvements in generating intricate scenes, along with a new benchmark, CompSG-Bench, for evaluating compositional image generation."
                },
                "zh": {
                    "title": "构建高质量数据集，提升图像生成能力",
                    "desc": "最近在文本到图像生成（T2I）方面取得了显著进展，能够从文本生成高质量图像。然而，现有的T2I模型在生成包含多个对象和复杂关系的图像时表现不佳。我们认为这个问题源于现有图像-文本对数据集的局限性，这些数据集缺乏精确的对象间关系注释。为了解决这个问题，我们构建了LAION-SG，这是一个具有高质量结构注释的大规模数据集，能够有效表示复杂场景中的语义结构，并基于此训练了新的基础模型SDXL-SG。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07797",
            "title": "Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation",
            "url": "https://huggingface.co/papers/2412.07797",
            "abstract": "In the field of text-to-motion generation, Bert-type Masked Models (MoMask, MMM) currently produce higher-quality outputs compared to GPT-type autoregressive models (T2M-GPT). However, these Bert-type models often lack the streaming output capability required for applications in video game and multimedia environments, a feature inherent to GPT-type models. Additionally, they demonstrate weaker performance in out-of-distribution generation. To surpass the quality of BERT-type models while leveraging a GPT-type structure, without adding extra refinement models that complicate scaling data, we propose a novel architecture, Mogo (Motion Only Generate Once), which generates high-quality lifelike 3D human motions by training a single transformer model. Mogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual vector quantization variational autoencoder, which discretizes continuous motion sequences with high precision; 2) Hierarchical Causal Transformer, responsible for generating the base motion sequences in an autoregressive manner while simultaneously inferring residuals across different layers. Experimental results demonstrate that Mogo can generate continuous and cyclic motion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10 seconds) length limitation of existing datasets like HumanML3D. On the HumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the GPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type model MMM (FID = 0.080). Furthermore, our model achieves the best quantitative performance in out-of-distribution generation.",
            "score": 5,
            "issue_id": 1080,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "ff9bb8b603f9d972",
            "authors": [
                "Dongjie Fu"
            ],
            "affiliations": [
                "Mogo AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07797.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#games",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Mogo: Революция в генерации движений из текста",
                    "desc": "Статья представляет новую архитектуру Mogo для генерации высококачественных трехмерных движений человека на основе текста. Mogo использует RVQ-VAE для дискретизации непрерывных последовательностей движений и иерархический причинный трансформер для генерации базовых последовательностей движений. Эксперименты показывают, что Mogo превосходит существующие модели по качеству генерации, включая GPT-подобные и BERT-подобные модели. Модель также демонстрирует лучшие результаты при генерации вне распределения обучающих данных."
                },
                "en": {
                    "title": "Mogo: Revolutionizing Text-to-Motion with High-Quality 3D Generation",
                    "desc": "This paper introduces Mogo, a new architecture for generating high-quality 3D human motions from text. Mogo combines a hierarchical residual vector quantization variational autoencoder (RVQ-VAE) with a hierarchical causal transformer to produce continuous and cyclic motion sequences efficiently. Unlike existing Bert-type models, Mogo maintains the streaming output capability of GPT-type models while improving performance in out-of-distribution scenarios. Experimental results show that Mogo not only generates longer motion sequences but also achieves superior quality metrics compared to both GPT-type and Bert-type models."
                },
                "zh": {
                    "title": "Mogo：高效生成高质量3D人类动作的创新架构",
                    "desc": "在文本到动作生成领域，Bert类型的模型（如MoMask, MMM）虽然输出质量较高，但缺乏流式输出能力，无法满足视频游戏和多媒体环境的需求。相比之下，GPT类型的自回归模型（如T2M-GPT）具备这一特性，但在生成质量上稍逊一筹。为了解决这一问题，我们提出了一种新架构Mogo（Motion Only Generate Once），它通过训练单一的变换器模型生成高质量的3D人类动作。Mogo结合了高精度的层次残差向量量化变分自编码器和层次因果变换器，能够生成连续且循环的动作序列，超越了现有数据集的限制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07825",
            "title": "3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark",
            "url": "https://huggingface.co/papers/2412.07825",
            "abstract": "3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop a comprehensive understanding of the 3D scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning capabilities by balancing the data distribution and adopting a novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark a wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. Our 3DSRBench provide valuable findings and insights about the future development of LMMs with strong 3D reasoning capabilities. Our project page and dataset is available https://3dsrbench.github.io.",
            "score": 2,
            "issue_id": 1082,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "91db0c60d08d4efd",
            "authors": [
                "Wufei Ma",
                "Haoyu Chen",
                "Guofeng Zhang",
                "Celso M de Melo",
                "Alan Yuille",
                "Jieneng Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "DEVCOM Army Research Laboratory",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07825.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "3DSRBench: новый стандарт для оценки 3D пространственного мышления у LMM",
                    "desc": "Эта статья представляет первый комплексный benchmark для оценки способностей больших мультимодальных моделей (LMM) к 3D пространственному мышлению на разнообразных естественных изображениях. Авторы создали датасет 3DSRBench, содержащий 2772 вручную размеченных пар вопрос-ответ по 12 типам вопросов, связанных с 3D пространственным мышлением. Исследование включает оценку робастности моделей к различным ракурсам камеры и использует стратегию FlipEval для тщательного тестирования. Результаты показывают ограничения существующих LMM в различных аспектах 3D восприятия, таких как высота, ориентация, расположение объектов и рассуждения о нескольких объектах."
                },
                "en": {
                    "title": "Enhancing 3D Spatial Reasoning in AI Models",
                    "desc": "This paper introduces 3DSRBench, a new benchmark designed to evaluate 3D spatial reasoning in large multi-modal models (LMMs). It includes 2,772 annotated visual question-answer pairs that cover various question types related to spatial relationships in 3D environments. The study highlights the limitations of current LMMs in understanding aspects like height, orientation, and location, especially when dealing with images taken from uncommon viewpoints. By providing a structured evaluation framework, this work aims to enhance the development of models with improved 3D reasoning capabilities."
                },
                "zh": {
                    "title": "推动3D空间推理的未来发展",
                    "desc": "3D空间推理是分析和理解三维空间中物体位置、方向和空间关系的能力。本文提出了第一个全面的3D空间推理基准，3DSRBench，包含2772个手动标注的视觉问答对，涵盖12种问题类型。我们通过平衡数据分布和采用新颖的FlipEval策略，对3D空间推理能力进行了全面评估。研究结果揭示了现有大型多模态模型在3D意识方面的局限性，并为未来的模型发展提供了重要见解。"
                }
            }
        }
    ],
    "link_prev": "2024-12-11.html",
    "link_next": "2024-12-13.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "11.12",
        "en": "12/11",
        "zh": "12月11日"
    },
    "short_date_next": {
        "ru": "13.12",
        "en": "12/13",
        "zh": "12月13日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了代码大语言模型（codeLLMs）在代码生成方面的进展。以前的代码相关基准测试主要关注生成正确的代码片段，但忽略了与人类偏好的一致性。为了弥补这一差距，作者提出了一个名为CodeArena的严格人工编制基准测试，模拟真实世界编码任务的复杂性和多样性。通过系统实验，作者发现在开源代码LLMs和专有LLMs之间存在显著的性能差距，强调了人类偏好一致性的重要性。",
        "title": "Evaluating and Aligning CodeLLMs on Human Preference",
        "pinyin": "这篇文章讨论了代码大语言模型（codeLLMs）在代码生成方面的进展。以前的代码相关基准测试主要关注生成正确的代码片段，但忽略了与人类偏好的一致性。为了弥补这一差距，作者提出了一个名为CodeArena的严格人工编制基准测试，模拟真实世界编码任务的复杂性和多样性。通过系统实验，作者发现在开源代码LLMs和专有LLMs之间存在显著的性能差距，强调了人类偏好一致性的重要性。\n\nzhè piān wén zhāng tǎo lùn le dài mǎ dà yǔ yán mó xíng (codeLLMs) zài dài mǎ shēng chéng fāng miàn de jìn zhàn. yǐ qián de dài mǎ xiāng guān jī zhǔn cè shì zhǔ yào guān zhù shēng chéng zhèng què de dài mǎ piàn duàn, dàn hū lüè le yǔ rén lèi piān hào de yī zhì xìng. wèi le mí bǔ zhè yī chā jù, zuò zhě tí chū le yī gè míng wèi CodeArena de yán gé rén gōng biān zhì jī zhǔn cè shì, mó nǐ zhēn shí shì jiè biān mǎ rèn wù de fú zà xìng hé duō yàng xìng. tōng guò xì tǒng shí yàn, zuò zhě fā xiàn zài kāi yuán dài mǎ LLMs hé zhuān yǒu LLMs zhī jiān cún zài xiǎn zhù de xìng néng chā jù, qiáng diào le rén lèi piān hào yī zhì xìng de zhòng yào xìng.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'},\n{'word': '以前', 'pinyin': 'yǐ qián', 'trans': 'before'},\n{'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'},\n{'word': '基准测试', 'pinyin': 'jī zhǔn cè shì', 'trans': 'benchmark test'},\n{'word': '主要', 'pinyin': 'zhǔ yào', 'trans': 'main'},\n{'word': '关注', 'pinyin': 'guān zhù', 'trans': 'focus on'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'},\n{'word': '正确', 'pinyin': 'zhèng què', 'trans': 'correct'},\n{'word': '片段', 'pinyin': 'piàn duàn', 'trans': 'segment'},\n{'word': '忽略', 'pinyin': 'hū lüè', 'trans': 'ignore'},\n{'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'},\n{'word': '弥补', 'pinyin': 'mí bǔ', 'trans': 'make up for'},\n{'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '严格', 'pinyin': 'yán gé', 'trans': 'strict'},\n{'word': '人工编制', 'pinyin': 'rén gōng biān zhì', 'trans': 'artificially compiled'},\n{'word': '模拟', 'pinyin': 'mó nǐ', 'trans': 'simulate'},\n{'word': '真实世界', 'pinyin': 'zhēn shí shì jiè', 'trans': 'real world'},\n{'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'},\n{'word': '多样性', 'pinyin': 'duō yàng xìng', 'trans': 'diversity'},\n{'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open source'},\n{'word': '专有', 'pinyin': 'zhuān yǒu', 'trans': 'proprietary'},\n{'word': '存在', 'pinyin': 'cún zài', 'trans': 'exist'},\n{'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '强调', 'pinyin': 'qiáng diào', 'trans': 'emphasize'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}]",
        "trans": "This article discusses the advancements in code generation by code large language models (codeLLMs). Previous code-related benchmark tests primarily focused on generating correct code snippets but overlooked consistency with human preferences. To address this gap, the authors propose a rigorous, human-crafted benchmark test called CodeArena, which simulates the complexity and diversity of real-world coding tasks. Through systematic experiments, the authors found a significant performance gap between open-source codeLLMs and proprietary LLMs, highlighting the importance of consistency with human preferences.",
        "update_ts": "2024-12-11 09:11"
    }
}