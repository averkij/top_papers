{
    "date": {
        "ru": "19 декабря",
        "en": "December 19",
        "zh": "12月19日"
    },
    "time_utc": "2024-12-19 03:23",
    "weekday": 3,
    "issue_id": 1204,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.14161",
            "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks",
            "url": "https://huggingface.co/papers/2412.14161",
            "abstract": "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at helping to accelerate or even autonomously perform work-related tasks? The answer to this question has important implications for both industry looking to adopt AI into their workflows, and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper, we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that with the most competitive agent, 24% of the tasks can be completed autonomously. This paints a nuanced picture on task automation with LM agents -- in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems.",
            "score": 12,
            "issue_id": 1204,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 декабря",
                "en": "December 18",
                "zh": "12月18日"
            },
            "hash": "4efc2187cb10b78e",
            "authors": [
                "Frank F. Xu",
                "Yufan Song",
                "Boxuan Li",
                "Yuxuan Tang",
                "Kritanjali Jain",
                "Mengxue Bao",
                "Zora Z. Wang",
                "Xuhui Zhou",
                "Zhitong Guo",
                "Murong Cao",
                "Mingyang Yang",
                "Hao Yang Lu",
                "Amaad Martin",
                "Zhe Su",
                "Leander Maben",
                "Raj Mehta",
                "Wayne Chi",
                "Lawrence Jang",
                "Yiqing Xie",
                "Shuyan Zhou",
                "Graham Neubig"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Duke University",
                "Independent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14161.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agi",
                    "#agents",
                    "#science",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ИИ-агенты в офисе: прогресс и ограничения в автоматизации рабочих задач",
                    "desc": "Статья представляет новый бенчмарк TheAgentCompany для оценки ИИ-агентов в выполнении профессиональных задач в виртуальной среде, имитирующей небольшую компанию-разработчика. Исследователи тестируют агентов на основе языковых моделей (ЯМ) в различных задачах, включая веб-серфинг, программирование и коммуникацию. Результаты показывают, что лучшие агенты способны автономно выполнить 24% задач. Это демонстрирует, что ИИ-агенты могут автоматизировать простые задачи, но сложные долгосрочные задачи все еще остаются недоступными для текущих систем."
                },
                "en": {
                    "title": "Evaluating AI Agents: The Future of Work Automation",
                    "desc": "This paper introduces TheAgentCompany, a benchmark designed to evaluate the performance of AI agents in completing work-related tasks similar to those of a digital worker. The study assesses how well these agents, powered by large language models (LLMs), can autonomously perform tasks such as web browsing, coding, and communication within a simulated software company environment. The results indicate that while 24% of tasks can be completed autonomously by the most effective agent, more complex tasks remain challenging for current AI systems. This research highlights the potential and limitations of AI in automating workplace functions, providing insights for industries considering AI integration."
                },
                "zh": {
                    "title": "AI代理助力工作任务自动化的探索",
                    "desc": "本文介绍了一个名为TheAgentCompany的基准测试，用于评估人工智能代理在执行真实工作任务中的表现。我们创建了一个模拟小型软件公司的环境，设计了多种任务，代理可以通过浏览网页、编写代码和与同事沟通来完成这些任务。测试结果显示，最先进的代理能够自主完成24%的任务，这表明在简单任务的自动化方面，当前的语言模型代理表现良好。尽管如此，对于更复杂的长期任务，现有系统仍然无法胜任。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12571",
            "title": "ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers",
            "url": "https://huggingface.co/papers/2412.12571",
            "abstract": "Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the inherent in-context generation capabilities of pretrained diffusion transformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks with minimal or no architectural modifications. These capabilities are unlocked by concatenating self-attention tokens across multiple input and target images, combined with grouped and masked generation pipelines. Building upon this foundation, we present ChatDiT, a zero-shot, general-purpose, and interactive visual generation framework that leverages pretrained diffusion transformers in their original form, requiring no additional tuning, adapters, or modifications. Users can interact with ChatDiT to create interleaved text-image articles, multi-page picture books, edit images, design IP derivatives, or develop character design settings, all through free-form natural language across one or more conversational rounds. At its core, ChatDiT employs a multi-agent system comprising three key components: an Instruction-Parsing agent that interprets user-uploaded images and instructions, a Strategy-Planning agent that devises single-step or multi-step generation actions, and an Execution agent that performs these actions using an in-context toolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench arXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with diverse instructions and varying numbers of input and target images. Despite its simplicity and training-free approach, ChatDiT surpasses all competitors, including those specifically designed and trained on extensive multi-task datasets. We further identify key limitations of pretrained DiTs in zero-shot adapting to tasks. We release all code, agents, results, and intermediate outputs to facilitate further research at https://github.com/ali-vilab/ChatDiT",
            "score": 2,
            "issue_id": 1204,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "8fa92ec2d65420c8",
            "authors": [
                "Lianghua Huang",
                "Wei Wang",
                "Zhi-Fan Wu",
                "Yupeng Shi",
                "Chen Liang",
                "Tong Shen",
                "Han Zhang",
                "Huanzhang Dou",
                "Yu Liu",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Alibaba Inc.",
                "Institute of Automation, Chinese Academy of Sciences",
                "Shanghai Jiao Tong University",
                "Taobao",
                "Tongyi Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12571.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#cv",
                    "#agents",
                    "#benchmark",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "ChatDiT: Универсальная визуальная генерация без дополнительного обучения",
                    "desc": "Исследование представляет ChatDiT - интерактивную систему для визуальной генерации, использующую предобученные диффузионные трансформеры без дополнительной настройки. ChatDiT использует многоагентный подход, включающий агентов для анализа инструкций, планирования стратегии и выполнения действий. Система позволяет создавать и редактировать изображения, дизайн-проекты и другие визуальные материалы с помощью естественного языка. ChatDiT превзошел конкурентов в тестировании на IDEA-Bench, несмотря на простоту подхода без дополнительного обучения."
                },
                "en": {
                    "title": "ChatDiT: Interactive Visual Generation with Zero-Tuning Diffusion Transformers",
                    "desc": "This paper introduces ChatDiT, a novel framework for visual generation that utilizes pretrained diffusion transformers (DiTs) without requiring any additional tuning or modifications. ChatDiT allows users to create and edit images, design characters, and generate text-image articles through natural language interactions. It operates using a multi-agent system that includes agents for instruction parsing, strategy planning, and execution of generation tasks. The framework has been evaluated on a diverse set of design tasks and has shown superior performance compared to other specialized models, highlighting the potential of DiTs in zero-shot learning scenarios."
                },
                "zh": {
                    "title": "ChatDiT：零-shot互动视觉生成的未来",
                    "desc": "最近的研究表明，预训练的扩散变换器（DiTs）具有内在的上下文生成能力，使其能够在不同的视觉任务中无缝适应，几乎不需要架构修改。这些能力通过在多个输入和目标图像之间连接自注意力标记，以及结合分组和掩蔽生成管道来实现。在此基础上，我们提出了ChatDiT，这是一个零-shot、通用且互动的视觉生成框架，利用预训练的扩散变换器，用户可以通过自然语言与ChatDiT互动，创建文本-图像文章、编辑图像等。ChatDiT的核心是一个多代理系统，包括指令解析代理、策略规划代理和执行代理，能够高效地完成用户的生成任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13746",
            "title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment",
            "url": "https://huggingface.co/papers/2412.13746",
            "abstract": "Despite the significant progress made by existing retrieval augmented language models (RALMs) in providing trustworthy responses and grounding in reliable sources, they often overlook effective alignment with human preferences. In the alignment process, reward models (RMs) act as a crucial proxy for human values to guide optimization. However, it remains unclear how to evaluate and select a reliable RM for preference alignment in RALMs. To this end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG settings. First, we design four crucial and challenging RAG-specific scenarios to assess RMs, including multi-hop reasoning, fine-grained citation, appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG subsets, six retrievers, and 24 RALMs to increase the diversity of data sources. Finally, we adopt an LLM-as-a-judge approach to improve preference annotation efficiency and effectiveness, exhibiting a strong correlation with human annotations. Based on the RAG-RewardBench, we conduct a comprehensive evaluation of 45 RMs and uncover their limitations in RAG scenarios. Additionally, we also reveal that existing trained RALMs show almost no improvement in preference alignment, highlighting the need for a shift towards preference-aligned training.We release our benchmark and code publicly at https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",
            "score": 2,
            "issue_id": 1204,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 декабря",
                "en": "December 18",
                "zh": "12月18日"
            },
            "hash": "9159fbad2530d02c",
            "authors": [
                "Zhuoran Jin",
                "Hongbang Yuan",
                "Tianyi Men",
                "Pengfei Cao",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao"
            ],
            "affiliations": [
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
                "The Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13746.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rag",
                    "#open_source",
                    "#rlhf",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "RAG-RewardBench: новый стандарт для оценки моделей вознаграждения в RAG",
                    "desc": "Статья представляет новый бенчмарк RAG-RewardBench для оценки моделей вознаграждения в контексте retrieval-augmented generation (RAG). Авторы разработали четыре сценария для тестирования моделей вознаграждения, включая многоходовые рассуждения и устойчивость к конфликтам. Бенчмарк использует 18 наборов данных RAG, 6 ретриверов и 24 модели RAG для увеличения разнообразия. Результаты показывают ограничения существующих моделей вознаграждения в сценариях RAG и отсутствие улучшений в согласовании предпочтений у обученных моделей RAG."
                },
                "en": {
                    "title": "Enhancing Human Preference Alignment in Retrieval Augmented Language Models",
                    "desc": "This paper introduces RAG-RewardBench, a benchmark designed to evaluate reward models (RMs) used in retrieval augmented language models (RALMs). The authors identify the challenge of aligning RMs with human preferences and propose four specific scenarios to test their effectiveness. They incorporate a diverse set of RAG subsets, retrievers, and RALMs to ensure comprehensive evaluation. The findings reveal significant limitations in current RMs and emphasize the necessity for training methods that prioritize preference alignment."
                },
                "zh": {
                    "title": "提升人类偏好的检索增强模型对齐",
                    "desc": "尽管现有的检索增强语言模型（RALMs）在提供可信响应和可靠来源方面取得了显著进展，但它们在与人类偏好的有效对齐上仍存在不足。在对齐过程中，奖励模型（RMs）作为人类价值观的重要代理，指导优化过程。然而，如何评估和选择可靠的RM以实现RALMs中的偏好对齐仍不明确。为此，我们提出了RAG-RewardBench，这是第一个用于评估RAG环境中RM的基准，设计了四个关键的RAG特定场景，并进行了全面评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13501",
            "title": "GUI Agents: A Survey",
            "url": "https://huggingface.co/papers/2412.13501",
            "abstract": "Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.",
            "score": 2,
            "issue_id": 1204,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 декабря",
                "en": "December 18",
                "zh": "12月18日"
            },
            "hash": "93d87411c70f693d",
            "authors": [
                "Dang Nguyen",
                "Jian Chen",
                "Yu Wang",
                "Gang Wu",
                "Namyong Park",
                "Zhengmian Hu",
                "Hanjia Lyu",
                "Junda Wu",
                "Ryan Aponte",
                "Yu Xia",
                "Xintong Li",
                "Jing Shi",
                "Hongjie Chen",
                "Viet Dac Lai",
                "Zhouhang Xie",
                "Sungchul Kim",
                "Ruiyi Zhang",
                "Tong Yu",
                "Mehrab Tanjim",
                "Nesreen K. Ahmed",
                "Puneet Mathur",
                "Seunghyun Yoon",
                "Lina Yao",
                "Branislav Kveton",
                "Thien Huu Nguyen",
                "Trung Bui",
                "Tianyi Zhou",
                "Ryan A. Rossi",
                "Franck Dernoncourt"
            ],
            "affiliations": [
                "Adobe Research",
                "Carnegie Mellon University",
                "Dolby Labs",
                "Intel AI Research",
                "Meta AI",
                "State University of New York at Buffalo",
                "University of California, San Diego",
                "University of Maryland",
                "University of New South Wales",
                "University of Oregon",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13501.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#survey"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "GUI-агенты: новая эра автоматизации взаимодействия человека с компьютером",
                    "desc": "Статья посвящена агентам с графическим пользовательским интерфейсом (GUI), работающим на основе больших языковых моделей. Эти агенты автономно взаимодействуют с цифровыми системами, имитируя действия человека. В работе представлен обзор бенчмарков, метрик оценки, архитектур и методов обучения GUI-агентов. Авторы предлагают единую структуру, описывающую возможности агентов в восприятии, рассуждении, планировании и действии."
                },
                "en": {
                    "title": "Empowering Automation: The Rise of GUI Agents with Large Models",
                    "desc": "This paper surveys the development of GUI agents that utilize Large Foundation Models to automate interactions with software applications. It categorizes various aspects of these agents, including their benchmarks, evaluation metrics, architectures, and training methods. The authors propose a unified framework that outlines the key capabilities of perception, reasoning, planning, and acting in GUI agents. Additionally, the paper highlights open challenges and future directions for research in this area, providing a foundational understanding for both practitioners and researchers."
                },
                "zh": {
                    "title": "GUI代理：人机交互的未来",
                    "desc": "图形用户界面（GUI）代理是由大型基础模型驱动的，能够自动化人机交互。这些代理可以自主与数字系统或软件应用程序进行交互，模拟人类的点击、输入和导航等操作。本文提供了一个全面的调查，分类了GUI代理的基准、评估指标、架构和训练方法，并提出了一个统一框架，描述了它们的感知、推理、规划和行动能力。我们还识别了重要的开放挑战，并讨论了未来的关键方向，为从业者和研究人员提供了对当前进展、技术、基准和待解决的关键问题的直观理解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13061",
            "title": "VidTok: A Versatile and Open-Source Video Tokenizer",
            "url": "https://huggingface.co/papers/2412.13061",
            "abstract": "Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.",
            "score": 2,
            "issue_id": 1204,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "488c580621c13ba2",
            "authors": [
                "Anni Tang",
                "Tianyu He",
                "Junliang Guo",
                "Xinle Cheng",
                "Li Song",
                "Jiang Bian"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13061.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#video",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VidTok: Прорыв в токенизации видео для эффективного машинного обучения",
                    "desc": "VidTok - это новый универсальный токенизатор видео, обеспечивающий современную производительность в непрерывной и дискретной токенизации. Он включает усовершенствования в архитектуре модели, использует конечную скалярную квантизацию (FSQ) для решения проблем обучения, связанных с векторной квантизацией, и применяет улучшенные стратегии обучения. VidTok демонстрирует превосходную производительность по нескольким метрикам, включая PSNR, SSIM, LPIPS и FVD, в стандартизированных условиях оценки."
                },
                "en": {
                    "title": "VidTok: Revolutionizing Video Tokenization for Enhanced Performance",
                    "desc": "This paper presents VidTok, a new video tokenizer designed to efficiently convert video content into compact latent tokens, which helps reduce redundancy in pixel data. VidTok stands out by utilizing advanced model architecture, including convolutional layers and up/downsampling modules, to enhance performance. It also addresses common issues in traditional Vector Quantization by implementing Finite Scalar Quantization, which stabilizes training and prevents codebook collapse. Overall, VidTok shows significant improvements in video tokenization performance, as evidenced by better scores in metrics like PSNR, SSIM, LPIPS, and FVD compared to existing methods."
                },
                "zh": {
                    "title": "VidTok：视频标记化的新突破",
                    "desc": "本论文介绍了一种名为VidTok的视频编码器，它能够将视频内容压缩为紧凑的潜在标记。VidTok在连续和离散标记化方面都表现出色，解决了传统向量量化方法中的训练不稳定性和代码本崩溃问题。通过采用卷积层、上下采样模块以及有限标量量化（FSQ），VidTok显著提高了视频标记化的性能。该方法在多个评估指标上，如PSNR、SSIM、LPIPS和FVD，均表现优于现有技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14015",
            "title": "Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation",
            "url": "https://huggingface.co/papers/2412.14015",
            "abstract": "Prompts play a critical role in unleashing the power of language and vision foundation models for specific tasks. For the first time, we introduce prompting into depth foundation models, creating a new paradigm for metric depth estimation termed Prompt Depth Anything. Specifically, we use a low-cost LiDAR as the prompt to guide the Depth Anything model for accurate metric depth output, achieving up to 4K resolution. Our approach centers on a concise prompt fusion design that integrates the LiDAR at multiple scales within the depth decoder. To address training challenges posed by limited datasets containing both LiDAR depth and precise GT depth, we propose a scalable data pipeline that includes synthetic data LiDAR simulation and real data pseudo GT depth generation. Our approach sets new state-of-the-arts on the ARKitScenes and ScanNet++ datasets and benefits downstream applications, including 3D reconstruction and generalized robotic grasping.",
            "score": 1,
            "issue_id": 1204,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 декабря",
                "en": "December 18",
                "zh": "12月18日"
            },
            "hash": "1d55ea1f2eb90ac0",
            "authors": [
                "Haotong Lin",
                "Sida Peng",
                "Jingxiao Chen",
                "Songyou Peng",
                "Jiaming Sun",
                "Minghuan Liu",
                "Hujun Bao",
                "Jiashi Feng",
                "Xiaowei Zhou",
                "Bingyi Kang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "ETH Zurich",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14015.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#optimization",
                    "#synthetic",
                    "#data",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Подсказки LiDAR для точной оценки глубины изображения",
                    "desc": "Статья представляет новый подход к оценке метрической глубины, названный Prompt Depth Anything. Авторы используют недорогой LiDAR в качестве подсказки для модели Depth Anything, чтобы получить точную метрическую глубину с разрешением до 4K. Ключевым элементом является компактный дизайн объединения подсказок, интегрирующий LiDAR на нескольких уровнях в декодере глубины. Для решения проблемы ограниченности обучающих данных предложен масштабируемый конвейер, включающий симуляцию синтетических данных LiDAR и генерацию псевдо-GT глубины для реальных данных."
                },
                "en": {
                    "title": "Revolutionizing Depth Estimation with LiDAR Prompts",
                    "desc": "This paper introduces a novel method called Prompt Depth Anything, which integrates prompting techniques into depth estimation models. By using low-cost LiDAR data as a guiding prompt, the model can produce accurate metric depth outputs at high resolutions, up to 4K. The authors present a unique prompt fusion design that effectively incorporates LiDAR information at various scales within the depth decoder. To overcome the challenges of limited training data, they propose a scalable pipeline that combines synthetic LiDAR simulations with real data to generate pseudo ground truth depth, achieving state-of-the-art results on benchmark datasets."
                },
                "zh": {
                    "title": "利用提示技术提升深度估计精度",
                    "desc": "本文介绍了一种新的深度估计方法，称为Prompt Depth Anything，首次将提示技术应用于深度基础模型。我们使用低成本的LiDAR作为提示，指导Depth Anything模型输出准确的度量深度，分辨率高达4K。该方法通过在深度解码器中多尺度融合LiDAR提示，解决了有限数据集带来的训练挑战。我们的研究在ARKitScenes和ScanNet++数据集上设定了新的最先进水平，并对3D重建和通用机器人抓取等下游应用产生了积极影响。"
                }
            }
        }
    ],
    "link_prev": "2024-12-18.html",
    "link_next": "2024-12-20.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "18.12",
        "en": "12/18",
        "zh": "12月18日"
    },
    "short_date_next": {
        "ru": "20.12",
        "en": "12/20",
        "zh": "12月20日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "大型语言模型（LLMs）在复杂推理任务中取得了显著进展。然而，基准测试性能与实际应用之间仍存在差距。主要原因是当前的评估方法和指标无法充分捕捉LLMs的全部能力，特别是在需要准确性和一致性的复杂推理任务中。本文提出了G-Pass@k，一个新的评估指标，能够在多次采样中连续评估模型性能，量化模型的最高性能潜力和稳定性。同时，我们推出了LiveMathBench，一个包含具有挑战性、当代数学问题的动态基准，旨在减少评估过程中的数据泄露风险。详细结果和基准可访问：https://github.com/open-compass/GPassK。",
        "title": "Are Your LLMs Capable of Stable Reasoning?",
        "pinyin": "Dàxíng yǔyán móxíng (LLMs) zài fùzá xīnglǐ rènwù zhōng qǔdéle xiǎnzhù jìnbù. Rán'ér, jīzhǔn cèshǐ xìngnéng yǔ shíjì yìngyòng zhījiān réng cúnzài chājù. Zhǔyào yuányīn shì dāngqián de pínggǔ fāngfǎ hé zhǐbiāo wúfǎ chōngfēn bǐngqǔ LLMs de quánbù nénglì, tèbié shì zài xūyào zhǔnquèxìng hé yīzhìxìng de fùzá xīnglǐ rènwù zhōng. Běnwén tíchūle G-Pass@k, yīgè xīn de pínggǔ zhǐbiāo, nénggòu zài duōcì cǎiyàng zhōng liánxù pínggǔ móxíng xìngnéng, liàngzhì móxíng de zuìgāo xìngnéng qiánlì hé wěndíngxìng. Tóngshí, wǒmen tuīchūle LiveMathBench, yīgè bāohán jùyǒu tiǎozhànxìng, dāngdài shùxué wèntí de dòngtài jīzhǔn, zhǐyú jiǎnshǎo pínggǔ guòchéng zhōng de shùjù lòushì fēngxiǎn. Xiángxì jiéguǒ hé jīzhǔn kě fāngwèn: https://github.com/open-compass/GPassK.",
        "vocab": "[{'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'},\n{'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'},\n{'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'},\n{'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'},\n{'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '实际', 'pinyin': 'shí jì', 'trans': 'actual'},\n{'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'application'},\n{'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'},\n{'word': '存在', 'pinyin': 'cún zài', 'trans': 'exist'},\n{'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluation'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '指标', 'pinyin': 'zhǐ biāo', 'trans': 'metric'},\n{'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'capability'},\n{'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'},\n{'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '采样', 'pinyin': 'cǎi yàng', 'trans': 'sampling'},\n{'word': '连续', 'pinyin': 'lián xù', 'trans': 'continuous'},\n{'word': '量化', 'pinyin': 'liàng huà', 'trans': 'quantify'},\n{'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'},\n{'word': '稳定性', 'pinyin': 'wěn dìng xìng', 'trans': 'stability'},\n{'word': '推出', 'pinyin': 'tuī chū', 'trans': 'launch'},\n{'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'},\n{'word': '挑战性', 'pinyin': 'tiǎo zhàn xìng', 'trans': 'challenging'},\n{'word': '当代', 'pinyin': 'dāng dài', 'trans': 'contemporary'},\n{'word': '数学', 'pinyin': 'shù xué', 'trans': 'mathematics'},\n{'word': '问题', 'pinyin': 'wèn tí', 'trans': 'problem'},\n{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},\n{'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'},\n{'word': '泄露', 'pinyin': 'xiè lòu', 'trans': 'leak'},\n{'word': '风险', 'pinyin': 'fēng xiǎn', 'trans': 'risk'},\n{'word': '详细', 'pinyin': 'xiáng xì', 'trans': 'detailed'},\n{'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'},\n{'word': '访问', 'pinyin': 'fǎng wèn', 'trans': 'access'}]",
        "trans": "Large language models (LLMs) have made significant strides in complex reasoning tasks. However, there remains a gap between benchmark test performance and real-world applications. The primary reason is that current evaluation methods and metrics fail to fully capture the capabilities of LLMs, especially in complex reasoning tasks that require accuracy and consistency. This paper introduces G-Pass@k, a new evaluation metric that can continuously assess model performance across multiple samples, quantifying the model's peak performance potential and stability. Additionally, we present LiveMathBench, a dynamic benchmark containing challenging, contemporary mathematical problems aimed at reducing the risk of data leakage during the evaluation process. Detailed results and benchmarks are available at: https://github.com/open-compass/GPassK.",
        "update_ts": "2024-12-18 09:11"
    }
}