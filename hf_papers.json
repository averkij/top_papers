{
    "date": {
        "ru": "20 января",
        "en": "January 20",
        "zh": "1月20日"
    },
    "time_utc": "2025-01-20 04:12",
    "weekday": 0,
    "issue_id": 1750,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.09891",
            "title": "Evolving Deeper LLM Thinking",
            "url": "https://huggingface.co/papers/2501.09891",
            "abstract": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.",
            "score": 3,
            "issue_id": 1750,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "f2f5bbede5781334",
            "authors": [
                "Kuang-Huei Lee",
                "Ian Fischer",
                "Yueh-Hua Wu",
                "Dave Marwood",
                "Shumeet Baluja",
                "Dale Schuurmans",
                "Xinyun Chen"
            ],
            "affiliations": [
                "Google DeepMind",
                "UC San Diego",
                "University of Alberta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09891.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эволюция мышления: новый подход к оптимизации вывода в языковых моделях",
                    "desc": "Статья представляет эволюционную стратегию поиска для масштабирования вычислений во время вывода в больших языковых моделях. Метод, названный Mind Evolution, использует языковую модель для генерации, рекомбинации и уточнения кандидатов-ответов. Этот подход устраняет необходимость формализации исходной задачи вывода, если доступен оценщик решений. При контроле за стоимостью вычислений, Mind Evolution значительно превосходит другие стратегии вывода в задачах планирования на естественном языке."
                },
                "en": {
                    "title": "Mind Evolution: Revolutionizing Inference in Large Language Models",
                    "desc": "This paper presents Mind Evolution, an innovative evolutionary search strategy designed to enhance the inference time of Large Language Models (LLMs). By leveraging a language model, Mind Evolution generates, recombines, and refines potential responses without needing to define the inference problem formally, as long as a solution evaluator is available. The results demonstrate that Mind Evolution significantly outperforms traditional inference methods like Best-of-N and Sequential Revision in natural language planning tasks. In benchmarks such as TravelPlanner and Natural Plan, Mind Evolution successfully solves over 98% of instances using Gemini 1.5 Pro, showcasing its effectiveness without relying on a formal solver."
                },
                "zh": {
                    "title": "Mind Evolution：推理效率的新突破",
                    "desc": "本文探讨了一种用于大语言模型推理时间计算的进化搜索策略，称为Mind Evolution。该方法利用语言模型生成、重组和优化候选响应，避免了在有解决方案评估器的情况下需要形式化推理问题。通过控制推理成本，我们发现Mind Evolution在自然语言规划任务中显著优于其他推理策略，如Best-of-N和Sequential Revision。在TravelPlanner和Natural Plan基准测试中，Mind Evolution在不使用正式求解器的情况下，解决了超过98%的问题实例。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10120",
            "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
            "url": "https://huggingface.co/papers/2501.10120",
            "abstract": "We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholarly queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.",
            "score": 1,
            "issue_id": 1750,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "bf3bfc73e6d5b31d",
            "authors": [
                "Yichen He",
                "Guanhua Huang",
                "Peiyuan Feng",
                "Yuan Lin",
                "Yuchen Zhang",
                "Hang Li",
                "Weinan E"
            ],
            "affiliations": [
                "ByteDance Research",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10120.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#synthetic",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "PaSa: ИИ-агент для эффективного поиска научных статей",
                    "desc": "PaSa - это продвинутый агент для поиска научных статей, основанный на больших языковых моделях. Он способен автономно принимать решения, включая использование поисковых инструментов, чтение статей и выбор релевантных ссылок для получения комплексных и точных результатов по сложным научным запросам. PaSa оптимизирован с помощью обучения с подкреплением на синтетическом наборе данных AutoScholarQuery, содержащем 35 тысяч детализированных академических запросов и соответствующих статей из ведущих конференций по ИИ. Несмотря на обучение на синтетических данных, PaSa значительно превосходит существующие базовые модели на реальном тестовом наборе RealScholarQuery, включая Google и ChatGPT."
                },
                "en": {
                    "title": "Revolutionizing Academic Search with PaSa!",
                    "desc": "The paper presents PaSa, a sophisticated Paper Search agent that utilizes large language models to enhance academic research. PaSa autonomously navigates the search process by making decisions such as invoking search tools, analyzing papers, and selecting pertinent references to deliver thorough and precise results for complex queries. It is optimized through reinforcement learning using a synthetic dataset called AutoScholarQuery, which contains 35,000 detailed academic queries and related papers from leading AI conferences. The performance of PaSa is evaluated against real-world queries using the RealScholarQuery benchmark, demonstrating significant improvements over existing search tools, including Google and various GPT models."
                },
                "zh": {
                    "title": "PaSa：智能论文搜索的新纪元",
                    "desc": "本文介绍了一种名为PaSa的先进论文搜索代理，利用大型语言模型进行自主决策。PaSa能够调用搜索工具、阅读论文并选择相关参考文献，以获取复杂学术查询的全面和准确结果。我们通过强化学习优化PaSa，使用了一个包含35,000个细粒度学术查询的合成数据集AutoScholarQuery。尽管在合成数据上训练，PaSa在真实学术查询基准RealScholarQuery上的表现显著优于现有的基线模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10132",
            "title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario",
            "url": "https://huggingface.co/papers/2501.10132",
            "abstract": "Enhancing large language models (LLMs) with real-time APIs can help generate more accurate and up-to-date responses. However, evaluating the function calling abilities of LLMs in real-world scenarios remains under-explored due to the complexity of data collection and evaluation. In this work, we introduce ComplexFuncBench, a benchmark for complex function calling across five real-world scenarios. Compared to existing benchmarks, ComplexFuncBench encompasses multi-step and constrained function calling, which requires long-parameter filing, parameter value reasoning, and 128k long context. Additionally, we propose an automatic framework, ComplexEval, for quantitatively evaluating complex function calling tasks. Through comprehensive experiments, we demonstrate the deficiencies of state-of-the-art LLMs in function calling and suggest future directions for optimizing these capabilities. The data and code are available at https://github.com/THUDM/ComplexFuncBench.",
            "score": 1,
            "issue_id": 1749,
            "pub_date": "2025-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "de405dcc4bfc8efc",
            "authors": [
                "Lucen Zhong",
                "Zhengxiao Du",
                "Xiaohan Zhang",
                "Haiyi Hu",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10132.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#optimization",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Новый бенчмарк для оценки сложных вызовов функций в больших языковых моделях",
                    "desc": "Данная статья представляет новый бенчмарк ComplexFuncBench для оценки способностей больших языковых моделей (LLM) вызывать сложные функции в реальных сценариях. Бенчмарк включает в себя многошаговые и ограниченные вызовы функций, требующие заполнения длинных параметров и рассуждений о значениях параметров. Авторы также предлагают автоматическую систему ComplexEval для количественной оценки задач сложного вызова функций. Эксперименты показывают недостатки современных LLM в вызове функций и предлагают направления для оптимизации этих возможностей."
                },
                "en": {
                    "title": "Benchmarking Complex Function Calling in LLMs",
                    "desc": "This paper presents ComplexFuncBench, a new benchmark designed to evaluate the function calling abilities of large language models (LLMs) in real-world scenarios. It focuses on complex tasks that involve multi-step and constrained function calling, which require advanced reasoning and handling of long contexts. The authors also introduce ComplexEval, an automatic framework for quantitatively assessing these complex function calling tasks. Through their experiments, they highlight the limitations of current state-of-the-art LLMs and propose directions for improving their performance in this area."
                },
                "zh": {
                    "title": "提升LLMs函数调用能力的基准与评估",
                    "desc": "本论文提出了ComplexFuncBench，这是一个用于评估大型语言模型（LLMs）在复杂函数调用方面的基准测试。该基准涵盖了五种真实场景，涉及多步骤和受限的函数调用，要求模型进行长参数填写和参数值推理。我们还提出了ComplexEval，一个自动化框架，用于定量评估复杂函数调用任务的能力。通过实验，我们展示了当前最先进的LLMs在函数调用方面的不足，并提出了未来优化的方向。"
                }
            }
        }
    ],
    "link_prev": "2025-01-17.html",
    "link_next": "2025-01-21.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "17.01",
        "en": "01/17",
        "zh": "1月17日"
    },
    "short_date_next": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1月21日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了生成模型在各个领域的影响。研究发现，大语言模型在推理时增加计算量可以提高性能。扩散模型也可以通过增加去噪步骤来调整计算量，但收益通常在几十步后趋于平缓。作者探讨了扩散模型在推理时的计算行为，并通过实验发现，增加计算量可以显著提高生成图像的质量。不同的组件组合可以适应不同的应用场景。",
        "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
        "pinyin": "这篇文章讨论了生成模型在各个领域的影响。\nZhè piān wénzhāng tǎolùn le shēngchéng móxíng zài gègè lǐngyù de yǐngxiǎng.\n\n研究发现，大语言模型在推理时增加计算量可以提高性能。\nYánjiū fāxiàn, dà yǔyán móxíng zài tuīlǐ shí zēngjiā jìsuàn liàng kěyǐ tígāo xìngnéng.\n\n扩散模型也可以通过增加去噪步骤来调整计算量，但收益通常在几十步后趋于平缓。\nKuòsàn móxíng yě kěyǐ tōngguò zēngjiā qùzào bùzhòu lái tiáozhěng jìsuàn liàng, dàn shōuyì tōngcháng zài jǐshí bù hòu qūyú píngchuǎn.\n\n作者探讨了扩散模型在推理时的计算行为，并通过实验发现，增加计算量可以显著提高生成图像的质量。\nZuòzhě tàntǎo le kuòsàn móxíng zài tuīlǐ shí de jìsuàn xíngwéi, bìng tōngguò shíyàn fāxiàn, zēngjiā jìsuàn liàng kěyǐ xiǎnzhù tígāo shēngchéng túxiàng de zhìliàng.\n\n不同的组件组合可以适应不同的应用场景。\nBùtóng de zǔjiàn zǔhé kěyǐ shìyìng bùtóng de yìngyòng chǎngjǐng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"field\"},\n    {\"word\": \"影响\", \"pinyin\": \"yǐng xiǎng\", \"trans\": \"influence\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"发现\", \"pinyin\": \"fā xiàn\", \"trans\": \"discover\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"增加\", \"pinyin\": \"zēng jiā\", \"trans\": \"increase\"},\n    {\"word\": \"计算量\", \"pinyin\": \"jì suàn liàng\", \"trans\": \"computational load\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"扩散\", \"pinyin\": \"kuò sàn\", \"trans\": \"diffusion\"},\n    {\"word\": \"去噪\", \"pinyin\": \"qù zào\", \"trans\": \"denoise\"},\n    {\"word\": \"步骤\", \"pinyin\": \"bù zhòu\", \"trans\": \"step\"},\n    {\"word\": \"调整\", \"pinyin\": \"tiáo zhěng\", \"trans\": \"adjust\"},\n    {\"word\": \"收益\", \"pinyin\": \"shōu yì\", \"trans\": \"benefit\"},\n    {\"word\": \"趋于\", \"pinyin\": \"qū yú\", \"trans\": \"tend towards\"},\n    {\"word\": \"平缓\", \"pinyin\": \"píng huǎn\", \"trans\": \"gentle\"},\n    {\"word\": \"作者\", \"pinyin\": \"zuò zhě\", \"trans\": \"author\"},\n    {\"word\": \"探讨\", \"pinyin\": \"tàn tǎo\", \"trans\": \"explore\"},\n    {\"word\": \"行为\", \"pinyin\": \"xíng wéi\", \"trans\": \"behavior\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"图像\", \"pinyin\": \"tú xiàng\", \"trans\": \"image\"},\n    {\"word\": \"质量\", \"pinyin\": \"zhì liàng\", \"trans\": \"quality\"},\n    {\"word\": \"组件\", \"pinyin\": \"zǔ jiàn\", \"trans\": \"component\"},\n    {\"word\": \"组合\", \"pinyin\": \"zǔ hé\", \"trans\": \"combination\"},\n    {\"word\": \"适应\", \"pinyin\": \"shì yìng\", \"trans\": \"adapt\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"场景\", \"pinyin\": \"chǎng jǐng\", \"trans\": \"scenario\"}\n]",
        "trans": "This article discusses the impact of generative models across various domains. Research has found that increasing the computational load during inference can enhance the performance of large language models. Diffusion models can also adjust their computational load by increasing the number of denoising steps, although the benefits typically plateau after a few dozen steps. The authors explore the computational behavior of diffusion models during inference and, through experiments, discover that increasing the computational load can significantly improve the quality of generated images. Different combinations of components can be adapted to suit different application scenarios.",
        "update_ts": "2025-01-19 12:36"
    }
}