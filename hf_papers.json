{
    "date": {
        "ru": "22 января",
        "en": "January 22",
        "zh": "1月22日"
    },
    "time_utc": "2025-01-22 08:13",
    "weekday": 2,
    "issue_id": 1800,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.11425",
            "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
            "url": "https://huggingface.co/papers/2501.11425",
            "abstract": "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive environments. Existing work mainly focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions based on correctness, Agent-R leverages MCTS to construct training data that recover correct trajectories from erroneous ones. A key challenge of agent reflection lies in the necessity for timely revision rather than waiting until the end of a rollout. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that Agent-R continuously improves the model's ability to recover from errors and enables timely error correction. Experiments on three interactive environments show that Agent-R effectively equips agents to correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%).",
            "score": 31,
            "issue_id": 1798,
            "pub_date": "2025-01-20",
            "pub_date_card": {
                "ru": "20 января",
                "en": "January 20",
                "zh": "1月20日"
            },
            "hash": "96d073b4606b0493",
            "authors": [
                "Siyu Yuan",
                "Zehui Chen",
                "Zhiheng Xi",
                "Junjie Ye",
                "Zhengyin Du",
                "Jiecao Chen"
            ],
            "affiliations": [
                "ByteDance",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11425.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#training",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самообучающиеся ИИ-агенты: исправление ошибок на лету",
                    "desc": "Статья представляет новый метод обучения языковых агентов на основе искусственного интеллекта под названием Agent-R. Этот подход использует самообучение и самокритику для улучшения способности модели исправлять ошибки в процессе выполнения задач. Agent-R применяет метод Монте-Карло для построения дерева поиска (MCTS) для создания обучающих данных, которые помогают агенту восстанавливаться после ошибочных действий. Эксперименты показывают, что Agent-R значительно повышает производительность агентов в интерактивных средах по сравнению с базовыми методами."
                },
                "en": {
                    "title": "Empowering Language Agents with Real-Time Self-Critique",
                    "desc": "This paper introduces Agent-R, an iterative self-training framework designed to enhance the performance of Large Language Models (LLMs) in interactive environments. Unlike traditional methods that rely on static feedback, Agent-R utilizes Monte Carlo Tree Search (MCTS) to dynamically create training data that helps models recover from mistakes in real-time. The framework focuses on timely error correction by identifying the first error in a trajectory and splicing it with a correct path, allowing the model to learn from its current policy. Experimental results show that Agent-R significantly improves the model's error recovery capabilities and overall performance, outperforming baseline methods by 5.59%."
                },
                "zh": {
                    "title": "Agent-R：实时反思，提升学习效率",
                    "desc": "大型语言模型（LLMs）在复杂任务的交互环境中变得越来越重要。现有研究主要通过模仿更强专家的行为来提升性能，但这种方法在实际应用中常常失败，主要是因为无法从错误中恢复。为了解决这个问题，我们提出了一种迭代自我训练框架Agent-R，使语言代理能够实时反思。Agent-R通过构建训练数据来纠正错误轨迹，从而提高模型的学习效率和错误恢复能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12380",
            "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
            "url": "https://huggingface.co/papers/2501.12380",
            "abstract": "We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark for evaluating foundation models in video understanding. MMVU includes 3,000 expert-annotated questions spanning 27 subjects across four core disciplines: Science, Healthcare, Humanities & Social Sciences, and Engineering. Compared to prior benchmarks, MMVU features three key advancements. First, it challenges models to apply domain-specific knowledge and perform expert-level reasoning to analyze specialized-domain videos, moving beyond the basic visual perception typically assessed in current video benchmarks. Second, each example is annotated by human experts from scratch. We implement strict data quality controls to ensure the high quality of the dataset. Finally, each example is enriched with expert-annotated reasoning rationals and relevant domain knowledge, facilitating in-depth analysis. We conduct an extensive evaluation of 32 frontier multimodal foundation models on MMVU. The latest System-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest performance among the tested models. However, they still fall short of matching human expertise. Through in-depth error analyses and case studies, we offer actionable insights for future advancements in expert-level, knowledge-intensive video understanding for specialized domains.",
            "score": 29,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "dcb04aaca349cc32",
            "authors": [
                "Yilun Zhao",
                "Lujing Xie",
                "Haowei Zhang",
                "Guo Gan",
                "Yitao Long",
                "Zhiyuan Hu",
                "Tongyan Hu",
                "Weiyuan Chen",
                "Chuhan Li",
                "Junyang Song",
                "Zhijian Xu",
                "Chengye Wang",
                "Weifeng Pan",
                "Ziyao Shangguan",
                "Xiangru Tang",
                "Zhenwen Liang",
                "Yixin Liu",
                "Chen Zhao",
                "Arman Cohan"
            ],
            "affiliations": [
                "Yale NLP"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12380.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#benchmark",
                    "#video",
                    "#healthcare",
                    "#reasoning"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "Новый рубеж в понимании видео: от базового восприятия к экспертному анализу",
                    "desc": "Статья представляет MMVU - многодисциплинарный экспертный бенчмарк для оценки фундаментальных моделей в понимании видео. MMVU включает 3000 вопросов по 27 предметам в четырех основных дисциплинах, требующих применения специализированных знаний и экспертного анализа. Бенчмарк отличается высоким качеством данных, аннотированных экспертами, и включает обоснования и релевантные знания для каждого примера. Оценка 32 мультимодальных моделей на MMVU показала, что даже лучшие модели пока не достигают уровня человека-эксперта в этой задаче."
                },
                "en": {
                    "title": "MMVU: Elevating Video Understanding to Expert Levels",
                    "desc": "The paper presents MMVU, a new benchmark designed to evaluate foundation models specifically in video understanding across various expert domains. It includes 3,000 questions that require advanced reasoning and domain-specific knowledge, moving beyond simple visual recognition tasks. Each question is meticulously annotated by human experts, ensuring high data quality and providing reasoning rationales to enhance analysis. The evaluation of 32 advanced multimodal models reveals that while some perform well, they still do not reach the level of human expertise, highlighting areas for future improvement in this field."
                },
                "zh": {
                    "title": "MMVU：视频理解的新标准",
                    "desc": "我们介绍了MMVU，这是一个全面的专家级多学科基准，用于评估基础模型在视频理解方面的表现。MMVU包含3000个专家注释的问题，涵盖科学、医疗、人文学科与社会科学和工程四个核心学科。与之前的基准相比，MMVU在三个关键方面有所改进，包括要求模型应用领域特定知识进行专家级推理，确保数据集的高质量，以及为每个示例提供专家注释的推理依据和相关领域知识。我们对32个前沿多模态基础模型在MMVU上的表现进行了广泛评估，发现最新的系统2能力模型o1和Gemini 2.0 Flash Thinking在测试模型中表现最佳，但仍未能达到人类专家的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11873",
            "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models",
            "url": "https://huggingface.co/papers/2501.11873",
            "abstract": "This paper revisits the implementation of Load-balancing Loss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as N_E sum_{i=1}^{N_E} f_i p_i, where N_E is the total number of experts, f_i represents the frequency of expert i being selected, and p_i denotes the average gating score of the expert i. Existing MoE training frameworks usually employ the parallel training strategy so that f_i and the LBL are calculated within a micro-batch and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence (e.g., code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a global-batch to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize f_i across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to 42.8B total parameters and 400B tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.",
            "score": 27,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "370d057fec504963",
            "authors": [
                "Zihan Qiu",
                "Zeyu Huang",
                "Bo Zheng",
                "Kaiyue Wen",
                "Zekun Wang",
                "Rui Men",
                "Ivan Titov",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Group",
                "Stanford University",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11873.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Глобальный подход к балансировке нагрузки экспертов в MoE моделях",
                    "desc": "Статья предлагает новый подход к реализации функции потерь балансировки нагрузки (LBL) при обучении моделей Mixture-of-Experts (MoE). Авторы предлагают вычислять LBL на уровне глобального батча, а не микро-батча, что позволяет ослабить ограничения на распределение токенов между экспертами. Эксперименты на крупномасштабных языковых моделях показывают, что этот метод улучшает перплексию при предобучении и результаты на задачах downstream. Анализ также демонстрирует улучшение специализации экспертов по доменам."
                },
                "en": {
                    "title": "Enhancing Expert Specialization with Global-Batch Load-Balancing",
                    "desc": "This paper focuses on improving the Load-balancing Loss (LBL) in training Mixture-of-Experts (MoEs) models. The authors highlight that traditional methods use micro-batches, which limit the diversity of sequences and hinder expert specialization. They propose a new approach that utilizes global-batches, allowing for a broader range of sequences and better load balancing across the entire dataset. Experimental results show that this global-batch LBL method significantly enhances model performance and expert specialization in large language models."
                },
                "zh": {
                    "title": "全局批次提升混合专家模型的负载均衡与专业化",
                    "desc": "本文重新审视了在训练混合专家模型（MoEs）时的负载均衡损失（LBL）实现。我们提出使用全局批次来计算LBL，以打破微批次的严格约束，从而在语料库层面上促进负载均衡。通过在训练中引入额外的通信步骤来同步专家选择频率，实验结果显示全局批次LBL策略在预训练困惑度和下游任务中均显著提升了性能。我们的分析表明，全局批次LBL还大大改善了MoE专家的领域专业化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11733",
            "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
            "url": "https://huggingface.co/papers/2501.11733",
            "abstract": "Smartphones have become indispensable in modern life, yet navigating complex tasks on mobile devices often remains frustrating. Recent advancements in large multimodal model (LMM)-based mobile agents have demonstrated the ability to perceive and act in mobile environments. However, current approaches face significant limitations: they fall short in addressing real-world human needs, struggle with reasoning-intensive and long-horizon tasks, and lack mechanisms to learn and improve from prior experiences. To overcome these challenges, we introduce Mobile-Agent-E, a hierarchical multi-agent framework capable of self-evolution through past experience. By hierarchical, we mean an explicit separation of high-level planning and low-level action execution. The framework comprises a Manager, responsible for devising overall plans by breaking down complex tasks into subgoals, and four subordinate agents--Perceptor, Operator, Action Reflector, and Notetaker--which handle fine-grained visual perception, immediate action execution, error verification, and information aggregation, respectively. Mobile-Agent-E also features a novel self-evolution module which maintains a persistent long-term memory comprising Tips and Shortcuts. Tips are general guidance and lessons learned from prior tasks on how to effectively interact with the environment. Shortcuts are reusable, executable sequences of atomic operations tailored for specific subroutines. The inclusion of Tips and Shortcuts facilitates continuous refinement in performance and efficiency. Alongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring complex mobile tasks requiring long-horizon, multi-app interactions. Empirical results show that Mobile-Agent-E achieves a 22% absolute improvement over previous state-of-the-art approaches across three foundation model backbones. Project page: https://x-plug.github.io/MobileAgent.",
            "score": 12,
            "issue_id": 1798,
            "pub_date": "2025-01-20",
            "pub_date_card": {
                "ru": "20 января",
                "en": "January 20",
                "zh": "1月20日"
            },
            "hash": "a9cddb8786536def",
            "authors": [
                "Zhenhailong Wang",
                "Haiyang Xu",
                "Junyang Wang",
                "Xi Zhang",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Heng Ji"
            ],
            "affiliations": [
                "Alibaba Group",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11733.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "Мобильный ИИ-ассистент с самообучением для сложных задач",
                    "desc": "Статья представляет Mobile-Agent-E - иерархическую мультиагентную систему для выполнения сложных задач на мобильных устройствах. Система включает Менеджера для планирования и четыре подчиненных агента для восприятия, выполнения действий, проверки ошибок и агрегации информации. Ключевой особенностью является модуль самоэволюции с долговременной памятью, содержащей Подсказки и Ярлыки для улучшения производительности. Эмпирические результаты показывают значительное улучшение по сравнению с предыдущими подходами на новом бенчмарке Mobile-Eval-E."
                },
                "en": {
                    "title": "Empowering Mobile Agents with Self-Evolution for Enhanced Task Performance",
                    "desc": "This paper presents Mobile-Agent-E, a hierarchical multi-agent framework designed to enhance mobile task performance by learning from past experiences. The framework separates high-level planning from low-level execution, utilizing a Manager for task decomposition and four specialized agents for perception, action, error checking, and information management. A key feature is the self-evolution module, which incorporates a long-term memory of Tips and Shortcuts to improve task efficiency and effectiveness. Experimental results demonstrate that Mobile-Agent-E significantly outperforms existing methods, achieving a 22% improvement in complex mobile tasks."
                },
                "zh": {
                    "title": "智能手机任务执行的新突破",
                    "desc": "本论文介绍了一种名为Mobile-Agent-E的层次化多智能体框架，旨在提升智能手机上的任务执行能力。该框架通过将高层规划与低层执行明确分离，包含一个管理者和四个子代理，分别负责视觉感知、动作执行、错误验证和信息聚合。Mobile-Agent-E还引入了自我进化模块，利用长期记忆中的提示和捷径来不断优化性能。实验结果表明，该框架在复杂移动任务中相较于现有方法有22%的绝对提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12326",
            "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
            "url": "https://huggingface.co/papers/2501.12326",
            "abstract": "This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.",
            "score": 9,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "1f98d8f49b073983",
            "authors": [
                "Yujia Qin",
                "Yining Ye",
                "Junjie Fang",
                "Haoming Wang",
                "Shihao Liang",
                "Shizuo Tian",
                "Junda Zhang",
                "Jiahao Li",
                "Yunxin Li",
                "Shijue Huang",
                "Wanjun Zhong",
                "Kuanye Li",
                "Jiale Yang",
                "Yu Miao",
                "Woyu Lin",
                "Longxiang Liu",
                "Xu Jiang",
                "Qianli Ma",
                "Jingyu Li",
                "Xiaojun Xiao",
                "Kai Cai",
                "Chuang Li",
                "Yaowei Zheng",
                "Chaolin Jin",
                "Chen Li",
                "Xiao Zhou",
                "Minchao Wang",
                "Haoli Chen",
                "Zhaojian Li",
                "Haihua Yang",
                "Haifeng Liu",
                "Feng Lin",
                "Tao Peng",
                "Xin Liu",
                "Guang Shi"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12326.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#agents",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "UI-TARS: Революция в мире GUI-агентов",
                    "desc": "Статья представляет UI-TARS - модель графического агента, которая воспринимает только скриншоты и выполняет операции, подобные человеческим. UI-TARS превосходит существующие фреймворки агентов, достигая лучших результатов в более чем 10 бенчмарках для GUI-агентов. Модель включает в себя несколько ключевых инноваций: улучшенное восприятие, унифицированное моделирование действий, рассуждение по системе-2 и итеративное обучение с рефлексивными онлайн-трассами. UI-TARS постоянно учится на своих ошибках и адаптируется к непредвиденным ситуациям с минимальным вмешательством человека."
                },
                "en": {
                    "title": "Revolutionizing GUI Interaction with UI-TARS: The End-to-End Agent Model",
                    "desc": "UI-TARS is a novel GUI agent model that processes screenshots to perform tasks like a human would, using keyboard and mouse actions. Unlike existing models that rely on complex commercial frameworks and pre-defined prompts, UI-TARS operates end-to-end and shows superior performance in various benchmarks. It achieves state-of-the-art results in GUI task execution by utilizing enhanced perception, unified action modeling, and system-2 reasoning for better decision-making. Additionally, its iterative training approach allows it to learn from past interactions, improving its adaptability with minimal human input."
                },
                "zh": {
                    "title": "UI-TARS：革新图形用户界面代理的全新模型",
                    "desc": "本文介绍了UI-TARS，这是一种原生的图形用户界面（GUI）代理模型，能够仅通过屏幕截图进行人类般的交互。与依赖复杂商业模型的现有代理框架不同，UI-TARS是一个端到端的模型，在多个GUI代理基准测试中表现优异，尤其在感知、定位和任务执行方面。UI-TARS通过增强感知、统一动作建模、系统-2推理和反思在线追踪等创新，显著提高了其性能。通过迭代训练和反思调优，UI-TARS能够不断学习并适应新的情况，减少对人类干预的需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12273",
            "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement",
            "url": "https://huggingface.co/papers/2501.12273",
            "abstract": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.",
            "score": 9,
            "issue_id": 1796,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "10499c8b820d5368",
            "authors": [
                "Maosong Cao",
                "Taolin Zhang",
                "Mo Li",
                "Chuyu Zhang",
                "Yunxin Liu",
                "Haodong Duan",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12273.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🦅",
                "ru": {
                    "title": "Condor: прорыв в создании синтетических данных для обучения языковых моделей",
                    "desc": "В статье представлен Condor - новый фреймворк для генерации синтетических данных для обучения больших языковых моделей (LLM). Он использует дерево мировых знаний и самоанализ для создания высококачественных обучающих данных. Эксперименты показали, что модель, обученная на 20 тысячах сгенерированных Condor примеров, превосходит аналоги. Исследование также выявило потенциал для улучшения производительности LLM при масштабировании синтетических данных."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Synthetic Data Generation",
                    "desc": "This paper addresses the challenge of obtaining high-quality Supervised Fine-Tuning (SFT) data for Large Language Models (LLMs). It presents Condor, a two-stage framework that generates synthetic training data using World Knowledge Tree and Self-Reflection Refinement techniques. The results show that models fine-tuned with just 20,000 samples from Condor outperform those trained with traditional methods. Additionally, the framework allows for iterative self-improvement, suggesting significant potential for enhancing LLM performance through synthetic data."
                },
                "zh": {
                    "title": "合成数据生成，提升对话能力的关键",
                    "desc": "本论文探讨了监督微调（SFT）数据的质量对大型语言模型（LLMs）对话能力的重要性。随着LLMs的进步，高质量的人类标注SFT数据变得稀缺，因此需要更多依赖合成训练数据。我们提出了一种名为Condor的两阶段合成数据生成框架，结合了世界知识树和自我反思精炼，以大规模生成高质量的SFT数据。实验结果表明，仅用20K个Condor生成的样本微调的基础模型，其性能优于其他模型，验证了我们方法的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12202",
            "title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
            "url": "https://huggingface.co/papers/2501.12202",
            "abstract": "We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: a large-scale shape generation model -- Hunyuan3D-DiT, and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly aligns with a given condition image, laying a solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps in the open-source 3D community for large-scale foundation generative models. The code and pre-trained weights of our models are available at: https://github.com/Tencent/Hunyuan3D-2",
            "score": 8,
            "issue_id": 1798,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "f95f069cba0bd83e",
            "authors": [
                "Zibo Zhao",
                "Zeqiang Lai",
                "Qingxiang Lin",
                "Yunfei Zhao",
                "Haolin Liu",
                "Shuhui Yang",
                "Yifei Feng",
                "Mingxin Yang",
                "Sheng Zhang",
                "Xianghui Yang",
                "Huiwen Shi",
                "Sicong Liu",
                "Junta Wu",
                "Yihang Lian",
                "Fan Yang",
                "Ruining Tang",
                "Zebin He",
                "Xinzhou Wang",
                "Jian Liu",
                "Xuhui Zuo",
                "Zhuo Chen",
                "Biwen Lei",
                "Haohan Weng",
                "Jing Xu",
                "Yiling Zhu",
                "Xinhai Liu",
                "Lixin Xu",
                "Changrong Hu",
                "Tianyu Huang",
                "Lifu Wang",
                "Jihong Zhang",
                "Meng Chen",
                "Liang Dong",
                "Yiwen Jia",
                "Yulin Cai",
                "Jiaao Yu",
                "Yixuan Tang",
                "Hao Zhang",
                "Zheng Ye",
                "Peng He",
                "Runzhou Wu",
                "Chao Zhang",
                "Yonghao Tan",
                "Jie Xiao",
                "Yangyu Tao",
                "Jianchen Zhu",
                "Jinbao Xue",
                "Kai Liu",
                "Chongqing Zhao",
                "Xinming Wu",
                "Zhichao Hu",
                "Lei Qin",
                "Jianbing Peng",
                "Zhan Li",
                "Minghui Chen",
                "Xipeng Zhang",
                "Lin Niu",
                "Paige Wang",
                "Yingkai Wang",
                "Haozhao Kuang",
                "Zhongyi Fan",
                "Xu Zheng",
                "Weihao Zhuang",
                "YingPing He",
                "Tian Liu",
                "Yong Yang",
                "Di Wang",
                "Yuhong Liu",
                "Jie Jiang",
                "Jingwei Huang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12202.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в 3D-генерации: от формы к текстуре",
                    "desc": "Hunyuan3D 2.0 - это продвинутая система для создания трехмерных текстурированных объектов высокого разрешения. Она состоит из двух основных компонентов: модели генерации форм Hunyuan3D-DiT и модели синтеза текстур Hunyuan3D-Paint. Модель генерации форм основана на масштабируемом диффузионном трансформере и создает геометрию, соответствующую заданному изображению. Модель синтеза текстур, используя геометрические и диффузионные праймы, создает высококачественные текстурные карты для сгенерированных или созданных вручную мешей."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Creation with Hunyuan3D 2.0",
                    "desc": "Hunyuan3D 2.0 is a sophisticated system designed for creating high-quality 3D models with detailed textures. It consists of two main components: Hunyuan3D-DiT for generating 3D shapes and Hunyuan3D-Paint for applying textures. The shape model uses a flow-based diffusion transformer to ensure that the generated geometry matches the input conditions, while the texture model leverages geometric and diffusion principles to create vibrant textures. This system not only enhances the quality of 3D assets but also provides an accessible platform for users to create and animate their models easily."
                },
                "zh": {
                    "title": "Hunyuan3D 2.0：高效生成高质量3D资产的系统",
                    "desc": "Hunyuan3D 2.0 是一个先进的大规模 3D 合成系统，能够生成高分辨率的纹理 3D 资产。该系统包含两个基础组件：Hunyuan3D-DiT 形状生成模型和 Hunyuan3D-Paint 纹理合成模型。形状生成模型基于可扩展的流式扩散变换器，旨在创建与给定条件图像相匹配的几何形状。纹理合成模型则利用强大的几何和扩散先验，为生成或手工制作的网格生成高分辨率的生动纹理图。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10687",
            "title": "EMO2: End-Effector Guided Audio-Driven Avatar Video Generation",
            "url": "https://huggingface.co/papers/2501.10687",
            "abstract": "In this paper, we propose a novel audio-driven talking head method capable of simultaneously generating highly expressive facial expressions and hand gestures. Unlike existing methods that focus on generating full-body or half-body poses, we investigate the challenges of co-speech gesture generation and identify the weak correspondence between audio features and full-body gestures as a key limitation. To address this, we redefine the task as a two-stage process. In the first stage, we generate hand poses directly from audio input, leveraging the strong correlation between audio signals and hand movements. In the second stage, we employ a diffusion model to synthesize video frames, incorporating the hand poses generated in the first stage to produce realistic facial expressions and body movements. Our experimental results demonstrate that the proposed method outperforms state-of-the-art approaches, such as CyberHost and Vlogger, in terms of both visual quality and synchronization accuracy. This work provides a new perspective on audio-driven gesture generation and a robust framework for creating expressive and natural talking head animations.",
            "score": 7,
            "issue_id": 1798,
            "pub_date": "2025-01-18",
            "pub_date_card": {
                "ru": "18 января",
                "en": "January 18",
                "zh": "1月18日"
            },
            "hash": "13c0931101eb51eb",
            "authors": [
                "Linrui Tian",
                "Siqi Hu",
                "Qi Wang",
                "Bang Zhang",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Institute for Intelligent Computing, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10687.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#audio",
                    "#video",
                    "#games",
                    "#diffusion"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Революция в анимации: от звука к выразительным жестам",
                    "desc": "В статье предлагается новый метод создания говорящей головы на основе аудио, способный одновременно генерировать выразительные мимику и жесты рук. Авторы определяют задачу как двухэтапный процесс: сначала генерируются позы рук непосредственно из аудиовхода, затем применяется диффузионная модель для синтеза видеокадров. Экспериментальные результаты показывают, что предложенный метод превосходит современные подходы по качеству изображения и точности синхронизации. Работа предоставляет новый взгляд на генерацию жестов на основе аудио и надежную основу для создания выразительных и естественных анимаций говорящей головы."
                },
                "en": {
                    "title": "Expressive Talking Heads: Bridging Audio and Gesture Generation",
                    "desc": "This paper introduces a new method for creating talking head animations that are driven by audio. It focuses on generating both facial expressions and hand gestures, addressing the limitations of previous methods that often overlook the connection between audio and gestures. The approach is divided into two stages: first, it generates hand poses from audio signals, and then it uses a diffusion model to create video frames that combine these hand poses with realistic facial movements. The results show that this method is more effective than existing techniques, providing better visual quality and synchronization with the audio."
                },
                "zh": {
                    "title": "音频驱动的生动表情与手势生成新方法",
                    "desc": "本文提出了一种新颖的音频驱动的说话头方法，能够同时生成高度表现力的面部表情和手势。与现有方法不同，我们关注于共语手势生成的挑战，并识别音频特征与全身手势之间的弱对应关系。为了解决这个问题，我们将任务重新定义为两个阶段：第一阶段直接从音频输入生成手势，第二阶段使用扩散模型合成视频帧，结合第一阶段生成的手势，产生逼真的面部表情和身体动作。实验结果表明，该方法在视觉质量和同步精度方面优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11223",
            "title": "Reasoning Language Models: A Blueprint",
            "url": "https://huggingface.co/papers/2501.11223",
            "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending large language models (LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures - uniquely combining Reinforcement Learning (RL), search heuristics, and LLMs - present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), and supervision schemes (Output-Based and Process-Based Supervision). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint's versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we outline how RLMs can integrate with a broader LLM ecosystem, including tools and databases. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and experimentation.",
            "score": 7,
            "issue_id": 1797,
            "pub_date": "2025-01-20",
            "pub_date_card": {
                "ru": "20 января",
                "en": "January 20",
                "zh": "1月20日"
            },
            "hash": "f554416ad9af3344",
            "authors": [
                "Maciej Besta",
                "Julia Barth",
                "Eric Schreiber",
                "Ales Kubicek",
                "Afonso Catarino",
                "Robert Gerstenberger",
                "Piotr Nyczyk",
                "Patrick Iff",
                "Yueling Li",
                "Sam Houliston",
                "Tomasz Sternal",
                "Marcin Copik",
                "Grzegorz Kwaśniewski",
                "Jürgen Müller",
                "Łukasz Flis",
                "Hannes Eberhard",
                "Hubert Niewiadomski",
                "Torsten Hoefler"
            ],
            "affiliations": [
                "BASF SE",
                "Cledar",
                "Cyfronet AGH",
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11223.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#math",
                    "#training",
                    "#survey",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Демократизация искусственного интеллекта: модульный подход к созданию моделей рассуждений",
                    "desc": "Статья представляет комплексный подход к созданию моделей рассуждений (RLM), объединяющих языковые модели с механизмами продвинутых рассуждений. Авторы предлагают модульную структуру, включающую различные стратегии рассуждений, концепции обучения с подкреплением и схемы обучения. Они демонстрируют применимость этой структуры на примере существующих моделей и представляют x1 - модульную реализацию для быстрого прототипирования RLM. Исследование направлено на демократизацию возможностей продвинутых рассуждений в ИИ и снижение барьеров для разработки RLM."
                },
                "en": {
                    "title": "Democratizing Advanced Reasoning in AI",
                    "desc": "This paper introduces a modular framework for Reasoning Language Models (RLMs), which enhance traditional Large Language Models (LLMs) with advanced reasoning capabilities. The authors address the challenges of high costs and complex architectures by organizing RLM components into a comprehensive blueprint that includes various reasoning structures and strategies. They provide mathematical formulations and algorithmic specifications to facilitate easier implementation of RLMs. Additionally, the paper presents x1, a tool for rapid prototyping, and discusses how RLMs can be integrated into the larger LLM ecosystem to promote accessibility and innovation in AI development."
                },
                "zh": {
                    "title": "简化推理语言模型，促进AI创新",
                    "desc": "推理语言模型（RLMs）通过结合强化学习、搜索启发式和大型语言模型（LLMs），重新定义了人工智能的解决问题能力。尽管它们具有强大的推理机制，但高成本和复杂架构使得其可访问性和可扩展性面临挑战。为了解决这些问题，我们提出了一个模块化框架，组织RLM组件，并提供详细的数学公式和算法规范，以简化RLM的实现。我们的工作旨在降低RLM开发和实验的门槛，促进创新，缩小“富有AI”和“贫穷AI”之间的差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08331",
            "title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise",
            "url": "https://huggingface.co/papers/2501.08331",
            "abstract": "Generative modeling aims to transform random noise into structured outputs. In this work, we enhance video diffusion models by allowing motion control via structured latent noise sampling. This is achieved by just a change in data: we pre-process training videos to yield structured noise. Consequently, our method is agnostic to diffusion model design, requiring no changes to model architectures or training pipelines. Specifically, we propose a novel noise warping algorithm, fast enough to run in real time, that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, while preserving the spatial Gaussianity. The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. The harmonization between temporal coherence and spatial Gaussianity in our warped noise leads to effective motion control while maintaining per-frame pixel quality. Extensive experiments and user studies demonstrate the advantages of our method, making it a robust and scalable approach for controlling motion in video diffusion models. Video results are available on our webpage: https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code and model checkpoints are available on GitHub: https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.",
            "score": 4,
            "issue_id": 1798,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "c48e19ef08e8d758",
            "authors": [
                "Ryan Burgert",
                "Yuancheng Xu",
                "Wenqi Xian",
                "Oliver Pilarski",
                "Pascal Clausen",
                "Mingming He",
                "Li Ma",
                "Yitong Deng",
                "Lingxiao Li",
                "Mohsen Mousavi",
                "Michael Ryoo",
                "Paul Debevec",
                "Ning Yu"
            ],
            "affiliations": [
                "Eyeline Studios",
                "Netflix",
                "Stanford University",
                "Stony Brook University",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08331.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#data"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Контроль движения в видео-диффузии через структурированный шум",
                    "desc": "Исследователи предложили метод улучшения видео-диффузионных моделей путем изменения структуры шумовых данных при обучении. Они разработали алгоритм искажения шума в реальном времени, который сохраняет пространственную гауссовость, но вводит временную корреляцию на основе оптического потока. Этот подход позволяет контролировать движение в генерируемых видео без изменения архитектуры модели. Эксперименты показали эффективность метода для управления локальным движением объектов, глобальным движением камеры и переносом движения."
                },
                "en": {
                    "title": "Transforming Noise into Motion: Enhanced Control in Video Diffusion Models",
                    "desc": "This paper presents an improvement in video diffusion models by introducing a method for controlling motion through structured latent noise sampling. The authors propose a novel noise warping algorithm that modifies the training data to replace random noise with correlated noise based on optical flow, enhancing temporal coherence while maintaining spatial quality. This approach allows for real-time processing and fine-tuning of existing video diffusion models without altering their architecture or training methods. The results show that this method effectively enables various motion control tasks, making it a versatile tool for video generation applications."
                },
                "zh": {
                    "title": "运动控制的新方法：扭曲噪声的力量",
                    "desc": "生成建模的目标是将随机噪声转化为结构化输出。本文通过结构化潜在噪声采样增强视频扩散模型，实现了运动控制。我们提出了一种新颖的噪声扭曲算法，能够实时运行，并用光流场导出的相关扭曲噪声替代随机时间高斯噪声，同时保持空间高斯性。我们的算法高效性使得在现代视频扩散基础模型中使用扭曲噪声进行微调成为可能，提供了用户友好的运动控制解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12375",
            "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
            "url": "https://huggingface.co/papers/2501.12375",
            "abstract": "Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos (< 10 seconds) and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos (over several minutes) without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.",
            "score": 4,
            "issue_id": 1798,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "00640fb6adcf39e3",
            "authors": [
                "Sili Chen",
                "Hengkai Guo",
                "Shengnan Zhu",
                "Feihu Zhang",
                "Zilong Huang",
                "Jiashi Feng",
                "Bingyi Kang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12375.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#small_models",
                    "#video",
                    "#cv",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Согласованная оценка глубины для сверхдлинных видео",
                    "desc": "В статье представлен метод Video Depth Anything для оценки глубины в сверхдлинных видео с высоким качеством и временной согласованностью. Модель основана на Depth Anything V2 с новой пространственно-временной головой и использует эффективную функцию потерь для обеспечения временной согласованности. Предложенный подход позволяет обрабатывать видео произвольной длительности без ущерба для качества и обобщающей способности. Метод достигает наилучших результатов в задаче zero-shot оценки глубины видео на нескольких бенчмарках."
                },
                "en": {
                    "title": "Achieving Consistent Depth Estimation in Long Videos",
                    "desc": "This paper introduces Video Depth Anything, a model designed for accurate depth estimation in long videos, overcoming the limitations of previous methods that struggled with temporal consistency. The model builds on Depth Anything V2, enhancing it with a spatial-temporal head and a novel temporal consistency loss that focuses on the depth gradient over time. By training on a combined dataset of video depth and unlabeled images, the model achieves high-quality depth estimation without the need for complex geometric priors. The results demonstrate that Video Depth Anything can handle videos of any length while maintaining efficiency and setting new benchmarks in zero-shot video depth estimation."
                },
                "zh": {
                    "title": "超长视频深度估计的新突破",
                    "desc": "本文提出了一种名为Video Depth Anything的新模型，旨在解决单目深度估计在视频中的时间一致性问题。该模型能够在超长视频（超过几分钟）中实现高质量和一致性的深度估计，而不牺牲计算效率。我们通过设计一个简单有效的时间一致性损失，来约束时间深度梯度，从而避免了额外几何先验的需求。实验结果表明，该模型在多个视频基准测试中表现出色，设定了零-shot视频深度估计的新状态。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12390",
            "title": "GPS as a Control Signal for Image Generation",
            "url": "https://huggingface.co/papers/2501.12390",
            "abstract": "We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure.",
            "score": 4,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 января",
                "en": "January 21",
                "zh": "1月21日"
            },
            "hash": "11d289e8a895bedd",
            "authors": [
                "Chao Feng",
                "Ziyang Chen",
                "Aleksander Holynski",
                "Alexei A. Efros",
                "Andrew Owens"
            ],
            "affiliations": [
                "UC Berkeley",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12390.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "GPS-метки открывают новые горизонты в генерации изображений и 3D-моделировании",
                    "desc": "Исследователи демонстрируют, как GPS-метки в метаданных фотографий могут использоваться для улучшения генерации изображений. Они обучают модели диффузии, генерирующие изображения на основе GPS-координат и текста, что позволяет точно отображать особенности различных районов и достопримечательностей. Авторы также извлекают 3D-модели из 2D GPS-моделей с помощью методики score distillation sampling. Результаты показывают, что GPS-обусловленные модели успешно генерируют изображения, варьирующиеся в зависимости от местоположения, и улучшают оценку 3D-структуры."
                },
                "en": {
                    "title": "Harnessing GPS Data for Location-Aware Image Generation",
                    "desc": "This paper explores the use of GPS data embedded in photo metadata as a control signal for generating images. The authors develop GPS-to-image models, particularly a diffusion model, that can create images based on both GPS coordinates and textual descriptions. The model effectively captures the unique characteristics of various urban environments, such as neighborhoods and landmarks. Additionally, they demonstrate the ability to extract 3D models from these images, enhancing the accuracy of 3D reconstructions by using GPS information to guide the process."
                },
                "zh": {
                    "title": "利用GPS标签生成城市图像的创新方法",
                    "desc": "本文展示了照片元数据中的GPS标签可以作为图像生成的有用控制信号。我们训练了GPS到图像的模型，并将其应用于需要细致理解城市中图像变化的任务。特别地，我们训练了一个扩散模型，生成同时依赖于GPS和文本的图像。评估结果表明，我们的GPS条件模型成功学习了基于位置生成变化图像，并且GPS条件改善了估计的3D结构。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10893",
            "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
            "url": "https://huggingface.co/papers/2501.10893",
            "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis. The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the corresponding environments they interact with. We propose Learn-by-interact, a data-centric framework to adapt LLM agents to any given environments without human annotations. Learn-by-interact synthesizes trajectories of agent-environment interactions based on documentations, and constructs instructions by summarizing or abstracting the interaction histories, a process called backward construction. We assess the quality of our synthetic data by using them in both training-based scenarios and training-free in-context learning (ICL), where we craft innovative retrieval approaches optimized for agents. Extensive experiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across realistic coding, web, and desktop environments show the effectiveness of Learn-by-interact in various downstream agentic tasks -- baseline results are improved by up to 12.2\\% for ICL with Claude-3.5 and 19.5\\% for training with Codestral-22B. We further demonstrate the critical role of backward construction, which provides up to 14.0\\% improvement for training. Our ablation studies demonstrate the efficiency provided by our synthesized data in ICL and the superiority of our retrieval pipeline over alternative approaches like conventional retrieval-augmented generation (RAG). We expect that Learn-by-interact will serve as a foundation for agent data synthesis as LLMs are increasingly deployed at real-world environments.",
            "score": 3,
            "issue_id": 1798,
            "pub_date": "2025-01-18",
            "pub_date_card": {
                "ru": "18 января",
                "en": "January 18",
                "zh": "1月18日"
            },
            "hash": "b6ab4c9ac3809941",
            "authors": [
                "Hongjin Su",
                "Ruoxi Sun",
                "Jinsung Yoon",
                "Pengcheng Yin",
                "Tao Yu",
                "Sercan Ö. Arık"
            ],
            "affiliations": [
                "Google",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10893.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#synthetic",
                    "#training",
                    "#data",
                    "#rag",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение ИИ-агентов через синтетическое взаимодействие",
                    "desc": "Статья представляет Learn-by-interact - фреймворк для адаптации агентов на основе больших языковых моделей (LLM) к различным средам без аннотаций человека. Метод синтезирует траектории взаимодействия агента со средой на основе документации и создает инструкции путем обобщения истории взаимодействий. Эксперименты показывают эффективность подхода в различных задачах, улучшая базовые результаты до 19.5% при обучении. Авторы демонстрируют критическую роль обратного конструирования и превосходство их метода над альтернативными подходами."
                },
                "en": {
                    "title": "Empowering LLM Agents through Synthetic Interaction Data",
                    "desc": "This paper introduces Learn-by-interact, a framework designed to enhance the performance of large language model (LLM) agents in various environments without needing human-generated data. The framework generates synthetic data by simulating interactions between agents and their environments, using documentation to guide the process. A key innovation is the backward construction method, which summarizes interaction histories to create effective instructions for the agents. Experimental results show significant improvements in agent performance across multiple tasks, highlighting the framework's potential for real-world applications."
                },
                "zh": {
                    "title": "通过交互学习，提升智能代理能力",
                    "desc": "本文提出了一种名为Learn-by-interact的数据中心框架，旨在使大型语言模型（LLMs）能够适应不同的环境，而无需人工标注。该框架通过文档生成代理与环境交互的轨迹，并通过总结或抽象交互历史来构建指令，这一过程称为反向构建。实验结果表明，Learn-by-interact在多种下游任务中显著提高了性能，尤其是在无监督学习和训练场景中。我们还展示了反向构建在训练中的重要性，进一步验证了合成数据的有效性和检索管道的优越性。"
                }
            }
        }
    ],
    "link_prev": "2025-01-21.html",
    "link_next": "2025-01-23.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1月21日"
    },
    "short_date_next": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1月23日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 4,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种名为GameFactory的框架，旨在通过生成游戏引擎来革新游戏开发。它使用预训练的视频扩散模型，能够创建全新且多样化的游戏。为了解决现有方法在场景生成上的局限，作者提出了一种多阶段训练策略。他们还发布了一个基于Minecraft的高质量视频数据集，并展示了框架能够生成开放域、多样化和可控的游戏视频。",
        "title": "GameFactory: Creating New Games with Generative Interactive Videos",
        "pinyin": "这篇文章介绍了一种名为GameFactory的框架，旨在通过生成游戏引擎来革新游戏开发。它使用预训练的视频扩散模型，能够创建全新且多样化的游戏。为了解决现有方法在场景生成上的局限，作者提出了一种多阶段训练策略。他们还发布了一个基于Minecraft的高质量视频数据集，并展示了框架能够生成开放域、多样化和可控的游戏视频。\n\nzhè piān wén zhāng jiè shào le yī zhǒng míng wèi GameFactory de kuàng jià, zhǐ zài tōng guò shēng chéng yòu xí yǐn qíng lái gé xīn yòu xí kāi fā. tā shǐ yòng yù xùn liàn de shì pín kuò sàn mó xíng, néng gòu chuàng jiàn quán xīn qiě duō yàng huà de yòu xí. wèi le jiě jué xiàn yǒu fāng fǎ zài chǎng jīng shēng chéng shàng de jú xiàn, zuò zhě tí chū le yī zhǒng duō jiē duàn xùn liàn cè lüè. tā men hái fā bù le yī gè jī yú Minecraft de gāo zhì liàng shì pín shù jù jí, bìng zhàn shì le kuàng jià néng gòu shēng chéng kāi fàng yù, duō yàng huà hé kě kòng de yòu xí shì pín.",
        "vocab": "[{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'},\n{'word': '革新', 'pinyin': 'géxīn', 'trans': 'innovate'},\n{'word': '引擎', 'pinyin': 'yǐnqíng', 'trans': 'engine'},\n{'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-trained'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '多样化', 'pinyin': 'duōyànghuà', 'trans': 'diversified'},\n{'word': '局限', 'pinyin': 'júxiàn', 'trans': 'limitation'},\n{'word': '提出', 'pinyin': 'tíchū', 'trans': 'propose'},\n{'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'},\n{'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'},\n{'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high quality'},\n{'word': '数据集', 'pinyin': 'shùjù jí', 'trans': 'dataset'},\n{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'demonstrate'},\n{'word': '开放域', 'pinyin': 'kāifàng yù', 'trans': 'open domain'},\n{'word': '可控', 'pinyin': 'kěkòng', 'trans': 'controllable'}]",
        "trans": "This article introduces a framework called GameFactory, which aims to revolutionize game development by generating game engines. It utilizes pre-trained video diffusion models to create novel and diverse games. To address the limitations of existing methods in scene generation, the authors propose a multi-stage training strategy. They also release a high-quality video dataset based on Minecraft and demonstrate that the framework can generate open-domain, diverse, and controllable game videos.",
        "update_ts": "2025-01-21 09:10"
    }
}