{
    "date": {
        "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 25",
        "zh": "11æœˆ25æ—¥"
    },
    "time_utc": "2024-11-25 11:09",
    "weekday": 0,
    "issue_id": 761,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.14793",
            "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
            "url": "https://huggingface.co/papers/2411.14793",
            "abstract": "Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new \"style templates\", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation.",
            "score": 19,
            "issue_id": 752,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "03859b57f29683ab",
            "authors": [
                "Jooyoung Choi",
                "Chaehun Shin",
                "Yeongtak Oh",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14793.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#multimodal",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Style-friendly SNR sampler, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»-ÑˆÑƒĞ¼ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑˆÑƒĞ¼Ğ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ¸Ğ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ĞºĞ²Ğ°Ñ€ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¸, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹, 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹ Ğ¸ Ğ¼ĞµĞ¼Ñ‹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Unlocking Unique Artistic Styles with Style-friendly SNR Sampler",
                    "desc": "This paper addresses the challenge of adapting large-scale diffusion models to generate personalized artistic styles. The authors introduce the Style-friendly SNR sampler, which modifies the signal-to-noise ratio (SNR) during fine-tuning to emphasize higher noise levels where stylistic features are more prominent. By doing so, the model improves its ability to capture unique styles, resulting in images that align better with the desired artistic expression. The proposed method expands the creative possibilities for generating diverse styles, including watercolor paintings and cartoons, thus enhancing personalized content creation."
                },
                "zh": {
                    "title": "æå‡ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼ç”Ÿæˆçš„ä¿¡å™ªæ¯”æ–¹æ³•",
                    "desc": "æœ€è¿‘çš„å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä½†åœ¨å­¦ä¹ æ–°çš„ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†ç‹¬ç‰¹é£æ ¼æ¨¡æ¿çš„åˆ›å»ºã€‚å¾®è°ƒå‚è€ƒå›¾åƒæ˜¯æœ€æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œä½†é€šå¸¸ç›²ç›®ä½¿ç”¨é¢„è®­ç»ƒæ—¶çš„ç›®æ ‡å’Œå™ªå£°æ°´å¹³åˆ†å¸ƒï¼Œå¯¼è‡´é£æ ¼å¯¹é½ä¸ç†æƒ³ã€‚æˆ‘ä»¬æå‡ºäº†é£æ ¼å‹å¥½çš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰é‡‡æ ·å™¨ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç§¯æå°†ä¿¡å™ªæ¯”åˆ†å¸ƒå‘æ›´é«˜çš„å™ªå£°æ°´å¹³è½¬ç§»ï¼Œä»¥ä¸“æ³¨äºé£æ ¼ç‰¹å¾å‡ºç°çš„å™ªå£°æ°´å¹³ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç‹¬ç‰¹é£æ ¼ï¼Œå¹¶ç”Ÿæˆå…·æœ‰æ›´é«˜é£æ ¼å¯¹é½çš„å›¾åƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15124",
            "title": "TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training",
            "url": "https://huggingface.co/papers/2411.15124",
            "abstract": "Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.   In addition to the T\\\"ULU 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the T\\\"ULU 3 approach to more domains.",
            "score": 17,
            "issue_id": 755,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "44809ea81d71ef97",
            "authors": [
                "Nathan Lambert",
                "Jacob Morrison",
                "Valentina Pyatkin",
                "Shengyi Huang",
                "Hamish Ivison",
                "Faeze Brahman",
                "Lester James V. Miranda",
                "Alisa Liu",
                "Nouha Dziri",
                "Shane Lyu",
                "Yuling Gu",
                "Saumya Malik",
                "Victoria Graf",
                "Jena D. Hwang",
                "Jiangjiang Yang",
                "Ronan Le Bras",
                "Oyvind Tafjord",
                "Chris Wilhelm",
                "Luca Soldaini",
                "Noah A. Smith",
                "Yizhong Wang",
                "Pradeep Dasigi",
                "Hannaneh Hajishirzi"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15124.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#data",
                    "#open_source",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ T\"ULU 3 - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SFT, DPO Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RLVR. T\"ULU 3 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Llama 3.1 Ğ¸ GPT-4o-mini. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Language Model Potential with T\"ULU 3",
                    "desc": "This paper presents T\"ULU 3, a set of fully-open post-trained language models that enhance performance and capabilities beyond existing models. It emphasizes the importance of transparency in training data and methodologies, providing a comprehensive guide for implementing modern post-training techniques. The models utilize advanced training algorithms, including supervised finetuning and a novel reinforcement learning method, achieving superior results compared to both open and closed counterparts. Additionally, the paper offers extensive resources for replication and adaptation, including datasets, training code, and evaluation tools."
                },
                "zh": {
                    "title": "T\"ULU 3ï¼šå¼€æ”¾çš„åè®­ç»ƒæ¨¡å‹æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†T\"ULU 3ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„æœ€æ–°åè®­ç»ƒæ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºå’ŒæŠ€èƒ½ã€‚æˆ‘ä»¬æä¾›äº†æ¨¡å‹çš„è®­ç»ƒæ•°æ®ã€ä»£ç å’Œè®­ç»ƒé…æ–¹ï¼Œå¡«è¡¥äº†å¼€æ”¾æŠ€æœ¯ä¸ä¸“æœ‰æŠ€æœ¯ä¹‹é—´çš„é€æ˜åº¦å·®è·ã€‚T\"ULU 3åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬Llama 3.1å’ŒGPT-4o-miniç­‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å¤šä»»åŠ¡è¯„ä¼°æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„æŠ¥å‘Šï¼Œä»¥ä¾¿äºåœ¨æ›´å¤šé¢†åŸŸä¸­å¤ç°å’Œé€‚åº”T\"ULU 3çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12946",
            "title": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection",
            "url": "https://huggingface.co/papers/2411.12946",
            "abstract": "Large Language Models are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety.",
            "score": 9,
            "issue_id": 756,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "de5ca9118a5cab35",
            "authors": [
                "Gabriel Chua",
                "Shing Yee Chan",
                "Shaun Khoo"
            ],
            "affiliations": [
                "Government Technology Agency Singapore",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12946.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#hallucinations",
                    "#synthetic",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ñ‚ Ğ½ĞµÑ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ·Ğ»Ğ¾ÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM."
                },
                "en": {
                    "title": "Building Better Guardrails for Large Language Models",
                    "desc": "This paper addresses the issue of off-topic misuse in Large Language Models (LLMs) by proposing a new methodology for developing guardrails without relying on real-world data. The authors create a synthetic dataset by using LLMs to generate diverse prompts based on a well-defined problem space, which helps in training more effective off-topic guardrails. Their approach not only reduces false positives but also enhances adaptability to various misuse scenarios, such as harmful prompts and jailbreak attempts. Furthermore, the authors contribute to the community by open-sourcing their synthetic dataset and guardrail models, promoting further research in LLM safety."
                },
                "zh": {
                    "title": "æ„å»ºçµæ´»çš„é˜²æŠ¤æªæ–½ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨ä¸­å¯èƒ½å‡ºç°çš„åç¦»ä¸»é¢˜çš„è¯¯ç”¨é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„ã€æ— æ•°æ®çš„é˜²æŠ¤æªæ–½å¼€å‘æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„é«˜è¯¯æŠ¥ç‡å’Œé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¯¹é—®é¢˜ç©ºé—´çš„å®šæ€§å®šä¹‰ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œç”¨äºåŸºå‡†æµ‹è¯•å’Œè®­ç»ƒé˜²æŠ¤æªæ–½ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€æºäº†åˆæˆæ•°æ®é›†å’Œé˜²æŠ¤æ¨¡å‹ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ç ”ç©¶å’Œå¼€å‘æä¾›äº†é‡è¦èµ„æºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15098",
            "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
            "url": "https://huggingface.co/papers/2411.15098",
            "abstract": "In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only ~0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation.",
            "score": 8,
            "issue_id": 754,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "9cd668db99ed0902",
            "authors": [
                "Zhenxiong Tan",
                "Songhua Liu",
                "Xingyi Yang",
                "Qiaochu Xue",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15098.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#synthetic",
                    "#data",
                    "#diffusion",
                    "#dataset",
                    "#architecture",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "OminiControl: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "OminiControl - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Diffusion Transformer (DiT). ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ DiT ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. OminiControl Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 0,1% Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ UNet Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ DiT ĞºĞ°Ğº Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Efficient Image Conditioning with OminiControl",
                    "desc": "OminiControl is a new framework that enhances pre-trained Diffusion Transformer (DiT) models by integrating image conditions efficiently. It uses a parameter reuse mechanism, allowing the DiT to process image conditions with minimal additional parameters, specifically around 0.1%. This framework can handle various image conditioning tasks, such as generating images based on specific subjects or aligning them with spatial features like edges and depth. OminiControl has shown superior performance compared to traditional UNet-based models and other DiT adaptations, and it comes with a large dataset, Subjects200K, to support further research."
                },
                "zh": {
                    "title": "OminiControlï¼šé«˜æ•ˆæ•´åˆå›¾åƒæ¡ä»¶çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†OminiControlï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜åº¦çµæ´»ä¸”å‚æ•°é«˜æ•ˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†å›¾åƒæ¡ä»¶é›†æˆåˆ°é¢„è®­ç»ƒçš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¨¡å‹ä¸­ã€‚OminiControlåˆ©ç”¨å‚æ•°é‡ç”¨æœºåˆ¶ï¼Œä½¿DiTèƒ½å¤Ÿä½¿ç”¨è‡ªèº«ä½œä¸ºå¼ºå¤§çš„åŸºç¡€ï¼Œç¼–ç å›¾åƒæ¡ä»¶ï¼Œå¹¶é€šè¿‡çµæ´»çš„å¤šæ¨¡æ€æ³¨æ„åŠ›å¤„ç†å™¨è¿›è¡Œå¤„ç†ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒOminiControlä»…éœ€çº¦0.1%çš„é¢å¤–å‚æ•°ï¼Œå°±èƒ½æœ‰æ•ˆåœ°æ•´åˆæ³¨å…¥çš„å›¾åƒæ¡ä»¶ï¼Œå¹¶ä»¥ç»Ÿä¸€çš„æ–¹å¼å¤„ç†å¤šç§å›¾åƒæ¡ä»¶ä»»åŠ¡ã€‚é€šè¿‡åœ¨DiTè‡ªèº«ç”Ÿæˆçš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼ŒOminiControlåœ¨ä¸»é¢˜é©±åŠ¨ç”Ÿæˆå’Œç©ºé—´å¯¹é½æ¡ä»¶ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„UNetå’ŒDiTé€‚åº”æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14982",
            "title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal Models",
            "url": "https://huggingface.co/papers/2411.14982",
            "abstract": "Recent advances in Large Multimodal Models (LMMs) lead to significant breakthroughs in both academia and industry. One question that arises is how we, as humans, can understand their internal neural representations. This paper takes an initial step towards addressing this question by presenting a versatile framework to identify and interpret the semantics within LMMs. Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the representations into human understandable features. 2) We then present an automatic interpretation framework to interpreted the open-semantic features learned in SAE by the LMMs themselves. We employ this framework to analyze the LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these features can effectively steer the model's behavior. Our results contribute to a deeper understanding of why LMMs excel in specific tasks, including EQ tests, and illuminate the nature of their mistakes along with potential strategies for their rectification. These findings offer new insights into the internal mechanisms of LMMs and suggest parallels with the cognitive processes of the human brain.",
            "score": 6,
            "issue_id": 761,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "7d4fae86425adb6c",
            "authors": [
                "Kaichen Zhang",
                "Yifei Shen",
                "Bo Li",
                "Ziwei Liu"
            ],
            "affiliations": [
                "LMMs-Lab Team, S-Lab, NTU, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14982.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaVA-NeXT-8B Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLaVA-OV-72B, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, ĞºĞ°Ğº Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LMM Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸ Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ°."
                },
                "en": {
                    "title": "Unlocking the Secrets of Large Multimodal Models",
                    "desc": "This paper explores how we can understand the internal workings of Large Multimodal Models (LMMs) by using a Sparse Autoencoder (SAE) to break down their representations into features that humans can comprehend. It introduces a framework for automatically interpreting these features, which are learned by the LMMs themselves. The study specifically analyzes the LLaVA-NeXT-8B model in relation to the LLaVA-OV-72B model, showing that the identified features can influence the model's performance on various tasks. The findings enhance our understanding of LMMs' strengths and weaknesses, drawing parallels to human cognitive processes."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å†…éƒ¨ç¥ç»è¡¨ç¤ºå¦‚ä½•è¢«ç†è§£ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰å°†è¡¨ç¤ºè§£è€¦ä¸ºäººç±»å¯ç†è§£çš„ç‰¹å¾ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨è§£é‡Šæ¡†æ¶ï¼Œç”¨äºè§£é‡ŠLMMsè‡ªèº«å­¦ä¹ çš„å¼€æ”¾è¯­ä¹‰ç‰¹å¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœåŠ æ·±äº†å¯¹LMMsåœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚åŸå› çš„ç†è§£ï¼Œå¹¶æ­ç¤ºäº†å®ƒä»¬é”™è¯¯çš„æ€§è´¨åŠå¯èƒ½çš„çº æ­£ç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14794",
            "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
            "url": "https://huggingface.co/papers/2411.14794",
            "abstract": "The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities. Our code and dataset will be released at: https://github.com/hshjerry/VideoEspresso",
            "score": 5,
            "issue_id": 757,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "f03437948b77773f",
            "authors": [
                "Songhao Han",
                "Wei Huang",
                "Hairong Shi",
                "Le Zhuo",
                "Xiu Su",
                "Shifeng Zhang",
                "Xu Zhou",
                "Xiaojuan Qi",
                "Yue Liao",
                "Si Liu"
            ],
            "affiliations": [
                "Beihang University",
                "CUHK",
                "Central South University",
                "Sangfor Technologies Inc.",
                "Shanghai AI Lab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14794.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#cv",
                    "#games",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VideoEspresso: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VideoEspresso Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Hybrid LVLMs Collaboration, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing VideoQA with VideoEspresso: A New Era of Multimodal Reasoning",
                    "desc": "This paper presents VideoEspresso, a new dataset designed to enhance video question-answering (VideoQA) by providing high-quality pairs that maintain spatial and temporal coherence. It addresses the limitations of existing datasets, which often rely on expensive manual annotations or ineffective automatic methods. The authors introduce a semantic-aware construction pipeline that reduces redundancy and generates QA pairs using GPT-4o, along with video Chain-of-Thought (CoT) annotations to improve reasoning. Additionally, they propose a Hybrid LVLMs Collaboration framework that optimally selects frames and performs reasoning, achieving superior performance on various tasks compared to existing models."
                },
                "zh": {
                    "title": "VideoEspressoï¼šæå‡è§†é¢‘æ¨ç†çš„æ–°æ•°æ®é›†ä¸æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘é—®ç­”æ•°æ®é›†VideoEspressoï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†åœ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³ã€‚è¯¥æ•°æ®é›†ä¿ç•™äº†é‡è¦çš„ç©ºé—´ç»†èŠ‚å’Œæ—¶é—´è¿è´¯æ€§ï¼Œå¹¶æä¾›äº†ä¸­é—´æ¨ç†æ­¥éª¤çš„å¤šæ¨¡æ€æ³¨é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åä½œæ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”é€‰æ‹©æ ¸å¿ƒå¸§å¹¶è¿›è¡Œè¿é”æ€ç»´æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†æ›´å¼ºçš„è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14762",
            "title": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction",
            "url": "https://huggingface.co/papers/2411.14762",
            "abstract": "Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled (x,y,t) coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128times128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.",
            "score": 4,
            "issue_id": 756,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "9490a40884583735",
            "authors": [
                "Huiwon Jang",
                "Sihyun Yu",
                "Jinwoo Shin",
                "Pieter Abbeel",
                "Younggyo Seo"
            ],
            "affiliations": [
                "KAIST",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14762.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "CoordTok: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "CoordTok - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ². ĞĞ½ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼ (x,y,t), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. CoordTok Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Efficient Video Tokenization with CoordTok",
                    "desc": "This paper presents CoordTok, a novel video tokenizer designed to efficiently encode long video clips by leveraging coordinate-based representations. Unlike traditional tokenizers that reconstruct all frames simultaneously, CoordTok uses a factorized triplane representation to map (x,y,t) coordinates to video patches, significantly reducing the number of tokens needed. The approach allows for training large models on long videos without incurring high computational costs. Experimental results demonstrate that CoordTok can encode a 128-frame video into just 1280 tokens, outperforming existing methods that require thousands of tokens for similar quality."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘æ ‡è®°åŒ–ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCoordTokçš„è§†é¢‘æ ‡è®°å™¨ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†é•¿è§†é¢‘çš„æ ‡è®°åŒ–é—®é¢˜ã€‚CoordToké€šè¿‡å­¦ä¹ åæ ‡è¡¨ç¤ºä¸è¾“å…¥è§†é¢‘è¡¥ä¸ä¹‹é—´çš„æ˜ å°„ï¼Œåˆ©ç”¨äº†è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCoordTokæ˜¾è‘—å‡å°‘äº†ç¼–ç é•¿è§†é¢‘æ‰€éœ€çš„æ ‡è®°æ•°é‡ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒCoordTokèƒ½å¤Ÿåœ¨ä¿æŒé‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œå°†128å¸§è§†é¢‘çš„æ ‡è®°æ•°é‡å‡å°‘åˆ°1280ä¸ªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14208",
            "title": "Novel View Extrapolation with Video Diffusion Priors",
            "url": "https://huggingface.co/papers/2411.14208",
            "abstract": "The field of novel view synthesis has made significant strides thanks to the development of radiance field methods. However, most radiance field techniques are far better at novel view interpolation than novel view extrapolation where the synthesis novel views are far beyond the observed training views. We design ViewExtrapolator, a novel view synthesis approach that leverages the generative priors of Stable Video Diffusion (SVD) for realistic novel view extrapolation. By redesigning the SVD denoising process, ViewExtrapolator refines the artifact-prone views rendered by radiance fields, greatly enhancing the clarity and realism of the synthesized novel views. ViewExtrapolator is a generic novel view extrapolator that can work with different types of 3D rendering such as views rendered from point clouds when only a single view or monocular video is available. Additionally, ViewExtrapolator requires no fine-tuning of SVD, making it both data-efficient and computation-efficient. Extensive experiments demonstrate the superiority of ViewExtrapolator in novel view extrapolation. Project page: https://kunhao-liu.github.io/ViewExtrapolator/.",
            "score": 2,
            "issue_id": 755,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "dddb7a1ecc9850f3",
            "authors": [
                "Kunhao Liu",
                "Ling Shao",
                "Shijian Lu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "UCAS-Terminus AI Lab, UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14208.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ”­",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ViewExtrapolator - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ²Ğ¸Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Stable Video Diffusion (SVD) Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ViewExtrapolator Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ SVD. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ViewExtrapolator Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Novel View Extrapolation with ViewExtrapolator",
                    "desc": "This paper introduces ViewExtrapolator, a new method for synthesizing novel views that go beyond the original training views. It utilizes the generative capabilities of Stable Video Diffusion (SVD) to improve the quality of these extrapolated views. By modifying the SVD denoising process, the method reduces artifacts and enhances the realism of the generated images. ViewExtrapolator is versatile, working with various 3D rendering types and requiring no fine-tuning, making it efficient in both data and computation."
                },
                "zh": {
                    "title": "æå‡æ–°è§†å›¾å¤–æ¨çš„æ¸…æ™°åº¦ä¸çœŸå®æ„Ÿ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†å›¾åˆæˆæ–¹æ³•ï¼Œç§°ä¸ºViewExtrapolatorï¼Œæ—¨åœ¨æ”¹å–„æ–°è§†å›¾å¤–æ¨çš„æ•ˆæœã€‚ä¼ ç»Ÿçš„è¾å°„åœºæŠ€æœ¯åœ¨æ–°è§†å›¾æ’å€¼æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–°è§†å›¾å¤–æ¨æ—¶æ•ˆæœè¾ƒå·®ã€‚ViewExtrapolatoråˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„ç”Ÿæˆå…ˆéªŒï¼Œé€šè¿‡é‡æ–°è®¾è®¡å»å™ªè¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†åˆæˆæ–°è§†å›¾çš„æ¸…æ™°åº¦å’ŒçœŸå®æ„Ÿã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä¸åŒç±»å‹çš„3Dæ¸²æŸ“ï¼Œå¹¶ä¸”æ— éœ€å¯¹SVDè¿›è¡Œå¾®è°ƒï¼Œå…·æœ‰æ•°æ®å’Œè®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14521",
            "title": "MyTimeMachine: Personalized Facial Age Transformation",
            "url": "https://huggingface.co/papers/2411.14521",
            "abstract": "Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20sim40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches.",
            "score": 2,
            "issue_id": 755,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "d104e8a00be886bb",
            "authors": [
                "Luchao Qi",
                "Jiaye Wu",
                "Bang Gong",
                "Annie N. Wang",
                "David W. Jacobs",
                "Roni Sengupta"
            ],
            "affiliations": [
                "University of Maryland, College Park",
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14521.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸ‘´",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ MyTimeMachine (MyTM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸ĞµĞ¹ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ğ°Ñ Ğ¡ĞµÑ‚ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ StyleGAN2. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Personalized Aging: Your Face, Your Time",
                    "desc": "This paper presents MyTimeMachine (MyTM), a novel approach to facial aging that combines global aging knowledge with personalized photo collections. It addresses the challenge of accurately predicting an individual's appearance at a target age by utilizing as few as 50 personal images. The method employs an Adapter Network that integrates personalized and global aging features, generating realistic re-aged images using StyleGAN2. Additionally, the authors introduce three specialized loss functions to enhance the personalization of the aging process, resulting in high-quality, identity-preserving, and temporally consistent aging effects."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–è€åŒ–ï¼ŒçœŸå®å†ç°",
                    "desc": "é¢éƒ¨è€åŒ–æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œå—åˆ°æ€§åˆ«ã€ç§æ—ã€ç”Ÿæ´»æ–¹å¼ç­‰å¤šç§å› ç´ çš„å½±å“ï¼Œå› æ­¤å¾ˆéš¾å­¦ä¹ åˆ°ä¸€ä¸ªé€šç”¨çš„è€åŒ–æ¨¡å‹æ¥å‡†ç¡®é¢„æµ‹ä¸ªä½“çš„è€åŒ–æƒ…å†µã€‚ç°æœ‰æŠ€æœ¯è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è€åŒ–æ•ˆæœï¼Œä½†é‡å¡‘çš„å›¾åƒå¾€å¾€ä¸ç›®æ ‡å¹´é¾„çš„å¤–è²Œä¸ç¬¦ï¼Œå› æ­¤éœ€è¦ä¸ªæ€§åŒ–å¤„ç†ã€‚æˆ‘ä»¬æå‡ºäº†MyTimeMachineï¼ˆMyTMï¼‰ï¼Œå®ƒç»“åˆäº†å…¨çƒè€åŒ–æ¨¡å‹å’Œä¸ªäººç…§ç‰‡é›†ï¼ˆä»…éœ€50å¼ å›¾åƒï¼‰æ¥å­¦ä¹ ä¸ªæ€§åŒ–çš„å¹´é¾„è½¬æ¢ã€‚æˆ‘ä»¬çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†é€‚é…å™¨ç½‘ç»œå’Œä¸‰ç§æŸå¤±å‡½æ•°ï¼Œä½¿å¾—ç”Ÿæˆçš„è€åŒ–å›¾åƒèƒ½å¤Ÿä¿æŒèº«ä»½ä¸€è‡´æ€§ï¼Œå¹¶åœ¨è§†é¢‘ä¸­å®ç°é«˜è´¨é‡çš„è€åŒ–æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15033",
            "title": "One to rule them all: natural language to bind communication, perception and action",
            "url": "https://huggingface.co/papers/2411.15033",
            "abstract": "In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have a wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans. This paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback. The Planner Module is the core of the system where LLMs embedded in a modified ReAct framework are employed to interpret and carry out user commands. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances a robot adaptability, task execution, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robot ability to perform tasks. Using a dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue.",
            "score": 1,
            "issue_id": 758,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "f76bbfe3f8854ad7",
            "authors": [
                "Simone Colombani",
                "Dimitri Ognibene",
                "Giuseppe Boccignone"
            ],
            "affiliations": [
                "Oversonic Robotics, Carate Brianza, Italy",
                "University of Milan, Italy",
                "University of Milano-Bicocca, Milan, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15033.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#architecture",
                    "#agents",
                    "#robotics",
                    "#optimization",
                    "#agi",
                    "#interpretability"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñ‹: Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğµ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞœĞ¾Ğ´ÑƒĞ»ÑŒ ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°, Ğ³Ğ´Ğµ LLM, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ² Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ReAct, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Robots with Natural Language Understanding and Dynamic Adaptability",
                    "desc": "This paper discusses a new architecture for robots that helps them understand and follow human instructions in real-time. It combines Large Language Models (LLMs) with a planning system that allows robots to interpret natural language commands and adapt to changing environments. The core of this system is a Planner Module that uses LLMs to translate user requests into actions while considering the current state of the environment. By incorporating feedback loops and a dynamic semantic map, the architecture improves the robot's ability to work alongside humans and adjust its plans as needed."
                },
                "zh": {
                    "title": "æ™ºèƒ½æœºå™¨äººï¼šè‡ªç„¶è¯­è¨€ä¸åŠ¨æ€ç¯å¢ƒçš„å®Œç¾ç»“åˆ",
                    "desc": "è¿‘å¹´æ¥ï¼Œäººæœºäº¤äº’é¢†åŸŸçš„ç ”ç©¶é›†ä¸­åœ¨å¼€å‘èƒ½å¤Ÿç†è§£å¤æ‚äººç±»æŒ‡ä»¤å¹¶åœ¨åŠ¨æ€å¤šæ ·ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡çš„æœºå™¨äººã€‚è¿™äº›ç³»ç»Ÿåœ¨ä¸ªäººåŠ©ç†å’Œå·¥ä¸šæœºå™¨äººç­‰å¤šä¸ªåº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œå¼ºè°ƒæœºå™¨äººä¸äººç±»çµæ´»ã€è‡ªç„¶å’Œå®‰å…¨çš„äº’åŠ¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æœºå™¨äººè¡ŒåŠ¨è§„åˆ’æ¶æ„ï¼Œç»“åˆäº†é€šä¿¡ã€æ„ŸçŸ¥å’Œè§„åˆ’ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°†è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„å‘½ä»¤è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„æœºå™¨äººåŠ¨ä½œã€‚é€šè¿‡å®æ—¶åé¦ˆåŠ¨æ€æ›´æ–°è®¡åˆ’ï¼Œè¯¥ç³»ç»Ÿæé«˜äº†æœºå™¨äººåœ¨å…±äº«å’ŒåŠ¨æ€ç¯å¢ƒä¸­é€‚åº”æ€§ã€ä»»åŠ¡æ‰§è¡Œå’Œä¸äººç±»ç”¨æˆ·çš„æ— ç¼åä½œèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15131",
            "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
            "url": "https://huggingface.co/papers/2411.15131",
            "abstract": "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world environments, which requires the robot to (1) have skills that generalize across object configurations; (2) be capable of long-horizon task execution in diverse environments; and (3) perform complex manipulation beyond pick-and-place. Quadruped robots with manipulators hold promise for extending the workspace and enabling robust locomotion, but existing results do not investigate such a capability. This paper proposes WildLMa with three components to address these issues: (1) adaptation of learned low-level controller for VR-enabled whole-body teleoperation and traversability; (2) WildLMa-Skill -- a library of generalizable visuomotor skills acquired via imitation learning or heuristics and (3) WildLMa-Planner -- an interface of learned skills that allow LLM planners to coordinate skills for long-horizon tasks. We demonstrate the importance of high-quality training data by achieving higher grasping success rate over existing RL baselines using only tens of demonstrations. WildLMa exploits CLIP for language-conditioned imitation learning that empirically generalizes to objects unseen in training demonstrations. Besides extensive quantitative evaluation, we qualitatively demonstrate practical robot applications, such as cleaning up trash in university hallways or outdoor terrains, operating articulated objects, and rearranging items on a bookshelf.",
            "score": 0,
            "issue_id": 755,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "c38df92e9599db09",
            "authors": [
                "Ri-Zhao Qiu",
                "Yuchen Song",
                "Xuanbin Peng",
                "Sai Aneesh Suryadevara",
                "Ge Yang",
                "Minghuan Liu",
                "Mazeyu Ji",
                "Chengzhe Jia",
                "Ruihan Yang",
                "Xueyan Zou",
                "Xiaolong Wang"
            ],
            "affiliations": [
                "MIT",
                "NVIDIA",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15131.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#robotics",
                    "#training",
                    "#agi",
                    "#transfer_learning",
                    "#long_context"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "WildLMa: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ WildLMa Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´Ğ»Ñ Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. WildLMa Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ CLIP Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑĞ²Ğ¾Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑƒĞ±Ğ¾Ñ€ĞºĞ° Ğ¼ÑƒÑĞ¾Ñ€Ğ° Ğ¸ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Robots for Real-World Manipulation with WildLMa",
                    "desc": "This paper presents WildLMa, a framework designed for mobile manipulation robots to operate effectively in varied real-world settings. It includes a low-level controller for teleoperation, a library of generalizable visuomotor skills learned through imitation, and a planner that coordinates these skills for complex tasks. The approach emphasizes the significance of high-quality training data, achieving better performance in grasping tasks compared to existing reinforcement learning methods. Additionally, WildLMa utilizes CLIP for language-conditioned imitation learning, enabling the robot to adapt to new objects not seen during training."
                },
                "zh": {
                    "title": "WildLMaï¼šæå‡æœºå™¨äººåœ¨çœŸå®ç¯å¢ƒä¸­çš„æ“æ§èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºWildLMaçš„ç§»åŠ¨æ“æ§æœºå™¨äººç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åœ¨å¤šæ ·åŒ–çœŸå®ç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šé€‚åº”æ€§ä½çº§æ§åˆ¶å™¨ã€é€šç”¨è§†è§‰è¿åŠ¨æŠ€èƒ½åº“å’Œé•¿æ—¶é—´ä»»åŠ¡è§„åˆ’æ¥å£ã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼ŒWildLMaåœ¨æŠ“å–æˆåŠŸç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†æœºå™¨äººåœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œå¦‚æ¸…ç†æ ¡å›­èµ°å»Šåƒåœ¾å’Œæ“ä½œå¤æ‚ç‰©ä½“ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-22.html",
    "link_next": "2024-11-26.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "22.11",
        "en": "11/22",
        "zh": "11æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 2,
        "#agi": 2,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†éš¾ä»¥å­¦ä¹ æ–°çš„ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼ã€‚ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒæ—¶ç›²ç›®ä½¿ç”¨é¢„è®­ç»ƒç›®æ ‡å’Œå™ªå£°æ°´å¹³åˆ†å¸ƒï¼Œå¯¼è‡´é£æ ¼å¯¹é½ä¸ä½³ã€‚ä½œè€…æå‡ºäº†é£æ ¼å‹å¥½çš„ä¿¡å™ªæ¯”é‡‡æ ·å™¨ï¼Œåœ¨å¾®è°ƒæ—¶å°†ä¿¡å™ªæ¯”åˆ†å¸ƒåå‘æ›´é«˜çš„å™ªå£°æ°´å¹³ï¼Œä»¥æ•æ‰ç‹¬ç‰¹é£æ ¼ã€‚è¿™ä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°ç”Ÿæˆå…·æœ‰é«˜é£æ ¼ä¸€è‡´æ€§çš„å›¾åƒï¼Œå¹¶å…è®¸åˆ›å»ºå’Œå…±äº«æ–°çš„é£æ ¼æ¨¡æ¿ã€‚ä½œè€…å±•ç¤ºäº†ç”Ÿæˆæ°´å½©ç”»ã€ç®€ç¬”ç”»ã€3Dæ¸²æŸ“ç­‰å¤šç§é£æ ¼çš„èƒ½åŠ›ã€‚",
        "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†éš¾ä»¥å­¦ä¹ æ–°çš„ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼ã€‚ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒæ—¶ç›²ç›®ä½¿ç”¨é¢„è®­ç»ƒç›®æ ‡å’Œå™ªå£°æ°´å¹³åˆ†å¸ƒï¼Œå¯¼è‡´é£æ ¼å¯¹é½ä¸ä½³ã€‚ä½œè€…æå‡ºäº†é£æ ¼å‹å¥½çš„ä¿¡å™ªæ¯”é‡‡æ ·å™¨ï¼Œåœ¨å¾®è°ƒæ—¶å°†ä¿¡å™ªæ¯”åˆ†å¸ƒåå‘æ›´é«˜çš„å™ªå£°æ°´å¹³ï¼Œä»¥æ•æ‰ç‹¬ç‰¹é£æ ¼ã€‚è¿™ä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°ç”Ÿæˆå…·æœ‰é«˜é£æ ¼ä¸€è‡´æ€§çš„å›¾åƒï¼Œå¹¶å…è®¸åˆ›å»ºå’Œå…±äº«æ–°çš„é£æ ¼æ¨¡æ¿ã€‚ä½œè€…å±•ç¤ºäº†ç”Ÿæˆæ°´å½©ç”»ã€ç®€ç¬”ç”»ã€3Dæ¸²æŸ“ç­‰å¤šç§é£æ ¼çš„èƒ½åŠ›ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ  xÃ­ng kuÃ² sÃ n mÃ³ xÃ­ng shÄ“ng chÃ©ng gÄo zhÃ¬ liÃ ng tÃº xiÃ ng, dÃ n nÃ¡n yÇ xuÃ© xÃ­ xÄ«n de gÃ¨ xÃ¬ng huÃ  yÃ¬ shÃ¹ fÄ“ng gÃ©. xiÃ n yÇ’u fÄng fÇ zÃ i wÄ“i tiÃ¡o shÃ­ mÄng mÃ¹ shÇ yÃ²ng yÃ¹ xÃ¹n liÃ n mÃ¹ biÄo hÃ© zÃ o shÄ“ng shuÇ pÃ­ng fÄ“n bÃ¹, dÇo zhÃ¬ fÄ“ng gÃ© duÃ¬ qÃ­ bÃ¹ jiÄ. zuÃ² zhÄ› tÃ­ chÅ« le fÄ“ng gÃ© yÇ’u hÇo de xÃ¬n zÃ o bÇ cÇi yÇng qÃ¬, zÃ i wÄ“i tiÃ¡o shÃ­ jiÄng xÃ¬n zÃ o bÇ fÄ“n bÃ¹ piÄn xiÃ ng gÃ¨ng gÄo de zÃ o shÄ“ng shuÇ pÃ­ng, yÇ bÇ” zhuÅ dÃº tÃ¨ fÄ“ng gÃ©. zhÃ¨ shÇ mÃ³ xÃ­ng nÃ©ng gÃ¨ng hÇo de shÄ“ng chÃ©ng jÃ¹ yÇ’u gÄo fÄ“ng gÃ© yÄ« zhÃ¬ xÃ¬ng de tÃº xiÃ ng, bÃ¬ng yÇ”n xÇ” chuÃ ng jiÃ n hÃ© gÃ²ng xiÇng xÄ«n de fÄ“ng gÃ© mÃº bÇn. zuÃ² zhÄ› zhÇn shÃ¬ le shÄ“ng chÃ©ng shuÇ cÇi huÃ , jiÇn bÇ huÃ , 3D xuÃ n rÃ¡n dÄ›ng duÅ zhÇ’ng fÄ“ng gÃ© de nÃ©ng lÃ¬.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high quality'}, {'word': 'ä¸ªæ€§åŒ–', 'pinyin': 'gÃ¨ xÃ¬ng huÃ ', 'trans': 'personalized'}, {'word': 'è‰ºæœ¯', 'pinyin': 'yÃ¬ shÃ¹', 'trans': 'art'}, {'word': 'é£æ ¼', 'pinyin': 'fÄ“ng gÃ©', 'trans': 'style'}, {'word': 'ç°æœ‰', 'pinyin': 'xiÃ n yÇ’u', 'trans': 'existing'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tune'}, {'word': 'ç›²ç›®', 'pinyin': 'mÃ¡ng mÃ¹', 'trans': 'blindly'}, {'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-trained'}, {'word': 'ç›®æ ‡', 'pinyin': 'mÃ¹ biÄo', 'trans': 'target'}, {'word': 'å™ªå£°', 'pinyin': 'zÃ o shÄ“ng', 'trans': 'noise'}, {'word': 'æ°´å¹³', 'pinyin': 'shuÇ pÃ­ng', 'trans': 'level'}, {'word': 'åˆ†å¸ƒ', 'pinyin': 'fÄ“n bÃ¹', 'trans': 'distribution'}, {'word': 'å¯¼è‡´', 'pinyin': 'dÇo zhÃ¬', 'trans': 'lead to'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'alignment'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'ä¿¡å™ªæ¯”', 'pinyin': 'xÃ¬n zÃ o bÇ', 'trans': 'signal-to-noise ratio'}, {'word': 'é‡‡æ ·å™¨', 'pinyin': 'cÇi yÃ ng qÃ¬', 'trans': 'sampler'}, {'word': 'åå‘', 'pinyin': 'piÄn xiÃ ng', 'trans': 'bias towards'}, {'word': 'æ•æ‰', 'pinyin': 'bÇ” zhuÅ', 'trans': 'capture'}, {'word': 'ç‹¬ç‰¹', 'pinyin': 'dÃº tÃ¨', 'trans': 'unique'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'}, {'word': 'æ¨¡æ¿', 'pinyin': 'mÃº bÇn', 'trans': 'template'}, {'word': 'å±•ç¤º', 'pinyin': 'zhÇn shÃ¬', 'trans': 'demonstrate'}, {'word': 'æ°´å½©ç”»', 'pinyin': 'shuÇ cÇi huÃ ', 'trans': 'watercolor painting'}, {'word': 'ç®€ç¬”ç”»', 'pinyin': 'jiÇn bÇ huÃ ', 'trans': 'line drawing'}, {'word': '3Dæ¸²æŸ“', 'pinyin': '3D xuÃ n rÃ¡n', 'trans': '3D rendering'}]",
        "trans": "This article discusses the ability of large diffusion models to generate high-quality images but their difficulty in learning new personalized artistic styles. Existing methods blindly use pre-trained objectives and noise level distributions during fine-tuning, leading to poor style alignment. The authors propose a style-friendly signal-to-noise ratio sampler that biases the signal-to-noise ratio distribution towards higher noise levels during fine-tuning to capture unique styles. This allows the model to generate images with higher style consistency and enables the creation and sharing of new style templates. The authors demonstrate the capability to generate various styles such as watercolor, sketch, and 3D rendering.",
        "update_ts": "2024-11-25 09:06"
    }
}