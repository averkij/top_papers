{
    "date": {
        "ru": "7 января",
        "en": "January 7",
        "zh": "1月7日"
    },
    "time_utc": "2025-01-07 17:09",
    "weekday": 1,
    "issue_id": 1540,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.02976",
            "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution",
            "url": "https://huggingface.co/papers/2501.02976",
            "abstract": "Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce~\\name (Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate~\\name~outperforms state-of-the-art methods on both synthetic and real-world datasets.",
            "score": 35,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "13ac412646c508f5",
            "authors": [
                "Rui Xie",
                "Yinhong Liu",
                "Penghao Zhou",
                "Chen Zhao",
                "Jun Zhou",
                "Kai Zhang",
                "Zhenyu Zhang",
                "Jian Yang",
                "Zhenheng Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "ByteDance",
                "Nanjing University",
                "Southwest University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02976.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Качественное суперразрешение видео с помощью T2V моделей",
                    "desc": "Представлена новая методика STAR для суперразрешения видео в реальных условиях с использованием моделей text-to-video. Предложен модуль LIEM для улучшения локальных деталей и устранения артефактов деградации. Введена функция потерь Dynamic Frequency для усиления точности восстановления на разных частотах. Эксперименты показывают превосходство STAR над современными методами на синтетических и реальных датасетах."
                },
                "en": {
                    "title": "Enhancing Video Quality with T2V Models for Real-World Super-Resolution",
                    "desc": "This paper presents a new method called Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution, which aims to improve video quality by addressing issues of over-smoothing and temporal consistency. Traditional image diffusion models struggle with video because they are designed for static images, leading to challenges in capturing motion dynamics. The proposed approach incorporates a Local Information Enhancement Module to enhance local details and reduce artifacts, along with a Dynamic Frequency Loss to maintain fidelity across different frequency components. Experimental results show that this method outperforms existing techniques in both synthetic and real-world scenarios, providing better spatial and temporal quality in restored videos."
                },
                "zh": {
                    "title": "提升视频超分辨率的时空一致性",
                    "desc": "本文提出了一种新方法，名为~\\name~，用于提高真实世界视频超分辨率的时空质量。该方法结合了文本到视频（T2V）模型，以解决传统生成对抗网络（GAN）方法中的过平滑问题。通过引入局部信息增强模块（LIEM）和动态频率损失（DF Loss），该方法能够有效改善视频的局部细节和时间一致性。实验结果表明，~\\name~在合成和真实世界数据集上均优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03226",
            "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning",
            "url": "https://huggingface.co/papers/2501.03226",
            "abstract": "Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical benchmarks, and 7.5\\% gain combined with MCTS.",
            "score": 20,
            "issue_id": 1532,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "94a01c7d4516c725",
            "authors": [
                "Beichen Zhang",
                "Yuhong Liu",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Pan Zhang",
                "Haodong Duan",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03226.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "BoostStep: Повышение точности рассуждений ИИ в решении математических задач",
                    "desc": "Статья представляет метод BoostStep для улучшения решения сложных математических задач большими языковыми моделями. BoostStep решает проблемы несоответствия детализации и негативного шума в примерах обучения в контексте. Метод выравнивает гранулярность между извлечением и рассуждением на уровне шагов, предоставляя релевантные примеры для каждого шага рассуждения. BoostStep повышает качество рассуждений модели и может интегрироваться с методами поиска по дереву Монте-Карло для улучшения генерации кандидатов и принятия решений."
                },
                "en": {
                    "title": "Boosting Reasoning Quality in Large Language Models with BoostStep",
                    "desc": "This paper introduces BoostStep, a method designed to enhance the reasoning quality of large language models (LLMs) when solving complex math problems. It addresses two main issues: granularity-mismatch and negative-effect noise in in-context learning (ICL) examples, which can lead to inaccurate reasoning. By aligning the granularity of retrieved examples with the specific reasoning steps required, BoostStep provides more relevant ICL examples, improving the model's performance. The method not only boosts standalone reasoning but also integrates effectively with Monte Carlo Tree Search (MCTS) to enhance decision-making processes."
                },
                "zh": {
                    "title": "提升推理质量的BoostStep方法",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在解决复杂数学问题时的表现，特别是通过分而治之的策略和上下文学习（ICL）示例的辅助。研究发现，ICL示例中的粒度不匹配和负面噪声问题限制了模型的改进潜力。为了解决这些问题，论文提出了BoostStep方法，它通过对每个推理步骤的粒度进行对齐，提供更相关的ICL示例，从而提高推理质量。BoostStep不仅提升了独立推理的性能，还能与蒙特卡洛树搜索（MCTS）方法无缝集成，进一步优化候选生成和决策过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03218",
            "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
            "url": "https://huggingface.co/papers/2501.03218",
            "abstract": "Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at https://github.com/Mark12Ding/Dispider.",
            "score": 19,
            "issue_id": 1532,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "1e9974be2d206516",
            "authors": [
                "Rui Qian",
                "Shuangrui Ding",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Zang",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03218.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#video",
                    "#optimization",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Dispider: Интеллектуальное взаимодействие с видео в реальном времени",
                    "desc": "Статья представляет систему Dispider для активного взаимодействия с видео в реальном времени с использованием языковых моделей. Система разделяет процессы восприятия, принятия решений и реакции, что позволяет эффективно обрабатывать потоковое видео и взаимодействовать с пользователем. Dispider использует легковесный модуль обработки видео для отслеживания потока и определения оптимальных моментов для взаимодействия. Асинхронная архитектура обеспечивает своевременные и точные ответы при длительной обработке видеопотоков."
                },
                "en": {
                    "title": "Dispider: Real-time Interaction Redefined for Video LLMs",
                    "desc": "This paper introduces Dispider, a system designed for active real-time interaction with video using large language models (LLMs). Unlike traditional offline models, Dispider can process video streams continuously while engaging with users, requiring three key capabilities: Perception, Decision, and Reaction. The system addresses conflicts between these capabilities by disentangling them, allowing for efficient monitoring and interaction without lag. Experimental results demonstrate that Dispider outperforms previous models in streaming scenarios, providing timely and contextually relevant responses during long-duration video interactions."
                },
                "zh": {
                    "title": "主动实时交互的新范式",
                    "desc": "本论文介绍了一种名为Dispider的系统，旨在实现视频大语言模型的主动实时交互。该系统通过分离感知、决策和反应三个能力，解决了实时交互中的固有冲突。Dispider具备轻量级的流媒体处理模块，能够实时监控视频流并识别最佳交互时机。实验结果表明，Dispider在传统视频问答任务中表现优异，并在流媒体场景响应上显著超越了之前的在线模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02157",
            "title": "Personalized Graph-Based Retrieval for Large Language Models",
            "url": "https://huggingface.co/papers/2501.02157",
            "abstract": "As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.",
            "score": 15,
            "issue_id": 1527,
            "pub_date": "2025-01-04",
            "pub_date_card": {
                "ru": "4 января",
                "en": "January 4",
                "zh": "1月4日"
            },
            "hash": "65e3736cfc1e3295",
            "authors": [
                "Steven Au",
                "Cameron J. Dimacali",
                "Ojasmitha Pedirappagari",
                "Namyong Park",
                "Franck Dernoncourt",
                "Yu Wang",
                "Nikos Kanakaris",
                "Hanieh Deilamsalehy",
                "Ryan A. Rossi",
                "Nesreen K. Ahmed"
            ],
            "affiliations": [
                "Adobe Research",
                "Cisco AI Research",
                "Meta AI",
                "University of California Santa Cruz",
                "University of Oregon",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02157.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#optimization",
                    "#graphs",
                    "#multimodal",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Графы знаний на службе персонализации языковых моделей",
                    "desc": "Статья представляет новый подход к персонализации ответов больших языковых моделей (LLM) под названием PGraphRAG. В отличие от существующих методов, полагающихся на историю пользователя, PGraphRAG использует ориентированные на пользователя графы знаний для обогащения контекста. Этот метод улучшает понимание контекста и качество генерируемых ответов, особенно в сценариях с ограниченными данными о пользователе. Экспериментальные результаты показывают, что PGraphRAG превосходит современные методы персонализации в различных задачах."
                },
                "en": {
                    "title": "Revolutionizing Personalization with Graph-based Retrieval",
                    "desc": "This paper introduces a new framework called Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) that enhances the personalization of large language models (LLMs). Unlike traditional methods that depend only on user history, PGraphRAG utilizes user-centric knowledge graphs to provide richer context for generating responses. By integrating structured user information into the retrieval process, it improves the model's understanding and the quality of its outputs, especially in situations where user data is limited. The authors also present a benchmark for evaluating personalized text generation, showing that PGraphRAG outperforms existing methods in various tasks."
                },
                "zh": {
                    "title": "个性化图谱提升生成质量",
                    "desc": "随着大型语言模型的发展，它们在提供个性化和上下文感知的响应方面展现出巨大的潜力。现有的个性化方法通常仅依赖用户历史数据来增强提示，这在数据稀疏的冷启动场景中效果有限。为了解决这些问题，我们提出了个性化图谱检索增强生成（PGraphRAG）框架，利用以用户为中心的知识图谱来丰富个性化。实验结果表明，PGraphRAG在多种任务中显著优于现有的个性化方法，展示了基于图谱的检索在个性化中的独特优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02497",
            "title": "Test-time Computing: from System-1 Thinking to System-2 Thinking",
            "url": "https://huggingface.co/papers/2501.02497",
            "abstract": "The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions.",
            "score": 13,
            "issue_id": 1528,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 января",
                "en": "January 5",
                "zh": "1月5日"
            },
            "hash": "7d9414c60fe7701d",
            "authors": [
                "Yixin Ji",
                "Juntao Li",
                "Hai Ye",
                "Kaixin Wu",
                "Jia Xu",
                "Linjian Mo",
                "Min Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Department of Computer Science, National University of Singapore",
                "School of Computer Science and Technology, Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02497.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#survey",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Масштабирование вычислений: путь к мышлению System-2",
                    "desc": "Эта статья рассматривает масштабирование вычислений во время тестирования для улучшения производительности моделей машинного обучения. Авторы прослеживают эволюцию этой концепции от моделей System-1 до моделей System-2. В работе описываются различные методы, такие как обновление параметров, модификация входных данных и древовидный поиск. Исследование подчеркивает ключевую роль вычислений во время тестирования в переходе от моделей System-1 к сильным моделям System-2."
                },
                "en": {
                    "title": "Unlocking Model Potential: The Power of Test-Time Computing",
                    "desc": "This paper explores the concept of test-time computing scaling and its impact on machine learning models, particularly in enhancing reasoning capabilities. It distinguishes between System-1 models, which focus on improving robustness and generalization through techniques like parameter updating and output calibration, and System-2 models, which utilize methods such as repeated sampling and self-correction for complex problem-solving. The authors trace the evolution from System-1 to System-2 thinking, emphasizing how test-time computing plays a crucial role in this transition. Additionally, the paper identifies potential future research directions in this area."
                },
                "zh": {
                    "title": "测试时计算：从系统-1到强系统-2的关键转变",
                    "desc": "这篇论文探讨了测试时计算扩展对机器学习模型的影响，特别是在复杂推理中的应用。作者指出，测试时计算可以通过参数更新、输入修改、表示编辑和输出校准来提高模型的鲁棒性和泛化能力。对于系统-2模型，测试时计算通过重复采样、自我修正和树搜索来增强模型的推理能力。论文还强调了测试时计算在从系统-1模型向弱系统-2模型再到强系统-2模型转变中的关键作用，并提出了一些未来的研究方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02045",
            "title": "METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring",
            "url": "https://huggingface.co/papers/2501.02045",
            "abstract": "We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.",
            "score": 11,
            "issue_id": 1528,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "60a3568f555ed60f",
            "authors": [
                "Ollie Liu",
                "Sami Jaghouar",
                "Johannes Hagemann",
                "Shangshang Wang",
                "Jason Wiemels",
                "Jeff Kaufman",
                "Willie Neiswanger"
            ],
            "affiliations": [
                "Nucleic Acid Observatory",
                "Prime Intellect",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02045.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#training",
                    "#architecture",
                    "#science",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "METAGENE-1: Метагеномная модель для мониторинга здоровья населения",
                    "desc": "METAGENE-1 - это автореграссивная трансформерная модель с 7 миллиардами параметров, обученная на разнообразных метагеномных последовательностях ДНК и РНК. Модель создана для анализа геномной информации из образцов сточных вод с целью мониторинга пандемий и обнаружения патогенов. Авторы описывают процесс предобучения, включая токенизацию и архитектуру модели, а также демонстрируют результаты на различных геномных задачах. METAGENE-1 показывает высокую эффективность в обнаружении патогенов человека и встраивании геномных последовательностей, что открывает перспективы для применения в общественном здравоохранении."
                },
                "en": {
                    "title": "Unlocking Metagenomics: METAGENE-1 for Pandemic Preparedness",
                    "desc": "The paper introduces METAGENE-1, a large autoregressive transformer model designed for metagenomic data analysis. It is pretrained on a vast dataset of metagenomic DNA and RNA sequences derived from human wastewater, totaling over 1.5 trillion base pairs. The model aims to enhance pandemic monitoring and pathogen detection by capturing the diverse genomic information present in wastewater samples. The authors detail their tokenization strategy and model architecture, demonstrating that METAGENE-1 achieves state-of-the-art performance in genomic benchmarks and applications related to public health."
                },
                "zh": {
                    "title": "METAGENE-1：元基因组基础模型助力公共卫生监测",
                    "desc": "我们预训练了METAGENE-1，这是一个拥有70亿参数的自回归变换器模型，称为元基因组基础模型。该模型在一个包含超过1.5万亿碱基对的多样化元基因组DNA和RNA序列的新数据集上进行训练，这些数据来自大量人类废水样本。METAGENE-1的目标是捕捉废水中存在的基因组信息的完整分布，以帮助进行疫情监测和病原体检测。我们展示了该模型在元基因组数据集上的预训练结果，证明其在公共卫生应用中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02690",
            "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
            "url": "https://huggingface.co/papers/2501.02690",
            "abstract": "4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.",
            "score": 10,
            "issue_id": 1530,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 января",
                "en": "January 5",
                "zh": "1月5日"
            },
            "hash": "b4c147a2637166a8",
            "authors": [
                "Weikang Bian",
                "Zhaoyang Huang",
                "Xiaoyu Shi",
                "Yijin Li",
                "Fu-Yun Wang",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Avolution AI",
                "Centre for Perceptual and Interactive Intelligence",
                "Multimedia Laboratory, The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02690.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Революция в генерации видео: 4D-контроль с помощью гауссовых полей",
                    "desc": "Эта статья представляет инновационный подход к генерации видео с 4D-контролем, используя псевдо-4D гауссовы поля и модель Diffusion Transformer (DiT). Авторы предлагают метод Dense 3D Point Tracking (D3D-PT) для эффективного построения гауссовых полей, превосходящий существующие решения по точности и скорости. Разработанная система GS-DiT позволяет генерировать видео с одинаковым динамическим содержанием, но с разными параметрами камеры, что открывает новые возможности для создания кинематографических эффектов. Метод демонстрирует сильные обобщающие способности и расширяет возможности 4D-контроля в генерации видео."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with 4D Control",
                    "desc": "This paper introduces a new method for generating videos that can be controlled in four dimensions (4D), which includes both camera movement and object motion. The authors propose a framework called GS-DiT that utilizes pseudo 4D Gaussian fields to enhance video generation, allowing for advanced cinematic effects. They also present a Dense 3D Point Tracking (D3D-PT) technique that improves the accuracy and speed of tracking 3D points compared to existing methods. Overall, GS-DiT enables the creation of dynamic videos with flexible camera parameters, significantly advancing the capabilities of video generation models."
                },
                "zh": {
                    "title": "伪4D高斯场：视频生成的新突破",
                    "desc": "本论文提出了一种新颖的框架，利用伪4D高斯场进行视频生成，以支持复杂的镜头技术。我们通过密集的3D点跟踪构建伪4D高斯场，并为所有视频帧渲染该高斯场。为了提升GS-DiT的训练效果，我们还提出了一种高效的密集3D点跟踪方法，显著提高了准确性和推理速度。GS-DiT能够在不同的相机参数下生成具有相同动态内容的视频，扩展了视频生成的4D可控性，成为创意视频制作的强大工具。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03059",
            "title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2501.03059",
            "abstract": "We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce \\benchmark, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark. Project page is available at https://guyyariv.github.io/TTM/.",
            "score": 9,
            "issue_id": 1532,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "4f24667b663efb7d",
            "authors": [
                "Guy Yariv",
                "Yuval Kirstain",
                "Amit Zohar",
                "Shelly Sheynin",
                "Yaniv Taigman",
                "Yossi Adi",
                "Sagie Benaim",
                "Adam Polyak"
            ],
            "affiliations": [
                "FAIR, Meta",
                "GenAI, Meta",
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03059.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Генерация реалистичных видео из статичных изображений с помощью масок траекторий движения",
                    "desc": "Статья представляет новый подход к генерации видео из изображений (I2V) на основе текстового описания. Авторы предлагают двухэтапную композиционную модель, которая сначала генерирует промежуточное представление в виде маски траектории движения объектов. Затем это представление используется для генерации видео с применением объектно-ориентированных целевых функций внимания. Эксперименты показывают, что предложенный метод достигает лучших результатов по временной согласованности, реалистичности движения и соответствию текстовому описанию."
                },
                "en": {
                    "title": "Transforming Images into Realistic Videos with Motion Precision",
                    "desc": "This paper addresses the challenge of generating videos from static images using textual descriptions, known as Image-to-Video (I2V) generation. The authors propose a two-stage framework that first creates an intermediate representation to capture object semantics and motion, followed by a video generation stage that utilizes this representation. A key innovation is the use of a mask-based motion trajectory, which helps maintain accurate object motion and consistency across frames. The method is evaluated against challenging benchmarks and shows superior performance in terms of motion realism and coherence, while also introducing a new benchmark for I2V generation."
                },
                "zh": {
                    "title": "图像到视频生成的新突破",
                    "desc": "本文探讨了图像到视频（I2V）生成的任务，即根据文本描述将静态图像转换为逼真的视频序列。尽管近期的进展能够生成照片级真实感的输出，但在多物体场景中，视频的物体运动准确性和一致性仍然存在挑战。为了解决这些问题，我们提出了一种两阶段的组合框架，首先生成明确的中间表示，然后基于该表示生成视频。我们的创新在于引入了一种基于掩码的运动轨迹作为中间表示，能够捕捉语义物体信息和运动，从而实现运动和语义的紧凑而富有表现力的表示。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03006",
            "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
            "url": "https://huggingface.co/papers/2501.03006",
            "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
            "score": 8,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "e85e5fa9a03d5d04",
            "authors": [
                "Luozhou Wang",
                "Yijun Li",
                "Zhifei Chen",
                "Jui-Hsien Wang",
                "Zhifei Zhang",
                "He Zhang",
                "Zhe Lin",
                "Yingcong Chen"
            ],
            "affiliations": [
                "Adobe Research",
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03006.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "TransPixar: Прорыв в генерации RGBA-видео для визуальных эффектов",
                    "desc": "TransPixar - это новый метод генерации RGBA-видео, расширяющий возможности предобученных видеомоделей. Он использует архитектуру диффузионного трансформера (DiT) и токены, специфичные для альфа-канала, для совместной генерации RGB и альфа-каналов с высокой согласованностью. Метод применяет тонкую настройку на основе LoRA и оптимизирует механизмы внимания для сохранения сильных сторон исходной RGB-модели. TransPixar эффективно генерирует разнообразные и согласованные RGBA-видео, открывая новые возможности для создания визуальных эффектов и интерактивного контента."
                },
                "en": {
                    "title": "TransPixar: Bridging RGB and Alpha for Enhanced Video Generation",
                    "desc": "This paper presents TransPixar, a novel method for generating RGBA videos, which include transparency information crucial for visual effects. The challenge lies in the limited datasets and the need to adapt existing models to handle alpha channels effectively. TransPixar utilizes a diffusion transformer architecture and incorporates alpha-specific tokens, allowing it to generate both RGB and alpha channels simultaneously. By optimizing attention mechanisms and employing LoRA-based fine-tuning, TransPixar achieves high consistency between RGB and alpha outputs, enhancing the quality of video generation for applications in VFX and interactive media."
                },
                "zh": {
                    "title": "TransPixar：生成高质量RGBA视频的新方法",
                    "desc": "本文介绍了一种名为TransPixar的方法，旨在生成包含透明通道的RGBA视频。传统的视频生成模型在处理透明效果时面临挑战，TransPixar通过扩展预训练模型来解决这一问题。该方法利用扩散变换器架构，结合特定的透明通道标记，并通过LoRA微调实现RGB和透明通道的高一致性生成。最终，TransPixar在有限的数据集上优化了注意力机制，成功生成多样且一致的RGBA视频，推动了视觉特效和互动内容创作的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01790",
            "title": "Ingredients: Blending Custom Photos with Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2501.01790",
            "abstract": "This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as Ingredients. Generally, our method consists of three primary modules: (i) a facial extractor that captures versatile and precise facial features for each human ID from both global and local perspectives; (ii) a multi-scale projector that maps face embeddings into the contextual space of image query in video diffusion transformers; (iii) an ID router that dynamically combines and allocates multiple ID embedding to the corresponding space-time regions. Leveraging a meticulously curated text-video dataset and a multi-stage training protocol, Ingredients demonstrates superior performance in turning custom photos into dynamic and personalized video content. Qualitative evaluations highlight the advantages of proposed method, positioning it as a significant advancement toward more effective generative video control tools in Transformer-based architecture, compared to existing methods. The data, code, and model weights are publicly available at: https://github.com/feizc/Ingredients.",
            "score": 6,
            "issue_id": 1528,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "dd1ccebdd2fcf276",
            "authors": [
                "Zhengcong Fei",
                "Debang Li",
                "Di Qiu",
                "Changqian Yu",
                "Mingyuan Fan"
            ],
            "affiliations": [
                "Kunlun Inc. Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01790.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#architecture",
                    "#video",
                    "#dataset",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Персонализированное видео из фотографий: новый уровень контроля в генеративных моделях",
                    "desc": "Статья представляет новый метод под названием Ingredients для создания персонализированных видео с использованием нескольких фотографий конкретных людей. Метод состоит из трех основных модулей: экстрактора лицевых признаков, многомасштабного проектора и маршрутизатора идентификаторов. Ingredients использует тщательно подобранный набор данных текст-видео и многоэтапный протокол обучения для достижения превосходных результатов. Качественная оценка показывает преимущества предложенного метода по сравнению с существующими подходами в области генеративного контроля видео на основе архитектуры Transformer."
                },
                "en": {
                    "title": "Transforming Photos into Personalized Videos with Ingredients",
                    "desc": "This paper introduces a novel framework called Ingredients for creating personalized videos using multiple identity photos. It employs a facial extractor to accurately capture facial features, a multi-scale projector to integrate these features into video diffusion transformers, and an ID router to manage the allocation of identity embeddings across different time and space regions in the video. The framework is trained on a carefully selected text-video dataset, enhancing its ability to generate dynamic video content from custom images. The results show that Ingredients outperforms existing methods, marking a significant step forward in generative video control using Transformer architectures."
                },
                "zh": {
                    "title": "个性化视频创作的新突破",
                    "desc": "本文提出了一种强大的框架，通过结合多个特定身份照片，定制视频创作，称为Ingredients。该方法主要由三个模块组成：面部提取器、多个尺度投影器和身份路由器，分别用于提取面部特征、映射面部嵌入和动态分配身份嵌入。通过精心策划的文本-视频数据集和多阶段训练协议，Ingredients在将自定义照片转化为动态个性化视频内容方面表现出色。定性评估显示，该方法在基于Transformer的架构中，相较于现有方法，显著提升了生成视频控制工具的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02576",
            "title": "DepthMaster: Taming Diffusion Models for Monocular Depth Estimation",
            "url": "https://huggingface.co/papers/2501.02576",
            "abstract": "Monocular depth estimation within the diffusion-denoising paradigm demonstrates impressive generalization ability but suffers from low inference speed. Recent methods adopt a single-step deterministic paradigm to improve inference efficiency while maintaining comparable performance. However, they overlook the gap between generative and discriminative features, leading to suboptimal results. In this work, we propose DepthMaster, a single-step diffusion model designed to adapt generative features for the discriminative depth estimation task. First, to mitigate overfitting to texture details introduced by generative features, we propose a Feature Alignment module, which incorporates high-quality semantic features to enhance the denoising network's representation capability. Second, to address the lack of fine-grained details in the single-step deterministic framework, we propose a Fourier Enhancement module to adaptively balance low-frequency structure and high-frequency details. We adopt a two-stage training strategy to fully leverage the potential of the two modules. In the first stage, we focus on learning the global scene structure with the Feature Alignment module, while in the second stage, we exploit the Fourier Enhancement module to improve the visual quality. Through these efforts, our model achieves state-of-the-art performance in terms of generalization and detail preservation, outperforming other diffusion-based methods across various datasets. Our project page can be found at https://indu1ge.github.io/DepthMaster_page.",
            "score": 5,
            "issue_id": 1536,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 января",
                "en": "January 5",
                "zh": "1月5日"
            },
            "hash": "a8429b95ef4eb7b7",
            "authors": [
                "Ziyang Song",
                "Zerong Wang",
                "Bo Li",
                "Hao Zhang",
                "Ruijie Zhu",
                "Li Liu",
                "Peng-Tao Jiang",
                "Tianzhu Zhang"
            ],
            "affiliations": [
                "School of Information Science and Technology, University of Science and Technology of China (USTC), Hefei 230026, P.R.China",
                "vivo Mobile Communication Co., Ltd., Hangzhou 310030, P.R.China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02576.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#diffusion",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "DepthMaster: Однопроходная диффузионная модель для точной оценки глубины с улучшенной генерализацией",
                    "desc": "DepthMaster - это однопроходная диффузионная модель для монокулярной оценки глубины. Она использует модуль выравнивания признаков для улучшения представления семантических особенностей и модуль улучшения Фурье для балансировки низкочастотной структуры и высокочастотных деталей. Модель обучается в два этапа: сначала фокусируется на глобальной структуре сцены, затем улучшает визуальное качество. DepthMaster превосходит другие диффузионные методы по обобщающей способности и сохранению деталей на различных наборах данных."
                },
                "en": {
                    "title": "DepthMaster: Bridging Generative and Discriminative Depth Estimation",
                    "desc": "This paper introduces DepthMaster, a single-step diffusion model aimed at improving monocular depth estimation. It addresses the inefficiencies of previous methods by integrating a Feature Alignment module to enhance the representation of semantic features and reduce overfitting to textures. Additionally, a Fourier Enhancement module is proposed to balance low-frequency structures with high-frequency details, ensuring finer depth estimation. The two-stage training strategy allows the model to first learn global scene structures and then refine visual quality, resulting in state-of-the-art performance across various datasets."
                },
                "zh": {
                    "title": "DepthMaster：提升深度估计的单步扩散模型",
                    "desc": "本文提出了一种名为DepthMaster的单步扩散模型，用于单目深度估计。该模型通过特征对齐模块和傅里叶增强模块，优化生成特征以适应判别性深度估计任务。特征对齐模块增强了去噪网络的表示能力，而傅里叶增强模块则平衡了低频结构和高频细节。通过两阶段训练策略，DepthMaster在泛化能力和细节保留方面达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01830",
            "title": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models",
            "url": "https://huggingface.co/papers/2501.01830",
            "abstract": "Automated red-teaming has become a crucial approach for uncovering vulnerabilities in large language models (LLMs). However, most existing methods focus on isolated safety flaws, limiting their ability to adapt to dynamic defenses and uncover complex vulnerabilities efficiently. To address this challenge, we propose Auto-RT, a reinforcement learning framework that automatically explores and optimizes complex attack strategies to effectively uncover security vulnerabilities through malicious queries. Specifically, we introduce two key mechanisms to reduce exploration complexity and improve strategy optimization: 1) Early-terminated Exploration, which accelerate exploration by focusing on high-potential attack strategies; and 2) Progressive Reward Tracking algorithm with intermediate downgrade models, which dynamically refine the search trajectory toward successful vulnerability exploitation. Extensive experiments across diverse LLMs demonstrate that, by significantly improving exploration efficiency and automatically optimizing attack strategies, Auto-RT detects a boarder range of vulnerabilities, achieving a faster detection speed and 16.63\\% higher success rates compared to existing methods.",
            "score": 4,
            "issue_id": 1529,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 января",
                "en": "January 3",
                "zh": "1月3日"
            },
            "hash": "5b08b81c52ec8da8",
            "authors": [
                "Yanjiang Liu",
                "Shuhen Zhou",
                "Yaojie Lu",
                "Huijia Zhu",
                "Weiqiang Wang",
                "Hongyu Lin",
                "Ben He",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Ant Group",
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01830.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Auto-RT: Умная защита больших языковых моделей",
                    "desc": "Авторы представляют Auto-RT - фреймворк на основе обучения с подкреплением для автоматизированного поиска уязвимостей в больших языковых моделях (LLM). Система использует механизмы раннего прекращения исследования и прогрессивного отслеживания наград для оптимизации стратегий атак. Auto-RT превосходит существующие методы, обнаруживая более широкий спектр уязвимостей с большей скоростью и на 16.63% более высоким уровнем успеха. Этот подход позволяет эффективно выявлять сложные уязвимости в LLM через вредоносные запросы."
                },
                "en": {
                    "title": "Auto-RT: Revolutionizing Vulnerability Detection in LLMs",
                    "desc": "This paper presents Auto-RT, a reinforcement learning framework designed to enhance automated red-teaming for large language models (LLMs). Unlike traditional methods that target isolated safety flaws, Auto-RT efficiently uncovers complex vulnerabilities by optimizing attack strategies through malicious queries. It introduces two innovative mechanisms: Early-terminated Exploration to prioritize promising attack strategies, and Progressive Reward Tracking to refine the search process dynamically. Experimental results show that Auto-RT significantly improves exploration efficiency and detection success rates, outperforming existing approaches."
                },
                "zh": {
                    "title": "自动化红队：高效发现语言模型漏洞的利器",
                    "desc": "自动化红队技术在发现大型语言模型（LLMs）中的漏洞方面变得至关重要。现有方法大多集中于孤立的安全缺陷，限制了其适应动态防御和高效发现复杂漏洞的能力。为了解决这个问题，我们提出了Auto-RT，一个强化学习框架，能够自动探索和优化复杂的攻击策略，通过恶意查询有效发现安全漏洞。我们的实验表明，Auto-RT显著提高了探索效率和攻击策略的自动优化，检测到更广泛的漏洞，检测速度更快，成功率提高了16.63%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02506",
            "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use",
            "url": "https://huggingface.co/papers/2501.02506",
            "abstract": "Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/bytedance-research/ToolHop.",
            "score": 4,
            "issue_id": 1529,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 января",
                "en": "January 5",
                "zh": "1月5日"
            },
            "hash": "f785173226e5f9fc",
            "authors": [
                "Junjie Ye",
                "Zhengyin Du",
                "Xuesong Yao",
                "Weijian Lin",
                "Yufei Xu",
                "Zehui Chen",
                "Zaiyuan Wang",
                "Sining Zhu",
                "Zhiheng Xi",
                "Siyu Yuan",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang",
                "Jiechao Chen"
            ],
            "affiliations": [
                "ByteDance",
                "Institute of Modern Languages and Linguistics, Fudan University",
                "School of Computer Science, Fudan University",
                "School of Data Science, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02506.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "ToolHop: новый стандарт для оценки многоэтапного использования инструментов в LLM",
                    "desc": "Статья представляет новый набор данных ToolHop для оценки многоэтапного использования инструментов большими языковыми моделями (LLM). ToolHop содержит 995 пользовательских запросов и 3912 связанных инструментов, обеспечивая разнообразие запросов, взаимозависимости и возможность локального выполнения. Авторы оценили 14 LLM из пяти семейств моделей, выявив значительные трудности в обработке сценариев многоэтапного использования инструментов. Лучшая модель, GPT-4o, достигла точности 49.04%, что указывает на большой потенциал для улучшения."
                },
                "en": {
                    "title": "ToolHop: Advancing Multi-Hop Tool Use Evaluation for LLMs",
                    "desc": "This paper introduces ToolHop, a new dataset designed to evaluate how well large language models (LLMs) can use multiple tools in a single task. It includes 995 user queries and 3,912 tools, focusing on diverse and interdependent queries that can be executed locally. The authors tested 14 different LLMs, revealing that even the best-performing model, GPT-4o, only achieved 49.04% accuracy, indicating significant challenges in multi-hop tool use. The findings highlight different strategies employed by various model families, providing insights for future improvements in LLM capabilities."
                },
                "zh": {
                    "title": "ToolHop：多跳工具使用的有效评估数据集",
                    "desc": "本文介绍了ToolHop数据集，该数据集包含995个用户查询和3912个相关工具，旨在有效评估大型语言模型（LLMs）在多跳工具使用中的理解、推理和功能调用能力。通过新颖的查询驱动数据构建方法，ToolHop确保了查询的多样性、工具的局部可执行性和可验证的答案。我们对14个不同模型（如LLaMA3.1、Qwen2.5等）进行了评估，发现它们在处理多跳工具使用场景时面临显著挑战。尽管GPT-4o模型的准确率为49.04%，但仍有很大的改进空间，分析还揭示了不同模型家族在工具使用策略上的差异，为未来的研究提供了有价值的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02423",
            "title": "Scaling Laws for Floating Point Quantization Training",
            "url": "https://huggingface.co/papers/2501.02423",
            "abstract": "Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within a wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits.",
            "score": 3,
            "issue_id": 1537,
            "pub_date": "2025-01-05",
            "pub_date_card": {
                "ru": "5 января",
                "en": "January 5",
                "zh": "1月5日"
            },
            "hash": "be6872257cb9a129",
            "authors": [
                "Xingwu Sun",
                "Shuaipeng Li",
                "Ruobing Xie",
                "Weidong Han",
                "Kan Wu",
                "Zhen Yang",
                "Yixing Li",
                "An Wang",
                "Shuai Li",
                "Jinbao Xue",
                "Yu Cheng",
                "Yangyu Tao",
                "Zhanhui Kang",
                "Chengzhong Xu",
                "Di Wang",
                "Jie Jiang"
            ],
            "affiliations": [
                "Tencent Hunyuan",
                "The Chinese University of Hong Kong",
                "Tokyo Institute of Technology",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02423.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Оптимизация точности вычислений в обучении языковых моделей",
                    "desc": "Статья исследует влияние квантования с плавающей запятой на обучение больших языковых моделей (LLM). Авторы анализируют роль экспоненциальных и мантиссных битов, а также размера обучающих данных в производительности моделей. Они представляют унифицированный закон масштабирования для квантования с плавающей запятой и дают рекомендации по оптимальному соотношению битов и размеру данных. Исследование показывает, что оптимальная точность квантования находится в диапазоне 4-8 бит для широкого спектра вычислительных мощностей."
                },
                "en": {
                    "title": "Optimizing Floating-Point Quantization for Better LLM Performance",
                    "desc": "This paper investigates the impact of floating-point quantization on the training performance of large language models (LLMs). It highlights that previous research primarily focused on integer quantization, neglecting the nuances of floating-point quantization. The authors establish a unified scaling law for floating-point quantization and provide insights on the optimal ratio of exponent to mantissa bits, emphasizing that exponent bits have a greater influence on model performance. Additionally, they identify a critical data size threshold, beyond which performance may degrade, and suggest that the best precision for cost-performance lies between 4-8 bits, depending on computational power."
                },
                "zh": {
                    "title": "低精度训练：优化浮点量化的关键",
                    "desc": "低精度训练被认为是降低训练和推理成本的有效策略。以往的研究主要集中在整数量化上，而对浮点量化的研究相对较少，导致无法很好地适应大语言模型的损失情况。本文深入探讨了浮点量化训练中目标、指数位、尾数位和缩放因子的计算粒度对大语言模型性能的影响，并提出了统一的浮点量化缩放法则。研究结果表明，指数位对模型性能的贡献略高于尾数位，并发现了低精度训练中的关键数据大小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02832",
            "title": "Samba-asr state-of-the-art speech recognition leveraging structured state-space models",
            "url": "https://huggingface.co/papers/2501.02832",
            "abstract": "We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.   Experimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.   Our contributions include:   A new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research.",
            "score": 3,
            "issue_id": 1530,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "ed3c4a6192d0c5f9",
            "authors": [
                "Syed Abdul Gaffar Shakhadri",
                "Kruthika KR",
                "Kartik Basavaraj Angadi"
            ],
            "affiliations": [
                "SandLogic Technologies Pvt Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02832.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#architecture",
                    "#benchmark",
                    "#low_resource",
                    "#open_source"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "Samba ASR: революция в распознавании речи с помощью моделей пространства состояний",
                    "desc": "Представлена модель Samba ASR - первая современная система автоматического распознавания речи, использующая архитектуру Mamba в качестве энкодера и декодера на основе моделей пространства состояний (SSM). В отличие от трансформерных моделей, Samba ASR эффективно моделирует локальные и глобальные временные зависимости, достигая значительных улучшений производительности. Экспериментальные результаты показывают, что Samba ASR превосходит существующие модели с открытым исходным кодом на основе трансформеров по различным стандартным показателям. Модель демонстрирует значительное снижение показателя Word Error Rate (WER) и высокую эффективность даже при ограниченных ресурсах."
                },
                "en": {
                    "title": "Samba ASR: Redefining Speech Recognition with State-Space Models",
                    "desc": "Samba ASR is a groundbreaking Automatic Speech Recognition model that utilizes the innovative Mamba architecture, which functions as both the encoder and decoder. This model departs from traditional transformer-based approaches by employing state-space models (SSMs) to effectively capture both local and global temporal dependencies, leading to enhanced performance. By overcoming the challenges associated with transformers, such as their inefficiency with long input sequences, Samba ASR achieves superior accuracy and efficiency in recognizing speech. Extensive testing shows that Samba ASR not only outperforms existing transformer-based models but also excels in low-resource environments, making it a robust solution for various ASR applications."
                },
                "zh": {
                    "title": "Samba ASR：超越变换器的语音识别新标杆",
                    "desc": "我们提出了Samba ASR，这是第一个利用新型Mamba架构作为编码器和解码器的最先进自动语音识别（ASR）模型。与基于变换器的ASR模型不同，Samba ASR通过高效的状态空间动态建模局部和全局时间依赖关系，从而实现显著的性能提升。该模型克服了变换器在处理长距离依赖和输入长度的平方扩展等方面的局限性，展现出更高的准确性和效率。实验结果表明，Samba ASR在多个标准基准测试中超越了现有的开源变换器ASR模型，确立了其在ASR领域的新标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00912",
            "title": "AutoPresent: Designing Structured Visuals from Scratch",
            "url": "https://huggingface.co/papers/2501.00912",
            "abstract": "Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SlidesBench benchmark, the first benchmark for slide generation with 7k training and 585 testing examples derived from 310 slide decks across 10 domains. SlidesBench supports evaluations that are (i)reference-based to measure similarity to a target slide, and (ii)reference-free to measure the design quality of generated slides alone. We benchmark end-to-end image generation and program generation methods with a variety of models, and find that programmatic methods produce higher-quality slides in user-interactable formats. Built on the success of program generation, we create AutoPresent, an 8B Llama-based model trained on 7k pairs of instructions paired with code for slide generation, and achieve results comparable to the closed-source model GPT-4o. We further explore iterative design refinement where the model is tasked to self-refine its own output, and we found that this process improves the slide's quality. We hope that our work will provide a basis for future work on generating structured visuals.",
            "score": 2,
            "issue_id": 1539,
            "pub_date": "2025-01-01",
            "pub_date_card": {
                "ru": "1 января",
                "en": "January 1",
                "zh": "1月1日"
            },
            "hash": "ea7b88fcc0a2025b",
            "authors": [
                "Jiaxin Ge",
                "Zora Zhiruo Wang",
                "Xuhui Zhou",
                "Yi-Hao Peng",
                "Sanjay Subramanian",
                "Qinyue Tan",
                "Maarten Sap",
                "Alane Suhr",
                "Daniel Fried",
                "Graham Neubig",
                "Trevor Darrell"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00912.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#story_generation",
                    "#training",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Автоматизация создания презентаций: от текста к структурированным визуальным материалам",
                    "desc": "Эта статья представляет новый бенчмарк SlidesBench для автоматической генерации слайдов презентаций на основе текстовых инструкций. Авторы сравнивают методы генерации изображений и программного кода, обнаружив преимущество последнего подхода. Они создают модель AutoPresent на базе Llama для генерации кода слайдов, достигающую результатов, сопоставимых с GPT-4. Исследователи также изучают итеративное улучшение дизайна слайдов с помощью самооптимизации модели."
                },
                "en": {
                    "title": "Automating Slide Generation with Advanced Models",
                    "desc": "This paper addresses the challenge of creating automated slide presentations from natural language instructions. It introduces the SlidesBench benchmark, which includes a large dataset for training and testing slide generation models. The authors evaluate various methods, finding that programmatic approaches yield higher-quality slides. They also present AutoPresent, a model that competes with advanced models like GPT-4o, and demonstrate that iterative design refinement enhances the quality of generated slides."
                },
                "zh": {
                    "title": "自动生成高质量演示幻灯片的未来",
                    "desc": "本研究旨在自动生成演示幻灯片，解决内容创作和视觉规划的挑战。我们首次引入SlidesBench基准，包含7000个训练样本和585个测试样本，涵盖10个领域的310个幻灯片集。通过对比不同模型的图像生成和程序生成方法，我们发现程序生成方法在用户交互格式中生成的幻灯片质量更高。基于程序生成的成功，我们开发了AutoPresent模型，并通过自我优化过程进一步提升幻灯片的质量。"
                }
            }
        }
    ],
    "link_prev": "2025-01-06.html",
    "link_next": "2025-01-08.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1月6日"
    },
    "short_date_next": {
        "ru": "08.01",
        "en": "01/08",
        "zh": "1月8日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 6,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 5,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了大语言模型（LLMs）在解决复杂数学问题时的表现。模型使用分而治之的方法和上下文学习（ICL）示例。然而，ICL示例存在两个关键问题：粒度不匹配和负面效应噪声问题。这些问题会影响模型的推理质量。作者提出了BoostStep方法，通过步骤粒度的对齐和“首次尝试”策略提供相关的ICL示例，提高推理质量。BoostStep还可以与蒙特卡罗树搜索方法（MCTS）结合使用，提升模型性能。实验结果显示，BoostStep显著提高了GPT-4o和Qwen2.5-Math-72B的表现。",
        "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning",
        "pinyin": "这篇文章讨论了大语言模型（LLMs）在解决复杂数学问题时的表现。模型使用分而治之的方法和上下文学习（ICL）示例。然而，ICL示例存在两个关键问题：粒度不匹配和负面效应噪声问题。这些问题会影响模型的推理质量。作者提出了BoostStep方法，通过步骤粒度的对齐和“首次尝试”策略提供相关的ICL示例，提高推理质量。BoostStep还可以与蒙特卡罗树搜索方法（MCTS）结合使用，提升模型性能。实验结果显示，BoostStep显著提高了GPT-4o和Qwen2.5-Math-72B的表现。\n\nZhè piān wénzhāng tǎolùn le dà yǔyán móxíng (LLMs) zài jiějué fùzá xùexué wèntí shí de biǎoxiàn. Móxíng shǐyòng fēn ér zhìzhī de fāngfǎ hé shàngxiàwén xuéxí (ICL) shìlì. Rán'ér, ICL shìlì cúnzài liǎng gè guǎnjiàn wèntí: lìdù bù pǐpěi hé fùmiàn xiàoyìng zàoshēng wèntí. Zhèxiē wèntí huì yǐngxiǎng móxíng de tuīlǐ zhìliàng. Zuòzhě tíchū le BoostStep fāngfǎ, tōngguò bùzhòu lìdù de duìqí hé “shǒucì chángshì” cèlüè tígōng xiāngguān de ICL shìlì, tígāo tuīlǐ zhìliàng. BoostStep hái kěyǐ yǔ méngtèkǎluó shù sōusuǒ fāngfǎ (MCTS) jiéhé shǐyòng, tíshēng móxíng xìngnéng. Shíyàn jiéguǒ xiǎnshì, BoostStep xiǎnzhù tígāo le GPT-4o hé Qwen2.5-Math-72B de biǎoxiàn.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'}, {'word': '数学', 'pinyin': 'shù xué', 'trans': 'mathematics'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '分而治之', 'pinyin': 'fēn ér zhì zhī', 'trans': 'divide and conquer'}, {'word': '上下文学习', 'pinyin': 'shàng xià wén xué xí', 'trans': 'in-context learning'}, {'word': '示例', 'pinyin': 'shì lì', 'trans': 'example'}, {'word': '关键', 'pinyin': 'guǎn jiàn', 'trans': 'key'}, {'word': '粒度', 'pinyin': 'lì dù', 'trans': 'granularity'}, {'word': '负面效应', 'pinyin': 'fù miàn xiào yìng', 'trans': 'negative effect'}, {'word': '噪声', 'pinyin': 'zào shēng', 'trans': 'noise'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': 'BoostStep', 'pinyin': 'BoostStep', 'trans': 'BoostStep'}, {'word': '对齐', 'pinyin': 'duì qí', 'trans': 'align'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'relevant'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '蒙特卡罗树搜索方法', 'pinyin': 'méng tè kǎ luó shù sōu suǒ fāng fǎ', 'trans': 'Monte Carlo Tree Search method'}, {'word': '结合', 'pinyin': 'jié hé', 'trans': 'combine'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': '提升', 'pinyin': 'tí shēng', 'trans': 'enhance'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': 'GPT-4o', 'pinyin': 'GPT-4o', 'trans': 'GPT-4o'}, {'word': 'Qwen2.5-Math-72B', 'pinyin': 'Qwen2.5-Math-72B', 'trans': 'Qwen2.5-Math-72B'}]",
        "trans": "This article discusses the performance of large language models (LLMs) in solving complex mathematical problems. The models use a divide-and-conquer approach and in-context learning (ICL) examples. However, ICL examples face two key issues: granularity mismatch and negative impact noise problems. These issues affect the quality of the model's reasoning. The authors propose the BoostStep method, which aligns step granularity and provides relevant ICL examples through a \"first attempt\" strategy, thereby improving reasoning quality. BoostStep can also be combined with the Monte Carlo Tree Search (MCTS) method to enhance model performance. Experimental results show that BoostStep significantly improves the performance of GPT-4o and Qwen2.5-Math-72B.",
        "update_ts": "2025-01-07 09:11"
    }
}