{
    "date": {
        "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 20",
        "zh": "11æœˆ20æ—¥"
    },
    "time_utc": "2024-11-20 16:12",
    "weekday": 2,
    "issue_id": 687,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.12372",
            "title": "RedPajama: an Open Dataset for Training Large Language Models",
            "url": "https://huggingface.co/papers/2411.12372",
            "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.",
            "score": 11,
            "issue_id": 682,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "9393337102332466",
            "authors": [
                "Maurice Weber",
                "Daniel Fu",
                "Quentin Anthony",
                "Yonatan Oren",
                "Shane Adams",
                "Anton Alexandrov",
                "Xiaozhong Lyu",
                "Huu Nguyen",
                "Xiaozhe Yao",
                "Virginia Adams",
                "Ben Athiwaratkun",
                "Rahul Chalamala",
                "Kezhen Chen",
                "Max Ryabinin",
                "Tri Dao",
                "Percy Liang",
                "Christopher RÃ©",
                "Irina Rish",
                "Ce Zhang"
            ],
            "affiliations": [
                "Caltech",
                "ETH Zurich",
                "EleutherAI",
                "Mila, MontrÃ©al, Canada",
                "Ohio State University",
                "Ontocord.ai",
                "Princeton University",
                "Stanford University",
                "Together AI",
                "University of Chicago",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12372.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#data",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RedPajama: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… RedPajama Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°: Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². RedPajama-V1 Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LLaMA, Ğ° RedPajama-V2 ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Advancing Open-Source Language Models with RedPajama Datasets",
                    "desc": "This paper discusses the importance of transparency and quality in the datasets used for training large language models. It identifies three main challenges: the need for clear data curation processes, access to high-quality data, and the availability of metadata for better dataset analysis. To tackle these issues, the authors introduce the RedPajama datasets, which include a comprehensive reproduction of the LLaMA training dataset and a large web-only dataset with quality signals. The findings highlight how these datasets can improve the development of open-source language models by providing high-quality data and insights into effective data curation practices."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¼€æ”¾è¯­è¨€æ¨¡å‹çš„é€æ˜ä¸é«˜æ•ˆ",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®é›†æ„å»ºå’Œè¿‡æ»¤æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†é€æ˜åº¦ã€æ•°æ®è´¨é‡å’Œå…ƒæ•°æ®å¯ç”¨æ€§çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†RedPajama-V1å’ŒRedPajama-V2æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæä¾›é«˜è´¨é‡çš„å¼€æ”¾æ•°æ®ã€‚RedPajamaæ•°æ®é›†åŒ…å«è¶…è¿‡100ä¸‡äº¿ä¸ªæ ‡è®°ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸï¼Œå¹¶æä¾›è´¨é‡ä¿¡å·ä»¥å¸®åŠ©æ•°æ®è¿‡æ»¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨ç½‘ç»œæ•°æ®çš„è´¨é‡ä¿¡å·å¯ä»¥æœ‰æ•ˆåœ°æ„å»ºé«˜è´¨é‡çš„æ•°æ®å­é›†ï¼Œæ¨åŠ¨é€æ˜ä¸”é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11925",
            "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2411.11925",
            "abstract": "Continuous-valued Autoregressive (AR) image generation models have demonstrated notable superiority over their discrete-token counterparts, showcasing considerable reconstruction quality and higher generation fidelity. However, the computational demands of the autoregressive framework result in significant inference overhead. While speculative decoding has proven effective in accelerating Large Language Models (LLMs), their adaptation to continuous-valued visual autoregressive models remains unexplored. This work generalizes the speculative decoding algorithm from discrete tokens to continuous space. By analyzing the intrinsic properties of output distribution, we establish a tailored acceptance criterion for the diffusion distributions prevalent in such models. To overcome the inconsistency that occurred in speculative decoding output distributions, we introduce denoising trajectory alignment and token pre-filling methods. Additionally, we identify the hard-to-sample distribution in the rejection phase. To mitigate this issue, we propose a meticulous acceptance-rejection sampling method with a proper upper bound, thereby circumventing complex integration. Experimental results show that our continuous speculative decoding achieves a remarkable 2.33times speed-up on off-the-shelf models while maintaining the output distribution. Codes will be available at https://github.com/MarkXCloud/CSpD",
            "score": 10,
            "issue_id": 674,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "17049106ecc06192",
            "authors": [
                "Zili Wang",
                "Robert Zhang",
                "Kun Ding",
                "Qi Yang",
                "Fei Li",
                "Shiming Xiang"
            ],
            "affiliations": [
                "China Tower Corporation Limited",
                "Institute of Automation, Chinese Academy of Sciences, China",
                "University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11925.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°Ğ½ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞ²ÑˆĞ¸Ğ¹ÑÑ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 2.33-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Speeding Up Image Generation with Continuous Speculative Decoding",
                    "desc": "This paper presents a new approach to improve the speed of continuous-valued autoregressive image generation models. It adapts speculative decoding, a technique previously used in large language models, to work with continuous data. The authors introduce methods to align denoising trajectories and pre-fill tokens to enhance the output quality during the decoding process. Their experiments demonstrate that this new method can significantly speed up the generation process by over two times while preserving the quality of the generated images."
                },
                "zh": {
                    "title": "åŠ é€Ÿè¿ç»­å€¼è‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ¨æµ‹è§£ç ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è¿ç»­å€¼è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨æµ‹è§£ç ç®—æ³•ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆé€Ÿåº¦ã€‚é€šè¿‡åˆ†æè¾“å‡ºåˆ†å¸ƒçš„å†…åœ¨ç‰¹æ€§ï¼Œå»ºç«‹äº†é€‚åˆæ‰©æ•£åˆ†å¸ƒçš„æ¥å—æ ‡å‡†ã€‚ä¸ºäº†è§£å†³æ¨æµ‹è§£ç è¾“å‡ºåˆ†å¸ƒçš„ä¸ä¸€è‡´æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†å»å™ªè½¨è¿¹å¯¹é½å’Œä»¤ç‰Œé¢„å¡«å……æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè¾“å‡ºåˆ†å¸ƒçš„åŒæ—¶ï¼Œå®ç°äº†2.33å€çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12044",
            "title": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements",
            "url": "https://huggingface.co/papers/2411.12044",
            "abstract": "Recent advances in foundational Vision Language Models (VLMs) have reshaped the evaluation paradigm in computer vision tasks. These foundational models, especially CLIP, have accelerated research in open-vocabulary computer vision tasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the initial results are promising, the dense prediction capabilities of VLMs still require further improvement. In this study, we enhance the semantic segmentation performance of CLIP by introducing new modules and modifications: 1) architectural changes in the last layer of ViT and the incorporation of attention maps from the middle layers with the last layer, 2) Image Engineering: applying data augmentations to enrich input image representations, and 3) using Large Language Models (LLMs) to generate definitions and synonyms for each class name to leverage CLIP's open-vocabulary capabilities. Our training-free method, ITACLIP, outperforms current state-of-the-art approaches on segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and Pascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP.",
            "score": 7,
            "issue_id": 684,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "d123699ae0dacdaa",
            "authors": [
                "M. Arda AydÄ±n",
                "Efe Mert Ã‡Ä±rpar",
                "Elvin Abdinli",
                "Gozde Unal",
                "Yusuf H. Sahin"
            ],
            "affiliations": [
                "Bilkent University",
                "Istanbul Technical University",
                "RWTH Aachen University",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12044.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ITACLIP: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¼ ÑĞ»Ğ¾Ğµ ViT Ğ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ CLIP Ğ¿Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing CLIP for Open-Vocabulary Semantic Segmentation",
                    "desc": "This paper discusses improvements to Vision Language Models (VLMs), particularly CLIP, for better performance in Open-Vocabulary Semantic Segmentation (OVSS). The authors propose enhancements through architectural modifications, including changes to the Vision Transformer (ViT) layers and the integration of attention maps. They also introduce data augmentation techniques to improve image representation and utilize Large Language Models (LLMs) to generate class definitions and synonyms, enhancing CLIP's open-vocabulary capabilities. The proposed method, ITACLIP, shows superior performance on various segmentation benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "æå‡CLIPçš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½",
                    "desc": "æœ¬ç ”ç©¶é’ˆå¯¹åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨è¿›è¡Œäº†æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯CLIPæ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥æ–°çš„æ¨¡å—å’Œä¿®æ”¹ï¼Œæå‡äº†CLIPçš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼ŒåŒ…æ‹¬å¯¹ViTæœ€åä¸€å±‚çš„æ¶æ„è°ƒæ•´å’Œä¸­é—´å±‚æ³¨æ„åŠ›å›¾çš„ç»“åˆã€‚æˆ‘ä»¬è¿˜é€šè¿‡å›¾åƒå·¥ç¨‹æŠ€æœ¯å¢å¼ºè¾“å…¥å›¾åƒçš„è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆæ¯ä¸ªç±»åˆ«åç§°çš„å®šä¹‰å’ŒåŒä¹‰è¯ï¼Œä»¥å……åˆ†åˆ©ç”¨CLIPçš„å¼€æ”¾è¯æ±‡èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ— è®­ç»ƒæ–¹æ³•ITACLIPåœ¨COCO-Stuffã€COCO-Objectã€Pascal Contextå’ŒPascal VOCç­‰åˆ†å‰²åŸºå‡†ä¸Šè¶…è¶Šäº†å½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12734",
            "title": "Soft Robotic Dynamic In-Hand Pen Spinning",
            "url": "https://huggingface.co/papers/2411.12734",
            "abstract": "Dynamic in-hand manipulation remains a challenging task for soft robotic systems that have demonstrated advantages in safe compliant interactions but struggle with high-speed dynamic tasks. In this work, we present SWIFT, a system for learning dynamic tasks using a soft and compliant robotic hand. Unlike previous works that rely on simulation, quasi-static actions and precise object models, the proposed system learns to spin a pen through trial-and-error using only real-world data without requiring explicit prior knowledge of the pen's physical attributes. With self-labeled trials sampled from the real world, the system discovers the set of pen grasping and spinning primitive parameters that enables a soft hand to spin a pen robustly and reliably. After 130 sampled actions per object, SWIFT achieves 100% success rate across three pens with different weights and weight distributions, demonstrating the system's generalizability and robustness to changes in object properties. The results highlight the potential for soft robotic end-effectors to perform dynamic tasks including rapid in-hand manipulation. We also demonstrate that SWIFT generalizes to spinning items with different shapes and weights such as a brush and a screwdriver which we spin with 10/10 and 5/10 success rates respectively. Videos, data, and code are available at https://soft-spin.github.io.",
            "score": 7,
            "issue_id": 676,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "f75a2283a0a8a06f",
            "authors": [
                "Yunchao Yao",
                "Uksang Yoo",
                "Jean Oh",
                "Christopher G. Atkeson",
                "Jeffrey Ichnowski"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.12734.jpg",
            "data": {
                "categories": [
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœÑĞ³ĞºĞ°Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ€ÑƒĞºĞ° Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ SWIFT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑĞ³ĞºĞ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ñ€ÑƒÑ‡ĞºÑƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ± Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞŸĞ¾ÑĞ»Ğµ 130 Ğ¿Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ SWIFT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 100% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¸ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ… Ñ€ÑƒÑ‡ĞµĞº Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ²ĞµÑĞ¾Ğ¼ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑÑ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ñ… Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ²ĞµÑĞ°."
                },
                "en": {
                    "title": "SWIFT: Mastering Dynamic Manipulation with Soft Robotics",
                    "desc": "This paper introduces SWIFT, a novel system designed for dynamic in-hand manipulation using a soft robotic hand. Unlike traditional methods that depend on simulations or precise object models, SWIFT learns to perform tasks like spinning a pen through real-world trial-and-error. The system effectively identifies optimal grasping and spinning parameters without needing prior knowledge of the object's characteristics. SWIFT demonstrates impressive generalizability, achieving a 100% success rate in spinning various pens and also successfully manipulating other objects like brushes and screwdrivers."
                },
                "zh": {
                    "title": "è½¯æœºå™¨äººåŠ¨æ€æ“ä½œçš„æ–°çªç ´",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSWIFTçš„ç³»ç»Ÿï¼Œç”¨äºå­¦ä¹ åŠ¨æ€ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨è½¯æœºå™¨äººæ‰‹ä¸­è¿›è¡Œå¿«é€Ÿçš„ç‰©ä½“æ“ä½œã€‚ä¸ä»¥å¾€ä¾èµ–äºæ¨¡æ‹Ÿå’Œç²¾ç¡®ç‰©ä½“æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒSWIFTé€šè¿‡çœŸå®ä¸–ç•Œçš„æ•°æ®è¿›è¡Œè¯•é”™å­¦ä¹ ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç‰©ä½“ç‰©ç†å±æ€§å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸåœ°æ—‹è½¬ç¬”ã€‚ç»è¿‡130æ¬¡é‡‡æ ·æ“ä½œï¼ŒSWIFTåœ¨ä¸‰ç§ä¸åŒé‡é‡å’Œåˆ†å¸ƒçš„ç¬”ä¸Šå®ç°äº†100%çš„æˆåŠŸç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ç‰©ä½“å±æ€§å˜åŒ–ä¸‹çš„é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚è¯¥ç³»ç»Ÿè¿˜èƒ½å¤Ÿæ¨å¹¿åˆ°å…¶ä»–å½¢çŠ¶å’Œé‡é‡çš„ç‰©ä½“ï¼Œå¦‚åˆ·å­å’Œèºä¸åˆ€ï¼Œåˆ†åˆ«å®ç°äº†10/10å’Œ5/10çš„æˆåŠŸç‡ï¼Œæ˜¾ç¤ºäº†è½¯æœºå™¨äººåœ¨åŠ¨æ€ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10161",
            "title": "SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning",
            "url": "https://huggingface.co/papers/2411.10161",
            "abstract": "Existing Image Quality Assessment (IQA) methods achieve remarkable success in analyzing quality for overall image, but few works explore quality analysis for Regions of Interest (ROIs). The quality analysis of ROIs can provide fine-grained guidance for image quality improvement and is crucial for scenarios focusing on region-level quality. This paper proposes a novel network, SEAGULL, which can SEe and Assess ROIs quality with GUidance from a Large vision-Language model. SEAGULL incorporates a vision-language model (VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and a meticulously designed Mask-based Feature Extractor (MFE) to extract global and local tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs. Moreover, this paper constructs two ROI-based IQA datasets, SEAGULL-100w and SEAGULL-3k, for training and evaluating ROI-based IQA. SEAGULL-100w comprises about 100w synthetic distortion images with 33 million ROIs for pre-training to improve the model's ability of regional quality perception, and SEAGULL-3k contains about 3k authentic distortion ROIs to enhance the model's ability to perceive real world distortions. After pre-training on SEAGULL-100w and fine-tuning on SEAGULL-3k, SEAGULL shows remarkable performance on fine-grained ROI quality assessment. Code and datasets are publicly available at the https://github.com/chencn2020/Seagull.",
            "score": 4,
            "issue_id": 684,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 15",
                "zh": "11æœˆ15æ—¥"
            },
            "hash": "f2475fbc98477ad9",
            "authors": [
                "Zewen Chen",
                "Juan Wang",
                "Wen Wang",
                "Sunhan Xu",
                "Hang Xiong",
                "Yun Zeng",
                "Jian Guo",
                "Shuxun Wang",
                "Chunfeng Yuan",
                "Bing Li",
                "Weiming Hu"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "Beijing Union University",
                "China University of Petroleum",
                "PeopleAI Inc.",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "School of Information Science and Technology, ShanghaiTech University",
                "State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10161.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#cv",
                    "#synthetic"
                ],
                "emoji": "ğŸ¦…",
                "ru": {
                    "title": "SEAGULL: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SEAGULL - Ğ½Ğ¾Ğ²ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ° (ROI). SEAGULL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°, Ğ¼Ğ°ÑĞºĞ¸ Ğ¸Ğ· Segment Anything Model Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ROI. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: SEAGULL-100w Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ SEAGULL-3k Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ SEAGULL Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ROI."
                },
                "en": {
                    "title": "SEAGULL: Fine-Grained Quality Assessment for Image Regions",
                    "desc": "This paper introduces SEAGULL, a novel network designed for assessing the quality of Regions of Interest (ROIs) in images, which is often overlooked by traditional Image Quality Assessment (IQA) methods. By leveraging a vision-language model and masks from the Segment Anything Model, SEAGULL effectively extracts both global and local features to provide detailed quality evaluations for specified ROIs. The authors also present two new datasets, SEAGULL-100w and SEAGULL-3k, which are used to train and evaluate the model, enhancing its ability to perceive both synthetic and real-world distortions. The results demonstrate that SEAGULL significantly improves fine-grained ROI quality assessment, making it a valuable tool for image quality enhancement."
                },
                "zh": {
                    "title": "ç²¾ç»†åŒ–åŒºåŸŸè´¨é‡è¯„ä¼°çš„æ–°æ–¹æ³•",
                    "desc": "ç°æœ‰çš„å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•åœ¨æ•´ä½“å›¾åƒè´¨é‡åˆ†æä¸Šå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å¯¹æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„è´¨é‡åˆ†æç ”ç©¶è¾ƒå°‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç½‘ç»œSEAGULLï¼Œèƒ½å¤Ÿé€šè¿‡å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„æŒ‡å¯¼æ¥è¯„ä¼°ROIçš„è´¨é‡ã€‚SEAGULLç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€ç”±Segment Anything Modelï¼ˆSAMï¼‰ç”Ÿæˆçš„æ©ç æ¥æŒ‡å®šROIï¼Œä»¥åŠç²¾å¿ƒè®¾è®¡çš„åŸºäºæ©ç çš„ç‰¹å¾æå–å™¨ï¼ˆMFEï¼‰ï¼Œå®ç°äº†å¯¹ROIçš„ç²¾ç»†è´¨é‡è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸¤ä¸ªåŸºäºROIçš„è´¨é‡è¯„ä¼°æ•°æ®é›†SEAGULL-100wå’ŒSEAGULL-3kï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°ROIçš„è´¨é‡è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12275",
            "title": "Building Trust: Foundations of Security, Safety and Transparency in AI",
            "url": "https://huggingface.co/papers/2411.12275",
            "abstract": "This paper explores the rapidly evolving ecosystem of publicly available AI models, and their potential implications on the security and safety landscape. As AI models become increasingly prevalent, understanding their potential risks and vulnerabilities is crucial. We review the current security and safety scenarios while highlighting challenges such as tracking issues, remediation, and the apparent absence of AI model lifecycle and ownership processes. Comprehensive strategies to enhance security and safety for both model developers and end-users are proposed. This paper aims to provide some of the foundational pieces for more standardized security, safety, and transparency in the development and operation of AI models and the larger open ecosystems and communities forming around them.",
            "score": 4,
            "issue_id": 682,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "628941b4647bf155",
            "authors": [
                "Huzaifa Sidhpurwala",
                "Garth Mollett",
                "Emily Fox",
                "Mark Bestavros",
                "Huamin Chen"
            ],
            "affiliations": [
                "Red Hat"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12275.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#security"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜: Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ¦ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ - Ğ·Ğ°Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Securing the Future of Open AI Models",
                    "desc": "This paper examines the growing availability of AI models and their impact on security and safety. It emphasizes the importance of identifying risks and vulnerabilities associated with these models as they become more common. The authors discuss challenges like tracking model usage, addressing security issues, and the lack of clear ownership and lifecycle management for AI models. They propose strategies to improve security and safety for developers and users, aiming to establish standards for transparency in AI model development and operation."
                },
                "zh": {
                    "title": "æå‡äººå·¥æ™ºèƒ½æ¨¡å‹çš„å®‰å…¨ä¸é€æ˜æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å…¬å¼€å¯ç”¨çš„äººå·¥æ™ºèƒ½æ¨¡å‹å¿«é€Ÿå‘å±•çš„ç”Ÿæ€ç³»ç»ŸåŠå…¶å¯¹å®‰å…¨å’Œå®‰å…¨æ€§å½±å“çš„æ½œåœ¨å«ä¹‰ã€‚éšç€äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ™®åŠï¼Œç†è§£å…¶æ½œåœ¨é£é™©å’Œè„†å¼±æ€§å˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å›é¡¾äº†å½“å‰çš„å®‰å…¨å’Œå®‰å…¨åœºæ™¯ï¼Œå¹¶å¼ºè°ƒäº†è·Ÿè¸ªé—®é¢˜ã€ä¿®å¤æªæ–½ä»¥åŠäººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿå‘½å‘¨æœŸå’Œæ‰€æœ‰æƒæµç¨‹ç¼ºå¤±ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†å¢å¼ºæ¨¡å‹å¼€å‘è€…å’Œæœ€ç»ˆç”¨æˆ·å®‰å…¨ä¸å®‰å…¨æ€§çš„ç»¼åˆç­–ç•¥ï¼Œæ—¨åœ¨ä¸ºäººå·¥æ™ºèƒ½æ¨¡å‹åŠå…¶å‘¨å›´å¼€æ”¾ç”Ÿæ€ç³»ç»Ÿå’Œç¤¾åŒºçš„å‘å±•æä¾›æ›´æ ‡å‡†åŒ–çš„å®‰å…¨æ€§ã€é€æ˜æ€§å’Œå®‰å…¨æ€§çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10818",
            "title": "FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations",
            "url": "https://huggingface.co/papers/2411.10818",
            "abstract": "Sketch animations offer a powerful medium for visual storytelling, from simple flip-book doodles to professional studio productions. While traditional animation requires teams of skilled artists to draw key frames and in-between frames, existing automation attempts still demand significant artistic effort through precise motion paths or keyframe specification. We present FlipSketch, a system that brings back the magic of flip-book animation -- just draw your idea and describe how you want it to move! Our approach harnesses motion priors from text-to-video diffusion models, adapting them to generate sketch animations through three key innovations: (i) fine-tuning for sketch-style frame generation, (ii) a reference frame mechanism that preserves visual integrity of input sketch through noise refinement, and (iii) a dual-attention composition that enables fluid motion without losing visual consistency. Unlike constrained vector animations, our raster frames support dynamic sketch transformations, capturing the expressive freedom of traditional animation. The result is an intuitive system that makes sketch animation as simple as doodling and describing, while maintaining the artistic essence of hand-drawn animation.",
            "score": 3,
            "issue_id": 687,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 16",
                "zh": "11æœˆ16æ—¥"
            },
            "hash": "2aee0f98e4694b74",
            "authors": [
                "Hmrishav Bandyopadhyay",
                "Yi-Zhe Song"
            ],
            "affiliations": [
                "SketchX, CVSSP, University of Surrey, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10818.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#cv",
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "ĞĞ¶Ğ¸Ğ²Ğ»ÑĞµĞ¼ ÑĞºĞµÑ‚Ñ‡Ğ¸ ÑĞ¸Ğ»Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ Ğ˜Ğ˜",
                    "desc": "FlipSketch - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºĞµÑ‚Ñ‡ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ° Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑĞºĞµÑ‚Ñ‡Ğ°Ñ…, Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğº Ğ¶Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾, ĞºĞ°Ğº Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ´ĞµÑ."
                },
                "en": {
                    "title": "Doodle Your Dreams: Effortless Sketch Animation with FlipSketch!",
                    "desc": "FlipSketch is a novel system that simplifies the process of creating sketch animations by allowing users to draw their ideas and describe desired movements. It leverages motion priors from text-to-video diffusion models, which are fine-tuned for generating sketch-style frames. The system incorporates a reference frame mechanism to ensure the visual integrity of the sketches and employs dual-attention composition for smooth motion while preserving consistency. This approach enables dynamic transformations in raster frames, making sketch animation accessible and intuitive, akin to traditional hand-drawn techniques."
                },
                "zh": {
                    "title": "è®©è‰å›¾åŠ¨ç”»åˆ›ä½œå˜å¾—ç®€å•å¦‚æ¶‚é¸¦",
                    "desc": "FlipSketch æ˜¯ä¸€ä¸ªæ–°ç³»ç»Ÿï¼Œæ—¨åœ¨ç®€åŒ–è‰å›¾åŠ¨ç”»çš„åˆ¶ä½œè¿‡ç¨‹ã€‚ç”¨æˆ·åªéœ€ç»˜åˆ¶è‰å›¾å¹¶æè¿°åŠ¨ç”»çš„è¿åŠ¨æ–¹å¼ï¼Œæ— éœ€å¤æ‚çš„å…³é”®å¸§è®¾ç½®ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿åŠ¨å…ˆéªŒï¼Œé€šè¿‡ä¸‰é¡¹åˆ›æ–°æ¥ç”Ÿæˆè‰å›¾åŠ¨ç”»ã€‚æœ€ç»ˆï¼ŒFlipSketch ä½¿å¾—è‰å›¾åŠ¨ç”»çš„åˆ›ä½œå˜å¾—åƒæ¶‚é¸¦ä¸€æ ·ç®€å•ï¼ŒåŒæ—¶ä¿ç•™äº†æ‰‹ç»˜åŠ¨ç”»çš„è‰ºæœ¯æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12240",
            "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
            "url": "https://huggingface.co/papers/2411.12240",
            "abstract": "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer's superior handling of Indic languages, GPT-4o's advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.",
            "score": 2,
            "issue_id": 675,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "aee934b73b340b71",
            "authors": [
                "S. Tamang",
                "D. J. Bora"
            ],
            "affiliations": [
                "Department of IT The Assam Kaziranga University Jorhat, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12240.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ‡®ğŸ‡³",
                "ru": {
                    "title": "SUTRA: Ğ›Ğ¸Ğ´ĞµÑ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² 12 Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ´Ğ»Ñ 22 Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ˜Ğ½Ğ´Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (NSL) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ SUTRA Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ 14 ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸."
                },
                "en": {
                    "title": "Optimizing Tokenization for Multilingual Mastery",
                    "desc": "This paper evaluates the effectiveness of tokenizers used in Large Language Models (LLMs) for all 22 official languages of India, emphasizing the importance of tokenization in multilingual contexts. The study introduces the Normalized Sequence Length (NSL) as a metric to assess the efficiency of different tokenizers. Results indicate that the SUTRA tokenizer significantly outperforms other models, particularly in handling Indic languages. The findings highlight the need for specialized tokenization strategies to improve the performance of LLMs in diverse linguistic settings."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å¤šè¯­è¨€æ¨¡å‹çš„åˆ†è¯ç­–ç•¥",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºå˜æ¢å™¨æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åˆ†è¯æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å°åº¦å®˜æ–¹è¯­è¨€ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å¯¹12ç§LLMsä½¿ç”¨çš„åˆ†è¯å™¨è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œé‡ç‚¹æ¯”è¾ƒäº†å®ƒä»¬çš„åˆ†è¯æ•ˆç‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒSUTRAåˆ†è¯å™¨åœ¨14ç§è¯­è¨€ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å°åº¦è¯­è¨€æ–¹é¢ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¸ºå¤šè¯­è¨€å’Œä»¥å°åº¦è¯­è¨€ä¸ºä¸­å¿ƒçš„æ¨¡å‹å¼€å‘é’ˆå¯¹æ€§åˆ†è¯ç­–ç•¥çš„é‡è¦æ€§ï¼Œä»¥æé«˜è¯­è¨€è¦†ç›–ç‡å’Œæ¨¡å‹æ•ˆç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-19.html",
    "link_next": "2024-11-21.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "19.11",
        "en": "11/19",
        "zh": "11æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "21.11",
        "en": "11/21",
        "zh": "11æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿ç»­å€¼è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹å±•ç¤ºäº†æ¯”ç¦»æ•£ç¬¦å·æ¨¡å‹æ›´é«˜çš„é‡å»ºè´¨é‡å’Œç”Ÿæˆä¿çœŸåº¦ã€‚ç„¶è€Œï¼Œè‡ªå›å½’æ¡†æ¶çš„è®¡ç®—éœ€æ±‚å¯¼è‡´æ˜¾è‘—çš„æ¨ç†å¼€é”€ã€‚è™½ç„¶æ¨æµ‹è§£ç å·²è¢«è¯æ˜æœ‰æ•ˆåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½†å…¶åº”ç”¨äºè¿ç»­å€¼è§†è§‰è‡ªå›å½’æ¨¡å‹ä»æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡å°†æ¨æµ‹è§£ç ç®—æ³•ä»ç¦»æ•£ç¬¦å·æ¨å¹¿åˆ°è¿ç»­ç©ºé—´ã€‚é€šè¿‡åˆ†æè¾“å‡ºåˆ†å¸ƒçš„å†…åœ¨ç‰¹æ€§ï¼Œæˆ‘ä»¬ä¸ºè¿™äº›æ¨¡å‹ä¸­å¸¸è§çš„æ‰©æ•£åˆ†å¸ƒå»ºç«‹äº†å®šåˆ¶çš„æ¥å—æ ‡å‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„è¿ç»­æ¨æµ‹è§£ç åœ¨ç»´æŒè¾“å‡ºåˆ†å¸ƒçš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„2.33å€åŠ é€Ÿã€‚ä»£ç å°†åœ¨https://github.com/MarkXCloud/CSpDä¸Šæä¾›ã€‚",
        "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
        "pinyin": "è¿ç»­å€¼è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹å±•ç¤ºäº†æ¯”ç¦»æ•£ç¬¦å·æ¨¡å‹æ›´é«˜çš„é‡å»ºè´¨é‡å’Œç”Ÿæˆä¿çœŸåº¦ã€‚ç„¶è€Œï¼Œè‡ªå›å½’æ¡†æ¶çš„è®¡ç®—éœ€æ±‚å¯¼è‡´æ˜¾è‘—çš„æ¨ç†å¼€é”€ã€‚è™½ç„¶æ¨æµ‹è§£ç å·²è¢«è¯æ˜æœ‰æ•ˆåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½†å…¶åº”ç”¨äºè¿ç»­å€¼è§†è§‰è‡ªå›å½’æ¨¡å‹ä»æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡å°†æ¨æµ‹è§£ç ç®—æ³•ä»ç¦»æ•£ç¬¦å·æ¨å¹¿åˆ°è¿ç»­ç©ºé—´ã€‚é€šè¿‡åˆ†æè¾“å‡ºåˆ†å¸ƒçš„å†…åœ¨ç‰¹æ€§ï¼Œæˆ‘ä»¬ä¸ºè¿™äº›æ¨¡å‹ä¸­å¸¸è§çš„æ‰©æ•£åˆ†å¸ƒå»ºç«‹äº†å®šåˆ¶çš„æ¥å—æ ‡å‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„è¿ç»­æ¨æµ‹è§£ç åœ¨ç»´æŒè¾“å‡ºåˆ†å¸ƒçš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„2.33å€åŠ é€Ÿã€‚ä»£ç å°†åœ¨https://github.com/MarkXCloud/CSpDä¸Šæä¾›ã€‚\n\nliÃ¡n xÃ¹ zhÃ­ zÃ¬ huÃ­ guÄ« (AR) tÃº xiÃ ng shÄ“ng chÃ©ng mÃ³ xÃ­ng zhÃ n shÃ¬ le bÇ lÃ­ sÃ n fÃº hÃ o mÃ³ xÃ­ng gÃ¨ng gÄo de chÃ³ng jiÃ n zhÃ¬ liÃ ng hÃ© shÄ“ng chÃ©ng bÇo zhÄ“n dÃ¹. rÃ¡n Ã©r, zÃ¬ huÃ­ guÄ« kuÃ ng jiÃ  de jÃ¬ suÇn xÅ« qiÃº dÇo zhÃ¬ xiÇn zhÃ¹ de tuÄ« lÇ kÄi xiÄo. suÄ« rÃ¡n, tuÄ« cÃ¨ jiÄ› mÇ yÇ bÃ¨i zhÃ¨ng mÃ­ng yÇ’u xiÃ o jÃ­ sÃ¹ dÃ  xÃ­ng yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs), dÃ n qÃ­ yÃ¬ng yÃ²ng yÃº liÃ¡n xÃ¹ zhÃ­ shÃ¬ jÃ¹ shÃ­ zÃ¬ huÃ­ guÄ« mÃ³ xÃ­ng rÃ©ng wÃ¨i bÃ¨i tuÃ n suÇ’. bÄ›n wÃ©n jiÄng tuÄ« cÃ¨ jiÄ› mÇ suÃ n fÇ cÃ³ng lÃ­ sÃ n fÃº hÃ o tuÄ« guÇng dÃ o liÃ¡n xÃ¹ kÅng jiÄn. tÅng guÃ² fÄ“n xÄ« chÅ« zhÃ¬ fÄ“n bÃ¹ de nÃ¨i zÃ i tÃ¨ xÃ¬ng, wÇ’ men wÃ¨i zhÃ¨ xiÄ“ mÃ³ xÃ­ng zhÅng chÃ¡ng jiÃ n de kuÃ² sÃ n fÄ“n bÃ¹ jiÃ n lÃ¬ le dÃ¬ng zhÃ¬ de jiÄ“ shÃ²u biÄo zhÇ”n. shÃ­ yÃ n jiÃ© guÇ’ xiÇn shÃ¬, wÇ’ men de liÃ¡n xÃ¹ tuÄ« cÃ¨ jiÄ› mÇ zÃ i wÃ©i chÃ­ chÅ« zhÃ¬ fÄ“n bÃ¹ de tÃ³ng shÃ­, shÃ­ xiÃ n le xiÇn zhÃ¹ de 2.33 bÃ¨i jÃ­ sÃ¹. dÃ i mÇ jiÄng zÃ i https://github.com/MarkXCloud/CSpD shÃ ng tÃ­ gÅng.",
        "vocab": "[{'word': 'è¿ç»­å€¼', 'pinyin': 'liÃ¡n xÃ¹ zhÃ­', 'trans': 'continuous value'},\n{'word': 'è‡ªå›å½’', 'pinyin': 'zÃ¬ huÃ­ guÄ«', 'trans': 'autoregressive'},\n{'word': 'å±•ç¤º', 'pinyin': 'zhÇn shÃ¬', 'trans': 'demonstrate'},\n{'word': 'ç¦»æ•£', 'pinyin': 'lÃ­ sÃ n', 'trans': 'discrete'},\n{'word': 'ç¬¦å·', 'pinyin': 'fÃº hÃ o', 'trans': 'symbol'},\n{'word': 'é‡å»º', 'pinyin': 'chÃ³ng jiÃ n', 'trans': 'reconstruction'},\n{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generation'},\n{'word': 'ä¿çœŸåº¦', 'pinyin': 'bÇo zhÄ“n dÃ¹', 'trans': 'fidelity'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'è®¡ç®—', 'pinyin': 'jÃ¬ suÃ n', 'trans': 'computation'},\n{'word': 'éœ€æ±‚', 'pinyin': 'xÅ« qiÃº', 'trans': 'demand'},\n{'word': 'å¯¼è‡´', 'pinyin': 'dÇo zhÃ¬', 'trans': 'result in'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'},\n{'word': 'å¼€é”€', 'pinyin': 'kÄi xiÄo', 'trans': 'cost'},\n{'word': 'æ¨æµ‹', 'pinyin': 'tuÄ« cÃ¨', 'trans': 'speculative'},\n{'word': 'è§£ç ', 'pinyin': 'jiÄ› mÇ', 'trans': 'decoding'},\n{'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'accelerate'},\n{'word': 'è¯­è¨€', 'pinyin': 'yÇ” yÃ¡n', 'trans': 'language'},\n{'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'},\n{'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ng yÃ²ng', 'trans': 'application'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},\n{'word': 'æ¨å¹¿', 'pinyin': 'tuÄ« guÇng', 'trans': 'extend'},\n{'word': 'ç©ºé—´', 'pinyin': 'kÅng jiÄn', 'trans': 'space'},\n{'word': 'åˆ†æ', 'pinyin': 'fÄ“n xÄ«', 'trans': 'analysis'},\n{'word': 'è¾“å‡º', 'pinyin': 'shÅ« chÅ«', 'trans': 'output'},\n{'word': 'åˆ†å¸ƒ', 'pinyin': 'fÄ“n bÃ¹', 'trans': 'distribution'},\n{'word': 'å†…åœ¨', 'pinyin': 'nÃ¨i zÃ i', 'trans': 'intrinsic'},\n{'word': 'ç‰¹æ€§', 'pinyin': 'tÃ¨ xÃ¬ng', 'trans': 'characteristic'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'},\n{'word': 'å»ºç«‹', 'pinyin': 'jiÃ n lÃ¬', 'trans': 'establish'},\n{'word': 'å®šåˆ¶', 'pinyin': 'dÃ¬ng zhÃ¬', 'trans': 'custom'},\n{'word': 'æ¥å—', 'pinyin': 'jiÄ“ shÃ²u', 'trans': 'acceptance'},\n{'word': 'æ ‡å‡†', 'pinyin': 'biÄo zhÇ”n', 'trans': 'standard'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'},\n{'word': 'ç»´æŒ', 'pinyin': 'wÃ©i chÃ­', 'trans': 'maintain'},\n{'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'},\n{'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'},\n{'word': 'æä¾›', 'pinyin': 'tÃ­ gÅng', 'trans': 'provide'}]",
        "trans": "Continuous-valued autoregressive (AR) image generation models have demonstrated higher reconstruction quality and generative fidelity compared to discrete symbol models. However, the computational requirements of the autoregressive framework result in significant inference overhead. While speculative decoding has been proven effective in accelerating large language models (LLMs), its application to continuous-valued visual autoregressive models has not been explored. This paper extends the speculative decoding algorithm from discrete symbols to continuous space. By analyzing the intrinsic properties of the output distribution, we establish custom acceptance criteria for the diffusion distributions commonly found in these models. Experimental results show that our continuous speculative decoding achieves a significant 2.33-fold speedup while maintaining the output distribution. The code will be available at https://github.com/MarkXCloud/CSpD.",
        "update_ts": "2024-11-20 09:11"
    }
}