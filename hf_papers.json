{
    "date": {
        "ru": "22 июля",
        "en": "July 22",
        "zh": "7月22日"
    },
    "time_utc": "2025-07-22 02:57",
    "weekday": 1,
    "issue_id": 4936,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.15061",
            "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization",
            "url": "https://huggingface.co/papers/2507.15061",
            "abstract": "A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.",
            "score": 12,
            "issue_id": 4936,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 июля",
                "en": "July 20",
                "zh": "7月20日"
            },
            "hash": "16ab84cfe7ace89e",
            "authors": [
                "Zhengwei Tao",
                "Jialong Wu",
                "Wenbiao Yin",
                "Junkai Zhang",
                "Baixuan Li",
                "Haiyang Shen",
                "Kuan Li",
                "Liwen Zhang",
                "Xinyu Wang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15061.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#synthetic",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Формализация для синтеза данных: новый подход к обучению ИИ-агентов поиску информации",
                    "desc": "WebShaper - это фреймворк для синтеза наборов данных для задач поиска информации, основанный на формализации с использованием теории множеств и Проекций Знаний. Он позволяет улучшить производительность агентов на основе больших языковых моделей (LLM) в открытых задачах. WebShaper систематически формализует задачи поиска информации и использует многоэтапный процесс расширения для создания сложных вопросов. Эксперименты показывают, что WebShaper достигает наилучших результатов среди открытых агентов поиска информации на бенчмарках GAIA и WebWalkerQA."
                },
                "en": {
                    "title": "Enhancing LLM Agents with Structured Data Synthesis",
                    "desc": "WebShaper is a framework designed to improve information-seeking datasets for Large Language Model (LLM)-powered agents. It uses set theory and a method called Knowledge Projections to create a structured approach for synthesizing data. This helps ensure that the reasoning behind questions and answers is consistent and logical. Experiments show that WebShaper significantly enhances the performance of these agents on various benchmarks."
                },
                "zh": {
                    "title": "WebShaper：提升信息检索智能体性能的创新框架",
                    "desc": "WebShaper是一个基于形式化驱动的框架，利用集合论和知识投影技术合成信息检索数据集，从而提升大型语言模型（LLM）驱动的智能体在开放式任务中的表现。该框架通过系统化的形式化过程，确保信息结构与推理结构的一致性，解决了现有方法中常见的数据不一致问题。WebShaper的核心是知识投影（KP）概念，通过KP操作组合实现对推理结构的精确控制。实验结果表明，WebShaper在GAIA和WebWalkerQA基准测试中，达到了开源信息检索智能体的最先进性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15846",
            "title": "GUI-G^2: Gaussian Reward Modeling for GUI Grounding",
            "url": "https://huggingface.co/papers/2507.15846",
            "abstract": "Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G^2), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G^2, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.",
            "score": 5,
            "issue_id": 4936,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "d36bacfa3f66add9",
            "authors": [
                "Fei Tang",
                "Zhangxuan Gu",
                "Zhengxi Lu",
                "Xuyang Liu",
                "Shuheng Shen",
                "Changhua Meng",
                "Wen Wang",
                "Wenqi Zhang",
                "Yongliang Shen",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15846.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🖱️",
                "ru": {
                    "title": "Гауссово моделирование для точного взаимодействия с GUI",
                    "desc": "Статья представляет новый подход к обучению моделей машинного обучения для взаимодействия с графическим пользовательским интерфейсом (GUI). Авторы предлагают метод GUI Gaussian Grounding Rewards (GUI-G^2), который моделирует элементы интерфейса как непрерывные гауссовы распределения на плоскости интерфейса. Этот метод включает в себя гауссовы точечные награды для точной локализации и награды за покрытие для оценки пространственного выравнивания. Эксперименты показывают, что GUI-G^2 значительно превосходит современные методы на нескольких бенчмарках, демонстрируя улучшение до 24.7% на ScreenSpot-Pro."
                },
                "en": {
                    "title": "Revolutionizing GUI Interaction with Continuous Gaussian Rewards",
                    "desc": "This paper presents a new method called GUI Gaussian Grounding Rewards (GUI-G^2) for improving how machines interact with graphical user interfaces (GUIs) using natural language instructions. Unlike traditional reinforcement learning methods that use simple binary rewards, GUI-G^2 models GUI elements as continuous Gaussian distributions, allowing for more nuanced and effective learning. The framework includes mechanisms for precise localization and spatial alignment, which help the model understand where to click based on human-like behavior. Experiments show that GUI-G^2 significantly outperforms existing methods, demonstrating better adaptability to different interface designs and improved overall performance in GUI tasks."
                },
                "zh": {
                    "title": "高斯奖励框架提升GUI交互精度",
                    "desc": "本论文提出了一种新的奖励框架，称为GUI Gaussian Grounding Rewards（GUI-G^2），用于将自然语言指令映射到图形用户界面（GUI）的精确位置。与传统的二元奖励方法不同，GUI-G^2通过将GUI元素建模为连续的高斯分布，提供了更丰富的梯度信号，促进了模型的优化。该框架结合了高斯点奖励和覆盖奖励，能够更好地处理不同元素的尺度，并提高了模型在界面变化中的鲁棒性。实验结果表明，GUI-G^2在多个基准测试中显著优于现有的最先进方法，展示了其在GUI交互任务中的新范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15778",
            "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR",
            "url": "https://huggingface.co/papers/2507.15778",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR.",
            "score": 4,
            "issue_id": 4936,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "8e0f7bdfedf50691",
            "authors": [
                "Jiakang Wang",
                "Runze Liu",
                "Fuzheng Zhang",
                "Xiu Li",
                "Guorui Zhou"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15778.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точное обучение с подкреплением: улучшение рассуждений ИИ с помощью энтропийно-адаптивного подхода",
                    "desc": "Статья представляет новый метод обучения с подкреплением для улучшения рассуждающих способностей больших языковых моделей, называемый Archer. Этот подход учитывает энтропию токенов и применяет различные ограничения к токенам знаний и рассуждений. Archer использует более слабую KL-регуляризацию и более высокие пороги отсечения для токенов рассуждений, чтобы стимулировать исследование, сохраняя при этом фактические знания. Экспериментальные результаты показывают, что Archer превосходит предыдущие методы RLVR на нескольких бенчмарках математических рассуждений и генерации кода."
                },
                "en": {
                    "title": "Archer: Smart Token Training for Better Reasoning in LLMs",
                    "desc": "This paper introduces Archer, a new method for Reinforcement Learning with Verifiable Rewards (RLVR) that enhances the reasoning capabilities of Large Language Models (LLMs). Unlike previous methods that treat all tokens equally, Archer distinguishes between low-entropy knowledge tokens and high-entropy reasoning tokens, applying different training strategies to each. By using weaker KL regularization for reasoning tokens, Archer promotes exploration while enforcing stronger constraints on knowledge tokens to preserve factual accuracy. The results demonstrate that Archer significantly improves performance on mathematical reasoning and code generation tasks, achieving state-of-the-art results for models of similar size."
                },
                "zh": {
                    "title": "提升推理能力的双重令牌强化学习",
                    "desc": "本文提出了一种新的强化学习方法，称为Archer，旨在提高大型语言模型的推理能力。Archer通过双重令牌约束和同步更新，分别对知识相关的低熵令牌和推理相关的高熵令牌施加不同的训练信号。与以往的算法不同，Archer在推理令牌上使用较弱的KL正则化，以鼓励探索，同时对知识令牌施加更强的约束，以保持事实知识的准确性。实验结果表明，Archer在多个数学推理和代码生成基准测试中显著优于之前的RLVR方法。"
                }
            }
        }
    ],
    "link_prev": "2025-07-21.html",
    "link_next": "2025-07-23.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "21.07",
        "en": "07/21",
        "zh": "7月21日"
    },
    "short_date_next": {
        "ru": "23.07",
        "en": "07/23",
        "zh": "7月23日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}