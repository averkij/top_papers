{
    "date": {
        "ru": "13 ноября",
        "en": "November 13",
        "zh": "11月13日"
    },
    "time_utc": "2024-11-13 04:12",
    "weekday": 2,
    "issue_id": 541,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07184",
            "title": "SAMPart3D: Segment Any Part in 3D Objects",
            "url": "https://huggingface.co/papers/2411.07184",
            "abstract": "3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.",
            "score": 10,
            "issue_id": 541,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 ноября",
                "en": "November 11",
                "zh": "11月11日"
            },
            "hash": "b4e58a99e4a7e86c",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "SAMPart3D: гибкая сегментация 3D-объектов без предварительного обучения",
                    "desc": "Статья представляет SAMPart3D - масштабируемый фреймворк для сегментации частей 3D-объектов без предварительного обучения. Авторы используют безтекстовые модели компьютерного зрения для извлечения признаков из 3D-данных, что позволяет обучаться на больших наборах неразмеченных 3D-объектов. Метод способен сегментировать объекты на части с разной степенью детализации, а затем присваивать семантические метки с помощью мультимодальных языковых моделей. SAMPart3D превосходит существующие методы и может применяться для редактирования и интерактивной сегментации 3D-объектов."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with SAMPart3D",
                    "desc": "This paper presents SAMPart3D, a novel framework for zero-shot 3D part segmentation that does not depend on predefined text prompts. It utilizes text-agnostic vision foundation models to extract 3D features, enabling it to scale effectively to large unlabeled datasets. The framework also incorporates scale-conditioned part-aware features, allowing for segmentation at various levels of detail. SAMPart3D outperforms existing methods and introduces a new benchmark to enhance the diversity and complexity of 3D part segmentation tasks."
                },
                "zh": {
                    "title": "SAMPart3D：无文本提示的3D部件分割新框架",
                    "desc": "3D部件分割是3D感知中的一项重要且具有挑战性的任务，广泛应用于机器人技术、3D生成和3D编辑等领域。本文提出了SAMPart3D框架，它能够在不依赖预定义文本提示的情况下，对任意3D对象进行多粒度的语义部件分割。该框架利用无文本依赖的视觉基础模型，从大规模未标记的3D数据集中提取丰富的3D特征，并通过条件化的部件感知特征实现灵活的分割。实验结果表明，SAMPart3D在处理复杂对象时显著优于现有的零样本3D部件分割方法，并能支持多种应用，如部件级编辑和交互式分割。"
                }
            }
        }
    ],
    "link_prev": "2024-11-12.html",
    "link_next": "2024-11-14.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11月12日"
    },
    "short_date_next": {
        "ru": "14.11",
        "en": "11/14",
        "zh": "11月14日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种名为Add-it的新方法，用于在图像中根据文本指令添加物体。Add-it利用扩展的注意力机制，结合场景图像、文本提示和生成图像本身的信息。这种方法在不需要任务特定的微调情况下，保持结构一致性和细节，并确保物体自然放置。Add-it在真实和生成图像插入基准上取得了最先进的结果，并在人类评估中胜出超过80%的案例。",
        "title": "Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models",
        "pinyin": "zhè piān wén zhāng jiè shào le yī zhǒng míng wèi Add-it de xīn fāng fǎ, yòng yú zài tú xiàng zhōng gēn jù wén běn zhǐ lǐng tiān jiā wù tǐ. Add-it lì yòng kuò zhǎn de zhù yì jī zhì, jié hé chǎng jǐng tú xiàng, wén běn tí shì hé shēng chéng tú xiàng běn shēn de xìn xī. zhè zhǒng fāng fǎ zài bù xū yào rèn wù tè dìng de wēi tiáo qíng kuàng xià, bǎo chí jiē gòu yī zhì xìng hé xì jié, bìng què shí Add-it zài zhēn shí hé shēng chéng tú xiàng chā rù jī zhǔn shàng quǎn dé le zuì xiān jìn de jié guǒ, bìng zài rén lèi píng guā zhōng shèng chū chāo guò 80% de àn lì.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jièshào\", \"trans\": \"introduce\"},\n    {\"word\": \"名为\", \"pinyin\": \"míngwéi\", \"trans\": \"named\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"根据\", \"pinyin\": \"gēnjù\", \"trans\": \"according to\"},\n    {\"word\": \"指令\", \"pinyin\": \"zhǐlìng\", \"trans\": \"instruction\"},\n    {\"word\": \"添加\", \"pinyin\": \"tiānjiā\", \"trans\": \"add\"},\n    {\"word\": \"物体\", \"pinyin\": \"wùtǐ\", \"trans\": \"object\"},\n    {\"word\": \"利用\", \"pinyin\": \"lìyòng\", \"trans\": \"utilize\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuòzhǎn\", \"trans\": \"extend\"},\n    {\"word\": \"注意力\", \"pinyin\": \"zhùyìlì\", \"trans\": \"attention\"},\n    {\"word\": \"机制\", \"pinyin\": \"jīzhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"结合\", \"pinyin\": \"jiéhé\", \"trans\": \"combine\"},\n    {\"word\": \"场景\", \"pinyin\": \"chǎngjǐng\", \"trans\": \"scene\"},\n    {\"word\": \"提示\", \"pinyin\": \"tíshì\", \"trans\": \"prompt\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēngchéng\", \"trans\": \"generate\"},\n    {\"word\": \"本身\", \"pinyin\": \"běnshēn\", \"trans\": \"itself\"},\n    {\"word\": \"信息\", \"pinyin\": \"xìnxī\", \"trans\": \"information\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"特定\", \"pinyin\": \"tèdìng\", \"trans\": \"specific\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēitiáo\", \"trans\": \"fine-tune\"},\n    {\"word\": \"情况\", \"pinyin\": \"qíngkuàng\", \"trans\": \"situation\"},\n    {\"word\": \"保持\", \"pinyin\": \"bǎochí\", \"trans\": \"maintain\"},\n    {\"word\": \"一致性\", \"pinyin\": \"yīzhìxìng\", \"trans\": \"consistency\"},\n    {\"word\": \"细节\", \"pinyin\": \"xìjié\", \"trans\": \"detail\"},\n    {\"word\": \"确保\", \"pinyin\": \"quèbǎo\", \"trans\": \"ensure\"},\n    {\"word\": \"自然\", \"pinyin\": \"zìrán\", \"trans\": \"natural\"},\n    {\"word\": \"放置\", \"pinyin\": \"fàngzhì\", \"trans\": \"place\"},\n    {\"word\": \"真实\", \"pinyin\": \"zhēnshí\", \"trans\": \"real\"},\n    {\"word\": \"插入\", \"pinyin\": \"chārù\", \"trans\": \"insert\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"取得\", \"pinyin\": \"qǔdé\", \"trans\": \"achieve\"},\n    {\"word\": \"最先进\", \"pinyin\": \"zuìxiānjìn\", \"trans\": \"state-of-the-art\"},\n    {\"word\": \"结果\", \"pinyin\": \"jiéguǒ\", \"trans\": \"result\"},\n    {\"word\": \"人类\", \"pinyin\": \"rénlèi\", \"trans\": \"human\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluation\"},\n    {\"word\": \"胜出\", \"pinyin\": \"shèngchū\", \"trans\": \"win\"},\n    {\"word\": \"超过\", \"pinyin\": \"chāoguò\", \"trans\": \"exceed\"},\n    {\"word\": \"案例\", \"pinyin\": \"ànlì\", \"trans\": \"case\"}\n]",
        "trans": "This article introduces a new method called Add-it for adding objects to images based on textual instructions. Add-it utilizes an extended attention mechanism, combining information from the scene image, textual prompts, and the generated image itself. This method maintains structural consistency and detail without requiring task-specific fine-tuning, ensuring that objects are naturally placed. Add-it achieves state-of-the-art results in both real and generated image insertion benchmarks and outperforms in over 80% of cases in human evaluations.",
        "update_ts": "2024-11-12 09:51"
    }
}