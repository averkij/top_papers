{
    "date": {
        "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 7",
        "zh": "11æœˆ7æ—¥"
    },
    "time_utc": "2024-11-07 14:11",
    "weekday": 3,
    "issue_id": 458,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.03562",
            "title": "Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level",
            "url": "https://huggingface.co/papers/2411.03562",
            "abstract": "We introduce Agent K v1.0, an end-to-end autonomous data science agent designed to automate, optimise, and generalise across diverse data science tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle by learning from experience. It leverages a highly flexible structured reasoning framework to enable it to dynamically process memory in a nested structure, effectively learning from accumulated experience stored to handle complex reasoning tasks. It optimises long- and short-term memory by selectively storing and retrieving key information, guiding future decisions based on environmental rewards. This iterative approach allows it to refine decisions without fine-tuning or backpropagation, achieving continuous improvement through experiential learning. We evaluate our agent's apabilities using Kaggle competitions as a case study. Following a fully automated protocol, Agent K v1.0 systematically addresses complex and multimodal data science tasks, employing Bayesian optimisation for hyperparameter tuning and feature engineering. Our new evaluation framework rigorously assesses Agent K v1.0's end-to-end capabilities to generate and send submissions starting from a Kaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\\% success rate across tasks, spanning tabular, computer vision, NLP, and multimodal domains. When benchmarking against 5,856 human Kaggle competitors by calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\\%, demonstrating an overall skill level comparable to Expert-level users. Notably, its Elo-MMR score falls between the first and third quartiles of scores achieved by human Grandmasters. Furthermore, our results indicate that Agent K v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a record of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's progression system.",
            "score": 21,
            "issue_id": 457,
            "pub_date": "2024-11-05",
            "pub_date_card": {
                "ru": "5 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 5",
                "zh": "11æœˆ5æ—¥"
            },
            "hash": "1db584382b826315",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Agent K v1.0: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Agent K v1.0 - Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. Agent K v1.0 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ°ÑÑŒ Ğ½Ğ° ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Kaggle, Ğ³Ğ´Ğµ Ğ¾Ğ½ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» 92.5% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Agent K v1.0: Your Autonomous Data Science Expert!",
                    "desc": "Agent K v1.0 is an autonomous data science agent that automates the entire data science life cycle, learning from its experiences to improve over time. It uses a structured reasoning framework to manage memory and handle complex tasks without the need for traditional fine-tuning methods. By employing Bayesian optimization for hyperparameter tuning and feature engineering, it effectively addresses a variety of data science challenges. The agent's performance has been validated through Kaggle competitions, where it achieved a high success rate and ranked competitively against human experts, demonstrating its advanced capabilities in multiple domains."
                },
                "zh": {
                    "title": "Agent K v1.0ï¼šè‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦çš„æœªæ¥",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Agent K v1.0ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è‡ªä¸»æ•°æ®ç§‘å­¦ä»£ç†ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–ã€ä¼˜åŒ–å’Œæ³›åŒ–å„ç§æ•°æ®ç§‘å­¦ä»»åŠ¡ã€‚Agent K v1.0 å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œèƒ½å¤Ÿç®¡ç†æ•´ä¸ªæ•°æ®ç§‘å­¦ç”Ÿå‘½å‘¨æœŸï¼Œå¹¶é€šè¿‡ç»éªŒå­¦ä¹ æ¥æå‡èƒ½åŠ›ã€‚å®ƒåˆ©ç”¨çµæ´»çš„ç»“æ„åŒ–æ¨ç†æ¡†æ¶ï¼ŒåŠ¨æ€å¤„ç†åµŒå¥—ç»“æ„ä¸­çš„è®°å¿†ï¼Œæœ‰æ•ˆåœ°ä»ç§¯ç´¯çš„ç»éªŒä¸­å­¦ä¹ ï¼Œä»¥åº”å¯¹å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡é€‰æ‹©æ€§å­˜å‚¨å’Œæ£€ç´¢å…³é”®ä¿¡æ¯ï¼ŒAgent K v1.0 ä¼˜åŒ–äº†çŸ­æœŸå’Œé•¿æœŸè®°å¿†ï¼ŒåŸºäºç¯å¢ƒå¥–åŠ±æŒ‡å¯¼æœªæ¥å†³ç­–ï¼Œå±•ç°å‡ºä¸äººç±»ä¸“å®¶ç›¸å½“çš„æŠ€èƒ½æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.03823",
            "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination",
            "url": "https://huggingface.co/papers/2411.03823",
            "abstract": "The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced.",
            "score": 17,
            "issue_id": 457,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "3f0a02ee67213e17",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ§Ğ¸ÑÑ‚Ğ¾Ñ‚Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - Ğ·Ğ°Ğ»Ğ¾Ğ³ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MM-Detect Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ² MLLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MM-Detect Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ÑÑ‚ĞµĞ¿ĞµĞ½ÑĞ¼ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·-Ğ·Ğ° ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Detecting Contamination in Multimodal Language Models",
                    "desc": "This paper addresses the challenge of data contamination in multimodal large language models (MLLMs), which can affect their performance evaluation. The authors propose a new framework called MM-Detect, specifically designed to identify contamination in MLLMs across different modalities and training phases. Their experiments show that MM-Detect can effectively detect varying levels of contamination and reveal how training set leakage impacts performance. Additionally, the study investigates contamination sources during both the pre-training and fine-tuning phases of MLLMs, providing valuable insights into potential contamination points."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ•°æ®æ±¡æŸ“æ£€æµ‹çš„æ–°çªç ´",
                    "desc": "æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ•°æ®æ±¡æŸ“æ£€æµ‹çš„æ–°æ¡†æ¶MM-Detectã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMM-Detectå¯¹ä¸åŒç¨‹åº¦çš„æ•°æ®æ±¡æŸ“éå¸¸æ•æ„Ÿï¼Œå¹¶èƒ½æ˜¾è‘—æå‡æ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†æ•°æ®æ±¡æŸ“å¯èƒ½æºè‡ªLLMsçš„é¢„è®­ç»ƒé˜¶æ®µå’ŒMLLMsçš„å¾®è°ƒé˜¶æ®µï¼Œä¸ºç†è§£æ±¡æŸ“å¼•å…¥çš„æ—¶æœºæä¾›äº†æ–°çš„è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.03884",
            "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
            "url": "https://huggingface.co/papers/2411.03884",
            "abstract": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.",
            "score": 4,
            "issue_id": 458,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "6ed1524392784244",
            "data": {
                "error": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04109",
            "title": "Self-Consistency Preference Optimization",
            "url": "https://huggingface.co/papers/2411.04109",
            "abstract": "Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. In this work, we extend the self-consistency concept to help train models. We thus introduce self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. We show ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku.",
            "score": 0,
            "issue_id": 458,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "213f2796c0bc72ae",
            "data": {
                "error": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
            }
        }
    ],
    "link_prev": "2024-11-06.html",
    "link_next": "2024-11-08.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "06.11",
        "en": "11/06",
        "zh": "11æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "08.11",
        "en": "11/08",
        "zh": "11æœˆ8æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ”¹è¿›çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œç§°ä¸ºHtmlRAGã€‚ä¼ ç»Ÿçš„RAGç³»ç»Ÿä»ç½‘é¡µæ£€ç´¢ä¿¡æ¯ï¼Œæå–çº¯æ–‡æœ¬å–‚ç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç„¶è€Œï¼Œè¿™ä¼šä¸¢å¤±HTMLä¸­çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚HtmlRAGç›´æ¥ä½¿ç”¨HTMLæ ¼å¼çš„çŸ¥è¯†ï¼Œä¿ç•™æ›´å¤šä¿¡æ¯ã€‚ä½†HTMLåŒ…å«é¢å¤–çš„æ ‡ç­¾å’Œå™ªå£°ï¼Œä½œè€…æå‡ºäº†æ¸…ç†å’Œå‹ç¼©ç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒHtmlRAGåœ¨å…­ä¸ªé—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°æ›´å¥½ã€‚",
        "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ”¹è¿›çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œç§°ä¸ºHtmlRAGã€‚ä¼ ç»Ÿçš„RAGç³»ç»Ÿä»ç½‘é¡µæ£€ç´¢ä¿¡æ¯ï¼Œæå–çº¯æ–‡æœ¬å–‚ç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç„¶è€Œï¼Œè¿™ä¼šä¸¢å¤±HTMLä¸­çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚HtmlRAGç›´æ¥ä½¿ç”¨HTMLæ ¼å¼çš„çŸ¥è¯†ï¼Œä¿ç•™æ›´å¤šä¿¡æ¯ã€‚ä½†HTMLåŒ…å«é¢å¤–çš„æ ‡ç­¾å’Œå™ªå£°ï¼Œä½œè€…æå‡ºäº†æ¸…ç†å’Œå‹ç¼©ç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒHtmlRAGåœ¨å…­ä¸ªé—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°æ›´å¥½ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng gÇi jÃ¬n de jiÇn suÇ’ zÄ“ng qiÃ¡ng shÄ“ng chÃ©ng (RAG) fÄng fÇ, chÄ“ng wÃ©i HtmlRAG. chuÃ¡ntÇ’ng de RAG xÃ¬ tÇ’ng cÃ³ng wÇng yÃ¨ jiÇn suÇ’ xÃ¬n xÄ«, tÄ« qÇ” chÃºn wÃ©n bÄ›n wÃ¨i gÄ›i dÃ  yÇ” yÃ¡n mÃ³ xÃ¬ng (LLMs). rÃ¡n Ã©r, zhÃ¨ huÃ¬ diÅ« shÄ« HTML zhÅng de jiÃ¨ gÃ²u hÃ© yÇ” yÃ¬ xÃ¬n xÄ«. HtmlRAG zhÃ­ jiÄ“ shÇ yÃ²ng HTML gÄ“ shÃ¬ de zhÄ« shÃ¬, bÇo liÃº gÃ¨ng duÅ xÃ¬n xÄ«. dÃ n HTML bÄo hÃ¡n Ã© xiÇo de biÇo qiÄn hÃ© zÃ o shÄ“ng, zuÃ² zhÄ› tÃ­ chÅ« le qÄ«ng lÇ hÃ© yÄ suÅ cÃ¨ lÃ¼Ã¨ lÃ¡i jiÄ› juÃ© zhÃ¨ gÃ¨ wÃ¨n tÃ­. shÃ­ yÃ n zhÃ¨ng mÃ­ng, HtmlRAG zÃ i liÃ¹ gÃ¨ wÃ¨n dÃ¡ shÃ¹ jÃ¹ zhÅng biÇo xiÃ n gÃ¨ng hÇo.",
        "vocab": "[\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇi jÃ¬n\", \"trans\": \"improvement\"},\n    {\"word\": \"æ£€ç´¢\", \"pinyin\": \"jiÇn suÇ’\", \"trans\": \"retrieval\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ng qiÃ¡ng\", \"trans\": \"enhancement\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generation\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"ç§°ä¸º\", \"pinyin\": \"chÄ“ng wÃ©i\", \"trans\": \"called\"},\n    {\"word\": \"ä¼ ç»Ÿ\", \"pinyin\": \"chuÃ¡n tÇ’ng\", \"trans\": \"traditional\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬ tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"ç½‘é¡µ\", \"pinyin\": \"wÇng yÃ¨\", \"trans\": \"webpage\"},\n    {\"word\": \"æå–\", \"pinyin\": \"tÃ­ qu\", \"trans\": \"extract\"},\n    {\"word\": \"çº¯æ–‡æœ¬\", \"pinyin\": \"chÃºn wÃ©n bÄ›n\", \"trans\": \"pure text\"},\n    {\"word\": \"å–‚ç»™\", \"pinyin\": \"wÃ¨i gÄ›i\", \"trans\": \"feed\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"ä¸¢å¤±\", \"pinyin\": \"diÅ« shÄ«\", \"trans\": \"lose\"},\n    {\"word\": \"ç»“æ„\", \"pinyin\": \"jiÃ© gÃ²u\", \"trans\": \"structure\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ” yÃ¬\", \"trans\": \"semantics\"},\n    {\"word\": \"ä¿¡æ¯\", \"pinyin\": \"xÃ¬n xÄ«\", \"trans\": \"information\"},\n    {\"word\": \"ç›´æ¥\", \"pinyin\": \"zhÃ­ jiÄ“\", \"trans\": \"directly\"},\n    {\"word\": \"æ ¼å¼\", \"pinyin\": \"gÃ© shÃ¬\", \"trans\": \"format\"},\n    {\"word\": \"çŸ¥è¯†\", \"pinyin\": \"zhÄ« shÃ¬\", \"trans\": \"knowledge\"},\n    {\"word\": \"ä¿ç•™\", \"pinyin\": \"bÇo liÃº\", \"trans\": \"retain\"},\n    {\"word\": \"é¢å¤–\", \"pinyin\": \"Ã© wÃ i\", \"trans\": \"extra\"},\n    {\"word\": \"æ ‡ç­¾\", \"pinyin\": \"biÄo qiÄn\", \"trans\": \"tag\"},\n    {\"word\": \"å™ªå£°\", \"pinyin\": \"zÃ o shÄ“ng\", \"trans\": \"noise\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"æ¸…ç†\", \"pinyin\": \"qÄ«ng lÇ\", \"trans\": \"clean\"},\n    {\"word\": \"å‹ç¼©\", \"pinyin\": \"yÄ suÅ\", \"trans\": \"compress\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"è§£å†³\", \"pinyin\": \"jiÄ› juÃ©\", \"trans\": \"solve\"},\n    {\"word\": \"é—®é¢˜\", \"pinyin\": \"wÃ¨n tÃ­\", \"trans\": \"problem\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"è¯æ˜\", \"pinyin\": \"zhÃ¨ng mÃ­ng\", \"trans\": \"prove\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"}\n]",
        "trans": "This article introduces an improved Retrieval-Augmented Generation (RAG) method called HtmlRAG. Traditional RAG systems retrieve information from web pages and extract plain text to feed into large language models (LLMs). However, this approach loses the structural and semantic information present in HTML. HtmlRAG directly uses knowledge in HTML format, preserving more information. But since HTML contains additional tags and noise, the authors propose cleaning and compression strategies to address this issue. Experiments show that HtmlRAG performs better on six question-answering datasets.",
        "update_ts": "2024-11-07 10:12"
    }
}