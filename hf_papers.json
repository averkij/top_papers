{
    "date": {
        "ru": "17 апреля",
        "en": "April 17",
        "zh": "4月17日"
    },
    "time_utc": "2025-04-17 02:23",
    "weekday": 3,
    "issue_id": 3280,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.12240",
            "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
            "url": "https://huggingface.co/papers/2504.12240",
            "abstract": "The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.",
            "score": 7,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "a237e12792a9a0c8",
            "authors": [
                "Junhao Zhuang",
                "Lingen Li",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Chun Yuan",
                "Ying Shan"
            ],
            "affiliations": [
                "Tencent ARC Lab, China",
                "The Chinese University of Hong Kong, China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12240.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#cv",
                    "#open_source",
                    "#architecture",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Cobra: Быстрая и точная колоризация комиксов с помощью контекстных референсов",
                    "desc": "В статье представлен метод Cobra для эффективной колоризации комиксов на основе множества эталонных изображений. Используется архитектура Causal Sparse DiT с позиционным кодированием и разреженным вниманием для обработки длинного контекста. Метод позволяет быстро и точно раскрашивать линейные рисунки с учетом более 200 референсов. Cobra демонстрирует высокую производительность и интерактивность, отвечая требованиям индустрии комиксов."
                },
                "en": {
                    "title": "Cobra: Revolutionizing Line Art Colorization with Contextual Efficiency",
                    "desc": "This paper presents Cobra, a novel method for line art colorization in the comic production industry, which requires high accuracy and efficiency. Cobra utilizes a Causal Sparse DiT architecture that incorporates advanced techniques like causal sparse attention and positional encodings to handle over 200 reference images effectively. The method addresses challenges such as slow inference times and the need for flexible control, ensuring color identity consistency across diverse characters and backgrounds. Experimental results show that Cobra significantly improves the quality and speed of line art colorization, making it a valuable tool for artists."
                },
                "zh": {
                    "title": "Cobra：高效灵活的线条艺术上色解决方案",
                    "desc": "本论文介绍了一种名为Cobra的高效线条艺术上色方法，旨在解决漫画制作行业中对高准确性和灵活控制的需求。Cobra能够处理超过200张参考图像，并保持低延迟，适应复杂的角色和背景。该方法采用了因果稀疏DiT架构，利用特殊设计的位置编码和因果稀疏注意力机制，有效管理长上下文参考。实验结果表明，Cobra在上色质量和推理速度上均有显著提升，满足了工业界的关键需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11952",
            "title": "Robust and Fine-Grained Detection of AI Generated Texts",
            "url": "https://huggingface.co/papers/2504.11952",
            "abstract": "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.",
            "score": 1,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "bdea465fe17b9401",
            "authors": [
                "Ram Mohan Rao Kadiyala",
                "Siddartha Pullakhandam",
                "Kanwal Mehreen",
                "Drishti Sharma",
                "Siddhant Gupta",
                "Jebish Purbey",
                "Ashay Srivastava",
                "Subhasya TippaReddy",
                "Arvind Reddy Bobbili",
                "Suraj Telugara Chandrashekhar",
                "Modabbir Adeeb",
                "Srinadh Vura",
                "Hamza Farooq"
            ],
            "affiliations": [
                "Cohere for AI Community",
                "IISc Bangalore",
                "IIT Roorkee",
                "M2ai.in",
                "Pulchowk Campus",
                "Stanford University",
                "Traversaal.ai",
                "University of California, Los Angeles",
                "University of Houston",
                "University of Maryland, College Park",
                "University of South Florida",
                "Vantager"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11952.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#hallucinations",
                    "#dataset",
                    "#benchmark",
                    "#multilingual",
                    "#security",
                    "#data"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Универсальный детектор ИИ-текстов: от токенов до языков",
                    "desc": "Статья представляет новый подход к обнаружению текстов, созданных искусственным интеллектом. Авторы разработали модели для классификации токенов, обученные на обширном наборе текстов, созданных совместно человеком и ИИ. Модели показали хорошие результаты на текстах из неизвестных областей, от неизвестных генераторов и на текстах с состязательными входными данными. Исследователи также представили новый датасет из 2,4 миллиона текстов на 23 языках, созданных в соавторстве с популярными проприетарными языковыми моделями."
                },
                "en": {
                    "title": "Advancing Detection of Human-LLM Co-Authored Texts",
                    "desc": "This paper addresses the challenge of detecting machine-generated content, particularly in cases where texts are co-authored by humans and language models (LLMs). The authors developed a set of token classification models trained on a large dataset of 2.4 million co-authored texts, which allows for better detection across various domains and generators. The models demonstrated strong performance even with adversarial inputs and texts from non-native speakers. Additionally, the paper provides insights into how the models perform based on text length and characteristics compared to purely human-authored content."
                },
                "zh": {
                    "title": "构建高效的机器生成内容检测系统",
                    "desc": "本文提出了一种理想的检测系统，旨在有效识别机器生成的内容，尤其是在短文本中。现有系统在识别AI生成内容时常常面临挑战，因此我们专注于人类与大型语言模型（LLM）共同创作的文本。我们构建了一系列用于标记分类的模型，这些模型在大量人机共创文本上进行训练，并在未见领域和生成器的文本上表现良好。我们还引入了一个包含240万条文本的新数据集，主要由多种流行的专有LLM共同创作，涵盖23种语言。"
                }
            }
        }
    ],
    "link_prev": "2025-04-16.html",
    "link_next": "2025-04-18.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4月16日"
    },
    "short_date_next": {
        "ru": "18.04",
        "en": "04/18",
        "zh": "4月18日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了提升大型语言模型（LLM）推理能力的兴趣。目前的方法依赖监督信号，存在可扩展性和高标注成本问题。作者提出了一种无监督的自训练框架，名为Genius。Genius通过步进式预测重采样策略和优势校准优化（ACO）损失函数，实现了无需外部监督的LLM推理能力提升。代码将在https://github.com/xufangzhi/Genius发布。",
        "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
        "pinyin": "这篇文章讨论了提升大型语言模型（LLM）推理能力的兴趣。\nZhè piān wénzhāng tǎolùn le tíshēng dàxíng yǔyán móxíng (LLM) tuīlǐ nénglì de xìngqù.\n\n目前的方法依赖监督信号，存在可扩展性和高标注成本问题。\nMùqián de fāngfǎ yīlài jiàndū xìnhà",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improve\"},\n    {\"word\": \"大型\", \"pinyin\": \"dà xíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔ yán mó xíng\", \"trans\": \"language model\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"兴趣\", \"pinyin\": \"xìng qù\", \"trans\": \"interest\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"监督\", \"pinyin\": \"jiàn dū\", \"trans\": \"supervised\"},\n    {\"word\": \"信号\", \"pinyin\": \"xìn hào\", \"trans\": \"signal\"},\n    {\"word\": \"可扩展性\", \"pinyin\": \"kě kuò zhǎn xìng\", \"trans\": \"scalability\"},\n    {\"word\": \"高\", \"pinyin\": \"gāo\", \"trans\": \"high\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotation\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"无监督\", \"pinyin\": \"wú jiàn dū\", \"trans\": \"unsupervised\"},\n    {\"word\": \"自训练\", \"pinyin\": \"zì xùn liàn\", \"trans\": \"self-training\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"名为\", \"pinyin\": \"míng wéi\", \"trans\": \"named\"},\n    {\"word\": \"步进式\", \"pinyin\": \"bù jìn shì\", \"trans\": \"stepwise\"},\n    {\"word\": \"预测\", \"pinyin\": \"yù cè\", \"trans\": \"prediction\"},\n    {\"word\": \"重采样\", \"pinyin\": \"chóng cǎi yàng\", \"trans\": \"resampling\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"优势\", \"pinyin\": \"yōu shì\", \"trans\": \"advantage\"},\n    {\"word\": \"校准\", \"pinyin\": \"jiào zhǔn\", \"trans\": \"calibration\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"损失函数\", \"pinyin\": \"sǔn shī hán shù\", \"trans\": \"loss function\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"外部\", \"pinyin\": \"wài bù\", \"trans\": \"external\"},\n    {\"word\": \"发布\", \"pinyin\": \"fā bù\", \"trans\": \"release\"}\n]",
        "trans": "This article discusses the interest in enhancing the reasoning capabilities of large language models (LLMs). Current methods rely on supervised signals, which present issues with scalability and high annotation costs. The authors",
        "update_ts": "2025-04-16 09:12"
    }
}