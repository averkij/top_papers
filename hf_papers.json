{
    "date": {
        "ru": "22 –∞–≤–≥—É—Å—Ç–∞",
        "en": "August 22",
        "zh": "8Êúà22Êó•"
    },
    "time_utc": "2025-08-22 03:36",
    "weekday": 4,
    "issue_id": 5486,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.15763",
            "title": "Intern-S1: A Scientific Multimodal Foundation Model",
            "url": "https://huggingface.co/papers/2508.15763",
            "abstract": "Intern-S1, a multimodal Mixture-of-Experts model with extensive pre-training and reinforcement learning, achieves top-tier performance in general reasoning and outperforms closed-source models in scientific tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.",
            "score": 110,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "55fe9cc77ae55101",
            "authors": [
                "Lei Bai",
                "Zhongrui Cai",
                "Maosong Cao",
                "Weihan Cao",
                "Chiyu Chen",
                "Haojiong Chen",
                "Kai Chen",
                "Pengcheng Chen",
                "Ying Chen",
                "Yongkang Chen",
                "Yu Cheng",
                "Yu Cheng",
                "Pei Chu",
                "Tao Chu",
                "Erfei Cui",
                "Ganqu Cui",
                "Long Cui",
                "Ziyun Cui",
                "Nianchen Deng",
                "Ning Ding",
                "Nanqin Dong",
                "Peijie Dong",
                "Shihan Dou",
                "Sinan Du",
                "Haodong Duan",
                "Caihua Fan",
                "Ben Gao",
                "Changjiang Gao",
                "Jianfei Gao",
                "Songyang Gao",
                "Yang Gao",
                "Zhangwei Gao",
                "Jiaye Ge",
                "Qiming Ge",
                "Lixin Gu",
                "Yuzhe Gu",
                "Aijia Guo",
                "Qipeng Guo",
                "Xu Guo",
                "Conghui He",
                "Junjun He",
                "Yili Hong",
                "Siyuan Hou",
                "Caiyu Hu",
                "Hanglei Hu",
                "Jucheng Hu",
                "Ming Hu",
                "Zhouqi Hua",
                "Haian Huang",
                "Junhao Huang",
                "Xu Huang",
                "Zixian Huang",
                "Zhe Jiang",
                "Lingkai Kong",
                "Linyang Li",
                "Peiji Li",
                "Pengze Li",
                "Shuaibin Li",
                "Tianbin Li",
                "Wei Li",
                "Yuqiang Li",
                "Dahua Lin",
                "Junyao Lin",
                "Tianyi Lin",
                "Zhishan Lin",
                "Hongwei Liu",
                "Jiangning Liu",
                "Jiyao Liu",
                "Junnan Liu",
                "Kai Liu",
                "Kaiwen Liu",
                "Kuikun Liu",
                "Shichun Liu",
                "Shudong Liu",
                "Wei Liu",
                "Xinyao Liu",
                "Yuhong Liu",
                "Zhan Liu",
                "Yinquan Lu",
                "Haijun Lv",
                "Hongxia Lv",
                "Huijie Lv",
                "Qidang Lv",
                "Ying Lv",
                "Chengqi Lyu",
                "Chenglong Ma",
                "Jianpeng Ma",
                "Ren Ma",
                "Runmin Ma",
                "Runyuan Ma",
                "Xinzhu Ma",
                "Yichuan Ma",
                "Zihan Ma",
                "Sixuan Mi",
                "Junzhi Ning",
                "Wenchang Ning",
                "Xinle Pang",
                "Jiahui Peng",
                "Runyu Peng",
                "Yu Qiao",
                "Jiantao Qiu",
                "Xiaoye Qu",
                "Yuan Qu",
                "Yuchen Ren",
                "Fukai Shang",
                "Wenqi Shao",
                "Junhao Shen",
                "Shuaike Shen",
                "Chunfeng Song",
                "Demin Song",
                "Diping Song",
                "Chenlin Su",
                "Weijie Su",
                "Weigao Sun",
                "Yu Sun",
                "Qian Tan",
                "Cheng Tang",
                "Huanze Tang",
                "Kexian Tang",
                "Shixiang Tang",
                "Jian Tong",
                "Aoran Wang",
                "Bin Wang",
                "Dong Wang",
                "Lintao Wang",
                "Rui Wang",
                "Weiyun Wang",
                "Wenhai Wang",
                "Yi Wang",
                "Ziyi Wang",
                "Ling-I Wu",
                "Wen Wu",
                "Yue Wu",
                "Zijian Wu",
                "Linchen Xiao",
                "Shuhao Xing",
                "Chao Xu",
                "Huihui Xu",
                "Jun Xu",
                "Ruiliang Xu",
                "Wanghan Xu",
                "GanLin Yang",
                "Yuming Yang",
                "Haochen Ye",
                "Jin Ye",
                "Shenglong Ye",
                "Jia Yu",
                "Jiashuo Yu",
                "Jing Yu",
                "Fei Yuan",
                "Bo Zhang",
                "Chao Zhang",
                "Chen Zhang",
                "Hongjie Zhang",
                "Jin Zhang",
                "Qiaosheng Zhang",
                "Qiuyinzhe Zhang",
                "Songyang Zhang",
                "Taolin Zhang",
                "Wenlong Zhang",
                "Wenwei Zhang",
                "Yechen Zhang",
                "Ziyang Zhang",
                "Haiteng Zhao",
                "Qian Zhao",
                "Xiangyu Zhao",
                "Xiangyu Zhao",
                "Bowen Zhou",
                "Dongzhan Zhou",
                "Peiheng Zhou",
                "Yuhao Zhou",
                "Yunhua Zhou",
                "Dongsheng Zhu",
                "Lin Zhu",
                "Yicheng Zou"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15763.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#science",
                    "#reasoning",
                    "#dataset",
                    "#open_source",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "Intern-S1: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-—ç–∫—Å–ø–µ—Ä—Ç –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á",
                    "desc": "Intern-S1 - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ç–∏–ø–∞ Mixture-of-Experts —Å 28 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ –º–∞—Å—à—Ç–∞–±–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 5 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤, –≤–∫–ª—é—á–∞—è 2.5 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –Ω–∞—É—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π. Intern-S1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ñ–ª–∞–π–Ω –∏ –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏–∫–∏ Mixture-of-Rewards –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª–µ–µ —á–µ–º 1000 –∑–∞–¥–∞—á–∞–º. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—â–µ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∑–∞–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –≤ –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ—Ä–º–æ–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∫—Ä–∏—Å—Ç–∞–ª–ª–æ–≤."
                },
                "en": {
                    "title": "Bridging the Gap in Scientific AI with Intern-S1",
                    "desc": "Intern-S1 is a multimodal Mixture-of-Experts model designed to enhance performance in scientific reasoning tasks. It utilizes extensive pre-training on a vast dataset, including a significant portion from scientific domains, to develop a deep understanding of complex data. The model employs reinforcement learning techniques, specifically Mixture-of-Rewards, to optimize its performance across numerous tasks simultaneously. As a result, Intern-S1 not only excels in general reasoning but also surpasses both open-source and closed-source models in specialized scientific applications."
                },
                "zh": {
                    "title": "Intern-S1ÔºöÁßëÂ≠¶È¢ÜÂüüÁöÑÊô∫ËÉΩ‰∏ìÂÆ∂",
                    "desc": "Intern-S1ÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅ‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºåÁªèËøáÂπøÊ≥õÁöÑÈ¢ÑËÆ≠ÁªÉÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåËÉΩÂ§üÂú®‰∏ÄËà¨Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Âú®ÁßëÂ≠¶‰ªªÂä°‰∏≠Ë∂ÖË∂äÈó≠Ê∫êÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÊã•Êúâ280‰∫ø‰∏™ÊøÄÊ¥ªÂèÇÊï∞Âíå2410‰∫ø‰∏™ÊÄªÂèÇÊï∞ÔºåÁªèËøá5‰∏á‰∫ø‰∏™Ê†áËÆ∞ÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉÔºåÂÖ∂‰∏≠Ë∂ÖËøá2.5‰∏á‰∫ø‰∏™Ê†áËÆ∞Êù•Ëá™ÁßëÂ≠¶È¢ÜÂüü„ÄÇ‰∏∫‰∫ÜÁº©Â∞èÂºÄÊîæÊ∫êÊ®°Âûã‰∏éÈó≠Ê∫êÊ®°ÂûãÂú®ÁßëÂ≠¶È¢ÜÂüüÁöÑÂ∑ÆË∑ùÔºåIntern-S1ÈááÁî®‰∫ÜÊ∑∑ÂêàÂ•ñÂä±Êú∫Âà∂ÔºåÂú®Ë∂ÖËøá1000‰∏™‰ªªÂä°‰∏äËøõË°åÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÇÈÄöËøáÁÆóÊ≥ï„ÄÅÊï∞ÊçÆÂíåËÆ≠ÁªÉÁ≥ªÁªüÁöÑÁªºÂêàÂàõÊñ∞ÔºåIntern-S1Âú®Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∏≠ÂèñÂæó‰∫ÜÈ°∂Â∞ñÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15144",
            "title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation",
            "url": "https://huggingface.co/papers/2508.15144",
            "abstract": "GUI-Owl and Mobile-Agent-v3 are open-source GUI agent models and frameworks that achieve state-of-the-art performance across various benchmarks using innovations in environment infrastructure, agent capabilities, and scalable reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.",
            "score": 21,
            "issue_id": 5486,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "f41c368d3d1b16f8",
            "authors": [
                "Jiabo Ye",
                "Xi Zhang",
                "Haiyang Xu",
                "Haowei Liu",
                "Junyang Wang",
                "Zhaoqing Zhu",
                "Ziwei Zheng",
                "Feiyu Gao",
                "Junjie Cao",
                "Zhengxi Lu",
                "Jitong Liao",
                "Qi Zheng",
                "Fei Huang",
                "Jingren Zhou",
                "Ming Yan"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15144.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#open_source",
                    "#agents",
                    "#rl",
                    "#training"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ø—Ä–æ—Ä—ã–≤–∞ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GUI-Owl –∏ Mobile-Agent-v3 - –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º. –≠—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º –≤ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥—ã, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –∞–≥–µ–Ω—Ç–æ–≤ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. GUI-Owl –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±–ª–∞—á–Ω—É—é –≤–∏—Ä—Ç—É–∞–ª—å–Ω—É—é —Å—Ä–µ–¥—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. Mobile-Agent-v3 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ GUI-–∞–≥–µ–Ω—Ç–æ–≤."
                },
                "en": {
                    "title": "Revolutionizing GUI Agents with State-of-the-Art Performance",
                    "desc": "This paper presents GUI-Owl, an advanced GUI agent model that excels in performance on various benchmarks for desktop and mobile environments. It introduces Mobile-Agent-v3, which enhances GUI-Owl's capabilities, achieving new state-of-the-art scores on AndroidWorld and OSWorld. Key innovations include a large-scale cloud-based environment for generating high-quality interaction data, diverse foundational agent capabilities for end-to-end decision-making, and a scalable reinforcement learning framework for real-world applications. Both models are open-source, promoting further research and development in GUI agent technology."
                },
                "zh": {
                    "title": "ÂºÄÊ∫êGUI‰ª£ÁêÜÁöÑÊñ∞Êó∂‰ª£",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜGUI-OwlÂíåMobile-Agent-v3Ëøô‰∏§‰∏™ÂºÄÊ∫êGUI‰ª£ÁêÜÊ®°ÂûãÂíåÊ°ÜÊû∂ÔºåÂÆÉ‰ª¨Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇGUI-OwlÊòØ‰∏Ä‰∏™Âü∫Á°ÄÁöÑGUI‰ª£ÁêÜÊ®°ÂûãÔºåÂú®Ê°åÈù¢ÂíåÁßªÂä®ÁéØÂ¢ÉÁöÑÂçÅ‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇMobile-Agent-v3ÂàôÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑGUI‰ª£ÁêÜÊ°ÜÊû∂ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊÄßËÉΩÔºåÂàõÈÄ†‰∫ÜÊñ∞ÁöÑÂºÄÊ∫êGUI‰ª£ÁêÜÊ°ÜÊû∂ÁöÑÊúÄ‰Ω≥ËÆ∞ÂΩï„ÄÇËØ•Á†îÁ©∂ÁöÑÂÖ≥ÈîÆÂàõÊñ∞ÂåÖÊã¨Â§ßËßÑÊ®°ÁéØÂ¢ÉÂü∫Á°ÄËÆæÊñΩ„ÄÅÂ§öÊ†∑ÂåñÁöÑÂü∫Á°Ä‰ª£ÁêÜËÉΩÂäõÂíåÂèØÊâ©Â±ïÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15260",
            "title": "Deep Think with Confidence",
            "url": "https://huggingface.co/papers/2508.15260",
            "abstract": "DeepConf enhances reasoning efficiency and performance by filtering low-quality reasoning traces using model-internal confidence signals, achieving high accuracy and reducing token generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown great potential in reasoning tasks through test-time scaling methods like self-consistency with majority voting. However, this approach often leads to diminishing returns in accuracy and high computational overhead. To address these challenges, we introduce Deep Think with Confidence (DeepConf), a simple yet powerful method that enhances both reasoning efficiency and performance at test time. DeepConf leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces during or after generation. It requires no additional model training or hyperparameter tuning and can be seamlessly integrated into existing serving frameworks. We evaluate DeepConf across a variety of reasoning tasks and the latest open-source models, including Qwen 3 and GPT-OSS series. Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full parallel thinking.",
            "score": 8,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "2ad4b155cf2ef39e",
            "authors": [
                "Yichao Fu",
                "Xuewei Wang",
                "Yuandong Tian",
                "Jiawei Zhao"
            ],
            "affiliations": [
                "Meta AI",
                "UCSD"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15260.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–£–º–Ω–µ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ: DeepConf –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò",
                    "desc": "DeepConf - —ç—Ç–æ –º–µ—Ç–æ–¥, —É–ª—É—á—à–∞—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å–∏–≥–Ω–∞–ª—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. DeepConf –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏. –í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤."
                },
                "en": {
                    "title": "Boosting Reasoning Efficiency with Confidence Filtering",
                    "desc": "DeepConf is a novel method that improves the efficiency and accuracy of reasoning in large language models by using internal confidence signals to filter out low-quality reasoning traces. This approach addresses the limitations of traditional test-time scaling methods, which often result in high computational costs and minimal accuracy gains. By dynamically selecting the most reliable reasoning paths, DeepConf enhances performance without requiring additional training or tuning. Evaluations show that DeepConf significantly boosts accuracy and reduces token generation, making it a valuable tool for various reasoning tasks."
                },
                "zh": {
                    "title": "Ê∑±Â∫¶ÁΩÆ‰ø°ÔºåÊèêÂçáÊé®ÁêÜÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß",
                    "desc": "DeepConfÊòØ‰∏ÄÁßçÂ¢ûÂº∫Êé®ÁêÜÊïàÁéáÂíåÊÄßËÉΩÁöÑÊñπÊ≥ïÔºåÈÄöËøá‰ΩøÁî®Ê®°ÂûãÂÜÖÈÉ®ÁöÑÁΩÆ‰ø°‰ø°Âè∑Êù•ËøáÊª§‰ΩéË¥®ÈáèÁöÑÊé®ÁêÜËΩ®Ëøπ„ÄÇËØ•ÊñπÊ≥ïÂú®ÊµãËØïÊó∂Êó†ÈúÄÈ¢ùÂ§ñÁöÑÊ®°ÂûãËÆ≠ÁªÉÊàñË∂ÖÂèÇÊï∞Ë∞ÉÊï¥ÔºåÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Áé∞ÊúâÁöÑÊúçÂä°Ê°ÜÊû∂‰∏≠„ÄÇDeepConfÂú®Â§öÁßçÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®AIME 2025Á≠âÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÔºåËææÂà∞‰∫Ü99.9%ÁöÑÈ´òÂáÜÁ°ÆÁéáÔºåÂπ∂ÂáèÂ∞ë‰∫ÜÂ§öËææ84.7%ÁöÑÁîüÊàê‰ª§Áâå„ÄÇÈÄöËøáÂä®ÊÄÅËøáÊª§ÔºåDeepConfÊúâÊïàÊèêÈ´ò‰∫ÜÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15769",
            "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
            "url": "https://huggingface.co/papers/2508.15769",
            "abstract": "SceneGen generates multiple 3D assets from a single scene image using a novel framework that integrates local and global scene information, enabling efficient and robust 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
            "score": 3,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "deff71bdd7ad6d46",
            "authors": [
                "Yanxu Meng",
                "Haoning Wu",
                "Ya Zhang",
                "Weidi Xie"
            ],
            "affiliations": [
                "School of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15769.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "–û—Ç 2D –∫ 3D: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω",
                    "desc": "SceneGen - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ü–µ–Ω—ã. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –ª–æ–∫–∞–ª—å–Ω—É—é –∏ –≥–ª–æ–±–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ü–µ–Ω–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –ø–æ–∏—Å–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏. SceneGen –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ VR/AR –∏ –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–º –ò–ò."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Creation from Single Images",
                    "desc": "SceneGen is a new framework designed to create multiple 3D assets from a single scene image by effectively combining local and global scene information. It uses a feature aggregation module that processes visual and geometric data to generate 3D models with their textures and spatial arrangements in one go, without needing optimization or asset retrieval. The framework is also adaptable, showing improved performance when handling multiple images, even though it was initially trained on single images. Overall, SceneGen represents a significant advancement in the field of 3D content generation, with promising applications in virtual and augmented reality."
                },
                "zh": {
                    "title": "SceneGenÔºöÈ´òÊïàÁîüÊàê3DËµÑ‰∫ßÁöÑÊñ∞Ê°ÜÊû∂",
                    "desc": "SceneGenÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•‰ªéÂçï‰∏ÄÂú∫ÊôØÂõæÂÉèÁîüÊàêÂ§ö‰∏™3DËµÑ‰∫ß„ÄÇÂÆÉÈÄöËøáÊï¥ÂêàÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÂú∫ÊôØ‰ø°ÊÅØÔºåËÉΩÂ§üÈ´òÊïà‰∏îÁ®≥ÂÅ•Âú∞ÂàõÂª∫3DÂÜÖÂÆπ„ÄÇËØ•Ê°ÜÊû∂Êó†ÈúÄ‰ºòÂåñÊàñËµÑ‰∫ßÊ£ÄÁ¥¢ÔºåÁõ¥Êé•ÁîüÊàêÂÖ∑ÊúâÂá†‰ΩïÂΩ¢Áä∂ÂíåÁ∫πÁêÜÁöÑ3DËµÑ‰∫ß„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåSceneGenÂú®ÁîüÊàêÊÄßËÉΩ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÊâ©Â±ïÊÄßÔºåÈÄÇÁî®‰∫éÂ§öÂõæÂÉèËæìÂÖ•Âú∫ÊôØ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15202",
            "title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial\n  Reasoning in Large Language Models",
            "url": "https://huggingface.co/papers/2508.15202",
            "abstract": "Fin-PRM, a domain-specialized reward model for finance, enhances intermediate reasoning in LLMs through step-level and trajectory-level supervision, improving performance in supervised learning, reinforcement learning, and test-time inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce Fin-PRM, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin.",
            "score": 0,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 –∞–≤–≥—É—Å—Ç–∞",
                "en": "August 21",
                "zh": "8Êúà21Êó•"
            },
            "hash": "05b2af7facdfcd18",
            "authors": [
                "Yuanchen Zhou",
                "Shuo Jiang",
                "Jie Zhu",
                "Junhui Li",
                "Lifan Guo",
                "Feng Chen",
                "Chi Zhang"
            ],
            "affiliations": [
                "Osaka University",
                "Qwen DianJin Team, Alibaba Cloud Computing",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15202.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#rlhf",
                    "#healthcare",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üíπ",
                "ru": {
                    "title": "Fin-PRM: –£–ª—É—á—à–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è",
                    "desc": "Fin-PRM - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä—ã, —É–ª—É—á—à–∞—é—â–∞—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—à–∞–≥–æ–≤—ã–π –∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–Ω—ã–π —É—Ä–æ–≤–Ω–∏ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º, –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≤—ã–≤–æ–¥–∞ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. Fin-PRM –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ–±–ª–∞—Å—Ç–∏ –Ω–∞ —Ç–∞–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∫–∞–∫ CFLUE –∏ FinQA. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Fin-PRM –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞."
                },
                "en": {
                    "title": "Fin-PRM: Elevating Financial Reasoning in LLMs",
                    "desc": "Fin-PRM is a specialized reward model designed for the finance domain, enhancing the reasoning capabilities of large language models (LLMs). It employs both step-level and trajectory-level supervision to evaluate and improve the reasoning process in financial tasks, which require precise and structured logic. By integrating this tailored approach, Fin-PRM significantly boosts performance in supervised learning, reinforcement learning, and inference tasks compared to general-purpose models. Experimental results show that models trained with Fin-PRM achieve notable improvements in financial reasoning benchmarks, demonstrating the effectiveness of domain-specific reward modeling."
                },
                "zh": {
                    "title": "ÈáëËûçÈ¢ÜÂüüÁöÑ‰∏ìÁî®Â•ñÂä±Ê®°ÂûãÊèêÂçáÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Fin-PRMÊòØ‰∏ÄÁßç‰∏ìÈó®ÈíàÂØπÈáëËûçÈ¢ÜÂüüÁöÑÂ•ñÂä±Ê®°ÂûãÔºåÊó®Âú®Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰∏≠Èó¥Êé®ÁêÜËøáÁ®ã‰∏≠ÁöÑË°®Áé∞„ÄÇÂÆÉÈÄöËøáÈÄêÊ≠•ÂíåËΩ®ËøπÁ∫ßÂà´ÁöÑÁõëÁù£ÔºåÊèê‰æõÂØπÈáëËûç‰ªªÂä°‰∏≠Êé®ÁêÜÊ≠•È™§ÁöÑÁªÜËá¥ËØÑ‰º∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåFin-PRMÂú®ÈáëËûçÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÈÄöÁî®Â•ñÂä±Ê®°ÂûãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁõëÁù£Â≠¶‰π†ÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑÊïàÊûú„ÄÇËØ•Ê®°ÂûãÁöÑÂ∫îÁî®Â±ïÁ§∫‰∫ÜÈ¢ÜÂüü‰∏ìÁî®Â•ñÂä±Âª∫Ê®°Âú®ÊèêÂçáLLMs‰∏éÈáëËûç‰∏ìÂÆ∂Êé®ÁêÜ‰∏ÄËá¥ÊÄßÊñπÈù¢ÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-08-21.html",
    "link_next": "2025-08-25.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "21.08",
        "en": "08/21",
        "zh": "8Êúà21Êó•"
    },
    "short_date_next": {
        "ru": "25.08",
        "en": "08/25",
        "zh": "8Êúà25Êó•"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    }
}