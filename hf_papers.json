{
    "date": {
        "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 10",
        "zh": "2æœˆ10æ—¥"
    },
    "time_utc": "2025-02-10 05:11",
    "weekday": 0,
    "issue_id": 2119,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.05173",
            "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
            "url": "https://huggingface.co/papers/2502.05173",
            "abstract": "While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.",
            "score": 15,
            "issue_id": 2118,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "ba284ed1a62b3c2c",
            "authors": [
                "Xilin Wei",
                "Xiaoran Liu",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Cao",
                "Jian Tong",
                "Haodong Duan",
                "Qipeng Guo",
                "Jiaqi Wang",
                "Xipeng Qiu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Fudan University, Shanghai, China",
                "Shanghai AI Laboratory, Shanghai, China",
                "Shanghai Innovation Institute, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05173.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#3d",
                    "#architecture",
                    "#video",
                    "#long_context"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VideoRoPE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoRoPE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Rotary Position Embedding. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ 4 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ RoPE Ğº Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ V-NIAH-D Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² RoPE. VideoRoPE Ğ¸Ğ¼ĞµĞµÑ‚ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ RoPE Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Understanding with VideoRoPE",
                    "desc": "This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data."
                },
                "zh": {
                    "title": "VideoRoPEï¼šè§†é¢‘ä¸­çš„æ—‹è½¬ä½ç½®åµŒå…¥æ–°çªç ´",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•å°†æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰æœ‰æ•ˆåœ°æ‰©å±•åˆ°è§†é¢‘æ•°æ®ä¸­ã€‚ç ”ç©¶åˆ†æäº†å››ä¸ªå…³é”®ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§å¯¹äºRoPEåœ¨è§†é¢‘ä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡V-NIAH-Dï¼Œå±•ç¤ºäº†ç°æœ‰RoPEå˜ä½“åœ¨å¤„ç†è§†é¢‘æ—¶å®¹æ˜“å—åˆ°å¹²æ‰°çš„ç¼ºé™·ã€‚åŸºäºè¿™äº›åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†VideoRoPEï¼Œå®ƒé€šè¿‡3Dç»“æ„æ¥ä¿æŒæ—¶ç©ºå…³ç³»ï¼Œå¹¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¹‹å‰çš„RoPEå˜ä½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05176",
            "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360Â° Unbounded Scene Inpainting",
            "url": "https://huggingface.co/papers/2502.05176",
            "abstract": "Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.",
            "score": 4,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "9b52f2788f53c3c0",
            "authors": [
                "Chung-Ho Wu",
                "Yang-Jung Chen",
                "Ying-Huan Chen",
                "Jie-Ying Lee",
                "Bo-Hsu Ke",
                "Chun-Wei Tuan Mu",
                "Yi-Chuan Huang",
                "Chin-Yang Lin",
                "Min-Hung Chen",
                "Yen-Yu Lin",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "NVIDIA",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05176.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸: AuraFusion360 Ğ´Ğ»Ñ Ğ±ĞµĞ·ÑƒĞ¿Ñ€ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½",
                    "desc": "AuraFusion360 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Gaussian Splatting. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SDEdit Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 360-USID Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ¾Ğ¼ 360 Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Inpainting with AuraFusion360",
                    "desc": "AuraFusion360 is a new method for filling in missing parts of 3D scenes, which is important for applications like virtual reality. It uses Gaussian Splatting to represent scenes and introduces techniques for better identifying what should be filled in, ensuring that the filled areas look realistic from different angles. The method also places points accurately without needing extra training and enhances details to keep the views consistent. Additionally, it comes with a new dataset for testing these 3D inpainting methods, showing that AuraFusion360 performs better than previous techniques in both quality and accuracy."
                },
                "zh": {
                    "title": "AuraFusion360ï¼šä¸‰ç»´åœºæ™¯ä¿®å¤çš„æ–°çªç ´",
                    "desc": "ä¸‰ç»´åœºæ™¯ä¿®å¤åœ¨è™šæ‹Ÿç°å®å’Œå»ºç­‘å¯è§†åŒ–ç­‰åº”ç”¨ä¸­éå¸¸é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨360åº¦æ— ç•Œåœºæ™¯ä¸­é¢ä¸´è§†å›¾ä¸€è‡´æ€§å’Œå‡ ä½•ç²¾åº¦çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†AuraFusion360ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºå‚è€ƒçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é«˜è´¨é‡çš„3Dåœºæ™¯ä¸­è¿›è¡Œç‰©ä½“ç§»é™¤å’Œå­”å¡«å……ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ·±åº¦æ„ŸçŸ¥çš„æœªè§æ©ç ç”Ÿæˆã€é€‚åº”æ€§å¼•å¯¼æ·±åº¦æ‰©æ•£å’ŒåŸºäºSDEditçš„ç»†èŠ‚å¢å¼ºï¼Œç¡®ä¿å¤šè§†å›¾çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒAuraFusion360åœ¨æ„ŸçŸ¥è´¨é‡å’Œå‡ ä½•ç²¾åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‰§çƒˆè§†è§’å˜åŒ–ä¸­ä¿æŒé«˜è´¨é‡çš„ä¿®å¤æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04520",
            "title": "Linear Correlation in LM's Compositional Generalization and Hallucination",
            "url": "https://huggingface.co/papers/2502.04520",
            "abstract": "The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., \"X lives in the city of\" rightarrow \"X lives in the country of\" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM's generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.",
            "score": 2,
            "issue_id": 2119,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "41ef9027d1533f06",
            "authors": [
                "Letian Peng",
                "Chenyang An",
                "Shibo Hao",
                "Chengyu Dong",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04520.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#architecture",
                    "#agi",
                    "#data",
                    "#hallucinations"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğº Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼Ñƒ. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ."
                },
                "en": {
                    "title": "Unlocking Language Models: The Power of Linear Correlations in Knowledge Composition",
                    "desc": "This paper explores how language models (LMs) handle knowledge composition, revealing that they exhibit linear correlations when predicting the next token. It shows that there is a linear transformation that connects related knowledge, allowing LMs to generalize information effectively, similar to how humans relate concepts. The study finds that while these linear transformations can adapt to new information through fine-tuning, they can also lead to incorrect outputs, or hallucinations, when the relationships are not aligned with reality. Overall, the research suggests that understanding these linear correlations can help identify how well LMs generalize knowledge."
                },
                "zh": {
                    "title": "è¯­è¨€æ¨¡å‹çš„çº¿æ€§ç›¸å…³æ€§ä¸çŸ¥è¯†ç»„åˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨çŸ¥è¯†ç»„åˆä¸­çš„çº¿æ€§ç›¸å…³ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼ŒæŸäº›ç›¸å…³çŸ¥è¯†ä¹‹é—´å­˜åœ¨çº¿æ€§å˜æ¢ï¼Œè¿™ç§å˜æ¢å¯ä»¥å°†ä¸€ä¸ªæç¤ºçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä»ä¸€ä¸ªæ˜ å°„åˆ°å¦ä¸€ä¸ªã€‚æ¯”å¦‚ï¼Œä»\"X ä½åœ¨åŸå¸‚\"å¯ä»¥è½¬å˜ä¸º\"X ä½åœ¨å›½å®¶\"ã€‚ç»“æœè¡¨æ˜ï¼Œçº¿æ€§å˜æ¢åœ¨å¤§è§„æ¨¡å¾®è°ƒä¸­å…·æœ‰éŸ§æ€§ï¼Œå¹¶ä¸”å½“ä¸ç°å®ä¸–ç•Œå…³ç³»ä¸€è‡´æ—¶èƒ½å¤Ÿæ¨å¹¿æ›´æ–°çš„çŸ¥è¯†ï¼Œä½†å½“åç¦»æ—¶åˆ™ä¼šå¯¼è‡´å¹»è§‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04403",
            "title": "Agency Is Frame-Dependent",
            "url": "https://huggingface.co/papers/2502.04403",
            "abstract": "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.",
            "score": 2,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "32ceb8df4d77794a",
            "authors": [
                "David Abel",
                "AndrÃ© Barreto",
                "Michael Bowling",
                "Will Dabney",
                "Shi Dong",
                "Steven Hansen",
                "Anna Harutyunyan",
                "Khimya Khetarpal",
                "Clare Lyle",
                "Razvan Pascanu",
                "Georgios Piliouras",
                "Doina Precup",
                "Jonathan Richens",
                "Mark Rowland",
                "Tom Schaul",
                "Satinder Singh"
            ],
            "affiliations": [
                "Amii, University of Alberta",
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04403.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agi",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ²ÑĞµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ‚ÑÑ‡ĞµÑ‚Ğ°. ĞĞ½Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ¾Ñ‚ Ñ‚ĞµĞ·Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡ĞµÑ‚Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ‚ÑÑ‡ĞµÑ‚Ğ° Ğ² Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Agency in Reinforcement Learning: A Frame-Dependent Perspective",
                    "desc": "This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning."
                },
                "zh": {
                    "title": "èƒ½åŠ¨æ€§ï¼šä¾èµ–äºæ¡†æ¶çš„ç³»ç»Ÿèƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ç³»ç»Ÿçš„èƒ½åŠ¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ çš„èƒŒæ™¯ä¸‹ã€‚èƒ½åŠ¨æ€§æ˜¯æŒ‡ç³»ç»Ÿæœç€ç›®æ ‡å¼•å¯¼ç»“æœçš„èƒ½åŠ›ï¼Œä½†åˆ¤æ–­ä¸€ä¸ªç³»ç»Ÿæ˜¯å¦å…·å¤‡èƒ½åŠ¨æ€§æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œèƒ½åŠ¨æ€§æ˜¯ä¾èµ–äºå‚è€ƒæ¡†æ¶çš„ï¼Œä»»ä½•å¯¹ç³»ç»Ÿèƒ½åŠ¨æ€§çš„æµ‹é‡éƒ½å¿…é¡»ç›¸å¯¹äºæŸä¸ªå‚è€ƒæ¡†æ¶è¿›è¡Œã€‚é€šè¿‡å“²å­¦è®ºè¯ï¼Œæˆ‘ä»¬æ”¯æŒè¿™ä¸€è§‚ç‚¹ï¼Œå¹¶è®¨è®ºäº†è¿™ä¸€ç»“è®ºå¯¹å¼ºåŒ–å­¦ä¹ çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04896",
            "title": "Goku: Flow Based Video Generative Foundation Models",
            "url": "https://huggingface.co/papers/2502.04896",
            "abstract": "This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.",
            "score": 1,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "ad6ef6eed233cc90",
            "authors": [
                "Shoufa Chen",
                "Chongjian Ge",
                "Yuqi Zhang",
                "Yida Zhang",
                "Fengda Zhu",
                "Hao Yang",
                "Hongxiang Hao",
                "Hui Wu",
                "Zhichao Lai",
                "Yifei Hu",
                "Ting-Che Lin",
                "Shilong Zhang",
                "Fu Li",
                "Chuan Li",
                "Xing Wang",
                "Yanghua Peng",
                "Peize Sun",
                "Ping Luo",
                "Yi Jiang",
                "Zehuan Yuan",
                "Bingyue Peng",
                "Xiaobing Liu"
            ],
            "affiliations": [
                "Bytedance Inc",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04896.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#video",
                    "#architecture",
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ‰",
                "ru": {
                    "title": "Goku: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Goku Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Goku Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Goku: Revolutionizing Image and Video Generation with Transformers",
                    "desc": "This paper presents Goku, a cutting-edge model for generating images and videos using rectified flow Transformers. It discusses key components that contribute to its high-quality output, such as the data curation process and the design of the model architecture. Goku sets new performance records in various tasks, achieving impressive scores on benchmarks for both text-to-image and text-to-video generation. The authors aim to offer insights and advancements that will benefit researchers working on similar generation models."
                },
                "zh": {
                    "title": "Gokuï¼šå›¾åƒä¸è§†é¢‘ç”Ÿæˆçš„æ–°æ ‡æ†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Gokuï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„è”åˆå›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨äº†ä¿®æ­£æµTransformerä»¥å®ç°è¡Œä¸šé¢†å…ˆçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¯¦ç»†é˜è¿°äº†é«˜è´¨é‡è§†è§‰ç”Ÿæˆçš„åŸºç¡€è¦ç´ ï¼ŒåŒ…æ‹¬æ•°æ®æ•´ç†æµç¨‹ã€æ¨¡å‹æ¶æ„è®¾è®¡ã€æµçš„å…¬å¼åŒ–ä»¥åŠé«˜æ•ˆç¨³å¥çš„å¤§è§„æ¨¡è®­ç»ƒåŸºç¡€è®¾æ–½ã€‚Gokuæ¨¡å‹åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä¸ºä¸»è¦ä»»åŠ¡è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚å…·ä½“è€Œè¨€ï¼ŒGokuåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­è¾¾åˆ°äº†0.76çš„GenEvalå’Œ83.65çš„DPG-Benchï¼Œåœ¨æ–‡æœ¬åˆ°è§†é¢‘ä»»åŠ¡ä¸­è¾¾åˆ°äº†84.85çš„VBenchã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05171",
            "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
            "url": "https://huggingface.co/papers/2502.05171",
            "abstract": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.",
            "score": 1,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "4386159312d9856b",
            "authors": [
                "Jonas Geiping",
                "Sean McLeish",
                "Neel Jain",
                "John Kirchenbauer",
                "Siddharth Singh",
                "Brian R. Bartoldson",
                "Bhavya Kailkhura",
                "Abhinav Bhatele",
                "Tom Goldstein"
            ],
            "affiliations": [
                "ELLIS Institute TÃ¼bingen, Max-Planck Institute for Intelligent Systems, TÃ¼bingen AI Center",
                "Lawrence Livermore National Laboratory",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05171.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ°, Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ÑÑÑŒ Ğ´Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ½Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ 3,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ 800 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Scaling Reasoning with Latent Space Computation",
                    "desc": "This paper presents a new language model architecture that enhances reasoning capabilities by performing computations in a latent space during test-time. Instead of generating more tokens to scale reasoning, the model uses a recurrent block that allows it to unroll computations to any depth. This method does not rely on specialized training data and can effectively handle small context windows, enabling it to capture complex reasoning patterns. The authors demonstrate that their model, with 3.5 billion parameters, can achieve significant improvements on reasoning tasks, comparable to models with much larger parameter counts."
                },
                "zh": {
                    "title": "éšå¼æ¨ç†ï¼Œæå‡è¯­è¨€æ¨¡å‹çš„è®¡ç®—èƒ½åŠ›",
                    "desc": "æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§æ–°é¢–çš„è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­éšå¼æ¨ç†æ¥æ‰©å±•æµ‹è¯•æ—¶çš„è®¡ç®—èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡è¿­ä»£é€’å½’å—å·¥ä½œï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶å¯ä»¥å±•å¼€åˆ°ä»»æ„æ·±åº¦ã€‚è¿™ä¸ä¸»æµæ¨ç†æ¨¡å‹ä¸åŒï¼Œåè€…é€šè¿‡ç”Ÿæˆæ›´å¤šçš„æ ‡è®°æ¥å¢åŠ è®¡ç®—é‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸éœ€è¦ç‰¹æ®Šçš„è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿå¤„ç†å°çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ•æ‰ä¸æ˜“ç”¨è¯­è¨€è¡¨ç¤ºçš„æ¨ç†ç±»å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04363",
            "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
            "url": "https://huggingface.co/papers/2502.04363",
            "abstract": "We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.",
            "score": 0,
            "issue_id": 2119,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "339b45dee733174c",
            "authors": [
                "Bosung Kim",
                "Kyuhwan Lee",
                "Isu Jeong",
                "Jungmin Cheon",
                "Yeojin Lee",
                "Seulki Lee"
            ],
            "affiliations": [
                "Ulsan National Institute of Science and Technology South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04363.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#low_resource"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ½Ğ° Ğ²Ğ°ÑˆĞµĞ¼ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğµ",
                    "desc": "On-device Sora Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ĞµĞµ Ğ½Ğ° ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Linear Proportional Leap (LPL) Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°, Temporal Dimension Token Merging (TDTM) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ»Ğ¾ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸ Concurrent Inference with Dynamic Loading (CI-DL) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° iPhone 15 Pro Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ On-device Sora ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Open-Sora Ğ½Ğ° Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… GPU."
                },
                "en": {
                    "title": "Empowering Video Creation on Your Smartphone!",
                    "desc": "On-device Sora is a groundbreaking solution for generating videos from text using diffusion models directly on smartphones. It introduces three innovative techniques: Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to optimize token processing in attention layers, and Concurrent Inference with Dynamic Loading (CI-DL) to manage memory efficiently. These advancements allow the system to produce high-quality videos comparable to those generated by powerful GPUs, all while operating within the constraints of mobile devices. This work not only enhances accessibility to advanced generative technologies but also prioritizes user privacy and reduces reliance on cloud services."
                },
                "zh": {
                    "title": "ç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆæ–°çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†On-device Soraï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ç§»åŠ¨è®¾å¤‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨æ™ºèƒ½æ‰‹æœºä¸Šè¿è¡Œã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†ä¸‰ç§æ–°æŠ€æœ¯æ¥è§£å†³ç§»åŠ¨è®¾å¤‡åœ¨è®¡ç®—å’Œå†…å­˜æ–¹é¢çš„é™åˆ¶ã€‚é¦–å…ˆï¼Œçº¿æ€§æ¯”ä¾‹è·³è·ƒï¼ˆLPLï¼‰é€šè¿‡é«˜æ•ˆçš„è·³è·ƒæ–¹æ³•å‡å°‘äº†è§†é¢‘æ‰©æ•£ä¸­æ‰€éœ€çš„å»å™ªæ­¥éª¤ã€‚å…¶æ¬¡ï¼Œæ—¶é—´ç»´åº¦ä»¤ç‰Œåˆå¹¶ï¼ˆTDTMï¼‰é€šè¿‡æ²¿æ—¶é—´ç»´åº¦åˆå¹¶è¿ç»­ä»¤ç‰Œï¼Œé™ä½äº†æ³¨æ„åŠ›å±‚ä¸­å¯†é›†çš„ä»¤ç‰Œå¤„ç†è®¡ç®—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04728",
            "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
            "url": "https://huggingface.co/papers/2502.04728",
            "abstract": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.",
            "score": 0,
            "issue_id": 2119,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "cd97668cd0eee0ee",
            "authors": [
                "Zhouliang Yu",
                "Yuhuan Yuan",
                "Tim Z. Xiao",
                "Fuxiang Frank Xia",
                "Jie Fu",
                "Ge Zhang",
                "Ge Lin",
                "Weiyang Liu"
            ],
            "affiliations": [
                "Environmental Systems Research Institute, Inc.",
                "Max Planck Institute for Intelligent Systems, TÃ¼bingen",
                "SEED, Bytedance",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04728.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ PDDL Ğ¸ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (PDDL) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Best-of-N Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² PDDL Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ."
                },
                "en": {
                    "title": "Enhancing LLMs for Optimal Planning with PDDL",
                    "desc": "This paper addresses the challenge of using Large Language Models (LLMs) for complex planning problems by introducing a method to generate Planning Domain Definition Language (PDDL) domains. PDDL serves as a formal language that helps in creating precise state descriptions, which is crucial for avoiding rule violations and ensuring optimal planning. The authors propose a novel algorithm that enhances LLMs' reasoning capabilities through a Best-of-N sampling approach, followed by fine-grained refinement using verbalized machine learning techniques. Their approach significantly improves the generation of PDDL domains, achieving over 50% success in generating high-quality plans from natural language descriptions without additional training."
                },
                "zh": {
                    "title": "åˆ©ç”¨PDDLæå‡è§„åˆ’é—®é¢˜è§£å†³èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³å¤æ‚çš„è§„åˆ’é—®é¢˜ã€‚ä¸ºäº†é¿å…è§„åˆ™è¿åå’Œç¡®ä¿æœ€ä¼˜æ€§ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰ï¼Œä½œä¸ºä¸€ç§ç²¾ç¡®çš„çŠ¶æ€æè¿°å·¥å…·ã€‚é€šè¿‡PDDLï¼Œå¯ä»¥ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶åº”ç”¨ç»å…¸æœç´¢ç®—æ³•ï¼ˆå¦‚A*ï¼‰æ¥å¯»æ‰¾æœ€ä¼˜è®¡åˆ’ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç®—æ³•ï¼Œé€šè¿‡Best-of-Né‡‡æ ·å’Œç»†è‡´çš„æœºå™¨å­¦ä¹ ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†PDDLé¢†åŸŸçš„ç”Ÿæˆè´¨é‡ï¼ŒæˆåŠŸç‡è¶…è¿‡50%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04404",
            "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
            "url": "https://huggingface.co/papers/2502.04404",
            "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.",
            "score": 0,
            "issue_id": 2118,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "a7bd201755c7ea1d",
            "authors": [
                "Xiao-Wen Yang",
                "Xuan-Yi Zhu",
                "Wen-Da Wei",
                "Ding-Chu Zhang",
                "Jie-Jing Shao",
                "Zhi Zhou",
                "Lan-Zhe Guo",
                "Yu-Feng Li"
            ],
            "affiliations": [
                "National Key Laboratory for Novel Software Technology, Nanjing University, China",
                "School of Artificial Intelligence, Nanjing University, China",
                "School of Intelligence Science and Technology, Nanjing University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04404.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#training",
                    "#inference",
                    "#agents",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ° (self-backtracking) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ Ğ³Ğ´Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒÑÑ Ğ½Ğ°Ğ·Ğ°Ğ´ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Backtracking for Enhanced Reasoning",
                    "desc": "This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å›æº¯æœºåˆ¶ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "å°†æ…¢æ€è€ƒæœºåˆ¶æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œä¸ºå®ç°äºŒçº§AGIæ¨ç†å™¨æä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ã€‚å½“å‰çš„æŒ‘æˆ˜åŒ…æ‹¬ä½æ•ˆçš„è¿‡åº¦æ€è€ƒå’Œå¯¹è¾…åŠ©å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¾èµ–ã€‚æˆ‘ä»¬æŒ‡å‡ºï¼Œè¿™äº›é™åˆ¶æºäºLLMsæ— æ³•å†…åŒ–æœç´¢è¿‡ç¨‹ï¼Œè€Œæœç´¢è¿‡ç¨‹æ˜¯æœ‰æ•ˆæ¨ç†çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘å›æº¯æœºåˆ¶ï¼Œä½¿LLMsèƒ½å¤Ÿåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•å›æº¯ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04350",
            "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
            "url": "https://huggingface.co/papers/2502.04350",
            "abstract": "Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.",
            "score": 0,
            "issue_id": 2118,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "ad7829c09c28de41",
            "authors": [
                "Yongchao Chen",
                "Yilun Hao",
                "Yueying Liu",
                "Yang Zhang",
                "Chuchu Fan"
            ],
            "affiliations": [
                "Harvard University, Boston, MA, USA",
                "MIT-IBM Watson AI Lab, Boston, MA, USA",
                "Massachusetts Institute of Technology, Boston, MA, USA",
                "University of Illinois Urbana-Champaign, Urbana, IL, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04350.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#rlhf",
                    "#open_source",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "CodeSteer: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° LLM Ğ² ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ…",
                    "desc": "CodeSteer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SymBench Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Llama-3-8B Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CodeSteerLLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "CodeSteer: Guiding LLMs to Master Code and Reasoning!",
                    "desc": "This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks."
                },
                "zh": {
                    "title": "CodeSteerï¼šå¼•å¯¼LLMå®ç°ç¬¦å·è®¡ç®—çš„çªç ´",
                    "desc": "ç°æœ‰çš„æ–¹æ³•æ— æ³•æœ‰æ•ˆå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬æ¨ç†å’Œä»£ç ç”Ÿæˆä¹‹é—´åˆ‡æ¢ï¼Œå¯¼è‡´ç¬¦å·è®¡ç®—èƒ½åŠ›æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCodeSteerçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼LLMçš„ä»£ç å’Œæ–‡æœ¬ç”Ÿæˆã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†SymBenchï¼ŒåŒ…å«37ä¸ªå…·æœ‰å¯è°ƒå¤æ‚åº¦çš„ç¬¦å·ä»»åŠ¡ï¼Œå¹¶åˆæˆäº†åŒ…å«1.2ä¸‡å¤šè½®æŒ‡å¯¼/ç”Ÿæˆè½¨è¿¹å’Œ5500å¯¹æŒ‡å¯¼æ¯”è¾ƒçš„æ•°æ®é›†ã€‚é€šè¿‡å¯¹Llama-3-8Bæ¨¡å‹è¿›è¡Œå¤šè½®ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°çš„CodeSteerLLMæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ›´å¤§æ¨¡å‹çš„ä»£ç /æ–‡æœ¬ç”Ÿæˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-07.html",
    "link_next": "2025-02-11.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "07.02",
        "en": "02/07",
        "zh": "2æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 3,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç³»ç»Ÿåœ°æ˜ å°„ç¨€ç–è‡ªç¼–ç å™¨åœ¨å¤§è¯­è¨€æ¨¡å‹çš„è¿ç»­å±‚ä¸­å‘ç°çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨æ— æ•°æ®ä½™å¼¦ç›¸ä¼¼åº¦æŠ€æœ¯ï¼Œè¿½è¸ªç‰¹å®šç‰¹å¾åœ¨æ¯ä¸ªé˜¶æ®µçš„æŒç»­ã€è½¬å˜æˆ–é¦–æ¬¡å‡ºç°ã€‚è¯¥æ–¹æ³•äº§ç”Ÿç‰¹å¾æ¼”å˜çš„ç»†ç²’åº¦æµå›¾ï¼Œæä¾›ç»†ç²’åº¦çš„å¯è§£é‡Šæ€§å’Œæœºåˆ¶æ´å¯Ÿã€‚å…³é”®æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¿™äº›è·¨å±‚ç‰¹å¾å›¾å¦‚ä½•é€šè¿‡æ”¾å¤§æˆ–æŠ‘åˆ¶é€‰å®šç‰¹å¾ï¼Œç›´æ¥å¼•å¯¼æ¨¡å‹è¡Œä¸ºï¼Œå®ç°æ–‡æœ¬ç”Ÿæˆçš„å®šå‘ä¸»é¢˜æ§åˆ¶ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å‘ç°çªæ˜¾äº†å› æœã€è·¨å±‚å¯è§£é‡Šæ€§æ¡†æ¶çš„å®ç”¨æ€§ï¼Œä¸ä»…æ¾„æ¸…äº†ç‰¹å¾é€šè¿‡å‰å‘ä¼ é€’çš„å‘å±•ï¼Œè¿˜æä¾›äº†é€æ˜æ“ä½œå¤§è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚",
        "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
        "pinyin": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç³»ç»Ÿåœ°æ˜ å°„ç¨€ç–è‡ªç¼–ç å™¨åœ¨å¤§è¯­è¨€æ¨¡å‹çš„è¿ç»­å±‚ä¸­å‘ç°çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨æ— æ•°æ®ä½™å¼¦ç›¸ä¼¼åº¦æŠ€æœ¯ï¼Œè¿½è¸ªç‰¹å®šç‰¹å¾åœ¨æ¯ä¸ªé˜¶æ®µçš„æŒç»­ã€è½¬å˜æˆ–é¦–æ¬¡å‡ºç°ã€‚è¯¥æ–¹æ³•äº§ç”Ÿç‰¹å¾æ¼”å˜çš„ç»†ç²’åº¦æµå›¾ï¼Œæä¾›ç»†ç²’åº¦çš„å¯è§£é‡Šæ€§å’Œæœºåˆ¶æ´å¯Ÿã€‚å…³é”®æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¿™äº›è·¨å±‚ç‰¹å¾å›¾å¦‚ä½•é€šè¿‡æ”¾å¤§æˆ–æŠ‘åˆ¶é€‰å®šç‰¹å¾ï¼Œç›´æ¥å¼•å¯¼æ¨¡å‹è¡Œä¸ºï¼Œå®ç°æ–‡æœ¬ç”Ÿæˆçš„å®šå‘ä¸»é¢˜æ§åˆ¶ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å‘ç°çªæ˜¾äº†å› æœã€è·¨å±‚å¯è§£é‡Šæ€§æ¡†æ¶çš„å®ç”¨æ€§ï¼Œä¸ä»…æ¾„æ¸…äº†ç‰¹å¾é€šè¿‡å‰å‘ä¼ é€’çš„å‘å±•ï¼Œè¿˜æä¾›äº†é€æ˜æ“ä½œå¤§è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚\n\nWÇ’men jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n fÄngfÇ, xÃ¬tÇ’ng de yÃ¬ngshÃ¨ xÄ«shÅ« zÃ¬biÄnmÇqÃ¬ zÃ i dÃ  yÇ”yÃ¡n mÃ³xÃ­ng de liÃ¡nxÃ¹ cÃ©ng zhÅng fÄxiÃ n de tÃ¨zhÄ“ng. WÇ’men shÇyÃ²ng wÃº shÃ¹jÃ¹ yÃºxiÃ n xiÃ ngsÃ¬dÃ¹ jÃ¬shÃ¹, zhuÄ«zÅng tÃ¨dÃ¬ng tÃ¨zhÄ“ng zÃ i mÄ›i ge jiÄ“duÃ n de chÃ­xÃ¹, zhuÇnbiÃ n huÃ² shÇ’ucÃ¬ chÅ«xiÃ n. GÇi fÄngfÇ chÇnshÄ“ng tÃ¨zhÄ“ng yÇnbiÃ n de xÃ¬lÃ¬dÃ¹ liÃº tÃº, tÃ­gÅng xÃ¬lÃ¬dÃ¹ de kÄ› jiÄ›shÃ¬xÃ¬ng hÃ© jÄ«zhÃ¬ dÃ²ngchÃ . GuÇnjiÃ n shÃ¬, wÇ’men zhÇnshÃ¬ le zhÃ¨xiÄ“ kuÃ cÃ©ng tÃ¨zhÄ“ng tÃº rÃºhÃ© tÅngguÃ² fÃ ngdÃ  huÃ² yÃ¬zhÃ¬ xuÇndÃ¬ng tÃ¨zhÄ“ng, zhÃ­jiÄ“ yÇndÇo mÃ³xÃ­ng xÃ­ngwÃ©i, shÃ­xiÃ n wÃ©nbÄ›n shÄ“ngchÃ©ng de dÃ¬ngxiÃ ng zhÇ”tÃ­ kÃ²ngzhÃ¬. ZÇ’ng de lÃ¡i shuÅ, wÇ’men de fÄxiÃ n tÅ«xÃ¬ le yÄ«nguÇ’, kuÃ cÃ©ng kÄ› jiÄ›shÃ¬xÃ¬ng kuÄngjiÃ  de shÃ­yÃ²ngxÃ¬ng, bÃ¹jÇn chÃ©ngqÄ«ng le tÃ¨zhÄ“ng tÅngguÃ² qiÃ¡nxiÃ ng chuÃ¡ndÃ¬ de fÄzhÇn, hÃ¡i tÃ­gÅng le tÃ²umÃ­ng cÄozuÃ² dÃ  yÇ”yÃ¡n mÃ³xÃ­ng de xÄ«n fÄngfÇ.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"ç³»ç»Ÿåœ°\", \"pinyin\": \"xÃ¬ tÇ’ng de\", \"trans\": \"systematically\"},\n    {\"word\": \"æ˜ å°„\", \"pinyin\": \"yÃ¬ng shÃ¨\", \"trans\": \"map\"},\n    {\"word\": \"ç¨€ç–\", \"pinyin\": \"xÄ« shÅ«\", \"trans\": \"sparse\"},\n    {\"word\": \"è‡ªç¼–ç å™¨\", \"pinyin\": \"zÃ¬ biÄn mÇ qÃ¬\", \"trans\": \"autoencoder\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"è¿ç»­å±‚\", \"pinyin\": \"liÃ¡n xÃ¹ cÃ©ng\", \"trans\": \"continuous layer\"},\n    {\"word\": \"å‘ç°\", \"pinyin\": \"fÄ xiÃ n\", \"trans\": \"discover\"},\n    {\"word\": \"ç‰¹å¾\", \"pinyin\": \"tÃ¨ zhÄ“ng\", \"trans\": \"feature\"},\n    {\"word\": \"æ— æ•°æ®\", \"pinyin\": \"wÃº shÃ¹ jÃ¹\", \"trans\": \"data-free\"},\n    {\"word\": \"ä½™å¼¦ç›¸ä¼¼åº¦\", \"pinyin\": \"yÃº xiÃ n xiÄng sÃ¬ dÃ¹\", \"trans\": \"cosine similarity\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬ shÃ¹\", \"trans\": \"technique\"},\n    {\"word\": \"è¿½è¸ª\", \"pinyin\": \"zhuÄ« zÅng\", \"trans\": \"track\"},\n    {\"word\": \"æŒç»­\", \"pinyin\": \"chÃ­ xÃ¹\", \"trans\": \"persist\"},\n    {\"word\": \"è½¬å˜\", \"pinyin\": \"zhuÇn biÃ n\", \"trans\": \"transform\"},\n    {\"word\": \"é¦–æ¬¡\", \"pinyin\": \"shÇ’u cÃ¬\", \"trans\": \"first time\"},\n    {\"word\": \"å‡ºç°\", \"pinyin\": \"chÅ« xiÃ n\", \"trans\": \"appear\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"äº§ç”Ÿ\", \"pinyin\": \"chÇn shÄ“ng\", \"trans\": \"generate\"},\n    {\"word\": \"æ¼”å˜\", \"pinyin\": \"yÇn biÃ n\", \"trans\": \"evolution\"},\n    {\"word\": \"ç»†ç²’åº¦\", \"pinyin\": \"xÃ¬ lÃ¬ dÃ¹\", \"trans\": \"fine-grained\"},\n    {\"word\": \"æµå›¾\", \"pinyin\": \"liÃº tÃº\", \"trans\": \"flow chart\"},\n    {\"word\": \"æä¾›\", \"pinyin\": \"tÃ­ gÅng\", \"trans\": \"provide\"},\n    {\"word\": \"å¯è§£é‡Šæ€§\", \"pinyin\": \"kÄ› jiÄ› shÃ¬ xÃ¬ng\", \"trans\": \"interpretability\"},\n    {\"word\": \"æœºåˆ¶\", \"pinyin\": \"jÄ« zhÃ¬\", \"trans\": \"mechanism\"},\n    {\"word\": \"æ´å¯Ÿ\", \"pinyin\": \"dÃ²ng chÄ\", \"trans\": \"insight\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇn jiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"è·¨å±‚\", \"pinyin\": \"kuÃ  cÃ©ng\", \"trans\": \"cross-layer\"},\n    {\"word\": \"ç‰¹å¾å›¾\", \"pinyin\": \"tÃ¨ zhÄ“ng tÃº\", \"trans\": \"feature map\"},\n    {\"word\": \"æ”¾å¤§\", \"pinyin\": \"fÃ ng dÃ \", \"trans\": \"amplify\"},\n    {\"word\": \"æŠ‘åˆ¶\", \"pinyin\": \"yÃ¬ zhÃ¬\", \"trans\": \"suppress\"},\n    {\"word\": \"é€‰å®š\", \"pinyin\": \"xuÇn dÃ¬ng\", \"trans\": \"select\"},\n    {\"word\": \"å¼•å¯¼\", \"pinyin\": \"yÇn dÇo\", \"trans\": \"guide\"},\n    {\"word\": \"æ¨¡å‹è¡Œä¸º\", \"pinyin\": \"mÃ³ xÃ­ng xÃ­ng wÃ©i\", \"trans\": \"model behavior\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"æ–‡æœ¬ç”Ÿæˆ\", \"pinyin\": \"wÃ©n bÄ›n shÄ“ng chÃ©ng\", \"trans\": \"text generation\"},\n    {\"word\": \"å®šå‘\", \"pinyin\": \"dÃ¬ng xiÃ ng\", \"trans\": \"directed\"},\n    {\"word\": \"ä¸»é¢˜æ§åˆ¶\", \"pinyin\": \"zhÇ” tÃ­ kÃ²ng zhÃ¬\", \"trans\": \"theme control\"},\n    {\"word\": \"æ€»çš„æ¥è¯´\", \"pinyin\": \"zÇ’ng de lÃ¡i shuÅ\", \"trans\": \"in summary\"},\n    {\"word\": \"å‘ç°\", \"pinyin\": \"fÄ xiÃ n\", \"trans\": \"discover\"},\n    {\"word\": \"çªæ˜¾\", \"pinyin\": \"tÅ« xiÇn\", \"trans\": \"highlight\"},\n    {\"word\": \"å› æœ\", \"pinyin\": \"yÄ«n guÇ’\", \"trans\": \"causal\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"å®ç”¨æ€§\", \"pinyin\": \"shÃ­ yÃ²ng xÃ¬ng\", \"trans\": \"practicality\"},\n    {\"word\": \"æ¾„æ¸…\", \"pinyin\": \"chÃ©ng qÄ«ng\", \"trans\": \"clarify\"},\n    {\"word\": \"å‰å‘ä¼ é€’\", \"pinyin\": \"qiÃ¡n xiÃ ng chuÃ¡n dÃ¬\", \"trans\": \"forward propagation\"},\n    {\"word\": \"å‘å±•\", \"pinyin\": \"fÄ zhÇn\", \"trans\": \"develop\"},\n    {\"word\": \"æä¾›\", \"pinyin\": \"tÃ­ gÅng\", \"trans\": \"provide\"},\n    {\"word\": \"é€æ˜\", \"pinyin\": \"tÃ²u mÃ­ng\", \"trans\": \"transparent\"},\n    {\"word\": \"æ“ä½œ\", \"pinyin\": \"cÄo zuÃ²\", \"trans\": \"operate\"},\n    {\"word\": \"æ–°æ–¹æ³•\", \"pinyin\": \"xÄ«n fÄng fÇ\", \"trans\": \"new method\"}\n]",
        "trans": "We introduce a new method that systematically maps the features discovered by sparse autoencoders in the continuous layers of large language models. Utilizing data-free cosine similarity techniques, we track the persistence, transformation, or initial appearance of specific features at each stage. This method generates fine-grained flow maps of feature evolution, offering detailed interpretability and mechanistic insights. Crucially, we demonstrate how these cross-layer feature maps directly guide model behavior by amplifying or suppressing selected features, achieving directed thematic control over text generation. Overall, our findings highlight the practicality of a causal, cross-layer interpretability framework, not only clarifying the development of features through forward propagation but also providing new methods for transparently operating large language models.",
        "update_ts": "2025-02-09 12:37"
    }
}