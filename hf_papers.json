{
    "date": {
        "ru": "2 октября",
        "en": "October 2",
        "zh": "10月2日"
    },
    "time_utc": "2025-10-02 06:16",
    "weekday": 3,
    "issue_id": 6202,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.25454",
            "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
            "url": "https://huggingface.co/papers/2509.25454",
            "abstract": "DeepSearch integrates Monte Carlo Tree Search into RLVR training to enhance exploration and credit assignment, achieving state-of-the-art performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.",
            "score": 45,
            "issue_id": 6201,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "c81bb4fe47e669e8",
            "authors": [
                "Fang Wu",
                "Weihao Xuan",
                "Heli Qi",
                "Ximing Lu",
                "Aaron Tu",
                "Li Erran Li",
                "Yejin Choi"
            ],
            "affiliations": [
                "Amazon AWS",
                "RIKEN AIP",
                "Stanford University",
                "UC Berkeley",
                "University of Tokyo",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25454.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "Систематический поиск вместо грубой силы: древовидное исследование для обучения LLM",
                    "desc": "DeepSearch интегрирует Monte Carlo Tree Search непосредственно в процесс обучения с подкреплением (RLVR) для улучшения исследования пространства решений и распределения кредита. Метод решает проблему плато в обучении, возникающую из-за недостаточного исследования возможных путей рассуждений в существующих подходах RLVR. DeepSearch использует глобальную стратегию выбора перспективных узлов, энтропийное руководство для выбора уверенных путей и адаптивный буфер воспроизведения с кэшированием решений. На бенчмарках математических рассуждений модель достигла точности 62.95% для моделей размером 1.5B параметров, используя в 5.7 раз меньше вычислительных ресурсов по сравнению с продолжительным обучением."
                },
                "en": {
                    "title": "Revolutionizing RLVR: Strategic Exploration with DeepSearch",
                    "desc": "DeepSearch is a novel framework that enhances Reinforcement Learning with Value Regression (RLVR) by incorporating Monte Carlo Tree Search (MCTS) into the training process. This integration allows for better exploration of the solution space and improves credit assignment, addressing the common issue of training plateaus in existing RLVR methods. By employing a global frontier selection strategy and entropy-based guidance, DeepSearch systematically identifies and prioritizes promising reasoning paths. The framework achieves state-of-the-art performance on mathematical reasoning tasks while significantly reducing computational costs, demonstrating the effectiveness of strategic exploration in machine learning."
                },
                "zh": {
                    "title": "深度搜索：通过系统探索提升推理能力",
                    "desc": "DeepSearch 是一种将蒙特卡洛树搜索（MCTS）集成到强化学习价值回归（RLVR）训练中的框架，旨在增强探索能力和信用分配。当前的 RLVR 方法在训练过程中存在探索不足的问题，导致性能提升减缓。DeepSearch 通过在训练循环中嵌入结构化搜索，系统性地探索解决方案空间，从而解决了这一瓶颈。实验结果表明，DeepSearch 在数学推理基准测试中取得了 62.95% 的平均准确率，并且使用的 GPU 计算时间比传统方法少了 5.7 倍，展示了算法创新在提升 RLVR 方法中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01051",
            "title": "GEM: A Gym for Agentic LLMs",
            "url": "https://huggingface.co/papers/2510.01051",
            "abstract": "GEM, an open-source environment simulator, facilitates experience-based learning for large language models by providing a standardized framework and diverse environments for training and benchmarking reinforcement learning algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.",
            "score": 30,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "6d69bb75ee0b2258",
            "authors": [
                "Zichen Liu",
                "Anya Sims",
                "Keyu Duan",
                "Changyu Chen",
                "Simon Yu",
                "Xiangxin Zhou",
                "Haotian Xu",
                "Shaopan Xiong",
                "Bo Liu",
                "Chenmien Tan",
                "Chuen Yang Beh",
                "Weixun Wang",
                "Hao Zhu",
                "Weiyan Shi",
                "Diyi Yang",
                "Michael Shieh",
                "Yee Whye Teh",
                "Wee Sun Lee",
                "Min Lin"
            ],
            "affiliations": [
                "NUS",
                "Northeastern",
                "OpenRLHF",
                "Oxford",
                "RL2",
                "ROLL",
                "SMU",
                "Sea AI Lab",
                "Stanford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01051.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#open_source",
                    "#benchmark",
                    "#training",
                    "#games",
                    "#agents"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "GEM: спортзал для тренировки LLM-агентов через reinforcement learning",
                    "desc": "В статье представлен GEM (General Experience Maker) — открытая среда-симулятор для обучения больших языковых моделей через взаимодействие с окружением. Это аналог OpenAI Gym, но специально разработанный для эпохи LLM, предоставляющий стандартизированный интерфейс между агентом и средой с поддержкой асинхронного векторизованного выполнения. GEM включает 24 разнообразные среды и базовые результаты с использованием алгоритма REINFORCE с Return Batch Normalization (ReBN), который лучше справляется с credit assignment по сравнению с GRPO. Авторы также проводят сравнительный анализ популярных RL-алгоритмов (PPO, GRPO, REINFORCE) и позиционируют GEM как инструмент для ускорения исследований агентных LLM."
                },
                "en": {
                    "title": "GEM: Empowering LLMs with Experience-Based Learning",
                    "desc": "GEM (General Experience Maker) is an open-source simulator designed to enhance experience-based learning for large language models (LLMs) by providing a standardized framework for training and benchmarking reinforcement learning (RL) algorithms. It allows agents to learn by interacting with various complex environments, moving away from static datasets. GEM includes features like asynchronous vectorized execution for efficient processing and flexible wrappers for easy customization. Additionally, it offers a suite of environments and tools for evaluating different RL algorithms, aiming to accelerate research in agentic LLMs."
                },
                "zh": {
                    "title": "GEM：加速大型语言模型的经验学习",
                    "desc": "GEM（通用经验生成器）是一个开源环境模拟器，旨在为大型语言模型提供基于经验的学习体验。它为强化学习算法的训练和基准测试提供了标准化框架和多样化环境，类似于传统强化学习中的OpenAI-Gym。GEM支持异步向量化执行，具有高吞吐量，并提供灵活的包装器以便于扩展。此外，GEM还包含多种环境、强大的集成工具和示例脚本，帮助研究人员加速未来的智能体语言模型研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25849",
            "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget\n  Allocation",
            "url": "https://huggingface.co/papers/2509.25849",
            "abstract": "An adaptive exploration budget allocation method for reinforcement learning in Large Language Models improves training efficiency and performance on mathematical reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an \"item\" with a distinct \"value\" and \"cost\", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational \"free lunch\", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.",
            "score": 16,
            "issue_id": 6201,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "7897a4d3a9007e61",
            "authors": [
                "Ziniu Li",
                "Congliang Chen",
                "Tianyun Yang",
                "Tian Ding",
                "Ruoyu Sun",
                "Ge Zhang",
                "Wenhao Huang",
                "Zhi-Quan Luo"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Shenzhen Research Institute of Big Data",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25849.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "🎒",
                "ru": {
                    "title": "Умное распределение вычислений: больше внимания сложным задачам",
                    "desc": "Исследователи предложили адаптивный метод распределения вычислительного бюджета для обучения LLM с подкреплением. Вместо равномерного выделения ресурсов на все задачи, алгоритм динамически распределяет их в зависимости от сложности: простые задачи получают меньше попыток, сложные — больше. Подход формализован как классическая задача о рюкзаке, где каждая задача имеет свою «ценность» и «стоимость». Метод повышает долю ненулевых градиентов на 20-40% и улучшает результаты на математических бенчмарках на 2-9 баллов при тех же вычислительных затратах."
                },
                "en": {
                    "title": "Smart Budgeting for Smarter Learning",
                    "desc": "This paper presents a new method for allocating exploration budgets in reinforcement learning, specifically for Large Language Models (LLMs). The authors identify that traditional uniform budget allocation leads to inefficiencies, where easy tasks succeed while difficult ones fail, resulting in zero gradients during training. By framing the exploration budget allocation as a knapsack problem, they develop an adaptive strategy that optimally distributes resources based on the learning status of each task. This approach significantly enhances training efficiency, increasing non-zero policy gradients by 20-40% and improving performance on mathematical reasoning tasks without requiring additional computational resources."
                },
                "zh": {
                    "title": "自适应探索预算，提升强化学习效率",
                    "desc": "本文提出了一种自适应的探索预算分配方法，用于强化学习中的大型语言模型（LLMs），以提高训练效率和数学推理基准的性能。传统方法在每个任务上均匀分配有限的探索预算，导致简单任务总是成功而困难任务总是失败，造成训练更新时梯度为零的问题。我们将每个任务的探索视为具有不同“价值”和“成本”的“物品”，并与经典的背包问题建立联系，从而推导出一种最佳分配规则。通过将资源动态分配到学习效果最显著的任务上，我们的方法在训练中有效地提高了非零策略梯度的比例，并在数学推理基准上实现了显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01174",
            "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
            "url": "https://huggingface.co/papers/2510.01174",
            "abstract": "Code2Video generates educational videos using a code-centric agent framework, improving coherence and interpretability compared to direct code generation.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.",
            "score": 15,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "26c2c9dd6c370251",
            "authors": [
                "Yanzhe Chen",
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01174.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#video",
                    "#games",
                    "#optimization",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "Генерация обучающих видео через программный код",
                    "desc": "Code2Video — это фреймворк на основе агентов для создания образовательных видео через исполняемый Python-код. Система использует трёх совместно работающих агентов: планировщик структурирует контент лекции, программист преобразует инструкции в код с автоматическим исправлением ошибок, а критик на основе vision-language моделей улучшает визуальную компоновку. Для оценки качества предложена метрика TeachQuiz, которая измеряет, насколько хорошо LLM может восстановить знания после просмотра сгенерированного видео. Подход показывает улучшение на 40% по сравнению с прямой генерацией кода и создаёт видео, сопоставимые с профессиональными обучающими материалами."
                },
                "en": {
                    "title": "Code2Video: Crafting Coherent Educational Videos with Code",
                    "desc": "Code2Video is a framework designed to create educational videos using a code-centric approach, which enhances coherence and interpretability compared to traditional methods. It consists of three main agents: the Planner organizes content into logical sequences, the Coder translates these sequences into executable Python code, and the Critic refines the visual layout using vision-language models. This method addresses the challenges of generating professional educational videos by allowing precise control over visual elements and transitions. The framework has shown a significant improvement in video quality and coherence, outperforming direct code generation by 40%."
                },
                "zh": {
                    "title": "Code2Video：教育视频生成的新方法",
                    "desc": "Code2Video 是一个基于代码的代理框架，用于生成教育视频，提升了视频的一致性和可解释性。该框架包含三个协作代理：规划者负责将讲座内容结构化并准备视觉资产；编码器将结构化指令转换为可执行的 Python 代码，并通过范围引导自动修复提高效率；评论者利用视觉语言模型优化空间布局，确保清晰度。通过建立专业制作的教育视频基准MMMC，我们评估了Code2Video在美学评分、代码效率和知识恢复等多个维度的表现，结果显示其在视频生成上优于直接代码生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00406",
            "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified\n  Rewards in World Simulators",
            "url": "https://huggingface.co/papers/2510.00406",
            "abstract": "VLA-RFT uses a data-driven world model to fine-tune VLA models efficiently, reducing sample requirements and improving robustness under perturbations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.",
            "score": 12,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "8dead5a43b0cdc61",
            "authors": [
                "Hengtao Li",
                "Pengxiang Ding",
                "Runze Suo",
                "Yihao Wang",
                "Zirui Ge",
                "Dongyuan Zang",
                "Kexian Yu",
                "Mingyang Sun",
                "Hongyin Zhang",
                "Donglin Wang",
                "Weihua Su"
            ],
            "affiliations": [
                "BUPT",
                "Fudan University",
                "Hebei University of Technology",
                "OpenHelix Team",
                "Westlake University",
                "Zhejiang University",
                "Zhengzhou University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00406.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение робота через мир-симулятор: эффективно и надёжно",
                    "desc": "Статья представляет VLA-RFT — метод дообучения Vision-Language-Action моделей с помощью reinforcement learning. Вместо дорогостоящих взаимодействий с реальным миром используется data-driven world model, которая предсказывает будущие визуальные наблюдения на основе действий агента. Метод требует менее 400 шагов fine-tuning для достижения результатов лучше, чем supervised baseline, и показывает высокую устойчивость к возмущениям. VLA-RFT решает проблему накопления ошибок в imitation learning и предлагает практичный подход к улучшению генерализации роботизированных политик."
                },
                "en": {
                    "title": "Enhancing VLA Models with Efficient Reinforcement Fine-Tuning",
                    "desc": "VLA-RFT is a framework that improves Vision-Language-Action (VLA) models by using a data-driven world model for reinforcement fine-tuning. This approach reduces the number of samples needed for training and enhances the model's ability to handle unexpected changes in the environment. By simulating future visual observations based on actions, it provides a more effective learning signal that aligns with the desired outcomes. The results show that VLA-RFT not only outperforms traditional supervised methods but also maintains strong performance even when conditions are altered."
                },
                "zh": {
                    "title": "利用世界模型提升VLA模型的鲁棒性与效率",
                    "desc": "VLA-RFT是一种强化学习微调框架，利用数据驱动的世界模型作为可控模拟器，从而提高VLA模型的效率。该框架通过真实交互数据训练，能够预测基于动作的未来视觉观察，提供密集的轨迹级奖励信号。与传统的监督学习方法相比，VLA-RFT在样本需求上大幅降低，且在少于400步的微调后超越了强大的基线模型。它在扰动条件下表现出强大的鲁棒性，确保任务执行的稳定性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00184",
            "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
            "url": "https://huggingface.co/papers/2510.00184",
            "abstract": "Reverse-engineering a model that learns multi-digit multiplication via implicit chain-of-thought reveals that it uses attention to encode long-range dependencies and represents partial products efficiently, insights that help address limitations in standard fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via implicit chain-of-thought, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.",
            "score": 10,
            "issue_id": 6198,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "27d7ce536d31aa04",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "🔢",
                "ru": {
                    "title": "Как нейросети учатся умножать: разгадка механизма длинных зависимостей",
                    "desc": "Исследователи провели обратную инженерию модели, которая научилась многозначному умножению через неявную цепочку рассуждений (chain-of-thought). Оказалось, что модель использует механизм attention для построения направленного ациклического графа, кэширующего промежуточные произведения, и представляет числа через базис Фурье и суммы Минковского. Стандартное fine-tuning застревает в локальном оптимуме из-за неспособности уловить необходимые длинные зависимости между разрядами чисел. Добавление вспомогательной функции потерь для предсказания промежуточных сумм создаёт правильное индуктивное смещение и позволяет модели успешно освоить умножение."
                },
                "en": {
                    "title": "Unlocking Multi-Digit Multiplication with Attention and Inductive Bias",
                    "desc": "This paper investigates how a model learns to perform multi-digit multiplication using an implicit chain-of-thought approach. It reveals that the model effectively encodes long-range dependencies through attention mechanisms, allowing it to manage partial products efficiently. The authors demonstrate that standard fine-tuning methods often fail to capture these dependencies, leading to suboptimal performance. By introducing an auxiliary loss that predicts running sums, they provide a solution to enhance learning dynamics and improve the model's ability to handle complex multiplication tasks."
                },
                "zh": {
                    "title": "揭示多位数乘法学习的关键机制",
                    "desc": "本研究通过逆向工程一个成功学习多位数乘法的模型，揭示了其使用注意力机制编码长距离依赖关系的方式。研究发现，该模型通过构建有向无环图来缓存和检索成对的部分积，从而有效地表示部分积。模型在注意力头中通过形成闵可夫斯基和来实现部分积，并使用傅里叶基表示数字，这些都是标准微调模型所缺乏的直观且高效的表示方式。通过引入辅助损失来预测“运行和”，我们为模型提供了一个归纳偏置，使其能够成功学习多位数乘法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00526",
            "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum",
            "url": "https://huggingface.co/papers/2510.00526",
            "abstract": "Research identifies probability-based objectives that outperform negative log likelihood for fine-tuning large language models, depending on model capability.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., -p, -p^{10}, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
            "score": 7,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "9f43fe314cbe69af",
            "authors": [
                "Gaotang Li",
                "Ruizhong Qiu",
                "Xiusi Chen",
                "Heng Ji",
                "Hanghang Tong"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00526.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Выбор функции потерь зависит от силы модели",
                    "desc": "Исследование показывает, что стандартная функция потерь negative log likelihood (NLL) не всегда оптимальна для файн-тюнинга больших языковых моделей. Авторы изучили семейство вероятностных функций потерь и обнаружили критическую закономерность: эффективность функции зависит от capability модели. Для сильных моделей лучше работают функции, которые снижают вес токенов с низкой вероятностью (например, -p или -p^10), для слабых моделей эффективнее остаётся классический NLL. Эксперименты на 7 архитектурах, 14 бенчмарках и 3 доменах подтверждают существование континуума model-capability, который определяет выбор оптимальной функции потерь."
                },
                "en": {
                    "title": "Optimizing Fine-Tuning: Beyond Negative Log Likelihood",
                    "desc": "This paper explores how different training objectives can improve the fine-tuning of large language models (LLMs) beyond the traditional negative log likelihood (NLL). It identifies that NLL may not be optimal for models that have already been pre-trained, as they possess inherent task-relevant knowledge. The authors propose a range of probability-based objectives that adapt to the model's capability, showing that stronger models benefit from objectives that prioritize high-probability tokens. Through extensive experiments, they demonstrate that the effectiveness of these objectives varies along a continuum of model strength, providing insights into how to select the best training objective based on model performance."
                },
                "zh": {
                    "title": "超越负对数似然的微调目标",
                    "desc": "本研究探讨了基于概率的目标函数在微调大型语言模型时的表现，发现其在不同模型能力下优于负对数似然（NLL）。传统的监督微调方法常常受限于NLL这一训练目标，而在后训练阶段，模型已经具备了任务相关的先验知识。我们通过大量实验和消融研究，揭示了模型能力的连续性对目标函数表现的影响。在强模型端，倾向于先验的目标函数表现优于NLL，而在弱模型端则是NLL占优，提供了根据模型能力调整目标函数的理论基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00232",
            "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model\n  Responses",
            "url": "https://huggingface.co/papers/2510.00232",
            "abstract": "BiasFreeBench evaluates bias mitigation techniques in large language models using a unified benchmark and response-level metric to ensure fair and safe outputs in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.",
            "score": 7,
            "issue_id": 6198,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "7a6ed8974cc83369",
            "authors": [
                "Xin Xu",
                "Xunzhi He",
                "Churan Zhi",
                "Ruizhe Chen",
                "Julian McAuley",
                "Zexue He"
            ],
            "affiliations": [
                "Columbia University",
                "MIT-IBM Watson Lab",
                "UC San Diego",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00232.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Единый бенчмарк для честной оценки методов борьбы с предвзятостью в LLM",
                    "desc": "Существующие исследования методов устранения предвзятости в больших языковых моделях используют разные базовые подходы и метрики, что затрудняет их сравнение. В работе представлен BiasFreeBench — унифицированный бенчмарк, который сравнивает восемь основных техник устранения bias (четыре на основе промптинга и четыре на основе обучения) в реальных сценариях взаимодействия с пользователями. Авторы вводят метрику уровня ответов Bias-Free Score, которая измеряет справедливость, безопасность и антистереотипность ответов LLM. Бенчмарк систематически сравнивает эффективность различных методов с учётом парадигмы (промптинг vs обучение), размера модели и способности обобщаться на новые типы предвзятости."
                },
                "en": {
                    "title": "Unifying Bias Mitigation Evaluation for Safer AI Outputs",
                    "desc": "BiasFreeBench is a new benchmark designed to evaluate bias mitigation techniques in large language models (LLMs). It addresses the inconsistency in previous studies by providing a unified framework for comparing various debiasing methods. The benchmark includes a novel response-level metric called Bias-Free Score, which assesses the fairness and safety of model outputs in real-world scenarios. By systematically analyzing different debiasing strategies, BiasFreeBench aims to enhance the reliability of LLMs in producing equitable and safe responses."
                },
                "zh": {
                    "title": "统一评估偏见缓解技术的基准工具",
                    "desc": "BiasFreeBench 是一个评估大型语言模型偏见缓解技术的基准工具，旨在确保模型输出在现实场景中公平和安全。该研究通过统一的查询-响应设置，比较了八种主流的偏见缓解方法，包括四种基于提示和四种基于训练的方法。我们引入了一个新的响应级别指标——无偏分数，来衡量模型响应的公平性、安全性和反刻板印象程度。该基准的发布将为偏见缓解研究提供一个统一的测试平台。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25301",
            "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel\n  Execution",
            "url": "https://huggingface.co/papers/2509.25301",
            "abstract": "Flash-Searcher, a parallel agent reasoning framework using directed acyclic graphs, enhances efficiency and performance in complex reasoning tasks by enabling concurrent execution and dynamic workflow optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while reducing agent execution steps by up to 35% compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.",
            "score": 6,
            "issue_id": 6200,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "6daa8de407c2b924",
            "authors": [
                "Tianrui Qin",
                "Qianben Chen",
                "Sinuo Wang",
                "He Xing",
                "King Zhu",
                "He Zhu",
                "Dingfeng Shi",
                "Xinxin Liu",
                "Ge Zhang",
                "Jiaheng Liu",
                "Yuchen Eleanor Jiang",
                "Xitong Gao",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "OPPO",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25301.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Параллельное мышление агентов через графы зависимостей",
                    "desc": "В статье представлен Flash-Searcher — новый фреймворк для параллельного рассуждения AI-агентов, основанный на направленных ациклических графах (DAG) вместо традиционной последовательной обработки. Система разбивает сложные задачи на подзадачи с явными зависимостями, что позволяет выполнять независимые цепочки рассуждений параллельно при сохранении логических связей. Flash-Searcher показывает точность 67.7% на BrowseComp и 83% на xbench-DeepSearch, сокращая количество шагов выполнения на 35% по сравнению с существующими подходами. Метод также успешно применяется для дистилляции параллельного рассуждения в отдельные модели, демонстрируя улучшение производительности на различных архитектурах."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with Parallel Execution",
                    "desc": "Flash-Searcher is a new framework designed to improve the efficiency of reasoning tasks in artificial intelligence. It uses directed acyclic graphs (DAGs) to allow multiple reasoning paths to be executed at the same time, rather than one after the other. This approach not only speeds up the process but also optimizes the workflow dynamically based on the results obtained during execution. The framework has shown significant performance improvements in various benchmarks, demonstrating its effectiveness over traditional sequential processing methods."
                },
                "zh": {
                    "title": "Flash-Searcher：高效的并行推理框架",
                    "desc": "Flash-Searcher 是一种新的并行智能体推理框架，使用有向无环图（DAG）来提高复杂推理任务的效率和性能。它通过将复杂任务分解为具有明确依赖关系的子任务，允许独立推理路径的并发执行，同时保持逻辑约束。该框架通过动态工作流优化，基于中间结果不断改进执行图，有效整合了摘要模块。综合评估显示，Flash-Searcher 在多个基准测试中表现优异，显著提高了推理效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00536",
            "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
            "url": "https://huggingface.co/papers/2510.00536",
            "abstract": "GUI-KV, a KV cache compression method for GUI agents, improves efficiency by exploiting spatial and temporal redundancies, reducing computational cost while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.",
            "score": 3,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "6687d7079b2d54e2",
            "authors": [
                "Kung-Hsiang Huang",
                "Haoyi Qiu",
                "Yutong Dai",
                "Caiming Xiong",
                "Chien-Sheng Wu"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00536.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Сжатие памяти для AI-агентов в графических интерфейсах",
                    "desc": "Исследователи представили GUI-KV — метод сжатия KV-кэша для AI-агентов, работающих с графическими интерфейсами. Метод использует две техники: пространственную оценку важности визуальных токенов через L2-норму скрытых состояний и удаление избыточной информации между последовательными скриншотами. В отличие от обработки естественных изображений, внимание в GUI равномерно разрежено во всех слоях трансформера, что позволяет использовать единую стратегию распределения бюджета памяти. На бенчмарке AgentNetBench метод снижает вычислительные затраты на 38.9% при улучшении точности на 4.1%, не требуя дополнительного обучения модели."
                },
                "en": {
                    "title": "Efficient GUI Agents with GUI-KV Cache Compression",
                    "desc": "The paper presents GUI-KV, a method for compressing key-value (KV) caches specifically designed for graphical user interface (GUI) agents. It addresses the inefficiencies in processing high-resolution screenshots by leveraging spatial and temporal redundancies, which reduces computational costs while maintaining accuracy. The authors analyze attention patterns in GUI workloads and propose a uniform budget allocation strategy that outperforms more complex methods. By introducing techniques like spatial saliency guidance and temporal redundancy scoring, GUI-KV achieves significant improvements in efficiency and accuracy across various benchmarks."
                },
                "zh": {
                    "title": "高效的GUI代理缓存压缩方法",
                    "desc": "GUI-KV是一种针对图形用户界面（GUI）代理的键值（KV）缓存压缩方法，通过利用空间和时间冗余来提高效率。该方法在处理高分辨率截图和长时间任务时，能够降低计算成本，同时保持准确性。研究表明，GUI代理的注意力模式与自然图像不同，所有变换器层的注意力稀疏性均较高，这促使我们提出了一种简单的均匀预算分配策略。GUI-KV结合了空间显著性引导和时间冗余评分两种新技术，显著提高了缓存压缩的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22887",
            "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents",
            "url": "https://huggingface.co/papers/2509.22887",
            "abstract": "Integrating Theory of Mind into LLMs improves dialogue effectiveness and goal achievement by enabling strategic reasoning and better partner relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.",
            "score": 3,
            "issue_id": 6200,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "de0a43468eb08889",
            "authors": [
                "EunJeong Hwang",
                "Yuwei Yin",
                "Giuseppe Carenini",
                "Peter West",
                "Vered Shwartz"
            ],
            "affiliations": [
                "University of British Columbia",
                "Vector Institute for AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22887.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Агенты с теорией разума для социального интеллекта",
                    "desc": "Исследователи интегрировали теорию разума (ToM) — способность понимать ментальные состояния других людей — в большие языковые модели для улучшения диалогов. Они разработали ToMAgent (ToMA), который генерирует предположения о ментальных состояниях собеседника между репликами и использует их для стратегического планирования. Эксперименты показали, что такой подход позволяет агентам более эффективно достигать целей в диалоге, демонстрируя стратегическое мышление и долгосрочную адаптацию. Результаты указывают на перспективность использования ToM для создания социально интеллектуальных AI-агентов."
                },
                "en": {
                    "title": "Empowering LLMs with Theory of Mind for Smarter Conversations",
                    "desc": "This paper explores how integrating Theory of Mind (ToM) into large language models (LLMs) enhances their ability to engage in dialogue and achieve specific goals. By understanding the mental states of conversation partners, LLMs can respond more strategically and maintain better relationships. The authors introduce ToMA, a dialogue agent that combines ToM with dialogue lookahead to optimize its responses for goal achievement. Experiments show that ToMA outperforms traditional models, demonstrating improved reasoning and adaptability in social interactions."
                },
                "zh": {
                    "title": "心智理论提升对话智能",
                    "desc": "本研究探讨了将心智理论（ToM）整合到大型语言模型（LLM）中的方法，以提高对话的有效性和目标达成率。心智理论是理解他人心理状态的能力，是人类社会智能的重要组成部分。我们展示了通过明确使用心智理论的LLM在对话中表现更好，能够更有效地实现目标。我们还提出了ToMA（心智理论对话代理），通过将心智理论与对话前瞻结合，训练出能够产生有助于实现对话目标的心理状态的代理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01180",
            "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
            "url": "https://huggingface.co/papers/2510.01180",
            "abstract": "BroRL enhances reinforcement learning by increasing rollouts per example, overcoming performance plateaus and achieving state-of-the-art results in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by a mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks.",
            "score": 2,
            "issue_id": 6202,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "2491de2a9eaf3c36",
            "authors": [
                "Jian Hu",
                "Mingjie Liu",
                "Ximing Lu",
                "Fang Wu",
                "Zaid Harchaoui",
                "Shizhe Diao",
                "Yejin Choi",
                "Pavlo Molchanov",
                "Jun Yang",
                "Jan Kautz",
                "Yi Dong"
            ],
            "affiliations": [
                "NVIDIA",
                "Stanford University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01180.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Больше роллаутов — лучше результат: масштабирование RL через расширение исследования",
                    "desc": "Статья представляет BroRL — метод масштабирования reinforcement learning для LLM через увеличение числа роллаутов (траекторий) на каждом примере вместо простого увеличения шагов обучения. Авторы показывают через анализ уравнений массового баланса, что при достаточно большом числе роллаутов гарантируется рост вероятностной массы правильных токенов. Подход позволяет преодолеть плато производительности, которое наблюдается в методе ProRL после тысяч шагов обучения. BroRL достигает state-of-the-art результатов на моделях размером 1.5B параметров, демонстрируя непрерывное улучшение там, где другие методы насыщаются."
                },
                "en": {
                    "title": "Broaden Exploration for Continuous Gains in RL",
                    "desc": "BroRL is a novel approach in reinforcement learning that enhances the training process by increasing the number of rollouts per example, which allows for broader exploration of the solution space. This method addresses the issue of performance plateaus that occur when traditional training steps reach diminishing returns. By analyzing the probability mass of correct and incorrect tokens, BroRL ensures that as the number of rollouts increases, the overall performance improves continuously. The empirical results show that BroRL outperforms previous methods, achieving state-of-the-art results in large language models after extensive training."
                },
                "zh": {
                    "title": "BroRL：突破强化学习的性能瓶颈",
                    "desc": "BroRL是一种增强强化学习的方法，通过增加每个示例的回合数来克服性能平台期。它能够在大语言模型中实现持续的性能提升，超越了传统方法ProRL的限制。通过对概率质量变化的分析，BroRL确保了正确标记的概率质量不断扩展。实验结果表明，BroRL在多个基准测试中取得了最先进的成果，尤其是在经过3000步ProRL训练后，模型的性能得到了显著恢复。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00931",
            "title": "Making, not Taking, the Best of N",
            "url": "https://huggingface.co/papers/2510.00931",
            "abstract": "Fusion-of-N (FusioN) method improves LLM generation quality by synthesizing elements from multiple samples, outperforming Best-of-N in various settings and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Obtaining high-quality generations in modern LLMs has largely been framed as a selection problem: identifying a single winning generation from a diverse pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently zero-sum, discarding diverse and potentially useful information from the pool. Instead, we explore a collaborative setup, where all candidates can potentially contribute to the final winning generation. To this end, we propose Fusion-of-N (FusioN): a method that uses a general LLM judge to synthesize the most informative elements of each sample into a single final answer. We compare FusioN to BoN in two settings, (i) test-time scaling, where we sample and aggregate from a single model at test-time (ii) synthetic data generation, where we fuse samples from a pool of diverse teachers to improve a student model. We extensively benchmark both setups across 11 languages, 3 diverse tasks and varying model scales. Across the bench, FusioN consistently outperforms BoN showing versatility and robustness both in test-time scaling and in downstream gains from synthetic data generation. We also perform extensive analysis on FusioN, where it shows surprising strengths and robustness under challenging settings. These results show that we should shift how we think about evaluating and utilizing LLM generations from a monolithic measure of quality, to embracing their polylithic nature. This shift allows us to integrate diverse strengths, unlock latent potential, and achieve improvements that were previously inaccessible through selection alone.",
            "score": 2,
            "issue_id": 6201,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "f203d81f3a65b787",
            "authors": [
                "Ammar Khairi",
                "Daniel D'souza",
                "Marzieh Fadaee",
                "Julia Kreutzer"
            ],
            "affiliations": [
                "Cohere Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00931.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#optimization",
                    "#synthetic",
                    "#multilingual"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Сила слияния: объединяем лучшее из многих генераций вместо выбора одной",
                    "desc": "В статье предлагается метод Fusion-of-N (FusioN), который улучшает качество генерации LLM путём синтеза элементов из множественных сэмплов, вместо простого выбора лучшего варианта как в Best-of-N. Метод использует LLM в качестве судьи для объединения наиболее информативных элементов каждого сэмпла в финальный ответ. FusioN тестировался в двух сценариях: масштабирование на этапе inference и генерация синтетических данных для обучения student-модели. Результаты на 11 языках и 3 задачах показывают стабильное превосходство FusioN над Best-of-N, демонстрируя преимущество коллаборативного подхода над конкурентным отбором."
                },
                "en": {
                    "title": "Unlocking the Power of Collaboration in LLM Generation",
                    "desc": "The Fusion-of-N (FusioN) method enhances the quality of large language model (LLM) outputs by integrating elements from multiple generated samples rather than selecting just one. This approach contrasts with the traditional Best-of-N (BoN) method, which often overlooks valuable information by focusing on a single best output. FusioN employs a general LLM judge to synthesize the most informative parts of each candidate, leading to a more comprehensive final generation. Extensive benchmarking across various languages and tasks demonstrates that FusioN consistently outperforms BoN, highlighting the benefits of leveraging diverse contributions in LLM generation."
                },
                "zh": {
                    "title": "融合多样性，提升生成质量",
                    "desc": "Fusion-of-N（FusioN）方法通过综合多个样本的元素，提升了大型语言模型（LLM）的生成质量，超越了传统的最佳选择方法（Best-of-N）。该方法不再仅仅选择一个最佳生成，而是允许所有候选样本共同贡献信息，形成最终的答案。FusioN在测试时扩展和合成数据生成的多种任务中表现出色，显示出其在多语言和不同模型规模下的灵活性和稳健性。研究结果表明，我们应当改变对LLM生成结果的评估方式，从单一的质量衡量转向接受其多样性，以实现更大的潜力和改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00777",
            "title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.00777",
            "abstract": "In-place feedback allows users to directly edit LLM responses, improving performance and reducing token usage in multi-turn reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine their outputs based on user-provided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM's previous response, and the model conditions on this modified response to generate its revision. Empirical evaluations on diverse reasoning-intensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using 79.1% fewer tokens. Complementary analyses on controlled environments further demonstrate that in-place feedback resolves a core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content. These findings suggest that in-place feedback offers a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks.",
            "score": 2,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "be36c5f4f29f17e4",
            "authors": [
                "Youngbin Choi",
                "Minjong Lee",
                "Saemi Moon",
                "Seunghyuk Cho",
                "Chaehyeon Chung",
                "MoonJeong Park",
                "Dongwoo Kim"
            ],
            "affiliations": [
                "Computer Science and Engineering, POSTECH",
                "Graduate School of Artificial Intelligence, POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00777.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "✏️",
                "ru": {
                    "title": "Редактируй прямо здесь: эффективная обратная связь для LLM",
                    "desc": "Исследователи предложили новый подход взаимодействия с языковыми моделями под названием in-place feedback, при котором пользователи напрямую редактируют ответы LLM вместо отправки новых сообщений. Эксперименты на задачах многошагового рассуждения показали, что этот метод улучшает производительность модели и снижает использование токенов на 79,1% по сравнению с традиционной многошаговой обратной связью. Ключевое преимущество заключается в том, что модели лучше применяют исправления именно к ошибочным частям ответа, избегая появления новых ошибок в ранее корректном содержании. Подход демонстрирует более естественный и эффективный механизм управления LLM в задачах, требующих сложных рассуждений."
                },
                "en": {
                    "title": "In-Place Feedback: Direct Edits for Smarter LLMs",
                    "desc": "This paper presents a new method called in-place feedback for improving large language models (LLMs) during multi-turn reasoning tasks. Instead of sending new messages for feedback, users can directly edit the model's previous responses, allowing the model to learn from these modifications. The results show that this approach not only enhances the model's performance but also significantly reduces the number of tokens used by 79.1%. Overall, in-place feedback addresses the limitations of traditional feedback methods by enabling more precise corrections and reducing the introduction of new errors."
                },
                "zh": {
                    "title": "就地反馈：提升LLM推理的有效新方式",
                    "desc": "本研究提出了一种新的交互模式——就地反馈，允许用户直接编辑大型语言模型（LLM）的响应。这种方法在多轮推理任务中表现出色，能够显著提高模型的性能，同时减少79.1%的令牌使用。通过实证评估，我们发现就地反馈比传统的多轮反馈更有效，能够更准确地应用用户的反馈，避免了模型在修正错误时引入新的错误。总的来说，就地反馈为指导LLM在复杂推理任务中提供了一种更自然和有效的机制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00553",
            "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
            "url": "https://huggingface.co/papers/2510.00553",
            "abstract": "Two fundamental properties of reinforcement learning-induced parameter updates in large language models are identified, leading to a plug-in acceleration framework that significantly speeds up training without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.",
            "score": 2,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "75c2581875112809",
            "authors": [
                "Yuchen Cai",
                "Ding Cao",
                "Xin Xu",
                "Zijun Yao",
                "Yuqing Huang",
                "Zhenyu Tan",
                "Benyi Zhang",
                "Guiquan Liu",
                "Junfeng Fang"
            ],
            "affiliations": [
                "HKUST",
                "NUS",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00553.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение обучения LLM через ранговую экстраполяцию",
                    "desc": "Исследование выявляет два фундаментальных свойства обновлений параметров в больших языковых моделях при reinforcement learning: доминирование ранга-1 и линейную динамику главного подпространства. Оказывается, что верхнее сингулярное подпространство матрицы обновлений параметров определяет более 99% улучшения способностей к рассуждению. На основе этих находок предложен фреймворк AlphaRL, который экстраполирует финальное обновление параметров, используя короткое окно раннего обучения. Метод обеспечивает ускорение до 2.5x с сохранением более 96% производительности без дополнительных модулей или настройки гиперпараметров."
                },
                "en": {
                    "title": "Accelerating RL Training in LLMs with AlphaRL",
                    "desc": "This paper explores how reinforcement learning (RL) affects the training of large language models (LLMs) by identifying two key properties of parameter updates. The first property, Rank-1 Dominance, shows that a specific part of the parameter update matrix is crucial for improving reasoning capabilities, capturing over 99% of performance gains. The second property, Rank-1 Linear Dynamics, indicates that this important part changes in a predictable way during training, allowing for accurate predictions from early training stages. Based on these insights, the authors introduce AlphaRL, a framework that accelerates training by predicting final updates from early data, achieving significant speed improvements while maintaining high performance."
                },
                "zh": {
                    "title": "加速强化学习训练的有效工具",
                    "desc": "本文识别了强化学习在大型语言模型（LLMs）中引起的参数更新的两个基本特性。这些特性包括：1）秩-1主导性，意味着参数更新矩阵的主导子空间几乎完全决定了推理的改进；2）秩-1线性动态，表明这个主导子空间在训练过程中线性演变。基于这些发现，提出了AlphaRL加速框架，可以在不牺牲性能的情况下显著加快训练速度。实验表明，该框架在多个大型语言模型和算法中具有广泛的适用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00510",
            "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
            "url": "https://huggingface.co/papers/2510.00510",
            "abstract": "A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.",
            "score": 1,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "4a1605179598a812",
            "authors": [
                "Jiarun Liu",
                "Shiyue Xu",
                "Shangkun Liu",
                "Yang Li",
                "Wen Liu",
                "Min Liu",
                "Xiaoqing Zhou",
                "Hanmin Wang",
                "Shilin Jia",
                "zhen Wang",
                "Shaohua Tian",
                "Hanhao Li",
                "Junbo Zhang",
                "Yongli Yu",
                "Peng Cao",
                "Haofen Wang"
            ],
            "affiliations": [
                "GAIA JINGDONG CHO-EI Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00510.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#optimization",
                    "#architecture",
                    "#multimodal",
                    "#agents",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Универсальный AI-агент через интеграцию памяти, планирования и инструментов",
                    "desc": "В статье предлагается универсальная архитектура AI-агента, которая объединяет три ключевых компонента для решения сложных задач. Первый компонент — это мультиагентная система с планированием, исполнением и голосованием критических моделей. Второй — иерархическая память, включающая рабочий, семантический и процедурный уровни. Третий — набор инструментов для поиска, выполнения кода и мультимодального парсинга. Система превосходит open-source решения и приближается к производительности проприетарных систем."
                },
                "en": {
                    "title": "Empowering AI with Integrated Generalist Agent Architecture",
                    "desc": "This paper presents a new architecture for generalist agents that enhances their performance in various tasks. It combines multi-agent planning, where different agents work together to make decisions, with a hierarchical memory system that organizes information at different levels. Additionally, it includes a refined tool suite that allows the agent to perform tasks like searching and executing code. The proposed system shows significant improvements over existing models, indicating that integrating these components leads to more robust and adaptable AI assistants."
                },
                "zh": {
                    "title": "通用智能体架构：提升AI助手的适应性与鲁棒性",
                    "desc": "本文提出了一种通用智能体架构，结合了多智能体规划、分层记忆和精细化工具套件，能够在多种任务中超越现有系统。该架构整合了集体多智能体框架、分层记忆系统以及用于搜索、代码执行和多模态解析的工具。通过全面的基准测试，我们的框架在性能上持续优于开源基线，并接近专有系统的表现。这些结果表明系统级集成的重要性，并为可扩展、弹性和适应性强的人工智能助手指明了方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25411",
            "title": "Boolean Satisfiability via Imitation Learning",
            "url": "https://huggingface.co/papers/2509.25411",
            "abstract": "ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT",
            "score": 1,
            "issue_id": 6199,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "8518038ce5781379",
            "authors": [
                "Zewei Zhang",
                "Huan Liu",
                "Yuanhao Yu",
                "Jun Chen",
                "Xiangyu Xu"
            ],
            "affiliations": [
                "McMaster University",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25411.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#open_source",
                    "#training",
                    "#optimization",
                    "#math"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Обучение на экспертных трассах для ускорения SAT-солверов",
                    "desc": "В статье представлен ImitSAT - новая стратегия ветвления для CDCL солверов SAT-задач, основанная на imitation learning. Метод обучается на экспертных трассах KeyTrace, которые представляют полный процесс решения как последовательность ключевых решений без конфликтов. Такой подход обеспечивает плотный supervised сигнал на уровне каждого решения и напрямую сокращает количество propagations - основной фактор времени выполнения. Эксперименты показывают, что ImitSAT превосходит современные методы на основе ML, сокращая время работы и количество propagations."
                },
                "en": {
                    "title": "ImitSAT: Learning from Experts for Faster SAT Solving",
                    "desc": "ImitSAT is a new branching policy designed for conflict-driven clause learning (CDCL) solvers that uses imitation learning from expert traces to improve performance on the Boolean satisfiability problem (SAT). Unlike traditional methods that rely on indirect signals or reinforcement learning, ImitSAT directly learns from a sequence of expert decisions, known as KeyTrace, which simplifies the decision-making process. This approach minimizes conflicts during execution, leading to fewer propagation counts and reduced runtime. The results show that ImitSAT significantly outperforms existing learned methods, providing a more efficient and effective solution for SAT problems."
                },
                "zh": {
                    "title": "ImitSAT：高效的CDCL求解器分支策略",
                    "desc": "ImitSAT是一种基于模仿学习的分支策略，专为冲突驱动子句学习（CDCL）求解器设计，旨在解决布尔可满足性问题（SAT）。与以往方法不同，ImitSAT通过学习专家的KeyTrace，直接提供决策级的监督，从而减少传播次数和运行时间。通过在同一实例上重放KeyTrace，ImitSAT几乎没有冲突，显著提高了分支的质量和收敛速度。大量实验表明，ImitSAT在传播次数和运行时间上优于现有的最先进学习方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23250",
            "title": "Training Vision-Language Process Reward Models for Test-Time Scaling in\n  Multimodal Reasoning: Key Insights and Lessons Learned",
            "url": "https://huggingface.co/papers/2509.23250",
            "abstract": "Hybrid data synthesis and perception-focused supervision improve the reliability of Vision-Language Process Reward Models (VL-PRMs) in guiding VLMs across diverse multimodal benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs.",
            "score": 1,
            "issue_id": 6198,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "c59cc3e092f9a705",
            "authors": [
                "Brandon Ong",
                "Tej Deep Pala",
                "Vernon Toh",
                "William Chandra Tjhi",
                "Soujanya Poria"
            ],
            "affiliations": [
                "AI Singapore",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23250.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#games",
                    "#multimodal",
                    "#data",
                    "#dataset"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Учим мультимодальные модели проверять свои рассуждения пошагово",
                    "desc": "Статья представляет улучшенные Vision-Language Process Reward Models (VL-PRMs), которые обеспечивают пошаговый контроль качества рассуждений в мультимодальных языковых моделях. Авторы предлагают гибридный подход к синтезу данных, комбинируя MCTS с оценками сильной VLM, и вводят специальную supervision на уровне визуального восприятия. Эксперименты на пяти бенчмарках показывают, что даже небольшие VL-PRMs могут эффективно выявлять ошибки в рассуждениях и улучшать производительность базовых моделей во время инференса. Ключевое открытие: VL-PRMs как Outcome Reward Models превосходят пошаговую селекцию, а supervision на уровне восприятия существенно повышает качество test-time scaling."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Hybrid Supervision and Data Synthesis",
                    "desc": "This paper discusses improvements in Vision-Language Process Reward Models (VL-PRMs) to enhance their effectiveness in guiding Vision Language Models (VLMs). The authors introduce a hybrid data synthesis method that merges Monte Carlo Tree Search with insights from a robust VLM, resulting in more precise step-level supervision. They also propose a perception-focused supervision approach that helps the model identify errors during visual reasoning. Through extensive testing on various multimodal benchmarks, the study demonstrates that these enhancements lead to better performance and reliability in VLMs, even in complex reasoning tasks."
                },
                "zh": {
                    "title": "混合数据合成与感知监督提升视觉语言模型的可靠性",
                    "desc": "本研究提出了一种混合数据合成框架，结合了蒙特卡洛树搜索（MCTS）和强大的视觉语言模型（VLM）的判断，以生成更准确的步骤级标签。我们还引入了以感知为中心的监督，帮助过程奖励模型（PRM）在推理的视觉基础阶段明确检测错误。通过系统评估多种测试时扩展策略，我们的实验表明，视觉语言过程奖励模型（VL-PRM）能够可靠地引导VLM朝向更准确的解决方案。我们的研究结果为进一步研究和视觉语言模型的进步提供了重要的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19185",
            "title": "An Empirical Study of Testing Practices in Open Source AI Agent\n  Frameworks and Agentic Applications",
            "url": "https://huggingface.co/papers/2509.19185",
            "abstract": "The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents.",
            "score": 1,
            "issue_id": 6199,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "e00a1db1e43fa4c8",
            "authors": [
                "Mohammed Mehedi Hasan",
                "Hao Li",
                "Emad Fallahzadeh",
                "Gopi Krishnan Rajbahadur",
                "Bram Adams",
                "Ahmed E. Hassan"
            ],
            "affiliations": [
                "School of Computing, Queens University, Kingston, ON, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19185.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#security",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Тестирование AI-агентов: фокус не там, где нужно",
                    "desc": "Исследование анализирует практики тестирования в 39 фреймворках AI-агентов и 439 приложениях, выявляя десять паттернов тестирования. Обнаружена критическая проблема: более 70% усилий по тестированию направлено на детерминированные компоненты (инструменты и воркфлоу), в то время как компоненты на основе foundation models получают менее 5% внимания. Особенно проблематично, что промпты (Trigger component) практически не тестируются, появляясь лишь в 1% тестов. Авторы призывают разработчиков фреймворков улучшить поддержку новых методов тестирования, а разработчиков приложений — внедрить regression-тестирование промптов для повышения надёжности AI-агентов."
                },
                "en": {
                    "title": "Enhancing AI Agent Testing: Bridging the Gap in Robustness",
                    "desc": "This paper investigates the testing practices used in AI agent frameworks and applications, revealing a significant focus on deterministic components while largely neglecting the Trigger component. The study analyzes 39 open-source frameworks and 439 applications, identifying ten distinct testing patterns, with traditional methods dominating the landscape. It highlights that over 70% of testing effort is spent on deterministic components, while less than 5% is allocated to the FM-based Plan Body, indicating a critical oversight. The authors suggest that improving testing methods and incorporating prompt regression testing are essential for enhancing the robustness of AI agents."
                },
                "zh": {
                    "title": "提升AI代理测试的鲁棒性",
                    "desc": "本研究分析了人工智能代理框架和应用中的测试实践，发现目前的测试主要集中在确定性组件上，而触发组件却被忽视。我们对39个开源代理框架和439个代理应用进行了大规模实证研究，识别出十种不同的测试模式。结果显示，像DeepEval这样的新型代理特定方法使用率极低，而传统的负面测试和成员测试被广泛应用以应对基础模型的不确定性。为了提高AI代理的鲁棒性，开发者需要改进对新测试方法的支持，并在应用中采用提示回归测试。"
                }
            }
        }
    ],
    "link_prev": "2025-10-01.html",
    "link_next": "2025-10-03.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "01.10",
        "en": "10/01",
        "zh": "10月1日"
    },
    "short_date_next": {
        "ru": "03.10",
        "en": "10/03",
        "zh": "10月3日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 11,
        "#agents": 8,
        "#cv": 0,
        "#rl": 8,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 0,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 2,
        "#reasoning": 10,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 12,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}