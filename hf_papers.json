{
    "date": {
        "ru": "24 апреля",
        "en": "April 24",
        "zh": "4月24日"
    },
    "time_utc": "2025-04-24 03:33",
    "weekday": 3,
    "issue_id": 3404,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15279",
            "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
            "url": "https://huggingface.co/papers/2504.15279",
            "abstract": "Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives. We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30% accuracy-only slightly above the 25% random baseline and far below the 51.4% achieved by humans-revealing significant gaps in visual reasoning. Furthermore, we provide a supplementary training dataset and a reinforcement-learning baseline to support further progress.",
            "score": 15,
            "issue_id": 3404,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "5ec39c2a399dba2b",
            "authors": [
                "Weiye Xu",
                "Jiahao Wang",
                "Weiyun Wang",
                "Zhe Chen",
                "Wengang Zhou",
                "Aijun Yang",
                "Lewei Lu",
                "Houqiang Li",
                "Xiaohua Wang",
                "Xizhou Zhu",
                "Wenhai Wang",
                "Jifeng Dai",
                "Jinguo Zhu"
            ],
            "affiliations": [
                "SenseTime Research",
                "Shanghai Artificial Intelligence Laboratory",
                "Tsinghua University",
                "University of Science and Technology of China",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15279.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VisuLogic: преодолевая разрыв в визуальном мышлении ИИ",
                    "desc": "Статья представляет VisuLogic - новый бенчмарк для оценки визуального мышления мультимодальных больших языковых моделей (MLLM). Бенчмарк состоит из 1000 проверенных человеком задач в шести категориях, включая количественные изменения и пространственные отношения. Тестирование ведущих MLLM на этом бенчмарке показало, что большинство моделей набирают менее 30% точности, что значительно ниже человеческого результата в 51.4%. Авторы также предоставляют дополнительный набор данных для обучения и базовую модель с обучением с подкреплением для дальнейших исследований."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning in Multimodal Models with VisuLogic",
                    "desc": "This paper introduces VisuLogic, a new benchmark designed to evaluate the visual reasoning abilities of multimodal large language models (MLLMs). Unlike previous evaluations that relied on text descriptions, VisuLogic presents 1,000 human-verified problems across six categories, focusing on genuine vision-centric reasoning. The results show that leading MLLMs struggle with these tasks, scoring below 30% accuracy, which is only slightly better than random guessing and significantly lower than human performance. To aid in improving these models, the authors also provide a supplementary training dataset and a reinforcement-learning baseline."
                },
                "zh": {
                    "title": "提升视觉推理能力的基准评估",
                    "desc": "视觉推理是人类智能的核心组成部分，也是高级多模态模型的重要能力。然而，目前对多模态大型语言模型（MLLMs）的推理评估往往依赖文本描述，允许语言基础的推理捷径，未能真实测量视觉中心的推理能力。为了解决这个问题，我们引入了VisuLogic：一个包含1000个经过人工验证的问题的基准，涵盖六个类别（例如，定量变化、空间关系、属性比较）。我们对领先的MLLMs在这个基准上的表现进行了评估，结果显示大多数模型的准确率低于30%，远低于人类的51.4%，揭示了视觉推理方面的显著差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15431",
            "title": "Trillion 7B Technical Report",
            "url": "https://huggingface.co/papers/2504.15431",
            "abstract": "We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10\\% of its 2T training tokens to multilingual data and requiring just 59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages demonstrate Trillion-7B's robust multilingual performance and exceptional cross-lingual consistency.",
            "score": 11,
            "issue_id": 3404,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "0d2a201b09695822",
            "authors": [
                "Sungjun Han",
                "Juyoung Suk",
                "Suyeong An",
                "Hyungguk Kim",
                "Kyuseok Kim",
                "Wonsuk Yang",
                "Seungtaek Choi",
                "Jamin Shin"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.15431.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual",
                    "#low_resource",
                    "#training",
                    "#transfer_learning",
                    "#architecture",
                    "#data"
                ],
                "emoji": "🌏",
                "ru": {
                    "title": "Эффективный перенос знаний между языками в компактной мультиязычной модели",
                    "desc": "Trillion-7B - это мультиязычная языковая модель с акцентом на корейский язык, отличающаяся высокой эффективностью использования токенов. Модель использует новый механизм межъязыкового внимания к документам (XLDA) для эффективного переноса знаний с английского на целевые языки. Благодаря оптимизированным наборам данных и специальной фильтрации, Trillion-7B достигает конкурентоспособной производительности, используя лишь 10% мультиязычных данных при обучении. Модель демонстрирует надежную многоязычную производительность и исключительную межъязыковую согласованность по результатам тестирования на 27 бенчмарках на четырех языках."
                },
                "en": {
                    "title": "Efficient Multilingual Mastery with Trillion-7B",
                    "desc": "Trillion-7B is a multilingual language model designed specifically for Korean and other languages. It utilizes a new technique called Cross-lingual Document Attention (XLDA) to efficiently transfer knowledge from English to languages like Korean and Japanese. The model is trained with a small portion of multilingual data, only 10% of its total training tokens, and is optimized for performance with specific data mixtures and tokenizer adjustments. Evaluations show that Trillion-7B performs well across multiple languages and maintains strong consistency in cross-lingual tasks."
                },
                "zh": {
                    "title": "高效的多语言大模型 Trillion-7B",
                    "desc": "Trillion-7B 是一种高效的多语言大模型，专注于韩语等语言的处理。它采用了新颖的跨语言文档注意力机制（XLDA），能够有效地将知识从英语转移到目标语言，如韩语和日语。通过优化数据混合、语言特定过滤和定制的分词器构建，Trillion-7B 在仅使用 10% 的多语言数据的情况下，仍能实现竞争力的性能。经过对 27 个基准测试的全面评估，Trillion-7B 展现了强大的多语言性能和卓越的跨语言一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15843",
            "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
            "url": "https://huggingface.co/papers/2504.15843",
            "abstract": "Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.",
            "score": 4,
            "issue_id": 3403,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "e8cb4456a20efe2a",
            "authors": [
                "Junshu Pan",
                "Wei Shen",
                "Shulin Huang",
                "Qiji Zhou",
                "Yue Zhang"
            ],
            "affiliations": [
                "Independent Researcher",
                "School of Engineering, Westlake University",
                "Shanghai Innovation Institute",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15843.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rlhf",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Pre-DPO: Эффективная оптимизация языковых моделей с учетом предпочтений",
                    "desc": "Эта статья представляет Pre-DPO - новый подход к обучению языковых моделей на основе предпочтений человека. Pre-DPO улучшает существующие методы DPO и SimPO, используя ориентирующую эталонную модель для оптимизации весов данных. Этот метод позволяет более эффективно использовать обучающие данные и достигать лучших результатов. Эксперименты на бенчмарках AlpacaEval 2.0 и Arena-Hard v0.1 показывают преимущества Pre-DPO без необходимости во внешних моделях или дополнительных данных."
                },
                "en": {
                    "title": "Enhancing Preference Learning with Pre-DPO",
                    "desc": "This paper introduces Pre-DPO, a new training method for large language models that enhances Direct Preference Optimization (DPO) by using a guiding reference model. The reference model helps adjust the importance of training data, allowing the model to focus on more relevant samples and improve learning efficiency. The authors highlight that traditional methods can lead to poor performance due to identical initialization of models and lack of robustness in simpler approaches. Through experiments, they show that Pre-DPO outperforms existing methods without needing extra data or external models."
                },
                "zh": {
                    "title": "提升偏好优化性能的Pre-DPO方法",
                    "desc": "直接偏好优化（DPO）通过直接优化人类偏好，简化了大型语言模型（LLM）的强化学习过程。研究发现，在DPO训练中，参考模型充当数据权重调整器，但常见的将策略模型和参考模型初始化为相同的做法可能导致数据利用效率低下。我们提出的Pre-DPO训练范式，通过利用指导性参考模型，增强了偏好优化的性能，能够自适应地为更适合模型的样本分配更高的权重。实验结果表明，Pre-DPO在多个基准测试中持续提升了DPO和SimPO的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-04-23.html",
    "link_next": "2025-04-25.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "23.04",
        "en": "04/23",
        "zh": "4月23日"
    },
    "short_date_next": {
        "ru": "25.04",
        "en": "04/25",
        "zh": "4月25日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一种新方法，将新语言整合到大型语言模型中。该方法成功地将之前未见的目标语言加入现有模型，不影响其原有知识。研究团队用15亿参数训练了一个名为Kuwain的小模型，将阿拉伯语加入主要用英语训练的开源模型。结果显示，阿拉伯语性能提高了8%，同时保留了原有知识。这提供了一种成本效益高的替代方案，避免了大规模重新训练。",
        "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
        "pinyin": "这篇文章介绍了一种新方法，将新语言整合到大型语言模型中。\nZhè piān wénzhāng jièshào le yī zhǒng xīn fāngfǎ, jiāng xīn yǔyán zhěnghé dào dàxíng yǔyán móxíng zhōng.\n\n该方法成功地将之前未见的目标语言加入现有模型，不影响其原有知识。\nGǎi fāngfǎ chénggōng de jiāng zhīqián wèijiàn de mùbiāo yǔyán jiārù xiànyǒu móxíng, bù yǐngxiǎng qí yuányǒu zhīshi.\n\n研究团队用15亿参数训练了一个名为Kuwain的小模型，将阿拉伯语加入主要用英语训练的开源模型。\nYánjiū tuánduì yòng 15 yì cānshǔ xùnliàn le yīgè míngwèi Kuwain de xiǎo móxíng, jiāng Ālābóyǔ jiārù zhǔyào yòng Yīngyǔ xùnliàn de kāiyuán móxíng.\n\n结果显示，阿拉伯语性能提高了8%，同时保留了原有知识。\nJiégǔo xiǎnshì, Ālābóyǔ xìngnéng tígāo le 8%, tóngshí bǎoliú le yuányǒu zhīshi.\n\n这提供了一种成本效益高的替代方案，避免了大规模重新训练。\nZhè tígōng le yī zhǒng chéngběn xiàoyì gāo de tìdài fāng'àn, bìmiǎn le dàguīmó chóngxīn xùnliàn.",
        "vocab": "[\n    {\"word\": \"整合\", \"pinyin\": \"zhěnghé\", \"trans\": \"integrate\"},\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"未见\", \"pinyin\": \"wèijiàn\", \"trans\": \"unseen\"},\n    {\"word\": \"目标\", \"pinyin\": \"mùbiāo\", \"trans\": \"target\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiànyǒu\", \"trans\": \"existing\"},\n    {\"word\": \"影响\", \"pinyin\": \"yǐngxiǎng\", \"trans\": \"affect\"},\n    {\"word\": \"原有\", \"pinyin\": \"yuányǒu\", \"trans\": \"original\"},\n    {\"word\": \"知识\", \"pinyin\": \"zhīshi\", \"trans\": \"knowledge\"},\n    {\"word\": \"研究\", \"pinyin\": \"yánjiū\", \"trans\": \"research\"},\n    {\"word\": \"团队\", \"pinyin\": \"tuánduì\", \"trans\": \"team\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshǔ\", \"trans\": \"parameters\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"train\"},\n    {\"word\": \"名为\", \"pinyin\": \"míngwéi\", \"trans\": \"named\"},\n    {\"word\": \"小模型\", \"pinyin\": \"xiǎo móxíng\", \"trans\": \"small model\"},\n    {\"word\": \"加入\", \"pinyin\": \"jiārù\", \"trans\": \"add\"},\n    {\"word\": \"主要\", \"pinyin\": \"zhǔyào\", \"trans\": \"main\"},\n    {\"word\": \"用英语\", \"pinyin\": \"yòng yīngyǔ\", \"trans\": \"using English\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāiyuán\", \"trans\": \"open-source\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìngnéng\", \"trans\": \"performance\"},\n    {\"word\": \"提高\", \"pinyin\": \"tígāo\", \"trans\": \"improve\"},\n    {\"word\": \"保留\", \"pinyin\": \"bǎoliú\", \"trans\": \"retain\"},\n    {\"word\": \"提供\", \"pinyin\": \"tígōng\", \"trans\": \"provide\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéngběn\", \"trans\": \"cost\"},\n    {\"word\": \"效益\", \"pinyin\": \"xiàoyì\", \"trans\": \"benefit\"},\n    {\"word\": \"高\", \"pinyin\": \"gāo\", \"trans\": \"high\"},\n    {\"word\": \"替代\", \"pinyin\": \"tìdài\", \"trans\": \"alternative\"},\n    {\"word\": \"方案\", \"pinyin\": \"fāngàn\", \"trans\": \"solution\"},\n    {\"word\": \"避免\", \"pinyin\": \"bìmiǎn\", \"trans\": \"avoid\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dàguīmó\", \"trans\": \"large-scale\"},\n    {\"word\": \"重新\", \"pinyin\": \"chóngxīn\", \"trans\": \"re-\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"train\"}\n]",
        "trans": "This article introduces a new method for integrating new languages into large language models. The method successfully incorporates previously unseen target languages into existing models without affecting their original knowledge. The research team trained a small model named Kuwain with 1.5 billion parameters, adding Arabic to a primarily English-trained open-source model. The results showed an 8% improvement in Arabic performance while retaining the original knowledge. This provides a cost-effective alternative, avoiding the need for large-scale retraining.",
        "update_ts": "2025-04-23 09:13"
    }
}