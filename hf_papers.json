{
    "date": {
        "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 23",
        "zh": "12æœˆ23æ—¥"
    },
    "time_utc": "2025-12-23 04:39",
    "weekday": 1,
    "issue_id": 192,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.16676",
            "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
            "url": "https://huggingface.co/papers/2512.16676",
            "abstract": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
            "score": 48,
            "issue_id": 192,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "22eafee308563f25",
            "authors": [
                "Hao Liang",
                "Xiaochen Ma",
                "Zhou Liu",
                "Zhen Hao Wong",
                "Zhengyang Zhao",
                "Zimo Meng",
                "Runming He",
                "Chengyu Shen",
                "Qifeng Cai",
                "Zhaoyang Han",
                "Meiyi Qiang",
                "Yalin Feng",
                "Tianyi Bai",
                "Zewei Pan",
                "Ziyi Guo",
                "Yizhen Jiang",
                "Jingwen Deng",
                "Qijie You",
                "Peichao Lai",
                "Tianyu Guo",
                "Chi Hsu Tsai",
                "Hengyi Feng",
                "Rui Hu",
                "Wenkai Yu",
                "Junbo Niu",
                "Bohan Zeng",
                "Ruichuan An",
                "Lu Ma",
                "Jihao Huang",
                "Yaowei Zheng",
                "Conghui He",
                "Linpeng Tang",
                "Bin Cui",
                "Weinan E",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Institute for Advanced Algorithms Research, Shanghai",
                "LLaMA-Factory Team",
                "OpenDataLab",
                "OriginHub Technology",
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16676.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#synthetic",
                    "#optimization",
                    "#data",
                    "#rag",
                    "#dataset",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DataFlow â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 200 Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ 6 Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. DataFlow-Agent Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ½Ğ° 3-7% Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Data Preparation for LLMs with DataFlow",
                    "desc": "DataFlow is a framework that uses Large Language Models (LLMs) to improve the quality and reproducibility of data preparation for various tasks. It addresses the limitations of current data preparation methods, which often rely on inconsistent scripts and workflows. By providing a modular and extensible system with nearly 200 reusable operators, DataFlow allows users to create optimized data pipelines easily. The framework has shown significant performance improvements in LLM tasks, outperforming traditional datasets and establishing a strong foundation for future advancements in data-centric AI."
                },
                "zh": {
                    "title": "DataFlowï¼šæå‡LLMæ€§èƒ½çš„æ•°æ®å‡†å¤‡æ–°æ¡†æ¶",
                    "desc": "DataFlowæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°æ®å‡†å¤‡æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ•°æ®è´¨é‡å’Œå¯é‡å¤æ€§ã€‚å®ƒé€šè¿‡è‡ªåŠ¨ç”Ÿæˆæ•°æ®å¤„ç†ç®¡é“ï¼Œå¢å¼ºäº†LLMçš„æ€§èƒ½ï¼Œè§£å†³äº†å½“å‰æ•°æ®å‡†å¤‡ä¸­å­˜åœ¨çš„ä¸´æ—¶è„šæœ¬å’Œä¸è§„èŒƒå·¥ä½œæµçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æä¾›äº†è¿‘200ä¸ªå¯é‡ç”¨çš„æ“ä½œç¬¦å’Œå¤šä¸ªé¢†åŸŸé€šç”¨çš„ç®¡é“ï¼Œæ”¯æŒæ–‡æœ¬ã€æ•°å­¦æ¨ç†ã€ä»£ç ç­‰å¤šç§ä»»åŠ¡ã€‚é€šè¿‡DataFlow-Agentï¼Œç”¨æˆ·å¯ä»¥å°†è‡ªç„¶è¯­è¨€è§„èŒƒè‡ªåŠ¨è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„ç®¡é“ï¼Œä»è€Œç®€åŒ–æ•°æ®å‡†å¤‡è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.18880",
            "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
            "url": "https://huggingface.co/papers/2512.18880",
            "abstract": "Large Language Models struggle to accurately estimate human cognitive difficulty due to a misalignment with human perceptions and a lack of introspection regarding their own limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.",
            "score": 14,
            "issue_id": 192,
            "pub_date": "2025-12-21",
            "pub_date_card": {
                "ru": "21 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 21",
                "zh": "12æœˆ21æ—¥"
            },
            "hash": "e0ec22a61e469da2",
            "authors": [
                "Ming Li",
                "Han Chen",
                "Yunze Xiao",
                "Jian Chen",
                "Hong Jiao",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "University at Buffalo",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.18880.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ: ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ğ½Ğ¸ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ»ĞµĞ¿Ñ‹Ğµ Ğ¿ÑÑ‚Ğ½Ğ°."
                },
                "en": {
                    "title": "Bridging the Gap: Aligning AI with Human Cognitive Difficulty",
                    "desc": "This paper investigates how well Large Language Models (LLMs) can estimate the difficulty of tasks from a human perspective. It highlights a significant misalignment between the models' assessments and actual human cognitive challenges, particularly in educational contexts. The study reveals that simply increasing the size of the models does not improve their ability to understand human difficulty levels, as they tend to converge on a machine-centric view. Additionally, the models show a lack of self-awareness regarding their limitations, which complicates their use for accurate difficulty prediction in educational assessments."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»è®¤çŸ¥éš¾åº¦çš„å¯¹é½æŒ‘æˆ˜",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‡†ç¡®ä¼°è®¡äººç±»è®¤çŸ¥éš¾åº¦æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¸äººç±»çš„æ„ŸçŸ¥ä¸ä¸€è‡´ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹è‡ªèº«å±€é™æ€§çš„åæ€ã€‚æ•™è‚²è¯„ä¼°ä¸­ï¼Œå‡†ç¡®ä¼°è®¡é¢˜ç›®ï¼ˆé—®é¢˜æˆ–ä»»åŠ¡ï¼‰éš¾åº¦è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´å†·å¯åŠ¨é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹20ç§ä¸åŒé¢†åŸŸçš„æ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯åˆ†æï¼Œå‘ç°æ¨¡å‹çš„è§„æ¨¡æ‰©å¤§å¹¶æœªæœ‰æ•ˆæ”¹å–„ä¸äººç±»çš„éš¾åº¦å¯¹é½ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹çš„é«˜æ€§èƒ½å¾€å¾€å¦¨ç¢äº†å‡†ç¡®çš„éš¾åº¦ä¼°è®¡ï¼Œå› ä¸ºå®ƒä»¬éš¾ä»¥æ¨¡æ‹Ÿå­¦ç”Ÿçš„èƒ½åŠ›é™åˆ¶ï¼Œå³ä½¿åœ¨æ˜ç¡®æç¤ºç‰¹å®šç†Ÿç»ƒåº¦æ—¶ä¹Ÿå¦‚æ­¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19134",
            "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2512.19134",
            "abstract": "QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
            "score": 13,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "fe346e71e1011948",
            "authors": [
                "Dehai Min",
                "Kailin Zhang",
                "Tongtong Wu",
                "Lu Cheng"
            ],
            "affiliations": [
                "Monash University",
                "New York University",
                "University of Illinois at Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19134.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#hallucinations",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ‚ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞµ Ğ² RAG",
                    "desc": "QuCo-RAG â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ»Ğ¾Ğ² (ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…), Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ¾Ğ³Ğ´Ğ° ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½ÑƒĞ»ĞµĞ²Ğ°Ñ, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ RAG. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5-14 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ EM Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Mitigating Hallucinations with Objective Corpus Insights",
                    "desc": "QuCo-RAG is a novel approach designed to reduce hallucinations in large language models (LLMs) during text generation by utilizing objective statistics from the training corpus. Instead of relying on the model's internal confidence signals, which can be unreliable, QuCo-RAG assesses uncertainty through two key stages: identifying low-frequency entities before generation and verifying entity co-occurrence during generation. This method employs Infini-gram for rapid queries over a vast dataset, allowing for dynamic retrieval when uncertainty is detected. Experimental results demonstrate significant improvements in accuracy across various benchmarks, showcasing QuCo-RAG's effectiveness in enhancing the reliability of LLM outputs."
                },
                "zh": {
                    "title": "ç”¨å®¢è§‚ç»Ÿè®¡æ¶ˆé™¤è¯­è¨€æ¨¡å‹çš„å¹»è§‰",
                    "desc": "QuCo-RAGæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è§‰ç°è±¡ã€‚å®ƒé€šè¿‡ä½¿ç”¨å®¢è§‚çš„è¯­æ–™åº“ç»Ÿè®¡æ•°æ®æ¥æé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œè€Œä¸æ˜¯ä¾èµ–æ¨¡å‹å†…éƒ¨ä¿¡å·ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè¯†åˆ«ä½é¢‘å®ä½“ä»¥å‘ç°çŸ¥è¯†ç¼ºå£ï¼Œå…¶æ¬¡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éªŒè¯å®ä½“çš„å…±ç°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuCo-RAGåœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17650",
            "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
            "url": "https://huggingface.co/papers/2512.17650",
            "abstract": "ReCo is a novel instructional video editing paradigm that enhances accuracy and reduces token interference by incorporating constraint modeling and regularization techniques during in-context generation.  \t\t\t\t\tAI-generated summary \t\t\t\t The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.",
            "score": 8,
            "issue_id": 192,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "95db48ea9c82c952",
            "authors": [
                "Zhongwei Zhang",
                "Fuchen Long",
                "Wei Li",
                "Zhaofan Qiu",
                "Wu Liu",
                "Ting Yao",
                "Tao Mei"
            ],
            "affiliations": [
                "HiDream.ai Inc.",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17650.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#video",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "ReCo â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½ĞµÑ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ â€” Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¸ attention Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ â€” Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¼ĞµÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ›Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ·Ğ¾Ğ½Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ²Ğ½Ğµ ÑÑ‚Ğ¸Ñ… Ğ·Ğ¾Ğ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ReCo-Data Ñ 500K Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ-Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "ReCo: Precision in Instructional Video Editing through Constraint Modeling",
                    "desc": "ReCo is a new approach to instructional video editing that improves the accuracy of edits and minimizes unwanted interference between different parts of the video. It uses constraint modeling to clearly define which areas of the video should be edited and which should not, enhancing the in-context generation process. By applying regularization techniques, ReCo ensures that the editing focus is maintained while reducing noise from non-editing areas. Additionally, it introduces a comprehensive dataset, ReCo-Data, to support the training of models for better video editing outcomes."
                },
                "zh": {
                    "title": "ReCoï¼šæå‡è§†é¢‘ç¼–è¾‘å‡†ç¡®æ€§çš„åˆ›æ–°èŒƒå¼",
                    "desc": "ReCoæ˜¯ä¸€ç§æ–°é¢–çš„æ•™å­¦è§†é¢‘ç¼–è¾‘èŒƒå¼ï¼Œé€šè¿‡åœ¨ä¸Šä¸‹æ–‡ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥çº¦æŸå»ºæ¨¡å’Œæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œæé«˜äº†ç¼–è¾‘çš„å‡†ç¡®æ€§å¹¶å‡å°‘äº†æ ‡è®°å¹²æ‰°ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨æ²¡æœ‰æ˜ç¡®æŒ‡å®šç¼–è¾‘åŒºåŸŸæ—¶ï¼Œç¼–è¾‘åŒºåŸŸä¸å‡†ç¡®å’Œç¼–è¾‘ä¸éç¼–è¾‘åŒºåŸŸä¹‹é—´çš„å¹²æ‰°é—®é¢˜ã€‚ReCoé€šè¿‡å®½åº¦æ‹¼æ¥æºè§†é¢‘å’Œç›®æ ‡è§†é¢‘è¿›è¡Œè”åˆå»å™ªï¼Œå¹¶åˆ©ç”¨æ½œåœ¨å’Œæ³¨æ„åŠ›æ­£åˆ™åŒ–æ¥æ ¡å‡†è§†é¢‘æ‰©æ•£å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReCoåœ¨å››ä¸ªä¸»è¦çš„åŸºäºæŒ‡ä»¤çš„è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19682",
            "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
            "url": "https://huggingface.co/papers/2512.19682",
            "abstract": "GenEnv, a framework using a co-evolutionary game with a generative environment simulator, enhances LLM agent performance by 40.3% over 7B baselines and uses less data than offline augmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective Î±-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to +40.3\\% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3times less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
            "score": 6,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "db555e252bf52944",
            "authors": [
                "Jiacheng Guo",
                "Ling Yang",
                "Peter Chen",
                "Qixin Xiao",
                "Yinjie Wang",
                "Xinzhe Juan",
                "Jiahao Qiu",
                "Ke Shen",
                "Mengdi Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Columbia University",
                "Princeton University",
                "University of Chicago",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19682.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#small_models",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ curriculum learning Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°",
                    "desc": "GenEnv â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ curriculum learning, Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğº Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ»ÑŒÑ„Ğ°-Curriculum Reward Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 40.3% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ 7B baseline Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 3.3 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "GenEnv: Adaptive Learning for Enhanced LLM Performance",
                    "desc": "GenEnv is a novel framework that enhances the performance of Large Language Model (LLM) agents by utilizing a co-evolutionary game with a generative environment simulator. This approach allows the simulator to create dynamic tasks that adapt to the agent's current skill level, promoting efficient learning. By implementing an Î±-Curriculum Reward, GenEnv aligns task difficulty with the agent's capabilities, leading to significant performance improvements. The framework demonstrates a 40.3% increase in performance over 7B baselines while using 3.3 times less data compared to traditional offline augmentation methods."
                },
                "zh": {
                    "title": "GenEnvï¼šåŠ¨æ€ç”Ÿæˆç¯å¢ƒæå‡LLMæ€§èƒ½çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "GenEnvæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡ä¸ç”Ÿæˆç¯å¢ƒæ¨¡æ‹Ÿå™¨çš„å…±åŒè¿›åŒ–æ¸¸æˆï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒGenEnvä½¿ç”¨åŠ¨æ€çš„è¯¾ç¨‹ç­–ç•¥ï¼ŒæŒç»­ç”Ÿæˆä¸ä»£ç†èƒ½åŠ›ç›¸åŒ¹é…çš„ä»»åŠ¡ï¼Œä»è€Œå®ç°æ•°æ®çš„åŠ¨æ€æ¼”å˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç®€å•æœ‰æ•ˆçš„Î±-è¯¾ç¨‹å¥–åŠ±æœºåˆ¶ï¼Œç¡®ä¿ä»»åŠ¡éš¾åº¦ä¸ä»£ç†çš„å½“å‰èƒ½åŠ›ç›¸ä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenEnvåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»£ç†æ€§èƒ½æé«˜äº†40.3%ï¼Œå¹¶ä¸”ä½¿ç”¨çš„æ•°æ®é‡æ¯”ç¦»çº¿å¢å¼ºæ–¹æ³•å°‘3.3å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19693",
            "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
            "url": "https://huggingface.co/papers/2512.19693",
            "abstract": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
            "score": 3,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "a014d388da9b25ca",
            "authors": [
                "Weichen Fan",
                "Haiwen Diao",
                "Quan Wang",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19693.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸŒˆ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°, Ğ° Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Unified Autoencoding, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ImageNet Ğ¸ MS-COCO Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµĞ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Harmonizing Semantic and Pixel Information in Unified Autoencoding",
                    "desc": "This paper introduces Unified Autoencoding (UAE), a novel approach that integrates semantic and pixel-level information using a frequency-band modulator. The authors analyze how different encoders capture various frequency components, revealing that semantic encoders focus on low-frequency data while pixel encoders retain high-frequency details. They propose the Prism Hypothesis, suggesting that different data modalities project onto a shared feature spectrum, similar to how a prism disperses light. Through extensive testing on image benchmarks like ImageNet and MS-COCO, the UAE demonstrates superior performance by effectively merging abstract meanings with detailed pixel information in a unified latent space."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è‡ªç¼–ç ï¼šèåˆè¯­ä¹‰ä¸åƒç´ çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç»Ÿä¸€è‡ªç¼–ç ï¼ˆUnified Autoencoding, UAEï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†è¯­ä¹‰ä¿¡æ¯å’Œåƒç´ çº§ä¿¡æ¯ç»“åˆèµ·æ¥ã€‚é€šè¿‡é¢‘ç‡å¸¦è°ƒåˆ¶å™¨ï¼ŒUAEèƒ½å¤Ÿåœ¨åŒä¸€ä¸ªæ½œåœ¨ç©ºé—´ä¸­æœ‰æ•ˆåœ°èåˆæŠ½è±¡çš„è¯­ä¹‰ç»“æ„å’Œç»†è‡´çš„åƒç´ ç»†èŠ‚ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­ä¹‰ç¼–ç å™¨ä¸»è¦æ•æ‰ä½é¢‘æˆåˆ†ï¼Œè€Œåƒç´ ç¼–ç å™¨åˆ™ä¿ç•™é«˜é¢‘ä¿¡æ¯ï¼Œè¿™ä¸ºç¼–ç å™¨çš„è¡Œä¸ºæä¾›äº†æ–°çš„è§†è§’ã€‚é€šè¿‡åœ¨ImageNetå’ŒMS-COCOåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†UAEåœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19678",
            "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
            "url": "https://huggingface.co/papers/2512.19678",
            "abstract": "WorldWarp addresses the challenge of generating consistent long-range videos by integrating a 3D geometric cache with a spatio-temporal diffusion model, ensuring structural consistency and textural refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: https://hyokong.github.io/worldwarp-page/{https://hyokong.github.io/worldwarp-page/}.",
            "score": 3,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "223c96ada398b23f",
            "authors": [
                "Hanyang Kong",
                "Xingyi Yang",
                "Xiaoxu Zheng",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19678.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ĞºÑÑˆĞ°",
                    "desc": "WorldWarp Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ 3D ĞºÑÑˆ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Gaussian Splatting Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¼ ÑˆÑƒĞ¼Ğ°, Ğ³Ğ´Ğµ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° ÑƒĞ¶Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ â€” Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹. Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºÑÑˆ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "WorldWarp: Bridging 3D Geometry and Video Fidelity",
                    "desc": "WorldWarp is a novel framework designed to generate long-range videos that maintain both geometric consistency and high-quality textures. It combines a 3D geometric cache, built using Gaussian Splatting, with a spatio-temporal diffusion model to ensure that new frames adhere to the established 3D structure. The framework addresses challenges like occlusions and complex camera movements by dynamically updating the 3D cache and employing a unique noise schedule for generating and refining video content. This approach allows WorldWarp to produce videos with superior fidelity, where 3D geometry guides the overall structure and diffusion techniques enhance the visual quality."
                },
                "zh": {
                    "title": "ç”Ÿæˆä¸€è‡´æ€§é•¿è§†é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "WorldWarp æ˜¯ä¸€ä¸ªè§£å†³ç”Ÿæˆä¸€è‡´æ€§é•¿è§†é¢‘æŒ‘æˆ˜çš„æ¡†æ¶ã€‚å®ƒç»“åˆäº† 3D å‡ ä½•ç¼“å­˜å’Œæ—¶ç©ºæ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿ç»“æ„ä¸€è‡´æ€§å’Œçº¹ç†ç»†åŒ–ã€‚é€šè¿‡é«˜æ–¯å–·æº…æŠ€æœ¯æ„å»ºçš„åœ¨çº¿ 3D å‡ ä½•ç¼“å­˜ï¼Œèƒ½å¤Ÿå°†å†å²å†…å®¹æ‰­æ›²åˆ°æ–°è§†è§’ï¼Œä»è€Œä¿æŒå‡ ä½•ç»“æ„ã€‚è¯¥æ¡†æ¶çš„åˆ›æ–°ä¹‹å¤„åœ¨äºé‡‡ç”¨æ—¶ç©ºå˜åŒ–çš„å™ªå£°è°ƒåº¦ï¼ŒåŠ¨æ€æ›´æ–° 3D ç¼“å­˜ï¼Œä»¥å®ç°è§†é¢‘ç‰‡æ®µä¹‹é—´çš„ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19539",
            "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
            "url": "https://huggingface.co/papers/2512.19539",
            "abstract": "StoryMem enhances multi-shot video generation with cinematic quality and long-range consistency using a memory bank and pre-trained single-shot video diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
            "score": 3,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "afe7f52e42f30a9f",
            "authors": [
                "Kaiwen Zhang",
                "Liming Jiang",
                "Angtian Wang",
                "Jacob Zhiyuan Fang",
                "Tiancheng Zhi",
                "Qing Yan",
                "Hao Kang",
                "Xin Lu",
                "Xingang Pan"
            ],
            "affiliations": [
                "Intelligent Creation, ByteDance",
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19539.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#diffusion",
                    "#benchmark",
                    "#video",
                    "#dataset",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "StoryMem â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ½Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ˜Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Memory-to-Video Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğº ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ÑÑ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ RoPE ÑĞ´Ğ²Ğ¸Ğ³Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Transforming Video Storytelling with Memory-Enhanced Generation",
                    "desc": "StoryMem is a new approach for creating multi-shot videos that look cinematic and maintain consistency over long durations. It uses a memory bank to store important frames from previous shots, which helps in generating new shots that are coherent with the story. By combining this memory with single-shot video diffusion models, StoryMem can produce high-quality videos that transition smoothly between scenes. The framework also includes a method for selecting keyframes and filtering for aesthetic quality, ensuring that the generated videos are both informative and visually appealing."
                },
                "zh": {
                    "title": "StoryMemï¼šæå‡å¤šé•œå¤´è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§ä¸è´¨é‡",
                    "desc": "StoryMem æ˜¯ä¸€ç§å¢å¼ºå¤šé•œå¤´è§†é¢‘ç”Ÿæˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ç”µå½±è´¨é‡å’Œé•¿è·ç¦»ä¸€è‡´æ€§ã€‚å®ƒé€šè¿‡ä¸€ä¸ªè®°å¿†åº“å’Œé¢„è®­ç»ƒçš„å•é•œå¤´è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†é•¿ç¯‡è§†é¢‘å™äº‹é‡æ–°æ„é€ æˆè¿­ä»£é•œå¤´åˆæˆã€‚è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§æ–°é¢–çš„è®°å¿†åˆ°è§†é¢‘ï¼ˆM2Vï¼‰è®¾è®¡ï¼ŒåŠ¨æ€æ›´æ–°å†å²ç”Ÿæˆé•œå¤´çš„å…³é”®å¸§è®°å¿†ã€‚å®éªŒè¡¨æ˜ï¼ŒStoryMem åœ¨è·¨é•œå¤´ä¸€è‡´æ€§å’Œç¾å­¦è´¨é‡æ–¹é¢ä¼˜äºä»¥å¾€çš„æ–¹æ³•ï¼Œæ¨åŠ¨äº†è¿è´¯çš„é•¿è§†é¢‘å™äº‹çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19432",
            "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
            "url": "https://huggingface.co/papers/2512.19432",
            "abstract": "MobileWorld, a more challenging benchmark than AndroidWorld, includes diverse real-world mobile tasks and interactions, revealing significant gaps in current model capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
            "score": 2,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "81607d027cddfd45",
            "authors": [
                "Quyu Kong",
                "Xu Zhang",
                "Zhenyu Yang",
                "Nolan Gao",
                "Chen Liu",
                "Panrong Tong",
                "Chenglin Cai",
                "Hanzhang Zhou",
                "Jianan Zhang",
                "Liangyu Chen",
                "Zhidan Liu",
                "Steven Hoi",
                "Yue Wang"
            ],
            "affiliations": [
                "HKUST (GZ)",
                "Tongyi Lab, Alibaba Group",
                "University of Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19432.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MobileWorld Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ AndroidWorld. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 201 Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² 20 Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ agentĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ MCP-Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 51.7% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²."
                },
                "en": {
                    "title": "MobileWorld: Elevating Mobile AI Benchmarking to Real-World Challenges",
                    "desc": "MobileWorld is a new benchmark designed to challenge existing mobile AI models by simulating real-world tasks and interactions more accurately than AndroidWorld. It includes 201 tasks across 20 applications, focusing on long-horizon tasks and cross-application interactions, which are essential for realistic mobile usage. The benchmark introduces novel task categories that go beyond simple GUI manipulation, highlighting the need for improved user interaction capabilities. Results show that current models perform poorly on this benchmark, indicating significant gaps in their ability to handle complex mobile tasks and interactions."
                },
                "zh": {
                    "title": "ç§»åŠ¨æ™ºèƒ½çš„æ–°æŒ‘æˆ˜ï¼šMobileWorldåŸºå‡†æµ‹è¯•",
                    "desc": "MobileWorldæ˜¯ä¸€ä¸ªæ¯”AndroidWorldæ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ›´å¥½åœ°åæ˜ çœŸå®çš„ç§»åŠ¨ä½¿ç”¨åœºæ™¯ã€‚å®ƒåŒ…å«201ä¸ªä»»åŠ¡ï¼Œæ¶µç›–20ä¸ªåº”ç”¨ç¨‹åºï¼Œå¼ºè°ƒè·¨åº”ç”¨çš„é•¿æ—¶é—´ä»»åŠ¡å’Œå¤šåº”ç”¨ä»»åŠ¡ã€‚ä¸AndroidWorldç›¸æ¯”ï¼ŒMobileWorldçš„ä»»åŠ¡å®Œæˆæ­¥éª¤å‡ ä¹ç¿»å€ï¼Œå¹¶å¼•å…¥äº†æ–°çš„ä»»åŠ¡ç±»åˆ«ï¼Œå¦‚ä»£ç†-ç”¨æˆ·äº¤äº’å’ŒMCPå¢å¼ºä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨ç”¨æˆ·äº¤äº’å’ŒMCPè°ƒç”¨æ–¹é¢è¡¨ç°ä¸ä½³ï¼ŒæŒ‡æ˜äº†æœªæ¥ç§»åŠ¨æ™ºèƒ½å‘å±•çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.18003",
            "title": "Name That Part: 3D Part Segmentation and Naming",
            "url": "https://huggingface.co/papers/2512.18003",
            "abstract": "ALIGN-Parts addresses semantic 3D part segmentation by aligning implicit 3D part representations with part descriptions using geometric, appearance, and semantic cues, supporting open-vocabulary part naming and creating a unified ontology for multiple datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.",
            "score": 1,
            "issue_id": 192,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "7796f8dea8db72a1",
            "authors": [
                "Soumava Paul",
                "Prakhar Kaushik",
                "Ankit Vaidya",
                "Anand Bhattad",
                "Alan Yuille"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.18003.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#3d",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ALIGN-Parts â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ¼Ñ‘Ğ½. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ 3D-Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ (Ğ¿Ğ°Ñ€Ñ‚Ğ»ĞµÑ‚Ñ‹) Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ¸Ğ½Ğ³-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with ALIGN-Parts",
                    "desc": "ALIGN-Parts is a method for semantic 3D part segmentation, which means it helps break down 3D objects into meaningful parts with names. It tackles the problem of inconsistent part definitions across different datasets, which makes training models difficult. The approach uses a combination of geometric information, visual features, and semantic descriptions to align 3D part representations with their names. This allows for flexible part naming and creates a unified system for categorizing parts across various datasets, enhancing the model's ability to recognize and name parts accurately."
                },
                "zh": {
                    "title": "ALIGN-Partsï¼šç»Ÿä¸€3Déƒ¨ä»¶åˆ†å‰²ä¸å‘½åçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "ALIGN-Parts è®ºæ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰3Déƒ¨ä»¶åˆ†å‰²çš„æ–¹æ³•ï¼Œé€šè¿‡å‡ ä½•ã€å¤–è§‚å’Œè¯­ä¹‰çº¿ç´¢å°†éšå¼3Déƒ¨ä»¶è¡¨ç¤ºä¸éƒ¨ä»¶æè¿°å¯¹é½ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æ•°æ®é›†ä¸­éƒ¨ä»¶æ³¨é‡Šä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæ”¯æŒå¼€æ”¾è¯æ±‡çš„éƒ¨ä»¶å‘½åã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†éƒ¨ä»¶å‘½åè§†ä¸ºç›´æ¥çš„é›†åˆå¯¹é½ä»»åŠ¡ï¼Œåˆ©ç”¨åŒå‘åˆ†é…å°†å½¢çŠ¶åˆ†è§£ä¸ºéƒ¨ä»¶å°å—ï¼Œå¹¶ä¸éƒ¨ä»¶æè¿°åŒ¹é…ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æœ¬ä½“ï¼Œæ•´åˆäº†å¤šä¸ªæ•°æ®é›†çš„éƒ¨ä»¶ä¿¡æ¯ï¼Œæ”¯æŒé›¶æ ·æœ¬åŒ¹é…å’Œç½®ä¿¡åº¦æ ¡å‡†çš„é¢„æµ‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-22.html",
    "link_next": "2025-12-24.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "22.12",
        "en": "12/22",
        "zh": "12æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.12",
        "en": "12/24",
        "zh": "12æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 3,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}