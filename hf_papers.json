{
    "date": {
        "ru": "8 ноября",
        "en": "November 8",
        "zh": "11月8日"
    },
    "time_utc": "2024-11-08 22:11",
    "weekday": 4,
    "issue_id": 481,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.04905",
            "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
            "url": "https://huggingface.co/papers/2411.04905",
            "abstract": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems.While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an ``open cookbook'' for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.",
            "score": 53,
            "issue_id": 465,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "799dedd6597ce7ab",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#synthetic",
                    "#agents",
                    "#plp"
                ],
                "emoji": "🧑‍💻",
                "ru": {
                    "title": "OpenCoder: открытая книга рецептов для создания топовых языковых моделей кода",
                    "desc": "OpenCoder - это высококачественная языковая модель для работы с кодом, сопоставимая по производительности с ведущими моделями. Авторы не только предоставляют веса модели и код для инференса, но и полный набор воспроизводимых данных для обучения, конвейер обработки данных и подробные протоколы обучения. Ключевыми ингредиентами для создания модели такого уровня являются оптимизированные эвристические правила очистки данных, методы дедупликации, использование текстового корпуса, связанного с кодом, и высококачественные синтетические данные. Эта открытость призвана ускорить исследования и обеспечить воспроизводимый прогресс в области ИИ для работы с кодом."
                },
                "en": {
                    "title": "OpenCoder: Unlocking Code AI with Transparency and Reproducibility",
                    "desc": "This paper introduces OpenCoder, a high-performance large language model (LLM) specifically designed for code generation and reasoning tasks. It addresses the lack of open-access models that provide reproducible data processing and transparent training protocols, which are essential for scientific research. OpenCoder not only matches the performance of proprietary models but also shares its model weights, inference code, and detailed training methodologies. By emphasizing data cleaning, corpus recall, and synthetic data generation, OpenCoder aims to enhance accessibility and foster reproducible advancements in the field of code AI."
                },
                "zh": {
                    "title": "OpenCoder：开放的顶级代码大语言模型",
                    "desc": "本文介绍了OpenCoder，一个高质量的代码大语言模型（LLM），旨在为科学研究提供开放的资源。与其他模型不同，OpenCoder不仅提供模型权重和推理代码，还包括可重复的训练数据和完整的数据处理流程。我们识别出构建顶级代码LLM的关键要素，包括数据清洗的启发式规则、与代码相关的文本语料库的回忆以及高质量的合成数据。通过这种开放性，我们希望加速代码人工智能的研究和可重复的进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05003",
            "title": "ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning",
            "url": "https://huggingface.co/papers/2411.05003",
            "abstract": "Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique.",
            "score": 37,
            "issue_id": 464,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "f71f2e0f1addbe57",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#hallucinations"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Переснимаем реальность: новые ракурсы для любого видео",
                    "desc": "ReCapture - это метод генерации новых видео с измененными траекториями камеры на основе одного пользовательского видео. Он позволяет перегенерировать исходное видео с другими углами обзора и кинематографическим движением камеры, включая правдоподобное восстановление невидимых частей сцены. Метод работает в два этапа: сначала создается шумное опорное видео с новой траекторией камеры, а затем оно очищается и делается временно согласованным. ReCapture использует мультивидовые диффузионные модели и рендеринг облака точек на основе глубины."
                },
                "en": {
                    "title": "ReCapture: Transforming User Videos with New Camera Perspectives",
                    "desc": "This paper introduces ReCapture, a novel method for generating new videos with different camera angles from a single user-provided video. It leverages multiview diffusion models and depth-based point cloud rendering to create an initial noisy video with a new camera trajectory. The method then refines this video using a masked video fine-tuning technique to ensure temporal consistency and clarity. Additionally, ReCapture can convincingly generate parts of the scene that were not visible in the original video, enhancing the overall viewing experience."
                },
                "zh": {
                    "title": "ReCapture：从用户视频生成新视角的魔法",
                    "desc": "最近在视频建模方面取得了突破，使得生成视频中的相机轨迹可控。然而，这些方法无法直接应用于用户提供的非生成视频。本文提出了一种名为ReCapture的方法，可以从单个用户提供的视频生成具有新相机轨迹的新视频。该方法不仅能够从不同角度重新生成参考视频，还能合理地幻觉出参考视频中不可见的场景部分。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04965",
            "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
            "url": "https://huggingface.co/papers/2411.04965",
            "abstract": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.",
            "score": 27,
            "issue_id": 466,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "dcfd440f9caf6714",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "BitNet a4.8: Эффективность и производительность в мире языковых моделей",
                    "desc": "Статья представляет BitNet a4.8 - усовершенствованную версию 1-битной языковой модели. Эта модель использует 4-битные активации для входных данных в слоях внимания и прямой связи, а также применяет разреживание и 8-битное квантование для промежуточных состояний. Эксперименты показывают, что BitNet a4.8 достигает производительности, сравнимой с BitNet b1.58, при эквивалентных затратах на обучение, но с более быстрым выводом. Модель активирует только 55% параметров и поддерживает 3-битный KV-кэш, повышая эффективность развертывания и вывода крупномасштабных языковых моделей."
                },
                "en": {
                    "title": "Efficient Inference with 4-bit Activations in LLMs",
                    "desc": "This paper introduces BitNet a4.8, a new model that enhances the efficiency of 1-bit Large Language Models (LLMs) by using 4-bit activations. It combines hybrid quantization and sparsification techniques to reduce errors from outlier channels, allowing for better performance. The model uses 4-bit activations in key layers while applying 8-bit quantization to intermediate states, resulting in faster inference times. Experiments show that BitNet a4.8 matches the performance of its predecessor, BitNet b1.58, while being more efficient in terms of parameter activation and supporting a 3-bit key-value cache."
                },
                "zh": {
                    "title": "提升大型语言模型推理效率的新方法",
                    "desc": "最近关于1位大型语言模型（LLMs）的研究，如BitNet b1.58，展示了在保持性能的同时降低推理成本的前景。本文介绍了BitNet a4.8，支持1位LLMs的4位激活。BitNet a4.8采用混合量化和稀疏化策略，以减轻由异常通道引入的量化误差。实验表明，BitNet a4.8在训练成本相当的情况下，推理速度更快，且仅激活55%的参数，支持3位KV缓存，进一步提高了大规模LLM的部署和推理效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04928",
            "title": "DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion",
            "url": "https://huggingface.co/papers/2411.04928",
            "abstract": "In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively represented through sequences of video frames. While recent video diffusion models have shown remarkable success in producing vivid visuals, they face limitations in directly recovering 3D/4D scenes due to limited spatial and temporal controllability during generation. To overcome this, we propose ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant data. This controllable video diffusion approach enables precise manipulation of spatial structure and temporal dynamics, allowing us to reconstruct both 3D and 4D representations from sequential frames with the combination of spatial and temporal dimensions. Additionally, to bridge the gap between generated videos and real-world scenes, we introduce a trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation. Extensive experiments on various real-world and synthetic datasets demonstrate that DimensionX achieves superior results in controllable video generation, as well as in 3D and 4D scene generation, compared with previous methods.",
            "score": 16,
            "issue_id": 466,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "b0958f934bc56a95",
            "data": {
                "categories": [
                    "#3d",
                    "#video",
                    "#synthetic"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "От 2D к 4D: DimensionX раздвигает границы генеративных моделей",
                    "desc": "DimensionX - это фреймворк для генерации фотореалистичных 3D и 4D сцен из одного изображения с помощью видео-диффузии. Ключевым компонентом является ST-Director, который разделяет пространственные и временные факторы в видео-диффузии, обучая размерно-зависимые LoRA на данных с различными измерениями. Для 3D генерации используется механизм, учитывающий траекторию, а для 4D - стратегия шумоподавления, сохраняющая идентичность. Эксперименты показывают превосходство DimensionX в контролируемой генерации видео и создании 3D/4D сцен по сравнению с существующими методами."
                },
                "en": {
                    "title": "Transforming Single Images into Stunning 3D and 4D Worlds!",
                    "desc": "DimensionX is a novel framework that generates photorealistic 3D and 4D scenes from a single image using video diffusion techniques. It addresses the limitations of existing video diffusion models by introducing ST-Director, which separates spatial and temporal factors, allowing for better control over the generated scenes. By learning dimension-aware Low-Rank Adaptations (LoRAs) from diverse data, DimensionX enhances the manipulation of both spatial structures and temporal dynamics. The framework also incorporates a trajectory-aware mechanism and an identity-preserving denoising strategy to improve the realism of generated scenes, outperforming previous methods in extensive experiments."
                },
                "zh": {
                    "title": "DimensionX：从单图像生成真实3D和4D场景的创新框架",
                    "desc": "本文介绍了DimensionX，一个框架，旨在通过单张图像和视频扩散生成逼真的3D和4D场景。我们的方法利用视频帧序列有效表示3D场景的空间结构和4D场景的时间演变。为了解决现有视频扩散模型在生成过程中空间和时间可控性不足的问题，我们提出了ST-Director，通过从维度变化的数据中学习维度感知的LoRA，解耦空间和时间因素。实验结果表明，DimensionX在可控视频生成以及3D和4D场景生成方面优于以往的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04996",
            "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models",
            "url": "https://huggingface.co/papers/2411.04996",
            "abstract": "The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. We evaluate MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8\\% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2\\% of the wall-clock time and text quality in 75.6\\% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs).",
            "score": 14,
            "issue_id": 465,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "53d29fd65eda072e",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективные мультимодальные трансформеры: меньше вычислений, та же мощность",
                    "desc": "Эта статья представляет Mixture-of-Transformers (MoT) - новую архитектуру для мультимодальных языковых моделей, которая значительно снижает вычислительные затраты при предобучении. MoT разделяет параметры модели по модальностям, позволяя эффективно обрабатывать текст, изображения и речь в единой системе. Эксперименты показывают, что MoT достигает производительности плотных базовых моделей, используя значительно меньше вычислительных ресурсов. Это позволяет создавать более эффективные мультимодальные системы искусственного интеллекта."
                },
                "en": {
                    "title": "Efficient Multi-Modal Processing with Mixture-of-Transformers",
                    "desc": "This paper presents Mixture-of-Transformers (MoT), a novel sparse multi-modal transformer architecture designed to efficiently handle text, images, and speech. By decoupling non-embedding parameters by modality, MoT allows for specialized processing while maintaining global self-attention across the entire input. The architecture significantly reduces computational costs, achieving comparable performance to dense models with fewer floating point operations (FLOPs). Evaluations show that MoT not only matches but often outperforms dense baselines in various settings, demonstrating its effectiveness in multi-modal tasks."
                },
                "zh": {
                    "title": "混合变换器：高效的多模态学习新方案",
                    "desc": "本论文介绍了一种新的稀疏多模态变换器架构，称为混合变换器（MoT），旨在降低大语言模型的预训练计算成本。MoT通过模态解耦模型的非嵌入参数，使得不同模态（如文本、图像和语音）可以进行特定处理，同时保持全局自注意力机制。实验结果表明，MoT在多个设置下表现出色，能够以更少的计算资源达到与密集基线相当的性能。该模型在图像生成和语音处理任务中均展现了显著的效率优势，证明了其在多模态学习中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04709",
            "title": "TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2411.04709",
            "abstract": "Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io.",
            "score": 10,
            "issue_id": 464,
            "pub_date": "2024-11-05",
            "pub_date_card": {
                "ru": "5 ноября",
                "en": "November 5",
                "zh": "11月5日"
            },
            "hash": "fcc8e4daf79a82b9",
            "data": {
                "categories": [
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "TIP-I2V: Революция в изучении промптов для генерации видео из изображений",
                    "desc": "Исследователи представили TIP-I2V - первый крупномасштабный набор данных, содержащий более 1,70 миллиона уникальных пользовательских текстовых и изображений-промптов для генерации видео из изображений. Датасет также включает соответствующие сгенерированные видео от пяти современных моделей преобразования изображений в видео. TIP-I2V позволяет анализировать предпочтения пользователей, оценивать многомерную производительность обученных моделей и решать проблемы безопасности, связанные с дезинформацией. Этот набор данных подчеркивает важность специализированного датасета промптов для генерации видео из изображений и открывает новые возможности для исследований в этой области."
                },
                "en": {
                    "title": "Empowering Image-to-Video Generation with TIP-I2V Dataset",
                    "desc": "This paper presents TIP-I2V, the first large-scale dataset containing over 1.70 million unique user-provided text and image prompts for image-to-video generation. The dataset aims to enhance the controllability and visual consistency of video generation models by providing a rich source of prompts. It also includes generated videos from five advanced image-to-video models, facilitating comparative analysis and model evaluation. By addressing the lack of dedicated datasets, TIP-I2V supports research in user preferences and model safety, particularly in mitigating misinformation issues."
                },
                "zh": {
                    "title": "TIP-I2V：图像到视频生成的新数据集",
                    "desc": "视频生成模型正在改变内容创作，图像到视频模型因其更好的可控性和视觉一致性而受到关注。尽管这些模型很受欢迎，但目前缺乏专门用于研究用户提供的文本和图像提示的数据集。本文介绍了TIP-I2V，这是第一个大规模的数据集，包含超过170万个独特的用户提供的文本和图像提示，专门用于图像到视频生成。该数据集的推出将推动图像到视频研究的进展，帮助研究人员分析用户偏好并评估模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04496",
            "title": "Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model",
            "url": "https://huggingface.co/papers/2411.04496",
            "abstract": "To increase social bonding with interlocutors, humans naturally acquire the ability to respond appropriately in a given situation by considering which conversational skill is most suitable for the response - a process we call skill-of-mind. For large language model (LLM)-based conversational agents, planning appropriate conversational skills, as humans do, is challenging due to the complexity of social dialogue, especially in interactive scenarios. To address this, we propose a skill-of-mind-annotated conversation dataset, named Multifaceted Skill-of-Mind, which includes multi-turn and multifaceted conversational skills across various interactive scenarios (e.g., long-term, counseling, task-oriented), grounded in diverse social contexts (e.g., demographics, persona, rules of thumb). This dataset consists of roughly 100K conversations. Using this dataset, we introduce a new family of skill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B parameters. With extensive experiments, these models successfully demonstrate the skill-of-mind process and exhibit strong generalizability in inferring multifaceted skills across a variety of domains. Moreover, we show that Thanos significantly enhances the quality of responses generated by LLM-based conversational agents and promotes prosocial behavior in human evaluations.",
            "score": 9,
            "issue_id": 466,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "bf7486353434568f",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Языковые модели учатся искусству общения",
                    "desc": "Исследователи представили новый набор данных под названием Multifaceted Skill-of-Mind, содержащий аннотации навыков ведения диалога в различных интерактивных сценариях. На основе этого набора данных была разработана серия языковых моделей Thanos, способных выбирать подходящие навыки общения в зависимости от контекста. Эксперименты показали, что модели Thanos успешно демонстрируют процесс выбора навыков и обладают хорошей обобщающей способностью в различных областях. Кроме того, использование Thanos значительно улучшило качество ответов диалоговых агентов на основе больших языковых моделей и способствовало более просоциальному поведению по оценкам людей."
                },
                "en": {
                    "title": "Enhancing Conversational Skills in AI with Thanos",
                    "desc": "This paper introduces a new dataset called Multifaceted Skill-of-Mind, which helps large language models (LLMs) learn how to respond appropriately in social conversations. The dataset contains around 100,000 conversations that cover various interactive scenarios and social contexts, allowing LLMs to understand different conversational skills. The authors also present a new family of LLMs named Thanos, which are designed to incorporate these skills into their responses. Through experiments, Thanos models show improved response quality and promote positive social interactions in conversations."
                },
                "zh": {
                    "title": "提升对话质量的技能思维模型",
                    "desc": "本文提出了一种名为多面技能思维的对话数据集，旨在帮助大型语言模型（LLM）更好地理解和应用社交对话中的适当回应技能。该数据集包含约10万条对话，涵盖了多轮和多方面的对话技能，适用于不同的互动场景，如长期对话、咨询和任务导向。我们还介绍了一种新的LLM家族，名为Thanos，具有1B、3B和8B参数规模，能够有效地展示技能思维过程，并在多种领域中推断多面技能。实验结果表明，Thanos显著提高了LLM对话代理生成的回应质量，并在人工评估中促进了亲社会行为。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04923",
            "title": "VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos",
            "url": "https://huggingface.co/papers/2411.04923",
            "abstract": "Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed for fine-grained pixel-level grounding in videos based on user-provided textual inputs. Our design seamlessly connects three key components: a Large Language Model, a dual vision encoder that emphasizes both spatial and temporal details, and a spatio-temporal decoder for accurate mask generation. This connection is facilitated via tunable V-L and L-V adapters that enable close Vision-Language (VL) alignment. The architecture is trained to synchronize both spatial and temporal elements of video content with textual instructions. To enable fine-grained grounding, we curate a multimodal dataset featuring detailed visually-grounded conversations using a semiautomatic annotation pipeline, resulting in a diverse set of 38k video-QA triplets along with 83k objects and 671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation. Experimental results show that our model consistently outperforms existing approaches across all three tasks.",
            "score": 8,
            "issue_id": 474,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "3db2b6994e9c5047",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#architecture",
                    "#games",
                    "#alignment",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Точная локализация в видео с помощью мультимодального ИИ",
                    "desc": "VideoGLaMM - это новая модель крупномасштабного мультимодального обучения, разработанная для точной пиксельной локализации в видео на основе текстовых запросов пользователя. Модель объединяет языковую модель, двойной энкодер зрения и пространственно-временной декодер для генерации масок. Для обучения модели был создан специальный набор данных с детальными визуально-обоснованными диалогами. Эксперименты показали превосходство VideoGLaMM над существующими подходами в задачах генерации обоснованных диалогов, визуальной локализации и сегментации видео по запросу."
                },
                "en": {
                    "title": "Achieving Pixel-Level Precision in Video-Text Alignment",
                    "desc": "This paper presents VideoGLaMM, a Large Multimodal Model (LMM) that enhances the alignment between videos and text at a fine-grained level. It addresses the challenges of pixel-level grounding by integrating a Large Language Model with a dual vision encoder that captures both spatial and temporal dynamics of video content. The model employs tunable adapters for effective Vision-Language alignment and is trained on a comprehensive multimodal dataset with 38k video-QA triplets. Experimental results demonstrate that VideoGLaMM outperforms existing models in tasks such as Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation."
                },
                "zh": {
                    "title": "视频与文本的精细对齐新突破",
                    "desc": "本论文介绍了一种名为VideoGLaMM的多模态模型，旨在实现视频与文本之间的精细像素级对齐。该模型结合了大型语言模型、双重视觉编码器和时空解码器，能够处理视频中的复杂空间和时间动态。通过可调的视觉-语言适配器，VideoGLaMM实现了视觉与语言的紧密对齐。实验结果表明，该模型在生成对话、视觉对齐和视频分割等任务上均优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05000",
            "title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?",
            "url": "https://huggingface.co/papers/2411.05000",
            "abstract": "As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data.",
            "score": 7,
            "issue_id": 474,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "72ef4dc00d41e203",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "🧵",
                "ru": {
                    "title": "Языковые модели в лабиринте длинного контекста: нити информации и границы понимания",
                    "desc": "Исследование посвящено анализу способностей современных языковых моделей (LLM) эффективно использовать увеличенный контекст. Авторы провели серию экспериментов по извлечению информации, оценивая 17 ведущих LLM на способность следовать нескольким информационным потокам в контексте. Результаты показали, что многие модели способны эффективно обрабатывать несколько потоков одновременно, но для некоторых моделей эффективная длина контекста оказалась значительно меньше заявленной. Исследование также подчеркивает важность учета различий в токенизации при сравнении моделей."
                },
                "en": {
                    "title": "Unlocking the Potential of Long-Context LLMs",
                    "desc": "This paper investigates how well large language models (LLMs) can utilize their extended context capabilities for complex information retrieval and reasoning tasks. The authors conduct experiments with 17 leading LLMs to assess their ability to track multiple threads of information within their context windows. They discover that while many models can maintain performance across various threads, the effective context limit is often shorter than the maximum supported length, leading to decreased accuracy with larger contexts. Additionally, the study emphasizes the need for caution when comparing token counts from different tokenizers, as they can represent different amounts of text."
                },
                "zh": {
                    "title": "长上下文模型的潜力与挑战",
                    "desc": "随着大型语言模型（LLMs）上下文限制的增加，应用范围和下游功能也在扩大。许多现实任务的决策依赖于分散在不同文档中的细节，这些文档通常包含大量无关信息。长上下文LLMs在复杂信息检索和推理方面表现出色，但我们对它们如何有效利用上下文的理解仍然滞后。我们的实验表明，尽管许多模型在跟踪信息线程方面表现良好，但有效的上下文限制往往比支持的上下文长度要短，且随着上下文窗口的增大，准确性会下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04999",
            "title": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation",
            "url": "https://huggingface.co/papers/2411.04999",
            "abstract": "Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems. Our code as well as our experiment and deployment videos are open sourced and can be found on our project website: https://dynamem.github.io/",
            "score": 7,
            "issue_id": 465,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "47171ef52d95552a",
            "data": {
                "categories": [
                    "#robotics",
                    "#3d",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Динамическая память для роботов в изменяющемся мире",
                    "desc": "DynaMem - новый подход к мобильной манипуляции с открытым словарем в динамических средах. Система использует динамическую пространственно-семантическую память для представления окружения робота, построенную на основе 3D структуры данных облаков точек. DynaMem применяет мультимодальные языковые модели и модели компьютерного зрения для локализации объектов по запросам на естественном языке. Эксперименты показали двукратное улучшение успешности захвата и перемещения нестационарных объектов по сравнению с современными статическими системами."
                },
                "en": {
                    "title": "Empowering Robots with Dynamic Memory for Open-World Manipulation",
                    "desc": "This paper introduces DynaMem, a novel approach for open-vocabulary mobile manipulation that allows robots to adapt to dynamic environments. Unlike traditional systems that rely on static environments, DynaMem utilizes a dynamic spatio-semantic memory to keep track of changes in the robot's surroundings. It employs a 3D data structure to manage point clouds and leverages multimodal large language models (LLMs) for object localization. The results show that DynaMem significantly improves the robot's ability to interact with non-stationary objects, achieving a 70% success rate in pick-and-drop tasks, which is more than double the performance of existing static systems."
                },
                "zh": {
                    "title": "动态记忆，智能操作！",
                    "desc": "本研究提出了一种新的动态空间语义记忆方法DynaMem，用于开放词汇的移动操作。与传统静态环境系统不同，DynaMem能够在不断变化的环境中进行物体定位和操作。该方法利用三维数据结构维护动态记忆，并通过多模态大语言模型进行查询。实验结果表明，DynaMem在非静态物体的抓取和放置任务中成功率达70%，显著优于现有静态系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05007",
            "title": "SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models",
            "url": "https://huggingface.co/papers/2411.05007",
            "abstract": "Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, na\\\"{\\i}vely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-Sigma, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5times, achieving 3.0times speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs. Our quantization library and inference engine are open-sourced.",
            "score": 6,
            "issue_id": 466,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "636ee9cbe15eefb6",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение диффузионных моделей с помощью 4-битного квантования",
                    "desc": "Статья представляет новый метод квантования под названием SVDQuant для ускорения диффузионных моделей. Метод использует низкоранговую ветвь для поглощения выбросов в весах и активациях, позволяя эффективно квантовать их до 4 бит. Авторы также разработали движок вывода Nunchaku, который оптимизирует выполнение квантованных моделей. Эксперименты показали значительное снижение использования памяти и ускорение работы крупных моделей генерации изображений."
                },
                "en": {
                    "title": "Accelerating Diffusion Models with SVDQuant: Efficient 4-Bit Quantization",
                    "desc": "This paper presents SVDQuant, a novel 4-bit quantization method designed to enhance the efficiency of diffusion models for image generation. As these models increase in size, they face challenges related to memory usage and latency, which SVDQuant aims to address by effectively managing outliers in weights and activations. The method utilizes Singular Value Decomposition (SVD) to absorb outliers into a low-rank branch, improving the quantization process without compromising image quality. Additionally, the co-designed inference engine, Nunchaku, optimizes memory access, resulting in significant reductions in memory usage and increased processing speed for large models."
                },
                "zh": {
                    "title": "加速扩散模型的4位量化新方法",
                    "desc": "扩散模型在生成高质量图像方面非常有效，但随着模型规模的增大，它们需要更多的内存并且延迟更高，这给部署带来了挑战。本文提出了一种新的4位量化方法SVDQuant，通过量化权重和激活值来加速扩散模型。与传统的平滑方法不同，SVDQuant通过低秩分支吸收异常值，从而减轻量化过程中的困难。实验结果表明，SVDQuant在保持图像质量的同时，显著减少了内存使用和提高了速度，适用于更多互动应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04989",
            "title": "SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2411.04989",
            "abstract": "Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds. Recent techniques address this issue by fine-tuning a pre-trained model to follow conditioning signals, such as bounding boxes or point trajectories. Yet, this fine-tuning procedure can be computationally expensive, and it requires datasets with annotated object motion, which can be difficult to procure. In this work, we introduce SG-I2V, a framework for controllable image-to-video generation that is self-guidedx2013offering zero-shot control by relying solely on the knowledge present in a pre-trained image-to-video diffusion model without the need for fine-tuning or external knowledge. Our zero-shot method outperforms unsupervised baselines while being competitive with supervised models in terms of visual quality and motion fidelity.",
            "score": 6,
            "issue_id": 465,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "a707043470b8dffd",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Управляемая генерация видео без дополнительного обучения",
                    "desc": "SG-I2V - это новый фреймворк для управляемой генерации видео из изображений. Он использует предобученную диффузионную модель без необходимости дополнительного обучения или внешних данных. Метод превосходит неконтролируемые базовые модели и конкурирует с контролируемыми по качеству изображения и точности движения. SG-I2V позволяет легко настраивать конкретные элементы генерируемых видео, такие как движение объектов или камеры."
                },
                "en": {
                    "title": "Zero-Shot Control in Image-to-Video Generation",
                    "desc": "This paper presents SG-I2V, a novel framework for generating videos from images with controllable features. Unlike traditional methods that require fine-tuning on annotated datasets, SG-I2V operates in a zero-shot manner, leveraging a pre-trained image-to-video diffusion model. This approach allows for easier manipulation of elements like object motion and camera movement without the computational costs associated with fine-tuning. The results show that SG-I2V achieves high visual quality and motion fidelity, outperforming unsupervised methods and competing with supervised ones."
                },
                "zh": {
                    "title": "SG-I2V：高效的图像到视频生成方法",
                    "desc": "本文介绍了一种名为SG-I2V的框架，用于可控的图像到视频生成。该方法利用预训练的图像到视频扩散模型，提供零-shot控制，避免了繁琐的微调过程。与无监督基线相比，我们的方法在视觉质量和运动保真度上表现优越，并且与监督模型相竞争。此研究为视频生成提供了一种更高效的解决方案，减少了对标注数据集的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04335",
            "title": "GazeGen: Gaze-Driven User Interaction for Visual Content Generation",
            "url": "https://huggingface.co/papers/2411.04335",
            "abstract": "We present GazeGen, a user interaction system that generates visual content (images and videos) for locations indicated by the user's eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and generative AI, GazeGen performs gaze-controlled image adding/deleting, repositioning, and surface material changes of image objects, and converts static images into videos. Central to GazeGen is the DFT Gaze (Distilled and Fine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters, performing accurate real-time gaze predictions tailored to individual users' eyes on small edge devices. GazeGen is the first system to combine visual content generation with real-time gaze estimation, made possible exclusively by DFT Gaze. This real-time gaze estimation enables various visual content generation tasks, all controlled by the user's gaze. The input for DFT Gaze is the user's eye images, while the inputs for visual content generation are the user's view and the predicted gaze point from DFT Gaze. To achieve efficient gaze predictions, we derive the small model from a large model (10x larger) via novel knowledge distillation and personal adaptation techniques. We integrate knowledge distillation with a masked autoencoder, developing a compact yet powerful gaze estimation model. This model is further fine-tuned with Adapters, enabling highly accurate and personalized gaze predictions with minimal user input. DFT Gaze ensures low-latency and precise gaze tracking, supporting a wide range of gaze-driven tasks. We validate the performance of DFT Gaze on AEA and OpenEDS2020 benchmarks, demonstrating low angular gaze error and low latency on the edge device (Raspberry Pi 4). Furthermore, we describe applications of GazeGen, illustrating its versatility and effectiveness in various usage scenarios.",
            "score": 5,
            "issue_id": 466,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "caa85d9f2385fc2a",
            "data": {
                "categories": [
                    "#agents",
                    "#cv",
                    "#video",
                    "#edge_computing",
                    "#training"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Взглядом управляй: революция в генерации визуального контента",
                    "desc": "Исследователи представляют систему GazeGen, которая генерирует визуальный контент на основе направления взгляда пользователя. В основе системы лежит сверхлегкая модель DFT Gaze, выполняющая точное предсказание направления взгляда в реальном времени на небольших устройствах. GazeGen позволяет интуитивно манипулировать визуальным контентом, используя передовые методы обнаружения объектов и генеративного ИИ. Система была протестирована на эталонных наборах данных, продемонстрировав низкую угловую ошибку определения взгляда и низкую задержку на периферийных устройствах."
                },
                "en": {
                    "title": "GazeGen: Transforming Eye Gaze into Visual Content Control",
                    "desc": "GazeGen is an innovative user interaction system that generates visual content based on where a user is looking. It utilizes advanced object detection and generative AI techniques to allow users to manipulate images and videos by simply gazing at specific areas. The core of GazeGen is the DFT Gaze agent, a lightweight model that accurately predicts gaze in real-time, making it suitable for use on small devices like the Raspberry Pi 4. By combining gaze estimation with visual content generation, GazeGen enables intuitive and personalized interactions with digital media."
                },
                "zh": {
                    "title": "眼动控制的视觉内容生成系统",
                    "desc": "GazeGen是一个用户交互系统，可以根据用户的眼动生成视觉内容（图像和视频）。它利用先进的物体检测和生成AI技术，实现了基于视线的图像添加、删除、重新定位和表面材质变化。系统的核心是DFT Gaze代理，这是一个轻量级模型，能够在小型边缘设备上进行实时的眼动预测。GazeGen是首个将视觉内容生成与实时眼动估计相结合的系统，展示了其在多种应用场景中的灵活性和有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04752",
            "title": "RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval",
            "url": "https://huggingface.co/papers/2411.04752",
            "abstract": "Code-mixing, the integration of lexical and grammatical elements from multiple languages within a single sentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman script, especially among migrant communities who form online groups to share relevant local information. This paper focuses on the challenges of extracting relevant information from code-mixed conversations, specifically within Roman transliterated Bengali mixed with English. This study presents a novel approach to address these challenges by developing a mechanism to automatically identify the most relevant answers from code-mixed conversations. We have experimented with a dataset comprising of queries and documents from Facebook, and Query Relevance files (QRels) to aid in this task. Our results demonstrate the effectiveness of our approach in extracting pertinent information from complex, code-mixed digital conversations, contributing to the broader field of natural language processing in multilingual and informal text environments. We use GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant documents to frame a mathematical model which helps to detect relevant documents corresponding to a query.",
            "score": 5,
            "issue_id": 466,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "e2dbb14c8f2ca6ef",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#data"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Извлечение информации из многоязычных разговоров: новый подход к обработке смешанных текстов",
                    "desc": "Статья посвящена проблеме извлечения релевантной информации из смешанных языковых разговоров в социальных сетях Индии. Авторы разработали новый подход для автоматического определения наиболее релевантных ответов в разговорах на смеси бенгальского и английского языков, записанных латиницей. В исследовании использовались данные из Facebook и файлы Query Relevance, а также применялась языковая модель GPT-3.5 Turbo. Результаты демонстрируют эффективность предложенного метода в обработке сложных многоязычных текстов в неформальной цифровой среде."
                },
                "en": {
                    "title": "Enhancing Information Retrieval in Code-Mixed Conversations",
                    "desc": "This paper addresses the challenge of extracting relevant information from code-mixed conversations, particularly in Roman transliterated Bengali and English. It presents a novel mechanism that utilizes GPT-3.5 Turbo and a mathematical model to identify pertinent answers from social media interactions. The study experiments with a dataset from Facebook, focusing on queries and documents to enhance information retrieval. The results indicate that the proposed approach effectively navigates the complexities of multilingual and informal text, contributing to advancements in natural language processing."
                },
                "zh": {
                    "title": "从代码混合对话中提取信息的新方法",
                    "desc": "本文研究了在多语言环境中，如何从代码混合的对话中提取相关信息。特别是在印度，社交媒体用户常用罗马字母书写的孟加拉语与英语混合交流。我们提出了一种新方法，通过自动识别代码混合对话中的相关答案来解决这一挑战。实验结果表明，该方法在提取复杂数字对话中的相关信息方面是有效的，推动了自然语言处理领域的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04075",
            "title": "M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models",
            "url": "https://huggingface.co/papers/2411.04075",
            "abstract": "Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address this gap, we introduce M3SciQA, a multi-modal, multi-document scientific question answering benchmark designed for a more comprehensive evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, where each cluster represents a primary paper along with all its cited documents, mirroring the workflow of comprehending a single paper by requiring multi-modal and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of 18 foundation models. Our results indicate that current foundation models still significantly underperform compared to human experts in multi-modal information retrieval and in reasoning across multiple scientific documents. Additionally, we explore the implications of these findings for the future advancement of applying foundation models in multi-modal scientific literature analysis.",
            "score": 2,
            "issue_id": 480,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 ноября",
                "en": "November 6",
                "zh": "11月6日"
            },
            "hash": "654e787a9f0ab7ff",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#science"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "M3SciQA: Новый рубеж в оценке фундаментальных моделей для научного анализа",
                    "desc": "M3SciQA - это новый бенчмарк для оценки фундаментальных моделей в области научных вопросов и ответов. Он включает в себя мультимодальные и мультидокументные задачи, охватывающие 70 кластеров статей по обработке естественного языка. Результаты тестирования 18 моделей показали, что их производительность значительно уступает экспертам-людям в задачах мультимодального поиска информации и рассуждений по нескольким научным документам. Этот бенчмарк открывает новые перспективы для развития применения фундаментальных моделей в анализе научной литературы."
                },
                "en": {
                    "title": "M3SciQA: Bridging the Gap in Multi-Modal Scientific Understanding",
                    "desc": "This paper introduces M3SciQA, a new benchmark for evaluating foundation models in the context of scientific question answering. Unlike existing benchmarks that focus solely on single-document, text-only tasks, M3SciQA incorporates multi-modal and multi-document elements to better reflect real research workflows. The benchmark includes 1,452 expert-annotated questions related to clusters of natural language processing papers, requiring models to interpret both textual and non-textual data. Evaluation results show that current foundation models still lag behind human experts in retrieving multi-modal information and reasoning across multiple documents, highlighting the need for further advancements in this area."
                },
                "zh": {
                    "title": "M3SciQA：多模态科学问答的新基准",
                    "desc": "现有的基础模型评估基准主要集中在单文档、文本任务上，无法全面捕捉研究工作流程的复杂性。为了解决这个问题，我们提出了M3SciQA，这是一个多模态、多文档的科学问答基准，旨在更全面地评估基础模型。M3SciQA包含1452个专家注释的问题，涵盖70个自然语言处理论文集群，模拟理解单篇论文的工作流程。我们的评估结果表明，当前的基础模型在多模态信息检索和跨多个科学文档推理方面仍显著低于人类专家的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05005",
            "title": "Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models",
            "url": "https://huggingface.co/papers/2411.05005",
            "abstract": "Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or as mere feature extractors. In contrast to these isolated and thus sub-optimal efforts, we introduce a unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through a unique exploitation of the diffusion-denoising process. Within this framework, we further enhance discriminative visual perception via multi-modal generation, by utilizing the denoising network to create multi-modal data that mirror the distribution of the original training set. Importantly, Diff-2-in-1 optimizes the utilization of the created diverse and faithful data by leveraging a novel self-improving learning mechanism. Comprehensive experimental evaluations validate the effectiveness of our framework, showcasing consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation characterized by both realism and usefulness.",
            "score": 2,
            "issue_id": 477,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "249deab8440380fc",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#data",
                    "#multimodal",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Diff-2-in-1: Объединение генерации и восприятия в диффузионных моделях",
                    "desc": "Статья представляет новый универсальный фреймворк на основе диффузионных моделей под названием Diff-2-in-1. Этот фреймворк способен одновременно выполнять мультимодальную генерацию данных и плотное визуальное восприятие, используя процесс диффузионного шумоподавления. Diff-2-in-1 улучшает дискриминативное визуальное восприятие путем генерации мультимодальных данных, отражающих распределение исходного обучающего набора. Фреймворк также использует новый механизм самосовершенствующегося обучения для оптимизации использования созданных разнообразных и достоверных данных."
                },
                "en": {
                    "title": "Unifying Data Generation and Visual Perception with Diff-2-in-1",
                    "desc": "This paper presents a new framework called Diff-2-in-1 that integrates diffusion models for both data generation and visual perception tasks. Unlike previous approaches that used diffusion models in isolation, this framework leverages the diffusion-denoising process to enhance the performance of visual perception. It generates multi-modal data that closely resembles the original training data, improving the discriminative capabilities of the model. The framework also includes a self-improving learning mechanism to optimize the use of the generated data, leading to better performance across various tasks."
                },
                "zh": {
                    "title": "Diff-2-in-1：多模态生成与视觉感知的统一框架",
                    "desc": "本文提出了一种新的框架，称为Diff-2-in-1，旨在同时处理多模态数据生成和密集视觉感知。与以往将扩散模型视为独立组件的做法不同，该框架利用扩散去噪过程的独特特性，增强了视觉感知的判别能力。通过生成与原始训练集分布相似的多模态数据，Diff-2-in-1优化了生成数据的使用，采用了一种新颖的自我改进学习机制。实验结果表明，该框架在多种判别模型上均表现出一致的性能提升，并生成了高质量的多模态数据，具有真实感和实用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04952",
            "title": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding",
            "url": "https://huggingface.co/papers/2411.04952",
            "abstract": "Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images.",
            "score": 1,
            "issue_id": 477,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "56e0d2f2775dbda9",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rag",
                    "#dataset",
                    "#multimodal",
                    "#games",
                    "#long_context"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "M3DocRAG: Мультимодальные ответы на вопросы по документам нового поколения",
                    "desc": "Статья представляет M3DocRAG - новую мультимодальную систему для ответов на вопросы по документам. Она использует мультимодальный ретривер и мультимодальную языковую модель для эффективной обработки одного или нескольких документов с сохранением визуальной информации. M3DocRAG способна работать с различными контекстами документов, типами вопросов и модальностями доказательств. Авторы также представляют новый бенчмарк M3DocVQA для оценки систем открытого домена на более чем 3000 PDF-документах."
                },
                "en": {
                    "title": "M3DocRAG: Revolutionizing Document Visual Question Answering",
                    "desc": "The paper presents M3DocRAG, a new framework for Document Visual Question Answering (DocVQA) that addresses limitations of existing methods. Unlike traditional approaches that focus on single-page documents and often overlook visual elements, M3DocRAG can process multiple pages and various document types while retaining important visual information. It utilizes a multi-modal retriever and a multi-modal language model (MLM) to efficiently find relevant documents and answer questions, accommodating both text and visual evidence. The authors also introduce M3DocVQA, a benchmark for evaluating open-domain DocVQA, demonstrating that their framework outperforms existing models in several benchmarks."
                },
                "zh": {
                    "title": "M3DocRAG：跨页文档问答的新突破",
                    "desc": "本文介绍了一种新的多模态检索增强生成框架M3DocRAG，旨在解决文档视觉问答（DocVQA）中的挑战。现有方法主要处理单页文档，无法有效应对跨页或多文档的信息检索。M3DocRAG能够灵活处理不同的文档上下文和问题跳跃，同时保留重要的视觉信息，如图表和图像。通过在超过3000个PDF文档上进行评估，M3DocRAG在多个基准测试中表现优异，超越了许多强基线。"
                }
            }
        }
    ],
    "link_prev": "2024-11-07.html",
    "link_next": "2024-11-11.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "07.11",
        "en": "11/07",
        "zh": "11月7日"
    },
    "short_date_next": {
        "ru": "11.11",
        "en": "11/11",
        "zh": "11月11日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 4,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 1,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0,
        "#edge_computing": 1
    },
    "zh": {
        "text": "这篇文章讨论了大型语言模型（LLMs）在代码生成、推理任务和代理系统中的重要性。虽然开放访问的代码LLMs性能接近专有模型，但适合严谨科学研究的高质量代码LLMs仍然有限。为了填补这一空白，文章介绍了OpenCoder，一个顶尖的代码LLMs，其性能堪比领先模型，并且为研究社区提供了一个“开放的食谱”。OpenCoder不仅发布了模型权重和推理代码，还包括可重复的训练数据、完整的数据处理流水线、严格的实验消融结果和详细的训练协议，以促进科学研究的开放和可重复性。",
        "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
        "pinyin": "Zhè piān wénzhāng tǎolùn le dàxíng yǔyán móxíng (LLMs) zài dàimǎ shēngchéng, tuīlǐ rènwù hé dàilǐ xìtǒng zhōng de zhòngyàoxìng. Suīrán kāifàng fǎngwèn de dàimǎ LLMs xìngnéng jìnkè zhuānyǒu móxíng, dàn shìhé yánkè kēxué yánjiū de gāo zhìliàng dàimǎ LLMs réngrán yǒuxiàn. Wèile tiánbǔ zhè yī kòngbái, wénzhāng jièshào le OpenCoder, yīgè dǐngjiān de dàimǎ LLMs, qí xìngnéng kānbǐ lǐngxiān móxíng, érqiě wèi yánjiū shèqū tiáo gòngle yīgè “kāifàng de shípǔ”. OpenCoder bùjǐn fābùle móxíng quánzhòng hé tuīlǐ dàimǎ, hái bāokuò kě chóngfù de xùnliàn shùjù, wánzhěng de shùjù chǔlǐ liúshuǐxiàn, yánjìn de shìyàn róngróng jiéguǒ hé xiángxì de xùnliàn xiéyì, yǐ cùjìn kēxué yánjiū de kāifàng hé kě chóngfùxìng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"大型\", \"pinyin\": \"dà xíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔ yán mó xíng\", \"trans\": \"language model\"},\n    {\"word\": \"代码生成\", \"pinyin\": \"dài mǎ shēng chéng\", \"trans\": \"code generation\"},\n    {\"word\": \"推理任务\", \"pinyin\": \"tuī lǐ rèn wù\", \"trans\": \"reasoning tasks\"},\n    {\"word\": \"代理系统\", \"pinyin\": \"dài lǐ xì tǒng\", \"trans\": \"proxy system\"},\n    {\"word\": \"重要性\", \"pinyin\": \"zhòng yào xìng\", \"trans\": \"importance\"},\n    {\"word\": \"开放访问\", \"pinyin\": \"kāi fàng fǎng wèn\", \"trans\": \"open access\"},\n    {\"word\": \"专有模型\", \"pinyin\": \"zhuān yǒu mó xíng\", \"trans\": \"proprietary model\"},\n    {\"word\": \"严谨\", \"pinyin\": \"yán jǐn\", \"trans\": \"rigorous\"},\n    {\"word\": \"科学研究\", \"pinyin\": \"kē xué yán jiū\", \"trans\": \"scientific research\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"有限\", \"pinyin\": \"yǒu xiàn\", \"trans\": \"limited\"},\n    {\"word\": \"填补\", \"pinyin\": \"tián bǔ\", \"trans\": \"fill\"},\n    {\"word\": \"空白\", \"pinyin\": \"kòng bái\", \"trans\": \"gap\"},\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"顶尖\", \"pinyin\": \"dǐng jiān\", \"trans\": \"top-notch\"},\n    {\"word\": \"堪比\", \"pinyin\": \"kān bǐ\", \"trans\": \"comparable\"},\n    {\"word\": \"领先模型\", \"pinyin\": \"lǐng xiān mó xíng\", \"trans\": \"leading model\"},\n    {\"word\": \"食谱\", \"pinyin\": \"shí pǔ\", \"trans\": \"recipe\"},\n    {\"word\": \"模型权重\", \"pinyin\": \"mó xíng quán zhòng\", \"trans\": \"model weights\"},\n    {\"word\": \"推理代码\", \"pinyin\": \"tuī lǐ dài mǎ\", \"trans\": \"inference code\"},\n    {\"word\": \"可重复\", \"pinyin\": \"kě chóng fù\", \"trans\": \"reproducible\"},\n    {\"word\": \"训练数据\", \"pinyin\": \"xùn liàn shù jù\", \"trans\": \"training data\"},\n    {\"word\": \"完整\", \"pinyin\": \"wán zhěng\", \"trans\": \"complete\"},\n    {\"word\": \"数据处理流水线\", \"pinyin\": \"shù jù chǔ lǐ liú shuǐ xiàn\", \"trans\": \"data processing pipeline\"},\n    {\"word\": \"严格\", \"pinyin\": \"yán gé\", \"trans\": \"strict\"},\n    {\"word\": \"实验消融结果\", \"pinyin\": \"shí yàn xiāo róng jié guǒ\", \"trans\": \"experimental ablation results\"},\n    {\"word\": \"详细\", \"pinyin\": \"xiáng xì\", \"trans\": \"detailed\"},\n    {\"word\": \"训练协议\", \"pinyin\": \"xùn liàn xié yì\", \"trans\": \"training protocol\"},\n    {\"word\": \"促进\", \"pinyin\": \"cù jìn\", \"trans\": \"promote\"},\n    {\"word\": \"开放\", \"pinyin\": \"kāi fàng\", \"trans\": \"open\"},\n    {\"word\": \"可重复性\", \"pinyin\": \"kě chóng fù xìng\", \"trans\": \"reproducibility\"}\n]",
        "trans": "This article discusses the importance of large language models (LLMs) in code generation, reasoning tasks, and agent systems. Although open-access code LLMs perform nearly as well as proprietary models, high-quality code LLMs suitable for rigorous scientific research remain limited. To fill this gap, the article introduces OpenCoder, a top-tier code LLM with performance comparable to leading models, and provides an \"open recipe\" for the research community. OpenCoder not only releases model weights and inference code but also includes reproducible training data, a complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols to promote openness and reproducibility in scientific research.",
        "update_ts": "2024-11-08 09:36"
    }
}