{
    "date": {
        "ru": "13 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 13",
        "zh": "11æœˆ13æ—¥"
    },
    "time_utc": "2024-11-13 04:12",
    "weekday": 2,
    "issue_id": 541,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07184",
            "title": "SAMPart3D: Segment Any Part in 3D Objects",
            "url": "https://huggingface.co/papers/2411.07184",
            "abstract": "3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.",
            "score": 10,
            "issue_id": 541,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "b4e58a99e4a7e86c",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "SAMPart3D: Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SAMPart3D - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±ĞµĞ·Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SAMPart3D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with SAMPart3D",
                    "desc": "This paper presents SAMPart3D, a novel framework for zero-shot 3D part segmentation that does not depend on predefined text prompts. It utilizes text-agnostic vision foundation models to extract 3D features, enabling it to scale effectively to large unlabeled datasets. The framework also incorporates scale-conditioned part-aware features, allowing for segmentation at various levels of detail. SAMPart3D outperforms existing methods and introduces a new benchmark to enhance the diversity and complexity of 3D part segmentation tasks."
                },
                "zh": {
                    "title": "SAMPart3Dï¼šæ— æ–‡æœ¬æç¤ºçš„3Déƒ¨ä»¶åˆ†å‰²æ–°æ¡†æ¶",
                    "desc": "3Déƒ¨ä»¶åˆ†å‰²æ˜¯3Dæ„ŸçŸ¥ä¸­çš„ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºæœºå™¨äººæŠ€æœ¯ã€3Dç”Ÿæˆå’Œ3Dç¼–è¾‘ç­‰é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†SAMPart3Dæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ä¾èµ–é¢„å®šä¹‰æ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹ï¼Œå¯¹ä»»æ„3Då¯¹è±¡è¿›è¡Œå¤šç²’åº¦çš„è¯­ä¹‰éƒ¨ä»¶åˆ†å‰²ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ— æ–‡æœ¬ä¾èµ–çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œä»å¤§è§„æ¨¡æœªæ ‡è®°çš„3Dæ•°æ®é›†ä¸­æå–ä¸°å¯Œçš„3Dç‰¹å¾ï¼Œå¹¶é€šè¿‡æ¡ä»¶åŒ–çš„éƒ¨ä»¶æ„ŸçŸ¥ç‰¹å¾å®ç°çµæ´»çš„åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAMPart3Dåœ¨å¤„ç†å¤æ‚å¯¹è±¡æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²æ–¹æ³•ï¼Œå¹¶èƒ½æ”¯æŒå¤šç§åº”ç”¨ï¼Œå¦‚éƒ¨ä»¶çº§ç¼–è¾‘å’Œäº¤äº’å¼åˆ†å‰²ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-12.html",
    "link_next": "2024-11-14.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "14.11",
        "en": "11/14",
        "zh": "11æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºAdd-itçš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨å›¾åƒä¸­æ ¹æ®æ–‡æœ¬æŒ‡ä»¤æ·»åŠ ç‰©ä½“ã€‚Add-itåˆ©ç”¨æ‰©å±•çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆåœºæ™¯å›¾åƒã€æ–‡æœ¬æç¤ºå’Œç”Ÿæˆå›¾åƒæœ¬èº«çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸éœ€è¦ä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒæƒ…å†µä¸‹ï¼Œä¿æŒç»“æ„ä¸€è‡´æ€§å’Œç»†èŠ‚ï¼Œå¹¶ç¡®ä¿ç‰©ä½“è‡ªç„¶æ”¾ç½®ã€‚Add-itåœ¨çœŸå®å’Œç”Ÿæˆå›¾åƒæ’å…¥åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ä¸­èƒœå‡ºè¶…è¿‡80%çš„æ¡ˆä¾‹ã€‚",
        "title": "Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models",
        "pinyin": "zhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ¨i Add-it de xÄ«n fÄng fÇ, yÃ²ng yÃº zÃ i tÃº xiÃ ng zhÅng gÄ“n jÃ¹ wÃ©n bÄ›n zhÇ lÇng tiÄn jiÄ wÃ¹ tÇ. Add-it lÃ¬ yÃ²ng kuÃ² zhÇn de zhÃ¹ yÃ¬ jÄ« zhÃ¬, jiÃ© hÃ© chÇng jÇng tÃº xiÃ ng, wÃ©n bÄ›n tÃ­ shÃ¬ hÃ© shÄ“ng chÃ©ng tÃº xiÃ ng bÄ›n shÄ“n de xÃ¬n xÄ«. zhÃ¨ zhÇ’ng fÄng fÇ zÃ i bÃ¹ xÅ« yÃ o rÃ¨n wÃ¹ tÃ¨ dÃ¬ng de wÄ“i tiÃ¡o qÃ­ng kuÃ ng xiÃ , bÇo chÃ­ jiÄ“ gÃ²u yÄ« zhÃ¬ xÃ¬ng hÃ© xÃ¬ jiÃ©, bÃ¬ng quÃ¨ shÃ­ Add-it zÃ i zhÄ“n shÃ­ hÃ© shÄ“ng chÃ©ng tÃº xiÃ ng chÄ rÃ¹ jÄ« zhÇ”n shÃ ng quÇn dÃ© le zuÃ¬ xiÄn jÃ¬n de jiÃ© guÇ’, bÃ¬ng zÃ i rÃ©n lÃ¨i pÃ­ng guÄ zhÅng shÃ¨ng chÅ« chÄo guÃ² 80% de Ã n lÃ¬.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"åä¸º\", \"pinyin\": \"mÃ­ngwÃ©i\", \"trans\": \"named\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"æ ¹æ®\", \"pinyin\": \"gÄ“njÃ¹\", \"trans\": \"according to\"},\n    {\"word\": \"æŒ‡ä»¤\", \"pinyin\": \"zhÇlÃ¬ng\", \"trans\": \"instruction\"},\n    {\"word\": \"æ·»åŠ \", \"pinyin\": \"tiÄnjiÄ\", \"trans\": \"add\"},\n    {\"word\": \"ç‰©ä½“\", \"pinyin\": \"wÃ¹tÇ\", \"trans\": \"object\"},\n    {\"word\": \"åˆ©ç”¨\", \"pinyin\": \"lÃ¬yÃ²ng\", \"trans\": \"utilize\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ²zhÇn\", \"trans\": \"extend\"},\n    {\"word\": \"æ³¨æ„åŠ›\", \"pinyin\": \"zhÃ¹yÃ¬lÃ¬\", \"trans\": \"attention\"},\n    {\"word\": \"æœºåˆ¶\", \"pinyin\": \"jÄ«zhÃ¬\", \"trans\": \"mechanism\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ©hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"åœºæ™¯\", \"pinyin\": \"chÇngjÇng\", \"trans\": \"scene\"},\n    {\"word\": \"æç¤º\", \"pinyin\": \"tÃ­shÃ¬\", \"trans\": \"prompt\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"æœ¬èº«\", \"pinyin\": \"bÄ›nshÄ“n\", \"trans\": \"itself\"},\n    {\"word\": \"ä¿¡æ¯\", \"pinyin\": \"xÃ¬nxÄ«\", \"trans\": \"information\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨nwÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"ç‰¹å®š\", \"pinyin\": \"tÃ¨dÃ¬ng\", \"trans\": \"specific\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“itiÃ¡o\", \"trans\": \"fine-tune\"},\n    {\"word\": \"æƒ…å†µ\", \"pinyin\": \"qÃ­ngkuÃ ng\", \"trans\": \"situation\"},\n    {\"word\": \"ä¿æŒ\", \"pinyin\": \"bÇochÃ­\", \"trans\": \"maintain\"},\n    {\"word\": \"ä¸€è‡´æ€§\", \"pinyin\": \"yÄ«zhÃ¬xÃ¬ng\", \"trans\": \"consistency\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬jiÃ©\", \"trans\": \"detail\"},\n    {\"word\": \"ç¡®ä¿\", \"pinyin\": \"quÃ¨bÇo\", \"trans\": \"ensure\"},\n    {\"word\": \"è‡ªç„¶\", \"pinyin\": \"zÃ¬rÃ¡n\", \"trans\": \"natural\"},\n    {\"word\": \"æ”¾ç½®\", \"pinyin\": \"fÃ ngzhÃ¬\", \"trans\": \"place\"},\n    {\"word\": \"çœŸå®\", \"pinyin\": \"zhÄ“nshÃ­\", \"trans\": \"real\"},\n    {\"word\": \"æ’å…¥\", \"pinyin\": \"chÄrÃ¹\", \"trans\": \"insert\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ«zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"å–å¾—\", \"pinyin\": \"qÇ”dÃ©\", \"trans\": \"achieve\"},\n    {\"word\": \"æœ€å…ˆè¿›\", \"pinyin\": \"zuÃ¬xiÄnjÃ¬n\", \"trans\": \"state-of-the-art\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ©guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"äººç±»\", \"pinyin\": \"rÃ©nlÃ¨i\", \"trans\": \"human\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"èƒœå‡º\", \"pinyin\": \"shÃ¨ngchÅ«\", \"trans\": \"win\"},\n    {\"word\": \"è¶…è¿‡\", \"pinyin\": \"chÄoguÃ²\", \"trans\": \"exceed\"},\n    {\"word\": \"æ¡ˆä¾‹\", \"pinyin\": \"Ã nlÃ¬\", \"trans\": \"case\"}\n]",
        "trans": "This article introduces a new method called Add-it for adding objects to images based on textual instructions. Add-it utilizes an extended attention mechanism, combining information from the scene image, textual prompts, and the generated image itself. This method maintains structural consistency and detail without requiring task-specific fine-tuning, ensuring that objects are naturally placed. Add-it achieves state-of-the-art results in both real and generated image insertion benchmarks and outperforms in over 80% of cases in human evaluations.",
        "update_ts": "2024-11-12 09:51"
    }
}