{
    "date": {
        "ru": "30 октября",
        "en": "October 30",
        "zh": "10月30日"
    },
    "time_utc": "2024-10-30 02:47",
    "weekday": 2,
    "issue_id": 334,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.22304",
            "title": "Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning",
            "url": "https://huggingface.co/papers/2410.22304",
            "abstract": "Mathematical reasoning is a crucial capability for Large Language Models (LLMs), yet generating detailed and accurate reasoning traces remains a significant challenge. This paper introduces a novel approach to produce high-quality reasoning traces for LLM fine-tuning using online learning Flows. Our method employs an incremental output production Flow, where component LLMs collaboratively construct solutions through iterative communication. We train the Flow using online Direct Preference Optimization (DPO) learning with rollouts, generating DPO pairs for each training example and updating models in real-time. We directly compare the quality of reasoning traces generated by our method with those produced through direct model inference, demonstrating the effectiveness of our approach in improving LLM performance in mathematical reasoning tasks.",
            "score": 2,
            "issue_id": 334,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "ffb1cb8e8855bf5d",
            "data": {
                "categories": [
                    "#math",
                    "#rlhf",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Улучшение математических рассуждений LLM через онлайн-обучение потоков",
                    "desc": "Эта статья представляет новый подход к созданию качественных цепочек рассуждений для дообучения больших языковых моделей (LLM) в задачах математических рассуждений. Метод использует инкрементальный процесс построения решения, где компонентные LLM совместно конструируют решение через итеративное взаимодействие. Обучение происходит с помощью онлайн-оптимизации прямых предпочтений (DPO) с использованием развёртываний, генерируя пары для DPO для каждого обучающего примера и обновляя модели в реальном времени. Авторы напрямую сравнивают качество цепочек рассуждений, созданных их методом, с теми, что получены прямым выводом модели, демонстрируя эффективность подхода в улучшении производительности LLM в задачах математических рассуждений."
                },
                "en": {
                    "title": "Enhancing Mathematical Reasoning in LLMs with Collaborative Learning Flows",
                    "desc": "This paper addresses the challenge of generating accurate reasoning traces for Large Language Models (LLMs) in mathematical tasks. It presents a new method that uses online learning Flows, where multiple LLMs work together to create solutions through iterative communication. The training process involves Direct Preference Optimization (DPO) with rollouts, allowing real-time updates and improvements. The results show that this approach significantly enhances the quality of reasoning traces compared to traditional model inference methods."
                },
                "zh": {
                    "title": "提升大型语言模型的数学推理能力",
                    "desc": "本文探讨了大型语言模型（LLMs）在数学推理中的能力，尤其是生成详细和准确的推理过程的挑战。我们提出了一种新方法，通过在线学习流（Flows）来生成高质量的推理过程，以便对LLM进行微调。该方法采用增量输出生成流，多个组件LLM通过迭代通信共同构建解决方案。我们使用在线直接偏好优化（DPO）学习进行训练，实时更新模型，从而提高LLM在数学推理任务中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.21465",
            "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
            "url": "https://huggingface.co/papers/2410.21465",
            "abstract": "With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6times larger batch sizes and boost throughput by up to 3.04times on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.",
            "score": 1,
            "issue_id": 334,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "f954b9ea6eb1a3ff",
            "data": {
                "categories": [
                    "#inference",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "ShadowKV: Ускорение вывода длинноконтекстных LLM без компромиссов",
                    "desc": "Статья представляет ShadowKV - систему для высокопроизводительного вывода длинноконтекстных больших языковых моделей (LLM). ShadowKV хранит кэш ключей низкого ранга и выгружает кэш значений для уменьшения объема памяти, что позволяет обрабатывать большие пакеты и длинные последовательности. Система использует точную стратегию выбора KV, восстанавливая минимальные разреженные пары KV на лету для минимизации задержки декодирования. Эксперименты показали, что ShadowKV может увеличить размер пакета до 6 раз и повысить производительность до 3,04 раз на GPU A100 без потери точности."
                },
                "en": {
                    "title": "Boosting Long-Context LLM Inference with ShadowKV",
                    "desc": "The paper introduces ShadowKV, a system designed to enhance the efficiency of long-context large language model (LLM) inference. It addresses the challenges of high memory usage and low throughput caused by the expanding key-value (KV) cache during token generation. ShadowKV reduces memory consumption by storing a low-rank key cache while offloading the value cache, allowing for larger batch sizes and longer sequences. The system also implements a KV selection strategy that dynamically reconstructs sparse KV pairs, significantly improving throughput without compromising accuracy."
                },
                "zh": {
                    "title": "高效推理，提升长上下文模型性能",
                    "desc": "随着长上下文大语言模型（LLMs）的广泛应用，对高吞吐量推理的需求不断增加。本文提出了ShadowKV，一个高吞吐量的长上下文LLM推理系统，通过存储低秩键缓存并将值缓存卸载，从而减少内存占用。ShadowKV采用准确的KV选择策略，实时重构最小稀疏KV对，以降低解码延迟。实验结果表明，ShadowKV在多个基准测试中表现优异，支持更大的批量大小并显著提高吞吐量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.21845",
            "title": "Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning",
            "url": "https://huggingface.co/papers/2410.21845",
            "abstract": "Reinforcement learning (RL) holds great promise for enabling autonomous acquisition of complex robotic manipulation skills, but realizing this potential in real-world settings has been challenging. We present a human-in-the-loop vision-based RL system that demonstrates impressive performance on a diverse set of dexterous manipulation tasks, including dynamic manipulation, precision assembly, and dual-arm coordination. Our approach integrates demonstrations and human corrections, efficient RL algorithms, and other system-level design choices to learn policies that achieve near-perfect success rates and fast cycle times within just 1 to 2.5 hours of training. We show that our method significantly outperforms imitation learning baselines and prior RL approaches, with an average 2x improvement in success rate and 1.8x faster execution. Through extensive experiments and analysis, we provide insights into the effectiveness of our approach, demonstrating how it learns robust, adaptive policies for both reactive and predictive control strategies. Our results suggest that RL can indeed learn a wide range of complex vision-based manipulation policies directly in the real world within practical training times. We hope this work will inspire a new generation of learned robotic manipulation techniques, benefiting both industrial applications and research advancements. Videos and code are available at our project website https://hil-serl.github.io/.",
            "score": 0,
            "issue_id": 334,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "b8302dbf79e25f7d",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Человек и ИИ: совместное обучение роботов сложным манипуляциям",
                    "desc": "В этой статье представлена система обучения с подкреплением (RL) для роботизированной манипуляции с участием человека. Система демонстрирует впечатляющие результаты в различных задачах ловкой манипуляции, включая динамическую манипуляцию, точную сборку и координацию двух рук. Подход интегрирует демонстрации и коррекции человека, эффективные алгоритмы RL и другие системные решения для обучения политик с почти идеальным уровнем успеха за 1-2,5 часа тренировки. Результаты показывают значительное превосходство над базовыми методами имитационного обучения и предыдущими подходами RL."
                },
                "en": {
                    "title": "Empowering Robots with Human-guided Reinforcement Learning for Complex Manipulation",
                    "desc": "This paper presents a novel human-in-the-loop reinforcement learning (RL) system designed for robotic manipulation tasks. By combining human demonstrations and corrections with efficient RL algorithms, the system achieves high success rates and quick training times for complex tasks like dynamic manipulation and dual-arm coordination. The results show a significant improvement over traditional imitation learning and previous RL methods, with a twofold increase in success rates and faster execution times. The findings indicate that RL can effectively learn complex manipulation skills in real-world scenarios, paving the way for advancements in robotic applications."
                },
                "zh": {
                    "title": "人机协作强化学习：实现复杂机器人操作的突破",
                    "desc": "强化学习（RL）在自主获取复杂机器人操作技能方面具有很大潜力，但在现实环境中实现这一潜力面临挑战。我们提出了一种人机协作的基于视觉的RL系统，在多种灵巧操作任务中表现出色，包括动态操作、精密组装和双臂协调。该方法结合了演示和人类纠正、有效的RL算法以及其他系统设计选择，使得在仅1到2.5小时的训练内学习到接近完美的成功率和快速的循环时间。我们的实验结果表明，该方法在成功率和执行速度上显著优于模仿学习基线和之前的RL方法，展示了RL在现实世界中学习复杂视觉操作策略的有效性。"
                }
            }
        }
    ],
    "link_prev": "2024-10-29.html",
    "link_next": "2024-10-31.html",
    "short_date_prev": {
        "ru": "29.10",
        "en": "10/29",
        "zh": "10月29日"
    },
    "short_date_next": {
        "ru": "31.10",
        "en": "10/31",
        "zh": "10月31日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1
    },
    "zh": {
        "text": "这篇文章介绍了 Bielik 7B v0.1，一个用于波兰语处理的生成文本模型。它使用创新技术训练，包括权重指令交叉熵损失和自适应学习率。为评估性能，创建了新的评估框架。Bielik 7B v0.1 在多个任务上表现出显著改进，特别是在推理和角色扮演方面。这个模型在波兰语人工智能领域取得了重大进展。",
        "title": "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation",
        "pinyin": "这篇文章介绍了 Bielik 7B v0.1，一个用于波兰语处理的生成文本模型。它使用创新技术训练，包括权重指令交叉熵损失和自适应学习率。为评估性能，创建了新的评估框架。Bielik 7B v0.1 在多个任务上表现出显著改进，特别是在推理和角色扮演方面。这个模型在波兰语人工智能领域取得了重大进展。\n\nzhè piān wén zhāng jiè shào le Bielik 7B v0.1, yī gè yòng yú Bōlán yǔ chǔ lǐ de shēng chéng wén běn mó xìng. tā shǐ yòng chuàng xīn jì shù xùn liàn, bāo guò quán zhòng zhǐ lìng jiāo chá shāng sǔn shī hé zì guǎ péi xué lǜ. wèi píng guǎ xìng néng, chuàng jiàn le xīn de píng guǎ kuàng jià. Bielik 7B v0.1 zài duō gè rèn wù shàng biǎo xiàn chū xiǎn zhù gǎi jìn, tè bié shì zài tuī lǐ hé jué sè bàn yǎn fāng miàn. zhè ge mó xìng zài Bōlán yǔ réngōng zhì néng lǐng yù zhù dà jìn bù.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jièshào\", \"trans\": \"introduce\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēngchéng\", \"trans\": \"generate\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàngxīn\", \"trans\": \"innovative\"},\n    {\"word\": \"技术\", \"pinyin\": \"jìshù\", \"trans\": \"technology\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"train\"},\n    {\"word\": \"权重\", \"pinyin\": \"quánzhòng\", \"trans\": \"weight\"},\n    {\"word\": \"指令\", \"pinyin\": \"zhǐlìng\", \"trans\": \"instruction\"},\n    {\"word\": \"交叉熵\", \"pinyin\": \"jiāochā shāng\", \"trans\": \"cross-entropy\"},\n    {\"word\": \"损失\", \"pinyin\": \"sǔnshī\", \"trans\": \"loss\"},\n    {\"word\": \"自适应\", \"pinyin\": \"zìshìyìng\", \"trans\": \"adaptive\"},\n    {\"word\": \"学习率\", \"pinyin\": \"xuéxílǜ\", \"trans\": \"learning rate\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"性能\", \"pinyin\": \"xíngnéng\", \"trans\": \"performance\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎnzhù\", \"trans\": \"significant\"},\n    {\"word\": \"改进\", \"pinyin\": \"gǎijìn\", \"trans\": \"improvement\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"角色\", \"pinyin\": \"juésè\", \"trans\": \"role\"},\n    {\"word\": \"扮演\", \"pinyin\": \"bànyǎn\", \"trans\": \"play\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐngyù\", \"trans\": \"field\"},\n    {\"word\": \"重大\", \"pinyin\": \"zhòngdà\", \"trans\": \"major\"},\n    {\"word\": \"进展\", \"pinyin\": \"jìnzhǎn\", \"trans\": \"progress\"}\n]",
        "trans": "This article introduces Bielik 7B v0.1, a generative text model designed for Polish language processing. It is trained using innovative technologies, including weighted instruction cross-entropy loss and adaptive learning rates. To evaluate its performance, a new evaluation framework was created. Bielik 7B v0.1 demonstrates significant improvements across multiple tasks, particularly in reasoning and role-playing. This model represents a major advancement in the field of Polish language artificial intelligence.",
        "update_ts": "2024-10-29 09:43"
    }
}