{
    "date": {
        "ru": "5 декабря",
        "en": "December 5",
        "zh": "12月5日"
    },
    "time_utc": "2024-12-05 11:09",
    "weekday": 3,
    "issue_id": 965,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.02687",
            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
            "url": "https://huggingface.co/papers/2412.02687",
            "abstract": "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.",
            "score": 30,
            "issue_id": 961,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 декабря",
                "en": "December 3",
                "zh": "12月3日"
            },
            "hash": "d766bad745d5f322",
            "authors": [
                "Viet Nguyen",
                "Anh Nguyen",
                "Trung Dao",
                "Khoi Nguyen",
                "Cuong Pham",
                "Toan Tran",
                "Anh Tran"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech.",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02687.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Повышение стабильности и гибкости одношаговых диффузионных моделей",
                    "desc": "Статья представляет SNOOPI - новый фреймворк для улучшения одношаговых диффузионных моделей генерации изображений. Авторы предлагают метод PG-SB для повышения стабильности обучения путем использования случайного масштаба бесклассификаторного руководства. Также вводится метод NASA для интеграции негативных промптов через кросс-внимание. Эксперименты показывают значительное улучшение базовых моделей по различным метрикам, достигая нового рекорда HPSv2 в 31.08 для одношаговых диффузионных моделей."
                },
                "en": {
                    "title": "SNOOPI: Enhancing One-Step Diffusion Models with Robust Guidance",
                    "desc": "This paper introduces SNOOPI, a new framework that improves one-step text-to-image diffusion models by addressing issues with guidance stability and negative prompt support. The authors enhance training stability using Proper Guidance-SwiftBrush (PG-SB), which applies a random-scale classifier-free guidance method to diversify output distributions. Additionally, they present Negative-Away Steer Attention (NASA), a training-free technique that incorporates negative prompts to eliminate unwanted elements in generated images. Experimental results demonstrate that SNOOPI outperforms existing models, achieving a new state-of-the-art HPSv2 score of 31.08."
                },
                "zh": {
                    "title": "SNOOPI：提升一步扩散模型的稳定性与生成质量",
                    "desc": "本论文提出了一种新框架SNOOPI，旨在解决现有一步扩散模型的局限性。我们通过Proper Guidance-SwiftBrush (PG-SB)方法增强了训练的稳定性，采用随机尺度的无分类器引导策略。我们还提出了一种无训练的方法Negative-Away Steer Attention (NASA)，通过交叉注意力将负提示集成到一步扩散模型中，以抑制生成图像中的不必要元素。实验结果表明，我们的方法在多个指标上显著提高了基线模型的性能，创造了一步扩散模型的新标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03552",
            "title": "Imagine360: Immersive 360 Video Generation from Perspective Anchor",
            "url": "https://huggingface.co/papers/2412.03552",
            "abstract": "360^circ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360^circ video format, we seek to lift standard perspective videos into 360^circ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360^circ video generation framework that creates high-quality 360^circ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360^circ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for 360^circ video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360^circ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360^circ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360^circ video creation.",
            "score": 16,
            "issue_id": 958,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "90dc986cabb575af",
            "authors": [
                "Jing Tan",
                "Shuai Yang",
                "Tong Wu",
                "Jingwen He",
                "Yuwei Guo",
                "Ziwei Liu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03552.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Погружение в 360°: от обычного видео к панорамному опыту",
                    "desc": "Статья представляет Imagine360 - первую систему для генерации 360-градусных видео из обычных перспективных видео. Система использует двухветвевую архитектуру с модулями шумоподавления для перспективного и панорамного видео, а также антиподальную маску для захвата дальних зависимостей движения. Предложены решения для адаптации к изменениям угла обзора во входных видео. Эксперименты показывают превосходное качество графики и согласованность движения по сравнению с существующими методами."
                },
                "en": {
                    "title": "Transforming Perspective Videos into Immersive 360° Experiences",
                    "desc": "The paper presents Imagine360, a novel framework for converting standard perspective videos into immersive 360-degree equirectangular videos. It employs a dual-branch architecture that integrates local and global constraints to enhance video quality and motion coherence. Key innovations include an antipodal mask for capturing long-range motion dependencies and elevation-aware designs to adapt to varying perspectives. Extensive experiments demonstrate that Imagine360 outperforms existing methods in generating high-quality, dynamic 360-degree videos."
                },
                "zh": {
                    "title": "Imagine360：个性化沉浸式360度视频创作的未来",
                    "desc": "360度视频提供了一种超沉浸式体验，让观众可以从全方位探索动态场景。为实现更友好和个性化的360度视频内容创作，我们提出了Imagine360，这是首个将标准视角视频转换为360度视频的框架。Imagine360通过有限的360度视频数据学习细致的球面视觉和运动模式，采用双分支设计来提供局部和全局约束。实验表明，Imagine360在图形质量和运动一致性方面优于现有的360度视频生成方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03515",
            "title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion",
            "url": "https://huggingface.co/papers/2412.03515",
            "abstract": "Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5times) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.",
            "score": 16,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "6e733cf9c0a1b851",
            "authors": [
                "Shengyuan Zhang",
                "An Zhao",
                "Ling Yang",
                "Zejian Li",
                "Chenye Meng",
                "Haoran Xu",
                "Tianrun Chen",
                "AnYang Wei",
                "Perry Pengyun GU",
                "Lingyun Sun"
            ],
            "affiliations": [
                "Peking University",
                "Zhejiang Green Zhixing Technology co., ltd",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03515.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#3d",
                    "#open_source"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Быстрое и качественное завершение 3D LiDAR-сцен для автономных транспортных средств",
                    "desc": "Статья представляет новый метод дистилляции для моделей завершения 3D LiDAR-сцен под названием ScoreLiDAR. Этот метод позволяет значительно ускорить процесс семплирования при сохранении высокого качества завершения сцены. Авторы также вводят новую Структурную Потерю, которая помогает дистиллированной модели лучше улавливать геометрическую структуру 3D LiDAR-сцены. Эксперименты показывают, что ScoreLiDAR ускоряет время завершения более чем в 5 раз и превосходит современные модели завершения 3D LiDAR-сцен."
                },
                "en": {
                    "title": "Accelerating 3D LiDAR Scene Completion with ScoreLiDAR",
                    "desc": "This paper introduces ScoreLiDAR, a new method for improving the efficiency of 3D LiDAR scene completion using diffusion models. The proposed distillation technique allows the model to generate high-quality scene completions in significantly fewer sampling steps, making it faster and more practical for real-time applications like autonomous vehicles. Additionally, a novel Structural Loss is introduced to enhance the model's ability to understand and replicate the geometric structure of the 3D scenes. Experimental results show that ScoreLiDAR reduces completion time dramatically while outperforming existing state-of-the-art models in quality."
                },
                "zh": {
                    "title": "高效3D LiDAR场景补全的新方法",
                    "desc": "扩散模型因其强大的训练稳定性和高质量的场景补全而被应用于3D LiDAR场景补全。然而，慢速采样速度限制了基于扩散的场景补全模型的实际应用，因为自动驾驶车辆需要高效感知周围环境。本文提出了一种新颖的蒸馏方法，称为ScoreLiDAR，旨在实现高效且高质量的场景补全。通过引入结构损失，ScoreLiDAR能够更好地捕捉3D LiDAR场景的几何结构，同时显著加快了每帧的补全时间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03555",
            "title": "PaliGemma 2: A Family of Versatile VLMs for Transfer",
            "url": "https://huggingface.co/papers/2412.03555",
            "abstract": "PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.",
            "score": 11,
            "issue_id": 964,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "12d0d9bcc8060099",
            "authors": [
                "Andreas Steiner",
                "André Susano Pinto",
                "Michael Tschannen",
                "Daniel Keysers",
                "Xiao Wang",
                "Yonatan Bitton",
                "Alexey Gritsenko",
                "Matthias Minderer",
                "Anthony Sherbondy",
                "Shangbang Long",
                "Siyang Qin",
                "Reeve Ingle",
                "Emanuele Bugliarello",
                "Sahar Kazemzadeh",
                "Thomas Mesnard",
                "Ibrahim Alabdulmohsin",
                "Lucas Beyer",
                "Xiaohua Zhai"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03555.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#cv",
                    "#transfer_learning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "PaliGemma 2: Новый уровень мультимодального ИИ",
                    "desc": "PaliGemma 2 - это улучшенная версия открытой мультимодальной модели PaliGemma, основанная на семействе языковых моделей Gemma 2. Модель сочетает визуальный энкодер SigLIP-So400m с рядом моделей Gemma 2 разных размеров, от 2B до 27B параметров. Обучение проводилось на изображениях разного разрешения (224px, 448px и 896px) в несколько этапов для приобретения широких знаний. PaliGemma 2 демонстрирует отличные результаты на различных задачах, включая распознавание структуры таблиц, молекулярных структур, нотных записей, а также генерацию подробных описаний изображений и радиологических отчетов."
                },
                "en": {
                    "title": "PaliGemma 2: Advancing Vision-Language Understanding",
                    "desc": "PaliGemma 2 is an enhanced Vision-Language Model (VLM) that builds on the original PaliGemma framework by integrating the SigLIP-So400m vision encoder with various sizes of the Gemma 2 language models. The models are trained at three different image resolutions to improve their ability to transfer knowledge through fine-tuning. This upgrade allows researchers to explore how different factors, like learning rates and model sizes, affect performance on various tasks. PaliGemma 2 also expands its capabilities to include a wider range of tasks, achieving state-of-the-art results in areas such as optical character recognition and detailed captioning."
                },
                "zh": {
                    "title": "PaliGemma 2：视觉与语言的完美结合",
                    "desc": "PaliGemma 2 是基于 Gemma 2 语言模型家族的 PaliGemma 开放视觉语言模型的升级版。我们结合了 SigLIP-So400m 视觉编码器和不同规模的 Gemma 2 模型，进行多阶段训练，以提高模型的知识迁移能力。通过在三种分辨率下训练，我们能够研究影响迁移性能的因素，如学习率，并分析任务类型、模型大小和分辨率之间的关系。PaliGemma 2 扩展了迁移任务的数量和范围，涵盖了多种光学字符识别相关任务，并在这些任务上取得了最先进的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03517",
            "title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images",
            "url": "https://huggingface.co/papers/2412.03517",
            "abstract": "Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.",
            "score": 10,
            "issue_id": 960,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "9d51bf0b60be344b",
            "authors": [
                "Lingen Li",
                "Zhaoyang Zhang",
                "Yaowei Li",
                "Jiale Xu",
                "Xiaoyu Li",
                "Wenbo Hu",
                "Weihao Cheng",
                "Jinwei Gu",
                "Tianfan Xue",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03517.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Синтез новых ракурсов без явного выравнивания видов",
                    "desc": "NVComposer - это новый подход к синтезу новых ракурсов, который устраняет необходимость во внешнем выравнивании видов. Он использует двухпоточную модель диффузии для одновременной генерации целевых ракурсов и позиций камер. Метод включает модуль выравнивания признаков с учетом геометрии, который извлекает геометрические закономерности из плотных стерео моделей во время обучения. Эксперименты показывают, что NVComposer достигает наилучших результатов в задачах генеративного многоракурсного синтеза новых видов."
                },
                "en": {
                    "title": "NVComposer: Generating Novel Views Without External Alignment",
                    "desc": "This paper introduces NVComposer, a new method for generating novel views from multiple images without needing external alignment processes like pose estimation. NVComposer uses a dual-stream diffusion model that generates new views while also predicting camera poses, allowing for a more integrated approach. Additionally, it incorporates a geometry-aware feature alignment module that learns geometric information from stereo models during training. The results show that NVComposer outperforms existing methods, especially when there are many unaligned input views, making it a more flexible solution for novel view synthesis."
                },
                "zh": {
                    "title": "NVComposer：无须外部对齐的生成新视图合成",
                    "desc": "最近生成模型的进展显著提升了多视图数据的新的视图合成（NVS）能力。然而，现有方法依赖于外部的多视图对齐过程，如显式的姿态估计或预重建，这限制了它们的灵活性和可访问性，尤其是在视图之间重叠不足或遮挡时对齐不稳定的情况下。本文提出了NVComposer，这是一种新颖的方法，消除了对显式外部对齐的需求。NVComposer通过引入两个关键组件，使生成模型能够隐式推断多个条件视图之间的空间和几何关系，从而在生成多视图NVS任务中实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03069",
            "title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2412.03069",
            "abstract": "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.",
            "score": 10,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "820e62e1bd498d55",
            "authors": [
                "Liao Qu",
                "Huichao Zhang",
                "Yiheng Liu",
                "Xu Wang",
                "Yi Jiang",
                "Yiming Gao",
                "Hu Ye",
                "Daniel K. Du",
                "Zehuan Yuan",
                "Xinglong Wu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03069.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "TokenFlow: единый токенизатор для понимания и генерации изображений",
                    "desc": "TokenFlow - это новый универсальный токенизатор изображений, объединяющий задачи мультимодального понимания и генерации. Он использует инновационную архитектуру с двойным кодбуком, которая разделяет обучение семантических и пиксельных признаков, сохраняя их выравнивание через общий механизм отображения. TokenFlow превосходит существующие модели в задачах понимания, достигая улучшения на 7.2% по сравнению с LLaVA-1.5 13B. Модель также показывает высокие результаты в реконструкции изображений и авторегрессивной генерации, достигая показателей на уровне современных моделей."
                },
                "en": {
                    "title": "TokenFlow: Bridging Understanding and Generation in Image Processing",
                    "desc": "TokenFlow is a new image tokenizer that improves how machines understand and generate images by using a dual-codebook architecture. This approach separates the learning of high-level semantic features from fine-grained pixel-level details, allowing for better performance in both understanding and generation tasks. By aligning these two types of information through a shared mapping, TokenFlow can effectively utilize both granularities of visual data. The results show that TokenFlow outperforms previous models in understanding and image generation, achieving significant improvements in performance metrics."
                },
                "zh": {
                    "title": "TokenFlow：多模态理解与生成的桥梁",
                    "desc": "本文介绍了一种新颖的图像标记器TokenFlow，它弥合了多模态理解与生成之间的差距。研究表明，理解和生成任务需要不同粒度的视觉信息，传统的单一重建目标向量量化编码器无法有效处理这一问题。TokenFlow通过创新的双代码本架构，解耦了语义和像素级特征学习，同时通过共享映射机制保持它们的对齐。实验结果表明，TokenFlow在多项任务中表现优越，首次证明离散视觉输入在理解性能上超越了LLaVA-1.5 13B。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00493",
            "title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding",
            "url": "https://huggingface.co/papers/2412.00493",
            "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models' learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.",
            "score": 8,
            "issue_id": 964,
            "pub_date": "2024-11-30",
            "pub_date_card": {
                "ru": "30 ноября",
                "en": "November 30",
                "zh": "11月30日"
            },
            "hash": "10c214b548697656",
            "authors": [
                "Duo Zheng",
                "Shijia Huang",
                "Liwei Wang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00493.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#3d",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Video-3D LLM: Прорыв в понимании трехмерных сцен",
                    "desc": "Статья представляет новую модель Video-3D LLM для понимания трехмерных сцен. Модель рассматривает 3D-сцены как динамические видео и использует 3D-позиционное кодирование для лучшего соответствия видеопредставлений реальным пространственным контекстам. Авторы применили технику выборки с максимальным покрытием для оптимизации баланса между вычислительными затратами и эффективностью. Эксперименты показывают, что модель достигает наилучших результатов на нескольких эталонных тестах по пониманию 3D-сцен."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Understanding with Video-3D LLM",
                    "desc": "This paper introduces a new model called Video-3D LLM, designed to improve understanding of 3D scenes by treating them like dynamic videos. The model incorporates 3D position encoding to better align video representations with real-world spatial contexts, addressing the limitations of existing Multimodal Large Language Models (MLLMs) that primarily learn from 2D data. To enhance efficiency, a maximum coverage sampling technique is used, balancing computational costs with performance. The results show that Video-3D LLM achieves state-of-the-art performance on multiple benchmarks for 3D scene understanding."
                },
                "zh": {
                    "title": "提升3D场景理解的创新模型",
                    "desc": "这篇论文介绍了一种新型的多模态大语言模型（MLLM），称为Video-3D LLM，旨在提高3D场景理解能力。传统的MLLM主要基于2D数据训练，导致它们在处理3D环境时存在局限性。通过将3D场景视为动态视频，并引入3D位置编码，Video-3D LLM能够更准确地对齐视频表示与现实世界的空间上下文。此外，论文还提出了一种最大覆盖采样技术，以优化计算成本和性能效率之间的平衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19103",
            "title": "VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models",
            "url": "https://huggingface.co/papers/2411.19103",
            "abstract": "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B.",
            "score": 7,
            "issue_id": 964,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 ноября",
                "en": "November 28",
                "zh": "11月28日"
            },
            "hash": "4507a3a2ac0bc8b5",
            "authors": [
                "Jeongho Ju",
                "Daeyoung Kim",
                "SunYoung Park",
                "Youngjune Kim"
            ],
            "affiliations": [
                "NC Research, NCSOFT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19103.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#multimodal",
                    "#training",
                    "#low_resource"
                ],
                "emoji": "🌏",
                "ru": {
                    "title": "VARCO-VISION: Прорыв в двуязычном компьютерном зрении",
                    "desc": "В этой статье представлена новая мультиязычная модель компьютерного зрения VARCO-VISION для корейского и английского языков. Авторы применили пошаговую стратегию обучения, позволяющую модели усваивать как лингвистическую, так и визуальную информацию. VARCO-VISION демонстрирует высокую производительность в различных задачах, требующих двуязычного понимания и генерации текста и изображений. Модель также способна выполнять задачи локализации объектов, референции и оптического распознавания символов, что расширяет ее потенциальное применение в реальных сценариях."
                },
                "en": {
                    "title": "VARCO-VISION: Bridging Korean and English through Vision-Language Learning",
                    "desc": "This paper presents VARCO-VISION, an open-source vision-language model designed for Korean-English tasks. It employs a step-by-step training approach that effectively integrates linguistic and visual information while maintaining the foundational knowledge of the backbone model. The model excels in bilingual image-text understanding and generation, outperforming similar-sized models in various applications. Additionally, VARCO-VISION supports grounding, referring, and optical character recognition (OCR), and the authors provide five Korean evaluation datasets to facilitate further research in this area."
                },
                "zh": {
                    "title": "VARCO-VISION：双语视觉语言模型的新里程碑",
                    "desc": "本文介绍了一种开源的韩英视觉语言模型VARCO-VISION。我们采用逐步训练策略，使模型能够同时学习语言和视觉信息，同时保留基础模型的知识。与同类模型相比，VARCO-VISION在双语图像文本理解和生成能力方面表现出色。该模型还具备定位、引用和光学字符识别（OCR）功能，扩展了其在现实场景中的应用潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01106",
            "title": "One Shot, One Talk: Whole-body Talking Avatar from a Single Image",
            "url": "https://huggingface.co/papers/2412.01106",
            "abstract": "Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.",
            "score": 7,
            "issue_id": 957,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "13d96f9bb346e344",
            "authors": [
                "Jun Xiang",
                "Yudong Guo",
                "Leipeng Hu",
                "Boyang Guo",
                "Yancheng Yuan",
                "Juyong Zhang"
            ],
            "affiliations": [
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01106.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#multimodal",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Реалистичный говорящий аватар из одного фото",
                    "desc": "Статья описывает новый метод создания реалистичных аватаров, способных говорить и двигаться, на основе всего одного изображения. Авторы используют диффузионные модели для генерации псевдо-видео кадров, которые служат обучающими данными. Они предлагают гибридное представление аватара, сочетающее 3D гауссовы сплаты и полигональную сетку. Метод позволяет точно контролировать жесты и мимику аватара, преодолевая ограничения существующих подходов."
                },
                "en": {
                    "title": "From One Image to a Lifelike Talking Avatar!",
                    "desc": "This paper presents a new method for creating realistic and animatable whole-body talking avatars using only a single image. The authors address two main challenges: modeling complex movements and ensuring the avatar can perform new gestures and expressions. They utilize pose-guided image-to-video diffusion models to generate video frames that serve as training data, despite being imperfect. To improve the quality of the avatar's animations, they introduce a hybrid representation that combines 3D mesh structures with regularization techniques to handle inconsistencies in the generated video frames."
                },
                "zh": {
                    "title": "从单张图像生成全身会说话的虚拟头像",
                    "desc": "本文提出了一种从单张图像构建全身会说话的虚拟头像的方法。我们解决了复杂动态建模和对新手势与表情的泛化这两个关键问题。通过使用姿态引导的图像到视频扩散模型，我们生成了不完美的视频帧作为伪标签，以实现无缝泛化。实验结果表明，我们的方法能够从单张图像创建出逼真、可精确动画和富有表现力的全身虚拟头像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03439",
            "title": "CleanDIFT: Diffusion Features without Noise",
            "url": "https://huggingface.co/papers/2412.03439",
            "abstract": "Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.",
            "score": 6,
            "issue_id": 963,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "cd474064bf17503a",
            "authors": [
                "Nick Stracke",
                "Stefan Andreas Baumann",
                "Kolja Bauer",
                "Frank Fundel",
                "Björn Ommer"
            ],
            "affiliations": [
                "CompVis @ LMU Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03439.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшение семантических признаков диффузионных моделей без шума",
                    "desc": "Исследователи обнаружили, что внутренние признаки, извлекаемые из предобученных диффузионных моделей, являются мощными семантическими дескрипторами для различных задач. Однако добавление шума к изображениям перед их обработкой моделью критически влияет на полезность этих признаков. Авторы предлагают метод легковесной неконтролируемой донастройки, позволяющий получать качественные семантические признаки без шума. Эти признаки значительно превосходят предыдущие подходы по эффективности в различных задачах при меньших вычислительных затратах."
                },
                "en": {
                    "title": "Unlocking Noise-Free Semantic Features from Diffusion Models",
                    "desc": "This paper discusses how internal features from large pre-trained diffusion models can be used as effective semantic descriptors for various tasks. It highlights the problem that these models require added noise to generate useful features, which limits their effectiveness. The authors propose a new, lightweight, unsupervised fine-tuning method that allows these models to produce high-quality semantic features without the need for noise. Their approach significantly improves performance across multiple tasks compared to previous methods, including ensemble techniques, while being more efficient."
                },
                "zh": {
                    "title": "无噪声的高质量语义特征提取",
                    "desc": "最近，大规模预训练扩散模型的内部特征被确立为强大的语义描述符，适用于多种下游任务。通常，这些特征需要在图像中添加噪声后才能提取，因为模型在处理几乎没有噪声的图像时，提供的特征效果不佳。我们发现噪声对特征的有效性有重要影响，且通过不同随机噪声的集成无法解决这个问题。为此，我们提出了一种轻量级的无监督微调方法，使扩散模型能够提供高质量、无噪声的语义特征，显著超越了之前的扩散特征。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03558",
            "title": "MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation",
            "url": "https://huggingface.co/papers/2412.03558",
            "abstract": "This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.",
            "score": 6,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "5e1a4c1e1017e7af",
            "authors": [
                "Zehuan Huang",
                "Yuan-Chen Guo",
                "Xingqiao An",
                "Yunhan Yang",
                "Yangguang Li",
                "Zi-Xin Zou",
                "Ding Liang",
                "Xihui Liu",
                "Yan-Pei Cao",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "The University of Hong Kong",
                "Tsinghua University",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03558.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#synthetic",
                    "#diffusion",
                    "#training",
                    "#3d"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "MIDI: Революционный подход к генерации 3D-сцен из одного изображения",
                    "desc": "Статья представляет MIDI - новый подход к генерации трехмерных сцен из одного изображения. MIDI использует предобученные модели генерации 3D-объектов и расширяет их до многоэкземплярных диффузионных моделей, позволяя одновременно генерировать несколько 3D-объектов с точными пространственными отношениями. Ключевой особенностью является новый механизм многоэкземплярного внимания, который эффективно учитывает взаимодействия между объектами и пространственную согласованность непосредственно в процессе генерации. MIDI демонстрирует передовые результаты в генерации сцен из изображений, что подтверждается оценками на синтетических данных, реальных сценах и стилизованных изображениях."
                },
                "en": {
                    "title": "MIDI: Revolutionizing 3D Scene Generation from Single Images",
                    "desc": "This paper presents MIDI, a new approach for creating 3D scenes from a single image. It improves upon traditional methods by using multi-instance diffusion models, allowing for the generation of multiple 3D objects at once while maintaining their spatial relationships. MIDI features a unique multi-instance attention mechanism that captures how objects interact and fit together in space, simplifying the generation process. The method is trained with a combination of scene-level and single-object data, ensuring high performance and generalization across various types of scenes."
                },
                "zh": {
                    "title": "MIDI：从单图像生成3D场景的新方法",
                    "desc": "本文介绍了一种名为MIDI的新方法，用于从单张图像生成组合3D场景。与现有依赖重建或检索技术的方法不同，MIDI扩展了预训练的图像到3D对象生成模型，采用多实例扩散模型，实现了多个3D实例的同时生成。MIDI的核心是一个新颖的多实例注意机制，能够有效捕捉对象间的交互和空间一致性，简化了生成过程。该方法在图像到场景生成方面表现出色，经过合成数据、真实场景数据和文本到图像扩散模型生成的风格化场景图像的评估验证。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03187",
            "title": "Weighted-Reward Preference Optimization for Implicit Model Fusion",
            "url": "https://huggingface.co/papers/2412.03187",
            "abstract": "While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex but also prone to introducing noise and errors. In this paper, we propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs. To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrate that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against GPT-4-0314 on Arena-Hard. Our code is available at https://github.com/SLIT-AI/WRPO.",
            "score": 4,
            "issue_id": 961,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "6da11fbf4e1ea7d9",
            "authors": [
                "Ziyi Yang",
                "Fanqi Wan",
                "Longguang Zhong",
                "Tianyuan Shi",
                "Xiaojun Quan"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Sun Yat-sen University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03187.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#open_source",
                    "#architecture",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "WRPO: Эффективное слияние языковых моделей без прямого объединения параметров",
                    "desc": "В этой статье предлагается новый метод слияния разнородных языковых моделей с открытым исходным кодом - Weighted-Reward Preference Optimization (WRPO). WRPO использует оптимизацию предпочтений между исходными и целевой моделями для эффективного переноса их возможностей, устраняя необходимость в выравнивании словарей и слиянии матриц распределения. Метод вводит стратегию прогрессивной адаптации для решения проблемы различий в распределениях между моделями. Эксперименты показывают, что WRPO превосходит существующие методы слияния знаний и базовые подходы к дообучению на нескольких бенчмарках."
                },
                "en": {
                    "title": "Effortless Fusion of LLMs with WRPO!",
                    "desc": "This paper introduces a new method called Weighted-Reward Preference Optimization (WRPO) for fusing different open-source large language models (LLMs). WRPO simplifies the fusion process by avoiding complex tasks like vocabulary alignment and distribution matrix merging, which often introduce errors. Instead, it uses a preference optimization approach to effectively transfer capabilities from source LLMs to a target LLM. The method also includes a progressive adaptation strategy to manage differences in distributions between models, leading to improved performance on various benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "加权奖励偏好优化：高效融合多种大语言模型",
                    "desc": "本论文提出了一种隐式融合方法，称为加权奖励偏好优化（WRPO），旨在有效整合不同架构和规模的开源大语言模型（LLMs）。WRPO通过优化源模型与目标模型之间的偏好，避免了词汇对齐和矩阵融合的复杂性。该方法引入了渐进适应策略，逐步调整对目标模型和源模型的依赖，从而解决了分布偏差问题。实验结果表明，WRPO在多个基准测试中表现优于现有的知识融合方法和微调基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03085",
            "title": "Mimir: Improving Video Diffusion Models for Precise Text Understanding",
            "url": "https://huggingface.co/papers/2412.03085",
            "abstract": "Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/",
            "score": 3,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "a065164e5fdadf2c",
            "authors": [
                "Shuai Tan",
                "Biao Gong",
                "Yutong Feng",
                "Kecheng Zheng",
                "Dandan Zheng",
                "Shuwei Shi",
                "Yujun Shen",
                "Jingdong Chen",
                "Ming Yang"
            ],
            "affiliations": [
                "Ant Group",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03085.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Mimir: Улучшение генерации видео с помощью больших языковых моделей",
                    "desc": "Эта статья представляет Mimir - новый подход к генерации видео на основе текста. Авторы предлагают использовать большие языковые модели (LLM) для улучшения понимания текста и воображения в процессе генерации. Ключевой элемент Mimir - специальный 'token fuser', который объединяет выходы текстовых энкодеров и LLM. Результаты показывают, что Mimir эффективен в создании качественных видео с хорошим пониманием текста, особенно для коротких описаний и динамичных сцен."
                },
                "en": {
                    "title": "Mimir: Bridging Text Understanding and Video Generation",
                    "desc": "This paper introduces Mimir, a new framework for text-to-video (T2V) generation that combines the strengths of text encoders and large language models (LLMs). It addresses the challenge of feature distribution gaps between these two text modeling approaches, which can hinder effective video generation. Mimir utilizes a specialized token fuser to integrate outputs from both models, enhancing text comprehension and video quality. The results show that Mimir excels in generating videos from short captions and effectively managing dynamic movements."
                },
                "zh": {
                    "title": "Mimir：提升文本到视频生成的智能",
                    "desc": "本文提出了一种名为Mimir的端到端训练框架，用于文本到视频生成（T2V）。该框架通过精心设计的令牌融合器，解决了文本编码器与大型语言模型（LLMs）之间的特征分布差距。Mimir能够充分利用学习到的视频先验，同时增强LLMs在文本理解方面的能力。实验结果表明，Mimir在生成高质量视频时，尤其在处理短文本和动态变化时，表现出色。"
                }
            }
        }
    ],
    "link_prev": "2024-12-04.html",
    "link_next": "2024-12-06.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "04.12",
        "en": "12/04",
        "zh": "12月4日"
    },
    "short_date_next": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12月6日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 8,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 5,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 7,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一种新的框架 SNOOPI，旨在改进单步扩散模型的指导机制。现有方法在处理不同扩散模型骨架时表现不稳定，且不支持负面提示指导。SNOOPI 通过 PG-SB 和 NASA 两种方法解决了这些问题。实验结果显示，SNOOPI 显著提升了基准模型的性能，达到了新的最佳水平。",
        "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
        "pinyin": "这篇文章介绍了一种新的框架 SNOOPI，旨在改进单步扩散模型的指导机制。现有方法在处理不同扩散模型骨架时表现不稳定，且不支持负面提示指导。SNOOPI 通过 PG-SB 和 NASA 两种方法解决了这些问题。实验结果显示，SNOOPI 显著提升了基准模型的性能，达到了新的最佳水平。\n\nZhè piān wénzhāng jièshào le yī zhǒng xīn de kuàngjià SNOOPI, zhǐ zài gǎijìn dān bù kuòsàn móxíng de zhǐdǎo jīzhì. Xiànyǒu fāngfǎ zài chǔlǐ bùtóng kuòsàn móxíng gǔjià shí biǎoxiàn bùículai, qiě bù zhīchí fùmiàn tíshì zhǐdǎo. SNOOPI tōngguò PG-SB hé NASA liǎng zhǒng fāngfǎ jiějué le zhèxiē wèntí. Shíyàn jiéguǒ xiǎnshì, SNOOPI xiǎnzhù tíshēng le jīzhǔn móxíng de xíngnéng, dá dào le xīn de zuìjiā shuǐpíng.",
        "vocab": "[{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'},\n{'word': '改进', 'pinyin': 'gǎijìn', 'trans': 'improve'},\n{'word': '单步', 'pinyin': 'dānbù', 'trans': 'single-step'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '指导', 'pinyin': 'zhǐdǎo', 'trans': 'guidance'},\n{'word': '机制', 'pinyin': 'jīzhì', 'trans': 'mechanism'},\n{'word': '现有', 'pinyin': 'xiànyǒu', 'trans': 'existing'},\n{'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'},\n{'word': '处理', 'pinyin': 'chǔlǐ', 'trans': 'handle'},\n{'word': '不同', 'pinyin': 'bùtóng', 'trans': 'different'},\n{'word': '骨架', 'pinyin': 'gǔjià', 'trans': 'skeleton'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '不稳定', 'pinyin': 'bùwěndìng', 'trans': 'unstable'},\n{'word': '且', 'pinyin': 'qiě', 'trans': 'and'},\n{'word': '不支持', 'pinyin': 'bù zhīchí', 'trans': 'not support'},\n{'word': '负面', 'pinyin': 'fùmiàn', 'trans': 'negative'},\n{'word': '提示', 'pinyin': 'tíshì', 'trans': 'prompt'},\n{'word': '通过', 'pinyin': 'tōngguò', 'trans': 'through'},\n{'word': 'PG-SB', 'pinyin': '', 'trans': 'PG-SB'},\n{'word': 'NASA', 'pinyin': '', 'trans': 'NASA'},\n{'word': '解决', 'pinyin': 'jiějué', 'trans': 'solve'},\n{'word': '这些', 'pinyin': 'zhèxiē', 'trans': 'these'},\n{'word': '问题', 'pinyin': 'wèntí', 'trans': 'problems'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'},\n{'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'},\n{'word': '性能', 'pinyin': 'xíngnéng', 'trans': 'performance'},\n{'word': '达到', 'pinyin': 'dádào', 'trans': 'reach'},\n{'word': '新的', 'pinyin': 'xīn de', 'trans': 'new'},\n{'word': '最佳', 'pinyin': 'zuìjiā', 'trans': 'best'},\n{'word': '水平', 'pinyin': 'shuǐpíng', 'trans': 'level'}]",
        "trans": "This article introduces a new framework called SNOOPI, aimed at improving the guidance mechanism of single-step diffusion models. Existing methods perform unstably when handling different diffusion model backbones and do not support negative prompt guidance. SNOOPI addresses these issues through the PG-SB and NASA methods. Experimental results demonstrate that SNOOPI significantly enhances the performance of benchmark models, achieving new best-in-class levels.",
        "update_ts": "2024-12-05 09:11"
    }
}