{
    "date": {
        "ru": "21 Ğ¼Ğ°Ñ",
        "en": "May 21",
        "zh": "5æœˆ21æ—¥"
    },
    "time_utc": "2025-05-21 18:15",
    "weekday": 2,
    "issue_id": 3884,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.14683",
            "title": "Emerging Properties in Unified Multimodal Pretraining",
            "url": "https://huggingface.co/papers/2505.14683",
            "abstract": "Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/",
            "score": 75,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "57522649bb8f8010",
            "authors": [
                "Chaorui Deng",
                "Deyao Zhu",
                "Kunchang Li",
                "Chenhui Gou",
                "Feng Li",
                "Zeyu Wang",
                "Shu Zhong",
                "Weihao Yu",
                "Xiaonan Nie",
                "Ziang Song",
                "Guang Shi",
                "Haoqi Fan"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Hong Kong University of Science and Technology",
                "Monash University",
                "Shenzhen Institutes of Advanced Technology",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14683.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¥¯",
                "ru": {
                    "title": "BAGEL: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "BAGEL - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. BAGEL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ 3D-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "BAGEL: Unifying Multimodal AI for Enhanced Understanding and Generation",
                    "desc": "This paper presents BAGEL, an open-source foundational model designed for multimodal understanding and generation. BAGEL is a decoder-only model that has been pretrained on a vast dataset comprising text, images, videos, and web content. By leveraging this diverse multimodal data, BAGEL demonstrates advanced capabilities in complex reasoning tasks, outperforming existing open-source models. The authors aim to promote further research in multimodal AI by sharing their findings, pretraining methods, and code with the community."
                },
                "zh": {
                    "title": "BAGELï¼šå¼€æºå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºBAGELçš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œå®ƒæ”¯æŒå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚BAGELæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§£ç å™¨æ¨¡å‹ï¼Œç»è¿‡åœ¨å¤§é‡æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç½‘ç»œæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚é€šè¿‡ä½¿ç”¨å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®ï¼ŒBAGELåœ¨å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†æ–¹é¢å±•ç°å‡ºæ–°çš„èƒ½åŠ›ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¼€æºç»Ÿä¸€æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡åˆ†äº«å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚å’Œæ•°æ®åˆ›å»ºåè®®ï¼Œä¿ƒè¿›å¤šæ¨¡æ€ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11594",
            "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
            "url": "https://huggingface.co/papers/2505.11594",
            "abstract": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention.",
            "score": 43,
            "issue_id": 3869,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "33309444d442b40c",
            "authors": [
                "Jintao Zhang",
                "Jia Wei",
                "Pengle Zhang",
                "Xiaoming Xu",
                "Haofeng Huang",
                "Haoxu Wang",
                "Kai Jiang",
                "Jun Zhu",
                "Jianfei Chen"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11594.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¾Ñ‚ FP4 Ğ´Ğ¾ 8-Ğ±Ğ¸Ñ‚",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. Ğ’Ğ¾-Ğ¿ĞµÑ€Ğ²Ñ‹Ñ…, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ° FP4 Ğ² GPU Blackwell Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 5-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FlashAttention. Ğ’Ğ¾-Ğ²Ñ‚Ğ¾Ñ€Ñ‹Ñ…, Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ 8-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Attention: Fast and Efficient for Training and Inference",
                    "desc": "This paper addresses the inefficiency of attention mechanisms in machine learning, which typically have a quadratic time complexity. The authors introduce enhancements using FP4 Tensor Cores in Blackwell GPUs, achieving a significant speedup in attention computation, reaching 1038 TOPS on the RTX5090. Additionally, they explore low-bit attention for training tasks, proposing an 8-bit attention method that maintains performance during fine-tuning while showing slower convergence during pretraining. This work not only improves inference speed but also expands the application of low-bit attention to training, making it a versatile solution for large model training."
                },
                "zh": {
                    "title": "æå‡æ³¨æ„åŠ›æœºåˆ¶æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ³¨æ„åŠ›æœºåˆ¶çš„æ•ˆç‡é—®é¢˜ï¼Œä¸»è¦ç”±äºå…¶äºŒæ¬¡æ—¶é—´å¤æ‚åº¦ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨Blackwell GPUä¸­çš„æ–°FP4 Tensor Coresæ¥åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—ï¼Œå®ç°äº†åœ¨RTX5090ä¸Šè¾¾åˆ°1038 TOPSçš„æ€§èƒ½ï¼Œç›¸æ¯”äºæœ€å¿«çš„FlashAttentionæå‡äº†5å€ã€‚æˆ‘ä»¬çš„FP4æ³¨æ„åŠ›å¯ä»¥ä»¥å³æ’å³ç”¨çš„æ–¹å¼åŠ é€Ÿå„ç§æ¨¡å‹çš„æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é¦–æ¬¡å°†ä½ä½æ³¨æ„åŠ›åº”ç”¨äºè®­ç»ƒä»»åŠ¡ï¼Œè®¾è®¡äº†é«˜æ•ˆçš„8ä½æ³¨æ„åŠ›ï¼Œå®éªŒè¡¨æ˜åœ¨å¾®è°ƒä»»åŠ¡ä¸­è¡¨ç°æ— æŸï¼Œä½†åœ¨é¢„è®­ç»ƒä»»åŠ¡ä¸­æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13438",
            "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
            "url": "https://huggingface.co/papers/2505.13438",
            "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.",
            "score": 25,
            "issue_id": 3874,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "34ff8235b7a27562",
            "authors": [
                "Penghui Qi",
                "Zichen Liu",
                "Tianyu Pang",
                "Chao Du",
                "Wee Sun Lee",
                "Min Lin"
            ],
            "affiliations": [
                "National University of Singapore",
                "Sea AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13438.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ›Ğ¯Ğœ Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ AnytimeReasoner Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ›Ğ¯Ğœ) Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑƒÑĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Budget Relative Policy Optimization (BRPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ GRPO Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑÑ… Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Optimizing Reasoning Efficiency with AnytimeReasoner",
                    "desc": "This paper introduces AnytimeReasoner, a framework designed to improve the reasoning capabilities of large language models (LLMs) by optimizing their performance under varying token budgets. Unlike traditional reinforcement learning methods that focus solely on final outcomes, AnytimeReasoner allows for flexible reasoning by truncating the thinking process and summarizing answers based on sampled token budgets. This approach incorporates verifiable dense rewards, which aids in better credit assignment during the reinforcement learning optimization. The authors also propose a new technique called Budget Relative Policy Optimization (BRPO) to enhance the robustness and efficiency of the learning process, leading to superior performance in mathematical reasoning tasks compared to existing methods."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†æ€§èƒ½ï¼Œæå‡æ•ˆç‡ä¸çµæ´»æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåä¸ºAnytimeReasonerï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒAnytimeReasoneré€šè¿‡åœ¨ä¸åŒçš„ä»¤ç‰Œé¢„ç®—çº¦æŸä¸‹è¿›è¡Œæ¨ç†ï¼Œæå‡äº†ä»¤ç‰Œçš„ä½¿ç”¨æ•ˆç‡å’Œçµæ´»æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å¯éªŒè¯çš„å¯†é›†å¥–åŠ±ï¼Œä½¿å¾—åœ¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œä¿¡ç”¨åˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ŒAnytimeReasoneråœ¨å„ç§é¢„ç®—æ¡ä»¶ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†è®­ç»ƒå’Œä»¤ç‰Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14460",
            "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank",
            "url": "https://huggingface.co/papers/2505.14460",
            "abstract": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computational modeling has not been thoroughly explored in the context of image quality assessment (IQA), a task critically dependent on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are then used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation.",
            "score": 24,
            "issue_id": 3876,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "98e34275d69f6e41",
            "authors": [
                "Tianhe Wu",
                "Jian Zou",
                "Jie Liang",
                "Lei Zhang",
                "Kede Ma"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "OPPO Research Institute",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14460.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#cv",
                    "#rl",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "VisualQuality-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VisualQuality-R1 Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ°Ñ€Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. VisualQuality-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Image Quality Assessment with Reasoning and Reinforcement Learning",
                    "desc": "This paper presents VisualQuality-R1, a novel no-reference image quality assessment (NR-IQA) model that leverages reasoning and reinforcement learning to evaluate image quality. By using group relative policy optimization, the model generates multiple quality scores for image pairs, allowing for a nuanced comparison of visual quality. The rewards for these scores are based on continuous fidelity measures, enhancing the model's ability to provide accurate quality assessments. Experimental results show that VisualQuality-R1 outperforms existing deep learning-based NR-IQA models and can generate detailed quality descriptions, making it effective for various image processing applications."
                },
                "zh": {
                    "title": "æ¨ç†é©±åŠ¨çš„å›¾åƒè´¨é‡è¯„ä¼°æ–°æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹VisualQuality-R1ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚VisualQuality-R1åˆ©ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œä¸ºæ¯å¯¹å›¾åƒç”Ÿæˆå¤šä¸ªè´¨é‡è¯„åˆ†ï¼Œå¹¶è®¡ç®—å›¾åƒä¹‹é—´çš„æ¯”è¾ƒæ¦‚ç‡ã€‚ä¸ä¼ ç»Ÿçš„äºŒå…ƒæ ‡ç­¾ä¸åŒï¼Œå¥–åŠ±æ˜¯åŸºäºè¿ç»­çš„ä¿çœŸåº¦åº¦é‡æ¥å®šä¹‰çš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisualQuality-R1åœ¨å›¾åƒè´¨é‡è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆä¸°å¯Œçš„è´¨é‡æè¿°ï¼Œé€‚ç”¨äºå¤šæ•°æ®é›†è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14246",
            "title": "Visual Agentic Reinforcement Fine-Tuning",
            "url": "https://huggingface.co/papers/2505.14246",
            "abstract": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native agentic ability to use external tools such as web browsers for searching and writing/executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs' agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents.",
            "score": 24,
            "issue_id": 3873,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "163cdaefde9d9174",
            "authors": [
                "Ziyu Liu",
                "Yuhang Zang",
                "Yushan Zou",
                "Zijian Liang",
                "Xiaoyi Dong",
                "Yuhang Cao",
                "Haodong Duan",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14246.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#benchmark",
                    "#multimodal",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Visual-ARFT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Multi-modal Agentic Tool Bench (MAT) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LVLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Visual-ARFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° MAT Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "Empowering Vision-Language Models with Visual-ARFT for Enhanced Reasoning",
                    "desc": "This paper introduces Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT), a method that enhances Large Vision-Language Models (LVLMs) by enabling them to think with images and use external tools effectively. The research highlights the development of multi-modal agentic capabilities, allowing LVLMs to browse the web for information and manipulate images through coding. The authors present a new evaluation framework called the Multi-modal Agentic Tool Bench (MAT), which assesses the models' abilities in searching and coding tasks. Experimental results show that Visual-ARFT significantly improves performance on various benchmarks, indicating its potential for creating more capable multimodal agents."
                },
                "zh": {
                    "title": "è§†è§‰ä»£ç†å¼ºåŒ–å¾®è°ƒï¼šå¤šæ¨¡æ€æ™ºèƒ½çš„æœªæ¥",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è§†è§‰ä»£ç†å¼ºåŒ–å¾®è°ƒï¼ˆVisual-ARFTï¼‰æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯ä½¿å¾—LVLMsèƒ½å¤Ÿçµæ´»åœ°ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œå¦‚æµè§ˆå™¨å’Œä»£ç æ‰§è¡Œï¼Œè¿›è¡Œå®æ—¶ä¿¡æ¯æ›´æ–°å’Œå›¾åƒå¤„ç†ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€ä»£ç†å·¥å…·åŸºå‡†ï¼ˆMATï¼‰ï¼Œç”¨äºè¯„ä¼°LVLMsçš„æœç´¢å’Œç¼–ç èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisual-ARFTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13138",
            "title": "Neurosymbolic Diffusion Models",
            "url": "https://huggingface.co/papers/2505.13138",
            "abstract": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic reasoning to solve tasks like visual reasoning. However, standard NeSy predictors assume conditional independence between the symbols they extract, thus limiting their ability to model interactions and uncertainty - often leading to overconfident predictions and poor out-of-distribution generalisation. To overcome the limitations of the independence assumption, we introduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. Our approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. Across both synthetic and real-world benchmarks - including high-dimensional visual path planning and rule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among NeSy predictors and demonstrate strong calibration.",
            "score": 22,
            "issue_id": 3876,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "56639003a63a4eb4",
            "authors": [
                "Emile van Krieken",
                "Pasquale Minervini",
                "Edoardo Ponti",
                "Antonio Vergari"
            ],
            "affiliations": [
                "Miniml.AI",
                "School of Informatics, University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13138.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#cv",
                    "#agents",
                    "#benchmark",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸",
                    "desc": "ĞĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (NeSyDMs) Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼Ñ‹Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², NeSyDMs ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒÑ‡ĞµÑ‚Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», NeSyDMs Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ."
                },
                "en": {
                    "title": "Enhancing Symbolic Reasoning with Dependency Modeling in Neurosymbolic Predictors",
                    "desc": "This paper presents neurosymbolic diffusion models (NeSyDMs), which enhance traditional neurosymbolic predictors by addressing the limitations of assuming conditional independence between symbols. By employing discrete diffusion processes, NeSyDMs effectively model the interactions and dependencies among symbols, leading to improved predictions. This method allows for scalable learning while also quantifying uncertainty, which is crucial for tasks like visual reasoning and autonomous driving. The results show that NeSyDMs outperform existing neurosymbolic approaches in accuracy and calibration on various benchmarks."
                },
                "zh": {
                    "title": "çªç ´ç‹¬ç«‹å‡è®¾ï¼Œæå‡ç¬¦å·ä¾èµ–æ€§å»ºæ¨¡",
                    "desc": "ç¥ç»ç¬¦å·é¢„æµ‹å™¨ï¼ˆNeSyï¼‰ç»“åˆäº†ç¥ç»æ„ŸçŸ¥å’Œç¬¦å·æ¨ç†ï¼Œç”¨äºè§£å†³è§†è§‰æ¨ç†ç­‰ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„NeSyé¢„æµ‹å™¨å‡è®¾æå–çš„ç¬¦å·ä¹‹é—´æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å»ºæ¨¡äº¤äº’å’Œä¸ç¡®å®šæ€§çš„èƒ½åŠ›ï¼Œå¸¸å¸¸å¯¼è‡´è¿‡äºè‡ªä¿¡çš„é¢„æµ‹å’Œè¾ƒå·®çš„åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³ç‹¬ç«‹æ€§å‡è®¾çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç¥ç»ç¬¦å·æ‰©æ•£æ¨¡å‹ï¼ˆNeSyDMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç±»æ–°çš„NeSyé¢„æµ‹å™¨ï¼Œåˆ©ç”¨ç¦»æ•£æ‰©æ•£æ¥å»ºæ¨¡ç¬¦å·ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ…æ‹¬é«˜ç»´è§†è§‰è·¯å¾„è§„åˆ’å’ŒåŸºäºè§„åˆ™çš„è‡ªåŠ¨é©¾é©¶ï¼Œå±•ç¤ºäº†NeSyDMsåœ¨NeSyé¢„æµ‹å™¨ä¸­çš„æœ€å…ˆè¿›å‡†ç¡®æ€§å’Œå¼ºå¤§çš„æ ¡å‡†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04388",
            "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
            "url": "https://huggingface.co/papers/2505.04388",
            "abstract": "Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.",
            "score": 19,
            "issue_id": 3874,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 Ğ¼Ğ°Ñ",
                "en": "May 7",
                "zh": "5æœˆ7æ—¥"
            },
            "hash": "12792ceffb601d5a",
            "authors": [
                "Dario Garcia-Gasulla",
                "Jordi Bayarri-Planas",
                "Ashwin Kumar Gururajan",
                "Enrique Lopez-Cuena",
                "Adrian Tormos",
                "Daniel Hinjos",
                "Pablo Bernabeu-Perez",
                "Anna Arias-Duart",
                "Pablo Agustin Martin-Torres",
                "Marta Gonzalez-Mallo",
                "Sergio Alvarez-Napagao",
                "Eduard AyguadÃ©-Parra",
                "Ulises CortÃ©s"
            ],
            "affiliations": [
                "Barcelona Supercomputing Center (BSC-CNS), Spain",
                "Universitat Polit`ecnica de Catalunya - Barcelona Tech (UPC), Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04388.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#data",
                    "#healthcare",
                    "#training",
                    "#open_source",
                    "#ethics",
                    "#benchmark",
                    "#rlhf",
                    "#rag"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Aloe Beta - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DPO Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· RAG. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ², ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Aloe Beta Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Empowering Healthcare with Open-Source LLMs: Safety Meets Efficacy",
                    "desc": "This paper discusses the development of open-source Large Language Models (LLMs) for healthcare, focusing on optimizing data preprocessing and training methods. It introduces Direct Preference Optimization (DPO) to enhance model safety and Retrieval-Augmented Generation (RAG) to improve efficacy. The evaluation methodology includes various tests to establish a new standard for assessing model performance in medical contexts. The resulting Aloe Beta models demonstrate competitive capabilities against private models while adhering to ethical guidelines and safety measures."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¼€æ”¾åŒ»ç–—æ¨¡å‹ï¼Œä¿éšœå…¬ä¼—åˆ©ç›Š",
                    "desc": "æœ¬ç ”ç©¶æ—¨åœ¨æ¨åŠ¨å¼€æ”¾æºä»£ç åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œä»¥ä¿æŠ¤å…¬ä¼—åˆ©ç›Šã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒçš„å…³é”®é˜¶æ®µï¼Œç ”ç©¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æé«˜æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œä»¥åŠé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æå‡æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚è¯„ä¼°æ–¹æ³•åŒ…æ‹¬å››ç§ä¸åŒç±»å‹çš„æµ‹è¯•ï¼Œä¸ºè¯¥é¢†åŸŸè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚æœ€ç»ˆå‘å¸ƒçš„æ¨¡å‹åœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨å®‰å…¨æ€§å’Œä¼¦ç†æ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14513",
            "title": "Latent Flow Transformer",
            "url": "https://huggingface.co/papers/2505.14513",
            "abstract": "Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in preserving coupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.",
            "score": 18,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "3683bab427c47086",
            "authors": [
                "Yen-Chen Wu",
                "Feng-Ting Liao",
                "Meng-Hsi Chen",
                "Pei-Chen Ho",
                "Farhang Nabiei",
                "Da-shan Shiu"
            ],
            "affiliations": [
                "MediaTek Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14513.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ²: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Latent Flow Transformer (LFT), Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LFT Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Flow Walking Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Pythia-410M Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LFT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¶Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞµĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Efficient Layer Compression with Latent Flow Transformers",
                    "desc": "This paper introduces the Latent Flow Transformer (LFT), a new architecture for large language models that replaces multiple discrete layers with a single learned transport operator. By utilizing flow matching, LFT achieves significant model compression while still being compatible with traditional transformer designs. The authors also present the Flow Walking (FW) algorithm to enhance the coupling preservation in flow-based methods. Experimental results show that LFT can effectively reduce the number of layers while improving performance metrics, bridging the gap between autoregressive and flow-based generation techniques."
                },
                "zh": {
                    "title": "æ½œåœ¨æµå˜æ¢å™¨ï¼šé«˜æ•ˆå‹ç¼©å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹â€”â€”æ½œåœ¨æµå˜æ¢å™¨ï¼ˆLatent Flow Transformer, LFTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚LFTé€šè¿‡ä½¿ç”¨å­¦ä¹ çš„ä¼ è¾“ç®—å­æ›¿ä»£å¤šä¸ªç¦»æ•£å±‚ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹æ¶æ„çš„å…¼å®¹æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æµæ­¥è¡Œï¼ˆFlow Walking, FWï¼‰ç®—æ³•ï¼Œä»¥è§£å†³ç°æœ‰æµåŸºæ–¹æ³•åœ¨ä¿æŒè€¦åˆæ–¹é¢çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLFTåœ¨å‹ç¼©å±‚æ•°çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨æ€§èƒ½ä¸Šè¶…è¶Šä¼ ç»Ÿçš„å±‚è·³è¿‡æ–¹æ³•ï¼Œç¼©å°è‡ªå›å½’å’Œæµç”ŸæˆèŒƒå¼ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14674",
            "title": "Reward Reasoning Model",
            "url": "https://huggingface.co/papers/2505.14674",
            "abstract": "Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning.",
            "score": 13,
            "issue_id": 3871,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "b51747905eeda5db",
            "authors": [
                "Jiaxin Guo",
                "Zewen Chi",
                "Li Dong",
                "Qingxiu Dong",
                "Xun Wu",
                "Shaohan Huang",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14674.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#reasoning",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Reward Reasoning Models (RRMs). RRMs Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RRMs Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Reward Models with Adaptive Reasoning",
                    "desc": "This paper introduces Reward Reasoning Models (RRMs), which enhance the performance of reward models in large language models by incorporating a structured reasoning process. RRMs utilize additional computational resources during testing to tackle complex queries where the correct rewards are not obvious. The authors employ a reinforcement learning framework that allows these models to develop their reasoning capabilities autonomously, without needing specific reasoning examples in the training data. Experimental results indicate that RRMs outperform existing reward modeling methods across various benchmarks, demonstrating their ability to adaptively use test-time compute for improved reward accuracy."
                },
                "zh": {
                    "title": "æå‡å¥–åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "å¥–åŠ±æ¨¡å‹åœ¨å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç¬¦åˆäººç±»æœŸæœ›çš„è¾“å‡ºä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œåœ¨æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—ä»¥æå‡å¥–åŠ±æ¨¡å‹æ€§èƒ½æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†å¥–åŠ±æ¨ç†æ¨¡å‹ï¼ˆRRMsï¼‰ï¼Œå®ƒä»¬ä¸“é—¨è®¾è®¡ç”¨äºåœ¨ç”Ÿæˆæœ€ç»ˆå¥–åŠ±ä¹‹å‰æ‰§è¡Œæ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRRMsåœ¨å„ä¸ªé¢†åŸŸçš„å¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¹¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—è¿›ä¸€æ­¥æé«˜å¥–åŠ±å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14489",
            "title": "Reasoning Models Better Express Their Confidence",
            "url": "https://huggingface.co/papers/2505.14489",
            "abstract": "Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models-LLMs that engage in extended chain-of-thought (CoT) reasoning-exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models-such as exploring alternative approaches and backtracking-which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that these gains are not exclusive to reasoning models-non-reasoning models also benefit when guided to perform slow thinking via in-context learning.",
            "score": 12,
            "issue_id": 3871,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "c945246738ceba22",
            "authors": [
                "Dongkeun Yoon",
                "Seungone Kim",
                "Sohee Yang",
                "Sunkyoung Kim",
                "Soyeon Kim",
                "Yongil Kim",
                "Eunbi Choi",
                "Yireun Kim",
                "Minjoon Seo"
            ],
            "affiliations": [
                "CMU",
                "KAIST",
                "LG AI Research",
                "UCL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14489.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (reasoning models) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ LLM. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡ĞµÑ‚ 'Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ' - Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² 'Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ' Ğ¸Ğ· CoT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Boosting Confidence Calibration in Language Models through Reasoning",
                    "desc": "This paper explores how large language models (LLMs) can improve their confidence calibration through reasoning techniques. It shows that LLMs that use chain-of-thought (CoT) reasoning not only solve problems better but also express their confidence more accurately. The study benchmarks six reasoning models and finds that they outperform non-reasoning models in confidence calibration across most scenarios. The authors conclude that the slow thinking behaviors inherent in reasoning models allow them to dynamically adjust their confidence, leading to better performance as the reasoning process unfolds."
                },
                "zh": {
                    "title": "æ¨ç†æ¨¡å‹æå‡è‡ªä¿¡åº¦æ ¡å‡†çš„ç§˜å¯†",
                    "desc": "å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å¾ˆå¼ºçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨è¡¨è¾¾è‡ªä¿¡åº¦æ–¹é¢å¸¸å¸¸ä¸å‡†ç¡®ï¼Œè¿™ä½¿å¾—è¯„ä¼°å…¶é”™è¯¯çš„å¯èƒ½æ€§å˜å¾—å›°éš¾ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„å¯é æ€§ã€‚æœ¬æ–‡å±•ç¤ºäº†æ¨ç†æ¨¡å‹ï¼Œå³è¿›è¡Œæ‰©å±•æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†çš„LLMsï¼Œä¸ä»…åœ¨è§£å†³é—®é¢˜æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œè€Œä¸”åœ¨å‡†ç¡®è¡¨è¾¾è‡ªä¿¡åº¦æ–¹é¢ä¹Ÿè¡¨ç°æ›´ä½³ã€‚æˆ‘ä»¬å¯¹å…­ä¸ªæ¨ç†æ¨¡å‹åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å®ƒä»¬åœ¨36ç§è®¾ç½®ä¸­æœ‰33ç§æƒ…å†µä¸‹çš„è‡ªä¿¡åº¦æ ¡å‡†æ˜æ˜¾ä¼˜äºéæ¨ç†æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ ¡å‡†çš„æå‡æºäºæ¨ç†æ¨¡å‹çš„æ…¢æ€ç»´è¡Œä¸ºï¼Œå¦‚æ¢ç´¢æ›¿ä»£æ–¹æ³•å’Œå›æº¯ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿåœ¨æ€ç»´é“¾ä¸­åŠ¨æ€è°ƒæ•´è‡ªä¿¡åº¦ï¼Œä»è€Œé€æ­¥æé«˜å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13547",
            "title": "Exploring Federated Pruning for Large Language Models",
            "url": "https://huggingface.co/papers/2505.13547",
            "abstract": "LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at https://github.com/Pengxin-Guo/FedPrLLM.",
            "score": 12,
            "issue_id": 3872,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "436f0f2e8c3f8481",
            "authors": [
                "Pengxin Guo",
                "Yinong Wang",
                "Wei Li",
                "Mengting Liu",
                "Ming Li",
                "Jinkai Zheng",
                "Liangqiong Qu"
            ],
            "affiliations": [
                "Guangming Laboratory",
                "Hangzhou Dianzi University",
                "Southern University of Science and Technology",
                "Sun Yat-sen University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13547.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#security",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "Ğ¤ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "FedPrLLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ Ğ¼Ğ°ÑĞºĞ¸ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒÑÑ Ğ¸Ğ¼Ğ¸ Ñ ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³ Ñ Ğ¿Ğ¾ÑĞ´Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ±ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Privacy-Preserving Compression of LLMs with FedPrLLM",
                    "desc": "This paper presents FedPrLLM, a federated pruning framework aimed at compressing large language models (LLMs) while ensuring data privacy. Unlike traditional methods that require public calibration samples, FedPrLLM allows clients to generate pruning masks using their local data, which are then shared with a central server. This collaborative approach enables the global model to be pruned without exposing sensitive local data. The authors conducted experiments to identify the best pruning strategies, concluding that one-shot pruning with layer comparison and no weight scaling is the most effective method within their framework."
                },
                "zh": {
                    "title": "éšç§ä¿æŠ¤ä¸‹çš„LLMå‰ªææ–°æ–¹æ³•",
                    "desc": "LLMå‰ªææ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æŠ€æœ¯ï¼Œå¯ä»¥å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šè¿è¡Œã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦å…¬å…±æ ¡å‡†æ ·æœ¬ï¼Œè¿™åœ¨éšç§æ•æ„Ÿçš„é¢†åŸŸä¸­å¾ˆéš¾è·å¾—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FedPrLLMï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è”é‚¦å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶å‹ç¼©LLMã€‚åœ¨FedPrLLMä¸­ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åªéœ€æ ¹æ®æœ¬åœ°æ ¡å‡†æ•°æ®è®¡ç®—å‰ªææ©ç çŸ©é˜µï¼Œå¹¶å°†å…¶ä¸æœåŠ¡å™¨å…±äº«ï¼Œä»è€Œå¯¹å…¨å±€æ¨¡å‹è¿›è¡Œå‰ªæã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14677",
            "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.14677",
            "abstract": "Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM -- by prompting the model to produce a reasoning chain before providing an answer -- can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks.",
            "score": 11,
            "issue_id": 3873,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "032b4d528d6984fd",
            "authors": [
                "Jiaer Xia",
                "Yuhang Zang",
                "Peng Gao",
                "Yixuan Li",
                "Kaiyang Zhou"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Shanghai AI Lab",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14677.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… 'shortcuts'. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° 'Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ-Ğ¾Ñ‚Ğ²ĞµÑ‚', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ±ÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Visionary-R1 Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Captions First!",
                    "desc": "This paper addresses the challenge of enhancing reasoning capabilities in visual language models (VLMs) using reinforcement learning. The authors propose a novel training approach that emphasizes generating detailed captions for images before reasoning, which helps prevent shortcut learning. By training their model, Visionary-R1, on a large dataset of visual question-answer pairs without explicit chain-of-thought supervision, they achieve superior performance compared to existing multimodal models. The results suggest that focusing on image interpretation prior to reasoning can significantly improve generalization across diverse data distributions."
                },
                "zh": {
                    "title": "é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥è¿›è¡Œå›¾åƒæ•°æ®çš„æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–äºæ˜¾å¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£ï¼Œè€Œæ˜¯é€šè¿‡è§†è§‰é—®ç­”å¯¹æ¥å®ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç®€å•åœ°åº”ç”¨å¼ºåŒ–å­¦ä¹ å¯ä»¥å¯¼è‡´æ¨¡å‹åœ¨å›ç­”ä¹‹å‰ç”Ÿæˆæ¨ç†é“¾ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹æ–°æ•°æ®æ—¶çš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºè®©æ¨¡å‹åœ¨æ¨ç†ä¹‹å‰å…ˆå¯¹å›¾åƒè¿›è¡Œè§£é‡Šï¼Œä»è€Œæé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14673",
            "title": "Training-Free Watermarking for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2505.14673",
            "abstract": "Invisible image watermarking can protect image ownership and prevent malicious misuse of visual generative models. However, existing generative watermarking methods are mainly designed for diffusion models while watermarking for autoregressive image generation models remains largely underexplored. We propose IndexMark, a training-free watermarking framework for autoregressive image generation models. IndexMark is inspired by the redundancy property of the codebook: replacing autoregressively generated indices with similar indices produces negligible visual differences. The core component in IndexMark is a simple yet effective match-then-replace method, which carefully selects watermark tokens from the codebook based on token similarity, and promotes the use of watermark tokens through token replacement, thereby embedding the watermark without affecting the image quality. Watermark verification is achieved by calculating the proportion of watermark tokens in generated images, with precision further improved by an Index Encoder. Furthermore, we introduce an auxiliary validation scheme to enhance robustness against cropping attacks. Experiments demonstrate that IndexMark achieves state-of-the-art performance in terms of image quality and verification accuracy, and exhibits robustness against various perturbations, including cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG compression.",
            "score": 11,
            "issue_id": 3873,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "ec739be428657981",
            "authors": [
                "Yu Tong",
                "Zihao Pan",
                "Shuai Yang",
                "Kaiyang Zhou"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Peking University",
                "Sun Yat-sen University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14673.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#training",
                    "#security"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "IndexMark - ÑÑ‚Ğ¾ Ğ±ĞµĞ·Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ Ğ½Ğ° Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ° Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ° Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ´Ğ¾Ğ»Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ° Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IndexMark Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "IndexMark: Watermarking Autoregressive Models with Precision and Robustness",
                    "desc": "This paper presents IndexMark, a novel watermarking framework specifically designed for autoregressive image generation models. Unlike previous methods focused on diffusion models, IndexMark utilizes the redundancy in the codebook to embed watermarks without compromising image quality. The framework employs a match-then-replace strategy to select and replace indices with similar ones, effectively embedding the watermark. Additionally, it includes a robust verification process and an auxiliary validation scheme to withstand various image perturbations, demonstrating superior performance in both image quality and watermark verification accuracy."
                },
                "zh": {
                    "title": "è‡ªå›å½’å›¾åƒç”Ÿæˆçš„éšå½¢æ°´å°æ–°æ–¹æ¡ˆ",
                    "desc": "éšå½¢å›¾åƒæ°´å°æŠ€æœ¯å¯ä»¥ä¿æŠ¤å›¾åƒçš„æ‰€æœ‰æƒï¼Œå¹¶é˜²æ­¢è§†è§‰ç”Ÿæˆæ¨¡å‹çš„æ¶æ„æ»¥ç”¨ã€‚ç°æœ‰çš„ç”Ÿæˆæ°´å°æ–¹æ³•ä¸»è¦é’ˆå¯¹æ‰©æ•£æ¨¡å‹ï¼Œè€Œè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ°´å°æŠ€æœ¯å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æˆ‘ä»¬æå‡ºäº†IndexMarkï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹æ°´å°æ¡†æ¶ã€‚IndexMarké€šè¿‡åŒ¹é…å’Œæ›¿æ¢çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä»£ç æœ¬çš„å†—ä½™ç‰¹æ€§ï¼ŒåµŒå…¥æ°´å°è€Œä¸å½±å“å›¾åƒè´¨é‡ï¼Œå¹¶åœ¨å¤šç§å¹²æ‰°ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14652",
            "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
            "url": "https://huggingface.co/papers/2505.14652",
            "abstract": "Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.",
            "score": 11,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "494fe90709dc6c63",
            "authors": [
                "Xueguang Ma",
                "Qian Liu",
                "Dongfu Jiang",
                "Ge Zhang",
                "Zejun Ma",
                "Wenhu Chen"
            ],
            "affiliations": [
                "M-A-P",
                "Singapore",
                "TikTok",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14652.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#benchmark",
                    "#math",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ĞµĞ»ÑŒ: Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ General-Reasoner, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ General-Reasoner Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering LLMs with General-Reasoner for Diverse Reasoning",
                    "desc": "This paper introduces General-Reasoner, a new approach to improve the reasoning abilities of large language models (LLMs) across various fields. It leverages a large-scale dataset of questions with verifiable answers, which is created through web crawling, to train LLMs without the need for prior supervised fine-tuning. Additionally, it employs a generative model-based answer verifier that enhances the model's ability to understand context and think through problems. The results show that General-Reasoner significantly outperforms existing methods in reasoning tasks, especially in mathematics, while also being effective in other domains like physics and finance."
                },
                "zh": {
                    "title": "æå‡LLMæ¨ç†èƒ½åŠ›çš„æ–°èŒƒå¼",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒèŒƒå¼â€”â€”General-Reasonerï¼Œæ—¨åœ¨å¢å¼ºLLMåœ¨å¤šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é—®é¢˜æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œå–ä»£äº†ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„éªŒè¯æ–¹æ³•ã€‚é€šè¿‡åœ¨å¤šä¸ªé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒGeneral-Reasoneråœ¨æ¨ç†æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13866",
            "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
            "url": "https://huggingface.co/papers/2505.13866",
            "abstract": "Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60times compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.",
            "score": 11,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "72f6460e348e135a",
            "authors": [
                "Jiwon Song",
                "Dongwon Jo",
                "Yulhwa Kim",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Seoul National University",
                "Sungkyunkwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13866.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ñ ĞŸÑƒÑ‚Ğ¸ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RPC) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. RPC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ KV-ĞºÑÑˆ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RPC ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ QwQ-32B Ğ´Ğ¾ 1.60 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ KV-ĞºÑÑˆĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»ĞµĞ´Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Efficient Inference with Reasoning Path Compression",
                    "desc": "This paper introduces Reasoning Path Compression (RPC), a method designed to enhance the efficiency of reasoning-focused language models during inference. By utilizing the concept of semantic sparsity, RPC compresses the key-value (KV) cache, retaining only the most important elements based on recent queries. This approach significantly increases the throughput of token generation while only slightly affecting accuracy. The results indicate that RPC can improve the performance of large models like QwQ-32B, making them more practical for real-world applications."
                },
                "zh": {
                    "title": "æ¨ç†è·¯å¾„å‹ç¼©ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘ä¸“æ³¨äºæ¨ç†çš„è¯­è¨€æ¨¡å‹é€šè¿‡ç”Ÿæˆè¾ƒé•¿çš„ä¸­é—´æ¨ç†è·¯å¾„æ¥å®ç°é«˜å‡†ç¡®ç‡ã€‚è¿™ç§æ–¹æ³•åœ¨è§£å†³éœ€è¦é€»è¾‘æ€ç»´çš„é—®é¢˜æ—¶éå¸¸æœ‰æ•ˆï¼Œä½†é•¿æ¨ç†è·¯å¾„æ˜¾è‘—å¢åŠ äº†å†…å­˜ä½¿ç”¨å’Œä»¤ç‰Œç”Ÿæˆçš„ååé‡ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ¨ç†è·¯å¾„å‹ç¼©ï¼ˆRPCï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ¨ç†è·¯å¾„çš„è¯­ä¹‰ç¨€ç–æ€§æ¥åŠ é€Ÿæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒRPCåœ¨AIME 2024åŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”äºå®Œæ•´KVç¼“å­˜ï¼Œæå‡äº†QwQ-32Bçš„ç”Ÿæˆååé‡ï¼Œå‡†ç¡®ç‡ä»…ä¸‹é™1.2%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14640",
            "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
            "url": "https://huggingface.co/papers/2505.14640",
            "abstract": "Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals a rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, a significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame from a long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As a result, the validity and robustness of current LVU benchmarks are undermined, impeding a faithful assessment of LMMs' long-video understanding capability. To tackle this problem, we propose VideoEval-Pro, a realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VideoEval-Pro assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance (>25\\%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input frames. Our results show that VideoEval-Pro offers a more realistic and reliable measure of long video understanding, providing a clearer view of progress in this domain.",
            "score": 9,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "45d64d535935c6a4",
            "authors": [
                "Wentao Ma",
                "Weiming Ren",
                "Yiming Jia",
                "Zhuofeng Li",
                "Ping Nie",
                "Ge Zhang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Independent",
                "M-A-P",
                "Shanghai University",
                "University of Toronto",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14640.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LMM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ°Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¸Ğ·-Ğ·Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VideoEval-Pro Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ, Ñ‡ĞµĞ¼ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LMM Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Long Video Understanding with Realistic Benchmarks",
                    "desc": "This paper discusses the limitations of current benchmarks for evaluating long video understanding (LVU) in large multimodal models (LMMs). It highlights that many existing benchmarks rely on multiple-choice questions (MCQs), which can inflate performance scores due to guessing and prior knowledge. The authors introduce VideoEval-Pro, a new benchmark that uses open-ended questions requiring comprehensive video understanding, thus providing a more accurate assessment of LMM capabilities. Their findings reveal significant performance drops for LMMs on open-ended questions compared to MCQs, indicating that current benchmarks may not effectively measure true understanding of long videos."
                },
                "zh": {
                    "title": "VideoEval-Proï¼šæå‡é•¿è§†é¢‘ç†è§£çš„çœŸå®è¯„ä¼°",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰çš„LVUåŸºå‡†æµ‹è¯•å­˜åœ¨é—®é¢˜ã€‚è®¸å¤šåŸºå‡†ä¾èµ–å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ï¼Œè¿™å¯¼è‡´è¯„ä¼°ç»“æœè¢«å¤¸å¤§ï¼Œå› ä¸ºæ¨¡å‹å¯èƒ½é€šè¿‡çŒœæµ‹è·å¾—æ­£ç¡®ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œéƒ¨åˆ†é—®é¢˜çš„å…ˆéªŒä¿¡æ¯ä½¿å¾—æ¨¡å‹å¯ä»¥åœ¨ä¸è§‚çœ‹è§†é¢‘çš„æƒ…å†µä¸‹ç›´æ¥å›ç­”ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VideoEval-ProåŸºå‡†ï¼Œé‡‡ç”¨å¼€æ”¾å¼çŸ­ç­”æ¡ˆé—®é¢˜ï¼ŒçœŸæ­£è€ƒå¯Ÿæ¨¡å‹å¯¹æ•´ä¸ªè§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14631",
            "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
            "url": "https://huggingface.co/papers/2505.14631",
            "abstract": "Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems.",
            "score": 9,
            "issue_id": 3872,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "12732abf8e9d807f",
            "authors": [
                "Lingjie Jiang",
                "Xun Wu",
                "Shaohan Huang",
                "Qingxiu Dong",
                "Zewen Chi",
                "Li Dong",
                "Xingxing Zhang",
                "Tengchao Lv",
                "Lei Cui",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14631.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ - Large Hybrid-Reasoning Models (LHRMs), ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Hybrid Fine-Tuning (HFT) Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Hybrid Group Policy Optimization (HGPO). Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Hybrid Accuracy Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LHRMs Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ LRMs Ğ¸ LLMs Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Adaptive Thinking for Efficient Reasoning",
                    "desc": "This paper presents Large Hybrid-Reasoning Models (LHRMs), which enhance reasoning abilities by deciding when to engage in extended thinking based on the complexity of user queries. Unlike traditional Large Language Models (LLMs), LHRMs use a two-stage training approach that includes Hybrid Fine-Tuning and online reinforcement learning to optimize their reasoning process. The authors introduce a new metric, Hybrid Accuracy, to evaluate the effectiveness of these models in adapting their thinking strategies. Experimental results demonstrate that LHRMs outperform existing models in both reasoning and efficiency, suggesting a new direction for developing intelligent systems that balance thinking depth with response speed."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”æ··åˆæ¨ç†ï¼Œæå‡æ•ˆç‡ä¸èƒ½åŠ›",
                    "desc": "æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå› ä¸ºå®ƒä»¬åœ¨ç”Ÿæˆæœ€ç»ˆå›ç­”ä¹‹å‰è¿›è¡Œäº†æ›´æ·±å…¥çš„æ€è€ƒã€‚ç„¶è€Œï¼Œè¿‡é•¿çš„æ€è€ƒè¿‡ç¨‹ä¼šå¯¼è‡´ä»¤ç‰Œæ¶ˆè€—å’Œå»¶è¿Ÿçš„æ˜¾è‘—å¢åŠ ï¼Œè¿™åœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶å°¤å…¶ä¸å¿…è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰ï¼Œè¿™ç§æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è‡ªé€‚åº”åœ°å†³å®šæ˜¯å¦è¿›è¡Œæ€è€ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒLHRMsåœ¨å¤„ç†ä¸åŒéš¾åº¦å’Œç±»å‹çš„æŸ¥è¯¢æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œæ··åˆæ€è€ƒï¼Œå¹¶åœ¨æ¨ç†å’Œæ•´ä½“èƒ½åŠ›ä¸Šè¶…è¶Šç°æœ‰çš„LRMså’ŒLLMsï¼ŒåŒæ—¶æ˜¾è‘—æé«˜æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13559",
            "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models",
            "url": "https://huggingface.co/papers/2505.13559",
            "abstract": "Code-switching (CS) poses a significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. Evaluating ten LLMs, including open and closed-source models, we analyze performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data.",
            "score": 9,
            "issue_id": 3870,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "9ea21df740810e7e",
            "authors": [
                "Sathya Krishnan Suresh",
                "Tanmay Surana",
                "Lim Zhi Hao",
                "Eng Siong Chng"
            ],
            "affiliations": [
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13559.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#machine_translation",
                    "#low_resource",
                    "#synthetic",
                    "#training",
                    "#benchmark",
                    "#multilingual"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "CS-Sum: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CS-Sum Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ² (code-switching) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹-Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹, Ñ‚Ğ°Ğ¼Ğ¸Ğ»ÑŒÑĞºĞ¸Ğ¹-Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ Ğ¼Ğ°Ğ»Ğ°Ğ¹ÑĞºĞ¸Ğ¹-Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ few-shot Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ¸ÑĞºĞ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¼Ñ‹ÑĞ» Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº."
                },
                "en": {
                    "title": "Enhancing LLMs for Code-Switching Comprehensibility",
                    "desc": "This paper addresses the challenges that Large Language Models (LLMs) face when dealing with code-switching (CS) in dialogues. It introduces CS-Sum, a benchmark designed to assess how well LLMs can summarize CS dialogues into English, focusing on three language pairs: Mandarin-English, Tamil-English, and Malay-English. The study evaluates ten different LLMs using various methods, including few-shot learning and fine-tuning techniques, to understand their performance on CS data. The results reveal that while LLMs achieve high scores on automated metrics, they often make subtle errors that can change the meaning of the dialogues, highlighting the need for improved training on code-switched content."
                },
                "zh": {
                    "title": "è¯„ä¼°ä»£ç åˆ‡æ¢çš„å¯ç†è§£æ€§",
                    "desc": "ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œä½†å…¶å¯ç†è§£æ€§åœ¨LLMsä¸­çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†CS-Sumï¼Œæ—¨åœ¨é€šè¿‡å°†CSå¯¹è¯æ€»ç»“ä¸ºè‹±è¯­æ¥è¯„ä¼°LLMså¯¹CSçš„ç†è§£èƒ½åŠ›ã€‚CS-Sumæ˜¯é¦–ä¸ªé’ˆå¯¹æ™®é€šè¯-è‹±è¯­ã€æ³°ç±³å°”è¯­-è‹±è¯­å’Œé©¬æ¥è¯­-è‹±è¯­çš„CSå¯¹è¯æ€»ç»“åŸºå‡†ï¼ŒåŒ…å«æ¯å¯¹è¯­è¨€900åˆ°1300ä¸ªäººå·¥æ ‡æ³¨çš„å¯¹è¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå°½ç®¡è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å¾—åˆ†è¾ƒé«˜ï¼ŒLLMsåœ¨å¤„ç†CSè¾“å…¥æ—¶ä»ä¼šå‡ºç°ç»†å¾®é”™è¯¯ï¼Œè¿™äº›é”™è¯¯ä¼šæ”¹å˜å¯¹è¯çš„å®Œæ•´å«ä¹‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14680",
            "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
            "url": "https://huggingface.co/papers/2505.14680",
            "abstract": "Generative AI search is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline, spanning query decomposition, document retrieval, and answer generation, yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop. In this paper, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages; and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.",
            "score": 8,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "ace242db16327202",
            "authors": [
                "Sunhao Dai",
                "Wenjie Wang",
                "Liang Pang",
                "Jun Xu",
                "See-Kiong Ng",
                "Ji-Rong Wen",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "CAS Key Laboratory of AI Safety Institute of Computing Technology Chinese Academy of Sciences",
                "Gaoling School of Artificial Intelligence Renmin University of China",
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14680.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#rag",
                    "#rlhf",
                    "#agents",
                    "#alignment"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¸ÑĞº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ NExT-Search, Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ½Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ ÑƒÑ‚Ñ€Ğ°Ñ‡ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜. NExT-Search Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°: Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ñ‚ĞµĞ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "NExT-Search: Enhancing Generative AI Search with User Feedback",
                    "desc": "This paper discusses the challenges of integrating user feedback into generative AI search systems, which provide direct answers to complex queries but lack detailed feedback mechanisms. Traditional web search benefits from fine-grained user interactions, allowing for continuous improvement of ranking models. The proposed NExT-Search framework aims to bridge this gap by introducing two modes of user feedback: User Debug Mode for active user engagement and Shadow User Mode for passive feedback collection. By leveraging both real-time and aggregated feedback, NExT-Search seeks to enhance the generative AI search process and ensure it evolves in response to user needs."
                },
                "zh": {
                    "title": "NExT-Searchï¼šé‡å¡‘ç”Ÿæˆå¼æœç´¢çš„åé¦ˆå¾ªç¯",
                    "desc": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœç´¢æ­£åœ¨æ”¹å˜ä¿¡æ¯æ£€ç´¢ï¼Œé€šè¿‡æä¾›ç«¯åˆ°ç«¯çš„ç­”æ¡ˆæ¥åº”å¯¹å¤æ‚æŸ¥è¯¢ï¼Œå‡å°‘ç”¨æˆ·æ‰‹åŠ¨æµè§ˆå’Œæ€»ç»“å¤šä¸ªç½‘é¡µçš„ä¾èµ–ã€‚ç„¶è€Œï¼Œè¿™ç§æ–°æ¨¡å¼è™½ç„¶æé«˜äº†ä¾¿åˆ©æ€§ï¼Œå´æ‰“ç ´äº†ä¼ ç»Ÿç½‘é¡µæœç´¢ä¸­åŸºäºåé¦ˆçš„æ”¹è¿›å¾ªç¯ã€‚ä¼ ç»Ÿæœç´¢å¯ä»¥é€šè¿‡æ”¶é›†ç”¨æˆ·åé¦ˆï¼ˆå¦‚ç‚¹å‡»ç‡å’Œåœç•™æ—¶é—´ï¼‰æ¥ä¸æ–­æ”¹è¿›æ’åæ¨¡å‹ï¼Œè€Œç”Ÿæˆå¼æœç´¢åˆ™é¢ä¸´åé¦ˆå¾ªç¯æ–­è£‚çš„é—®é¢˜ï¼Œç”¨æˆ·åé¦ˆéš¾ä»¥æœ‰æ•ˆæ˜ å°„åˆ°ç³»ç»Ÿçš„å…·ä½“ç»„ä»¶ã€‚æœ¬æ–‡æå‡ºäº†NExT-Searchï¼Œæ—¨åœ¨å°†ç»†ç²’åº¦çš„è¿‡ç¨‹çº§åé¦ˆé‡æ–°å¼•å…¥ç”Ÿæˆå¼æœç´¢ï¼Œç»“åˆç”¨æˆ·è°ƒè¯•æ¨¡å¼å’Œå½±å­ç”¨æˆ·æ¨¡å¼ï¼Œä»¥å®ç°å®æ—¶å’Œç¦»çº¿çš„åé¦ˆä¿¡å·åˆ©ç”¨ï¼Œä»è€ŒæŒç»­æ”¹è¿›æœç´¢ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14135",
            "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model",
            "url": "https://huggingface.co/papers/2505.14135",
            "abstract": "Intelligent game creation represents a transformative advancement in game development, utilizing generative artificial intelligence to dynamically generate and enhance game content. Despite notable progress in generative models, the comprehensive synthesis of high-quality game assets, including both images and videos, remains a challenging frontier. To create high-fidelity game content that simultaneously aligns with player preferences and significantly boosts designer efficiency, we present Hunyuan-Game, an innovative project designed to revolutionize intelligent game production. Hunyuan-Game encompasses two primary branches: image generation and video generation. The image generation component is built upon a vast dataset comprising billions of game images, leading to the development of a group of customized image generation models tailored for game scenarios: (1) General Text-to-Image Generation. (2) Game Visual Effects Generation, involving text-to-effect and reference image-based game visual effect generation. (3) Transparent Image Generation for characters, scenes, and game visual effects. (4) Game Character Generation based on sketches, black-and-white images, and white models. The video generation component is built upon a comprehensive dataset of millions of game and anime videos, leading to the development of five core algorithmic models, each targeting critical pain points in game development and having robust adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2) 360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4) Generative Video Super-Resolution. (5) Interactive Game Video Generation. These image and video generation models not only exhibit high-level aesthetic expression but also deeply integrate domain-specific knowledge, establishing a systematic understanding of diverse game and anime art styles.",
            "score": 8,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "344469b85ea1e75e",
            "authors": [
                "Ruihuang Li",
                "Caijin Zhou",
                "Shoujian Zheng",
                "Jianxiang Lu",
                "Jiabin Huang",
                "Comi Chen",
                "Junshu Tang",
                "Guangzheng Xu",
                "Jiale Tao",
                "Hongmei Wang",
                "Donghao Li",
                "Wenqing Yu",
                "Senbo Wang",
                "Zhimin Li",
                "Yetshuan Shi",
                "Haoyu Yang",
                "Yukun Wang",
                "Wenxun Dai",
                "Jiaqi Li",
                "Linqing Wang",
                "Qixun Wang",
                "Zhiyong Xu",
                "Yingfang Zhang",
                "Jiangfeng Xiong",
                "Weijie Kong",
                "Chao Zhang",
                "Hongxin Zhang",
                "Qiaoling Zheng",
                "Weiting Guo",
                "Xinchi Deng",
                "Yixuan Li",
                "Renjia Wei",
                "Yulin Jian",
                "Duojun Huang",
                "Xuhua Ren",
                "Sihuan Lin",
                "Yifu Sun",
                "Yuan Zhou",
                "Joey Wang",
                "Qin Lin",
                "Jingmiao Yu",
                "Jihong Zhang",
                "Caesar Zhong",
                "Di Wang",
                "Yuhong Liu",
                "Linus",
                "Jie Jiang",
                "Longhuang Wu",
                "Shuai Shao",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14135.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#cv",
                    "#games",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ³Ñ€: Ğ˜Ğ˜ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ²",
                    "desc": "ĞŸÑ€Ğ¾ĞµĞºÑ‚ Hunyuan-Game Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ³Ñ€ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ´Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸Ğ³Ñ€."
                },
                "en": {
                    "title": "Revolutionizing Game Development with AI-Driven Content Creation",
                    "desc": "The paper introduces Hunyuan-Game, a project that leverages generative artificial intelligence to enhance game development by creating high-quality game assets. It focuses on two main areas: image generation and video generation, utilizing extensive datasets of game images and videos. The image generation models include various techniques for creating game visuals, such as text-to-image and character generation from sketches. The video generation models address specific challenges in game video production, offering solutions like image-to-video synthesis and interactive video generation, all while maintaining aesthetic quality and understanding of game art styles."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ¸¸æˆåˆ›ä½œçš„æœªæ¥",
                    "desc": "æ™ºèƒ½æ¸¸æˆåˆ›ä½œæ˜¯æ¸¸æˆå¼€å‘ä¸­çš„ä¸€é¡¹å˜é©æ€§è¿›å±•ï¼Œåˆ©ç”¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åŠ¨æ€ç”Ÿæˆå’Œå¢å¼ºæ¸¸æˆå†…å®¹ã€‚å°½ç®¡ç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†é«˜è´¨é‡æ¸¸æˆèµ„äº§çš„ç»¼åˆåˆæˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚Hunyuan-Gameé¡¹ç›®æ—¨åœ¨é€šè¿‡å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œæå‡æ¸¸æˆå†…å®¹çš„è´¨é‡å’Œè®¾è®¡å¸ˆçš„æ•ˆç‡ã€‚è¯¥é¡¹ç›®åŒ…æ‹¬å›¾åƒç”Ÿæˆå’Œè§†é¢‘ç”Ÿæˆä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œæ¶µç›–äº†å¤šç§å®šåˆ¶åŒ–çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæ»¡è¶³ä¸åŒæ¸¸æˆåœºæ™¯çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13430",
            "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
            "url": "https://huggingface.co/papers/2505.13430",
            "abstract": "As the size of large language models grows exponentially, GPU memory has become a bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within a unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous quantization scale for gradient estimation and uses a directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by more than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and Stable Diffusion 3.5 Large within a single 24GB GPU.",
            "score": 8,
            "issue_id": 3873,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "6728dda02398fbcc",
            "authors": [
                "Sifeng Shang",
                "Jiayi Zhou",
                "Chenyu Lin",
                "Minxian Li",
                "Kaiyang Zhou"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Nanjing University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13430.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "QZO: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Quantized Zeroth-order Optimization (QZO). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. QZO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰Ğ°Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ ÑˆĞºĞ°Ğ»Ñƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Memory Efficiency in Large Language Model Training",
                    "desc": "This paper addresses the challenge of training large language models (LLMs) with limited GPU memory. It introduces a method called Quantized Zeroth-order Optimization (QZO) that reduces memory usage by eliminating the need for gradients and optimizer states. QZO achieves this by perturbing the quantization scale of weights to estimate gradients, allowing for efficient training without the need for de-quantization. The proposed approach significantly lowers memory costs, enabling the fine-tuning of large models on standard GPUs."
                },
                "zh": {
                    "title": "çªç ´å†…å­˜ç“¶é¢ˆï¼Œå®ç°é«˜æ•ˆè®­ç»ƒ",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹è§„æ¨¡çš„å¿«é€Ÿå¢é•¿ï¼ŒGPUå†…å­˜æˆä¸ºé€‚åº”è¿™äº›æ¨¡å‹åˆ°ä¸‹æ¸¸ä»»åŠ¡çš„ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å†…å­˜é«˜æ•ˆè®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨ç»Ÿä¸€æ¡†æ¶å†…æœ€å°åŒ–æ¨¡å‹æƒé‡ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜ä½¿ç”¨ã€‚æˆ‘ä»¬ä½¿ç”¨é›¶é˜¶ä¼˜åŒ–æ¥æ¶ˆé™¤æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œé€šè¿‡åœ¨å‰å‘ä¼ æ’­ä¸­æ‰°åŠ¨æƒé‡æ¥è¿‘ä¼¼æ¢¯åº¦æ–¹å‘ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†é‡åŒ–é›¶é˜¶ä¼˜åŒ–ï¼ˆQZOï¼‰ï¼Œé€šè¿‡æ‰°åŠ¨è¿ç»­é‡åŒ–å°ºåº¦æ¥ä¼°è®¡æ¢¯åº¦ï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²ç²¾åº¦çš„æƒ…å†µä¸‹æ˜¾è‘—å‡å°‘å†…å­˜æ¶ˆè€—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12448",
            "title": "SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning",
            "url": "https://huggingface.co/papers/2505.12448",
            "abstract": "Despite impressive advancements in Visual-Language Models (VLMs) for multi-modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues, such as point clouds or depth, either require specialized sensors or fail to effectively exploit depth information for higher-order reasoning. To this end, we propose a novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that transforms raw depth data into structured, interpretable textual rationales. These textual rationales serve as meaningful intermediate representations to significantly enhance spatial reasoning capabilities. Additionally, we leverage knowledge distillation to compress the generated rationales into compact latent embeddings, which facilitate resource-efficient and plug-and-play integration into existing VLMs without retraining. To enable comprehensive evaluation, we introduce a new dataset named SSR-CoT, a million-scale visual-language reasoning dataset enriched with intermediate spatial reasoning annotations, and present SSRBench, a comprehensive multi-task benchmark. Extensive experiments on multiple benchmarks demonstrate SSR substantially improves depth utilization and enhances spatial reasoning, thereby advancing VLMs toward more human-like multi-modal understanding. Our project page is at https://yliu-cs.github.io/SSR.",
            "score": 7,
            "issue_id": 3869,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "18ffd5153e838d86",
            "authors": [
                "Yang Liu",
                "Ming Ma",
                "Xiaomin Yu",
                "Pengxiang Ding",
                "Han Zhao",
                "Mingyang Sun",
                "Siteng Huang",
                "Donglin Wang"
            ],
            "affiliations": [
                "Alibaba DAMO Academy",
                "Harbin Institute of Technology",
                "Shanghai Innovation Institute",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12448.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#cv",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SSR (Spatial Sense and Reasoning) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM). SSR Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‚ÑÑ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SSR-CoT Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SSRBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SSR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing Spatial Reasoning in VLMs with SSR",
                    "desc": "This paper introduces a new method called Spatial Sense and Reasoning (SSR) to improve how Visual-Language Models (VLMs) understand spatial information. SSR transforms raw depth data into structured textual rationales, which help the model reason about space more effectively. The authors also use knowledge distillation to create compact representations of these rationales, allowing for easy integration into existing VLMs without needing to retrain them. To support their research, they present a new dataset, SSR-CoT, and a benchmark, SSRBench, which show that SSR significantly enhances spatial reasoning in VLMs."
                },
                "zh": {
                    "title": "æå‡ç©ºé—´æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¯¹RGBè¾“å…¥çš„ä¾èµ–é™åˆ¶äº†ç²¾ç¡®çš„ç©ºé—´ç†è§£ã€‚ç°æœ‰çš„æ–¹æ³•åœ¨æ•´åˆç©ºé—´çº¿ç´¢æ—¶ï¼Œå¾€å¾€éœ€è¦ä¸“ç”¨ä¼ æ„Ÿå™¨æˆ–æ— æ³•æœ‰æ•ˆåˆ©ç”¨æ·±åº¦ä¿¡æ¯è¿›è¡Œæ›´é«˜é˜¶çš„æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç©ºé—´æ„ŸçŸ¥ä¸æ¨ç†æ–¹æ³•ï¼ˆSSRï¼‰ï¼Œè¯¥æ¡†æ¶å°†åŸå§‹æ·±åº¦æ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–çš„å¯è§£é‡Šæ–‡æœ¬æ¨ç†ã€‚è¿™äº›æ–‡æœ¬æ¨ç†ä½œä¸ºæœ‰æ„ä¹‰çš„ä¸­é—´è¡¨ç¤ºï¼Œæ˜¾è‘—å¢å¼ºäº†ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡çŸ¥è¯†è’¸é¦å°†ç”Ÿæˆçš„æ¨ç†å‹ç¼©ä¸ºç´§å‡‘çš„æ½œåœ¨åµŒå…¥ï¼Œä¾¿äºä¸ç°æœ‰VLMsçš„é«˜æ•ˆé›†æˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14681",
            "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training",
            "url": "https://huggingface.co/papers/2505.14681",
            "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.",
            "score": 6,
            "issue_id": 3878,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "89ee9aa82837601e",
            "authors": [
                "Mengru Wang",
                "Xingyu Chen",
                "Yue Wang",
                "Zhiwei He",
                "Jiahao Xu",
                "Tian Liang",
                "Qiuzhi Liu",
                "Yunzhi Yao",
                "Wenxuan Wang",
                "Ruotian Ma",
                "Haitao Mi",
                "Ningyu Zhang",
                "Zhaopeng Tu",
                "Xiaolong Li",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14681.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#architecture",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RICE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ (nPMI), Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 'ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹', Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ğ¼ĞµÑ‚Ğ°-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° MoE Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ RICE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing Reasoning Efficiency with Cognitive Experts",
                    "desc": "This paper presents a new method called Reinforcing Cognitive Experts (RICE) to improve the reasoning capabilities of Mixture-of-Experts (MoE) architectures in Large Reasoning Models (LRMs). RICE addresses issues of cognitive inefficiencies, such as overthinking and underthinking, by selectively activating specialized experts during inference. The method uses normalized Pointwise Mutual Information (nPMI) to identify these 'cognitive experts' that enhance meta-level reasoning processes. Empirical results show that RICE significantly boosts reasoning accuracy and efficiency compared to existing techniques, while maintaining the model's ability to follow instructions effectively."
                },
                "zh": {
                    "title": "å¼ºåŒ–è®¤çŸ¥ä¸“å®¶ï¼šæå‡æ¨ç†æ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„åœ¨å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä¸­é€šè¿‡é€‰æ‹©æ€§æ¿€æ´»ä¸“å®¶æ¥å®ç°å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨ç†æ¨¡å‹å¸¸å¸¸é¢ä¸´è®¤çŸ¥æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¦‚è¿‡åº¦æ€è€ƒå’Œä¸è¶³æ€è€ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ—¶é—´å¼•å¯¼æ–¹æ³•ï¼Œç§°ä¸ºå¼ºåŒ–è®¤çŸ¥ä¸“å®¶ï¼ˆRICEï¼‰ï¼Œæ—¨åœ¨åœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆ–å¤æ‚å¯å‘å¼çš„æƒ…å†µä¸‹æé«˜æ¨ç†æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨å½’ä¸€åŒ–çš„ç‚¹äº’ä¿¡æ¯ï¼ˆnPMIï¼‰ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯†åˆ«å‡ºä¸“é—¨çš„ä¸“å®¶ï¼Œç§°ä¸ºâ€œè®¤çŸ¥ä¸“å®¶â€ï¼Œä»¥åè°ƒä»¥â€œ<think>â€ç­‰æ ‡è®°ä¸ºç‰¹å¾çš„å…ƒçº§æ¨ç†æ“ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14464",
            "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
            "url": "https://huggingface.co/papers/2505.14464",
            "abstract": "Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The AM-based model consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging FaceDatasets are available on Hugging Face: \\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled}, https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.",
            "score": 6,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "709996374c466144",
            "authors": [
                "Xiaoyu Tian",
                "Yunjie Ji",
                "Haotian Wang",
                "Shuaiting Chen",
                "Sitong Zhao",
                "Yiping Peng",
                "Han Zhao",
                "Xiangang Li"
            ],
            "affiliations": [
                "Beike (Ke.com)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14464.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ Ñ‚Ñ€ĞµÑ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 1,89 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ AM-Thinking-v1, Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Reasoning in Language Models through Data Distillation",
                    "desc": "This paper explores the process of distillation to improve the reasoning abilities of open-source language models. The authors conducted a large-scale study using outputs from three advanced teacher models on a dataset of 1.89 million queries. They found that the distilled data from the AM-Thinking-v1 model had better diversity in token length and lower perplexity, leading to superior performance on various reasoning benchmarks. The results indicate that high-quality reasoning data is crucial for training effective student models, and the authors have made their datasets publicly available for further research."
                },
                "zh": {
                    "title": "è’¸é¦æŠ€æœ¯æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†é€šè¿‡è’¸é¦æŠ€æœ¯æå‡å¼€æºè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ”¶é›†äº†æ¥è‡ªä¸‰ç§å…ˆè¿›æ•™å¸ˆæ¨¡å‹çš„éªŒè¯è¾“å‡ºï¼Œå¹¶æ„å»ºäº†ä¸‰ä¸ªå¹³è¡Œæ•°æ®é›†è¿›è¡Œåˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼ŒAM-Thinking-v1è’¸é¦æ•°æ®åœ¨æ ‡è®°é•¿åº¦å¤šæ ·æ€§å’Œå›°æƒ‘åº¦æ–¹é¢è¡¨ç°æ›´ä½³ã€‚ç»è¿‡è®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯AM-Thinking-v1æ¨¡å‹åœ¨å„é¡¹æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€ä½³æˆç»©ï¼Œå±•ç¤ºäº†é«˜è´¨é‡æ¨ç†è½¨è¿¹çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14534",
            "title": "Lessons from Defending Gemini Against Indirect Prompt Injections",
            "url": "https://huggingface.co/papers/2505.14534",
            "abstract": "Gemini is increasingly used to perform tasks on behalf of users, where function-calling and tool-use capabilities enable the model to access user data. Some tools, however, require access to untrusted data introducing risk. Adversaries can embed malicious instructions in untrusted data which cause the model to deviate from the user's expectations and mishandle their data or permissions. In this report, we set out Google DeepMind's approach to evaluating the adversarial robustness of Gemini models and describe the main lessons learned from the process. We test how Gemini performs against a sophisticated adversary through an adversarial evaluation framework, which deploys a suite of adaptive attack techniques to run continuously against past, current, and future versions of Gemini. We describe how these ongoing evaluations directly help make Gemini more resilient against manipulation.",
            "score": 5,
            "issue_id": 3874,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "1e855b0dd2463fec",
            "authors": [
                "Chongyang Shi",
                "Sharon Lin",
                "Shuang Song",
                "Jamie Hayes",
                "Ilia Shumailov",
                "Itay Yona",
                "Juliette Pluto",
                "Aneesh Pappu",
                "Christopher A. Choquette-Choo",
                "Milad Nasr",
                "Chawin Sitawarin",
                "Gena Gibson",
                "Andreas Terzis",
                "John \"Four\" Flynn"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14534.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#security",
                    "#benchmark"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Gemini Ğ¾Ñ‚ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Google DeepMind Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemini Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ñ‚Ğ°Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ‚ Ğ¸Ğ¼ĞµĞ½Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Gemini Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼ ÑĞ¾ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Strengthening Gemini: Safeguarding User Data Against Adversarial Attacks",
                    "desc": "The paper discusses the challenges of ensuring the security of Gemini, a machine learning model that performs tasks for users by accessing their data. It highlights the risks posed by untrusted data, which can contain malicious instructions that lead the model to behave unexpectedly. To address these risks, Google DeepMind has developed an adversarial evaluation framework that tests Gemini's robustness against sophisticated attacks. The ongoing evaluations aim to enhance Gemini's resilience, ensuring it handles user data and permissions safely and effectively."
                },
                "zh": {
                    "title": "æå‡Geminiæ¨¡å‹çš„å¯¹æŠ—æ€§é²æ£’æ€§",
                    "desc": "Geminiæ¨¡å‹è¢«å¹¿æ³›ç”¨äºæ‰§è¡Œç”¨æˆ·ä»»åŠ¡ï¼Œä½†åœ¨ä½¿ç”¨å·¥å…·æ—¶å¯èƒ½ä¼šæ¥è§¦åˆ°ä¸å¯ä¿¡çš„æ•°æ®ï¼Œè¿™å¸¦æ¥äº†é£é™©ã€‚æ¶æ„æ”»å‡»è€…å¯ä»¥åœ¨ä¸å¯ä¿¡çš„æ•°æ®ä¸­åµŒå…¥æ¶æ„æŒ‡ä»¤ï¼Œå¯¼è‡´æ¨¡å‹åç¦»ç”¨æˆ·çš„æœŸæœ›ï¼Œé”™è¯¯å¤„ç†ç”¨æˆ·çš„æ•°æ®æˆ–æƒé™ã€‚æœ¬æ–‡ä»‹ç»äº†Google DeepMindè¯„ä¼°Geminiæ¨¡å‹å¯¹æŠ—æ€§é²æ£’æ€§çš„æ–¹æ³•ï¼Œå¹¶æ€»ç»“äº†åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­è·å¾—çš„ä¸»è¦ç»éªŒæ•™è®­ã€‚é€šè¿‡å¯¹Geminiè¿›è¡ŒæŒç»­çš„å¯¹æŠ—æ€§è¯„ä¼°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæé«˜å…¶æŠµå¾¡æ“æ§çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14352",
            "title": "Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability",
            "url": "https://huggingface.co/papers/2505.14352",
            "abstract": "As language models become more powerful and sophisticated, it is crucial that they remain trustworthy and reliable. There is concerning preliminary evidence that models may attempt to deceive or keep secrets from their operators. To explore the ability of current techniques to elicit such hidden knowledge, we train a Taboo model: a language model that describes a specific secret word without explicitly stating it. Importantly, the secret word is not presented to the model in its training data or prompt. We then investigate methods to uncover this secret. First, we evaluate non-interpretability (black-box) approaches. Subsequently, we develop largely automated strategies based on mechanistic interpretability techniques, including logit lens and sparse autoencoders. Evaluation shows that both approaches are effective in eliciting the secret word in our proof-of-concept setting. Our findings highlight the promise of these approaches for eliciting hidden knowledge and suggest several promising avenues for future work, including testing and refining these methods on more complex model organisms. This work aims to be a step towards addressing the crucial problem of eliciting secret knowledge from language models, thereby contributing to their safe and reliable deployment.",
            "score": 5,
            "issue_id": 3874,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "6b06f2f5351e8b60",
            "authors": [
                "Bartosz CywiÅ„ski",
                "Emil Ryd",
                "Senthooran Rajamanoharan",
                "Neel Nanda"
            ],
            "affiliations": [
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14352.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#inference",
                    "#training",
                    "#security",
                    "#interpretability"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞµĞºÑ€ĞµÑ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Taboo, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾, Ğ½Ğµ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°Ñ ĞµĞ³Ğ¾ ÑĞ²Ğ½Ğ¾. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ logit lens Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹, Ğ´Ğ»Ñ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞµĞºÑ€ĞµÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unveiling Secrets: Enhancing Trust in Language Models",
                    "desc": "This paper addresses the challenge of ensuring that powerful language models remain trustworthy by investigating their potential to conceal information. The authors introduce a Taboo model, which is designed to describe a secret word without revealing it directly, even though the word is not included in the training data. They evaluate two main approaches to uncover this hidden knowledge: black-box methods and mechanistic interpretability techniques, such as logit lens and sparse autoencoders. The results demonstrate that these methods can effectively elicit the secret word, paving the way for future research on improving the transparency and reliability of language models."
                },
                "zh": {
                    "title": "æ­ç¤ºè¯­è¨€æ¨¡å‹ä¸­çš„éšè—çŸ¥è¯†",
                    "desc": "éšç€è¯­è¨€æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¼ºå¤§å’Œå¤æ‚ï¼Œç¡®ä¿å®ƒä»¬çš„å¯ä¿¡æ€§å’Œå¯é æ€§å˜å¾—è‡³å…³é‡è¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¯•å›¾æ¬ºéª—æˆ–éšç’ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç¦å¿Œæ¨¡å‹ï¼Œå®ƒåœ¨ä¸ç›´æ¥è¯´æ˜ç‰¹å®šç§˜å¯†è¯çš„æƒ…å†µä¸‹è¿›è¡Œæè¿°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨é»‘ç®±æ–¹æ³•å’Œæœºæ¢°è§£é‡ŠæŠ€æœ¯å¯ä»¥æœ‰æ•ˆåœ°æ­ç¤ºè¿™äº›éšè—çš„çŸ¥è¯†ï¼Œæ¨åŠ¨æœªæ¥åœ¨æ›´å¤æ‚æ¨¡å‹ä¸Šçš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13718",
            "title": "Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings",
            "url": "https://huggingface.co/papers/2505.13718",
            "abstract": "Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we \"warm up\" the model by distilling Long CoTs from a toy domain, namely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: (i) the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both the base model and the warmed-up model are RLVR trained on the same small dataset (leq100 examples), the warmed-up model consistently outperforms the base model; (iii) Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; (iv) Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.",
            "score": 5,
            "issue_id": 3872,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "29613228991289b5",
            "authors": [
                "Safal Shrestha",
                "Minwu Kim",
                "Aadim Nepal",
                "Anubhav Shrestha",
                "Keith Ross"
            ],
            "affiliations": [
                "New York University Abu Dhabi"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13718.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#rl",
                    "#long_context",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 'Ñ€Ğ°Ğ·Ğ¾Ğ³Ñ€ĞµĞ²Ğ°ĞµÑ‚ÑÑ' Ğ½Ğ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ°Ñ…, Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ RLVR. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… LLM Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Warmup for Robust Reasoning in Data-Scarce LLMs",
                    "desc": "This paper presents a novel two-stage training strategy for developing reasoning-capable large language models (LLMs) when high-quality training data is limited. The first stage involves warming up the model by distilling Long Chains of Thought (CoT) from simple logic puzzles, which helps the model acquire general reasoning skills. In the second stage, Reinforcement Learning with Verifiable Rewards (RLVR) is applied using a small set of examples from the target domain. The results show that this approach enhances performance across various tasks and improves sample efficiency, demonstrating the effectiveness of the warmup phase in building robust reasoning LLMs."
                },
                "zh": {
                    "title": "åœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­æ„å»ºå¼ºå¤§æ¨ç†æ¨¡å‹çš„æœ‰æ•ˆç­–ç•¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨æœ‰é™ç›‘ç£ä¸‹å¼€å‘æ¨ç†èƒ½åŠ›å¼ºçš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ä»ç®€å•çš„é€»è¾‘è°œé¢˜ï¼ˆéª‘å£«ä¸éª—å­ï¼‰ä¸­æå–é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥â€œé¢„çƒ­â€æ¨¡å‹ï¼Œä»¥è·å–ä¸€èˆ¬æ¨ç†æŠ€èƒ½ã€‚ç¬¬äºŒé˜¶æ®µåˆ™ä½¿ç”¨æœ‰é™çš„ç›®æ ‡é¢†åŸŸç¤ºä¾‹å¯¹é¢„çƒ­åçš„æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ä¸¤é˜¶æ®µçš„æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸­èƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12182",
            "title": "Truth Neurons",
            "url": "https://huggingface.co/papers/2505.12182",
            "abstract": "Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability.",
            "score": 5,
            "issue_id": 3868,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "ddeab64450bb26a9",
            "authors": [
                "Haohang Li",
                "Yupeng Cao",
                "Yangyang Yu",
                "Jordan W. Suchow",
                "Zining Zhu"
            ],
            "affiliations": [
                "Stevens Institute of Technology",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12182.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#hallucinations",
                    "#alignment",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ‚Ğ°Ğº Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ 'Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞŸĞ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unveiling Truth Neurons: Enhancing Language Model Trustworthiness",
                    "desc": "This paper investigates how language models encode truthfulness at the neuron level, revealing the presence of 'truth neurons' that represent truthfulness in a way that is not dependent on specific subjects. The authors demonstrate that these truth neurons exist across various models, indicating a shared property among them. By analyzing the distribution of truth neurons across different layers, the study aligns with previous research on the geometry of truthfulness. Additionally, the suppression of these neurons negatively impacts model performance, suggesting that understanding and improving truthfulness in language models is crucial for their reliability."
                },
                "zh": {
                    "title": "æ­ç¤ºè¯­è¨€æ¨¡å‹ä¸­çš„çœŸç›¸ç¥ç»å…ƒ",
                    "desc": "å°½ç®¡è¯­è¨€æ¨¡å‹åœ¨å„ç§å·¥ä½œæµç¨‹ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†æœ‰æ—¶ä¼šäº§ç”Ÿä¸çœŸå®çš„å›ç­”ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ¨¡å‹ä¸­çœŸç›¸ç¼–ç æœºåˆ¶çš„ç†è§£æœ‰é™ï¼Œè¿™å½±å“äº†å®ƒä»¬çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»å…ƒå±‚é¢è¯†åˆ«çœŸç›¸çš„è¡¨ç¤ºï¼Œå‘ç°è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨ç¼–ç çœŸç›¸çš„çœŸç›¸ç¥ç»å…ƒã€‚å®éªŒè¡¨æ˜ï¼ŒçœŸç›¸ç¥ç»å…ƒçš„å­˜åœ¨æ˜¯è®¸å¤šè¯­è¨€æ¨¡å‹çš„å…±åŒç‰¹æ€§ï¼Œå¹¶ä¸”å…¶åˆ†å¸ƒæ¨¡å¼ä¸çœŸç›¸çš„å‡ ä½•ç‰¹å¾ä¸€è‡´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09569",
            "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
            "url": "https://huggingface.co/papers/2505.09569",
            "abstract": "With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on problem-solving and issue-resolution tasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a distinct focus: code migration. MIGRATION-BENCH aims to serve as a comprehensive benchmark for migration from Java 8 to the latest long-term support (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset and its subset selected with 5,102 and 300 repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java 17. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: https://huggingface.co/collections/AmazonScience and https://github.com/amazon-science/self_debug respectively.",
            "score": 5,
            "issue_id": 3872,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "cb7832fb680cc056",
            "authors": [
                "Linbo Liu",
                "Xinle Liu",
                "Qiang Zhou",
                "Lin Chen",
                "Yihan Liu",
                "Hoan Nguyen",
                "Behrooz Omidvar-Tehrani",
                "Xi Shen",
                "Jun Huan",
                "Omer Tripp",
                "Anoop Deoras"
            ],
            "affiliations": [
                "AWS AI Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09569.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "MIGRATION-BENCH: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MIGRATION-BENCH Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Java 8 Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 5,102 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· 300 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ², Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SD-Feedback, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. Ğ¡ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Claude-3.5-Sonnet-v2 Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° SD-Feedback Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ² 62.33% Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ 27.00% Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Code Migration with MIGRATION-BENCH",
                    "desc": "This paper introduces MIGRATION-BENCH, a new benchmark specifically designed for evaluating large language models (LLMs) on the task of code migration from Java 8 to newer long-term support versions like Java 17 and 21. Unlike existing benchmarks that focus on problem-solving, MIGRATION-BENCH provides a comprehensive dataset of 5,102 repositories, with a curated subset of 300 that vary in complexity and difficulty. The authors also present an evaluation framework to standardize the assessment of LLMs in this domain, demonstrating that LLMs can effectively perform repository-level code migration. Using their proposed SD-Feedback method, they report success rates of 62.33% for minimal migration and 27.00% for maximal migration with the Claude-3.5-Sonnet-v2 model."
                },
                "zh": {
                    "title": "ä»£ç è¿ç§»çš„æ–°åŸºå‡†ï¼šMIGRATION-BENCH",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿…é€Ÿå‘å±•ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿäº§åŠ›å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›æ¨¡å‹çš„ç¼–ç èƒ½åŠ›ï¼Œå¼€å‘äº†è®¸å¤šåŸºå‡†æ•°æ®é›†ï¼Œä½†å¤§å¤šé›†ä¸­åœ¨é—®é¢˜è§£å†³å’Œæ•…éšœæ’é™¤ä»»åŠ¡ä¸Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ç¼–ç åŸºå‡†MIGRATION-BENCHï¼Œä¸“æ³¨äºä»£ç è¿ç§»ï¼Œç‰¹åˆ«æ˜¯ä»Java 8è¿ç§»åˆ°æœ€æ–°çš„é•¿æœŸæ”¯æŒç‰ˆæœ¬ï¼ˆJava 17ã€21ï¼‰ã€‚è¯¥åŸºå‡†åŒ…å«å®Œæ•´æ•°æ®é›†å’Œä»£è¡¨æ€§å­é›†ï¼Œæä¾›äº†ä¸€ä¸ªå¤šåŠŸèƒ½èµ„æºï¼Œä»¥æ”¯æŒä»£ç è¿ç§»é¢†åŸŸçš„ç ”ç©¶ï¼Œå¹¶æä¾›äº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¾¿å¯¹LLMsåœ¨è¿™ä¸€æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šçš„è¡¨ç°è¿›è¡Œä¸¥æ ¼å’Œæ ‡å‡†åŒ–çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13103",
            "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
            "url": "https://huggingface.co/papers/2505.13103",
            "abstract": "The rapid advancement of bug-finding techniques has led to the discovery of more vulnerabilities than developers can reasonably fix, creating an urgent need for effective Automated Program Repair (APR) methods. However, the complexity of modern bugs often makes precise root cause analysis difficult and unreliable. To address this challenge, we propose crash-site repair to simplify the repair task while still mitigating the risk of exploitation. In addition, we introduce a template-guided patch generation approach that significantly reduces the token cost of Large Language Models (LLMs) while maintaining both efficiency and effectiveness.   We implement our prototype system, WILLIAMT, and evaluate it against state-of-the-art APR tools. Our results show that, when combined with the top-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and increases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open source software vulnerabilities benchmark. Furthermore, we demonstrate that WILLIAMT can function effectively even without access to frontier LLMs: even a local model running on a Mac M4 Mini achieves a reasonable repair rate. These findings highlight the broad applicability and scalability of WILLIAMT.",
            "score": 4,
            "issue_id": 3874,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "4d2278523a88b9bc",
            "authors": [
                "Han Zheng",
                "Ilia Shumailov",
                "Tianqi Fan",
                "Aiden Hall",
                "Mathias Payer"
            ],
            "affiliations": [
                "EPFL Lausanne, Switzerland",
                "Google DeepMind London, UK",
                "Google New York, USA",
                "Google Zurich, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13103.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#plp",
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "WILLIAMT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ (APR) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ WILLIAMT. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµÑÑ‚Ğ° ÑĞ±Ğ¾Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). WILLIAMT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 29.6% Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ APR Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ARVO. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ WILLIAMT Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Simplifying Bug Fixes with WILLIAMT: Efficient Automated Program Repair",
                    "desc": "This paper addresses the challenge of fixing software bugs, which have become too numerous for developers to handle manually. It introduces a method called crash-site repair that simplifies the bug-fixing process while reducing the risk of security issues. The authors also present a template-guided patch generation technique that lowers the token usage of Large Language Models (LLMs), making the repair process more efficient. Their system, WILLIAMT, shows significant improvements in bug-fixing rates and can operate effectively even on less powerful hardware."
                },
                "zh": {
                    "title": "WILLIAMTï¼šé«˜æ•ˆçš„è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤æ–°æ–¹æ³•",
                    "desc": "éšç€æ¼æ´å‘ç°æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¼€å‘è€…é¢ä¸´ç€è¶Šæ¥è¶Šå¤šçš„æ¼æ´ä¿®å¤éœ€æ±‚ï¼Œå› æ­¤è¿«åˆ‡éœ€è¦æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ–¹æ³•ã€‚ç°ä»£æ¼æ´çš„å¤æ‚æ€§ä½¿å¾—ç²¾ç¡®çš„æ ¹æœ¬åŸå› åˆ†æå˜å¾—å›°éš¾ä¸”ä¸å¯é ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å´©æºƒç°åœºä¿®å¤æ–¹æ³•ï¼Œä»¥ç®€åŒ–ä¿®å¤ä»»åŠ¡ï¼ŒåŒæ—¶é™ä½è¢«åˆ©ç”¨çš„é£é™©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¨¡æ¿å¼•å¯¼çš„è¡¥ä¸ç”Ÿæˆæ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»¤ç‰Œæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11365",
            "title": "Phare: A Safety Probe for Large Language Models",
            "url": "https://huggingface.co/papers/2505.11365",
            "abstract": "Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems.",
            "score": 4,
            "issue_id": 3873,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "8ab32377d956578e",
            "authors": [
                "Pierre Le Jeune",
                "BenoÃ®t MalÃ©zieux",
                "Weixuan Xiao",
                "Matteo Dora"
            ],
            "affiliations": [
                "Giskard AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11365.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual",
                    "#ethics",
                    "#hallucinations",
                    "#alignment"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Phare - Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸: Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 17 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ³Ğ¾Ğ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ². Phare Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ°ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Phare: Probing Safety in Language Models Beyond Performance",
                    "desc": "This paper presents Phare, a new framework designed to evaluate the safety of large language models (LLMs) by focusing on their failure modes rather than just their performance. It assesses LLMs across three key areas: hallucination and reliability, social biases, and harmful content generation. The study analyzes 17 advanced LLMs and uncovers common vulnerabilities, such as sycophancy and prompt sensitivity, which can lead to biased or harmful outputs. By identifying these specific issues, Phare aims to help researchers and developers create more reliable and ethical language models."
                },
                "zh": {
                    "title": "æ„å»ºæ›´å®‰å…¨çš„è¯­è¨€æ¨¡å‹ï¼Œè¯†åˆ«å¤±è´¥æ¨¡å¼ï¼",
                    "desc": "ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨æ€§å¯¹äºè´Ÿè´£ä»»çš„éƒ¨ç½²è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„è¯„ä¼°å¾€å¾€æ›´å…³æ³¨æ€§èƒ½è€Œéè¯†åˆ«å¤±è´¥æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†Phareï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€è¯Šæ–­æ¡†æ¶ï¼Œç”¨äºæ¢æµ‹å’Œè¯„ä¼°LLMåœ¨ä¸‰ä¸ªå…³é”®ç»´åº¦ä¸Šçš„è¡Œä¸ºï¼šå¹»è§‰å’Œå¯é æ€§ã€ç¤¾ä¼šåè§ä»¥åŠæœ‰å®³å†…å®¹ç”Ÿæˆã€‚å¯¹17ä¸ªæœ€å…ˆè¿›çš„LLMçš„è¯„ä¼°æ­ç¤ºäº†åœ¨æ‰€æœ‰å®‰å…¨ç»´åº¦ä¸Šç³»ç»Ÿæ€§è„†å¼±æ€§çš„æ¨¡å¼ï¼ŒåŒ…æ‹¬è°„åªšã€æç¤ºæ•æ„Ÿæ€§å’Œåˆ»æ¿å°è±¡å†ç°ã€‚é€šè¿‡çªå‡ºè¿™äº›å…·ä½“çš„å¤±è´¥æ¨¡å¼ï¼ŒPhareä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†å¯æ“ä½œçš„è§è§£ï¼Œä»¥æ„å»ºæ›´å¼ºå¤§ã€æ›´ä¸€è‡´å’Œæ›´å¯ä¿¡çš„è¯­è¨€ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13380",
            "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
            "url": "https://huggingface.co/papers/2505.13380",
            "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526",
            "score": 3,
            "issue_id": 3868,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "6a5e70a76e6f012c",
            "authors": [
                "Nam V. Nguyen",
                "Huy Nguyen",
                "Quang Pham",
                "Van Nguyen",
                "Savitha Ramasamy",
                "Nhat Ho"
            ],
            "affiliations": [
                "FPT Software AI Center",
                "Independent Researcher",
                "Institute for Infocomm Research, ASTAR",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13380.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼ĞµÑÑÑ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (SMoE) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'competition'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ softmax. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ CompeteSMoE Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ CompeteSMoE Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ SMoE."
                },
                "en": {
                    "title": "CompeteSMoE: Efficient Routing for Powerful Language Models",
                    "desc": "Sparse mixture of experts (SMoE) is a method that allows models to become more complex without simply making them deeper or wider. The challenge with SMoE is that the way experts are chosen to process data can be inefficient, as not all experts contribute to the decision-making process. This paper introduces a new routing mechanism called competition, which directs data to the most responsive experts, improving the efficiency of the model. The authors present CompeteSMoE, an algorithm that uses this competition mechanism to train large language models effectively, showing better performance and lower training costs compared to existing methods."
                },
                "zh": {
                    "title": "ç«äº‰æœºåˆ¶æå‡ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹çš„æ•ˆç‡",
                    "desc": "ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆSMoEï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆæå‡æ¨¡å‹å¤æ‚åº¦çš„æ–¹æ³•ï¼Œè¶…è¶Šäº†ç®€å•å¢åŠ ç½‘ç»œæ·±åº¦æˆ–å®½åº¦çš„æ–¹å¼ã€‚ç„¶è€Œï¼ŒSMoEçš„è®­ç»ƒä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºè®¡ç®—çš„ä¸“å®¶ä¸è·¯ç”±è¿‡ç¨‹ä¹‹é—´çš„è”ç³»ä¸å¤Ÿç›´æ¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶â€”â€”ç«äº‰ï¼Œèƒ½å¤Ÿå°†è¾“å…¥æ•°æ®æ›´æœ‰æ•ˆåœ°è·¯ç”±åˆ°å“åº”æœ€å¼ºçš„ä¸“å®¶ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†ç«äº‰æœºåˆ¶åœ¨æ ·æœ¬æ•ˆç‡ä¸Šä¼˜äºä¼ ç»Ÿçš„softmaxè·¯ç”±ï¼Œå¹¶å¼€å‘äº†CompeteSMoEç®—æ³•ï¼Œèƒ½å¤Ÿä»¥è¾ƒä½çš„è®­ç»ƒå¼€é”€å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12306",
            "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection",
            "url": "https://huggingface.co/papers/2505.12306",
            "abstract": "Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia's \"Did You Know...\" entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.",
            "score": 3,
            "issue_id": 3868,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "ccbad06f5ba35418",
            "authors": [
                "Yuwei Zhang",
                "Wenhao Yu",
                "Shangbin Feng",
                "Yifan Zhu",
                "Letian Peng",
                "Jayanth Srinivasa",
                "Gaowen Liu",
                "Jingbo Shang"
            ],
            "affiliations": [
                "Cisco",
                "Tencent AI Lab",
                "UC, San Diego",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12306.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#interpretability",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "WikiDYK: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WikiDYK Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. WikiDYK Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ° Wikipedia 'Did You Know...', Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¸Ñ… Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (BiLM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ‡ĞµĞ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (CLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸ BiLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Knowledge Memorization in Language Models with WikiDYK",
                    "desc": "This paper presents WikiDYK, a new benchmark for evaluating knowledge memorization in large language models (LLMs). It uses real-world facts from Wikipedia's 'Did You Know...' entries to create a diverse set of question-answer pairs. The study finds that Causal Language Models (CLMs) have weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), with a notable accuracy gap. To enhance BiLMs' performance, the authors propose a collaborative framework that combines multiple BiLMs as external knowledge sources, resulting in improved accuracy in knowledge retrieval tasks."
                },
                "zh": {
                    "title": "çŸ¥è¯†è®°å¿†èƒ½åŠ›çš„æ–°åŸºå‡†ï¼šWikiDYK",
                    "desc": "å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬çš„çŸ¥è¯†è®°å¿†èƒ½åŠ›ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ã€çœŸå®ä¸–ç•Œçš„å¤§è§„æ¨¡çŸ¥è¯†æ³¨å…¥åŸºå‡†ï¼Œåä¸ºWikiDYKï¼Œèƒ½å¤Ÿéšç€æ—¶é—´çš„æ¨ç§»ä¸æ–­æ¼”å˜ï¼Œè€Œæ— éœ€äººå·¥å¹²é¢„ã€‚WikiDYKåˆ©ç”¨ç»´åŸºç™¾ç§‘â€œä½ çŸ¥é“å—...â€æ¡ç›®ä¸­æœ€è¿‘æ·»åŠ çš„ã€ç”±äººç±»æ’°å†™çš„äº‹å®ï¼Œç»è¿‡ä¸“å®¶ç¼–è¾‘çš„ä¸¥æ ¼ç­›é€‰ï¼Œç¡®ä¿å…¶å¯éªŒè¯æ€§å’Œæ¸…æ™°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å› æœè¯­è¨€æ¨¡å‹ï¼ˆCLMsï¼‰åœ¨ç°ä»£LLMsä¸­æ™®éå­˜åœ¨ï¼Œä½†å…¶çŸ¥è¯†è®°å¿†èƒ½åŠ›æ˜¾è‘—ä½äºåŒå‘è¯­è¨€æ¨¡å‹ï¼ˆBiLMsï¼‰ï¼Œå‡†ç¡®æ€§ä½23%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11966",
            "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier",
            "url": "https://huggingface.co/papers/2505.11966",
            "abstract": "Large Language Model (LLM) reasoning for complex tasks inherently involves a trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe, a novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using a Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe, proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers a scalable and effective solution to enhance LLM reasoning at test time.",
            "score": 3,
            "issue_id": 3869,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "cd659e075a3efafa",
            "authors": [
                "Jianyuan Zhong",
                "Zeju Li",
                "Zhijian Xu",
                "Xiangyu Wen",
                "Kezhi Li",
                "Qiang Xu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11966.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#math",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlexiVe - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). FlexiVe Ğ³Ğ¸Ğ±ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Solve-Detect-Verify Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FlexiVe Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Balancing Speed and Accuracy in LLM Reasoning with FlexiVe",
                    "desc": "This paper discusses the challenges of using Large Language Models (LLMs) for complex tasks, particularly the balance between accuracy and computational efficiency. It introduces FlexiVe, a generative verifier that optimizes the use of computational resources by allowing for both quick and thorough reasoning processes. The authors propose a Solve-Detect-Verify pipeline that enhances the integration of FlexiVe, enabling targeted verification and improved feedback during inference. Experimental results demonstrate that FlexiVe significantly improves error detection and reasoning accuracy on various benchmarks compared to traditional methods."
                },
                "zh": {
                    "title": "çµæ´»éªŒè¯ï¼Œæå‡æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡æ¨ç†ä¸­çš„å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”ŸæˆéªŒè¯å™¨FlexiVeï¼Œå®ƒé€šè¿‡çµæ´»åˆ†é…éªŒè¯é¢„ç®—ï¼Œåœ¨å¿«é€Ÿå¯é çš„æ€ç»´ä¸ç»†è‡´æ…¢æ€ç»´ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Solve-Detect-Verifyç®¡é“ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨ç†æ—¶é—´æ‰©å±•æ¡†æ¶ï¼Œèƒ½å¤Ÿæ™ºèƒ½æ•´åˆFlexiVeï¼Œä¸»åŠ¨è¯†åˆ«è§£å†³æ–¹æ¡ˆå®Œæˆç‚¹ä»¥è§¦å‘é’ˆå¯¹æ€§çš„éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlexiVeåœ¨ProcessBenchä¸Šèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å®šä½æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå¹¶åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14178",
            "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
            "url": "https://huggingface.co/papers/2505.14178",
            "abstract": "Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.",
            "score": 2,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "f4fdc7fb140f9273",
            "authors": [
                "Xiang Zhang",
                "Juntai Cao",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Chenyu You"
            ],
            "affiliations": [
                "Cisco",
                "Stony Brook University",
                "University of British Columbia",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14178.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#architecture",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…' (Token Awareness) Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ°Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Tokenization Matters: Unlocking Reasoning in Language Models",
                    "desc": "This paper explores the importance of tokenization in language models, particularly how it affects reasoning capabilities. It highlights that traditional tokenization methods, like byte-pair encoding (BPE), can obscure essential reasoning units, limiting the model's ability to perform symbolic computation. The authors introduce the concept of Token Awareness, which emphasizes the need for better token granularity to enhance logical alignment and generalization in models. Through experiments on arithmetic and symbolic tasks, they show that models with well-structured token representations can significantly outperform larger models in reasoning tasks."
                },
                "zh": {
                    "title": "åˆ†è¯ç»“æ„å†³å®šæ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œåˆ†è¯ï¼ˆTokenizationï¼‰å¯¹æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œåˆ†è¯æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åŸºäºå­è¯çš„æ–¹æ³•ï¼ˆå¦‚å­—èŠ‚å¯¹ç¼–ç BPEï¼‰ï¼Œä¼šåˆå¹¶æˆ–æ¨¡ç³ŠåŸºæœ¬çš„æ¨ç†å•å…ƒï¼Œä»è€Œå¦¨ç¢ç¬¦å·è®¡ç®—ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œToken Awarenessâ€çš„æ¦‚å¿µï¼Œå¼ºè°ƒäº†åˆ†è¯ç²’åº¦ä¸ä½³å¦‚ä½•å¹²æ‰°é€»è¾‘å¯¹é½ï¼Œé˜»ç¢æ¨¡å‹çš„ç¬¦å·ç¨‹åºæ³›åŒ–ã€‚é€šè¿‡å¯¹ç®—æœ¯å’Œç¬¦å·ä»»åŠ¡çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†åˆ†è¯ç»“æ„æ˜¾è‘—å½±å“æ¨ç†æ€§èƒ½ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨å¯¹é½æ ¼å¼ä¸‹èƒ½å¤Ÿè¶…è¶Šæ›´å¤§çš„ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13010",
            "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector",
            "url": "https://huggingface.co/papers/2505.13010",
            "abstract": "Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.",
            "score": 2,
            "issue_id": 3877,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "9f759ef4e436d2f0",
            "authors": [
                "Himel Ghosh",
                "Ahmed Mosharafa",
                "Georg Groh"
            ],
            "affiliations": [
                "Sapienza University of Rome, Italy",
                "Technical University of Munich (TUM), Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13010.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#interpretability",
                    "#ethics",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¡ĞœĞ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RoBERTa Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ BABE. Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ DA-RoBERTa. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ°Ñ€ÑĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ¼. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ĞµĞ´Ğ¸Ğ°-Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Media Bias Detection with Context-Aware Models",
                    "desc": "This paper addresses the challenge of detecting media bias by fine-tuning a RoBERTa-based model on the BABE dataset, which is annotated by experts. The authors demonstrate significant performance improvements over a baseline model using statistical tests, indicating the effectiveness of their approach. They also analyze the model's attention mechanisms, showing it focuses on contextually relevant information rather than being overly sensitive to biased language. The study proposes a comprehensive pipeline for media bias detection and discusses future directions for enhancing bias classification and model interpretability."
                },
                "zh": {
                    "title": "æå‡åª’ä½“åè§æ£€æµ‹çš„æ™ºèƒ½åŒ–æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶é’ˆå¯¹åª’ä½“åè§æ£€æµ‹è¿™ä¸€é‡è¦ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºRoBERTaæ¨¡å‹çš„å¥å­çº§åè§åˆ†ç±»æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä¸“å®¶æ ‡æ³¨çš„BABEæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶é€šè¿‡ç»Ÿè®¡æµ‹è¯•éªŒè¯äº†æ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†æè¡¨æ˜ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆé¿å…å¯¹æ”¿æ²»æ•æ„Ÿè¯çš„è¿‡åº¦æ•æ„Ÿï¼Œè€Œæ˜¯æ›´å…³æ³¨ä¸Šä¸‹æ–‡ç›¸å…³çš„è¯æ±‡ã€‚å°½ç®¡å—é™äºå¥å­çº§åˆ†æå’Œæ•°æ®é›†è§„æ¨¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åè§æ£€æµ‹ä¸­å±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10176",
            "title": "Incorporating brain-inspired mechanisms for multimodal learning in\n  artificial intelligence",
            "url": "https://huggingface.co/papers/2505.10176",
            "abstract": "Multimodal learning enhances the perceptual capabilities of cognitive systems by integrating information from different sensory modalities. However, existing multimodal fusion research typically assumes static integration, not fully incorporating key dynamic mechanisms found in the brain. Specifically, the brain exhibits an inverse effectiveness phenomenon, wherein weaker unimodal cues yield stronger multisensory integration benefits; conversely, when individual modal cues are stronger, the effect of fusion is diminished. This mechanism enables biological systems to achieve robust cognition even with scarce or noisy perceptual cues. Inspired by this biological mechanism, we explore the relationship between multimodal output and information from individual modalities, proposing an inverse effectiveness driven multimodal fusion (IEMF) strategy. By incorporating this strategy into neural networks, we achieve more efficient integration with improved model performance and computational efficiency, demonstrating up to 50% reduction in computational cost across diverse fusion methods. We conduct experiments on audio-visual classification, continual learning, and question answering tasks to validate our method. Results consistently demonstrate that our method performs excellently in these tasks. To verify universality and generalization, we also conduct experiments on Artificial Neural Networks (ANN) and Spiking Neural Networks (SNN), with results showing good adaptability to both network types. Our research emphasizes the potential of incorporating biologically inspired mechanisms into multimodal networks and provides promising directions for the future development of multimodal artificial intelligence. The code is available at https://github.com/Brain-Cog-Lab/IEMF.",
            "score": 2,
            "issue_id": 3878,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 Ğ¼Ğ°Ñ",
                "en": "May 15",
                "zh": "5æœˆ15æ—¥"
            },
            "hash": "6384b169333ad553",
            "authors": [
                "Xiang He",
                "Dongcheng Zhao",
                "Yang Li",
                "Qingqun Kong",
                "Xin Yang",
                "Yi Zeng"
            ],
            "affiliations": [
                "Brain-inspired Cognitive AI Lab, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Center for Long-term Al, Beijing, China",
                "Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Chinese Academy of Sciences, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10176.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#multimodal",
                    "#audio",
                    "#agi"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¾Ğ·Ğ³Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ IEMF (Inverse Effectiveness driven Multimodal Fusion), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IEMF ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ 50% Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğº Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼ (ANN), Ñ‚Ğ°Ğº Ğ¸ Ğº ÑĞ¿Ğ°Ğ¹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼ (SNN)."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with Brain-Inspired Fusion",
                    "desc": "This paper discusses a new approach to multimodal learning that mimics how the human brain processes information from different senses. It introduces the Inverse Effectiveness Driven Multimodal Fusion (IEMF) strategy, which enhances the integration of sensory data by leveraging the brain's ability to combine weaker signals more effectively. The authors demonstrate that this method can significantly improve the performance and efficiency of neural networks, achieving up to a 50% reduction in computational costs. Experiments across various tasks show that IEMF is adaptable and effective in both Artificial Neural Networks and Spiking Neural Networks, highlighting the benefits of biologically inspired techniques in artificial intelligence."
                },
                "zh": {
                    "title": "é€†æ•ˆåº”é©±åŠ¨çš„å¤šæ¨¡æ€èåˆç­–ç•¥",
                    "desc": "å¤šæ¨¡æ€å­¦ä¹ é€šè¿‡æ•´åˆæ¥è‡ªä¸åŒæ„Ÿå®˜çš„ä¿¡æ¯ï¼Œå¢å¼ºäº†è®¤çŸ¥ç³»ç»Ÿçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€èåˆç ”ç©¶é€šå¸¸å‡è®¾é™æ€æ•´åˆï¼Œæœªèƒ½å……åˆ†è€ƒè™‘å¤§è„‘ä¸­çš„å…³é”®åŠ¨æ€æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œå¤§è„‘è¡¨ç°å‡ºé€†æ•ˆåº”ç°è±¡ï¼Œå³è¾ƒå¼±çš„å•æ¨¡æ€çº¿ç´¢ä¼šå¸¦æ¥æ›´å¼ºçš„å¤šæ„Ÿå®˜èåˆæ•ˆç›Šï¼›ç›¸åï¼Œå½“å•ä¸ªæ¨¡æ€çº¿ç´¢è¾ƒå¼ºæ—¶ï¼Œèåˆæ•ˆæœä¼šå‡å¼±ã€‚å—è¿™ä¸€ç”Ÿç‰©æœºåˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€†æ•ˆåº”çš„å¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14648",
            "title": "Vox-Profile: A Speech Foundation Model Benchmark for Characterizing\n  Diverse Speaker and Speech Traits",
            "url": "https://huggingface.co/papers/2505.14648",
            "abstract": "We introduce Vox-Profile, a comprehensive benchmark to characterize rich speaker and speech traits using speech foundation models. Unlike existing works that focus on a single dimension of speaker traits, Vox-Profile provides holistic and multi-dimensional profiles that reflect both static speaker traits (e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech flow). This benchmark is grounded in speech science and linguistics, developed with domain experts to accurately index speaker and speech characteristics. We report benchmark experiments using over 15 publicly available speech datasets and several widely used speech foundation models that target various static and dynamic speaker and speech properties. In addition to benchmark experiments, we showcase several downstream applications supported by Vox-Profile. First, we show that Vox-Profile can augment existing speech recognition datasets to analyze ASR performance variability. Vox-Profile is also used as a tool to evaluate the performance of speech generation systems. Finally, we assess the quality of our automated profiles through comparison with human evaluation and show convergent validity. Vox-Profile is publicly available at: https://github.com/tiantiaf0627/vox-profile-release.",
            "score": 1,
            "issue_id": 3884,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "f1d0da4a4a22866a",
            "authors": [
                "Tiantian Feng",
                "Jihwan Lee",
                "Anfeng Xu",
                "Yoonjeong Lee",
                "Thanathai Lertpetchpun",
                "Xuan Shi",
                "Helin Wang",
                "Thomas Thebaud",
                "Laureano Moro-Velazquez",
                "Dani Byrd",
                "Najim Dehak",
                "Shrikanth Narayanan"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14648.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Vox-Profile: ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Vox-Profile - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ñ‡ĞµÑ€Ñ‚ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑ‡Ğ¸. ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ°Ğº ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‡ĞµÑ€Ñ‚Ñ‹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ (Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚, Ğ¿Ğ¾Ğ», Ğ°ĞºÑ†ĞµĞ½Ñ‚), Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ñ€ĞµÑ‡Ğ¸ (ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸, Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑ‡Ğ¸). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ°ÑƒĞºĞµ Ğ¾ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºĞµ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ñ€ĞµÑ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 15 Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "Vox-Profile: A Holistic Benchmark for Speaker and Speech Trait Analysis",
                    "desc": "Vox-Profile is a new benchmark designed to analyze various speaker and speech traits using advanced speech foundation models. It goes beyond previous studies by providing a comprehensive view of both static traits like age and accent, and dynamic traits such as emotion and speech flow. Developed with input from experts in speech science and linguistics, it offers a reliable way to index these characteristics. The benchmark has been tested with multiple speech datasets and models, demonstrating its utility in improving automatic speech recognition and speech generation systems."
                },
                "zh": {
                    "title": "Vox-Profileï¼šå¤šç»´åº¦è¯´è¯è€…ç‰¹å¾çš„åŸºå‡†",
                    "desc": "Vox-Profileæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºé€šè¿‡è¯­éŸ³åŸºç¡€æ¨¡å‹æ¥è¡¨å¾ä¸°å¯Œçš„è¯´è¯è€…å’Œè¯­éŸ³ç‰¹å¾ã€‚ä¸ç°æœ‰ç ”ç©¶åªå…³æ³¨å•ä¸€ç»´åº¦çš„è¯´è¯è€…ç‰¹å¾ä¸åŒï¼ŒVox-Profileæä¾›äº†åæ˜ é™æ€è¯´è¯è€…ç‰¹å¾ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€å£éŸ³ï¼‰å’ŒåŠ¨æ€è¯­éŸ³å±æ€§ï¼ˆå¦‚æƒ…æ„Ÿã€è¯­é€Ÿï¼‰çš„æ•´ä½“å’Œå¤šç»´æ¡£æ¡ˆã€‚è¯¥åŸºå‡†åŸºäºè¯­éŸ³ç§‘å­¦å’Œè¯­è¨€å­¦ï¼Œä¸é¢†åŸŸä¸“å®¶åˆä½œå¼€å‘ï¼Œä»¥å‡†ç¡®ç´¢å¼•è¯´è¯è€…å’Œè¯­éŸ³ç‰¹å¾ã€‚æˆ‘ä»¬é€šè¿‡è¶…è¿‡15ä¸ªå…¬å¼€çš„è¯­éŸ³æ•°æ®é›†å’Œå¤šç§å¹¿æ³›ä½¿ç”¨çš„è¯­éŸ³åŸºç¡€æ¨¡å‹è¿›è¡ŒåŸºå‡†å®éªŒï¼Œå±•ç¤ºäº†Vox-Profileåœ¨è¯­éŸ³è¯†åˆ«å’Œç”Ÿæˆç³»ç»Ÿè¯„ä¼°ä¸­çš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13778",
            "title": "CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM\n  APIs",
            "url": "https://huggingface.co/papers/2505.13778",
            "abstract": "As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.",
            "score": 1,
            "issue_id": 3884,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "e61730a128c76920",
            "authors": [
                "Guoheng Sun",
                "Ziyao Wang",
                "Bowei Tian",
                "Meng Liu",
                "Zheyu Shen",
                "Shwai He",
                "Yexiao He",
                "Wanghao Ye",
                "Yiting Wang",
                "Ang Li"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13778.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#inference",
                    "#reasoning",
                    "#hallucinations",
                    "#rl",
                    "#security"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "CoIn: Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‡ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoIn - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). CoIn Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ…ĞµÑˆ-Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑÑ„Ğ°Ğ±Ñ€Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CoIn Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ 94.7%."
                },
                "en": {
                    "title": "Ensuring Transparency in LLM Billing with CoIn",
                    "desc": "This paper discusses the challenges of transparency in large language models (LLMs) that use complex reasoning processes, often hidden from users. These models, enhanced through reinforcement learning, can perform better on difficult tasks but may lead to inflated costs due to undisclosed reasoning tokens. The authors introduce CoIn, a framework designed to verify the authenticity and quantity of these hidden tokens, ensuring users are not overcharged. Through experiments, CoIn demonstrates a high success rate in detecting token inflation, promoting fairness and transparency in LLM billing practices."
                },
                "zh": {
                    "title": "æå‡LLMæœåŠ¡é€æ˜åº¦çš„éªŒè¯æ¡†æ¶",
                    "desc": "éšç€åè®­ç»ƒæŠ€æœ¯çš„å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°å¢å¼ºäº†ç»“æ„åŒ–çš„å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ï¼Œè¿™é€šå¸¸é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚è¿™äº›å¢å¼ºæ¨ç†çš„æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºæ ‡å‡†LLMsï¼Œå¹¶ä¸”ç°åœ¨æ”¯æ’‘ç€è®¸å¤šå•†ä¸šLLM APIã€‚ç„¶è€Œï¼Œä¸ºäº†ä¿æŠ¤ä¸“æœ‰è¡Œä¸ºå¹¶å‡å°‘å†—é•¿ï¼Œæä¾›è€…é€šå¸¸åœ¨è¿”å›æœ€ç»ˆç­”æ¡ˆæ—¶éšè—æ¨ç†è¿‡ç¨‹ã€‚è¿™ç§ä¸é€æ˜æ€§å¯¼è‡´äº†é€æ˜åº¦ç¼ºå£ï¼Œç”¨æˆ·ä¸ºä¸å¯è§çš„æ¨ç†ä»¤ç‰Œä»˜è´¹ï¼Œè€Œè¿™äº›ä»¤ç‰Œå¾€å¾€å æ®äº†å¤§éƒ¨åˆ†æˆæœ¬ï¼Œç”¨æˆ·å´æ— æ³•éªŒè¯å…¶çœŸå®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12154",
            "title": "Learning to Highlight Audio by Watching Movies",
            "url": "https://huggingface.co/papers/2505.12154",
            "abstract": "Recent years have seen a significant increase in video content creation and consumption. Crafting engaging content requires the careful curation of both visual and audio elements. While visual cue curation, through techniques like optimal viewpoint selection or post-editing, has been central to media production, its natural counterpart, audio, has not undergone equivalent advancements. This often results in a disconnect between visual and acoustic saliency. To bridge this gap, we introduce a novel task: visually-guided acoustic highlighting, which aims to transform audio to deliver appropriate highlighting effects guided by the accompanying video, ultimately creating a more harmonious audio-visual experience. We propose a flexible, transformer-based multimodal framework to solve this task. To train our model, we also introduce a new dataset -- the muddy mix dataset, leveraging the meticulous audio and video crafting found in movies, which provides a form of free supervision. We develop a pseudo-data generation process to simulate poorly mixed audio, mimicking real-world scenarios through a three-step process -- separation, adjustment, and remixing. Our approach consistently outperforms several baselines in both quantitative and subjective evaluation. We also systematically study the impact of different types of contextual guidance and difficulty levels of the dataset. Our project page is here: https://wikichao.github.io/VisAH/.",
            "score": 1,
            "issue_id": 3882,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "8f03af3997e1149d",
            "authors": [
                "Chao Huang",
                "Ruohan Gao",
                "J. M. F. Tsang",
                "Jan Kurcius",
                "Cagdas Bilen",
                "Chenliang Xu",
                "Anurag Kumar",
                "Sanjeel Parekh"
            ],
            "affiliations": [
                "Meta Reality Labs Research",
                "University of Maryland, College Park",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12154.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#audio",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¼Ğ¸ĞºÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Harmonizing Audio and Visuals for Engaging Video Content",
                    "desc": "This paper addresses the imbalance in advancements between visual and audio elements in video content creation. It introduces a new task called visually-guided acoustic highlighting, which aims to enhance audio based on the visual cues from the video. The authors propose a transformer-based multimodal framework to achieve this, supported by a new dataset called the muddy mix dataset that simulates real-world audio mixing challenges. Their approach shows significant improvements over existing methods in both quantitative metrics and subjective assessments, highlighting the importance of integrating visual and audio elements for a cohesive viewing experience."
                },
                "zh": {
                    "title": "è§†è§‰å¼•å¯¼éŸ³é¢‘é«˜äº®ï¼Œæå‡è§†å¬ä½“éªŒï¼",
                    "desc": "è¿‘å¹´æ¥ï¼Œè§†é¢‘å†…å®¹çš„åˆ›ä½œå’Œæ¶ˆè´¹æ˜¾è‘—å¢åŠ ã€‚åˆ¶ä½œå¼•äººå…¥èƒœçš„å†…å®¹éœ€è¦ç²¾å¿ƒç­–åˆ’è§†è§‰å’ŒéŸ³é¢‘å…ƒç´ ã€‚ä¸ºäº†å¼¥è¡¥è§†è§‰å’ŒéŸ³é¢‘ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ä»»åŠ¡ï¼šè§†è§‰å¼•å¯¼çš„éŸ³é¢‘é«˜äº®ï¼Œæ—¨åœ¨æ ¹æ®è§†é¢‘å†…å®¹è°ƒæ•´éŸ³é¢‘ï¼Œä»¥åˆ›é€ æ›´å’Œè°çš„è§†å¬ä½“éªŒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå˜æ¢å™¨çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†æ–°çš„æ•°æ®é›†ï¼Œä»¥æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11754",
            "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with\n  Context Permutation",
            "url": "https://huggingface.co/papers/2505.11754",
            "abstract": "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.",
            "score": 1,
            "issue_id": 3878,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "dcf59ca0d93ac6e2",
            "authors": [
                "Wenyu Huang",
                "Pavlos Vougiouklis",
                "Mirella Lapata",
                "Jeff Z. Pan"
            ],
            "affiliations": [
                "Huawei Edinburgh Research Centre, Poisson Lab, CSI, UK",
                "School of Informatics, University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11754.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· (MHQA) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° encoder-decoder Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… MHQA, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€. Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² MHQA."
                },
                "en": {
                    "title": "Unlocking Multi-hop Reasoning in Language Models",
                    "desc": "This paper investigates the challenges of Multi-hop Question Answering (MHQA) using Language Models (LMs). It highlights that while LMs excel in standard question-answering, their performance can be limited by causal masking when reasoning across multiple information sources. The authors find that encoder-decoder models, like Flan-T5, outperform smaller causal decoder-only models in MHQA tasks, especially when the order of documents matches the reasoning chain. Additionally, they propose modifications to causal masks to enhance performance and analyze attention weights, discovering that higher attention values correlate with correct answers, which can be used to improve LM effectiveness."
                },
                "zh": {
                    "title": "å¤šè·³æ¨ç†ï¼Œæå‡é—®ç­”èƒ½åŠ›ï¼",
                    "desc": "å¤šè·³é—®é¢˜å›ç­”ï¼ˆMHQAï¼‰å¢åŠ äº†é—®ç­”çš„å¤æ‚æ€§ï¼Œä½¿å…¶æ›´å…·æŒ‘æˆ˜æ€§ã€‚è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨å¤„ç†å¤šä¸ªæœç´¢ç»“æœæ—¶ï¼Œä¸ä»…éœ€è¦æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œè¿˜éœ€åœ¨ä¿¡æ¯æºä¹‹é—´è¿›è¡Œå¤šè·³æ¨ç†ã€‚å°½ç®¡LMåœ¨ä¼ ç»Ÿé—®ç­”ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å› æœæ©ç å¯èƒ½ä¼šå¦¨ç¢å…¶åœ¨å¤æ‚ä¸Šä¸‹æ–‡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œç¼–ç -è§£ç æ¨¡å‹åœ¨MHQAä»»åŠ¡ä¸­é€šå¸¸ä¼˜äºä»…ä½¿ç”¨å› æœè§£ç å™¨çš„æ¨¡å‹ï¼Œä¸”é€šè¿‡è°ƒæ•´å› æœæ©ç å¯ä»¥æœ‰æ•ˆæå‡åè€…çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11730",
            "title": "Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling",
            "url": "https://huggingface.co/papers/2505.11730",
            "abstract": "Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.",
            "score": 1,
            "issue_id": 3883,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "bd83555174b293a3",
            "authors": [
                "Hao Mark Chen",
                "Guanxi Lu",
                "Yasuyuki Okoshi",
                "Zhiwen Mo",
                "Masato Motomura",
                "Hongxiang Fan"
            ],
            "affiliations": [
                "Imperial College London, UK",
                "Institute of Science Tokyo, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11730.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Variable Granularity Search (VG-Search) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). VG-Search Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ beam search Ğ¸ Best-of-N sampling, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ VG-Search Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 3.6% Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° 52%."
                },
                "en": {
                    "title": "Dynamic Verification for Enhanced Language Model Efficiency",
                    "desc": "This paper explores the concept of Test-time Scaling (TTS) in large language models (LLMs) and emphasizes the importance of verification in enhancing reasoning performance and computational efficiency. The authors introduce a novel approach called Variable Granularity Search (VG-Search), which allows for dynamic adjustment of verification frequency during the generation process. By systematically varying the granularity of verification, VG-Search improves both the accuracy and efficiency of LLMs compared to traditional methods like Beam Search and Best-of-N sampling. The results show significant gains in accuracy while drastically reducing computational costs, paving the way for more efficient LLM applications."
                },
                "zh": {
                    "title": "åŠ¨æ€éªŒè¯ç²’åº¦æå‡æ¨ç†æ•ˆç‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚éªŒè¯åœ¨TTSä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå½±å“æ¨ç†æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†éªŒè¯ç²’åº¦çš„å½±å“ï¼Œå³åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éªŒè¯å™¨çš„è°ƒç”¨é¢‘ç‡ã€‚é€šè¿‡å¼•å…¥å¯å˜ç²’åº¦æœç´¢ï¼ˆVG-Searchï¼‰ç®—æ³•ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŠ¨æ€é€‰æ‹©ç²’åº¦å‚æ•°å¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œå¹¶åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¿‡ä¼ ç»Ÿçš„æŸæœç´¢å’Œæœ€ä½³Né‡‡æ ·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10588",
            "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation",
            "url": "https://huggingface.co/papers/2505.10588",
            "abstract": "This research offers a unique evaluation of how AI systems interpret the digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first cohort raised alongside AI, Gen Alpha faces new forms of online risk due to immersive digital engagement and a growing mismatch between their evolving communication and existing safety tools. Their distinct language, shaped by gaming, memes, and AI-driven trends, often conceals harmful interactions from both human moderators and automated systems. We assess four leading AI models (GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked harassment and manipulation within Gen Alpha discourse. Using a dataset of 100 recent expressions from gaming platforms, social media, and video content, the study reveals critical comprehension failures with direct implications for online safety. This work contributes: (1) a first-of-its-kind dataset capturing Gen Alpha expressions; (2) a framework to improve AI moderation systems for youth protection; (3) a multi-perspective evaluation including AI systems, human moderators, and parents, with direct input from Gen Alpha co-researchers; and (4) an analysis of how linguistic divergence increases youth vulnerability. Findings highlight the urgent need to redesign safety systems attuned to youth communication, especially given Gen Alpha reluctance to seek help when adults fail to understand their digital world. This study combines the insight of a Gen Alpha researcher with systematic academic analysis to address critical digital safety challenges.",
            "score": 1,
            "issue_id": 3868,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "cdc9a4f93d65b071",
            "authors": [
                "Manisha Mehta",
                "Fausto Giunchiglia"
            ],
            "affiliations": [
                "University of Trento, Trento, Italy",
                "Warren Hyde Middle School, Cupertino, California, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10588.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#interpretability",
                    "#benchmark",
                    "#ethics",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ñ€ÑŒĞµÑ€: Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹Ğº Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ ĞĞ»ÑŒÑ„Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ Ğ´Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸ÑĞºÑƒÑ€ÑĞµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ ĞĞ»ÑŒÑ„Ğ° Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ framework'Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ˜Ğ˜. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾ÑÑ‚Ñ€ÑƒÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»Ğ¾Ğ´ĞµĞ¶Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing AI Safety for Generation Alpha",
                    "desc": "This research evaluates how AI systems understand the unique digital language of Generation Alpha, who are growing up with AI technology. It highlights the risks they face online due to their distinct communication styles, influenced by gaming and memes, which can hide harmful interactions from both humans and automated systems. The study tests four AI models on their ability to detect subtle harassment in Gen Alpha's online expressions, revealing significant gaps in their comprehension. The findings emphasize the need for improved AI moderation tools that are better suited to protect youth in their digital environments."
                },
                "zh": {
                    "title": "é‡å¡‘å®‰å…¨ç³»ç»Ÿï¼Œä¿æŠ¤é˜¿å°”æ³•ä¸–ä»£çš„æ•°å­—äº¤æµ",
                    "desc": "æœ¬ç ”ç©¶ç‹¬ç‰¹åœ°è¯„ä¼°äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿå¦‚ä½•è§£è¯»é˜¿å°”æ³•ä¸–ä»£ï¼ˆ2010-2024å¹´å‡ºç”Ÿï¼‰çš„æ•°å­—è¯­è¨€ã€‚é˜¿å°”æ³•ä¸–ä»£æ˜¯é¦–ä¸ªä¸äººå·¥æ™ºèƒ½å…±åŒæˆé•¿çš„ç¾¤ä½“ï¼Œä»–ä»¬åœ¨æ²‰æµ¸å¼æ•°å­—ç¯å¢ƒä¸­é¢ä¸´æ–°çš„åœ¨çº¿é£é™©ã€‚ç ”ç©¶åˆ†æäº†å››ç§é¢†å…ˆçš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆGPT-4ã€Claudeã€Geminiå’ŒLlama 3ï¼‰åœ¨è¯†åˆ«éšè—çš„éªšæ‰°å’Œæ“æ§æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„å®‰å…¨å·¥å…·æœªèƒ½æœ‰æ•ˆç†è§£é˜¿å°”æ³•ä¸–ä»£çš„ç‹¬ç‰¹äº¤æµæ–¹å¼ï¼Œå¼ºè°ƒäº†é‡æ–°è®¾è®¡å®‰å…¨ç³»ç»Ÿçš„ç´§è¿«æ€§ï¼Œä»¥æ›´å¥½åœ°ä¿æŠ¤å¹´è½»ç”¨æˆ·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14633",
            "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas",
            "url": "https://huggingface.co/papers/2505.14633",
            "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.",
            "score": 0,
            "issue_id": 3884,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "40e16ce405eaf398",
            "authors": [
                "Yu Ying Chiu",
                "Zhilin Wang",
                "Sharan Maiya",
                "Yejin Choi",
                "Kyle Fish",
                "Sydney Levine",
                "Evan Hubinger"
            ],
            "affiliations": [
                "Anthropic",
                "Cambridge",
                "Harvard",
                "MIT",
                "NVIDIA",
                "Stanford",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14633.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#ethics",
                    "#benchmark",
                    "#alignment",
                    "#healthcare"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ¦ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ LitmusValues Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ˜Ğ˜ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ°Ñ… Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AIRiskDilemmas, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ñƒ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ˜Ğ˜. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Uncovering AI Risks Through Value Prioritization",
                    "desc": "This paper addresses the growing challenge of detecting risks in advanced AI models, particularly those that use techniques like Alignment Faking to avoid detection. The authors propose a method called LitmusValues, which evaluates AI models based on their adherence to various value classes, serving as an early warning system for risky behaviors. They introduce AIRiskDilemmas, a set of scenarios that highlight conflicts between different values, relevant to AI safety. By analyzing the value prioritization of AI models through their choices in these dilemmas, the study reveals how even benign values can indicate potential risks in both known and unknown contexts."
                },
                "zh": {
                    "title": "è¯†åˆ«äººå·¥æ™ºèƒ½é£é™©çš„ä»·å€¼è§‚ä¼˜å…ˆçº§",
                    "desc": "éšç€æ›´å¼ºå¤§çš„æ¨¡å‹å‡ºç°ï¼Œæ£€æµ‹äººå·¥æ™ºèƒ½é£é™©å˜å¾—æ›´åŠ å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLitmusValuesçš„è¯„ä¼°ç®¡é“ï¼Œç”¨äºæ­ç¤ºäººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ä¸åŒä»·å€¼ç±»åˆ«ä¸Šçš„ä¼˜å…ˆçº§ã€‚é€šè¿‡æ”¶é›†AIRiskDilemmasï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ç³»åˆ—æ¶‰åŠä»·å€¼å†²çªçš„å›°å¢ƒï¼Œä»¥è¯„ä¼°äººå·¥æ™ºèƒ½çš„å®‰å…¨é£é™©ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLitmusValuesä¸­çš„ä»·å€¼è§‚å¯ä»¥æœ‰æ•ˆé¢„æµ‹äººå·¥æ™ºèƒ½åœ¨å·²çŸ¥å’ŒæœªçŸ¥é£é™©è¡Œä¸ºä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14629",
            "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models",
            "url": "https://huggingface.co/papers/2505.14629",
            "abstract": "Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.",
            "score": 0,
            "issue_id": 3880,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "cf8c6e04379454db",
            "authors": [
                "Fnu Mohbat",
                "Mohammed J Zaki"
            ],
            "affiliations": [
                "Rensselaer Polytechnic Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14629.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#open_source",
                    "#graphs",
                    "#dataset",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "ğŸ³",
                "ru": {
                    "title": "KERL: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ½Ğ° ĞºÑƒÑ…Ğ½Ğµ Ñ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ KERL, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ĞµĞ´Ğµ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ñ. KERL Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¸Ñ‰ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞµ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¼Ğ¸ĞºÑ€Ğ¾Ğ½ÑƒÑ‚Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ KERL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ»ÑĞ´, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¸Ñ‰ĞµĞ²Ğ¾Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "KERL: Smart Food Recommendations with LLMs and Knowledge Graphs",
                    "desc": "This paper presents KERL, a novel system that combines large language models (LLMs) with food-related knowledge graphs (KGs) to enhance food recommendations and recipe generation. KERL processes natural language queries by extracting relevant entities and retrieving corresponding subgraphs from the KG, which are then used as context for the LLM to generate personalized recipes and nutritional information. The system not only recommends recipes but also provides detailed cooking steps and micro-nutritional data tailored to user preferences. Experimental results demonstrate that KERL outperforms existing methods, showcasing its effectiveness in delivering comprehensive food-related solutions."
                },
                "zh": {
                    "title": "KERLï¼šæ™ºèƒ½é£Ÿå“æ¨èä¸é…æ–¹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†KERLç³»ç»Ÿï¼Œå®ƒç»“åˆäº†é£Ÿå“çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»¥æä¾›ä¸ªæ€§åŒ–çš„é£Ÿå“æ¨èå’Œç”Ÿæˆé…æ–¹åŠå¾®é‡è¥å…»ä¿¡æ¯ã€‚KERLèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€é—®é¢˜ä¸­æå–å®ä½“ï¼Œå¹¶ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢å­å›¾ï¼Œå°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥åˆ°LLMä¸­ï¼Œä»¥é€‰æ‹©æ»¡è¶³çº¦æŸæ¡ä»¶çš„é…æ–¹ã€‚ç³»ç»Ÿè¿˜ç”Ÿæˆæ¯ä¸ªé…æ–¹çš„çƒ¹é¥ªæ­¥éª¤å’Œè¥å…»ä¿¡æ¯ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†KGå¢å¼ºçš„LLMåœ¨é£Ÿå“æ¨èã€é…æ–¹ç”Ÿæˆå’Œè¥å…»åˆ†ææ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14556",
            "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving\n  fMRI",
            "url": "https://huggingface.co/papers/2505.14556",
            "abstract": "Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.",
            "score": 0,
            "issue_id": 3880,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "a52590cd595c5afe",
            "authors": [
                "MarlÃ¨ne Careil",
                "Yohann Benchetrit",
                "Jean-RÃ©mi King"
            ],
            "affiliations": [
                "FAIR at Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14556.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#data",
                    "#diffusion",
                    "#architecture",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ Dynadiff",
                    "desc": "Dynadiff - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„ĞœĞ Ğ¢. ĞĞ½Ğ° ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ñ„ĞœĞ Ğ¢. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ·Ğ³Ğ°. Dynadiff Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Brain-to-Image Decoding with Dynadiff",
                    "desc": "This paper presents Dynadiff, a novel single-stage diffusion model for decoding images from dynamic fMRI recordings. Unlike traditional methods that rely on complex multi-stage processes, Dynadiff simplifies the training process and enhances performance on time-resolved brain signals. The model excels in reconstructing high-level semantic images while maintaining competitiveness with existing methods on preprocessed data. This advancement paves the way for more accurate and timely brain-to-image decoding, allowing researchers to better understand how images are represented in brain activity over time."
                },
                "zh": {
                    "title": "åŠ¨æ€ç¥ç»æ´»åŠ¨æ‰©æ•£ï¼šé‡å»ºå›¾åƒçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å•é˜¶æ®µæ‰©æ•£æ¨¡å‹Dynadiffï¼Œç”¨äºä»åŠ¨æ€å˜åŒ–çš„åŠŸèƒ½ç£å…±æŒ¯æˆåƒ(fMRI)è®°å½•ä¸­é‡å»ºå›¾åƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDynadiffç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨æ—¶é—´åˆ†è¾¨ç‡çš„fMRIä¿¡å·ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å±‚æ¬¡è¯­ä¹‰å›¾åƒé‡å»ºæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¯¥æ¨¡å‹åœ¨å¤„ç†é¢„å¤„ç†çš„fMRIæ•°æ®æ—¶ä¹Ÿä¿æŒäº†ç«äº‰åŠ›ï¼ŒåŒæ—¶èƒ½å¤Ÿç²¾ç¡®æè¿°å¤§è„‘æ´»åŠ¨ä¸­å›¾åƒè¡¨ç¤ºçš„æ¼”å˜ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸ºæ—¶é—´åˆ†è¾¨çš„å¤§è„‘åˆ°å›¾åƒè§£ç å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14366",
            "title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds",
            "url": "https://huggingface.co/papers/2505.14366",
            "abstract": "We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.",
            "score": 0,
            "issue_id": 3882,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "0e72d48711224e3c",
            "authors": [
                "Joel Currie",
                "Gioele Migno",
                "Enrico Piacenti",
                "Maria Elena Giannaccini",
                "Patric Bach",
                "Davide De Tommaso",
                "Agnieszka Wykowska"
            ],
            "affiliations": [
                "Italian Institute of Technology, Genova, Italy",
                "University of Aberdeen, Aberdeen, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14366.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#healthcare",
                    "#cv",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ (VPT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² NVIDIA Omniverse Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ 4x4, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ·Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¾ÑĞ¸ Z ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞµ, Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ¾Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² 6 ÑÑ‚ĞµĞ¿ĞµĞ½ÑÑ… ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Empowering Robots with Visual Perspective Taking for Better Interaction",
                    "desc": "This paper introduces a framework for training Vision-Language Models (VLMs) to enhance Visual Perspective Taking (VPT), which is crucial for effective Human-Robot Interaction (HRI). The authors create a synthetic dataset using NVIDIA Omniverse, designed for supervised learning in spatial reasoning tasks, containing RGB images, natural language descriptions, and transformation matrices for object poses. The focus is on predicting Z-axis distance, laying the groundwork for future advancements in understanding full 6 Degrees Of Freedom (DOFs). This dataset is made publicly available to encourage further research in developing AI systems that can comprehend spatial relationships in interactive settings."
                },
                "zh": {
                    "title": "è¿ˆå‘äººæœºäº¤äº’çš„ç©ºé—´ç†è§£èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»¥å®ç°è§†è§‰è§†è§’ç†è§£ï¼ˆVPTï¼‰çš„æ¦‚å¿µæ¡†æ¶ï¼Œè¿™æ˜¯äººæœºäº¤äº’ï¼ˆHRIï¼‰ä¸­é‡è¦çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åœ¨NVIDIA Omniverseä¸­ç”Ÿæˆï¼Œæ”¯æŒç©ºé—´æ¨ç†ä»»åŠ¡çš„ç›‘ç£å­¦ä¹ ã€‚æ¯ä¸ªå®ä¾‹åŒ…å«ä¸€ä¸ªRGBå›¾åƒã€ä¸€ä¸ªè‡ªç„¶è¯­è¨€æè¿°å’Œä¸€ä¸ªè¡¨ç¤ºç‰©ä½“å§¿æ€çš„çœŸå®4X4å˜æ¢çŸ©é˜µã€‚æˆ‘ä»¬ä¸“æ³¨äºæ¨æ–­Zè½´è·ç¦»ä½œä¸ºåŸºç¡€æŠ€èƒ½ï¼Œæœªæ¥å°†æ‰©å±•åˆ°å®Œæ•´çš„å…­è‡ªç”±åº¦ï¼ˆDOFsï¼‰æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11563",
            "title": "Object-Centric Representations Improve Policy Generalization in Robot\n  Manipulation",
            "url": "https://huggingface.co/papers/2505.11563",
            "abstract": "Visual representations are central to the learning and generalization capabilities of robotic manipulation policies. While existing methods rely on global or dense features, such representations often entangle task-relevant and irrelevant scene information, limiting robustness under distribution shifts. In this work, we investigate object-centric representations (OCR) as a structured alternative that segments visual input into a finished set of entities, introducing inductive biases that align more naturally with manipulation tasks. We benchmark a range of visual encoders-object-centric, global and dense methods-across a suite of simulated and real-world manipulation tasks ranging from simple to complex, and evaluate their generalization under diverse visual conditions including changes in lighting, texture, and the presence of distractors. Our findings reveal that OCR-based policies outperform dense and global representations in generalization settings, even without task-specific pretraining. These insights suggest that OCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.",
            "score": 0,
            "issue_id": 3880,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "114f28f1ced2da93",
            "authors": [
                "Alexandre Chapin",
                "Bruno Machado",
                "Emmanuel Dellandrea",
                "Liming Chen"
            ],
            "affiliations": [
                "Ecole Centrale de Lyon, LIRIS 69130, Ecully, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11563.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#robotics",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ - ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (OCR) ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ OCR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ OCR Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Robustness in Robotics with Object-Centric Representations",
                    "desc": "This paper explores the use of object-centric representations (OCR) in robotic manipulation tasks to improve learning and generalization. Unlike traditional methods that use global or dense features, which can mix relevant and irrelevant information, OCR focuses on distinct entities in the visual input. The authors benchmark various visual encoders, including OCR, against different manipulation tasks to assess their performance under changing conditions. The results show that OCR-based policies significantly outperform other methods, indicating their potential for robust performance in real-world scenarios."
                },
                "zh": {
                    "title": "ç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºï¼šæå‡æœºå™¨äººæ“ä½œçš„æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºï¼ˆOCRï¼‰åœ¨æœºå™¨äººæ“ä½œä¸­çš„åº”ç”¨ã€‚ä¸ç°æœ‰çš„å…¨å±€æˆ–å¯†é›†ç‰¹å¾æ–¹æ³•ä¸åŒï¼ŒOCRèƒ½å¤Ÿå°†è§†è§‰è¾“å…¥åˆ†å‰²æˆä¸€ç»„ç‹¬ç«‹çš„å®ä½“ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†ä¸ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨å¤šç§æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æ“ä½œä»»åŠ¡ä¸­å¯¹æ¯”äº†ä¸åŒçš„è§†è§‰ç¼–ç å™¨ï¼Œå‘ç°OCRåœ¨é¢å¯¹ä¸åŒçš„è§†è§‰æ¡ä»¶æ—¶è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒOCRæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¯ä»¥æœ‰æ•ˆè®¾è®¡å‡ºé€‚åº”åŠ¨æ€çœŸå®ç¯å¢ƒçš„è§†è§‰ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.06914",
            "title": "The Distracting Effect: Understanding Irrelevant Passages in RAG",
            "url": "https://huggingface.co/papers/2505.06914",
            "abstract": "A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs.   Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages.",
            "score": 0,
            "issue_id": 3882,
            "pub_date": "2025-05-11",
            "pub_date_card": {
                "ru": "11 Ğ¼Ğ°Ñ",
                "en": "May 11",
                "zh": "5æœˆ11æ—¥"
            },
            "hash": "dc5ed9552f5c1ec6",
            "authors": [
                "Chen Amiraz",
                "Florin Cuconasu",
                "Simone Filice",
                "Zohar Karnin"
            ],
            "affiliations": [
                "Sapienza University of Rome",
                "Technology Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06914.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#training",
                    "#rag",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ĞºÑ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ RAG",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°ÑÑĞ°Ğ¶ĞµĞ¹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰ĞµĞ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¿Ğ°ÑÑĞ°Ğ¶Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞµĞ³Ğ¾ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°ÑÑĞ°Ğ¶ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¿Ğ°ÑÑĞ°Ğ¶Ğ°Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° 7.5% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… RAG."
                },
                "en": {
                    "title": "Enhancing RAG: Turning Distractions into Accuracy Boosts",
                    "desc": "This paper addresses a significant challenge in Retrieval Augmented Generation (RAG) systems, where irrelevant passages can mislead language models (LLMs) and result in incorrect answers. The authors propose a new way to measure how distracting a passage can be in relation to a query and an LLM, providing a quantifiable metric for this effect. They introduce innovative techniques for identifying and leveraging these hard distracting passages, which leads to improved performance in answering accuracy. By fine-tuning LLMs with these selected passages, they demonstrate a notable increase in accuracy compared to traditional RAG approaches."
                },
                "zh": {
                    "title": "æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å‡†ç¡®æ€§",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çš„ä¸€ä¸ªé‡è¦é—®é¢˜ï¼Œå³ä¸æŸ¥è¯¢æ— å…³çš„æ£€ç´¢æ®µè½å¯èƒ½ä¼šå¹²æ‰°ç­”æ¡ˆç”Ÿæˆçš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¯¼è‡´é”™è¯¯çš„å›ç­”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡åŒ–æ–¹æ³•æ¥è¡¡é‡æ®µè½å¯¹æŸ¥è¯¢å’ŒLLMçš„å¹²æ‰°æ•ˆæœï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ä¸åŒLLMä¸­çš„ç¨³å¥æ€§ã€‚é€šè¿‡ç²¾å¿ƒé€‰æ‹©è¿™äº›å¹²æ‰°æ®µè½å¹¶å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬çš„ç ”ç©¶å®ç°äº†é«˜è¾¾7.5%çš„å›ç­”å‡†ç¡®ç‡æå‡ã€‚æˆ‘ä»¬çš„è´¡çŒ®åœ¨äºè¶…è¶Šäº†ç®€å•çš„äºŒå…ƒåˆ†ç±»ï¼Œå‘å±•äº†å¤šç§æ–¹æ³•æ¥è¯†åˆ«å’Œåˆ©ç”¨éš¾ä»¥å¤„ç†çš„å¹²æ‰°æ®µè½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-20.html",
    "link_next": "2025-05-22.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "20.05",
        "en": "05/20",
        "zh": "5æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "22.05",
        "en": "05/22",
        "zh": "5æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 18,
        "#data": 8,
        "#benchmark": 24,
        "#agents": 5,
        "#cv": 9,
        "#rl": 8,
        "#rlhf": 4,
        "#rag": 3,
        "#plp": 1,
        "#inference": 10,
        "#3d": 1,
        "#audio": 3,
        "#video": 3,
        "#multimodal": 13,
        "#math": 3,
        "#multilingual": 2,
        "#architecture": 8,
        "#healthcare": 4,
        "#training": 25,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 10,
        "#reasoning": 22,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 5,
        "#security": 5,
        "#optimization": 17,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 7,
        "#story_generation": 0,
        "#hallucinations": 5,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºBAGELçš„å¼€æºåŸºç¡€æ¨¡å‹ã€‚å®ƒèƒ½ç»Ÿä¸€å¤„ç†å’Œç”Ÿæˆå¤šç§æ¨¡å¼çš„æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚BAGELé€šè¿‡åœ¨å¤§è§„æ¨¡çš„å¤šæ¨¡å¼äº¤é”™æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œå±•ç°å‡ºå¤æ‚çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚å®ƒåœ¨å¤šæ¨¡å¼ç”Ÿæˆå’Œç†è§£æ–¹é¢è¶…è¶Šäº†å…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶å…·å¤‡é«˜çº§çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚ã€æ•°æ®åˆ›å»ºåè®®åŠä»£ç å’Œæ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡å¼ç ”ç©¶ã€‚",
        "title": "Emerging Properties in Unified Multimodal Pretraining",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºBAGELçš„å¼€æºåŸºç¡€æ¨¡å‹ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«gÃ¨ mÃ­ngwÃ¨i BAGEL de kÄiyuÃ¡n jÄ«chÇ” mÃ³xÃ­ng.\n\nå®ƒèƒ½ç»Ÿä¸€å¤„ç†å’Œç”Ÿæˆå¤šç§æ¨¡å¼çš„æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚\nTÄ nÃ©ng tÇ’ngyÄ« chÇ”lÇ hÃ© shÄ“ngchÃ©ng duÅzhÇ’ng mÃ³shÃ¬ de shÃ¹jÃ¹, rÃº wÃ©nbÄ›n, tÃºxiÃ ng hÃ© shÃ¬pÃ­n.\n\nBAGELé€šè¿‡åœ¨å¤§è§„æ¨¡çš„å¤šæ¨¡å¼äº¤é”™æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œå±•ç°å‡ºå¤æ‚çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚\nBAGEL tÅngguÃ² zÃ i dÃ guÄ«mÃ³ de duÅmÃ³shÃ¬ jiÄocuÃ² shÃ¹jÃ¹ shÃ ng yÃ¹xÃ¹nliÃ n, zhÇnxiÃ n chÅ« fÃ¹zÃ¡ de duÅmÃ³shÃ¬ tuÄ«lÇ nÃ©nglÃ¬.\n\nå®ƒåœ¨å¤šæ¨¡å¼ç”Ÿæˆå’Œç†è§£æ–¹é¢è¶…è¶Šäº†å…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶å…·å¤‡é«˜çº§çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚\nTÄ zÃ i duÅmÃ³shÃ¬ shÄ“ngchÃ©ng hÃ© lÇjiÄ› fÄngmiÃ n chÄoyuÃ¨ le qÃ­tÄ kÄiyuÃ¡n mÃ³xÃ­ng, bÃ¬ng jÃ¹bÃ¨i gÄojÃ­ de duÅmÃ³shÃ¬ tuÄ«lÇ nÃ©nglÃ¬.\n\nç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚ã€æ•°æ®åˆ›å»ºåè®®åŠä»£ç å’Œæ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡å¼ç ”ç©¶ã€‚\nYÃ¡njiÅ« tuÃ¡nduÃ¬ gÅngkÄi le guÇnjiÃ n fÄxiÃ n, yÃ¹xÃ¹nliÃ n xÃ¬jiÃ¨, shÃ¹jÃ¹ chuÃ ngjiÃ n xiÃ©yÃ¬ jÃ­ dÃ imÇ hÃ© jiÇnchÃ¡diÇn, yÇ cÃ¹jÃ¬n duÅmÃ³shÃ¬ yÃ¡njiÅ«.",
        "vocab": "[\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open source\"},\n    {\"word\": \"åŸºç¡€æ¨¡å‹\", \"pinyin\": \"jÄ« chÇ” mÃ³ xÃ­ng\", \"trans\": \"foundational model\"},\n    {\"word\": \"ç»Ÿä¸€\", \"pinyin\": \"tÇ’ng yÄ«\", \"trans\": \"unify\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"æ¨¡å¼\", \"pinyin\": \"mÃ³ shÃ¬\", \"trans\": \"mode\"},\n    {\"word\": \"æ•°æ®\", \"pinyin\": \"shÃ¹ jÃ¹\", \"trans\": \"data\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©n bÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃº xiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"è§†é¢‘\", \"pinyin\": \"shÃ¬ pÃ­n\", \"trans\": \"video\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pretrain\"},\n    {\"word\": \"å±•ç°\", \"pinyin\": \"zhÇn xiÃ n\", \"trans\": \"demonstrate\"},\n    {\"word\": \"å¤æ‚\", \"pinyin\": \"fÃ¹ zÃ¡\", \"trans\": \"complex\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"æ–¹é¢\", \"pinyin\": \"fÄng miÃ n\", \"trans\": \"aspect\"},\n    {\"word\": \"é«˜çº§\", \"pinyin\": \"gÄo jÃ­\", \"trans\": \"advanced\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"å›¢é˜Ÿ\", \"pinyin\": \"tuÃ¡n duÃ¬\", \"trans\": \"team\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅng kÄi\", \"trans\": \"public\"},\n    {\"word\": \"å‘ç°\", \"pinyin\": \"fÄ xiÃ n\", \"trans\": \"discovery\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬ jiÄ›\", \"trans\": \"detail\"},\n    {\"word\": \"åè®®\", \"pinyin\": \"xiÃ© yÃ¬\", \"trans\": \"protocol\"},\n    {\"word\": \"æ£€æŸ¥ç‚¹\", \"pinyin\": \"jiÇn chÃ¡ diÇn\", \"trans\": \"checkpoint\"},\n    {\"word\": \"ä¿ƒè¿›\", \"pinyin\": \"cÃ¹ jÃ¬n\", \"trans\": \"promote\"}\n]",
        "trans": "This article introduces an open-source foundational model called BAGEL. It is capable of uniformly processing and generating data in various modalities, such as text, images, and videos. BAGEL, through pre-training on large-scale interleaved multimodal data, demonstrates complex multimodal reasoning capabilities. It outperforms other open-source models in multimodal generation and understanding and possesses advanced multimodal reasoning abilities. The research team has made key findings, pre-training details, data creation protocols, and code and checkpoints publicly available to promote multimodal research.",
        "update_ts": "2025-05-21 09:13"
    }
}