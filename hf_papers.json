{
    "date": {
        "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 27",
        "zh": "2æœˆ27æ—¥"
    },
    "time_utc": "2025-02-27 03:19",
    "weekday": 3,
    "issue_id": 2432,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.16776",
            "title": "AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement",
            "url": "https://huggingface.co/papers/2502.16776",
            "abstract": "As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at https://github.com/thu-coai/AISafetyLab, and we are committed to its continuous maintenance and improvement.",
            "score": 2,
            "issue_id": 2432,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "3401008394925d16",
            "authors": [
                "Zhexin Zhang",
                "Leqi Lei",
                "Junxiao Yang",
                "Xijie Huang",
                "Yida Lu",
                "Shiyao Cui",
                "Renmiao Chen",
                "Qinglin Zhang",
                "Xinyuan Wang",
                "Hao Wang",
                "Hao Li",
                "Xianqi Lei",
                "Chengwei Pan",
                "Lei Sha",
                "Hongning Wang",
                "Minlie Huang"
            ],
            "affiliations": [
                "Beihang University, Beijing, China",
                "The Conversational AI (CoAI) group, DCST, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16776.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#security"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜",
                    "desc": "AISafetyLab - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ñ‚Ğ°Ğº, Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Vicuna, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ½Ğ° GitHub Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ ÑĞ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "AISafetyLab: A Unified Toolkit for AI Safety",
                    "desc": "This paper addresses the critical challenge of ensuring AI safety as models are used in real-world applications. It introduces AISafetyLab, a comprehensive framework and toolkit that combines various attack, defense, and evaluation methods for AI safety. The toolkit is designed to be user-friendly, allowing developers to easily implement safety techniques while providing a flexible codebase for future enhancements. The authors also present empirical studies on the Vicuna model, comparing different safety strategies to highlight their effectiveness."
                },
                "zh": {
                    "title": "æ„å»ºå®‰å…¨çš„äººå·¥æ™ºèƒ½ï¼šAISafetyLabæ¡†æ¶",
                    "desc": "éšç€äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨å„ç§ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œç¡®ä¿å…¶å®‰å…¨æ€§æˆä¸ºä¸€ä¸ªé‡è¦ä½†å°šæœªå……åˆ†æ¢ç´¢çš„æŒ‘æˆ˜ã€‚è™½ç„¶åœ¨è¯„ä¼°å’Œå¢å¼ºäººå·¥æ™ºèƒ½å®‰å…¨æ€§æ–¹é¢å·²ç»åšäº†å¤§é‡å·¥ä½œï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–æ¡†æ¶å’Œå…¨é¢å·¥å…·åŒ…ä»ç„¶æ˜¯ç³»ç»Ÿç ”ç©¶å’Œå®é™…åº”ç”¨çš„é‡å¤§éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AISafetyLabï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å’Œå·¥å…·åŒ…ï¼Œé›†æˆäº†ä»£è¡¨æ€§çš„æ”»å‡»ã€é˜²å¾¡å’Œè¯„ä¼°æ–¹æ³•ã€‚AISafetyLabæä¾›äº†ç›´è§‚çš„ç•Œé¢ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿæ— ç¼åº”ç”¨å„ç§æŠ€æœ¯ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½ç»“æ„å’Œå¯æ‰©å±•çš„ä»£ç åº“ï¼Œä»¥ä¾¿æœªæ¥çš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18906",
            "title": "VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model",
            "url": "https://huggingface.co/papers/2502.18906",
            "abstract": "Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an environment-free RL framework that decouples value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM). VEM predicts state-action values directly from offline data, distilling human-like priors about GUI interaction outcomes without requiring next-state prediction or environmental feedback. This avoids compounding errors and enhances resilience to UI changes by focusing on semantic reasoning (e.g., Does this action advance the user's goal?). The framework operates in two stages: (1) pretraining VEM to estimate long-term action utilities and (2) guiding policy exploration with frozen VEM signals, enabling layout-agnostic GUI automation. Evaluated on Android-in-the-Wild benchmarks, VEM achieves state-of-the-art performance in both offline and online settings, outperforming environment-free baselines significantly and matching environment-based approaches without interaction costs. Importantly, VEM demonstrates that semantic-aware value estimation can achieve comparable performance with online-trained methods.",
            "score": 2,
            "issue_id": 2432,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "5ac6f89ffc897bbe",
            "authors": [
                "Jiani Zheng",
                "Lu Wang",
                "Fangkai Yang",
                "Chaoyun Zhang",
                "Lingrui Mei",
                "Wenjie Yin",
                "Qingwei Lin",
                "Dongmei Zhang",
                "Saravan Rajmohan",
                "Qi Zhang"
            ],
            "affiliations": [
                "KTH Royal Institute of Technology",
                "Microsoft",
                "Peking University",
                "University of the Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18906.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#agents",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±ĞµĞ·ÑÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ (VEM). VEM Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Android-in-the-Wild, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ±ĞµĞ·ÑÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑÑŒ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing GUI Automation with Semantic-Aware Value Estimation",
                    "desc": "This paper addresses the challenges of training Vision-Language Models (VLMs) for Graphical User Interfaces (GUIs) using Reinforcement Learning (RL). It introduces a novel environment-free RL framework that separates value estimation from policy optimization by utilizing a pretrained Value Environment Model (VEM). VEM predicts state-action values from offline data, allowing for better generalization and resilience to changes in the user interface. The proposed method shows superior performance on benchmarks, demonstrating that semantic reasoning can effectively guide GUI automation without the need for costly interactions."
                },
                "zh": {
                    "title": "æ— ç¯å¢ƒå¼ºåŒ–å­¦ä¹ ï¼šæå‡GUIä»£ç†çš„æ™ºèƒ½åŒ–",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¯å¢ƒæ— å…³çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»¥æ”¯æŒå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„ä»·å€¼ç¯å¢ƒæ¨¡å‹ï¼ˆVEMï¼‰ï¼Œå°†ä»·å€¼ä¼°è®¡ä¸ç­–ç•¥ä¼˜åŒ–è§£è€¦ï¼Œé¿å…äº†ç¯å¢ƒåé¦ˆçš„éœ€æ±‚ã€‚VEMèƒ½å¤Ÿç›´æ¥ä»ç¦»çº¿æ•°æ®ä¸­é¢„æµ‹çŠ¶æ€-åŠ¨ä½œå€¼ï¼Œå¢å¼ºäº†å¯¹ç”¨æˆ·ç•Œé¢å˜åŒ–çš„é€‚åº”èƒ½åŠ›ï¼Œå¹¶ä¸“æ³¨äºè¯­ä¹‰æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVEMåœ¨Android-in-the-WildåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç¯å¢ƒæ— å…³çš„åŸºçº¿ï¼Œå¹¶åœ¨æ²¡æœ‰äº¤äº’æˆæœ¬çš„æƒ…å†µä¸‹ä¸åŸºäºç¯å¢ƒçš„æ–¹æ³•ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.19328",
            "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems",
            "url": "https://huggingface.co/papers/2502.19328",
            "abstract": "Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).",
            "score": 1,
            "issue_id": 2432,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "57e2794756efc952",
            "authors": [
                "Hao Peng",
                "Yunjia Qi",
                "Xiaozhi Wang",
                "Zijun Yao",
                "Bin Xu",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.19328.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#open_source",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ RewardAgent, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RewardAgent Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Enhancing LLM Training with Agentic Reward Modeling",
                    "desc": "This paper introduces agentic reward modeling, which enhances the training of large language models (LLMs) by integrating human preferences with verifiable correctness signals. The proposed RewardAgent combines rewards based on factual accuracy and adherence to instructions, leading to more reliable training outcomes. Through extensive experiments, RewardAgent demonstrates superior performance over traditional reward models in various natural language processing tasks. The authors also provide their code to support further research in this area."
                },
                "zh": {
                    "title": "ä»£ç†å¥–åŠ±å»ºæ¨¡ï¼šæå‡è¯­è¨€æ¨¡å‹çš„å¯é æ€§",
                    "desc": "å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®­ç»ƒå’Œæ¨ç†ä¸­è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹ä¸»è¦å…³æ³¨äººç±»åå¥½ï¼Œå¿½è§†äº†å¯éªŒè¯çš„æ­£ç¡®æ€§ä¿¡å·ï¼Œè€Œè¿™äº›ä¿¡å·åœ¨è®­ç»ƒLLMsä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»£ç†å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œå°†å¥–åŠ±æ¨¡å‹ä¸æ¥è‡ªä¸åŒæ–¹é¢çš„å¯éªŒè¯æ­£ç¡®æ€§ä¿¡å·ç›¸ç»“åˆï¼Œä»¥æä¾›æ›´å¯é çš„å¥–åŠ±ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåä¸ºRewardAgentçš„å¥–åŠ±ä»£ç†ï¼Œå®ƒç»“åˆäº†äººç±»åå¥½å¥–åŠ±å’Œä¸¤ä¸ªå¯éªŒè¯ä¿¡å·ï¼šäº‹å®æ€§å’ŒæŒ‡ä»¤éµå¾ªï¼Œä»è€Œåœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18864",
            "title": "Towards an AI co-scientist",
            "url": "https://huggingface.co/papers/2502.18864",
            "abstract": "Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, inspired by the scientific method and accelerated by scaling test-time compute. Key contributions include: (1) a multi-agent architecture with an asynchronous task execution framework for flexible compute scaling; (2) a tournament evolution process for self-improving hypotheses generation. Automated evaluations show continued benefits of test-time compute, improving hypothesis quality. While general purpose, we focus development and validation in three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution and anti-microbial resistance. For drug repurposing, the system proposes candidates with promising validation findings, including candidates for acute myeloid leukemia that show tumor inhibition in vitro at clinically applicable concentrations. For novel target discovery, the AI co-scientist proposed new epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and liver cell regeneration in human hepatic organoids. Finally, the AI co-scientist recapitulated unpublished experimental results via a parallel in silico discovery of a novel gene transfer mechanism in bacterial evolution. These results, detailed in separate, co-timed reports, demonstrate the potential to augment biomedical and scientific discovery and usher an era of AI empowered scientists.",
            "score": 0,
            "issue_id": 2432,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "f5b7ae00d9351ca7",
            "authors": [
                "Juraj Gottweis",
                "Wei-Hung Weng",
                "Alexander Daryin",
                "Tao Tu",
                "Anil Palepu",
                "Petar Sirkovic",
                "Artiom Myaskovsky",
                "Felix Weissenberger",
                "Keran Rong",
                "Ryutaro Tanno",
                "Khaled Saab",
                "Dan Popovici",
                "Jacob Blum",
                "Fan Zhang",
                "Katherine Chou",
                "Avinatan Hassidim",
                "Burak Gokturk",
                "Amin Vahdat",
                "Pushmeet Kohli",
                "Yossi Matias",
                "Andrew Carroll",
                "Kavita Kulkarni",
                "Nenad Tomasev",
                "Yuan Guan",
                "Vikram Dhillon",
                "Eeshit Dhaval Vaishnav",
                "Byron Lee",
                "Tiago R D Costa",
                "JosÃ© R PenadÃ©s",
                "Gary Peltz",
                "Yunhan Xu",
                "Annalisa Pawlosky",
                "Alan Karthikesalingam",
                "Vivek Natarajan"
            ],
            "affiliations": [
                "Fleming Initiative and Imperial College London",
                "Google Cloud AI Research",
                "Google DeepMind",
                "Google Research",
                "Houston Methodist",
                "Sequome",
                "Stanford University School of Medicine"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18864.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#science",
                    "#healthcare",
                    "#architecture"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ˜Ğ˜-ÑĞ¾ÑƒÑ‡ĞµĞ½Ñ‹Ğ¹: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ˜Ğ˜-ÑĞ¾ÑƒÑ‡ĞµĞ½Ğ¾Ğ³Ğ¾, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Gemini 2.0, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ ÑƒÑ‡ĞµĞ½Ñ‹Ğ¼ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ' Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ñ‡ĞµÑ€ĞµĞ· Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² Ñ‚Ñ€ĞµÑ… Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ², Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¸ÑˆĞµĞ½ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ±Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Empowering Science with AI: Unleashing Novel Hypotheses",
                    "desc": "This paper presents an AI co-scientist, a multi-agent system designed to assist in scientific discovery by generating and validating novel research hypotheses. Built on the Gemini 2.0 framework, it employs a generate, debate, and evolve methodology that mimics the scientific method while leveraging enhanced computational resources. The system's architecture allows for flexible scaling of tasks and includes a tournament evolution process to improve hypothesis generation over time. Focused on biomedical applications, the AI co-scientist has successfully proposed drug candidates and novel targets, demonstrating its potential to significantly enhance research outcomes."
                },
                "zh": {
                    "title": "AIåŠ©åŠ›ç§‘å­¦å‘ç°çš„æ–°çºªå…ƒ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAIå…±åŒç§‘å­¦å®¶çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨å¸®åŠ©ç§‘å­¦å®¶ç”Ÿæˆæ–°çš„ç ”ç©¶å‡è®¾å¹¶è¿›è¡ŒéªŒè¯ã€‚è¯¥ç³»ç»ŸåŸºäºGemini 2.0ï¼Œé‡‡ç”¨ç”Ÿæˆã€è¾©è®ºå’Œæ¼”å˜çš„æ–¹æ³•æ¥ç”Ÿæˆå‡è®¾ï¼Œçµæ„Ÿæ¥æºäºç§‘å­¦æ–¹æ³•ã€‚å®ƒåœ¨è¯ç‰©é‡å®šä½ã€æ–°é¶ç‚¹å‘ç°å’Œç»†èŒè¿›åŒ–æœºåˆ¶ç­‰ç”Ÿç‰©åŒ»å­¦é¢†åŸŸè¿›è¡Œäº†å¼€å‘å’ŒéªŒè¯ï¼Œå±•ç¤ºäº†å…¶åœ¨ç§‘å­¦å‘ç°ä¸­çš„æ½œåŠ›ã€‚é€šè¿‡è‡ªåŠ¨è¯„ä¼°ï¼Œç³»ç»Ÿåœ¨å‡è®¾è´¨é‡ä¸Šæ˜¾ç¤ºå‡ºæŒç»­çš„æ”¹è¿›ï¼Œè¡¨æ˜AIå¯ä»¥å¢å¼ºç§‘å­¦ç ”ç©¶çš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.19204",
            "title": "Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator",
            "url": "https://huggingface.co/papers/2502.19204",
            "abstract": "Monocular depth estimation (MDE) aims to predict scene depth from a single RGB image and plays a crucial role in 3D scene understanding. Recent advances in zero-shot MDE leverage normalized depth representations and distillation-based learning to improve generalization across diverse scenes. However, current depth normalization methods for distillation, relying on global normalization, can amplify noisy pseudo-labels, reducing distillation effectiveness. In this paper, we systematically analyze the impact of different depth normalization strategies on pseudo-label distillation. Based on our findings, we propose Cross-Context Distillation, which integrates global and local depth cues to enhance pseudo-label quality. Additionally, we introduce a multi-teacher distillation framework that leverages complementary strengths of different depth estimation models, leading to more robust and accurate depth predictions. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, both quantitatively and qualitatively.",
            "score": 0,
            "issue_id": 2432,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "5f3b79d04ea09388",
            "authors": [
                "Xiankang He",
                "Dongyan Guo",
                "Hongji Li",
                "Ruibo Li",
                "Ying Cui",
                "Chi Zhang"
            ],
            "affiliations": [
                "AGI Lab, Westlake University",
                "Lanzhou University",
                "Nanyang Technological University",
                "Zhejiang University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.19204.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cross-Context Distillation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing Depth Estimation with Cross-Context Distillation",
                    "desc": "This paper focuses on monocular depth estimation (MDE), which is the process of predicting depth information from a single RGB image. The authors identify that existing depth normalization techniques can worsen the quality of pseudo-labels used in distillation, which is crucial for improving model performance. To address this, they propose a new method called Cross-Context Distillation that combines both global and local depth information to enhance the quality of these pseudo-labels. Their multi-teacher distillation framework utilizes various depth estimation models to improve prediction accuracy, showing significant improvements over current leading methods in extensive experiments."
                },
                "zh": {
                    "title": "è·¨ä¸Šä¸‹æ–‡è’¸é¦ï¼šæå‡æ·±åº¦ä¼°è®¡çš„è´¨é‡",
                    "desc": "å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰æ—¨åœ¨ä»å•å¼ RGBå›¾åƒä¸­é¢„æµ‹åœºæ™¯æ·±åº¦ï¼Œå¯¹3Dåœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„é›¶æ ·æœ¬MDEè¿›å±•åˆ©ç”¨å½’ä¸€åŒ–æ·±åº¦è¡¨ç¤ºå’ŒåŸºäºè’¸é¦çš„å­¦ä¹ æ¥æé«˜åœ¨ä¸åŒåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ·±åº¦å½’ä¸€åŒ–æ–¹æ³•ä¾èµ–äºå…¨å±€å½’ä¸€åŒ–ï¼Œå¯èƒ½ä¼šæ”¾å¤§å™ªå£°ä¼ªæ ‡ç­¾ï¼Œä»è€Œé™ä½è’¸é¦æ•ˆæœã€‚æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†ä¸åŒæ·±åº¦å½’ä¸€åŒ–ç­–ç•¥å¯¹ä¼ªæ ‡ç­¾è’¸é¦çš„å½±å“ï¼Œå¹¶æå‡ºäº†è·¨ä¸Šä¸‹æ–‡è’¸é¦æ–¹æ³•ï¼Œä»¥å¢å¼ºä¼ªæ ‡ç­¾è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.19400",
            "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding",
            "url": "https://huggingface.co/papers/2502.19400",
            "abstract": "Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.",
            "score": 0,
            "issue_id": 2432,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "87db4026705c319e",
            "authors": [
                "Max Ku",
                "Thomas Chong",
                "Jonathan Leung",
                "Krish Shah",
                "Alvin Yu",
                "Wenhu Chen"
            ],
            "affiliations": [
                "University of Waterloo",
                "Vector Institute",
                "Votee AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.19400.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#agents",
                    "#interpretability",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TheoremExplainAgent - Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Manim. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TheoremExplainBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 240 Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… STEM-Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚ o3-mini Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 93.8% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ¼ĞµÑÑ‚ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Theorem Understanding with Visual Explanations",
                    "desc": "This paper presents TheoremExplainAgent, a novel approach for creating long-form video explanations of mathematical theorems using animations. It addresses the limitations of large language models in generating coherent visual content for educational purposes. The authors introduce TheoremExplainBench, a benchmark for evaluating these multimodal explanations across various STEM fields, utilizing 240 theorems and five automated metrics. The findings indicate that while the agentic planning significantly enhances video quality, there are still challenges in visual layout and reasoning that need to be addressed."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€è§£é‡Šï¼šè¶…è¶Šæ–‡æœ¬çš„ç†è§£",
                    "desc": "ç†è§£ç‰¹å®šé¢†åŸŸçš„å®šç†ä¸ä»…éœ€è¦æ–‡æœ¬æ¨ç†ï¼Œè¿˜éœ€è¦é€šè¿‡ç»“æ„åŒ–çš„è§†è§‰è§£é‡Šè¿›è¡Œæœ‰æ•ˆæ²Ÿé€šï¼Œä»¥ä¾¿æ›´æ·±å…¥åœ°ç†è§£ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºäºæ–‡æœ¬çš„å®šç†æ¨ç†ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ç”Ÿæˆè¿è´¯ä¸”å…·æœ‰æ•™å­¦æ„ä¹‰çš„è§†è§‰è§£é‡Šçš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†TheoremExplainAgentï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆé•¿æ ¼å¼å®šç†è§£é‡Šè§†é¢‘çš„ä»£ç†æ–¹æ³•ï¼Œä½¿ç”¨ManimåŠ¨ç”»è¿›è¡Œå±•ç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»£ç†è§„åˆ’å¯¹äºç”Ÿæˆè¯¦ç»†çš„é•¿æ ¼å¼è§†é¢‘è‡³å…³é‡è¦ï¼Œo3-miniä»£ç†çš„æˆåŠŸç‡è¾¾åˆ°93.8%ï¼Œä½†ç”Ÿæˆçš„è§†é¢‘åœ¨è§†è§‰å…ƒç´ å¸ƒå±€ä¸Šå­˜åœ¨ä¸€äº›å°é—®é¢˜ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-26.html",
    "link_next": "2025-02-28.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "26.02",
        "en": "02/26",
        "zh": "2æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "28.02",
        "en": "02/28",
        "zh": "2æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "æœ€è¿‘çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§å›ç­”æ ¼å¼ï¼Œä»¥æå‡MLLMsä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼å¯¹é½çš„äººå·¥æ ‡æ³¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œä¿ç•™å…¶åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨https://github.com/PhoenixZ810/OmniAlign-Vã€‚",
        "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
        "pinyin": "æœ€è¿‘çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„å…¨é¢æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§å›ç­”æ ¼å¼ï¼Œä»¥æå‡MLLMsä¸äººç±»åå¥½çš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsä¸äººç±»ä»·å€¼å¯¹é½çš„äººå·¥æ ‡æ³¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†äººç±»åå¥½å¯¹é½ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ï¼Œä¿ç•™å…¶åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨https://github.com/PhoenixZ810/OmniAlign-Vã€‚\n\nZuÃ¬jÃ¬n de kÄiyuÇn duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (MLLMs) zhÇ”yÃ o jÃ­zhÅng zÃ i zÄ“ngqiÃ¡ng jÄ«chÇ” nÃ©nglÃ¬, dÃ n zÃ i yÇ” rÃ©nlÃ¨i piÄnhÇo duÃ¬qÃ­ fÄngmiÃ n rÃ©ng yÇ’u hÄ›n dÃ  chÄjÃ¹. BÄ›nwÃ©n jiÃ¨shÃ o le OmniAlign-V, yÄ«gÃ¨ bÄohÃ¡n 20 wÃ n gÄo zhÃ¬liÃ ng xÃ¹nliÃ n yÃ ngbÄ›n de quÃ¡nmiÃ n shÃ¹jÃ¹jÃ­, hÃ¡njiÄ“ duÅyÃ nghuÃ  tÃºxiÃ ng, fÃ¹zÃ¡ xuÃ¡nzhÃ²ng wÃ¨ntÃ­ hÃ© duÅzhÇ’ng huÃ­dÃ¡ gÃ©shÃ¬, yÇ tÃ­shÄ“ng MLLMs yÇ” rÃ©nlÃ¨i piÄnhÇo de duÃ¬qÃ­. WÇ’men hÃ¡i tuÄ«chÅ« le MM-AlignBench, yÄ«gÃ¨ zhuÄnmÃ©n shÃ¨jÃ¬ yÃ²ngyÃº pÃ­nggÇ” MLLMs yÇ” rÃ©nlÃ¨i jiÃ zhÃ­ duÃ¬qÃ­ de rÃ©ngÅng biÄozhÃ¹ jÄ«zhÇ”n. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, shÇyÃ²ng jiÃ ndÅ« wÄ“itiÃ¡o (SFT) huÃ² zhÃ­jiÄ“ piÄnhÇo yÅuhuÃ  (DPO) duÃ¬ MLLMs jÃ¬nxÃ­ng wÄ“itiÃ¡o, xiÇnzhÃ¹ tÃ­gÄo le rÃ©nlÃ¨i piÄnhÇo duÃ¬qÃ­, tÃ³ngshÃ­ zÃ i biÄozhÇ”n VQA jÄ«zhÇ”n shÃ ng bÇochÃ­ huÃ² tÃ­shÄ“ng xÃ¬ngnÃ©ng, bÇoliÃº qÃ­ jÄ«bÄ›n nÃ©nglÃ¬. WÇ’men de shÃ¹jÃ¹jÃ­, jÄ«zhÇ”n, dÃ imÇ hÃ© jiÇnchÃ¡diÇn yÇ fÄbÃ¹ zÃ i https://github.com/PhoenixZ810/OmniAlign-V.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"åŸºç¡€èƒ½åŠ›\", \"pinyin\": \"jÄ« chÇ” nÃ©ng lÃ¬\", \"trans\": \"basic capabilities\"},\n    {\"word\": \"å¯¹é½\", \"pinyin\": \"duÃ¬ qÃ­\", \"trans\": \"alignment\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"æ ·æœ¬\", \"pinyin\": \"yÃ ng bÄ›n\", \"trans\": \"sample\"},\n    {\"word\": \"æ¶µç›–\", \"pinyin\": \"hÃ¡n gÃ i\", \"trans\": \"cover\"},\n    {\"word\": \"å¤šæ ·åŒ–\", \"pinyin\": \"duÅ yÃ ng huÃ \", \"trans\": \"diversified\"},\n    {\"word\": \"å¤æ‚\", \"pinyin\": \"fÃ¹ zÃ¡\", \"trans\": \"complex\"},\n    {\"word\": \"æ ¼å¼\", \"pinyin\": \"gÃ© shÃ¬\", \"trans\": \"format\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­ shÄ“ng\", \"trans\": \"improve\"},\n    {\"word\": \"äººå·¥æ ‡æ³¨\", \"pinyin\": \"rÃ©n gÅng biÄo zhÃ¹\", \"trans\": \"manual annotation\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"ç›‘ç£å¾®è°ƒ\", \"pinyin\": \"jiÃ n dÅ« wÄ“i tiÃ¡o\", \"trans\": \"supervised fine-tuning\"},\n    {\"word\": \"ç›´æ¥åå¥½ä¼˜åŒ–\", \"pinyin\": \"zhÃ­ jiÄ“ piÄn hÃ o yÅu huÃ \", \"trans\": \"direct preference optimization\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"ä¿æŒ\", \"pinyin\": \"bÇo chÃ­\", \"trans\": \"maintain\"},\n    {\"word\": \"æ ‡å‡†\", \"pinyin\": \"biÄo zhÇ”n\", \"trans\": \"standard\"},\n    {\"word\": \"VQA\", \"pinyin\": \"VQA\", \"trans\": \"Visual Question Answering\"},\n    {\"word\": \"æ£€æŸ¥ç‚¹\", \"pinyin\": \"jiÇn chÃ¡ diÇn\", \"trans\": \"checkpoint\"}\n]",
        "trans": "Recent open-source multimodal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, but there remains a significant gap in aligning with human preferences. This paper introduces OmniAlign-V, a comprehensive dataset containing 200,000 high-quality training samples that cover diverse images, complex questions, and various answer formats to improve the alignment of MLLMs with human preferences. We also present MM-AlignBench, a specially designed, human-annotated benchmark for evaluating the alignment of MLLMs with human values. Experimental results demonstrate that fine-tuning MLLMs using supervised fine-tuning (SFT) or direct preference optimization (DPO) significantly improves human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our dataset, benchmark, code, and checkpoints are available at https://github.com/PhoenixZ810/OmniAlign-V.",
        "update_ts": "2025-02-26 09:11"
    }
}