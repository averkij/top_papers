{
    "date": {
        "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 19",
        "zh": "9æœˆ19æ—¥"
    },
    "time_utc": "2025-09-21 06:33",
    "weekday": 4,
    "issue_id": 6002,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.15221",
            "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
            "url": "https://huggingface.co/papers/2509.15221",
            "abstract": "ScaleCUA, a large-scale dataset and model for computer use agents, achieves state-of-the-art performance across multiple platforms and tasks by leveraging data-driven scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.",
            "score": 92,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "6081b7f60504d5ef",
            "authors": [
                "Zhaoyang Liu",
                "JingJing Xie",
                "Zichen Ding",
                "Zehao Li",
                "Bowen Yang",
                "Zhenyu Wu",
                "Xuehui Wang",
                "Qiushi Sun",
                "Shi Liu",
                "Weiyun Wang",
                "Shenglong Ye",
                "Qingyun Li",
                "Zeyue Tian",
                "Gen Luo",
                "Xiangyu Yue",
                "Biqing Qi",
                "Kai Chen",
                "Bowen Zhou",
                "Yu Qiao",
                "Qifeng Chen",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15221.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#cv"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ScaleCUA: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ScaleCUA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ScaleCUA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 6 Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ 3 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ScaleCUA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Computer Use Agents with ScaleCUA",
                    "desc": "ScaleCUA is a groundbreaking dataset and model designed for computer use agents (CUAs) that enhances their ability to operate graphical user interfaces (GUIs) autonomously. By integrating a large-scale dataset that covers six operating systems and three task domains, ScaleCUA significantly improves the performance of CUAs through data-driven scaling techniques. The model demonstrates impressive results, surpassing previous benchmarks and achieving state-of-the-art performance on various tasks. This work not only advances the capabilities of CUAs but also provides open-source resources to foster further research in the field."
                },
                "zh": {
                    "title": "æ•°æ®é©±åŠ¨æ‰©å±•ï¼ŒåŠ©åŠ›è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„æœªæ¥",
                    "desc": "ScaleCUAæ˜¯ä¸€ä¸ªå¤§å‹æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¸“ä¸ºè®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå¹³å°å’Œä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å…­ç§æ“ä½œç³»ç»Ÿå’Œä¸‰ä¸ªä»»åŠ¡é¢†åŸŸï¼Œé€šè¿‡å°†è‡ªåŠ¨åŒ–ä»£ç†ä¸äººç±»ä¸“å®¶ç»“åˆçš„é—­ç¯æµç¨‹æ„å»ºè€Œæˆã€‚ç»è¿‡å¤§è§„æ¨¡æ•°æ®è®­ç»ƒï¼ŒScaleCUAèƒ½å¤Ÿåœ¨ä¸åŒå¹³å°ä¸Šæ— ç¼æ“ä½œï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—æ˜¾è‘—æå‡ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ•°æ®é©±åŠ¨æ‰©å±•åœ¨é€šç”¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15207",
            "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
            "url": "https://huggingface.co/papers/2509.15207",
            "abstract": "FlowRL enhances LLM reinforcement learning by matching the full reward distribution through flow balancing, improving diversity and performance over reward-maximizing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.",
            "score": 81,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "e1295c2f57ad673c",
            "authors": [
                "Xuekai Zhu",
                "Daixuan Cheng",
                "Dinghuai Zhang",
                "Hengli Li",
                "Kaiyan Zhang",
                "Che Jiang",
                "Youbang Sun",
                "Ermo Hua",
                "Yuxin Zuo",
                "Xingtai Lv",
                "Qizheng Zhang",
                "Lin Chen",
                "Fanghao Shao",
                "Bo Xue",
                "Yunchong Song",
                "Zhenjie Yang",
                "Ganqu Cui",
                "Ning Ding",
                "Jianfeng Gao",
                "Xiaodong Liu",
                "Bowen Zhou",
                "Hongyuan Mei",
                "Zhouhan Lin"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University",
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "Toyota Technological Institute at Chicago",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15207.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#rl"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "FlowRL: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlowRL - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, FlowRL ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FlowRL."
                },
                "en": {
                    "title": "FlowRL: Enhancing LLMs through Reward Distribution Matching",
                    "desc": "FlowRL is a novel approach in reinforcement learning for large language models (LLMs) that focuses on matching the entire reward distribution rather than just maximizing rewards. By using flow balancing, it addresses the issue of over-optimization seen in traditional methods like PPO and GRPO, which often ignore less frequent but valid reasoning paths. This method transforms scalar rewards into a normalized target distribution and minimizes the reverse KL divergence, promoting diverse exploration and better reasoning. Experiments show that FlowRL significantly outperforms existing methods on math and code reasoning tasks, demonstrating the importance of reward distribution-matching for effective learning."
                },
                "zh": {
                    "title": "FlowRLï¼šé€šè¿‡æµå¹³è¡¡å®ç°å¤šæ ·åŒ–æ¨ç†",
                    "desc": "FlowRLæ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡æµå¹³è¡¡åŒ¹é…å®Œæ•´çš„å¥–åŠ±åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å•çº¯æœ€å¤§åŒ–å¥–åŠ±ã€‚ä¼ ç»Ÿçš„å¥–åŠ±æœ€å¤§åŒ–æ–¹æ³•ï¼ˆå¦‚PPOå’ŒGRPOï¼‰å¾€å¾€è¿‡åº¦ä¼˜åŒ–ä¸»è¦çš„å¥–åŠ±ä¿¡å·ï¼Œå¿½è§†äº†è¾ƒå°‘å‡ºç°ä½†æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ï¼Œä»è€Œé™ä½äº†å¤šæ ·æ€§ã€‚FlowRLé€šè¿‡å¯å­¦ä¹ çš„åˆ†åŒºå‡½æ•°å°†æ ‡é‡å¥–åŠ±è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„ç›®æ ‡åˆ†å¸ƒï¼Œå¹¶æœ€å°åŒ–ç­–ç•¥ä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„åå‘KLæ•£åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowRLåœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†10.0%ï¼Œåœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¹Ÿæ›´ä¸ºä¼˜è¶Šï¼Œå¼ºè°ƒäº†åŒ¹é…å¥–åŠ±åˆ†å¸ƒåœ¨LLMå¼ºåŒ–å­¦ä¹ ä¸­å®ç°é«˜æ•ˆæ¢ç´¢å’Œå¤šæ ·åŒ–æ¨ç†çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14760",
            "title": "Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration",
            "url": "https://huggingface.co/papers/2509.14760",
            "abstract": "Align3, a lightweight method using Test-Time Deliberation, enhances specification alignment in large language models across diverse scenarios with minimal overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.",
            "score": 47,
            "issue_id": 5978,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "e9e06e0e548bde84",
            "authors": [
                "Haoran Zhang",
                "Yafu Li",
                "Xuyang Hu",
                "Dongrui Liu",
                "Zhilin Wang",
                "Bo Li",
                "Yu Cheng"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "University of Illinois at Urbana-Champaign",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14760.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#architecture",
                    "#benchmark",
                    "#training",
                    "#alignment"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Align3: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Align3, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Test-Time Deliberation Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'specification alignment' - ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼, ÑÑ†ĞµĞ½Ğ°Ñ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SpecBench, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 5 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², 103 ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ 1500 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Test-Time Deliberation Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼, Ğ° Align3 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Aligning Language Models with Dynamic Specifications Efficiently",
                    "desc": "The paper introduces Align3, a method that improves how large language models (LLMs) align with specific behavioral and safety requirements in various scenarios. It uses Test-Time Deliberation (TTD) to help models reflect on and adjust their responses according to dynamic specifications. The authors also present SpecBench, a benchmark designed to evaluate how well models adhere to these specifications across multiple scenarios and prompts. The findings demonstrate that Align3 not only enhances alignment but also balances safety and helpfulness with minimal computational cost."
                },
                "zh": {
                    "title": "Align3ï¼šè½»é‡çº§çš„è§„èŒƒå¯¹é½æ–¹æ³•",
                    "desc": "Align3æ˜¯ä¸€ç§è½»é‡çº§çš„æ–¹æ³•ï¼Œåˆ©ç”¨æµ‹è¯•æ—¶æ·±æ€ï¼ˆTest-Time Deliberationï¼‰æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹çš„è§„èŒƒå¯¹é½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…³æ³¨æ¨¡å‹åœ¨åŠ¨æ€ã€ç‰¹å®šåœºæ™¯ä¸‹éµå¾ªç”¨æˆ·æˆ–ç»„ç»‡å®šåˆ¶çš„è¡Œä¸ºå’Œå®‰å…¨è§„èŒƒçš„èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥å±‚æ¬¡åæ€å’Œä¿®æ­£ï¼ŒAlign3èƒ½å¤Ÿåœ¨è§„èŒƒè¾¹ç•Œä¸Šè¿›è¡Œæ¨ç†ï¼Œç¡®ä¿æ¨¡å‹çš„è¾“å‡ºç¬¦åˆé¢„æœŸã€‚æˆ‘ä»¬è¿˜æå‡ºäº†SpecBenchï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†ï¼Œç”¨äºæµ‹é‡è§„èŒƒå¯¹é½ï¼Œæ¶µç›–äº†å¤šç§åœºæ™¯å’Œè§„èŒƒï¼Œå¸®åŠ©è¯†åˆ«å¯¹é½å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15194",
            "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
            "url": "https://huggingface.co/papers/2509.15194",
            "abstract": "EVOL-RL, a label-free reinforcement learning method, enhances large language models by balancing stability and variation, preventing entropy collapse and improving generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.",
            "score": 29,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "271ee6d47770b19f",
            "authors": [
                "Yujun Zhou",
                "Zhenwen Liang",
                "Haolin Liu",
                "Wenhao Yu",
                "Kishan Panaganti",
                "Linfeng Song",
                "Dian Yu",
                "Xiangliang Zhang",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "University of Notre Dame",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15194.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¼ĞµÑ‚Ğ¾Ğº",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ EVOL-RL Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. EVOL-RL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ·Ğ° Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EVOL-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…."
                },
                "en": {
                    "title": "Evolving Language Models with Label-Free Reinforcement Learning",
                    "desc": "EVOL-RL is a novel reinforcement learning method designed to enhance large language models without relying on labeled data. It addresses the problem of entropy collapse, where models become less diverse and informative over time. By combining stability through majority voting with a novelty-aware reward system, EVOL-RL encourages exploration and variation in model responses. This approach not only improves generalization across different tasks but also significantly boosts performance metrics compared to existing methods."
                },
                "zh": {
                    "title": "EVOL-RLï¼šæ— æ ‡ç­¾å¼ºåŒ–å­¦ä¹ çš„è¿›åŒ–ä¹‹è·¯",
                    "desc": "EVOL-RLæ˜¯ä¸€ç§æ— æ ‡ç­¾çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å®šæ€§å’Œå¤šæ ·æ€§ã€‚å®ƒé€šè¿‡é˜²æ­¢ç†µå´©æºƒï¼Œä¿æŒç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒEVOL-RLç»“åˆäº†ç¨³å®šæ€§å’Œå˜åŒ–æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ²¡æœ‰å¤–éƒ¨æ ‡ç­¾çš„æƒ…å†µä¸‹è‡ªæˆ‘æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEVOL-RLåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„æ— æ ‡ç­¾å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15185",
            "title": "Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation",
            "url": "https://huggingface.co/papers/2509.15185",
            "abstract": "Self-guided Training for AutoRegressive models (ST-AR) enhances image understanding and generation quality in autoregressive models by addressing key visual semantics challenges through self-supervised objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.",
            "score": 26,
            "issue_id": 5976,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "3da8f7302a159d5f",
            "authors": [
                "Xiaoyu Yue",
                "Zidong Wang",
                "Yuqing Wang",
                "Wenlong Zhang",
                "Xihui Liu",
                "Wanli Ouyang",
                "Lei Bai",
                "Luping Zhou"
            ],
            "affiliations": [
                "Chinese University of Hong Kong",
                "Shanghai AI Laboratory",
                "University of Hong Kong",
                "University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15185.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - Self-guided Training for AutoRegressive models (ST-AR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ¼ĞµÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ST-AR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Image Generation with Self-Guided Training",
                    "desc": "The paper introduces Self-guided Training for AutoRegressive models (ST-AR), a new framework aimed at improving image understanding and generation in autoregressive models. It identifies three main challenges in applying next-token prediction to visual data: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. By incorporating self-supervised objectives during training, ST-AR effectively addresses these challenges, enhancing the model's ability to learn high-level visual semantics. The results show significant improvements in image generation quality, with FID scores increasing by approximately 42% and 49% for different model versions without using pre-trained representations."
                },
                "zh": {
                    "title": "è‡ªæŒ‡å¯¼è®­ç»ƒæå‡å›¾åƒç”Ÿæˆä¸ç†è§£",
                    "desc": "è‡ªæŒ‡å¯¼è®­ç»ƒï¼ˆST-ARï¼‰é€šè¿‡è‡ªç›‘ç£ç›®æ ‡è§£å†³äº†è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆä¸­çš„å…³é”®è§†è§‰è¯­ä¹‰æŒ‘æˆ˜ï¼Œä»è€Œæå‡äº†å›¾åƒç†è§£å’Œç”Ÿæˆè´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜è´¨é‡çš„è§†è§‰è¡¨ç¤ºåœ¨å›¾åƒç”Ÿæˆä¸­è‡³å…³é‡è¦ï¼Œè€Œè‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç†è§£æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†å°†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹èŒƒå¼åº”ç”¨äºè§†è§‰é¢†åŸŸçš„æœºåˆ¶ï¼Œå¹¶è¯†åˆ«å‡ºå½±å“é«˜å±‚è§†è§‰è¯­ä¹‰å­¦ä¹ çš„ä¸‰ä¸ªå…³é”®ç‰¹æ€§ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥è‡ªç›‘ç£ç›®æ ‡ï¼ŒST-ARæ˜¾è‘—æé«˜äº†è‡ªå›å½’æ¨¡å‹çš„å›¾åƒç†è§£èƒ½åŠ›ï¼Œå¹¶æ”¹å–„äº†ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13160",
            "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
            "url": "https://huggingface.co/papers/2509.13160",
            "abstract": "FinSearchComp is an open-source benchmark for evaluating financial search and reasoning capabilities of end-to-end agents, featuring realistic tasks and professional annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Search has emerged as core infrastructure for LLM-based agents and is widely viewed as critical on the path toward more general intelligence. Finance is a particularly demanding proving ground: analysts routinely conduct complex, multi-step searches over time-sensitive, domain-specific data, making it ideal for assessing both search proficiency and knowledge-grounded reasoning. Yet no existing open financial datasets evaluate data searching capability of end-to-end agents, largely because constructing realistic, complicated tasks requires deep financial expertise and time-sensitive data is hard to evaluate. We present FinSearchComp, the first fully open-source agent benchmark for realistic, open-domain financial search and reasoning. FinSearchComp comprises three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and Complex Historical Investigation -- closely reproduce real-world financial analyst workflows. To ensure difficulty and reliability, we engage 70 professional financial experts for annotation and implement a rigorous multi-stage quality-assurance pipeline. The benchmark includes 635 questions spanning global and Greater China markets, and we evaluate 21 models (products) on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy. DouBao (web) leads on the Greater China subset. Experimental analyses show that equipping agents with web search and financial plugins substantially improves results on FinSearchComp, and the country origin of models and tools impact performance significantly.By aligning with realistic analyst tasks and providing end-to-end evaluation, FinSearchComp offers a professional, high-difficulty testbed for complex financial search and reasoning.",
            "score": 24,
            "issue_id": 5975,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "a1faa8bf123c24e6",
            "authors": [
                "Liang Hu",
                "Jianpeng Jiao",
                "Jiashuo Liu",
                "Yanle Ren",
                "Zhoufutu Wen",
                "Kaiyuan Zhang",
                "Xuanliang Zhang",
                "Xiang Gao",
                "Tianci He",
                "Fei Hu",
                "Yali Liao",
                "Zaiyuan Wang",
                "Chenghao Yang",
                "Qianyu Yang",
                "Mingren Yin",
                "Zhiyuan Zeng",
                "Ge Zhang",
                "Xinyi Zhang",
                "Xiying Zhao",
                "Zhenwei Zhu",
                "Hongseok Namkoong",
                "Wenhao Huang",
                "Yuwen Tang"
            ],
            "affiliations": [
                "ByteDance",
                "Columbia Business School"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13160.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#open_source",
                    "#agents",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "FinSearchComp: Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "FinSearchComp - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²: Ğ¿Ğ¾Ğ¸ÑĞº Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ¢ĞµÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 635 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ Ñ€Ñ‹Ğ½ĞºĞ°Ğ¼, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ 70 Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° FinSearchComp."
                },
                "en": {
                    "title": "FinSearchComp: Benchmarking AI in Financial Search and Reasoning",
                    "desc": "FinSearchComp is an innovative benchmark designed to assess the financial search and reasoning abilities of end-to-end AI agents. It includes realistic tasks that mimic the workflows of financial analysts, such as fetching time-sensitive data and conducting historical investigations. The benchmark is supported by professional annotations from financial experts, ensuring high-quality and relevant evaluation criteria. By testing various models on this benchmark, researchers can better understand the effectiveness of AI in handling complex financial queries and improve their performance through enhanced search capabilities."
                },
                "zh": {
                    "title": "é‡‘èæœç´¢ä¸æ¨ç†çš„ä¸“ä¸šåŸºå‡†æµ‹è¯•",
                    "desc": "FinSearchCompæ˜¯ä¸€ä¸ªå¼€æºåŸºå‡†ï¼Œç”¨äºè¯„ä¼°ç«¯åˆ°ç«¯æ™ºèƒ½ä½“åœ¨é‡‘èæœç´¢å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«ä¸‰ä¸ªä»»åŠ¡ï¼Œæ¨¡æ‹ŸçœŸå®çš„é‡‘èåˆ†æå¸ˆå·¥ä½œæµç¨‹ï¼Œç¡®ä¿ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¯é æ€§ã€‚é€šè¿‡ä¸70ä½ä¸“ä¸šé‡‘èä¸“å®¶åˆä½œè¿›è¡Œæ ‡æ³¨ï¼ŒFinSearchCompæä¾›äº†635ä¸ªé—®é¢˜ï¼Œæ¶µç›–å…¨çƒåŠå¤§ä¸­åå¸‚åœºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆç½‘ç»œæœç´¢å’Œé‡‘èæ’ä»¶çš„æ™ºèƒ½ä½“åœ¨FinSearchCompä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚é‡‘èæœç´¢å’Œæ¨ç†ä¸­çš„ä¸“ä¸šæ€§å’Œé«˜éš¾åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15130",
            "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance",
            "url": "https://huggingface.co/papers/2509.15130",
            "abstract": "WorldForge, a training-free framework, enhances video diffusion models with precise motion control and photorealistic content generation through recursive refinement, flow-gated latent fusion, and dual-path self-corrective guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.",
            "score": 20,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "f91495ad752f8344",
            "authors": [
                "Chenxi Song",
                "Yanming Yang",
                "Tong Zhao",
                "Ruibo Li",
                "Chi Zhang"
            ],
            "affiliations": [
                "AGI Lab, School of Engineering, Westlake University, Hangzhou, China",
                "The College of Computing and Data Science, Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15130.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#inference",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "WorldForge: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "WorldForge - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. WorldForge Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "WorldForge: Training-Free Control for Photorealistic Video Synthesis",
                    "desc": "WorldForge is a novel framework that enhances video diffusion models without the need for retraining. It introduces three key modules: Intra-Step Recursive Refinement for optimizing predictions during inference, Flow-Gated Latent Fusion for separating motion from appearance, and Dual-Path Self-Corrective Guidance to correct trajectory drift. These components work together to provide precise motion control and generate photorealistic content. The framework demonstrates significant improvements in realism and consistency, making it a valuable tool for controllable video synthesis."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒçš„è§†é¢‘åˆæˆæ–°èŒƒå¼",
                    "desc": "WorldForgeæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡é€’å½’ä¼˜åŒ–ã€æµé—¨æ§æ½œåœ¨èåˆå’ŒåŒè·¯å¾„è‡ªæˆ‘æ ¡æ­£æŒ‡å¯¼ï¼Œå¢å¼ºäº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿åŠ¨æ§åˆ¶å’ŒçœŸå®æ„Ÿå†…å®¹ç”Ÿæˆã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å¯æ§æ€§å’Œå‡ ä½•ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ï¼Œé¿å…äº†é‡æ–°è®­ç»ƒå¸¦æ¥çš„çŸ¥è¯†é€€åŒ–å’Œé«˜è®¡ç®—æˆæœ¬ã€‚é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥é€’å½’ä¼˜åŒ–æœºåˆ¶ï¼ŒWorldForgeèƒ½å¤Ÿç²¾ç¡®åœ°æ³¨å…¥è¿åŠ¨è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ„Ÿã€è½¨è¿¹ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15212",
            "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
            "url": "https://huggingface.co/papers/2509.15212",
            "abstract": "RynnVLA-001, a vision-language-action model, uses a two-stage pretraining approach and ActionVAE to achieve superior performance on robotics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.",
            "score": 18,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "0f4ee15179f6aeef",
            "authors": [
                "Yuming Jiang",
                "Siteng Huang",
                "Shengke Xue",
                "Yaxi Zhao",
                "Jun Cen",
                "Sicong Leng",
                "Kehan Li",
                "Jiayan Guo",
                "Kexiang Wang",
                "Mingxiu Chen",
                "Fan Wang",
                "Deli Zhao",
                "Xin Li"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15212.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#optimization",
                    "#training",
                    "#cv",
                    "#games",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "RynnVLA-001: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸",
                    "desc": "RynnVLA-001 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ ActionVAE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Robotics with RynnVLA-001: A Vision-Language-Action Breakthrough!",
                    "desc": "RynnVLA-001 is a vision-language-action model designed to improve robotics tasks through a two-stage pretraining approach. The first stage involves Ego-Centric Video Generative Pretraining, where the model learns to predict future video frames based on initial frames and language instructions using a large dataset of manipulation videos. The second stage, Human-Centric Trajectory-Aware Modeling, enhances the model's ability to predict keypoint trajectories, linking visual predictions with action outcomes. Additionally, the introduction of ActionVAE helps to simplify the action representation by compressing action sequences into compact latent embeddings, leading to better performance on robotics tasks compared to existing models."
                },
                "zh": {
                    "title": "RynnVLA-001ï¼šæå‡æœºå™¨äººä»»åŠ¡çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
                    "desc": "RynnVLA-001æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µé¢„è®­ç»ƒæ–¹æ³•å’ŒActionVAEæ¥æå‡æœºå™¨äººä»»åŠ¡çš„è¡¨ç°ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆé¢„è®­ç»ƒï¼Œåˆ©ç”¨1200ä¸‡æ®µè‡ªæˆ‘ä¸­å¿ƒçš„æ“ä½œè§†é¢‘è®­ç»ƒå›¾åƒåˆ°è§†é¢‘æ¨¡å‹ï¼Œä»¥æ ¹æ®åˆå§‹å¸§å’Œè¯­è¨€æŒ‡ä»¤é¢„æµ‹æœªæ¥å¸§ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯ä»¥äººä¸ºä¸­å¿ƒçš„è½¨è¿¹æ„ŸçŸ¥å»ºæ¨¡ï¼Œé€šè¿‡è”åˆé¢„æµ‹æœªæ¥å…³é”®ç‚¹è½¨è¿¹ï¼Œæœ‰æ•ˆåœ°å°†è§†è§‰å¸§é¢„æµ‹ä¸åŠ¨ä½œé¢„æµ‹ç»“åˆèµ·æ¥ã€‚æ­¤å¤–ï¼ŒActionVAEä½œä¸ºå˜åˆ†è‡ªç¼–ç å™¨ï¼Œå‹ç¼©åŠ¨ä½œåºåˆ—ä¸ºç´§å‡‘çš„æ½œåœ¨åµŒå…¥ï¼Œé™ä½äº†VLAè¾“å‡ºç©ºé—´çš„å¤æ‚æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14476",
            "title": "AToken: A Unified Tokenizer for Vision",
            "url": "https://huggingface.co/papers/2509.14476",
            "abstract": "AToken, a unified visual tokenizer, achieves high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets using a 4D transformer architecture with adversarial-free training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.",
            "score": 17,
            "issue_id": 5975,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "4904c9c747d48b89",
            "authors": [
                "Jiasen Lu",
                "Liangchen Song",
                "Mingze Xu",
                "Byeongjoo Ahn",
                "Yanjun Wang",
                "Chen Chen",
                "Afshin Dehghan",
                "Yinfei Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2509.14476.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#3d",
                    "#architecture",
                    "#video",
                    "#training",
                    "#cv",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "AToken - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ 4D-Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. AToken Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾-ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ“Ñ€Ğ°Ğ¼Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Unified Visual Tokenization for Next-Gen AI",
                    "desc": "AToken is a novel visual tokenizer that integrates high-fidelity reconstruction and semantic understanding for images, videos, and 3D assets using a 4D transformer architecture. It encodes various visual inputs into a shared latent space, allowing it to handle multiple modalities simultaneously. The model employs an adversarial-free training approach, utilizing perceptual and Gram matrix losses to ensure stable and high-quality outputs. AToken's performance is demonstrated through impressive metrics across different tasks, paving the way for advanced multimodal AI applications."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è§†è§‰æ ‡è®°ï¼Œé‡å»ºä¸ç†è§£çš„å®Œç¾ç»“åˆ",
                    "desc": "ATokenæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰æ ‡è®°å™¨ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒã€è§†é¢‘å’Œ3Dèµ„äº§ä¸­å®ç°é«˜ä¿çœŸé‡å»ºå’Œè¯­ä¹‰ç†è§£ã€‚ä¸ç°æœ‰çš„ä¸“æ³¨äºå•ä¸€æ¨¡æ€çš„æ ‡è®°å™¨ä¸åŒï¼ŒATokenå°†å¤šæ ·çš„è§†è§‰è¾“å…¥ç¼–ç åˆ°ä¸€ä¸ªå…±äº«çš„4Dæ½œåœ¨ç©ºé—´ä¸­ï¼Œä»è€Œç»Ÿä¸€äº†é‡å»ºå’Œç†è§£ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é‡‡ç”¨çº¯å˜æ¢å™¨æ¶æ„å’Œ4Dæ—‹è½¬ä½ç½®åµŒå…¥ï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„åˆ†è¾¨ç‡å’Œæ—¶é—´é•¿åº¦çš„è§†è§‰è¾“å…¥ã€‚é€šè¿‡æ— å¯¹æŠ—è®­ç»ƒç›®æ ‡ï¼Œç»“åˆæ„ŸçŸ¥æŸå¤±å’ŒGramçŸ©é˜µæŸå¤±ï¼ŒATokenåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†ä¸‹ä¸€ä»£å¤šæ¨¡æ€AIç³»ç»Ÿçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14638",
            "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks",
            "url": "https://huggingface.co/papers/2509.14638",
            "abstract": "MultiEdit, a comprehensive dataset with over 107K high-quality image editing samples, improves performance on sophisticated editing tasks using a novel pipeline with multi-modal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at https://huggingface.co/datasets/inclusionAI/MultiEdit.",
            "score": 7,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "35e0066dc539ba98",
            "authors": [
                "Mingsong Li",
                "Lin Liu",
                "Hongjun Wang",
                "Haoxing Chen",
                "Xijun Gu",
                "Shizhan Liu",
                "Dong Gong",
                "Junbo Zhao",
                "Zhenzhong Lan",
                "Jianguo Li"
            ],
            "affiliations": [
                "Inclusion AI",
                "The University of Hong Kong",
                "University of New South Wales",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14638.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "MultiEdit: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MultiEdit - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 107 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 6 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· 18 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ 38 Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑÑ‚Ğ¸Ğ»Ñ. Ğ”Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° MultiEdit Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "MultiEdit: Elevating Image Editing with a Rich Dataset",
                    "desc": "The paper introduces MultiEdit, a new dataset designed to enhance instruction-based image editing (IBIE) methods by providing over 107,000 high-quality image editing samples. It addresses the limitations of existing datasets, which often contain noisy image-caption pairs and lack diversity in editing tasks. MultiEdit includes six challenging editing tasks and a variety of editing types, from style transfer to complex semantic operations. By utilizing multi-modal large language models for dataset construction, the paper demonstrates that fine-tuning models with MultiEdit significantly improves their performance on complex editing tasks while maintaining their effectiveness on standard benchmarks."
                },
                "zh": {
                    "title": "MultiEditï¼šæå‡å¤æ‚å›¾åƒç¼–è¾‘èƒ½åŠ›çš„å…¨æ–°æ•°æ®é›†",
                    "desc": "MultiEditæ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡107Ké«˜è´¨é‡å›¾åƒç¼–è¾‘æ ·æœ¬çš„ç»¼åˆæ•°æ®é›†ï¼Œæ—¨åœ¨æå‡å¤æ‚ç¼–è¾‘ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†6ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¼–è¾‘ä»»åŠ¡ï¼ŒåŒ…å«18ç§éé£æ ¼è½¬ç§»ç¼–è¾‘ç±»å‹å’Œ38ç§é£æ ¼è½¬ç§»æ“ä½œã€‚é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ç§æ–°é¢–çš„æ•°æ®é›†ç”Ÿæˆç®¡é“ï¼Œä»¥ç”Ÿæˆè§†è§‰é€‚åº”çš„ç¼–è¾‘æŒ‡ä»¤å¹¶åˆ¶ä½œé«˜ä¿çœŸç¼–è¾‘å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MultiEditè®­ç»ƒé›†å¾®è°ƒåŸºç¡€å¼€æºæ¨¡å‹æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤æ‚ç¼–è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™äº†å…¶åœ¨æ ‡å‡†ç¼–è¾‘åŸºå‡†ä¸Šçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14233",
            "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments",
            "url": "https://huggingface.co/papers/2509.14233",
            "abstract": "Apertus is a suite of open large language models that ensure data compliance and multilingual representation through ethical data sourcing, the Goldfish objective, and comprehensive artifact release.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting robots.txt exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.",
            "score": 7,
            "issue_id": 5988,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "a50d6618d5feca99",
            "authors": [
                "Alejandro HernÃ¡ndez-Cano",
                "Alexander HÃ¤gele",
                "Allen Hao Huang",
                "Angelika Romanou",
                "Antoni-Joan Solergibert",
                "Barna Pasztor",
                "Bettina Messmer",
                "Dhia Garbaya",
                "Eduard Frank Äurech",
                "Ido Hakimi",
                "Juan GarcÃ­a Giraldo",
                "Mete Ismayilzada",
                "Negar Foroutan",
                "Skander Moalla",
                "Tiancheng Chen",
                "Vinko SabolÄec",
                "Yixuan Xu",
                "Michael Aerni",
                "Badr AlKhamissi",
                "Ines Altemir Marinas",
                "Mohammad Hossein Amani",
                "Matin Ansaripour",
                "Ilia Badanin",
                "Harold Benoit",
                "Emanuela Boros",
                "Nicholas Browning",
                "Fabian BÃ¶sch",
                "Maximilian BÃ¶ther",
                "Niklas Canova",
                "Camille Challier",
                "Clement Charmillot",
                "Jonathan Coles",
                "Jan Deriu",
                "Arnout Devos",
                "Lukas Drescher",
                "Daniil Dzenhaliou",
                "Maud Ehrmann",
                "Dongyang Fan",
                "Simin Fan",
                "Silin Gao",
                "Miguel Gila",
                "MarÃ­a Grandury",
                "Diba Hashemi",
                "Alexander Hoyle",
                "Jiaming Jiang",
                "Mark Klein",
                "Andrei Kucharavy",
                "Anastasiia Kucherenko",
                "Frederike LÃ¼beck",
                "Roman Machacek",
                "Theofilos Manitaras",
                "Andreas Marfurt",
                "Kyle Matoba",
                "Simon Matrenok",
                "Henrique MendoncÃ§a",
                "Fawzi Roberto Mohamed",
                "Syrielle Montariol",
                "Luca Mouchel",
                "Sven Najem-Meyer",
                "Jingwei Ni",
                "Gennaro Oliva",
                "Matteo Pagliardini",
                "Elia Palme",
                "Andrei Panferov",
                "LÃ©o Paoletti",
                "Marco Passerini",
                "Ivan Pavlov",
                "Auguste Poiroux",
                "Kaustubh Ponkshe",
                "Nathan Ranchin",
                "Javi Rando",
                "Mathieu Sauser",
                "Jakhongir Saydaliev",
                "Muhammad Ali Sayfiddinov",
                "Marian Schneider",
                "Stefano Schuppli",
                "Marco Scialanga",
                "Andrei Semenov",
                "Kumar Shridhar",
                "Raghav Singhal",
                "Anna Sotnikova",
                "Alexander Sternfeld",
                "Ayush Kumar Tarun",
                "Paul Teiletche",
                "Jannis Vamvas",
                "Xiaozhe Yao",
                "Hao Zhao Alexander Ilic",
                "Ana Klimovic",
                "Andreas Krause",
                "Caglar Gulcehre",
                "David Rosenthal",
                "Elliott Ash",
                "Florian TramÃ¨r",
                "Joost VandeVondele",
                "Livio Veraldi",
                "Martin Rajman",
                "Thomas Schulthess",
                "Torsten Hoefler",
                "Antoine Bosselut",
                "Martin Jaggi",
                "Imanol Schlag"
            ],
            "affiliations": [
                "CSCS",
                "EPFL",
                "ETH Zurich",
                "HES-SO Valais-Wallis",
                "HSLU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14233.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#open_source",
                    "#ethics",
                    "#dataset",
                    "#training",
                    "#data",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ­Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ LLM Ğ´Ğ»Ñ Ğ²ÑĞµÑ…",
                    "desc": "Apertus - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†ĞµĞ² ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒÑ Ğ½ĞµĞ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. Ğ”Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Goldfish, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑÑ‰Ğ°Ñ Ğ´Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Apertus Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1800 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Apertus: Ethical and Multilingual Large Language Models for All",
                    "desc": "Apertus is a collection of open large language models (LLMs) that focus on ethical data sourcing and multilingual capabilities. It ensures data compliance by using only openly available data and respecting content-owner rights, while also filtering out harmful content. The models employ the Goldfish objective to reduce the risk of memorization, allowing them to perform well on various tasks without recalling specific training data verbatim. With training on a vast amount of multilingual data, Apertus achieves competitive results on multilingual benchmarks and promotes transparency by releasing all development artifacts."
                },
                "zh": {
                    "title": "Apertusï¼šå¼€æ”¾ä¸åˆè§„çš„å¤šè¯­è¨€æ¨¡å‹",
                    "desc": "Apertusæ˜¯ä¸€å¥—å¼€æ”¾çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å½“å‰å¼€æ”¾æ¨¡å‹ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ•°æ®åˆè§„æ€§å’Œå¤šè¯­è¨€è¡¨ç¤ºé—®é¢˜ã€‚ä¸è®¸å¤šä¹‹å‰çš„æ¨¡å‹ä¸åŒï¼ŒApertusæ¨¡å‹ä»…åœ¨å…¬å¼€å¯ç”¨çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶éµå¾ªrobots.txtçš„æ’é™¤è§„åˆ™ï¼Œè¿‡æ»¤æ‰ä¸å…è®¸çš„ã€æœ‰æ¯’çš„å’Œä¸ªäººå¯è¯†åˆ«çš„ä¿¡æ¯ã€‚ä¸ºäº†å‡å°‘è®°å¿†é£é™©ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†é‡‘é±¼ç›®æ ‡ï¼Œå¼ºçƒˆæŠ‘åˆ¶é€å­—å›å¿†æ•°æ®ï¼ŒåŒæ—¶ä¿æŒä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚Apertusæ¨¡å‹åœ¨å¤šè¯­è¨€è¦†ç›–æ–¹é¢ä¹Ÿæœ‰æ‰€æ‰©å±•ï¼Œä½¿ç”¨æ¥è‡ª1800å¤šç§è¯­è¨€çš„15Tæ ‡è®°è¿›è¡Œè®­ç»ƒï¼Œçº¦40%çš„é¢„è®­ç»ƒæ•°æ®åˆ†é…ç»™éè‹±è¯­å†…å®¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15178",
            "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding",
            "url": "https://huggingface.co/papers/2509.15178",
            "abstract": "A zero-shot framework using multimodal large language models for spatio-temporal video grounding employs decomposed spatio-temporal highlighting and temporal-augmented assembling strategies to improve grounding accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as grounding tokens, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (e.g., attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
            "score": 5,
            "issue_id": 5976,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "f4b09bebd69f88b8",
            "authors": [
                "Zaiquan Yang",
                "Yuhao Liu",
                "Gerhard Hancke",
                "Rynson W. H. Lau"
            ],
            "affiliations": [
                "Department of Computer Science, City University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15178.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (STVG) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±ĞµĞ·ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½ÑƒÑ (zero-shot) ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ (DSTH) Ğ¸ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ (TAS). DSTH Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ğ¼ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ LRA Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ…. TAS ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾-Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Video Grounding with Multimodal Language Models",
                    "desc": "This paper presents a zero-shot framework for spatio-temporal video grounding (STVG) using multimodal large language models (MLLMs). The authors identify that MLLMs can dynamically assign grounding tokens but often struggle with integrating all relevant cues from text queries. To address this, they introduce two innovative strategies: decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS), which enhance the model's reasoning capabilities. The proposed methods improve grounding accuracy by effectively separating and utilizing attributes and actions from queries while ensuring temporal consistency in predictions."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ¨¡å‹åŠ©åŠ›æ—¶ç©ºè§†é¢‘ç²¾å‡†å®šä½",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶-shotæ¡†æ¶ï¼Œç”¨äºæ—¶ç©ºè§†é¢‘å®šä½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬æŸ¥è¯¢æ—¶ï¼Œèƒ½å¤ŸåŠ¨æ€åˆ†é…ç‰¹å®šçš„æ ‡è®°æ¥è¿›è¡Œå®šä½ï¼Œä½†åœ¨æ•´åˆæ–‡æœ¬ä¸­çš„çº¿ç´¢æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡å¼•å…¥äº†åˆ†è§£æ—¶ç©ºé«˜äº®å’Œæ—¶é—´å¢å¼ºç»„è£…ç­–ç•¥ï¼Œä»¥æé«˜å®šä½çš„å‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›åˆ›æ–°æ–¹æ³•ï¼Œç ”ç©¶å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10397",
            "title": "RecoWorld: Building Simulated Environments for Agentic Recommender\n  Systems",
            "url": "https://huggingface.co/papers/2509.10397",
            "abstract": "RecoWorld is a simulated environment for agentic recommender systems that uses a dual-view architecture with user and recommender interactions, leveraging LLMs and multi-turn RL to enhance user retention and engagement.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RecoWorld, a blueprint for building simulated environments tailored to agentic recommender systems. Such environments give agents a proper training space where they can learn from errors without impacting real users. RecoWorld distinguishes itself with a dual-view architecture: a simulated user and an agentic recommender engage in multi-turn interactions aimed at maximizing user retention. The user simulator reviews recommended items, updates its mindset, and when sensing potential user disengagement, generates reflective instructions. The agentic recommender adapts its recommendations by incorporating these user instructions and reasoning traces, creating a dynamic feedback loop that actively engages users. This process leverages the exceptional reasoning capabilities of modern LLMs. We explore diverse content representations within the simulator, including text-based, multimodal, and semantic ID modeling, and discuss how multi-turn RL enables the recommender to refine its strategies through iterative interactions. RecoWorld also supports multi-agent simulations, allowing creators to simulate the responses of targeted user populations. It marks an important first step toward recommender systems where users and agents collaboratively shape personalized information streams. We envision new interaction paradigms where \"user instructs, recommender responds,\" jointly optimizing user retention and engagement.",
            "score": 5,
            "issue_id": 5986,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 12",
                "zh": "9æœˆ12æ—¥"
            },
            "hash": "071b32228f73fa1f",
            "authors": [
                "Fei Liu",
                "Xinyu Lin",
                "Hanchao Yu",
                "Mingyuan Wu",
                "Jianyu Wang",
                "Qiang Zhang",
                "Zhuokai Zhao",
                "Yinglong Xia",
                "Yao Zhang",
                "Weiwei Li",
                "Mingze Gao",
                "Qifan Wang",
                "Lizhu Zhang",
                "Benyu Zhang",
                "Xiangjun Fan"
            ],
            "affiliations": [
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10397.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#games",
                    "#reasoning",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "RecoWorld: Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "RecoWorld - ÑÑ‚Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒĞ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. RecoWorld Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "RecoWorld: Enhancing User Engagement through Dynamic Recommender Interactions",
                    "desc": "RecoWorld is a simulated environment designed for training agentic recommender systems, allowing them to learn from mistakes without affecting real users. It features a dual-view architecture where a simulated user interacts with the recommender in multi-turn dialogues to enhance user retention. The user simulator provides feedback by reviewing recommendations and generating instructions when it detects disengagement, which the recommender uses to adjust its suggestions. This setup utilizes large language models (LLMs) and multi-turn reinforcement learning (RL) to create a dynamic feedback loop that fosters user engagement and optimizes personalized content delivery."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ¨èç³»ç»Ÿçš„æ–°çºªå…ƒï¼šç”¨æˆ·ä¸æ¨èè€…çš„åä½œ",
                    "desc": "RecoWorldæ˜¯ä¸€ä¸ªä¸ºæ™ºèƒ½æ¨èç³»ç»Ÿè®¾è®¡çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œé‡‡ç”¨åŒè§†è§’æ¶æ„ï¼Œä¸“æ³¨äºç”¨æˆ·ä¸æ¨èè€…ä¹‹é—´çš„äº’åŠ¨ã€‚è¯¥ç¯å¢ƒå…è®¸æ™ºèƒ½ä½“åœ¨ä¸å½±å“çœŸå®ç”¨æˆ·çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡é”™è¯¯å­¦ä¹ æ¥æå‡æ€§èƒ½ã€‚ç”¨æˆ·æ¨¡æ‹Ÿå™¨ä¼šè¯„ä¼°æ¨èé¡¹ç›®å¹¶æ›´æ–°å…¶æ€ç»´ï¼Œå½“æ„ŸçŸ¥åˆ°ç”¨æˆ·å¯èƒ½ disengagement æ—¶ï¼Œä¼šç”Ÿæˆåæ€æŒ‡ä»¤ã€‚æ™ºèƒ½æ¨èè€…åˆ™æ ¹æ®è¿™äº›æŒ‡ä»¤å’Œæ¨ç†è½¨è¿¹è°ƒæ•´æ¨èï¼Œå½¢æˆä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯ï¼Œä»è€Œå¢å¼ºç”¨æˆ·çš„å‚ä¸åº¦å’Œç•™å­˜ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09307",
            "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
            "url": "https://huggingface.co/papers/2509.09307",
            "abstract": "MatCha is a benchmark for evaluating the performance of multimodal large language models in understanding materials characterization images, revealing significant limitations compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.",
            "score": 5,
            "issue_id": 5988,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "dab6381d366ed933",
            "authors": [
                "Zhengzhao Lai",
                "Youbin Zheng",
                "Zhenyang Cai",
                "Haonan Lyu",
                "Jinpu Yang",
                "Hongqing Liang",
                "Yan Hu",
                "Benyou Wang"
            ],
            "affiliations": [
                "Northeastern University",
                "The Chinese University of Hong Kong, Shenzhen",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09307.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#science",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "MatCha: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ˜Ğ˜ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²",
                    "desc": "MatCha - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñ‹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1500 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 21 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating MLLMs in Materials Characterization",
                    "desc": "MatCha is a new benchmark designed to assess how well multimodal large language models (MLLMs) can interpret materials characterization images. It includes 1,500 expert-level questions across 21 tasks that reflect real challenges in materials science. The study shows that current MLLMs significantly underperform compared to human experts, especially in tasks requiring advanced expertise and visual understanding. This research aims to highlight the limitations of MLLMs in practical applications and encourage further advancements in the field."
                },
                "zh": {
                    "title": "MatChaï¼šææ–™è¡¨å¾å›¾åƒç†è§£çš„æ–°åŸºå‡†",
                    "desc": "MatChaæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£ææ–™è¡¨å¾å›¾åƒæ–¹é¢æ€§èƒ½çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«1500ä¸ªé—®é¢˜ï¼Œè¦æ±‚å…·å¤‡ä¸“å®¶çº§çš„é¢†åŸŸçŸ¥è¯†ï¼Œæ¶µç›–ææ–™ç ”ç©¶çš„å››ä¸ªå…³é”®é˜¶æ®µå’Œ21ä¸ªä¸åŒä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦é«˜æ°´å¹³ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚è§†è§‰æ„ŸçŸ¥çš„é—®é¢˜æ—¶ï¼Œè¡¨ç°æ˜æ˜¾ä¸å¦‚äººç±»ä¸“å®¶ã€‚MatChaçš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ç°æœ‰æ¨¡å‹åœ¨çœŸå®ææ–™è¡¨å¾åœºæ™¯ä¸­çš„é€‚åº”æ€§ä»ç„¶æœ‰é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06216",
            "title": "Agentic Software Engineering: Foundational Pillars and a Research\n  Roadmap",
            "url": "https://huggingface.co/papers/2509.06216",
            "abstract": "Agentic Software Engineering introduces a dual modality approach with human and agent collaboration, redefining software engineering processes and tools to achieve complex, goal-oriented objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic Software Engineering (SE 3.0) represents a new era where intelligent agents are tasked not with simple code generation, but with achieving complex, goal-oriented SE objectives. To harness these new capabilities while ensuring trustworthiness, we must recognize a fundamental duality within the SE field in the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE for Agents. This duality demands a radical reimagining of the foundational pillars of SE (actors, processes, tools, and artifacts) which manifest differently across each modality. We propose two purpose-built workbenches to support this vision. The Agent Command Environment (ACE) serves as a command center where humans orchestrate and mentor agent teams, handling outputs such as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The Agent Execution Environment (AEE) is a digital workspace where agents perform tasks while invoking human expertise when facing ambiguity or complex trade-offs. This bi-directional partnership, which supports agent-initiated human callbacks and handovers, gives rise to new, structured engineering activities (i.e., processes) that redefine human-AI collaboration, elevating the practice from agentic coding to true agentic software engineering. This paper presents the Structured Agentic Software Engineering (SASE) vision, outlining several of the foundational pillars for the future of SE. The paper culminates in a research roadmap that identifies a few key challenges and opportunities while briefly discussing the resulting impact of this future on SE education. Our goal is not to offer a definitive solution, but to provide a conceptual scaffold with structured vocabulary to catalyze a community-wide dialogue, pushing the SE community to think beyond its classic, human-centric tenets toward a disciplined, scalable, and trustworthy agentic future.",
            "score": 5,
            "issue_id": 5986,
            "pub_date": "2025-09-07",
            "pub_date_card": {
                "ru": "7 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 7",
                "zh": "9æœˆ7æ—¥"
            },
            "hash": "ace980da7b8fafa9",
            "authors": [
                "Ahmed E. Hassan",
                "Hao Li",
                "Dayi Lin",
                "Bram Adams",
                "Tse-Hsun Chen",
                "Yutaro Kashiwa",
                "Dong Qiu"
            ],
            "affiliations": [
                "Concordia University, Canada",
                "Huawei Canada, Canada",
                "Nara Institute of Science and Technology, Japan",
                "Queens University, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06216.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ğ¶Ğ¸Ğ½Ğ¸Ñ€Ğ¸Ğ½Ğ³: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ¶Ğ¸Ğ½Ğ¸Ñ€Ğ¸Ğ½Ğ³Ğ° (SE 3.0), Ğ³Ğ´Ğµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ, Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ SE Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ SE Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹: Agent Command Environment (ACE) Ğ´Ğ»Ñ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Agent Execution Environment (AEE) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰ĞµĞ¹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Software Engineering with Human-Agent Collaboration",
                    "desc": "Agentic Software Engineering (SE 3.0) introduces a collaborative framework where humans and intelligent agents work together to achieve complex software engineering goals. This approach emphasizes a dual modality, distinguishing between software engineering processes designed for humans and those tailored for agents. The paper proposes two specialized environments: the Agent Command Environment (ACE) for human oversight and the Agent Execution Environment (AEE) for agent task execution, fostering a bi-directional partnership. By redefining foundational aspects of software engineering, this vision aims to enhance human-AI collaboration and set the stage for future developments in the field."
                },
                "zh": {
                    "title": "ä»£ç†è½¯ä»¶å·¥ç¨‹ï¼šäººæœºåä½œçš„æ–°çºªå…ƒ",
                    "desc": "ä»£ç†è½¯ä»¶å·¥ç¨‹ï¼ˆSE 3.0ï¼‰å¼•å…¥äº†ä¸€ç§äººç±»ä¸æ™ºèƒ½ä»£ç†åä½œçš„åŒé‡æ¨¡å¼ï¼Œé‡æ–°å®šä¹‰äº†è½¯ä»¶å·¥ç¨‹çš„è¿‡ç¨‹å’Œå·¥å…·ï¼Œä»¥å®ç°å¤æ‚çš„ç›®æ ‡å¯¼å‘ä»»åŠ¡ã€‚è¯¥æ–¹æ³•å¼ºè°ƒäººç±»ä¸ä»£ç†ä¹‹é—´çš„åŒå‘åˆä½œï¼Œæå‡ºäº†ä¸¤ä¸ªä¸“é—¨çš„å·¥ä½œç¯å¢ƒï¼šä»£ç†æŒ‡æŒ¥ç¯å¢ƒï¼ˆACEï¼‰å’Œä»£ç†æ‰§è¡Œç¯å¢ƒï¼ˆAEEï¼‰ã€‚ACEä½œä¸ºæŒ‡æŒ¥ä¸­å¿ƒï¼Œå¸®åŠ©äººç±»åè°ƒä»£ç†å›¢é˜Ÿï¼Œè€ŒAEEåˆ™æ˜¯ä»£ç†æ‰§è¡Œä»»åŠ¡çš„æ•°å­—å·¥ä½œç©ºé—´ã€‚é€šè¿‡è¿™ç§åˆä½œï¼Œè½¯ä»¶å·¥ç¨‹çš„å®è·µä»ç®€å•çš„ç¼–ç æå‡åˆ°çœŸæ­£çš„ä»£ç†è½¯ä»¶å·¥ç¨‹ï¼Œæ¨åŠ¨äº†è½¯ä»¶å·¥ç¨‹çš„æœªæ¥å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13399",
            "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable,\n  Fine-Grained Evaluation of Multi-Turn Editing",
            "url": "https://huggingface.co/papers/2509.13399",
            "abstract": "EdiVal-Agent is an automated evaluation framework for instruction-based image editing that integrates VLMs, object detectors, and human preference models to assess instruction following, content consistency, and visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images -- resulting in limited coverage and inheriting biases from prior generative models -- or (ii) rely solely on zero-shot vision-language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise.   To address this, we introduce EdiVal-Agent, an automated, scalable, and fine-grained evaluation framework for multi-turn instruction-based editing from an object-centric perspective, supported by a suite of expert tools. Given an image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions. For evaluation, it integrates VLMs with open-vocabulary object detectors to assess instruction following, uses semantic-level feature extractors to evaluate content consistency, and leverages human preference models to judge visual quality. We show that combining VLMs with object detectors yields stronger agreement with human judgments in instruction-following evaluation compared to using VLMs alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows future tools to be seamlessly integrated, enhancing evaluation accuracy over time.   Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types and 11 state-of-the-art editing models spanning autoregressive (AR) (including Nano Banana, GPT-Image-1), flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models. Project page: https://tianyucodings.github.io/EdiVAL-page/.",
            "score": 3,
            "issue_id": 5994,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "d68786190b03b715",
            "authors": [
                "Tianyu Chen",
                "Yasi Zhang",
                "Zhi Zhang",
                "Peiyu Yu",
                "Shu Wang",
                "Zhendong Wang",
                "Kevin Lin",
                "Xiaofei Wang",
                "Zhengyuan Yang",
                "Linjie Li",
                "Chung-Ching Lin",
                "Jianwen Xie",
                "Oscar Leong",
                "Lijuan Wang",
                "Ying Nian Wu",
                "Mingyuan Zhou"
            ],
            "affiliations": [
                "Lambda, Inc",
                "Microsoft",
                "University of California, Los Angeles",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13399.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#multimodal",
                    "#optimization",
                    "#cv",
                    "#interpretability"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "EdiVal-Agent: ĞĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "EdiVal-Agent - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM), Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. EdiVal-Agent Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ VLM Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CLIP."
                },
                "en": {
                    "title": "Revolutionizing Image Editing Evaluation with EdiVal-Agent",
                    "desc": "EdiVal-Agent is a new framework designed to evaluate instruction-based image editing by combining various machine learning techniques. It uses vision-language models (VLMs) and object detectors to assess how well images follow editing instructions, maintain content consistency, and achieve visual quality. By breaking down images into meaningful objects and generating context-aware editing instructions, EdiVal-Agent provides a more accurate evaluation compared to previous methods. The framework's modular design allows for the integration of new tools, improving evaluation accuracy and helping to identify weaknesses in current editing models."
                },
                "zh": {
                    "title": "EdiVal-Agentï¼šæ™ºèƒ½å›¾åƒç¼–è¾‘çš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "EdiVal-Agent æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ã€‚å®ƒç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€ç‰©ä½“æ£€æµ‹å™¨å’Œäººç±»åå¥½æ¨¡å‹ï¼Œä»¥è¯„ä¼°æŒ‡ä»¤éµå¾ªã€å†…å®¹ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å›¾åƒåˆ†è§£ä¸ºè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å¯¹è±¡ï¼Œå¹¶ç”Ÿæˆå¤šæ ·çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¼–è¾‘æŒ‡ä»¤ï¼Œä»è€Œå®ç°ç²¾ç»†åŒ–è¯„ä¼°ã€‚EdiVal-Agent çš„æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—æœªæ¥çš„å·¥å…·å¯ä»¥æ— ç¼é›†æˆï¼Œéšç€æ—¶é—´çš„æ¨ç§»æé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15020",
            "title": "Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs",
            "url": "https://huggingface.co/papers/2509.15020",
            "abstract": "Tokenizing the space with the answer letter in multiple-choice question answering improves LLM accuracy and calibration.  \t\t\t\t\tAI-generated summary \t\t\t\t When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string \"Answer:\" to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy -- tokenizing the space together with the answer letter -- as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model's confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results.",
            "score": 2,
            "issue_id": 5984,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "6e87639367d48430",
            "authors": [
                "Mario Sanz-Guerrero",
                "Minh Duc Bui",
                "Katharina von der Wense"
            ],
            "affiliations": [
                "Johannes Gutenberg University Mainz, Germany",
                "University of Colorado Boulder, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15020.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#data",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ±ÑƒĞºĞ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ°, ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ±Ñ‹, Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğµ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 11% Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñƒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Tokenization Matters: Boosting LLM Accuracy in MCQA!",
                    "desc": "This paper investigates the impact of tokenization strategies on the performance of large language models (LLMs) in multiple-choice question answering (MCQA). It reveals that the way space is tokenized after the prompt can lead to significant accuracy differences, affecting model rankings by up to 11%. The authors propose a specific method of tokenizing the space along with the answer letter, which consistently improves both accuracy and model calibration. These findings emphasize the necessity for standardized evaluation practices to ensure the reliability of LLM comparisons."
                },
                "zh": {
                    "title": "ä¼˜åŒ–åˆ†è¯æå‡LLMå‡†ç¡®æ€§ä¸æ ¡å‡†æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å¤šé€‰é¢˜é—®ç­”ä¸­ï¼Œå¦‚ä½•å¤„ç†å†’å·åç©ºæ ¼çš„åˆ†è¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡†ç¡®æ€§å’Œæ ¡å‡†çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œé‡‡ç”¨ä¸åŒçš„åˆ†è¯æ–¹å¼å¯èƒ½å¯¼è‡´å‡†ç¡®ç‡å·®å¼‚é«˜è¾¾11%ï¼Œå¹¶ä¸”å¯èƒ½æ”¹å˜æ¨¡å‹æ’åï¼Œå½±å“LLMæ¯”è¾ƒçš„å¯é æ€§ã€‚æˆ‘ä»¬æ¨èå°†ç©ºæ ¼ä¸ç­”æ¡ˆå­—æ¯ä¸€èµ·åˆ†è¯ï¼Œè¿™ç§æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºä¸€è‡´ä¸”æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶ä¹Ÿæ”¹å–„äº†æ¨¡å‹çš„æ ¡å‡†æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†è¯„ä¼°è®¾è®¡çš„é‡è¦æ€§ï¼Œå¹¶å‘¼åå»ºç«‹æ ‡å‡†åŒ–å’Œé€æ˜çš„è¯„ä¼°åè®®ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é æ€§å’Œå¯æ¯”æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14977",
            "title": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal\n  Ultrasound Intelligence",
            "url": "https://huggingface.co/papers/2509.14977",
            "abstract": "EchoVLM, a vision-language model with a Mixture of Experts architecture, improves ultrasound report generation and diagnosis by leveraging data from multiple anatomical regions.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at https://github.com/Asunatan/EchoVLM.",
            "score": 2,
            "issue_id": 5980,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "36663a681181ac46",
            "authors": [
                "Chaoyin She",
                "Ruifang Lu",
                "Lida Chen",
                "Wei Wang",
                "Qinghua Huang"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "The First Affiliated Hospital of Sun Yat-Sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14977.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#architecture",
                    "#games",
                    "#cv",
                    "#open_source",
                    "#training",
                    "#science",
                    "#dataset"
                ],
                "emoji": "ğŸ”Š",
                "ru": {
                    "title": "EchoVLM: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸",
                    "desc": "EchoVLM - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture of Experts. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞµĞ¼Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ², Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·. EchoVLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ², ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ BLEU-1 Ğ¸ ROUGE-1. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ² ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Ultrasound Diagnosis with EchoVLM",
                    "desc": "EchoVLM is a specialized vision-language model designed to enhance ultrasound report generation and diagnosis by utilizing a Mixture of Experts (MoE) architecture. This model is trained on diverse data from seven anatomical regions, allowing it to effectively handle multiple tasks such as report generation, diagnosis, and visual question-answering. The results show that EchoVLM significantly outperforms existing models, achieving notable improvements in BLEU-1 and ROUGE-1 scores for ultrasound report generation. This advancement indicates that EchoVLM can greatly improve diagnostic accuracy in ultrasound imaging, making it a promising tool for clinical applications."
                },
                "zh": {
                    "title": "EchoVLMï¼šæå‡è¶…å£°è¯Šæ–­çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "EchoVLMæ˜¯ä¸€ç§ä¸“é—¨ä¸ºè¶…å£°åŒ»å­¦æˆåƒè®¾è®¡çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ©ç”¨æ¥è‡ªä¸ƒä¸ªè§£å‰–åŒºåŸŸçš„æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†è¶…å£°æŠ¥å‘Šç”Ÿæˆå’Œè¯Šæ–­çš„æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEchoVLMåœ¨è¶…å£°æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºQwen2-VLï¼ŒBLEU-1å’ŒROUGE-1å¾—åˆ†åˆ†åˆ«æé«˜äº†10.15å’Œ4.77åˆ†ã€‚è¿™è¡¨æ˜EchoVLMåœ¨æé«˜è¶…å£°æˆåƒè¯Šæ–­å‡†ç¡®æ€§æ–¹é¢å…·æœ‰é‡è¦æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ä¸´åºŠåº”ç”¨æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10402",
            "title": "Developer-LLM Conversations: An Empirical Study of Interactions and\n  Generated Code Quality",
            "url": "https://huggingface.co/papers/2509.10402",
            "abstract": "Analysis of real-world developer-LLM conversations reveals patterns in task outcomes, code quality, and common issues across multiple programming languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are becoming integral to modern software development workflows, assisting developers with code generation, API explanation, and iterative problem-solving through natural language conversations. Despite widespread adoption, there is limited understanding of how developers interact with LLMs in practice and how these conversational dynamics influence task outcomes, code quality, and software engineering workflows. To address this, we leverage CodeChat, a large dataset comprising 82,845 real-world developer-LLM conversations, containing 368,506 code snippets generated across over 20 programming languages, derived from the WildChat dataset. We find that LLM responses are substantially longer than developer prompts, with a median token-length ratio of 14:1. Multi-turn conversations account for 68% of the dataset and often evolve due to shifting requirements, incomplete prompts, or clarification requests. Topic analysis identifies web design (9.6% of conversations) and neural network training (8.7% of conversations) as the most frequent LLM-assisted tasks. Evaluation across five languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and language-specific issues in LLM-generated code: generated Python and JavaScript code often include undefined variables (83.4% and 75.3% of code snippets, respectively); Java code lacks required comments (75.9%); C++ code frequently omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a conversation, syntax and import errors persist across turns; however, documentation quality in Java improves by up to 14.7%, and import handling in Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code generated in prior turns and explicitly request a fix are most effective for resolving errors.",
            "score": 0,
            "issue_id": 5986,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 12",
                "zh": "9æœˆ12æ—¥"
            },
            "hash": "fd7d34991fe810de",
            "authors": [
                "Suzhen Zhong",
                "Ying Zou",
                "Bram Adams"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, Queens University, Kingston, ON K7L 3N6, Canada",
                "Maintenance, Construction and Intelligence of Software Lab (MCIS), School of Computing, Queens University, Kingston, ON K7L 3N6, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10402.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#plp",
                    "#games",
                    "#optimization",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº-Ğ˜Ğ˜ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ÑĞ°Ğ½ÑÑ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CodeChat. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ±ĞµÑĞµĞ´Ñ‹ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ 68% Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ Ğ²ĞµĞ±-Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ² Python Ğ¸ JavaScript Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ² Ğ² Java."
                },
                "en": {
                    "title": "Enhancing Code Quality through Developer-LLM Conversations",
                    "desc": "This paper analyzes how developers interact with Large Language Models (LLMs) during coding tasks, revealing important patterns in the outcomes and quality of the generated code. By examining a dataset of over 82,000 real-world conversations, the study highlights that LLM responses are typically much longer than the prompts given by developers. It identifies common issues in the generated code across various programming languages, such as undefined variables in Python and JavaScript, and missing comments in Java. The findings suggest that multi-turn conversations can improve code quality, especially when developers explicitly request corrections for previous errors."
                },
                "zh": {
                    "title": "å¼€å‘è€…ä¸LLMå¯¹è¯çš„æ·±åº¦åˆ†æ",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†å¼€å‘è€…ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„å¯¹è¯ï¼Œæ­ç¤ºäº†ä»»åŠ¡ç»“æœã€ä»£ç è´¨é‡å’Œå¸¸è§é—®é¢˜çš„æ¨¡å¼ã€‚æˆ‘ä»¬åˆ©ç”¨åŒ…å«82,845ä¸ªçœŸå®å¼€å‘è€…-LLMå¯¹è¯çš„æ•°æ®é›†ï¼Œå‘ç°LLMçš„å“åº”é€šå¸¸æ¯”å¼€å‘è€…çš„æç¤ºé•¿å¾—å¤šï¼Œä¸”å¤šè½®å¯¹è¯å æ®äº†68%çš„æ•°æ®é›†ã€‚é€šè¿‡ä¸»é¢˜åˆ†æï¼Œæˆ‘ä»¬å‘ç°ç½‘é¡µè®¾è®¡å’Œç¥ç»ç½‘ç»œè®­ç»ƒæ˜¯æœ€å¸¸è§çš„LLMè¾…åŠ©ä»»åŠ¡ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œä¸åŒç¼–ç¨‹è¯­è¨€ç”Ÿæˆçš„ä»£ç å­˜åœ¨ç‰¹å®šé—®é¢˜ï¼Œä¾‹å¦‚Pythonå’ŒJavaScriptä»£ç å¸¸å¸¸åŒ…å«æœªå®šä¹‰çš„å˜é‡ï¼Œè€ŒJavaä»£ç ç¼ºå°‘å¿…è¦çš„æ³¨é‡Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06482",
            "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution\n  Remote Sensing Change Detection",
            "url": "https://huggingface.co/papers/2509.06482",
            "abstract": "FSG-Net addresses false alarms and semantic gaps in change detection by using a frequency-spatial synergistic approach with wavelet interaction, attention mechanisms, and gated fusion.  \t\t\t\t\tAI-generated summary \t\t\t\t Change detection from high-resolution remote sensing images lies as a cornerstone of Earth observation applications, yet its efficacy is often compromised by two critical challenges. First, false alarms are prevalent as models misinterpret radiometric variations from temporal shifts (e.g., illumination, season) as genuine changes. Second, a non-negligible semantic gap between deep abstract features and shallow detail-rich features tends to obstruct their effective fusion, culminating in poorly delineated boundaries. To step further in addressing these issues, we propose the Frequency-Spatial Synergistic Gated Network (FSG-Net), a novel paradigm that aims to systematically disentangle semantic changes from nuisance variations. Specifically, FSG-Net first operates in the frequency domain, where a Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates pseudo-changes by discerningly processing different frequency components. Subsequently, the refined features are enhanced in the spatial domain by a Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the saliency of genuine change regions. To finally bridge the semantic gap, a Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to selectively gate and integrate crucial details from shallow layers. Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at https://github.com/zxXie-Air/FSG-Net after a possible publication.",
            "score": 0,
            "issue_id": 5976,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "89822775d7f8c973",
            "authors": [
                "Zhongxiang Xie",
                "Shuangxi Miao",
                "Yuhan Jiang",
                "Zhewei Zhang",
                "Jing Yao",
                "Xuecao Li",
                "Jianxi Huang",
                "Pedram Ghamisi"
            ],
            "affiliations": [
                "Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China",
                "College of Land Science and Technology, China Agricultural University, Beijing 100193, China",
                "Faculty of Geosciences and Engineering, Southwest Jiaotong University, Chengdu 60031, China",
                "Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, 09599 Freiberg, Germany",
                "Lancaster Environment Centre, Lancaster University, LA1 4YR Lancaster, U.K."
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06482.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "FSG-Net: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…",
                    "desc": "FSG-Net - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. FSG-Net Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ DAWIM Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚, STSAM Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ LGFU Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ FSG-Net Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Change Detection with FSG-Net: Bridging Gaps and Reducing False Alarms",
                    "desc": "FSG-Net is a novel approach designed to improve change detection in high-resolution remote sensing images by addressing false alarms and semantic gaps. It utilizes a frequency-spatial synergistic method that includes a Discrepancy-Aware Wavelet Interaction Module to reduce misinterpretations caused by radiometric variations. The model further enhances feature representation through a Synergistic Temporal-Spatial Attention Module, which focuses on highlighting genuine changes. Finally, a Lightweight Gated Fusion Unit effectively integrates high-level semantic information with detailed features, achieving state-of-the-art performance on several benchmarks."
                },
                "zh": {
                    "title": "FSG-Netï¼šç²¾å‡†å˜åŒ–æ£€æµ‹çš„æ–°æ–¹æ³•",
                    "desc": "FSG-Netæ˜¯ä¸€ç§æ–°é¢–çš„ç½‘ç»œæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒå˜åŒ–æ£€æµ‹ä¸­çš„å‡è­¦æŠ¥å’Œè¯­ä¹‰å·®è·é—®é¢˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨é¢‘ç‡-ç©ºé—´ååŒçš„æ–¹æ³•ï¼Œé€šè¿‡å°æ³¢äº¤äº’æ¨¡å—å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆåŒºåˆ†çœŸå®å˜åŒ–ä¸å¹²æ‰°å˜åŒ–ã€‚FSG-Neté¦–å…ˆåœ¨é¢‘ç‡åŸŸä¸­å¤„ç†ä¸åŒé¢‘ç‡æˆåˆ†ï¼Œä»¥å‡å°‘ä¼ªå˜åŒ–çš„å½±å“ï¼Œç„¶ååœ¨ç©ºé—´åŸŸä¸­å¢å¼ºçœŸå®å˜åŒ–åŒºåŸŸçš„æ˜¾è‘—æ€§ã€‚æœ€åï¼Œé€šè¿‡è½»é‡çº§é—¨æ§èåˆå•å…ƒï¼ŒFSG-Netå°†é«˜å±‚è¯­ä¹‰ä¸ä½å±‚ç»†èŠ‚æœ‰æ•ˆç»“åˆï¼Œæ˜¾è‘—æé«˜äº†å˜åŒ–æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-18.html",
    "link_next": "2025-09-22.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "18.09",
        "en": "09/18",
        "zh": "9æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "22.09",
        "en": "09/22",
        "zh": "9æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 10,
        "#agents": 4,
        "#cv": 9,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 1,
        "#agi": 2,
        "#games": 8,
        "#interpretability": 3,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 10,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 1
    }
}