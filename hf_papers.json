{
    "date": "8 –æ–∫—Ç—è–±—Ä—è",
    "issue_id": 21,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.05258",
            "title": "Differential Transformer",
            "url": "https://huggingface.co/papers/2410.05258",
            "abstract": "Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",
            "score": 46,
            "issue_id": 14,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Diff Transformer, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∏ –ø–æ–¥–∞–≤–ª—è–µ—Ç —à—É–º. –ú–µ—Ö–∞–Ω–∏–∑–º –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∫–∞–∫ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –¥–≤—É–º—è –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è softmax. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Diff Transformer –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—ã—á–Ω—ã–π Transformer –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–Ω–∏–∂–µ–Ω–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∏ –æ–±—É—á–µ–Ω–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.",
                "tags": [
                    "#DiffTransformer",
                    "#DifferentialAttention",
                    "#LongContextModeling"
                ],
                "emoji": "üîç",
                "title": "Diff Transformer: —Ç–æ—á–Ω–µ–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –≤–∞–∂–Ω–æ–º, –æ—Ç—Å–µ–∫–∞—è —à—É–º"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02707",
            "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations",
            "url": "https://huggingface.co/papers/2410.02707",
            "abstract": "Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as \"hallucinations\". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.",
            "score": 24,
            "issue_id": 15,
            "pub_date": "2024-10-03",
            "pub_date_ru": "3 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–æ–¥–µ—Ä–∂–∞—Ç –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á–µ–º —Å—á–∏—Ç–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —ç—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫. –û–¥–Ω–∞–∫–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –æ—à–∏–±–æ–∫ –Ω–µ –æ–±–æ–±—â–∞—é—Ç—Å—è –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ LLM –º–æ–≥—É—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–π.",
                "tags": [
                    "#hallucinations",
                    "#errorDetection",
                    "#internalRepresentations"
                ],
                "emoji": "üîç",
                "title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Ç–∞–π–Ω –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π LLM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—à–∏–±–æ–∫"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04364",
            "title": "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide",
            "url": "https://huggingface.co/papers/2410.04364",
            "abstract": "Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model's denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method. Project Page: http://videoguide2025.github.io/",
            "score": 21,
            "issue_id": 13,
            "pub_date": "2024-10-06",
            "pub_date_ru": "6 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "VideoGuide - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≥–∏–¥–∞ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö –≤—ã–≤–æ–¥–∞, –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É—è –µ–µ –æ–±—Ä–∞–∑—Ü—ã –≤ –ø—Ä–æ—Ü–µ—Å—Å —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –æ–±—ä–µ–¥–∏–Ω—è—è —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –ø—Ä–∏–æ—Ä–∞, –ø–æ–∑–≤–æ–ª—è—è –±–∞–∑–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —É–ª—É—á—à–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å.",
                "tags": [
                    "#text2video",
                    "#temporalConsistency",
                    "#diffusionModels"
                ],
                "emoji": "üé¨",
                "title": "VideoGuide: –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02675",
            "title": "FAN: Fourier Analysis Networks",
            "url": "https://huggingface.co/papers/2410.02675",
            "abstract": "Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlying principles of periodicity. However, periodicity is a crucial trait in various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, a novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena. By introducing Fourier Series, the periodicity is naturally integrated into the structure and computational processes of the neural network, thus achieving a more accurate expression and prediction of periodic patterns. As a promising substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in various models with fewer parameters and FLOPs. Through extensive experiments, we demonstrate the effectiveness of FAN in modeling and reasoning about periodic functions, and the superiority and generalizability of FAN across a range of real-world tasks, including symbolic formula representation, time series forecasting, and language modeling.",
            "score": 21,
            "issue_id": 12,
            "pub_date": "2024-10-03",
            "pub_date_ru": "3 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º FAN, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –§—É—Ä—å–µ. FAN —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ —è–≤–ª–µ–Ω–∏—è, –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ FAN –Ω–∞–¥ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–º –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–æ–º (MLP) –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. FAN –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–µ—Ç–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.",
                "tags": [
                    "#FourierAnalysisNetwork",
                    "#–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ–ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ",
                    "#–ó–∞–º–µ–Ω–∞MLP"
                ],
                "emoji": "üîÑ",
                "title": "FAN: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05046",
            "title": "Named Clinical Entity Recognition Benchmark",
            "url": "https://huggingface.co/papers/2410.05046",
            "abstract": "This technical report introduces a Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support.   The leaderboard provides a standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. A curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and a comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes a brief analysis of models evaluated to date, highlighting observed trends and limitations.   By establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP.",
            "score": 13,
            "issue_id": 17,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–≠—Ç–æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π. –ë–µ–Ω—á–º–∞—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —è–≤–ª—è–µ—Ç—Å—è F1-–º–µ—Ä–∞, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –¥—Ä—É–≥–∏–º–∏ —Ä–µ–∂–∏–º–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –¶–µ–ª—å—é –±–µ–Ω—á–º–∞—Ä–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π.",
                "tags": [
                    "#–∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–µ–ù–õ–ü",
                    "#–∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ–°—É—â–Ω–æ—Å—Ç–∏",
                    "#–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ–î–∞–Ω–Ω—ã–µ"
                ],
                "emoji": "üè•",
                "title": "–ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º NLP"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05080",
            "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
            "url": "https://huggingface.co/papers/2410.05080",
            "abstract": "The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about the true capabilities of such agents. In this work, we argue that for an agent to fully automate scientific discovery, it must be able to complete all essential tasks in the workflow. Thus, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. These results underscore the limited capacities of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.",
            "score": 10,
            "issue_id": 19,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ScienceAgentBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –∏–∑–≤–ª–µ–∫–ª–∏ 102 –∑–∞–¥–∞—á–∏ –∏–∑ 44 —Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –≤ —á–µ—Ç—ã—Ä–µ—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ö –∏ –ø—Ä–∏–≤–ª–µ–∫–ª–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –∏—Ö –≤–∞–ª–∏–¥–∞—Ü–∏–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å Python-–∫–æ–¥ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–π –∞–≥–µ–Ω—Ç —Ä–µ—à–∞–µ—Ç —Ç–æ–ª—å–∫–æ 32.4% –∑–∞–¥–∞—á —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –Ω–∞—É—á–Ω–æ–π —Å—Ñ–µ—Ä–µ.",
                "tags": [
                    "#languageAgents",
                    "#scientificDiscovery",
                    "#codeGeneration"
                ],
                "emoji": "üß™",
                "title": "–†–µ–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É–∫–µ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04534",
            "title": "UniMuMo: Unified Text, Music and Motion Generation",
            "url": "https://huggingface.co/papers/2410.04534",
            "abstract": "We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the https://hanyangclarence.github.io/unimumo_demo/{project page}.",
            "score": 10,
            "issue_id": 14,
            "pub_date": "2024-10-06",
            "pub_date_ru": "6 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "UniMuMo - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –º—É–∑—ã–∫—É –∏ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —ç—Ç–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–µ—Å–ø–∞—Ä–µ–Ω–Ω—ã—Ö –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö –∏ –¥–≤–∏–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ UniMuMo –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ —Ç–∏–ø–∞ —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä —Å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ –æ–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –º—É–∑—ã–∫–∏, –¥–≤–∏–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞.",
                "tags": [
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è–ì–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–ú–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π",
                    "#—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–µ–ú–æ–¥–µ–ª–∏"
                ],
                "emoji": "üé≠",
                "title": "UniMuMo: –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –º—É–∑—ã–∫–∏ –∏ –¥–≤–∏–∂–µ–Ω–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04734",
            "title": "TLDR: Token-Level Detective Reward Model for Large Vision Language Models",
            "url": "https://huggingface.co/papers/2410.04734",
            "abstract": "Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one binary feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose a Token-Level Detective Reward Model (TLDR) to provide fine-grained annotations to each text token. We first introduce a perturbation-based method to generate synthetic hard negatives and their token-level labels to train TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as a hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation by 3 times to acquire a broader range of high-quality vision language data.",
            "score": 10,
            "issue_id": 13,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤ (TLDR) –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. TLDR –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—É—é, —Ç–∞–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ '—Å–ª–æ–∂–Ω—ã–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã' –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. TLDR –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –¥–ª—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π –∏ –æ—Ü–µ–Ω–∫–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π, –∞ —Ç–∞–∫–∂–µ —É—Å–∫–æ—Ä—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ 3 —Ä–∞–∑–∞.",
                "tags": [
                    "#TokenLevelRewardModel",
                    "#MultimodalLLM",
                    "#HallucinationDetection"
                ],
                "emoji": "üîç",
                "title": "–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05167",
            "title": "Presto! Distilling Steps and Layers for Accelerating Music Generation",
            "url": "https://huggingface.co/papers/2410.05167",
            "abstract": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) -- the fastest high-quality TTM to our knowledge. Sound examples can be found at https://presto-music.github.io/web/.",
            "score": 10,
            "issue_id": 13,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Presto! - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–∑—ã–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π (DMD) –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∏ —Ç–∞–∫–∂–µ —É–ª—É—á—à–∏–ª–∏ –º–µ—Ç–æ–¥ –ø–æ—Å–ª–æ–π–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –º—É–∑—ã–∫—É –≤ 10-18 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏.",
                "tags": [
                    "#text-to-music",
                    "#diffusion-models",
                    "#model-distillation"
                ],
                "emoji": "üéµ",
                "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–∑—ã–∫–∏ –ø–æ —Ç–µ–∫—Å—Ç—É —Å –ø–æ–º–æ—â—å—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05243",
            "title": "Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents",
            "url": "https://huggingface.co/papers/2410.05243",
            "abstract": "Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly take pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.",
            "score": 9,
            "issue_id": 19,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (GUI) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–µ–ª—å UGround, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ UGround –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ –¥–ª—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ 20%. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, –Ω–∞–≤–∏–≥–∏—Ä—É—é—â–∏—Ö –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–º –º–∏—Ä–µ –ø–æ–¥–æ–±–Ω–æ –ª—é–¥—è–º.",
                "tags": [
                    "#–≤–∏–∑—É–∞–ª—å–Ω–∞—è_–ø—Ä–∏–≤—è–∑–∫–∞",
                    "#GUI_–∞–≥–µ–Ω—Ç—ã",
                    "#UGround"
                ],
                "emoji": "üëÅÔ∏è",
                "title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º GUI-–∞–≥–µ–Ω—Ç–∞–º"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04932",
            "title": "OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction",
            "url": "https://huggingface.co/papers/2410.04932",
            "abstract": "We present OmniBooth, an image generation framework that enables spatial control with instance-level multi-modal customization. For all instances, the multimodal instruction can be described through text prompts or image references. Given a set of user-defined masks and associated text or image guidance, our objective is to generate an image, where multiple objects are positioned at specified coordinates and their attributes are precisely aligned with the corresponding guidance. This approach significantly expands the scope of text-to-image generation, and elevates it to a more versatile and practical dimension in controllability. In this paper, our core contribution lies in the proposed latent control signals, a high-dimensional spatial feature that provides a unified representation to integrate the spatial, textual, and image conditions seamlessly. The text condition extends ControlNet to provide instance-level open-vocabulary generation. The image condition further enables fine-grained control with personalized identity. In practice, our method empowers users with more flexibility in controllable generation, as users can choose multi-modal conditions from text or images as needed. Furthermore, thorough experiments demonstrate our enhanced performance in image synthesis fidelity and alignment across different tasks and datasets. Project page: https://len-li.github.io/omnibooth-web/",
            "score": 8,
            "issue_id": 14,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "OmniBooth - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∏ –∞—Ç—Ä–∏–±—É—Ç—ã –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π-–æ–±—Ä–∞–∑—Ü–æ–≤. –ö–ª—é—á–µ–≤—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ª–∞—Ç–µ–Ω—Ç–Ω—ã–π —Å–∏–≥–Ω–∞–ª —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è - –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ, —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è. OmniBooth —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≥–∏–±–∫–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.",
                "tags": [
                    "#–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è_–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π_—Å–∏–Ω—Ç–µ–∑",
                    "#–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ_—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"
                ],
                "emoji": "üé®",
                "title": "–£–ø—Ä–∞–≤–ª—è–µ–º–æ–µ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03825",
            "title": "MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion",
            "url": "https://huggingface.co/papers/2410.03825",
            "abstract": "Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.",
            "score": 8,
            "issue_id": 14,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "MonST3R - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏. –í–º–µ—Å—Ç–æ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã—Ö —Å–∏—Å—Ç–µ–º, –æ–Ω –Ω–∞–ø—Ä—è–º—É—é –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞, –∞–¥–∞–ø—Ç–∏—Ä—É—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ DUST3R –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Ö–≤–∞—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∞–≤—Ç–æ—Ä—ã —Å–º–æ–≥–ª–∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. MonST3R –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ –∏ –ø–æ–∑—ã –∫–∞–º–µ—Ä—ã, –∞ —Ç–∞–∫–∂–µ –æ–±–µ—â–∞—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ 4D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.",
                "tags": [
                    "#–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è_–≥–µ–æ–º–µ—Ç—Ä–∏—è",
                    "#3D_—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è",
                    "#–º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–µ_–≤–∏–¥–µ–æ"
                ],
                "emoji": "üé•",
                "title": "MonST3R: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05229",
            "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
            "url": "https://huggingface.co/papers/2410.05229",
            "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.",
            "score": 7,
            "issue_id": 18,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GSM-Symbolic, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è –±–æ–ª–µ–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –æ—Ü–µ–Ω–∫–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –≤–æ–ø—Ä–æ—Å–∞—Ö –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É—Å–ª–æ–≤–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∫ –ø–æ–¥–ª–∏–Ω–Ω—ã–º –ª–æ–≥–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º, –∞ –ª–∏—à—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥—è—Ç —à–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.",
                "tags": [
                    "#–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ_—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "#–æ—Ü–µ–Ω–∫–∞_LLM",
                    "#GSM-Symbolic"
                ],
                "emoji": "üßÆ",
                "title": "–†–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ –∏–ª–ª—é–∑–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02884",
            "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2410.02884",
            "abstract": "This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.",
            "score": 7,
            "issue_id": 16,
            "pub_date": "2024-10-03",
            "pub_date_ru": "3 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "LLaMA-Berry - —ç—Ç–æ –ø–µ—Ä–µ–¥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –¥–µ—Ä–µ–≤—É (MCTS) —Å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–º —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—É—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å –ø–∞—Ä–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—É—Ç–µ–π —Ä–µ—à–µ–Ω–∏—è. LLaMA-Berry –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –æ–ª–∏–º–ø–∏–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.",
                "tags": [
                    "#–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ",
                    "#MCTS",
                    "#—Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–µ-LLM"
                ],
                "emoji": "üßÆ",
                "title": "LLaMA-Berry: –ü—Ä–æ—Ä—ã–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04698",
            "title": "MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2410.04698",
            "abstract": "Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark.",
            "score": 7,
            "issue_id": 15,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MathHay –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, MathHay —Ç—Ä–µ–±—É–µ—Ç –æ—Ç –º–æ–¥–µ–ª–µ–π –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–æ –∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –≤–æ—Å–µ–º—å—é –≤–µ–¥—É—â–∏–º–∏ LLM –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å Gemini-1.5-Pro-002 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ 51.26% –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö –¥–ª–∏–Ω–æ–π 128 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –≤ –æ–±–ª–∞—Å—Ç–∏ –¥–ª–∏–Ω–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.",
                "tags": [
                    "#LongContextMathReasoning",
                    "#MathHayBenchmark",
                    "#LLMEvaluation"
                ],
                "emoji": "üßÆ",
                "title": "MathHay: –ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05262",
            "title": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles",
            "url": "https://huggingface.co/papers/2410.05262",
            "abstract": "As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic interactions with users. Moreover, these benchmarks often depend on specific background knowledge, complicating the measurement of a model's logical reasoning capabilities. Other dynamic evaluation methods based on strong models or manual efforts may introduce biases and incur high costs and time demands, hindering large-scale application. To address these issues, we propose TurtleBench. TurtleBench collects real user guesses from our online Turtle Soup Puzzle platform that we developed. This approach allows for the relatively dynamic generation of evaluation datasets, mitigating the risk of model cheating while aligning assessments more closely with genuine user needs for reasoning capabilities, thus enhancing the reliability of evaluations. TurtleBench includes 1,532 user guesses along with the correctness of guesses after annotation. Using this dataset, we thoroughly evaluated nine of the most advanced LLMs available today. Notably, the OpenAI o1 series models did not achieve leading results in these evaluations. We propose several hypotheses for further research, such as \"the latent reasoning of o1 utilizes trivial Chain-of-Thought (CoT) techniques\" and \"increasing CoT length not only provides reasoning benefits but also incurs noise costs.\"",
            "score": 7,
            "issue_id": 13,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "TurtleBench - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–æ–≥–∞–¥–∫–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∏–≥—Ä–µ Turtle Soup Puzzle. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏, —Å–Ω–∏–∂–∞—è —Ä–∏—Å–∫ –æ–±–º–∞–Ω–∞ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –º–æ–¥–µ–ª–µ–π –∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–ª–∏—Å—å –¥–µ–≤—è—Ç—å –ø–µ—Ä–µ–¥–æ–≤—ã—Ö LLM, –ø—Ä–∏—á–µ–º –º–æ–¥–µ–ª–∏ —Å–µ—Ä–∏–∏ OpenAI o1 –Ω–µ –ø–æ–∫–∞–∑–∞–ª–∏ –ª–∏–¥–∏—Ä—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–≤–∏–≥–∞—é—Ç –≥–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –≤–∫–ª—é—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) –≤ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö o1.",
                "tags": [
                    "#TurtleBench",
                    "#–û—Ü–µ–Ω–∫–∞LLM",
                    "#–¶–µ–ø–æ—á–∫–∞–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–π"
                ],
                "emoji": "üê¢",
                "title": "TurtleBench: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–æ–≥–∞–¥–æ–∫"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03187",
            "title": "Autonomous Character-Scene Interaction Synthesis from Text Instruction",
            "url": "https://huggingface.co/papers/2410.03187",
            "abstract": "Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and human-object interaction, presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.",
            "score": 5,
            "issue_id": 15,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Å–ª–æ–∂–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –≤ 3D-—Å—Ä–µ–¥–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ —Ü–µ–ª–µ–≤–æ–≥–æ –ø–æ–ª–æ–∂–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–≤–∏–∂–µ–Ω–∏—è –∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏ –¥–µ–π—Å—Ç–≤–∏–π. –î–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ü–µ–Ω—ã, —É—á–∏—Ç—ã–≤–∞—é—â–µ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ. –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–Ω –æ–±—à–∏—Ä–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 16 —á–∞—Å–æ–≤ –∑–∞—Ö–≤–∞—á–µ–Ω–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π –≤ 120 indoor-—Å—Ü–µ–Ω–∞—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.",
                "tags": [
                    "#—Å–∏–Ω—Ç–µ–∑_–¥–≤–∏–∂–µ–Ω–∏–π",
                    "#–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ_–º–æ–¥–µ–ª–∏",
                    "#3D_–∞–Ω–∏–º–∞—Ü–∏—è"
                ],
                "emoji": "ü§ñ",
                "title": "–ò–ò –æ–∂–∏–≤–ª—è–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ 3D –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∫–æ–º–∞–Ω–¥–∞–º"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05057",
            "title": "SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification",
            "url": "https://huggingface.co/papers/2410.05057",
            "abstract": "Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT, the first large-scale benchmark of curation strategies for image classification.   In order to generate baseline methods for the SELECT benchmark, we create a new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K to date. Our dataset extends ImageNet with 5 new training-data shifts, each approximately the size of ImageNet-1K itself, and each assembled using a distinct curation strategy. We evaluate our data curation baselines in two ways: (i) using each training-data shift to train identical image classification models from scratch (ii) using the data itself to fit a pretrained self-supervised representation.   Our findings show interesting trends, particularly pertaining to recent methods for data curation such as synthetic data generation and lookup based on CLIP embeddings. We show that although these strategies are highly competitive for certain tasks, the curation strategy used to assemble the original ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark can illuminate the path for new methods to further reduce the gap. We release our checkpoints, code, documentation, and a link to our dataset at https://github.com/jimmyxu123/SELECT.",
            "score": 4,
            "issue_id": 18,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç SELECT –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ImageNet++, –≤–∫–ª—é—á–∞—é—â–∏–π 5 –Ω–æ–≤—ã—Ö —Å–¥–≤–∏–≥–æ–≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–≤–µ–¥–µ–Ω–∞ –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–∏—Å–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ CLIP-—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è ImageNet-1K –æ—Å—Ç–∞–µ—Ç—Å—è –∑–æ–ª–æ—Ç—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º.",
                "tags": [
                    "#datasetCuration",
                    "#ImageNetPlusPlus",
                    "#benchmarkEvaluation"
                ],
                "emoji": "üîç",
                "title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03959",
            "title": "Grounding Language in Multi-Perspective Referential Communication",
            "url": "https://huggingface.co/papers/2410.03959",
            "abstract": "We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them. We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model.",
            "score": 3,
            "issue_id": 12,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö. –î–≤–∞ –∞–≥–µ–Ω—Ç–∞ –≤ –æ–±—â–µ–π —Å—Ü–µ–Ω–µ –¥–æ–ª–∂–Ω—ã —É—á–∏—Ç—ã–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—É—é –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—É –¥—Ä—É–≥ –¥—Ä—É–≥–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä–µ–∫—Ç—ã –∏ –∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å–æ–±—Ä–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 2970 —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Å—Å—ã–ª–æ–∫ –æ—Ç—Å—Ç–∞–µ—Ç –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø–∞—Ä, –Ω–æ –æ–±—É—á–µ–Ω–∏–µ —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é –æ–± —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ —É–ª—É—á—à–∏–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.",
                "tags": [
                    "#—Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ_–≤—ã—Ä–∞–∂–µ–Ω–∏—è",
                    "#–º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ_—Å–∏—Å—Ç–µ–º—ã",
                    "#–∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω–æ–µ_–æ–±—É—á–µ–Ω–∏–µ"
                ],
                "emoji": "üë•",
                "title": "–£–ª—É—á—à–µ–Ω–∏–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —É—á–µ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05255",
            "title": "SePPO: Semi-Policy Preference Optimization for Diffusion Alignment",
            "url": "https://huggingface.co/papers/2410.05255",
            "abstract": "Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace \"losing images\" in preference pairs. This approach allows us to optimize using only off-policy \"winning images.\" Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in https://github.com/DwanZhang-AI/SePPO.",
            "score": 2,
            "issue_id": 19,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ SePPO (Semi-Policy Preference Optimization) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∑–∞–º–µ–Ω—ã '–ø—Ä–æ–∏–≥—Ä—ã—à–Ω—ã—Ö' –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø–∞—Ä–∞—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∫—Ä–∏—Ç–µ—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∫–æ—Ä–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. SePPO –ø—Ä–µ–≤–∑–æ—à–µ–ª —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö text-to-image –∏ text-to-video.",
                "tags": [
                    "#diffusion_models",
                    "#preference_optimization",
                    "#text_to_image_generation"
                ],
                "emoji": "üñºÔ∏è",
                "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03617",
            "title": "What Matters for Model Merging at Scale?",
            "url": "https://huggingface.co/papers/2410.03617",
            "abstract": "Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.",
            "score": 2,
            "issue_id": 18,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏, –∫–∞—á–µ—Å—Ç–≤–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 1 –¥–æ 64 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —á–µ—Ç—ã—Ä–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–µ—Ç–æ–¥–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–∏–ª—å–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞ —Ç–∞–∫–∂–µ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ü–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã –æ —Å–≤–æ–π—Å—Ç–≤–∞—Ö –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#model_merging",
                    "#scaling_study",
                    "#zero_shot_generalization"
                ],
                "emoji": "üîÄ",
                "title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ –º–∞—Å—à—Ç–∞–±–µ: –±–æ–ª—å—à–µ, –ª—É—á—à–µ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03960",
            "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation",
            "url": "https://huggingface.co/papers/2410.03960",
            "abstract": "LLM inference for popular enterprise use cases, such as summarization, RAG, and code-generation, typically observes orders of magnitude longer prompt lengths than generation lengths. This characteristic leads to high cost of prefill and increased response latency. In this paper, we present SwiftKV, a novel model transformation and distillation procedure specifically designed to reduce the time and cost of processing prompt tokens while preserving high quality of generated tokens. SwiftKV combines three key mechanisms: i) SingleInputKV, which prefills later layers' KV cache using a much earlier layer's output, allowing prompt tokens to skip much of the model computation, ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the memory footprint and support larger batch size for higher throughput, and iii) a knowledge-preserving distillation procedure that can adapt existing LLMs for SwiftKV with minimal accuracy impact and low compute and data requirement. For Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50% and the memory requirement of the KV cache by 62.5% while incurring minimum quality degradation across a wide range of tasks. In the end-to-end inference serving using an optimized vLLM implementation, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100 GPUs.",
            "score": 0,
            "issue_id": 21,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "SwiftKV - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–æ–º–ø—Ç–∞ –≤ LLM. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞: SingleInputKV, AcrossKV –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—É –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π. SwiftKV —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ 50% –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏ KV-–∫—ç—à–∞ –Ω–∞ 62,5% –¥–ª—è –º–æ–¥–µ–ª–µ–π Llama-3.1. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –¥–≤—É–∫—Ä–∞—Ç–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—â–µ–π –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–æ–∫–µ–Ω –Ω–∞ 60%.",
                "tags": [
                    "#SwiftKV",
                    "#LLMOptimization",
                    "#InferenceAcceleration"
                ],
                "emoji": "üöÄ",
                "title": "SwiftKV: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —É—Å–∫–æ—Ä–µ–Ω–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ LLM"
            }
        }
    ]
}