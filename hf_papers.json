{
    "date": {
        "ru": "18 июня",
        "en": "June 18",
        "zh": "6月18日"
    },
    "time_utc": "2025-06-18 17:13",
    "weekday": 2,
    "issue_id": 4362,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.14028",
            "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation",
            "url": "https://huggingface.co/papers/2506.14028",
            "abstract": "MultiFinBen is a multilingual and multimodal benchmark for financial domain tasks, evaluating LLMs across modalities and linguistic settings, revealing challenges in complex cross-lingual and multimodal financial reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.",
            "score": 54,
            "issue_id": 4360,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "e94d60496c8d96d1",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#financial",
                    "#dataset"
                ],
                "emoji": "💹",
                "ru": {
                    "title": "MultiFinBen: Преодолевая языковые и модальные барьеры в финансовом ИИ",
                    "desc": "MultiFinBen - это многоязычный и мультимодальный бенчмарк для задач в финансовой сфере, оценивающий большие языковые модели (LLM) в различных модальностях и языковых настройках. Он включает в себя новые задачи, такие как PolyFiQA и OCR-встроенные финансовые вопросно-ответные задачи, требующие сложных рассуждений над смешанными языковыми входными данными и визуально-текстовыми финансовыми документами. Бенчмарк использует динамический механизм отбора с учетом сложности и представляет собой компактный, сбалансированный набор данных. Оценка 22 современных моделей показала, что даже самые сильные из них испытывают значительные трудности при решении сложных межъязыковых и мультимодальных задач в финансовой области."
                },
                "en": {
                    "title": "MultiFinBen: Bridging Multilingual and Multimodal Gaps in Financial AI",
                    "desc": "MultiFinBen is a new benchmark designed to test large language models (LLMs) in the financial sector using multiple languages and types of data, such as text, images, and audio. It addresses the limitations of previous benchmarks that only focused on single languages and simple tasks, which do not reflect the complexities of real-world financial communication. The benchmark includes innovative tasks that require models to understand and reason with mixed-language inputs and to extract information from visual financial documents. Evaluation of various advanced models shows that even the best-performing ones struggle with these challenging tasks, highlighting the need for improved capabilities in financial reasoning across different languages and modalities."
                },
                "zh": {
                    "title": "多语言多模态金融基准，推动金融智能进步",
                    "desc": "MultiFinBen是一个针对金融领域任务的多语言和多模态基准，旨在评估大型语言模型（LLMs）在不同模态和语言环境下的表现。该基准揭示了在复杂的跨语言和多模态金融推理中存在的挑战。我们引入了两个新任务，PolyFiQA-Easy和PolyFiQA-Expert，要求模型在混合语言输入上进行复杂推理。此外，MultiFinBen还提供了一种动态的、关注难度的选择机制，以确保基准的平衡性和有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12928",
            "title": "Scaling Test-time Compute for LLM Agents",
            "url": "https://huggingface.co/papers/2506.12928",
            "abstract": "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.",
            "score": 33,
            "issue_id": 4351,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 июня",
                "en": "June 15",
                "zh": "6月15日"
            },
            "hash": "39c8f3e831e90d93",
            "authors": [
                "King Zhu",
                "Hanhao Li",
                "Siwei Wu",
                "Tianshun Xing",
                "Dehua Ma",
                "Xiangru Tang",
                "Minghao Liu",
                "Jian Yang",
                "Jiaheng Liu",
                "Yuchen Eleanor Jiang",
                "Changwang Zhang",
                "Chenghua Lin",
                "Jun Wang",
                "Ge Zhang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.12928.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Масштабирование вычислений улучшает работу языковых агентов",
                    "desc": "Исследование методов масштабирования во время тестирования для языковых агентов на основе больших языковых моделей (LLM) показало улучшение их эффективности. Были изучены различные стратегии, включая параллельную выборку, последовательную корректировку, верификацию и диверсификацию развертываний. Результаты демонстрируют, что увеличение вычислительных ресурсов на этапе тестирования повышает производительность агентов. Особенно эффективными оказались методы списочной верификации и увеличения разнообразия развертываний."
                },
                "en": {
                    "title": "Boosting Language Agents with Test-Time Scaling",
                    "desc": "This paper investigates how increasing computational resources at test time can enhance the performance of large language models (LLMs). It systematically examines various test-time scaling methods, such as parallel sampling, sequential revisions, and verification techniques. The findings indicate that scaling up computation not only boosts reasoning capabilities but also highlights the importance of strategic reflection and diverse rollouts. Notably, the study reveals that the list-wise verification method yields the best results among different merging approaches."
                },
                "zh": {
                    "title": "测试时间扩展提升语言代理性能",
                    "desc": "本文系统探讨了在大型语言模型（LLMs）中应用测试时间扩展方法的效果。研究表明，计算扩展能够显著提升语言代理的推理能力，尤其是通过并行采样、顺序修订、有效验证和增加多样化的回滚策略。我们分析了不同设计策略对语言代理性能的影响，并发现测试时间计算的扩展确实能提高代理的表现。特别是，采用列表式验证方法效果最佳，而多样化的回滚策略也对任务表现有积极影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14429",
            "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
            "url": "https://huggingface.co/papers/2506.14429",
            "abstract": "This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textit{stable perplexity} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textit{local perception} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.",
            "score": 32,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "d0032538675516d6",
            "authors": [
                "Xiaoran Liu",
                "Zhigeng Liu",
                "Zengfeng Huang",
                "Qipeng Guo",
                "Ziwei He",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "School of Computer Science, Fudan University",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14429.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#architecture",
                    "#benchmark",
                    "#diffusion",
                    "#rl"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Диффузионные языковые модели: новые горизонты в обработке длинного контекста",
                    "desc": "Это исследование сравнивает производительность диффузионных и авторегрессивных языковых моделей при работе с длинным контекстом. Авторы обнаружили, что диффузионные модели демонстрируют стабильную перплексивность при расширении контекста и обладают феноменом 'локального восприятия'. На основе этих наблюдений был разработан метод LongLLaDA для расширения контекстного окна диффузионных моделей без дополнительного обучения. Исследование также выявило задачи, в которых диффузионные модели превосходят авторегрессивные при работе с длинным контекстом."
                },
                "en": {
                    "title": "Unlocking Long Contexts in Diffusion LLMs with LongLLaDA",
                    "desc": "This paper explores how diffusion large language models (LLMs) perform with long contexts compared to traditional auto-regressive LLMs. It highlights that diffusion LLMs maintain stable perplexity when extending context, unlike their auto-regressive counterparts, which struggle with longer inputs. The authors introduce LongLLaDA, a method that allows for context window extension without additional training, leveraging insights from Rotary Position Embedding (RoPE) scaling. The findings reveal specific tasks where diffusion LLMs excel and others where they do not, paving the way for future research in long-context applications."
                },
                "zh": {
                    "title": "扩散模型的长上下文新方法：LongLLaDA",
                    "desc": "本研究探讨了扩散大语言模型（diffusion LLMs）与自回归大语言模型（auto-regressive LLMs）在长上下文性能方面的比较，识别了它们的独特特性，并提出了一种无训练的方法LongLLaDA来扩展上下文窗口。研究发现，扩散LLMs在直接上下文外推时保持了显著稳定的困惑度，而自回归模型在上下文超出预训练长度时则表现不佳。扩散LLMs展现出独特的局部感知现象，使其能够成功从最近的上下文片段中检索信息。通过旋转位置嵌入（RoPE）缩放理论，我们解释了这些现象，并验证了扩散LLMs的上下文外推方法的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12285",
            "title": "CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\n  Following",
            "url": "https://huggingface.co/papers/2506.12285",
            "abstract": "CMI-Bench introduces a comprehensive instruction-following benchmark for audio-text LLMs to evaluate them on a diverse range of music information retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking: reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.",
            "score": 32,
            "issue_id": 4360,
            "pub_date": "2025-06-14",
            "pub_date_card": {
                "ru": "14 июня",
                "en": "June 14",
                "zh": "6月14日"
            },
            "hash": "c0c91a24a40dfd14",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#open_source",
                    "#survey",
                    "#audio",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "CMI-Bench: Новый стандарт оценки музыкальных LLM",
                    "desc": "CMI-Bench представляет собой комплексный бенчмарк для оценки аудио-текстовых языковых моделей (LLM) в задачах извлечения музыкальной информации. Бенчмарк включает широкий спектр задач, таких как классификация жанров, распознавание эмоций, определение инструментов и транскрипция текстов. CMI-Bench использует стандартизированные метрики оценки, что обеспечивает прямое сравнение с современными специализированными моделями. Результаты экспериментов выявляют значительные различия в производительности между LLM и специализированными моделями, а также культурные, хронологические и гендерные предубеждения LLM."
                },
                "en": {
                    "title": "CMI-Bench: Advancing Music Understanding with LLMs",
                    "desc": "CMI-Bench is a new benchmark designed to evaluate audio-text large language models (LLMs) on various music information retrieval (MIR) tasks. It addresses the limitations of existing benchmarks by providing a comprehensive set of instruction-following tasks that reflect real-world music analysis complexities. The benchmark includes diverse tasks such as genre classification, emotion tagging, and melody extraction, using standardized evaluation metrics for consistency with state-of-the-art models. Results from experiments show performance gaps between LLMs and supervised models, revealing both the potential and limitations of current LLMs in handling MIR challenges."
                },
                "zh": {
                    "title": "CMI-Bench：音乐信息检索的新基准",
                    "desc": "CMI-Bench是一个全面的音频文本大语言模型（LLM）指令跟随基准，旨在评估它们在多样化的音乐信息检索（MIR）任务上的表现。该基准重新解释了传统MIR注释为指令跟随格式，涵盖了如流派分类、情感回归、乐器分类等多项任务。与以往的基准不同，CMI-Bench采用标准化评估指标，确保与现有的监督学习模型直接可比。实验结果显示LLM与监督模型之间存在显著的性能差距，揭示了当前模型在MIR任务中的潜力和局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14245",
            "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs",
            "url": "https://huggingface.co/papers/2506.14245",
            "abstract": "RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the Pass@K metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the Pass@K metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, CoT-Pass@K, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using CoT-Pass@K, we observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.",
            "score": 23,
            "issue_id": 4348,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "c78cc63a970ea4e9",
            "authors": [
                "Xumeng Wen",
                "Zihan Liu",
                "Shun Zheng",
                "Zhijian Xu",
                "Shengyu Ye",
                "Zhirong Wu",
                "Xiao Liang",
                "Yang Wang",
                "Junjie Li",
                "Ziming Miao",
                "Jiang Bian",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Peking University",
                "The Chinese University of Hong Kong",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14245.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RLVR: путь к логически обоснованным рассуждениям ИИ",
                    "desc": "Статья представляет новый подход к улучшению рассуждений моделей машинного обучения - Reinforcement Learning with Verifiable Rewards (RLVR). Авторы вводят более точную метрику оценки CoT-Pass@K, которая учитывает корректность как цепочки рассуждений, так и конечного ответа. Исследование показывает, что RLVR действительно способствует обобщению правильных рассуждений для всех значений K. Результаты подтверждают потенциал RLVR для существенного улучшения машинных рассуждений."
                },
                "en": {
                    "title": "Enhancing Machine Reasoning with RLVR and CoT-Pass@K",
                    "desc": "Reinforcement Learning with Verifiable Rewards (RLVR) enhances the reasoning abilities of Large Language Models (LLMs) by promoting logical thought processes. The study identifies a flaw in the existing evaluation metric, Pass@K, which inaccurately rewards correct answers that may stem from faulty reasoning paths. To improve this, the authors propose a new metric, CoT-Pass@K, that ensures both the reasoning chain and the final answer are accurate. The findings demonstrate that RLVR can effectively encourage correct reasoning from the early stages of training, leading to better generalization across various scenarios."
                },
                "zh": {
                    "title": "RLVR：推动机器推理的新方法",
                    "desc": "RLVR（可验证奖励的强化学习）通过激励正确和逻辑的思维链，推动了机器推理的发展。研究发现，传统的评估指标Pass@K存在缺陷，可能会错误地认可不完整的思维链所得到的正确答案。为了解决这个问题，本文提出了更精确的评估指标CoT-Pass@K，要求推理路径和最终答案都必须正确。我们的实验证明，RLVR能够有效激励正确推理的泛化，并且这种增强的推理能力在训练早期就能显现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14234",
            "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team",
            "url": "https://huggingface.co/papers/2506.14234",
            "abstract": "Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.",
            "score": 22,
            "issue_id": 4348,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "70ebdc96484832ea",
            "authors": [
                "Md Tanzib Hosain",
                "Salman Rahman",
                "Md Kishor Morol",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "American International University-Bangladesh",
                "Cornell University",
                "Qatar Computing Research Institute",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14234.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#agi",
                    "#open_source",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Xolver: Опыт-ориентированные языковые агенты для экспертного рассуждения",
                    "desc": "Xolver - это фреймворк мультиагентного рассуждения, который улучшает работу больших языковых моделей (LLM) за счет постоянной памяти и разнообразных модальностей опыта. Он позволяет моделям накапливать и интегрировать экспериентальные знания, подобно экспертам-решателям задач. Xolver включает в себя различные модальности опыта, такие как внешний и самостоятельный поиск, использование инструментов, совместные взаимодействия и итеративное улучшение. Даже с легковесными моделями Xolver превосходит специализированные агенты рассуждений и достигает новых лучших результатов на нескольких бенчмарках."
                },
                "en": {
                    "title": "Empowering Language Models with Experience-Aware Reasoning",
                    "desc": "Xolver is a multi-agent reasoning framework designed to enhance large language models (LLMs) by incorporating persistent memory and diverse experience modalities. Unlike traditional LLMs that treat each problem independently, Xolver allows agents to accumulate knowledge from past experiences, similar to expert problem solvers. This framework integrates various methods such as self-retrieval, tool usage, and collaborative interactions to refine reasoning and improve performance on complex tasks. As a result, Xolver consistently outperforms specialized reasoning agents, achieving state-of-the-art results on several benchmarks, demonstrating the importance of experience-aware learning in AI."
                },
                "zh": {
                    "title": "Xolver：经验驱动的推理框架",
                    "desc": "Xolver是一个多智能体推理框架，通过持久记忆和多样化的经验模式增强大型语言模型（LLM），从而提高复杂推理任务的表现。与传统的LLM孤立处理每个问题不同，Xolver能够整合和积累经验知识，模拟专家问题解决者的思维方式。它通过外部和自我检索、工具使用、协作互动等多种经验模式，避免从头生成解决方案。Xolver在多个基准测试中表现优异，展示了整体经验学习在实现通用智能体方面的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13642",
            "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
            "url": "https://huggingface.co/papers/2506.13642",
            "abstract": "Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.",
            "score": 21,
            "issue_id": 4347,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "0d0624980a111254",
            "authors": [
                "Shaolei Zhang",
                "Shoutao Guo",
                "Qingkai Fang",
                "Yan Zhou",
                "Yang Feng"
            ],
            "affiliations": [
                "Key Laboratory of AI Safety, Chinese Academy of Sciences",
                "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13642.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#audio",
                    "#transfer_learning",
                    "#cv",
                    "#benchmark",
                    "#agi"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Эффективная интеграция модальностей для мощных мультимодальных ИИ-моделей",
                    "desc": "Stream-Omni - это крупная мультимодальная модель, объединяющая текст, изображения и речь. Она использует конкатенацию по измерению последовательности для визуальной информации и отображение по измерению слоев для речи, что позволяет эффективно выравнивать модальности. Модель достигает высокой производительности с меньшим количеством данных по сравнению с существующими подходами. Stream-Omni демонстрирует сильные результаты в задачах визуального понимания, речевого взаимодействия и речевого взаимодействия на основе изображений."
                },
                "en": {
                    "title": "Stream-Omni: Efficient Multimodal Integration for Enhanced Interaction",
                    "desc": "Stream-Omni is a large multimodal model that effectively integrates text, vision, and speech by using innovative alignment techniques. It employs sequence-dimension concatenation for aligning vision with text and a layer-dimension mapping for aligning speech with text, which allows for more efficient learning of modality relationships. This approach reduces the reliance on large datasets, particularly for speech, while still achieving strong performance across various multimodal tasks. The model's design enables it to provide intermediate outputs during speech interactions, enhancing the overall user experience in multimodal applications."
                },
                "zh": {
                    "title": "Stream-Omni：高效的多模态整合模型",
                    "desc": "Stream-Omni是一种大型多模态模型，能够有效整合文本、视觉和语音。它通过序列维度连接实现视觉与文本的对齐，并通过基于CTC的层维度映射实现语音与文本的对齐，从而在数据较少的情况下也能达到良好的性能。该模型支持多种模态组合的交互，能够在视觉理解、语音交互和视觉引导的语音交互任务中表现出色。Stream-Omni的设计使得用户在语音交互时可以同时获得中间文本输出，提供了全面的多模态体验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13363",
            "title": "Efficient Medical VIE via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.13363",
            "abstract": "An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE.",
            "score": 21,
            "issue_id": 4348,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "29de6bf10e7470ad",
            "authors": [
                "Lijun Liu",
                "Ruiyang Li",
                "Zhaocheng Liu",
                "Chenglin Zhu",
                "Chong Li",
                "Jiehan Cheng",
                "Qiang Ju",
                "Jian Xie"
            ],
            "affiliations": [
                "Baichuan Inc.",
                "Peking University",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13363.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#training",
                    "#optimization",
                    "#healthcare",
                    "#reasoning",
                    "#rl",
                    "#multimodal"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "RLVR: Прорыв в извлечении медицинской информации из изображений",
                    "desc": "Представлена система RLVR на основе модели Qwen2.5-VL-7B для извлечения визуальной информации из медицинских документов. Система достигает наилучших результатов при ограниченном количестве размеченных примеров. RLVR улучшает способности к рассуждению и балансирует точность и полноту извлечения. Метод показывает высокую эффективность на медицинских данных, но требует оптимизации для других доменов."
                },
                "en": {
                    "title": "Revolutionizing Medical VIE with Limited Data and Enhanced Reasoning",
                    "desc": "This paper presents a Reinforcement Learning with Verifiable Rewards (RLVR) framework that utilizes a fine-tuned Qwen2.5-VL-7B model to enhance Visual Information Extraction (VIE) in medical contexts. By leveraging only 100 annotated samples, the framework effectively balances precision and recall, addressing the challenges posed by limited annotated data and high annotation costs. The approach incorporates innovative sampling strategies and a balanced reward mechanism to improve reasoning capabilities and reduce hallucinations in the output. The results show significant improvements in F1 score, precision, and recall, although the model's performance varies with the similarity of the tasks to the training data."
                },
                "zh": {
                    "title": "医疗视觉信息提取的创新突破",
                    "desc": "本文提出了一种基于强化学习可验证奖励（RLVR）框架的方法，利用微调的Qwen2.5-VL-7B模型，在医疗视觉信息提取（VIE）任务中实现了最先进的性能。该方法仅使用100个标注样本，解决了传统方法在医疗领域面临的高标注成本和领域特定模式的问题。通过确保数据集的多样性和平衡的精确率-召回率奖励机制，减少了模型的幻觉现象，并提高了领域覆盖率。案例研究进一步证明了在训练和推理过程中推理能力的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14758",
            "title": "Reasoning with Exploration: An Entropy Perspective",
            "url": "https://huggingface.co/papers/2506.14758",
            "abstract": "Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.",
            "score": 17,
            "issue_id": 4349,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "14595ff25bf8a37c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Энтропия как ключ к глубоким рассуждениям языковых моделей",
                    "desc": "Статья представляет новый подход к улучшению рассуждений языковых моделей с помощью обучения с подкреплением. Авторы вводят энтропийный член в функцию преимущества, что способствует исследовательскому поведению модели. Этот метод приводит к значительному улучшению производительности на сложных задачах рассуждения, особенно по метрике Pass@K. Исследование показывает, что высокая энтропия коррелирует с ключевыми токенами, рефлексивными действиями и редкими поведениями модели."
                },
                "en": {
                    "title": "Enhancing Language Model Reasoning through Entropy-Driven Exploration",
                    "desc": "This paper introduces a new approach to enhance exploratory reasoning in language models (LMs) by modifying the advantage function in reinforcement learning (RL) with an entropy-based term. The authors highlight that traditional methods often focus on exploitation, leading to performance plateaus, and argue that incorporating entropy can promote better exploration. Their empirical analysis shows that high-entropy regions correlate with key reasoning actions, such as pivotal tokens and reflective behaviors. The proposed method not only encourages deeper reasoning chains but also significantly improves performance on the Pass@K metric, demonstrating its effectiveness in advancing LM reasoning capabilities."
                },
                "zh": {
                    "title": "增强语言模型推理的探索性",
                    "desc": "本文提出了一种基于熵的术语，应用于强化学习中的优势函数，以增强语言模型的探索性推理能力。这种方法通过引入熵信号，促进了探索与利用之间的平衡，解决了语言模型在复杂推理任务中性能停滞的问题。研究表明，高熵区域与三种探索性推理行为之间存在强正相关，包括关键标记、反思性行为和稀有行为。通过简单的代码修改，我们的方法显著提高了语言模型的推理能力，尤其在Pass@K指标上取得了显著进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12860",
            "title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning",
            "url": "https://huggingface.co/papers/2506.12860",
            "abstract": "Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios.",
            "score": 13,
            "issue_id": 4348,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 июня",
                "en": "June 15",
                "zh": "6月15日"
            },
            "hash": "4e8d6c1da3d2fdd1",
            "authors": [
                "Wanlong Liu",
                "Junxiao Xu",
                "Fei Yu",
                "Yukang Lin",
                "Ke Ji",
                "Wenyu Chen",
                "Yan Xu",
                "Yasheng Wang",
                "Lifeng Shang",
                "Benyou Wang"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong, Shenzhen",
                "University of Electronic Science and Technology of China, Chengdu, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12860.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#long_context",
                    "#reasoning",
                    "#low_resource"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "QFFT: эффективное обучение ИИ гибкому мышлению",
                    "desc": "Статья представляет новый метод обучения когнитивных моделей - Question-Free Fine-Tuning (QFFT). QFFT позволяет моделям эффективно использовать как короткие, так и длинные цепочки рассуждений. Этот подход сокращает среднюю длину ответов более чем на 50%, сохраняя при этом производительность на уровне стандартного обучения с учителем. QFFT также демонстрирует превосходные результаты в сценариях с шумом, вне домена и при ограниченных ресурсах."
                },
                "en": {
                    "title": "Efficient Reasoning with Question-Free Fine-Tuning",
                    "desc": "This paper introduces Question-Free Fine-Tuning (QFFT), a method that enhances cognitive models by combining short and long chain-of-thought reasoning patterns. QFFT addresses the issue of overthinking in Long Chain-of-Thought models, which can lead to unnecessary complexity in responses. By training models without input questions, QFFT allows them to learn from Long CoT responses while primarily using Short CoT patterns for efficiency. The results show that QFFT significantly reduces response length and performs well across various challenging scenarios, outperforming traditional Supervised Fine-Tuning methods in specific contexts."
                },
                "zh": {
                    "title": "无问微调：高效适应的推理新方法",
                    "desc": "这篇论文提出了一种新的微调方法，称为无问微调（QFFT），旨在提高认知模型的效率和适应性。通过结合短链和长链推理模式，QFFT能够在保持性能的同时减少响应长度。研究表明，短链推理在简单问题上表现出色，而长链推理在复杂任务中更具优势。实验结果显示，QFFT在多个数学数据集上平均响应长度减少超过50%，并在噪声、域外和低资源场景中表现优于传统的监督微调（SFT）。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14603",
            "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
            "url": "https://huggingface.co/papers/2506.14603",
            "abstract": "Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.",
            "score": 12,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "c235653dff87ea28",
            "authors": [
                "Amirmojtaba Sabour",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14603.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#diffusion",
                    "#small_models"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Flow maps: революция в эффективной генерации изображений",
                    "desc": "Статья представляет новый подход к генеративному моделированию под названием 'flow maps'. Эта техника позволяет эффективно соединять любые два уровня шума за один шаг, сохраняя высокую производительность при различном количестве шагов. Авторы вводят новые непрерывные по времени целевые функции и техники обучения для flow maps. Модели, обученные с помощью этого метода, достигают наилучших результатов в генерации изображений и преобразовании текста в изображения за небольшое количество шагов."
                },
                "en": {
                    "title": "Flow Maps: Efficient Few-Step Generative Modeling",
                    "desc": "This paper introduces flow maps, a new approach in generative modeling that connects different noise levels in a single step, allowing for efficient image and text-to-image generation. Unlike traditional diffusion and consistency models, which require many sampling steps and degrade in performance with increased steps, flow maps maintain effectiveness across all step counts. The authors propose two continuous-time training objectives and novel techniques that enhance the training of flow maps, including autoguidance and adversarial finetuning. The results demonstrate that their method, called Align Your Flow, achieves state-of-the-art performance in few-step generation tasks on various benchmarks, outperforming existing models in both image and text-conditioned synthesis."
                },
                "zh": {
                    "title": "流图模型：高效的少步骤生成新方法",
                    "desc": "本文介绍了一种新的流图模型，旨在提高图像和文本到图像生成的效率。流图通过在单一步骤中连接任意两个噪声水平，克服了传统扩散和流模型在多步骤采样中的性能下降问题。我们提出了两种新的连续时间目标和训练技术，进一步优化了流图的训练过程。实验结果表明，流图模型在图像生成基准测试中表现出色，尤其是在少步骤生成任务中，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12278",
            "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure",
            "url": "https://huggingface.co/papers/2506.12278",
            "abstract": "TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.",
            "score": 12,
            "issue_id": 4348,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "c852db550c523453",
            "authors": [
                "Zheyuan Yang",
                "Zexi Kuang",
                "Xue Xia",
                "Yilun Zhao"
            ],
            "affiliations": [
                "HKUST",
                "Northeastern University",
                "Tongji University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12278.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "TestCase-Eval: Новый стандарт оценки ЯМ в генерации тестов",
                    "desc": "TestCase-Eval - это новый бенчмарк для оценки способности языковых моделей (ЯМ) генерировать тестовые случаи для алгоритмических задач. Он включает 500 задач и 100 000 решений с платформы Codeforces. Бенчмарк фокусируется на двух ключевых задачах: покрытие ошибок и выявление ошибок. Авторы провели оценку 19 современных ЯМ на TestCase-Eval, предоставив анализ их сильных и слабых сторон в генерации эффективных тестовых случаев."
                },
                "en": {
                    "title": "Evaluating LLMs for Effective Test Case Generation",
                    "desc": "TestCase-Eval is a benchmark designed to assess the performance of large language models (LLMs) in generating effective test cases for algorithmic problems. It consists of 500 algorithm problems paired with 100,000 human-created solutions sourced from the Codeforces platform. The evaluation focuses on two main aspects: Fault Coverage, which checks how well the generated test cases explore various input scenarios, and Fault Exposure, which determines the ability of LLMs to create specific test inputs that can uncover flaws in code implementations. The study evaluates 19 different LLMs, providing valuable insights into their capabilities and limitations in this area."
                },
                "zh": {
                    "title": "评估LLM生成测试用例的新基准",
                    "desc": "TestCase-Eval是一个用于评估大型语言模型（LLMs）生成算法问题测试用例的新基准。它包含500个算法问题和来自Codeforces平台的100,000个人工解决方案。该基准关注两个关键任务：故障覆盖性，评估LLM生成的测试集是否能够探测多样的输入场景；故障暴露性，评估LLM是否能够生成特定的测试输入以揭示代码实现中的错误。我们对19个最先进的开源和专有LLM在TestCase-Eval上的表现进行了全面评估，提供了它们在生成有效测试用例方面的优缺点的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14606",
            "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
            "url": "https://huggingface.co/papers/2506.14606",
            "abstract": "A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.",
            "score": 10,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "c414a1f31e0417da",
            "authors": [
                "Ahmed Heakl",
                "Sarim Hashmi",
                "Chaimaa Abi",
                "Celine Lee",
                "Abdulrahman Mahmoud"
            ],
            "affiliations": [
                "Cornell University",
                "MBZUAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14606.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#data",
                    "#science"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "ЯМБ и тестирование объединяются для эффективной транспиляции между архитектурами процессоров",
                    "desc": "Статья представляет новый подход к транспиляции программ между различными архитектурами процессоров с использованием больших языковых моделей и методов тестирования программного обеспечения. Метод GG (Guaranteed Guess) генерирует варианты перевода кода с одной архитектуры на другую с помощью ЯМБ и проверяет их корректность через встроенную систему тестирования. Результаты показывают высокую точность перевода (99% для HumanEval и 49% для BringupBench) и превосходство над существующими решениями по производительности, энергоэффективности и использованию памяти. Авторы планируют открыть исходный код, данные и модели для дальнейших исследований в этой области."
                },
                "en": {
                    "title": "Efficient ISA Translation with Guaranteed Guess",
                    "desc": "This paper presents a new transpilation pipeline called GG (Guaranteed Guess) that focuses on translating programs between complex (CISC) and reduced (RISC) instruction set architectures (ISAs). By leveraging large language models (LLMs) for generating translation candidates, the pipeline integrates software testing to ensure high correctness and efficiency. The authors demonstrate that their approach achieves over 99% functional correctness on specific benchmarks and outperforms the existing Rosetta 2 framework in terms of runtime speed, energy efficiency, and memory usage. The research aims to enhance the portability of code across different hardware architectures and will provide open-source resources for further exploration in ISA-level code translation."
                },
                "zh": {
                    "title": "高效准确的ISA转译新方法",
                    "desc": "本文提出了一种新的ISA中心的转译管道，利用大型语言模型（LLMs）和软件测试技术，实现了在复杂和简化硬件架构之间的高效且正确的代码转换。该方法通过LLM生成候选翻译，并将其嵌入软件测试框架中，以提高翻译的可靠性。我们在两个不同的数据集上评估了该方法，达到了99%的功能和语义正确率，并且在性能上优于现有的Rosetta 2框架。最终，我们将开源代码、数据、模型和基准，以推动ISA级代码翻译研究的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13977",
            "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios",
            "url": "https://huggingface.co/papers/2506.13977",
            "abstract": "A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at https://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.",
            "score": 7,
            "issue_id": 4351,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "d72fbcfec0e28e92",
            "authors": [
                "Shiting Huang",
                "Zhen Fang",
                "Zehui Chen",
                "Siyu Yuan",
                "Junjie Ye",
                "Yu Zeng",
                "Lin Chen",
                "Qi Mao",
                "Feng Zhao"
            ],
            "affiliations": [
                "Communication University of China",
                "Fudan University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13977.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "Повышение надежности языковых моделей при работе с инструментами",
                    "desc": "CRITICTOOL - это комплексный бенчмарк для оценки и повышения устойчивости больших языковых моделей (LLM) при обработке ошибок во время использования инструментов. Он анализирует типы ошибок, возникающих в процессе вызова функций, и использует эволюционную стратегию для создания набора данных с разнообразными ошибками различной сложности. CRITICTOOL проводит обширные эксперименты для оценки способности LLM к рефлексии при использовании инструментов. Этот бенчмарк предлагает новый взгляд на область обучения инструментам в контексте больших языковых моделей."
                },
                "en": {
                    "title": "Enhancing LLM Robustness with CRITICTOOL",
                    "desc": "This paper introduces CRITICTOOL, a benchmark designed to assess and improve the robustness of large language models (LLMs) when using external tools. It identifies and categorizes various errors that can occur during the function-calling process, especially as tasks become more complex. The benchmark employs an innovative evolutionary strategy for dataset construction, ensuring a wide range of tool-use errors that mimic real-world challenges. Through extensive experiments, the authors demonstrate the effectiveness of CRITICTOOL in enhancing the error-handling capabilities of LLMs and provide insights into their tool reflection abilities."
                },
                "zh": {
                    "title": "提升大型语言模型工具使用的鲁棒性",
                    "desc": "本文介绍了CRITICTOOL，一个全面的基准测试工具，用于评估和增强大型语言模型在使用工具时处理错误的能力。随着任务的复杂性增加，工具使用过程中可能会出现各种意外错误，因此有效处理这些错误成为了一个重要的研究方向。我们分析了在多个竞争性工具评估基准中遇到的错误类型，并基于此构建了CRITICTOOL，专注于工具学习的批判性评估。通过广泛的实验，我们验证了该基准策略的有效性，并提供了对不同大型语言模型工具反应能力的深入分析。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13651",
            "title": "xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations",
            "url": "https://huggingface.co/papers/2506.13651",
            "abstract": "We introduce xbench, a dynamic, profession-aligned evaluation suite designed to bridge the gap between AI agent capabilities and real-world productivity. While existing benchmarks often focus on isolated technical skills, they may not accurately reflect the economic value agents deliver in professional settings. To address this, xbench targets commercially significant domains with evaluation tasks defined by industry professionals. Our framework creates metrics that strongly correlate with productivity value, enables prediction of Technology-Market Fit (TMF), and facilitates tracking of product capabilities over time. As our initial implementations, we present two benchmarks: Recruitment and Marketing. For Recruitment, we collect 50 tasks from real-world headhunting business scenarios to evaluate agents' abilities in company mapping, information retrieval, and talent sourcing. For Marketing, we assess agents' ability to match influencers with advertiser needs, evaluating their performance across 50 advertiser requirements using a curated pool of 836 candidate influencers. We present initial evaluation results for leading contemporary agents, establishing a baseline for these professional domains. Our continuously updated evalsets and evaluations are available at https://xbench.org.",
            "score": 6,
            "issue_id": 4351,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "ce4f94d367671b84",
            "authors": [
                "Kaiyuan Chen",
                "Yixin Ren",
                "Yang Liu",
                "Xiaobo Hu",
                "Haotong Tian",
                "Tianbao Xie",
                "Fangfu Liu",
                "Haoye Zhang",
                "Hongzhang Liu",
                "Yuan Gong",
                "Chen Sun",
                "Han Hou",
                "Hui Yang",
                "James Pan",
                "Jianan Lou",
                "Jiayi Mao",
                "Jizheng Liu",
                "Jinpeng Li",
                "Kangyi Liu",
                "Kenkun Liu",
                "Rui Wang",
                "Run Li",
                "Tong Niu",
                "Wenlong Zhang",
                "Wenqi Yan",
                "Xuanzheng Wang",
                "Yuchen Zhang",
                "Yi-Hsin Hung",
                "Yuan Jiang",
                "Zexuan Liu",
                "Zihan Yin",
                "Zijian Ma",
                "Zhiwen Mo"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Fudan University",
                "Imperial College London",
                "Massachusetts Institute of Technology",
                "National University of Singapore",
                "Peking University",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "The Chinese University of Hong Kong (Shenzhen)",
                "The Ohio State University",
                "Tsinghua University",
                "University of Chinese Academy of Sciences",
                "University of Oxford",
                "University of Pennsylvania",
                "University of Science and Technology of China",
                "University of Sydney",
                "University of Toronto",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13651.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "xbench: оценка ИИ-агентов в реальных профессиональных задачах",
                    "desc": "Представлен xbench - динамический набор оценок для ИИ-агентов, ориентированный на реальные профессиональные задачи. В отличие от существующих бенчмарков, xbench фокусируется на коммерчески значимых областях с задачами, определенными профессионалами индустрии. Фреймворк создает метрики, коррелирующие с продуктивностью, позволяет прогнозировать соответствие технологий рынку и отслеживать возможности продуктов. Представлены два начальных бенчмарка: для рекрутинга и маркетинга, оценивающие способности агентов в реальных бизнес-сценариях."
                },
                "en": {
                    "title": "Bridging AI Performance with Real-World Productivity",
                    "desc": "The paper introduces xbench, a new evaluation suite aimed at assessing AI agents in real-world professional contexts. Unlike traditional benchmarks that focus on isolated skills, xbench emphasizes the economic impact of AI agents in industries like recruitment and marketing. It includes tasks defined by industry experts to ensure relevance and creates metrics that correlate with productivity value. The initial benchmarks evaluate agents' performance in real-world scenarios, providing a baseline for future assessments and tracking improvements over time."
                },
                "zh": {
                    "title": "xbench：连接AI能力与真实生产力的桥梁",
                    "desc": "我们介绍了xbench，这是一个动态的、与职业相关的评估套件，旨在弥合人工智能代理能力与现实世界生产力之间的差距。现有的基准测试通常关注孤立的技术技能，但可能无法准确反映代理在专业环境中所带来的经济价值。为了解决这个问题，xbench针对商业上重要的领域，评估任务由行业专业人士定义。我们的框架创建了与生产力价值高度相关的指标，能够预测技术市场契合度（TMF），并便于跟踪产品能力的变化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10100",
            "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2506.10100",
            "abstract": "EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.",
            "score": 5,
            "issue_id": 4348,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "6a877c4c5f1d1f72",
            "authors": [
                "Yantai Yang",
                "Yuhao Wang",
                "Zichen Wen",
                "Luo Zhongwei",
                "Chang Zou",
                "Zhipeng Zhang",
                "Chuan Wen",
                "Linfeng Zhang"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "School of Artificial Intelligence, Shanghai Jiao Tong University",
                "University of Electronic Science and Technology of China",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10100.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#architecture",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "EfficientVLA: ускорение моделей VLA без потери качества",
                    "desc": "EfficientVLA - это фреймворк для ускорения вывода моделей Vision-Language-Action (VLA). Он использует три основные стратегии: обрезание избыточных слоев в языковом модуле, оптимизацию отбора визуальных токенов и кэширование промежуточных признаков в диффузионной голове действий. Применение EfficientVLA к модели CogACT позволило достичь ускорения вывода в 1,93 раза и сокращения FLOP на 71,1% при минимальном снижении точности. Этот подход позволяет эффективно устранить вычислительные и память барьеры в моделях VLA."
                },
                "en": {
                    "title": "Accelerating VLA Models with EfficientVLA",
                    "desc": "EfficientVLA is a framework designed to speed up Vision-Language-Action (VLA) models by addressing their high computational and memory requirements. It achieves this by pruning unnecessary language layers, optimizing the selection of visual tokens, and caching important features during the action generation process. This approach not only reduces the overall processing time but also minimizes the number of floating-point operations (FLOPs) needed for inference. As a result, EfficientVLA significantly enhances the efficiency of VLA models while maintaining a high level of performance."
                },
                "zh": {
                    "title": "高效加速视觉-语言-动作模型的解决方案",
                    "desc": "EfficientVLA是一种加速视觉-语言-动作（VLA）模型的框架，通过修剪语言层、优化视觉标记选择和缓存中间特征来提高效率。该方法系统性地消除了计算和内存瓶颈，解决了现有加速方法无法全面应对的问题。通过分析层间冗余，EfficientVLA去除了功能不重要的语言模块层，并采用任务感知策略优化视觉处理路径。实验结果表明，应用EfficientVLA后，标准VLA模型CogACT的推理速度提高了1.93倍，FLOPs减少至28.9%，成功率仅下降0.6%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09985",
            "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\n  and Planning",
            "url": "https://huggingface.co/papers/2506.09985",
            "abstract": "A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.  \t\t\t\t\tAI-generated summary \t\t\t\t A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.",
            "score": 5,
            "issue_id": 4359,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "2a9d0d368d8caa0a",
            "authors": [
                "Mido Assran",
                "Adrien Bardes",
                "David Fan",
                "Quentin Garrido",
                "Russell Howes",
                "Mojtaba",
                "Komeili",
                "Matthew Muckley",
                "Ammar Rizvi",
                "Claire Roberts",
                "Koustuv Sinha",
                "Artem Zholus",
                "Sergio Arnaud",
                "Abha Gejji",
                "Ada Martin",
                "Francois Robert Hogan",
                "Daniel Dugas",
                "Piotr Bojanowski",
                "Vasil Khalidov",
                "Patrick Labatut",
                "Francisco Massa",
                "Marc Szafraniec",
                "Kapil Krishnakumar",
                "Yong Li",
                "Xiaodong Ma",
                "Sarath Chandar",
                "Franziska Meier",
                "Yann LeCun",
                "Michael Rabbat",
                "Nicolas Ballas"
            ],
            "affiliations": [
                "FAIR at Meta",
                "Mila Quebec AI Institute and Polytechnique Montréal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09985.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#transfer_learning",
                    "#dataset",
                    "#agi",
                    "#cv",
                    "#robotics",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самообучение на масштабных данных для понимания и планирования в физическом мире",
                    "desc": "Статья представляет самообучающийся подход, сочетающий масштабные видеоданные из интернета с минимальным взаимодействием робота. Модель V-JEPA 2 предобучается на более чем 1 миллионе часов видео и достигает высоких результатов в понимании движений и предсказании действий. После выравнивания с большой языковой моделью, V-JEPA 2 показывает передовые результаты в задачах ответов на вопросы по видео. Дополнительное обучение на 62 часах видео с роботами позволяет модели V-JEPA 2-AC осуществлять планирование действий для манипуляторов без специфического обучения под задачу."
                },
                "en": {
                    "title": "Learning to Act by Watching: Self-Supervised Motion Understanding and Planning",
                    "desc": "This paper presents a self-supervised learning method that leverages vast amounts of internet video data alongside minimal robot interaction to enhance motion understanding, action anticipation, and robotic planning. The authors introduce V-JEPA 2, a joint-embedding-predictive architecture pre-trained on over 1 million hours of video, achieving impressive results in various tasks without the need for specific training or rewards. By aligning V-JEPA 2 with a large language model, they also achieve state-of-the-art performance in video question-answering tasks. Furthermore, they demonstrate the application of this approach in robotic planning, enabling robots to perform tasks like object manipulation using learned models without additional data collection."
                },
                "zh": {
                    "title": "自监督学习：从视频到机器人规划的突破",
                    "desc": "本论文提出了一种自监督学习方法，结合了互联网视频数据和少量机器人交互数据，以实现运动理解、动作预测、视频问答和机器人规划等任务。我们首先在超过100万小时的视频数据集上预训练了一个无动作的联合嵌入预测架构V-JEPA 2，取得了运动理解和人类动作预测的优异表现。通过与大型语言模型对齐，V-JEPA 2在多个视频问答任务上也达到了最先进的性能。最后，我们展示了如何将自监督学习应用于机器人规划任务，成功实现了在不同实验室中使用V-JEPA 2-AC进行物体的抓取和放置，而无需特定任务的训练或奖励。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05336",
            "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
            "url": "https://huggingface.co/papers/2506.05336",
            "abstract": "VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at https://github.com/mbzuai-oryx/VideoMolmo.",
            "score": 5,
            "issue_id": 4348,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "def5ee56ea3b6157",
            "authors": [
                "Ghazi Shazan Ahmad",
                "Ahmed Heakl",
                "Hanan Gani",
                "Abdelrahman Shaker",
                "Zhiqiang Shen",
                "Ranjay Krishna",
                "Fahad Shahbaz Khan",
                "Salman Khan"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Australian National University",
                "Linköping University",
                "Mohamed Bin Zayed University of Artificial Intelligence",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05336.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#interpretability",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точная локализация объектов в видео с помощью ИИ",
                    "desc": "VideoMolmo - это мультимодальная модель для точной пространственно-временной локализации объектов в видео на основе текстовых описаний. Она использует временной модуль с механизмом внимания для обеспечения временной согласованности между кадрами. Модель также применяет SAM2 для двунаправленного распространения точек, что значительно улучшает согласованность масок объектов в видеопоследовательностях. VideoMolmo превосходит существующие модели по точности указания объектов и способности к рассуждениям в различных сценариях реального мира."
                },
                "en": {
                    "title": "Enhancing Spatio-Temporal Reasoning with VideoMolmo",
                    "desc": "VideoMolmo is a multimodal model designed to improve spatio-temporal pointing accuracy and reasoning in various real-world applications. It combines a temporal attention mechanism with a novel mask fusion technique called SAM2, which enhances the coherence of video sequences. By generating precise pointing coordinates through a large language model and then refining them with a mask-fusion module, VideoMolmo simplifies the task and improves interpretability. The model is evaluated on a newly curated dataset and a challenging benchmark, demonstrating significant advancements over existing video-based approaches."
                },
                "zh": {
                    "title": "VideoMolmo：提升时空指向与推理能力的多模态模型",
                    "desc": "VideoMolmo是一种多模态模型，结合了时间注意机制和SAM2进行掩膜融合，显著提高了时空指向的准确性和推理能力。该模型专为基于文本描述的细粒度时空指向而设计，能够在不同的真实场景中进行精确交互。通过引入时间模块和双向点传播的掩膜融合管道，VideoMolmo确保了视频序列的时间一致性和连贯性。我们还创建了一个包含72,000个视频-字幕对的数据集，以评估模型在多种真实场景中的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14755",
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "url": "https://huggingface.co/papers/2506.14755",
            "abstract": "LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as \"invalid thinking\" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.",
            "score": 4,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "837a56d067dd6e74",
            "authors": [
                "Zhengxiang Cheng",
                "Dongping Chen",
                "Mingyang Fu",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14755.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "LC-R1: Оптимизация рассуждений ИИ без потери качества",
                    "desc": "LC-R1 - это метод пост-обучения для больших моделей рассуждений (LRM), основанный на принципах краткости и достаточности. Он нацелен на устранение избыточных рассуждений в выводах моделей без существенной потери точности. LC-R1 использует комбинацию наград за длину и сжатие в рамках оптимизации групповой относительной политики (GRPO). Эксперименты показывают, что метод сокращает длину последовательностей примерно на 50% при падении точности всего на 2%."
                },
                "en": {
                    "title": "Streamlining Reasoning: LC-R1 for Efficient Large Models",
                    "desc": "The paper introduces LC-R1, a post-training method aimed at improving Large Reasoning Models (LRMs) by reducing unnecessary reasoning while maintaining accuracy. It identifies 'invalid thinking' as a key issue where models redundantly verify correct answers, leading to verbosity. To combat this, the authors propose two principles: Brevity, which focuses on cutting out redundant reasoning, and Sufficiency, which ensures essential reasoning steps are retained. Through experiments, LC-R1 demonstrates a significant reduction in reasoning sequence length by about 50% with only a slight accuracy drop of around 2%, showcasing an effective balance between compression and performance."
                },
                "zh": {
                    "title": "简化推理，提升效率！",
                    "desc": "LC-R1是一种后训练方法，旨在通过简洁性和充分性原则来减少大型推理模型中的不必要推理，同时保持较小的准确性损失。该方法识别出模型在推理过程中存在的“无效思维”问题，即模型在得出正确答案后仍然反复检查。为了解决这一低效问题，LC-R1提出了两个新原则：简洁性，强调消除冗余；充分性，确保关键推理步骤得以保留。通过对多个推理基准的广泛实验，LC-R1实现了序列长度的显著减少（约50%），而准确性仅下降约2%，在高压缩率和准确性之间达成了良好的平衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14002",
            "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
            "url": "https://huggingface.co/papers/2506.14002",
            "abstract": "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically prove that this algorithm correctly recovers all monosemantic features when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.",
            "score": 4,
            "issue_id": 4347,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "f27daadffcce7200",
            "authors": [
                "Siyu Chen",
                "Heejune Sheen",
                "Xuyuan Xiong",
                "Tianhao Wang",
                "Zhuoran Yang"
            ],
            "affiliations": [
                "Antai College of Economics and Management, Shanghai Jiao Tong University",
                "Department of Statistics and Data Science, Yale University",
                "Toyota Technological Institute at Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14002.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#interpretability",
                    "#math",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прорыв в интерпретации языковых моделей: теоретически обоснованное извлечение признаков",
                    "desc": "Исследователи представили новый статистический фреймворк и алгоритм обучения под названием Group Bias Adaptation для улучшения разреженных автоэнкодеров (Sparse Autoencoders). Этот метод позволяет извлекать моносемантические признаки из больших языковых моделей (Large Language Models) с теоретическими гарантиями. Авторы предложили новое понятие идентифицируемости признаков, моделируя полисемантические признаки как разреженные смеси моносемантических концепций. Эксперименты показали превосходную производительность метода на языковых моделях с до 1,5 миллиардов параметров по сравнению с эталонными методами."
                },
                "en": {
                    "title": "Enhancing Feature Recovery in Language Models with Group Bias Adaptation",
                    "desc": "This paper introduces a new method called Group Bias Adaptation (GBA) to improve Sparse Autoencoders (SAEs) for extracting clear features from Large Language Models (LLMs). The authors address the limitations of existing SAE training methods, which often lack solid mathematical backing and can be unstable. They propose a statistical framework that models complex features as combinations of simpler, clear concepts, ensuring better feature recovery. The new training algorithm not only provides theoretical guarantees for recovering these features but also shows better performance compared to traditional methods when tested on large models."
                },
                "zh": {
                    "title": "群体偏差适应：提升稀疏自编码器的单义特征恢复能力",
                    "desc": "本文提出了一种新的统计框架和训练算法，称为群体偏差适应（Group Bias Adaptation），旨在增强稀疏自编码器（Sparse Autoencoders）在大型语言模型中的单义特征恢复能力。现有的稀疏自编码器训练算法缺乏严格的数学保证，并且在超参数敏感性和不稳定性方面存在实际限制。我们通过引入特征可识别性的新概念，解决了特征恢复问题，并提出了一种基于偏差适应的新训练算法。理论证明该算法能够在特定统计模型下正确恢复所有单义特征，从而为稀疏自编码器的训练提供了理论支持。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10038",
            "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
            "url": "https://huggingface.co/papers/2506.10038",
            "abstract": "Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.",
            "score": 4,
            "issue_id": 4349,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "f746e9dd50fb7b78",
            "authors": [
                "Giannis Daras",
                "Adrian Rodriguez-Munoz",
                "Adam Klivans",
                "Antonio Torralba",
                "Constantinos Daskalakis"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10038.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#dataset",
                    "#synthetic",
                    "#diffusion",
                    "#data"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Извлечение пользы из шума: улучшение диффузионных моделей с помощью низкокачественных изображений",
                    "desc": "Статья представляет фреймворк Ambient Diffusion Omni, который использует низкокачественные изображения для улучшения диффузионных моделей. Авторы показывают, что даже отбракованные изображения могут быть полезны при обучении, используя свойства естественных изображений, такие как спектральный степенной закон затухания и локальность. Фреймворк успешно применяется для улучшения FID на ImageNet и качества генерации изображений по тексту. Теоретическое обоснование подхода анализирует компромисс между обучением на смещенных данных и ограниченных несмещенных данных."
                },
                "en": {
                    "title": "Unlocking Potential: Enhancing Diffusion Models with Low-Quality Images",
                    "desc": "The Ambient Diffusion Omni framework enhances diffusion models by effectively utilizing low-quality images, which are often overlooked. It demonstrates that these lower-quality images can significantly improve model performance on tasks like text-to-image generation. The framework leverages natural image properties, such as spectral power law decay and locality, to extract valuable signals during training. By validating its approach with various synthetic corruptions, the framework achieves state-of-the-art results in ImageNet FID, showcasing improved image quality and diversity."
                },
                "zh": {
                    "title": "利用低质量图像提升扩散模型的质量",
                    "desc": "本论文提出了一种名为Ambient Diffusion Omni的框架，利用低质量图像来提升扩散模型的性能。研究表明，通常被丢弃的低质量图像实际上具有很大的价值，可以改善模型的训练效果。该框架利用自然图像的两个特性——谱功率法则衰减和局部性，成功地从合成模糊、JPEG压缩和运动模糊的图像中提取信号。最终，我们在ImageNet FID上取得了最先进的结果，并显著提高了文本到图像生成模型的图像质量和多样性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09033",
            "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.09033",
            "abstract": "Router-R1, a reinforcement learning-based framework, improves multi-LLM routing by interleaving think and route actions, optimizing performance-cost trade-offs, and generalizing to unseen models.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (i.e., assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present Router-R1, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave \"think\" actions (internal deliberation) with \"route\" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1.",
            "score": 3,
            "issue_id": 4357,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "6f6eee917a3ef0d9",
            "authors": [
                "Haozhen Zhang",
                "Tao Feng",
                "Jiaxuan You"
            ],
            "affiliations": [
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09033.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#multimodal",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умная маршрутизация запросов между языковыми моделями",
                    "desc": "Router-R1 - это фреймворк на основе обучения с подкреплением для маршрутизации между несколькими языковыми моделями. Он улучшает существующие подходы, чередуя действия обдумывания и маршрутизации, что позволяет решать сложные задачи. Router-R1 оптимизирует соотношение производительности и стоимости, используя легковесную систему вознаграждений. Фреймворк способен обобщаться на новые модели, опираясь только на простые дескрипторы вроде цены и задержки."
                },
                "en": {
                    "title": "Optimizing Multi-LLM Routing with Reinforcement Learning",
                    "desc": "Router-R1 is a reinforcement learning framework designed to enhance the routing of user queries among multiple large language models (LLMs). Unlike traditional routers that assign queries to a single model, Router-R1 interleaves 'think' and 'route' actions, allowing it to consider multiple models' strengths for complex tasks. It uses a rule-based reward system to optimize the balance between performance and cost, making it efficient in selecting the best model for each query. The framework demonstrates strong generalization capabilities, performing well on various benchmarks while managing costs effectively."
                },
                "zh": {
                    "title": "智能路由，优化性能与成本的平衡",
                    "desc": "Router-R1 是一个基于强化学习的框架，旨在改善多大型语言模型（LLM）的路由。它通过交替进行思考和路由动作，优化性能与成本之间的权衡，并能够推广到未见过的模型。与传统的单轮一对一映射不同，Router-R1 将路由和聚合视为一个序列决策过程，利用其推理能力进行动态模型调用。实验结果表明，Router-R1 在多个基准测试中表现优异，展现了强大的泛化能力和成本管理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14761",
            "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
            "url": "https://huggingface.co/papers/2506.14761",
            "abstract": "An autoregressive U-Net learns to embed its own tokens during training, enabling a multi-scale view of text sequences and improved handling of character-level tasks and low-resource languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.",
            "score": 2,
            "issue_id": 4362,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "24f72bf9457e3acf",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#optimization",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Гибкая токенизация для улучшенного понимания языка",
                    "desc": "Статья представляет автореграссивную U-Net архитектуру, которая обучается встраивать собственные токены в процессе тренировки. Это позволяет модели получить многоуровневое представление текстовых последовательностей. Такой подход улучшает обработку задач на уровне символов и работу с малоресурсными языками. Модель читает необработанные байты и объединяет их в слова и фразы, получая многомасштабный взгляд на последовательность."
                },
                "en": {
                    "title": "Flexible Tokenization for Enhanced Text Understanding",
                    "desc": "This paper presents an autoregressive U-Net model that learns to create its own token embeddings during training, allowing for a flexible approach to text processing. By reading raw bytes and progressively pooling them into larger units, the model gains a multi-scale perspective on text sequences. This design enables the model to predict further into the future at deeper layers, focusing on broader semantic patterns while earlier layers manage finer details. The approach not only improves performance on character-level tasks but also enhances the model's ability to work with low-resource languages by integrating tokenization within the model itself."
                },
                "zh": {
                    "title": "自回归U-Net：灵活处理文本的未来",
                    "desc": "这篇论文介绍了一种自回归U-Net模型，它在训练过程中学习嵌入自己的标记，从而实现对文本序列的多尺度视图。这种方法打破了传统标记化的固定粒度限制，使模型能够更灵活地处理数据。通过读取原始字节并逐步聚合成词，模型在不同深度阶段预测更远的未来，关注更广泛的语义模式。最终，这种模型不仅能处理字符级任务，还能在低资源语言中传递知识。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14731",
            "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
            "url": "https://huggingface.co/papers/2506.14731",
            "abstract": "Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.",
            "score": 2,
            "issue_id": 4350,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "7c1c5a66d6e8f898",
            "authors": [
                "Ring Team",
                "Bin Hu",
                "Cai Chen",
                "Deng Zhao",
                "Ding Liu",
                "Dingnan Jin",
                "Feng Zhu",
                "Hao Dai",
                "Hongzhi Luan",
                "Jia Guo",
                "Jiaming Liu",
                "Jiewei Wu",
                "Jun Mei",
                "Jun Zhou",
                "Junbo Zhao",
                "Junwu Xiong",
                "Kaihong Zhang",
                "Kuan Xu",
                "Lei Liang",
                "Liang Jiang",
                "Liangcheng Fu",
                "Longfei Zheng",
                "Qiang Gao",
                "Qing Cui",
                "Quan Wan",
                "Shaomian Zheng",
                "Shuaicheng Li",
                "Tongkai Yang",
                "Wang Ren",
                "Xiaodong Yan",
                "Xiaopei Wan",
                "Xiaoyun Feng",
                "Xin Zhao",
                "Xinxing Yang",
                "Xinyu Kong",
                "Xuemin Yang",
                "Yang Li",
                "Yingting Wu",
                "Yongkang Liu",
                "Zhankai Xu",
                "Zhenduo Zhang",
                "Zhenglei Zhou",
                "Zhenyu Huang",
                "Zhiqiang Zhang",
                "Zihao Wang",
                "Zujie Wen"
            ],
            "affiliations": [
                "Ring Team, Inclusion AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14731.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#open_source",
                    "#rl",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное рассуждение с меньшими вычислительными затратами",
                    "desc": "Ring-lite - это модель на основе архитектуры Mixture-of-Experts (MoE), оптимизированная с помощью обучения с подкреплением для эффективного рассуждения. Модель достигает производительности современных моделей рассуждения, активируя лишь треть параметров. Авторы представляют новый метод C3PO для повышения стабильности обучения и вычислительной эффективности. Они также предлагают двухэтапную парадигму обучения для интеграции данных из разных доменов."
                },
                "en": {
                    "title": "Efficient Reasoning with Fewer Parameters: Introducing Ring-lite",
                    "desc": "Ring-lite is a large language model that uses a Mixture-of-Experts (MoE) architecture combined with reinforcement learning (RL) to enhance reasoning capabilities while minimizing parameter activation. It builds on the Ling-lite model, achieving state-of-the-art performance on various reasoning benchmarks with only a fraction of the parameters activated compared to similar models. The paper introduces a novel training method called Constrained Contextual Computation Policy Optimization (C3PO) to improve stability during RL training and optimize computational efficiency. Additionally, it highlights the importance of selecting distillation checkpoints based on entropy loss for better performance in RL training and proposes a two-stage training approach to manage domain conflicts in mixed datasets."
                },
                "zh": {
                    "title": "高效推理，激活更少参数的Ring-lite",
                    "desc": "Ring-lite是一种基于专家混合（MoE）架构的大型语言模型，通过强化学习（RL）进行优化，以实现高效且稳健的推理能力。该模型在Ling-lite的基础上构建，具有168亿个参数，但仅激活2.75亿个参数，能够在多个具有挑战性的基准测试中与小规模的最先进推理模型相匹配。我们提出了一种联合训练流程，将蒸馏与强化学习结合，解决了MoE强化学习训练中的一些未记录的挑战。通过引入受限上下文计算策略优化（C3PO），我们提高了训练的稳定性，并通过算法与系统的共同设计方法改善了计算吞吐量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14702",
            "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers",
            "url": "https://huggingface.co/papers/2506.14702",
            "abstract": "A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: \"Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?\" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.",
            "score": 2,
            "issue_id": 4350,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "2fbff0f4b562f92e",
            "authors": [
                "Daniel D'souza",
                "Julia Kreutzer",
                "Adrien Morisot",
                "Ahmet Üstün",
                "Sara Hooker"
            ],
            "affiliations": [
                "Cohere",
                "Cohere Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14702.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точная настройка моделей для редких случаев",
                    "desc": "Статья представляет новый подход к дообучению моделей машинного обучения для улучшения их производительности и управляемости на редких и недопредставленных случаях использования. Авторы разработали таксономию характеристик данных и происхождения задач для явного контроля атрибутов генерации. Модель обучается автоматически определять эти маркеры, что делает их опциональными при выводе. Такой подход показывает значительные улучшения производительности, особенно на примерах из длинного хвоста распределения обучающих данных."
                },
                "en": {
                    "title": "Optimizing Model Performance for Rare Use Cases",
                    "desc": "This paper addresses the challenge of improving machine learning model performance on rare and underrepresented use cases, often referred to as the long tail. It proposes a method for fine-tuning models that enhances both controllability and performance by automatically inferring generation attributes during inference. The authors introduce a taxonomy of data characteristics to help guide the model's output, allowing for better adaptation to specific tasks without relying heavily on prompt engineering. Their approach demonstrates significant performance improvements, particularly in underrepresented domains, achieving notable gains in generation quality and task-specific evaluations."
                },
                "zh": {
                    "title": "优化模型以提升稀有用例的性能与可控性",
                    "desc": "本文提出了一种系统的方法，通过自动推断生成属性来微调模型，以提高在稀有和未充分代表的用例上的性能和可控性。现代机器学习面临的一个主要挑战是如何在长尾特征上表现良好，尤其是在训练数据中较少出现的特征。我们重新审视训练和推理技术之间的差距，以改善长尾性能，并为用户提供一组可控的生成属性。通过对基础模型进行微调，我们实现了在推理时自动推断这些标记，从而在未充分代表的领域中显著提高了模型的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13599",
            "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
            "url": "https://huggingface.co/papers/2506.13599",
            "abstract": "CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose CityGPT-Powered Agentic framework for Mobility Simulation (CAMS), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. CAMS comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that CAMS achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, CAMS generates more realistic and plausible trajectories. In general, CAMS establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.",
            "score": 2,
            "issue_id": 4348,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "0b0a6282d1310e1b",
            "authors": [
                "Yuwei Du",
                "Jie Feng",
                "Jian Yuan",
                "Yong Li"
            ],
            "affiliations": [
                "Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13599.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#synthetic",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "Умное моделирование городской мобильности с помощью ИИ",
                    "desc": "CAMS (CityGPT-Powered Agentic framework for Mobility Simulation) - это новый подход к моделированию человеческой мобильности в городском пространстве. Он использует языковые модели с глубоким пониманием городской среды для более реалистичного моделирования индивидуальных и коллективных паттернов передвижения. CAMS включает три ключевых модуля: MobExtractor для извлечения шаблонов мобильности, GeoGenerator для генерации опорных точек, и TrajEnhancer для создания траекторий. Эксперименты показывают, что CAMS превосходит традиционные методы и создает более правдоподобные траектории движения."
                },
                "en": {
                    "title": "Revolutionizing Urban Mobility Simulation with CAMS",
                    "desc": "CAMS introduces a novel framework that combines agent-based modeling with large language models to enhance the simulation of human mobility in urban environments. It addresses the limitations of traditional methods by integrating individual and collective mobility patterns, allowing for more realistic trajectory generation. The framework consists of three main components: MobExtractor for mobility pattern extraction, GeoGenerator for urban geospatial knowledge generation, and TrajEnhancer for trajectory refinement. Experiments demonstrate that CAMS outperforms existing approaches by generating plausible mobility trajectories without needing external geospatial data."
                },
                "zh": {
                    "title": "城市移动模拟的新范式",
                    "desc": "CAMS是一个结合了代理框架和城市知识的大型语言模型，用于更真实地模拟人类的移动行为。它通过三个核心模块来实现这一目标：MobExtractor提取和合成用户的移动模式，GeoGenerator生成考虑集体知识的城市地理信息，TrajEnhancer根据移动模式生成符合真实偏好的轨迹。与传统方法相比，CAMS在不依赖外部地理信息的情况下，能够更好地建模个体和集体的移动模式。实验结果表明，CAMS生成的轨迹更加真实可信，开创了人类移动模拟的新范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05426",
            "title": "Mixture-of-Experts Meets In-Context Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.05426",
            "abstract": "T2MIR, a framework using token-wise and task-wise MoE in transformer-based decision models, enhances in-context reinforcement learning by addressing multi-modality and task diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR.",
            "score": 2,
            "issue_id": 4356,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "4e79eb4ebca225c7",
            "authors": [
                "Wenhao Wu",
                "Fuhong Liu",
                "Haoru Li",
                "Zican Hu",
                "Daoyi Dong",
                "Chunlin Chen",
                "Zhi Wang"
            ],
            "affiliations": [
                "Australian Artificial Intelligence Institute, University of Technology Sydney",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05426.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#games",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "T2MIR: Смесь экспертов для улучшения обучения с подкреплением в контексте",
                    "desc": "T2MIR - это новая архитектура для обучения с подкреплением в контексте (ICRL), использующая смесь экспертов (MoE) в трансформерных моделях принятия решений. Она решает проблемы мультимодальности входных данных и разнообразия задач с помощью токен-ориентированного и задаче-ориентированного MoE. T2MIR также применяет контрастивное обучение для улучшения маршрутизации задач. Эксперименты показывают значительное улучшение способности к обучению в контексте по сравнению с базовыми моделями."
                },
                "en": {
                    "title": "Enhancing In-Context Learning with T2MIR: A Mixture-of-Experts Approach",
                    "desc": "The paper introduces T2MIR, a novel framework that enhances in-context reinforcement learning (ICRL) by integrating mixture-of-experts (MoE) into transformer-based decision models. It addresses the challenges of multi-modality in state-action-reward data and the diversity of decision tasks by implementing token-wise and task-wise MoE layers. The token-wise MoE captures different meanings of input tokens, while the task-wise MoE directs tasks to specialized experts, reducing conflicts during training. Experimental results demonstrate that T2MIR improves the learning capacity of ICRL and outperforms existing methods, showcasing its potential for advancements in both language and vision tasks."
                },
                "zh": {
                    "title": "T2MIR：提升上下文强化学习的专家混合框架",
                    "desc": "T2MIR是一个框架，利用基于Transformer的决策模型中的逐token和逐任务的专家混合（MoE）方法，增强了上下文强化学习（ICRL）。该框架解决了状态-动作-奖励数据的多模态性和决策任务的多样性问题。T2MIR通过引入两个并行层，分别是逐token的MoE和逐任务的MoE，来捕捉输入token的不同语义，并将多样化的任务分配给专门的专家。实验结果表明，T2MIR显著提高了上下文学习能力，并在多种基准测试中表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13901",
            "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations",
            "url": "https://huggingface.co/papers/2506.13901",
            "abstract": "A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.   To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.   Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.",
            "score": 1,
            "issue_id": 4351,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "5492fc2feb0ae2f3",
            "authors": [
                "Abhilekh Borah",
                "Chhavi Sharma",
                "Danush Khanna",
                "Utkarsh Bhatt",
                "Gurpreet Singh",
                "Hasnat Md Abdullah",
                "Raghav Kaushik Ravi",
                "Vinija Jain",
                "Jyoti Patel",
                "Shubham Singh",
                "Vasu Sharma",
                "Arpita Vats",
                "Rahul Raja",
                "Aman Chadha",
                "Amitava Das"
            ],
            "affiliations": [
                "Amazon AI",
                "BITS Goa, India",
                "Evalueserve",
                "IIIT Guwahati, India",
                "IIT Kharagpur, India",
                "LinkedIn",
                "Manipal University Jaipur, India",
                "Meta AI",
                "New York University, USA",
                "Texas A&M University, USA",
                "Vellore Institute of Technology, Chennai, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13901.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#rlhf",
                    "#alignment",
                    "#benchmark",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "AQI: Геометрический подход к оценке выравнивания языковых моделей",
                    "desc": "Предложена новая метрика оценки выравнивания (alignment) больших языковых моделей - Индекс Качества Выравнивания (AQI). AQI анализирует активации в скрытом пространстве модели, оценивая качество кластеризации для выявления скрытых несоответствий и фальшивого выравнивания. Метрика дополняет существующие поведенческие прокси-метрики, такие как частота отказов и токсичность. AQI также может служить ранним индикатором попыток обхода ограничений модели, предоставляя надежный инструмент для аудита безопасности."
                },
                "en": {
                    "title": "Ensuring True Alignment in Language Models with AQI",
                    "desc": "The paper introduces a new evaluation metric called the Alignment Quality Index (AQI) to assess the alignment of large language models (LLMs). AQI analyzes latent space activations to measure clustering quality, helping to identify misalignments and instances of alignment faking that traditional behavioral proxies may overlook. By utilizing established clustering metrics like the Davies-Bouldin Score and Dunn Index, AQI provides a more reliable assessment of model safety and alignment in high-stakes applications. The authors also present the LITMUS dataset to support rigorous evaluation, demonstrating AQI's effectiveness in revealing vulnerabilities that other metrics fail to detect."
                },
                "zh": {
                    "title": "对齐质量指数：确保大型语言模型的安全与可靠",
                    "desc": "本文提出了一种新的评估指标，称为对齐质量指数（AQI），用于评估大型语言模型的对齐情况。AQI通过分析潜在空间中的激活分离，捕捉聚类质量，以检测模型的错误对齐和伪对齐现象。与现有的行为代理相比，AQI能够更有效地识别模型的安全性和潜在风险。我们还提出了LITMUS数据集，以支持在复杂条件下的稳健评估，并展示了AQI与外部评审者的相关性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13387",
            "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
            "url": "https://huggingface.co/papers/2506.13387",
            "abstract": "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2M's great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: https://github.com/BeileiCui/TR2M)",
            "score": 1,
            "issue_id": 4347,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "6d0fc497ae4dcfd0",
            "authors": [
                "Beilei Cui",
                "Yiming Huang",
                "Long Bai",
                "Hongliang Ren"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13387.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Универсальное преобразование глубины с помощью мультимодального обучения",
                    "desc": "TR2M - это фреймворк для преобразования относительной глубины в метрическую с использованием мультимодальных входных данных. Он применяет кросс-модальное внимание и контрастное обучение для улучшения производительности на различных наборах данных. TR2M использует как текстовое описание, так и изображение в качестве входных данных и оценивает две карты масштабирования для преобразования относительной глубины в метрическую на уровне пикселей. Модель демонстрирует отличные результаты как на знакомых, так и на новых наборах данных, показывая большой потенциал в попиксельном преобразовании глубины с помощью языковой информации."
                },
                "en": {
                    "title": "Transforming Relative Depth to Metric Depth with TR2M",
                    "desc": "The paper introduces TR2M, a framework that effectively converts relative depth information into metric depth using multimodal inputs, specifically images and text. It addresses the limitations of existing monocular depth estimation methods by combining the strengths of metric and relative depth estimation. TR2M employs cross-modality attention to enhance feature fusion and utilizes contrastive learning to improve scale alignment. The framework demonstrates strong performance across various datasets, including impressive zero-shot capabilities on unseen data, showcasing its versatility and effectiveness in depth estimation tasks."
                },
                "zh": {
                    "title": "TR2M：相对深度到度量深度的智能转换",
                    "desc": "本文提出了一种名为TR2M的框架，旨在将相对深度转换为度量深度，利用多模态输入提升在不同数据集上的表现。当前的单目深度估计方法主要分为度量深度估计和相对深度估计，前者在特定领域表现良好，但局限性较大，而后者在不同领域具有良好的泛化能力，但存在尺度不确定性的问题。TR2M通过融合文本描述和图像输入，利用交叉模态注意力模块和对比学习策略，构建了两个重标定图以在像素级别上进行深度转换。实验结果表明，TR2M在已知数据集上表现优异，并在五个未知数据集上展现出卓越的零样本能力，显示出在像素级别上利用语言辅助进行深度转换的巨大潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12880",
            "title": "Universal Jailbreak Suffixes Are Strong Attention Hijackers",
            "url": "https://huggingface.co/papers/2506.12880",
            "abstract": "Suffix-based jailbreaks exploit adversarial suffixes to hijack large language models, with effectiveness linked to suffix universality; the method can be enhanced and mitigated with minimal computational or utility cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We study suffix-based jailbreaksx2013a powerful family of attacks against large language models (LLMs) that optimize adversarial suffixes to circumvent safety alignment. Focusing on the widely used foundational GCG attack (Zou et al., 2023), we observe that suffixes vary in efficacy: some markedly more universalx2013generalizing to many unseen harmful instructionsx2013than others. We first show that GCG's effectiveness is driven by a shallow, critical mechanism, built on the information flow from the adversarial suffix to the final chat template tokens before generation. Quantifying the dominance of this mechanism during generation, we find GCG irregularly and aggressively hijacks the contextualization process. Crucially, we tie hijacking to the universality phenomenon, with more universal suffixes being stronger hijackers. Subsequently, we show that these insights have practical implications: GCG universality can be efficiently enhanced (up to times5 in some cases) at no additional computational cost, and can also be surgically mitigated, at least halving attack success with minimal utility loss. We release our code and data at http://github.com/matanbt/interp-jailbreak.",
            "score": 1,
            "issue_id": 4357,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 июня",
                "en": "June 15",
                "zh": "6月15日"
            },
            "hash": "b2ad9af4ff256800",
            "authors": [
                "Matan Ben-Tov",
                "Mor Geva",
                "Mahmood Sharif"
            ],
            "affiliations": [
                "Blavatnik School of Computer Science and AI, Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12880.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#security",
                    "#hallucinations",
                    "#multimodal",
                    "#open_source",
                    "#alignment",
                    "#data"
                ],
                "emoji": "🔓",
                "ru": {
                    "title": "Уязвимость языковых моделей: атаки на основе суффиксов",
                    "desc": "Исследование посвящено атакам на основе суффиксов против больших языковых моделей (LLM), которые используют оптимизированные враждебные суффиксы для обхода мер безопасности. Авторы обнаружили, что эффективность атаки GCG зависит от универсальности суффиксов и их способности захватывать процесс контекстуализации. Более универсальные суффиксы оказались сильнее в захвате модели. На основе этих наблюдений предложены методы повышения эффективности атак и их смягчения с минимальными вычислительными затратами."
                },
                "en": {
                    "title": "Unlocking and Mitigating Suffix-Based Attacks on Language Models",
                    "desc": "This paper investigates suffix-based jailbreaks, which are attacks on large language models (LLMs) that use specific suffixes to bypass safety measures. The authors focus on the GCG attack, revealing that some suffixes are more effective than others due to their universality, meaning they can apply to a wider range of harmful instructions. They identify a key mechanism in how these suffixes influence the model's output, showing that more universal suffixes are better at hijacking the model's contextualization process. The study also presents methods to enhance the effectiveness of these attacks without extra computational costs and suggests ways to mitigate them while maintaining model utility."
                },
                "zh": {
                    "title": "后缀攻击：绕过安全机制的新方法",
                    "desc": "本论文研究了一种基于后缀的攻击方法，旨在绕过大型语言模型的安全机制。我们发现，不同的后缀在攻击效果上存在显著差异，某些后缀具有更强的通用性，能够适用于更多未见过的有害指令。通过分析信息流动，我们揭示了攻击的关键机制，并指出更通用的后缀能够更有效地劫持上下文处理过程。最后，我们提出了增强和缓解这种攻击的方法，能够在不增加计算成本的情况下显著提高防御效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14629",
            "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning",
            "url": "https://huggingface.co/papers/2506.14629",
            "abstract": "VisText-Mosquito is a multimodal dataset combining visual and textual data for automated mosquito breeding site detection, segmentation, and reasoning, utilizing YOLOv9s, YOLOv11n-Seg, and a fine-tuned BLIP model.  \t\t\t\t\tAI-generated summary \t\t\t\t Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme \"Prevention is Better than Cure\", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito",
            "score": 0,
            "issue_id": 4358,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "3308b767733e40b9",
            "authors": [
                "Md. Adnanul Islam",
                "Md. Faiyaz Abdullah Sayeedi",
                "Md. Asaduzzaman Shuvo",
                "Muhammad Ziaur Rahman",
                "Shahanur Rahman Bappy",
                "Raiyan Rahman",
                "Swakkhar Shatabda"
            ],
            "affiliations": [
                "BRAC University, Bangladesh",
                "United International University, Bangladesh",
                "University of Portsmouth, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14629.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#dataset",
                    "#open_source",
                    "#multimodal",
                    "#healthcare",
                    "#reasoning"
                ],
                "emoji": "🦟",
                "ru": {
                    "title": "ИИ на страже здоровья: предотвращение угрозы комаров",
                    "desc": "VisText-Mosquito - это мультимодальный набор данных, объединяющий визуальную и текстовую информацию для автоматизированного обнаружения, сегментации и анализа мест размножения комаров. В исследовании использовались модели YOLOv9s для обнаружения объектов, YOLOv11n-Seg для сегментации водной поверхности и дообученная модель BLIP для генерации рассуждений. Набор данных включает 1828 размеченных изображений для обнаружения объектов, 142 изображения для сегментации водной поверхности и связанные с каждым изображением тексты для рассуждений. Результаты показывают высокую точность моделей в задачах обнаружения, сегментации и генерации текста, что демонстрирует потенциал использования искусственного интеллекта для профилактики заболеваний, переносимых комарами."
                },
                "en": {
                    "title": "Harnessing AI for Mosquito Control: Detect, Segment, Reason!",
                    "desc": "The paper introduces VisText-Mosquito, a unique dataset that combines images and text to help identify and analyze mosquito breeding sites. It includes 1,828 annotated images for detecting objects and 142 images specifically for segmenting water surfaces, along with reasoning texts for each image. The study employs advanced models like YOLOv9s and YOLOv11n-Seg for detection and segmentation tasks, achieving high precision scores. Additionally, a fine-tuned BLIP model is used for generating natural language reasoning, demonstrating the effectiveness of multimodal approaches in combating mosquito-borne diseases."
                },
                "zh": {
                    "title": "多模态数据助力蚊子滋生地自动检测",
                    "desc": "VisText-Mosquito是一个多模态数据集，结合了视觉和文本数据，用于自动化检测和分析蚊子滋生地。该数据集包含1828张标注图像用于目标检测，142张图像用于水面分割，以及与每张图像相关的自然语言推理文本。使用YOLOv9s模型进行目标检测时，达到了最高的精度0.92926，而YOLOv11n-Seg在分割任务中达到了0.91587的精度。通过微调的BLIP模型，我们在推理生成方面取得了良好的效果，强调了“预防胜于治疗”的主题，展示了基于AI的检测如何主动应对蚊媒疾病风险。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13922",
            "title": "DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance",
            "url": "https://huggingface.co/papers/2506.13922",
            "abstract": "DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io",
            "score": 0,
            "issue_id": 4359,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 июня",
                "en": "June 16",
                "zh": "6月16日"
            },
            "hash": "bc27538b9a430f57",
            "authors": [
                "Maximilian Du",
                "Shuran Song"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13922.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "DynaGuide: гибкое управление роботами с помощью внешней модели динамики",
                    "desc": "DynaGuide - это метод управления диффузионными политиками с использованием внешней модели динамики. Он позволяет адаптировать политики к множественным целям и сохранять устойчивость, превосходя обусловливание целями, особенно при низкокачественных целях. DynaGuide отделяет модель динамики от базовой политики, что дает ряд преимуществ. Метод продемонстрировал среднюю успешность управления 70% на наборе задач CALVIN и превзошел обусловливание целями в 5,4 раза при управлении с низкокачественными целями."
                },
                "en": {
                    "title": "Steering Policies with DynaGuide: Adapting to Multiple Goals Robustly!",
                    "desc": "DynaGuide is a novel steering method that enhances diffusion policies by utilizing an external dynamics model. This approach allows the policies to adapt to various objectives while ensuring robustness, particularly when dealing with low-quality objectives. Unlike traditional goal-conditioning methods, DynaGuide separates the dynamics model from the base policy, enabling it to steer towards multiple goals and improve underrepresented behaviors. The effectiveness of DynaGuide is demonstrated through experiments, achieving a 70% steering success rate and significantly outperforming existing methods."
                },
                "zh": {
                    "title": "DynaGuide：多目标引导的智能策略",
                    "desc": "DynaGuide是一种使用外部动态模型的引导方法，旨在增强扩散策略的适应性。它允许策略适应多个目标，并在低质量目标下保持鲁棒性，表现优于传统的目标条件方法。通过将动态模型与基础策略分离，DynaGuide能够引导策略朝向多个目标，并增强基础策略的表现。实验结果显示，DynaGuide在多种任务中取得了70%的引导成功率，尤其在低质量目标下比目标条件方法提高了5.4倍的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12015",
            "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
            "url": "https://huggingface.co/papers/2506.12015",
            "abstract": "EMLoC, an memory-efficient fine-tuning framework using activation-aware SVD and LoRA, allows model adaptation within inference memory constraints for diverse applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.",
            "score": 0,
            "issue_id": 4356,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "d559dcf057099acf",
            "authors": [
                "Hsi-Che Lin",
                "Yu-Chu Yu",
                "Kai-Po Chang",
                "Yu-Chiang Frank Wang"
            ],
            "affiliations": [
                "NVIDIA",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12015.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#open_source",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Тонкая настройка гигантских моделей на обычном ПК",
                    "desc": "EMLoC - это фреймворк для эффективной по памяти тонкой настройки больших языковых моделей. Он использует активационно-ориентированное SVD и LoRA для создания легковесного эмулятора модели. Это позволяет выполнять тонкую настройку в рамках тех же ограничений по памяти, что и при обычном выводе. EMLoC превосходит другие подходы на различных наборах данных и модальностях, делая адаптацию моделей доступной для индивидуальных пользователей."
                },
                "en": {
                    "title": "Efficient Fine-Tuning for Everyone with EMLoC!",
                    "desc": "EMLoC is a memory-efficient framework designed for fine-tuning large foundation models while adhering to inference memory limits. It utilizes activation-aware singular value decomposition (SVD) to create a lightweight emulator from a small calibration dataset, allowing for effective model adaptation. Fine-tuning is achieved through Low-Rank Adaptation (LoRA), which is then corrected with a novel compensation algorithm to ensure alignment with the original model. This approach enables users to fine-tune large models on standard consumer hardware, making advanced machine learning accessible and practical for diverse applications."
                },
                "zh": {
                    "title": "EMLoC：高效内存微调的新选择",
                    "desc": "EMLoC是一种内存高效的微调框架，利用激活感知的奇异值分解（SVD）和LoRA技术，使得在推理内存限制下进行模型适应成为可能。该框架通过在小型下游校准集上构建任务特定的轻量级模拟器，来实现微调。为了纠正原始模型与压缩模拟器之间的错位，EMLoC提出了一种新颖的补偿算法，使得微调后的LoRA模块可以合并回原始模型中进行推理。实验结果表明，EMLoC在多个数据集和模态上优于其他基线，且无需量化即可在单个24GB的消费级GPU上微调38B模型，极大地提高了模型适应的效率和实用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03939",
            "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning",
            "url": "https://huggingface.co/papers/2506.03939",
            "abstract": "Graph Counselor enhances Large Language Models by using multi-agent collaboration and adaptive reasoning to integrate knowledge effectively, improving factual accuracy and generation quality in specialized domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.",
            "score": 0,
            "issue_id": 4362,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 июня",
                "en": "June 4",
                "zh": "6月4日"
            },
            "hash": "44afebcf0fc38495",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#agents",
                    "#graphs",
                    "#rag"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умные графы для умных машин: новый подход к обучению ИИ",
                    "desc": "Graph Counselor - это метод улучшения работы больших языковых моделей (LLM) с использованием многоагентного сотрудничества и адаптивного рассуждения. Он эффективно интегрирует знания из графовых структур, повышая фактическую точность и качество генерации в специализированных областях. Метод использует адаптивное извлечение информации из графов и механизм самоанализа для улучшения результатов рассуждений. Эксперименты показывают, что Graph Counselor превосходит существующие методы в задачах рассуждения на графах."
                },
                "en": {
                    "title": "Empowering LLMs with Adaptive Multi-Agent Collaboration",
                    "desc": "Graph Counselor is a novel approach that enhances Large Language Models (LLMs) by employing multi-agent collaboration and adaptive reasoning techniques. It addresses the limitations of existing methods in knowledge integration by utilizing an Adaptive Graph Information Extraction Module (AGIEM) that allows agents to work together to model complex graph structures. This method dynamically adjusts information extraction strategies, improving the model's ability to handle multi-level dependencies and reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module ensures higher accuracy and semantic consistency in reasoning results, leading to superior performance in graph reasoning tasks."
                },
                "zh": {
                    "title": "图顾问：提升语言模型的知识整合能力",
                    "desc": "Graph Counselor 是一种基于多智能体协作的图检索增强生成方法，旨在提高大型语言模型在专业领域的知识整合能力。该方法通过自适应图信息提取模块（AGIEM），使规划、思考和执行智能体协同工作，从而精确建模复杂的图结构并动态调整信息提取策略。它还引入了多视角自我反思模块（SR），通过自我反思和逆向推理机制提高推理结果的准确性和语义一致性。实验结果表明，Graph Counselor 在多个图推理任务中优于现有方法，展现出更高的推理准确性和泛化能力。"
                }
            }
        }
    ],
    "link_prev": "2025-06-17.html",
    "link_next": "2025-06-19.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "17.06",
        "en": "06/17",
        "zh": "6月17日"
    },
    "short_date_next": {
        "ru": "19.06",
        "en": "06/19",
        "zh": "6月19日"
    },
    "categories": {
        "#dataset": 12,
        "#data": 4,
        "#benchmark": 14,
        "#agents": 6,
        "#cv": 5,
        "#rl": 8,
        "#rlhf": 3,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 2,
        "#video": 1,
        "#multimodal": 14,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 8,
        "#healthcare": 2,
        "#training": 16,
        "#robotics": 2,
        "#agi": 3,
        "#games": 3,
        "#interpretability": 3,
        "#reasoning": 14,
        "#transfer_learning": 3,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 16,
        "#survey": 1,
        "#diffusion": 5,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 2,
        "#financial": 1
    }
}