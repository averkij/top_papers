{
    "date": {
        "ru": "4 Ğ¸ÑĞ»Ñ",
        "en": "July 4",
        "zh": "7æœˆ4æ—¥"
    },
    "time_utc": "2025-07-05 01:58",
    "weekday": 4,
    "issue_id": 4660,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.02592",
            "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
            "url": "https://huggingface.co/papers/2507.02592",
            "abstract": "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.",
            "score": 56,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ»Ñ",
                "en": "July 3",
                "zh": "7æœˆ3æ—¥"
            },
            "hash": "0a8cc61c0251e5da",
            "authors": [
                "Kuan Li",
                "Zhongwang Zhang",
                "Huifeng Yin",
                "Liwen Zhang",
                "Litu Ou",
                "Jialong Wu",
                "Wenbiao Yin",
                "Baixuan Li",
                "Zhengwei Tao",
                "Xinyu Wang",
                "Weizhou Shen",
                "Junkai Zhang",
                "Dingchu Zhang",
                "Xixi Wu",
                "Yong Jiang",
                "Ming Yan",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02592.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "WebSailor: Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¾ĞºĞµĞ°Ğ½Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ WebSailor. ĞĞ½ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. WebSailor Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WebSailor Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Open-Source Agents to Compete with the Best",
                    "desc": "This paper discusses advancements in training large language models (LLMs) to surpass human cognitive limitations. It highlights the success of proprietary systems like DeepResearch, which excel in complex information-seeking tasks due to their unique reasoning abilities. The authors introduce WebSailor, a post-training methodology that enhances LLMs by generating high-uncertainty tasks and employing a novel reinforcement learning algorithm called Duplicating Sampling Policy Optimization (DUPO). The results show that WebSailor significantly improves the performance of open-source agents, enabling them to compete with proprietary models in challenging information retrieval scenarios."
                },
                "zh": {
                    "title": "è¶…è¶Šè®¤çŸ¥å±€é™ï¼Œæå‡ä¿¡æ¯æ£€ç´¢èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è¶…è¶Šäººç±»è®¤çŸ¥å±€é™æ€§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºçš„WebSailoræ–¹æ³•é€šè¿‡ç³»ç»Ÿæ€§åœ°å‡å°‘åœ¨å¹¿é˜”ä¿¡æ¯ç¯å¢ƒä¸­å¯¼èˆªæ—¶çš„æç«¯ä¸ç¡®å®šæ€§ï¼Œæ¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç»“æ„åŒ–é‡‡æ ·å’Œä¿¡æ¯æ¨¡ç³ŠåŒ–ç­‰æŠ€æœ¯ï¼Œç”Ÿæˆæ–°çš„é«˜ä¸ç¡®å®šæ€§ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebSailoråœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ‰€æœ‰å¼€æºä»£ç†ï¼Œç¼©å°äº†ä¸ä¸“æœ‰ä»£ç†çš„èƒ½åŠ›å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02813",
            "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with\n  TriMap Video Diffusion",
            "url": "https://huggingface.co/papers/2507.02813",
            "abstract": "Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X.",
            "score": 45,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ»Ñ",
                "en": "July 3",
                "zh": "7æœˆ3æ—¥"
            },
            "hash": "726c080e7ea88c4b",
            "authors": [
                "Fangfu Liu",
                "Hao Li",
                "Jiawei Chi",
                "Hanyang Wang",
                "Minghui Yang",
                "Fudong Wang",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Ant Group",
                "NTU",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02813.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#3d",
                    "#games",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "LangScene-X - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TriMap Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RGB, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ) Ğ¸Ğ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€ (LQC) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. LangScene-X Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with Language-Embedded Insights",
                    "desc": "This paper presents LangScene-X, a new framework for creating 3D structures from 2D images using language information. It addresses the limitations of previous methods that relied on dense-view reconstructions, which often resulted in poor quality when only limited views were available. LangScene-X utilizes a TriMap video diffusion model to generate consistent visual and semantic data from sparse inputs, enhancing the reconstruction process. Additionally, it introduces a Language Quantized Compressor to efficiently encode language embeddings, allowing for better generalization across different scenes without needing to retrain for each one."
                },
                "zh": {
                    "title": "LangScene-Xï¼šä»ç¨€ç–è§†å›¾ç”Ÿæˆä¸€è‡´çš„3Dåœºæ™¯",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆæ¡†æ¶LangScene-Xï¼Œç”¨äºä»ç¨€ç–è§†å›¾ä¸­æ¢å¤3Dç»“æ„å¹¶è¿›è¡Œåœºæ™¯ç†è§£ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¯­è¨€ä¿¡æ¯å’Œå¤šæ¨¡æ€æ•°æ®ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸€è‡´çš„3Dåœºæ™¯ã€‚æˆ‘ä»¬é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ªTriMapè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»ç¨€ç–è¾“å…¥ä¸­ç”Ÿæˆå¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡å¼•å…¥è¯­è¨€é‡åŒ–å‹ç¼©å™¨ï¼ˆLQCï¼‰ï¼Œæˆ‘ä»¬å®ç°äº†è·¨åœºæ™¯çš„æ³›åŒ–ï¼Œé¿å…äº†é€åœºæ™¯çš„é‡æ–°è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02321",
            "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate\n  Features Feedback",
            "url": "https://huggingface.co/papers/2507.02321",
            "abstract": "InnerControl enforces spatial consistency across all diffusion steps by training lightweight convolutional probes to improve control fidelity and generation quality in text-to-image diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth).",
            "score": 33,
            "issue_id": 4646,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ»Ñ",
                "en": "July 3",
                "zh": "7æœˆ3æ—¥"
            },
            "hash": "f340ac71ebc152be",
            "authors": [
                "Nina Konovalova",
                "Maxim Nikolaev",
                "Andrey Kuznetsov",
                "Aibek Alanov"
            ],
            "affiliations": [
                "AIRI, Russia",
                "HSE University, Russia",
                "Innopolis, Russia",
                "Sber, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02321.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#alignment",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "InnerControl - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ InnerControl Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Spatial Consistency in Diffusion Models with InnerControl",
                    "desc": "InnerControl is a novel training strategy designed to enhance spatial consistency in text-to-image diffusion models. It utilizes lightweight convolutional probes to reconstruct control signals from intermediate features during the denoising process. By applying an alignment loss that minimizes the difference between predicted and target conditions at every diffusion step, InnerControl significantly improves control fidelity and overall generation quality. This approach, when combined with existing methods like ControlNet++, achieves state-of-the-art results across various conditioning techniques."
                },
                "zh": {
                    "title": "InnerControlï¼šæå‡æ‰©æ•£æ¨¡å‹çš„ç©ºé—´ä¸€è‡´æ€§ä¸ç”Ÿæˆè´¨é‡",
                    "desc": "InnerControlæ˜¯ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æ‰€æœ‰æ‰©æ•£æ­¥éª¤ä¸­çš„ç©ºé—´ä¸€è‡´æ€§ã€‚é€šè¿‡è®­ç»ƒè½»é‡çº§å·ç§¯æ¢é’ˆï¼ŒInnerControlèƒ½å¤Ÿåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ä»ä¸­é—´UNetç‰¹å¾ä¸­é‡å»ºè¾“å…¥æ§åˆ¶ä¿¡å·ï¼ˆå¦‚è¾¹ç¼˜å’Œæ·±åº¦ï¼‰ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°æå–ä¿¡å·ï¼Œå³ä½¿åœ¨é«˜åº¦å™ªå£°çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä¹Ÿèƒ½ä¸ºè®­ç»ƒæä¾›ä¼ªçœŸå®æ§åˆ¶ã€‚é€šè¿‡æœ€å°åŒ–æ•´ä¸ªæ‰©æ•£è¿‡ç¨‹ä¸­çš„é¢„æµ‹æ¡ä»¶ä¸ç›®æ ‡æ¡ä»¶ä¹‹é—´çš„å·®å¼‚ï¼ŒInnerControlæ˜¾è‘—æé«˜äº†æ§åˆ¶çš„å‡†ç¡®æ€§å’Œç”Ÿæˆçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02025",
            "title": "IntFold: A Controllable Foundation Model for General and Specialized\n  Biomolecular Structure Prediction",
            "url": "https://huggingface.co/papers/2507.02025",
            "abstract": "We introduce IntFold, a controllable foundation model for both general and specialized biomolecular structure prediction. IntFold demonstrates predictive accuracy comparable to the state-of-the-art AlphaFold3, while utilizing a superior customized attention kernel. Beyond standard structure prediction, IntFold can be adapted to predict allosteric states, constrained structures, and binding affinity through the use of individual adapters. Furthermore, we introduce a novel confidence head to estimate docking quality, offering a more nuanced assessment for challenging targets such as antibody-antigen complexes. Finally, we share insights gained during the training process of this computationally intensive model.",
            "score": 31,
            "issue_id": 4642,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "5e8a0f59d9493778",
            "authors": [
                "The IntFold Team",
                "Leon Qiao",
                "Wayne Bai",
                "He Yan",
                "Gary Liu",
                "Nova Xi",
                "Xiang Zhang"
            ],
            "affiliations": [
                "IntelliGen AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02025.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#healthcare"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "IntFold: Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€",
                    "desc": "IntFold - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ±Ñ‰Ğ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€. ĞĞ½Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ AlphaFold3, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ ÑĞ´Ñ€Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. IntFold Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ°Ğ»Ğ»Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¸ Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºĞ¸Ğ½Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑÑ‹ Ğ°Ğ½Ñ‚Ğ¸Ñ‚ĞµĞ»Ğ¾-Ğ°Ğ½Ñ‚Ğ¸Ğ³ĞµĞ½."
                },
                "en": {
                    "title": "IntFold: Advancing Biomolecular Structure Prediction with Precision and Flexibility",
                    "desc": "IntFold is a new foundation model designed for predicting the structures of biomolecules, both in general and specialized contexts. It achieves high accuracy in predictions, rivaling the leading model AlphaFold3, by employing a unique attention mechanism. The model is versatile, allowing for adaptations to predict various states and properties of biomolecules, such as allosteric states and binding affinities, through the use of specific adapters. Additionally, IntFold includes a confidence head that assesses the quality of docking predictions, particularly for complex targets like antibody-antigen interactions, and shares valuable insights from its training process."
                },
                "zh": {
                    "title": "IntFoldï¼šç”Ÿç‰©åˆ†å­ç»“æ„é¢„æµ‹çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†IntFoldï¼Œè¿™æ˜¯ä¸€ç§å¯æ§çš„åŸºç¡€æ¨¡å‹ï¼Œç”¨äºä¸€èˆ¬å’Œä¸“ä¸šçš„ç”Ÿç‰©åˆ†å­ç»“æ„é¢„æµ‹ã€‚IntFoldçš„é¢„æµ‹å‡†ç¡®æ€§ä¸æœ€å…ˆè¿›çš„AlphaFold3ç›¸å½“ï¼ŒåŒæ—¶é‡‡ç”¨äº†æ›´ä¼˜çš„å®šåˆ¶æ³¨æ„åŠ›æ ¸ã€‚é™¤äº†æ ‡å‡†çš„ç»“æ„é¢„æµ‹ï¼ŒIntFoldè¿˜å¯ä»¥é€šè¿‡ä½¿ç”¨å•ç‹¬çš„é€‚é…å™¨æ¥é¢„æµ‹å˜æ„çŠ¶æ€ã€å—é™ç»“æ„å’Œç»“åˆäº²å’ŒåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ç½®ä¿¡åº¦å¤´ï¼Œä»¥è¯„ä¼°å¯¹æ¥è´¨é‡ï¼Œä¸ºæŠ—ä½“-æŠ—åŸå¤åˆç‰©ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ç›®æ ‡æä¾›æ›´ç»†è‡´çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01352",
            "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy",
            "url": "https://huggingface.co/papers/2507.01352",
            "abstract": "Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality.",
            "score": 31,
            "issue_id": 4638,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "955bbdefa8606d12",
            "authors": [
                "Chris Yuhao Liu",
                "Liang Zeng",
                "Yuzhen Xiao",
                "Jujie He",
                "Jiacai Liu",
                "Chaojie Wang",
                "Rui Yan",
                "Wei Shen",
                "Fuxiang Zhang",
                "Jiacheng Xu",
                "Yang Liu",
                "Yahui Zhou"
            ],
            "affiliations": [
                "2050 Research, Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01352.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#rlhf",
                    "#training",
                    "#alignment",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ SynPref-40M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 40 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Skywork-Reward-V2 Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ 0.6B Ğ´Ğ¾ 8B. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼, Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF)."
                },
                "en": {
                    "title": "Unlocking Human Preferences with Skywork-Reward-V2",
                    "desc": "This paper addresses the limitations of current reward models (RMs) in reinforcement learning from human feedback (RLHF), which struggle to accurately reflect complex human preferences. The authors propose a new large-scale preference dataset, SynPref-40M, containing 40 million preference pairs, to improve the training of RMs. They introduce a two-stage human-AI curation pipeline that combines human annotation with AI scalability to ensure high-quality data. The resulting Skywork-Reward-V2 models, trained on a refined subset of this dataset, demonstrate superior performance across various benchmarks, showcasing the importance of quality data in enhancing reward model effectiveness."
                },
                "zh": {
                    "title": "æå‡å¥–åŠ±æ¨¡å‹çš„è´¨é‡ä¸æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¥–åŠ±æ¨¡å‹åœ¨ä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„é‡è¦æ€§ã€‚å½“å‰çš„å¼€æ”¾å¥–åŠ±æ¨¡å‹åœ¨è¯„ä¼°åŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰äººç±»åå¥½çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŒ…å«4000ä¸‡å¯¹åå¥½çš„å¤§è§„æ¨¡æ•°æ®é›†SynPref-40Mï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªäººæœºåä½œçš„ä¸¤é˜¶æ®µæ•°æ®å¤„ç†æµç¨‹ã€‚é€šè¿‡é«˜è´¨é‡çš„æ•°æ®æ ‡æ³¨å’ŒAIçš„è‡ªåŠ¨åŒ–å¤„ç†ï¼Œä½œè€…è®­ç»ƒäº†Skywork-Reward-V2ç³»åˆ—å¥–åŠ±æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23918",
            "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and\n  Future Frontiers",
            "url": "https://huggingface.co/papers/2506.23918",
            "abstract": "Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental \"semantic gap\" between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as a dynamic mental sketchpad. A similar evolution is now unfolding in AI, marking a fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from a passive input into a dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along a trajectory of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the think with image paradigm and its three-stage framework. (2) We provide a comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. By providing this structured overview, we aim to offer a clear roadmap for future research towards more powerful and human-aligned multimodal AI.",
            "score": 26,
            "issue_id": 4640,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "8526b6b1e8d4b31e",
            "authors": [
                "Zhaochen Su",
                "Peng Xia",
                "Hangyu Guo",
                "Zhenhua Liu",
                "Yan Ma",
                "Xiaoye Qu",
                "Jiaqi Liu",
                "Yanshu Li",
                "Kaide Zeng",
                "Zhengyuan Yang",
                "Linjie Li",
                "Yu Cheng",
                "Heng Ji",
                "Junxian He",
                "Yi R. Fung"
            ],
            "affiliations": [
                "Microsoft",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "UIUC",
                "UNC-Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23918.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#alignment",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğº Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ, Ğ³Ğ´Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹: Ğ¾Ñ‚ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½ÑƒÑ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğº Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Transforming Vision into Dynamic Thought in AI",
                    "desc": "This paper discusses the advancements in multimodal reasoning, particularly focusing on the 'think with images' paradigm. It highlights the limitations of traditional text-based reasoning, which treats visual data as static, leading to a disconnect between perception and thought. The authors propose a three-stage framework that evolves from using images as tools to integrating them into cognitive processes, allowing for dynamic manipulation of visual information. The survey aims to provide a structured overview of this emerging field, outlining foundational principles, core methods, evaluation benchmarks, and future research directions."
                },
                "zh": {
                    "title": "ä»å›¾åƒæ€è€ƒåˆ°æ€è€ƒå›¾åƒçš„è½¬å˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€æ¨ç†çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åœ¨è¯­è¨€æ¨ç†ä¸­çš„åº”ç”¨ã€‚ä½œè€…æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„æ–‡æœ¬ä¸­å¿ƒæ–¹æ³•å°†è§†è§‰è§†ä¸ºé™æ€èƒŒæ™¯ï¼Œå¯¼è‡´æ„ŸçŸ¥æ•°æ®ä¸ç¬¦å·æ€ç»´ä¹‹é—´å­˜åœ¨â€œè¯­ä¹‰å·®è·â€ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ€ç»´æ¨¡å¼ï¼Œå¼ºè°ƒè§†è§‰ä¿¡æ¯åœ¨æ€ç»´è¿‡ç¨‹ä¸­çš„åŠ¨æ€ä½œç”¨ï¼Œä½¿å…¶æˆä¸ºå¯æ“ä½œçš„è®¤çŸ¥å·¥ä½œç©ºé—´ã€‚é€šè¿‡å»ºç«‹ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œè®ºæ–‡ä¸ºæœªæ¥çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç ”ç©¶æä¾›äº†æ¸…æ™°çš„è·¯çº¿å›¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02754",
            "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
            "url": "https://huggingface.co/papers/2507.02754",
            "abstract": "Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.   In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that 2-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.",
            "score": 13,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ»Ñ",
                "en": "July 3",
                "zh": "7æœˆ3æ—¥"
            },
            "hash": "a9492490e5a70bc4",
            "authors": [
                "Aurko Roy",
                "Timothy Chou",
                "Sai Surya Duvvuri",
                "Sijia Chen",
                "Jiecao Yu",
                "Xiaodong Wang",
                "Manzil Zaheer",
                "Rohan Anil"
            ],
            "affiliations": [
                "Department of Computer Science University of Texas at Austin",
                "Meta Menlo Park, CA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02754.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#reasoning",
                    "#math",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2-ÑĞ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° 2-ÑĞ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ°Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ñ‚Ñ€Ğ¸Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸. ĞŸÑ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ ÑĞ¾ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸. ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ÑÑ Ğ² Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ Ğ² Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Token Efficiency with 2-Simplicial Transformers",
                    "desc": "This paper explores the limitations of current scaling laws in machine learning, particularly in the context of large language models that are no longer purely compute-bound due to their reliance on vast datasets. It introduces the 2-simplicial Transformer, a new architecture that enhances standard attention mechanisms by using trilinear functions, which improves token efficiency. The authors show that this new architecture allows models to perform better on various tasks, such as mathematics and reasoning, while using the same number of tokens. By quantifying the improvements, they reveal that the 2-simplicial attention modifies the scaling laws, leading to better performance in knowledge and reasoning tasks compared to traditional dot-product attention."
                },
                "zh": {
                    "title": "æå‡æ ‡è®°æ•ˆç‡çš„2-å•çº¯å½¢å˜æ¢å™¨",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè®­ç»ƒæŸå¤±ä¸æ¨¡å‹å¤§å°å’Œæ ‡è®°æ•°é‡å‘ˆå¹‚å¾‹å…³ç³»ï¼Œè¾¾åˆ°è®¡ç®—æœ€ä¼˜æ¨¡å‹éœ€è¦åŒæ—¶æ‰©å¤§æ¨¡å‹å¤§å°å’Œæ ‡è®°æ•°é‡ã€‚ç„¶è€Œï¼Œè¿™äº›ç¼©æ”¾æ³•åˆ™å‡è®¾æ•°æ®æ˜¯æ— é™çš„ï¼Œå¹¶ä¸»è¦é€‚ç”¨äºè®¡ç®—å—é™çš„ç¯å¢ƒã€‚éšç€ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šä¾èµ–äºå¤§è§„æ¨¡äº’è”ç½‘æ•°æ®é›†ï¼Œè¿™ç§å‡è®¾å˜å¾—ä¸å†æœ‰æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¼˜å…ˆè€ƒè™‘æ ‡è®°æ•ˆç‡çš„æ¶æ„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02652",
            "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search",
            "url": "https://huggingface.co/papers/2507.02652",
            "abstract": "Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.",
            "score": 13,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ»Ñ",
                "en": "July 3",
                "zh": "7æœˆ3æ—¥"
            },
            "hash": "e833cc483ac9b10c",
            "authors": [
                "Jiajie Jin",
                "Xiaoxi Li",
                "Guanting Dong",
                "Yuyao Zhang",
                "Yutao Zhu",
                "Yang Zhao",
                "Hongjin Qian",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02652.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rag",
                    "#benchmark",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "HiRA: Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HiRA - Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². HiRA Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ HiRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "HiRA: Enhancing Search Efficiency through Hierarchical Reasoning",
                    "desc": "This paper presents HiRA, a new framework designed to improve complex information retrieval tasks by separating high-level planning from detailed execution. Traditional methods often struggle because they use a single model for both tasks, which can lead to inefficiencies. HiRA addresses this by breaking down complex search tasks into smaller subtasks, each handled by specialized agents that have their own tools and reasoning abilities. The results show that HiRA outperforms existing systems in both the quality of answers and overall efficiency, demonstrating the benefits of this hierarchical approach."
                },
                "zh": {
                    "title": "HiRAï¼šåˆ†å±‚æ¡†æ¶æå‡æœç´¢æ•ˆç‡ä¸è´¨é‡",
                    "desc": "åœ¨ç°å®ä¸–ç•Œçš„æœç´¢åœºæ™¯ä¸­ï¼Œå¤æ‚çš„ä¿¡æ¯éœ€æ±‚éœ€è¦æ·±åº¦æ¨ç†å’ŒçŸ¥è¯†ç»¼åˆï¼Œè€Œä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“éš¾ä»¥æœ‰æ•ˆåº”å¯¹ã€‚å½“å‰çš„æ¨ç†æ–¹æ³•å­˜åœ¨ä¸€ä¸ªæ ¹æœ¬æ€§é™åˆ¶ï¼šå®ƒä»¬ä½¿ç”¨å•ä¸€æ¨¡å‹å¤„ç†é«˜å±‚æ¬¡è§„åˆ’å’Œè¯¦ç»†æ‰§è¡Œï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹å’Œå¯æ‰©å±•æ€§æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†HiRAï¼Œä¸€ä¸ªåˆ†å±‚æ¡†æ¶ï¼Œå°†æˆ˜ç•¥è§„åˆ’ä¸ä¸“ä¸šæ‰§è¡Œåˆ†å¼€ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒHiRAåœ¨å¤æ‚çš„è·¨æ¨¡æ€æ·±åº¦æœç´¢åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGå’ŒåŸºäºä»£ç†çš„ç³»ç»Ÿï¼Œæå‡äº†ç­”æ¡ˆè´¨é‡å’Œç³»ç»Ÿæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02726",
            "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
            "url": "https://huggingface.co/papers/2507.02726",
            "abstract": "Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.",
            "score": 12,
            "issue_id": 4638,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ»Ñ",
                "en": "July 3",
                "zh": "7æœˆ3æ—¥"
            },
            "hash": "42e132c4863440b8",
            "authors": [
                "Matthieu Zimmer",
                "Xiaotong Ji",
                "Rasul Tutunov",
                "Anthony Bordg",
                "Jun Wang",
                "Haitham Bou Ammar"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "Imperial College London",
                "Lagrange Center",
                "UCL Centre for AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02726.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#benchmark",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ñƒ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº sG-MDP, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ğ¿Ñ€ĞµÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ñ†ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Monte Carlo Tree Search, Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ sG-MDP. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Bourbaki (7B), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ PutnamBench Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°."
                },
                "en": {
                    "title": "Empowering Reasoning with Self-Generated Goals in Theorem Proving",
                    "desc": "This paper addresses the difficulties that large language models (LLMs) face in reasoning tasks, particularly in automated theorem proving (ATP) where rewards are sparse and proofs are complex. The authors propose a novel framework called self-generated goal-conditioned MDPs (sG-MDPs), which allows agents to create and pursue subgoals based on the current state of the proof. By structuring goal generation, the problem becomes easier to navigate and search. The framework is implemented in a system called Bourbaki (7B), which utilizes multiple LLMs to enhance subgoal generation and tactic synthesis, achieving state-of-the-art results on the challenging PutnamBench benchmark."
                },
                "zh": {
                    "title": "è‡ªç”Ÿæˆç›®æ ‡åŠ©åŠ›æ¨ç†æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆATPï¼‰ä¸­çš„æ¨ç†æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¨€ç–å¥–åŠ±å’Œè¯æ˜è§„æ¨¡åºå¤§çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”è‡ªç”Ÿæˆç›®æ ‡æ¡ä»¶é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆsG-MDPsï¼‰ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®ä¸æ–­å˜åŒ–çš„è¯æ˜çŠ¶æ€ç”Ÿæˆå’Œè¿½æ±‚å­ç›®æ ‡ã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–çš„ç›®æ ‡ç”Ÿæˆï¼Œé—®é¢˜å˜å¾—æ›´æ˜“äºæœç´¢ã€‚æœ€åï¼Œåº”ç”¨ç±»ä¼¼è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„ç®—æ³•è§£å†³sG-MDPï¼Œå¹¶åœ¨Bourbakiï¼ˆ7Bï¼‰ç³»ç»Ÿä¸­å®ç°ï¼ŒæˆåŠŸåœ¨PutnamBenchä¸Šè§£å†³äº†26ä¸ªé—®é¢˜ï¼Œåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02694",
            "title": "Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers",
            "url": "https://huggingface.co/papers/2507.02694",
            "abstract": "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.",
            "score": 9,
            "issue_id": 4641,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ»Ñ",
                "en": "July 3",
                "zh": "7æœˆ3æ—¥"
            },
            "hash": "d7b392be540c08ba",
            "authors": [
                "Zhijian Xu",
                "Yilun Zhao",
                "Manasi Patwardhan",
                "Lovekesh Vig",
                "Arman Cohan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.02694.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rag",
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#science"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸: LLM ĞºĞ°Ğº Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ² Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LimitGen Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. LimitGen Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğµ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ¸Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¿Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğµ."
                },
                "en": {
                    "title": "Empowering Peer Review with AI: LimitGen for Identifying Research Limitations",
                    "desc": "This paper addresses the challenges of peer review in scientific research due to the increasing number of publications. It introduces a taxonomy of limitation types specifically for AI research, which helps in understanding the weaknesses of scientific papers. The authors present LimitGen, a benchmark designed to evaluate how well large language models (LLMs) can assist in identifying these limitations and provide feedback. By incorporating literature retrieval, the study enhances LLMs' ability to generate relevant and constructive critiques of research papers."
                },
                "zh": {
                    "title": "æå‡åŒè¡Œè¯„å®¡çš„æ™ºèƒ½åŒ–æ”¯æŒ",
                    "desc": "åŒè¡Œè¯„å®¡æ˜¯ç§‘å­¦ç ”ç©¶çš„é‡è¦ç¯èŠ‚ï¼Œä½†éšç€å‡ºç‰ˆç‰©æ•°é‡çš„å¢åŠ ï¼Œè¿™ä¸€è¿‡ç¨‹é¢ä¸´è¶Šæ¥è¶Šå¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„ç§‘å­¦ç ”ç©¶å±€é™æ€§åˆ†ç±»æ³•ï¼Œç‰¹åˆ«å…³æ³¨äººå·¥æ™ºèƒ½é¢†åŸŸã€‚æˆ‘ä»¬ä»‹ç»äº†LimitGenï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ”¯æŒæ—©æœŸåé¦ˆå’Œè¡¥å……äººç±»è¯„å®¡æ–¹é¢èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡ç»“åˆæ–‡çŒ®æ£€ç´¢ï¼Œæˆ‘ä»¬å¢å¼ºäº†LLMç³»ç»Ÿè¯†åˆ«ç ”ç©¶å±€é™æ€§çš„èƒ½åŠ›ï¼Œä»è€Œèƒ½å¤Ÿæä¾›æ›´å…·ä½“å’Œå»ºè®¾æ€§çš„åé¦ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02778",
            "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs",
            "url": "https://huggingface.co/papers/2507.02778",
            "abstract": "Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending \"Wait\" notably improves their ability to correct errors in their outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.",
            "score": 6,
            "issue_id": 4645,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ»Ñ",
                "en": "July 3",
                "zh": "7æœˆ3æ—¥"
            },
            "hash": "4b54d8d384329494",
            "authors": [
                "Ken Tsui"
            ],
            "affiliations": [
                "Independent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02778.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#alignment",
                    "#benchmark",
                    "#hallucinations"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ: ĞºĞ»ÑÑ‡ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ĞµÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸ĞµĞ¹, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ñ‚Ğ¾Ğ¼Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ñ…Ğ¾Ñ‚Ñ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ğ²Ğ²Ğ¾Ğ´Ğµ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ„Ñ€Ğ°Ğ·Ñ‹ \"Wait\" Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ñ… Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Self-Correction in Language Models",
                    "desc": "This paper introduces Self-Correction Bench, a framework designed to measure the self-correction capabilities of large language models (LLMs). It identifies a significant issue known as the 'Self-Correction Blind Spot', where LLMs fail to correct errors in their own outputs despite being able to recognize errors in user inputs. The study reveals that this blind spot is largely due to the training data, which often consists of error-free examples rather than sequences that include error corrections. Notably, the simple addition of the word 'Wait' to prompts can significantly enhance the models' self-correction abilities, indicating that these capabilities can be activated with the right cues."
                },
                "zh": {
                    "title": "æ¿€æ´»è‡ªæˆ‘çº æ­£ï¼Œæå‡è¯­è¨€æ¨¡å‹çš„å¯é æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†è‡ªæˆ‘çº æ­£åŸºå‡†ï¼ˆSelf-Correction Benchï¼‰ï¼Œç”¨äºæµ‹é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªæˆ‘çº æ­£æ–¹é¢çš„ç›²ç‚¹ã€‚ç ”ç©¶å‘ç°ï¼Œä¸»è¦åœ¨æ— é”™è¯¯çš„å“åº”ä¸Šè¿›è¡Œè®­ç»ƒä¼šå¯¼è‡´æ¨¡å‹åœ¨è‡ªèº«è¾“å‡ºä¸­æ— æ³•çº æ­£ç›¸åŒçš„é”™è¯¯ã€‚é€šè¿‡å¯¹14ä¸ªæ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œå‘ç°å¹³å‡æœ‰64.5%çš„ç›²ç‚¹ç‡ã€‚ç®€å•åœ°åœ¨è¾“å‡ºä¸­æ·»åŠ â€œç­‰å¾…â€ä¸€è¯å¯ä»¥å°†ç›²ç‚¹å‡å°‘89.3%ï¼Œè¿™è¡¨æ˜æ¨¡å‹å…·å¤‡è‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ï¼Œä½†éœ€è¦æ¿€æ´»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02092",
            "title": "Energy-Based Transformers are Scalable Learners and Thinkers",
            "url": "https://huggingface.co/papers/2507.02092",
            "abstract": "Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question \"Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?\" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.",
            "score": 6,
            "issue_id": 4642,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "9f33fd27885f443d",
            "authors": [
                "Alexi Gladstone",
                "Ganesh Nanduru",
                "Md Mofijul Islam",
                "Peixuan Han",
                "Hyeonjeong Ha",
                "Aman Chadha",
                "Yilun Du",
                "Heng Ji",
                "Jundong Li",
                "Tariq Iqbal"
            ],
            "affiliations": [
                "Amazon GenAI",
                "Harvard University",
                "Stanford University",
                "UIUC",
                "UVA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02092.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "EBTs: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Energy-Based Transformers (EBTs), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. EBTs Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ñ‹ 2' Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EBTs Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡ĞµĞ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹, Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EBTs Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰ÑƒÑ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Energy-Based Transformers: Scaling Learning and Thinking in AI",
                    "desc": "This paper introduces Energy-Based Transformers (EBTs), a new type of model that enhances inference-time computation by mimicking human System 2 Thinking. EBTs learn to verify the compatibility between inputs and predictions without needing additional supervision, making them more generalizable across different modalities and tasks. The authors demonstrate that EBTs can scale faster than existing models like Transformer++ and achieve superior performance in both language and image tasks. Overall, EBTs represent a significant advancement in the efficiency and effectiveness of machine learning models."
                },
                "zh": {
                    "title": "èƒ½é‡åŸºç¡€å˜æ¢å™¨ï¼šæ— ç›‘ç£å­¦ä¹ çš„æ–°æ€ç»´æ–¹å¼",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æ¨¡å‹â€”â€”èƒ½é‡åŸºç¡€å˜æ¢å™¨ï¼ˆEBTsï¼‰ï¼Œæ—¨åœ¨é€šè¿‡æ— ç›‘ç£å­¦ä¹ æ¥å®ç°æ›´å¥½çš„æ¨ç†èƒ½åŠ›ã€‚EBTsé€šè¿‡æ˜¾å¼éªŒè¯è¾“å…¥ä¸å€™é€‰é¢„æµ‹ä¹‹é—´çš„å…¼å®¹æ€§ï¼Œå°†é¢„æµ‹é—®é¢˜é‡æ–°æ¡†æ¶ä¸ºä¼˜åŒ–é—®é¢˜ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒEBTsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯”ä¼ ç»Ÿçš„Transformer++æ–¹æ³•å…·æœ‰æ›´å¿«çš„æ‰©å±•é€Ÿåº¦ï¼Œå¹¶åœ¨æ¨ç†æ—¶åœ¨è¯­è¨€ä»»åŠ¡ä¸Šæé«˜äº†29%çš„æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼ŒEBTsåœ¨å¤§å¤šæ•°ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01663",
            "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training",
            "url": "https://huggingface.co/papers/2507.01663",
            "abstract": "Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the corresponding resource idling and workload imbalance. Moreover, most existing frameworks are tightly coupled with LLM training or inference engines, making it difficult to support custom-designed engines. To address these challenges, we propose AsyncFlow, an asynchronous streaming RL framework for efficient post-training. Specifically, we introduce a distributed data storage and transfer module that provides a unified data management and fine-grained scheduling capability in a fully streamed manner. This architecture inherently facilitates automated pipeline overlapping among RL tasks and dynamic load balancing. Moreover, we propose a producer-consumer-based asynchronous workflow engineered to minimize computational idleness by strategically deferring parameter update process within staleness thresholds. Finally, the core capability of AsynFlow is architecturally decoupled from underlying training and inference engines and encapsulated by service-oriented user interfaces, offering a modular and customizable user experience. Extensive experiments demonstrate an average of 1.59 throughput improvement compared with state-of-the-art baseline. The presented architecture in this work provides actionable insights for next-generation RL training system designs.",
            "score": 4,
            "issue_id": 4639,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "8a3f43a4a9e735d7",
            "authors": [
                "Zhenyu Han",
                "Ansheng You",
                "Haibo Wang",
                "Kui Luo",
                "Guang Yang",
                "Wenqi Shi",
                "Menglong Chen",
                "Sicheng Zhang",
                "Zeshun Lan",
                "Chunshi Deng",
                "Huazhong Ji",
                "Wenjie Liu",
                "Yu Huang",
                "Yixiang Zhang",
                "Chenyi Pan",
                "Jing Wang",
                "Xin Huang",
                "Chunsheng Li",
                "Jianping Wu"
            ],
            "affiliations": [
                "Huawei"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01663.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "AsyncFlow: ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "AsyncFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. AsyncFlow Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ-Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ĞµĞ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² 1,59 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "AsyncFlow: Revolutionizing RL for Large Language Models",
                    "desc": "This paper introduces AsyncFlow, a new framework for reinforcement learning (RL) that enhances the post-training phase of large language models (LLMs). It addresses scalability issues found in traditional RL frameworks by implementing a distributed data storage and transfer system, which allows for efficient data management and scheduling. The framework also features an asynchronous workflow that reduces idle computation time by optimizing the timing of parameter updates. Overall, AsyncFlow is designed to be modular and customizable, making it easier to integrate with various training and inference engines while improving throughput significantly."
                },
                "zh": {
                    "title": "AsyncFlowï¼šé«˜æ•ˆçš„å¼‚æ­¥æµå¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åè®­ç»ƒé˜¶æ®µå˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„ä»»åŠ¡å…±å­˜RLæ¡†æ¶é¢ä¸´å¯æ‰©å±•æ€§ç“¶é¢ˆï¼Œè€Œä»»åŠ¡åˆ†ç¦»çš„RLæ¡†æ¶åœ¨å¤æ‚æ•°æ®æµå’Œèµ„æºé—²ç½®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AsyncFlowï¼Œä¸€ä¸ªé«˜æ•ˆçš„å¼‚æ­¥æµå¼RLæ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°è‡ªåŠ¨åŒ–çš„ç®¡é“é‡å å’ŒåŠ¨æ€è´Ÿè½½å¹³è¡¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼ŒAsyncFlowåœ¨ååé‡ä¸Šå¹³å‡æé«˜äº†1.59å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01004",
            "title": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention",
            "url": "https://huggingface.co/papers/2507.01004",
            "abstract": "A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.",
            "score": 4,
            "issue_id": 4645,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ»Ñ",
                "en": "July 1",
                "zh": "7æœˆ1æ—¥"
            },
            "hash": "c104abc218e38a97",
            "authors": [
                "Yuhong Chou",
                "Zehao Liu",
                "Ruijie Zhu",
                "Xinyi Wan",
                "Tianjian Li",
                "Congying Chu",
                "Qian Liu",
                "Jibin Wu",
                "Zejun Ma"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "National University of Singapore",
                "The Hong Kong Polytechnic University",
                "TikTok",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01004.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ZeCO: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸",
                    "desc": "ZeCO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ZeCO Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ² ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ All-Scan, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ½Ğ³Ñƒ SP Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ZeCO, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° 256 GPU Ñ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 8M ZeCO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° 60% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ SP."
                },
                "en": {
                    "title": "ZeCO: Revolutionizing Long Sequence Training with Zero Communication Overhead",
                    "desc": "This paper presents ZeCO, a novel sequence parallelism method that eliminates communication overhead during the training of large language models (LLMs) with ultra-long sequences. By utilizing linear attention mechanisms, ZeCO allows for efficient processing of sequences up to 1 million tokens across multiple devices without the typical bottlenecks caused by communication delays. The core innovation, All-Scan, enables each device to access the necessary operator state with minimal communication, resulting in near-linear scalability for long sequence training. Empirical results show that ZeCO significantly outperforms existing methods, achieving a 60% speedup on 256 GPUs with an 8M sequence length, paving the way for training next-generation LLMs."
                },
                "zh": {
                    "title": "ZeCOï¼šé«˜æ•ˆè®­ç»ƒè¶…é•¿åºåˆ—çš„å¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„é›¶é€šä¿¡å¼€é”€åºåˆ—å¹¶è¡Œæ–¹æ³•ZeCOï¼Œæ—¨åœ¨é«˜æ•ˆè®­ç»ƒå…·æœ‰è¶…é•¿åºåˆ—çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ZeCOé€šè¿‡å¼•å…¥All-Scanè¿™ä¸€æ–°çš„é›†ä½“é€šä¿¡åŸè¯­ï¼Œæ˜¾è‘—å‡å°‘äº†è®¾å¤‡é—´çš„é€šä¿¡å¼€é”€ï¼Œä»è€Œå®ç°äº†æ¥è¿‘çº¿æ€§çš„å¯æ‰©å±•æ€§ã€‚ä¸ä¼ ç»Ÿçš„åºåˆ—å¹¶è¡Œæ–¹æ³•ç›¸æ¯”ï¼ŒZeCOåœ¨å¤šä¸ªè®¾å¤‡ä¸Šå¤„ç†1Måºåˆ—æ—¶çš„è®­ç»ƒæ—¶é—´ä¸åœ¨å•ä¸ªè®¾å¤‡ä¸Šå¤„ç†16kåºåˆ—çš„æ—¶é—´ç›¸å½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZeCOåœ¨256ä¸ªGPUä¸Šå¤„ç†8Måºåˆ—æ—¶ï¼Œç›¸è¾ƒäºç°æœ‰çš„æœ€ä½³åºåˆ—å¹¶è¡Œæ–¹æ³•å®ç°äº†60%çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22813",
            "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity\n  Recognition with Large Language Models",
            "url": "https://huggingface.co/papers/2506.22813",
            "abstract": "Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework.",
            "score": 4,
            "issue_id": 4644,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ½Ñ",
                "en": "June 28",
                "zh": "6æœˆ28æ—¥"
            },
            "hash": "762c8f77cac2babf",
            "authors": [
                "Zhuojun Ding",
                "Wei Wei",
                "Chenghao Fan"
            ],
            "affiliations": [
                "School of Computer Science & Technology, Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22813.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#transfer_learning",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ SaM Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, SaM Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SaM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 10% Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Dynamic Expert Selection for Enhanced Domain Adaptation",
                    "desc": "The paper introduces the SaM framework, which enhances the performance of large language models in information extraction tasks by dynamically selecting and merging expert models tailored to specific domains. Instead of training a single model for all domains, SaM identifies domain-specific experts based on their relevance and performance, allowing for better adaptation to target tasks. This approach not only improves generalization across various domains but also offers scalability by enabling the addition or removal of experts without retraining. Experimental results show that the SaM framework outperforms traditional unified models by an average of 10%, highlighting its effectiveness in optimizing task-specific performance."
                },
                "zh": {
                    "title": "åŠ¨æ€é€‰æ‹©ä¸åˆå¹¶ä¸“å®¶æ¨¡å‹ï¼Œæå‡è·¨é¢†åŸŸæ€§èƒ½",
                    "desc": "ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¹¿æ³›åº”ç”¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ä¿¡æ¯æå–ï¼ˆIEï¼‰ä»»åŠ¡å¯¹é½ï¼Œä¾‹å¦‚å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€‚ç„¶è€Œï¼Œæ ‡æ³¨è¿™äº›ç»†ç²’åº¦æ ‡ç­¾å’Œè®­ç»ƒç‰¹å®šé¢†åŸŸçš„æ¨¡å‹æˆæœ¬é«˜æ˜‚ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨å¤šä¸ªé¢†åŸŸè®­ç»ƒç»Ÿä¸€æ¨¡å‹ï¼Œä½†è¿™ç§æ–¹æ³•ç¼ºä¹é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå› ä¸ºå¹¶éæ‰€æœ‰è®­ç»ƒæ•°æ®éƒ½èƒ½æƒ åŠç›®æ ‡é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†SaMæ¡†æ¶ï¼Œåœ¨æ¨ç†æ—¶åŠ¨æ€é€‰æ‹©å’Œåˆå¹¶ä¸“å®¶æ¨¡å‹ï¼Œä»è€Œæé«˜äº†è·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23121",
            "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for\n  Multi-Organ Segmentation",
            "url": "https://huggingface.co/papers/2506.23121",
            "abstract": "Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP\\_SAM2.git.",
            "score": 2,
            "issue_id": 4660,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ½Ñ",
                "en": "June 29",
                "zh": "6æœˆ29æ—¥"
            },
            "hash": "925d84357946a3a1",
            "authors": [
                "Xinlei Yu",
                "Chanmiao Wang",
                "Hui Jin",
                "Ahmed Elazab",
                "Gangyong Jia",
                "Xiang Wan",
                "Changqing Zou",
                "Ruiquan Ge"
            ],
            "affiliations": [
                "Hangzhou Dianzi University, Hangzhou, China",
                "Shenzhen Research Institute of Big Data, Shenzhen, China",
                "Shenzhen University, Shenzhen, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23121.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#healthcare"
                ],
                "emoji": "ğŸ©»",
                "ru": {
                    "title": "CRISP-SAM2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CRISP-SAM2 Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SAM2, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. CRISP-SAM2 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ CRISP-SAM2 Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Medical Image Segmentation with CRISP-SAM2",
                    "desc": "This paper presents CRISP-SAM2, a new model for multi-organ medical segmentation that improves upon existing methods by addressing common issues like detail accuracy and reliance on geometric prompts. The model utilizes a cross-modal interaction mechanism to integrate visual and textual data, enhancing the understanding of medical images. By implementing a semantic prompting strategy, CRISP-SAM2 reduces dependency on geometric cues, allowing for better identification of complex organ structures. Experimental results show that this model significantly outperforms previous approaches across multiple datasets, confirming its effectiveness in medical image processing."
                },
                "zh": {
                    "title": "CRISP-SAM2ï¼šæå‡å¤šè„å™¨åŒ»å­¦åˆ†å‰²çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "å¤šè„å™¨åŒ»å­¦åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒå¤„ç†ä¸­çš„é‡è¦ç¯èŠ‚ï¼Œå¸®åŠ©åŒ»ç”Ÿè¿›è¡Œå‡†ç¡®è¯Šæ–­å’Œåˆ¶å®šæœ‰æ•ˆæ²»ç–—æ–¹æ¡ˆã€‚å°½ç®¡è¯¥é¢†åŸŸå·²æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„å¤šè„å™¨åˆ†å‰²æ¨¡å‹å¸¸å¸¸é¢ä¸´ç»†èŠ‚ä¸å‡†ç¡®ã€ä¾èµ–å‡ ä½•æç¤ºå’Œç©ºé—´ä¿¡æ¯ä¸¢å¤±ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¨¡å‹CRISP-SAM2ï¼ŒåŸºäºSAM2çš„è·¨æ¨¡æ€äº¤äº’å’Œè¯­ä¹‰æç¤ºã€‚è¯¥æ¨¡å‹é€šè¿‡é€æ­¥çš„è·¨æ³¨æ„åŠ›äº¤äº’æœºåˆ¶ï¼Œå°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºè·¨æ¨¡æ€ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œä»è€Œå¢å¼ºå¯¹è§†è§‰ä¿¡æ¯çš„ç»†è‡´ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21546",
            "title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation\n  Hallucination Evaluation",
            "url": "https://huggingface.co/papers/2506.21546",
            "abstract": "HalluSegBench provides a benchmark for evaluating hallucinations in vision-language segmentation models by analyzing counterfactual scene edits.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in vision-language segmentation has significantly advanced grounded visual understanding. However, these models often exhibit hallucinations by producing segmentation masks for objects not grounded in the image content or by incorrectly labeling irrelevant regions. Existing evaluation protocols for segmentation hallucination primarily focus on label or textual hallucinations without manipulating the visual context, limiting their capacity to diagnose critical failures. In response, we introduce HalluSegBench, the first benchmark specifically designed to evaluate hallucinations in visual grounding through the lens of counterfactual visual reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual instance pairs spanning 281 unique object classes, and a set of newly introduced metrics that quantify hallucination sensitivity under visually coherent scene edits. Experiments on HalluSegBench with state-of-the-art vision-language segmentation models reveal that vision-driven hallucinations are significantly more prevalent than label-driven ones, with models often persisting in false segmentation, highlighting the need for counterfactual reasoning to diagnose grounding fidelity.",
            "score": 1,
            "issue_id": 4655,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "d577c3d7e6c15a10",
            "authors": [
                "Xinzhuo Li",
                "Adheesh Juvekar",
                "Xingyou Liu",
                "Muntasir Wahed",
                "Kiet A. Nguyen",
                "Ismini Lourentzou"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21546.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#hallucinations",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·",
                    "desc": "HalluSegBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 1340 Ğ¿Ğ°Ñ€ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ†ĞµĞ½. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ¹, Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ñ‹, Ñ‡ĞµĞ¼ Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Evaluating Hallucinations in Vision-Language Models with Counterfactuals",
                    "desc": "HalluSegBench is a new benchmark designed to evaluate hallucinations in vision-language segmentation models by using counterfactual scene edits. It addresses the limitations of existing evaluation methods that mainly focus on label or textual hallucinations without altering the visual context. The benchmark includes a dataset of 1340 counterfactual instance pairs across 281 object classes and introduces metrics to measure hallucination sensitivity. Experiments show that vision-driven hallucinations are more common than label-driven ones, emphasizing the importance of counterfactual reasoning for assessing model accuracy in visual grounding."
                },
                "zh": {
                    "title": "HalluSegBenchï¼šè¯„ä¼°è§†è§‰å¹»è§‰çš„æ–°åŸºå‡†",
                    "desc": "HalluSegBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€åˆ†å‰²æ¨¡å‹ä¸­å¹»è§‰ç°è±¡çš„åŸºå‡†ã€‚è¯¥åŸºå‡†é€šè¿‡åˆ†æåäº‹å®åœºæ™¯ç¼–è¾‘ï¼Œå¸®åŠ©è¯†åˆ«æ¨¡å‹åœ¨å›¾åƒå†…å®¹ä¸­æœªæ­£ç¡®å®šä½çš„å¯¹è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨æ ‡ç­¾æˆ–æ–‡æœ¬å¹»è§‰ï¼Œè€Œå¿½è§†äº†è§†è§‰ä¸Šä¸‹æ–‡çš„å˜åŒ–ã€‚HalluSegBenchæä¾›äº†1340å¯¹åäº‹å®å®ä¾‹å’Œæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ­ç¤ºäº†è§†è§‰é©±åŠ¨çš„å¹»è§‰ç°è±¡æ¯”æ ‡ç­¾é©±åŠ¨çš„æ›´ä¸ºæ™®éï¼Œå¼ºè°ƒäº†åäº‹å®æ¨ç†åœ¨è¯Šæ–­æ¨¡å‹å‡†ç¡®æ€§ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-03.html",
    "link_next": "2025-07-07.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "03.07",
        "en": "07/03",
        "zh": "7æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "07.07",
        "en": "07/07",
        "zh": "7æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 2,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 8,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}