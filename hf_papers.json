{
    "date": {
        "ru": "15 Ğ¼Ğ°Ñ",
        "en": "May 15",
        "zh": "5æœˆ15æ—¥"
    },
    "time_utc": "2025-05-15 07:12",
    "weekday": 3,
    "issue_id": 3773,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.09568",
            "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
            "url": "https://huggingface.co/papers/2505.09568",
            "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.",
            "score": 7,
            "issue_id": 3768,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "822f8dd79d39211b",
            "authors": [
                "Jiuhai Chen",
                "Zhiyang Xu",
                "Xichen Pan",
                "Yushi Hu",
                "Can Qin",
                "Tom Goldstein",
                "Lifu Huang",
                "Tianyi Zhou",
                "Saining Xie",
                "Silvio Savarese",
                "Le Xue",
                "Caiming Xiong",
                "Ran Xu"
            ],
            "affiliations": [
                "New York University",
                "Salesforce Research",
                "UC Davis",
                "University of Maryland",
                "University of Washington",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09568.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… CLIP-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… VAE-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BLIP3-o, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ°Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unifying Image Understanding and Generation with BLIP3-o",
                    "desc": "This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å›¾åƒç†è§£ä¸ç”Ÿæˆçš„åˆ›æ–°æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹ï¼Œå¼ºè°ƒäº†è‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡ç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿ç”¨æ‰©æ•£å˜æ¢å™¨ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„CLIPå›¾åƒç‰¹å¾ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚é€šè¿‡å…ˆè¿›è¡Œå›¾åƒç†è§£è®­ç»ƒï¼Œå†è¿›è¡Œå›¾åƒç”Ÿæˆè®­ç»ƒçš„é¡ºåºé¢„è®­ç»ƒç­–ç•¥ï¼Œä¿æŒäº†å›¾åƒç†è§£èƒ½åŠ›çš„åŒæ—¶å¢å¼ºäº†ç”Ÿæˆèƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºäº†é«˜è´¨é‡çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†BLIP3o-60kï¼Œä»¥æ”¯æŒå›¾åƒç”Ÿæˆä»»åŠ¡çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09343",
            "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
            "url": "https://huggingface.co/papers/2505.09343",
            "abstract": "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.",
            "score": 3,
            "issue_id": 3773,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "3c249078ec32a334",
            "authors": [
                "Chenggang Zhao",
                "Chengqi Deng",
                "Chong Ruan",
                "Damai Dai",
                "Huazuo Gao",
                "Jiashi Li",
                "Liyue Zhang",
                "Panpan Huang",
                "Shangyan Zhou",
                "Shirong Ma",
                "Wenfeng Liang",
                "Ying He",
                "Yuqing Wang",
                "Yuxuan Liu",
                "Y. X. Wei"
            ],
            "affiliations": [
                "DeepSeek-AI Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09343.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek-V3/R1 Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ˜Ğ˜, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Multi-head Latent Attention (MLA) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture of Experts (MoE) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ FP8 Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ DeepSeek-V3 Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Innovating AI: Bridging Hardware and Model Design for Scalable Solutions",
                    "desc": "This paper discusses the limitations of current hardware when training large language models (LLMs) and introduces DeepSeek-V3 as a solution. It emphasizes hardware-aware model co-design, which improves memory efficiency and computational performance. Key innovations include Multi-head Latent Attention for better memory use, Mixture of Experts for efficient computation, and FP8 mixed-precision training to maximize hardware capabilities. The authors also explore future hardware advancements needed to support the growing demands of AI workloads, highlighting the importance of integrating hardware and model design."
                },
                "zh": {
                    "title": "ç¡¬ä»¶ä¸æ¨¡å‹å…±åŒè®¾è®¡ï¼Œæ¨åŠ¨AIåˆ›æ–°",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¡¬ä»¶æ¶æ„ä¸Šçš„é™åˆ¶ï¼ŒåŒ…æ‹¬å†…å­˜å®¹é‡ã€è®¡ç®—æ•ˆç‡å’Œäº’è¿å¸¦å®½ç­‰é—®é¢˜ã€‚DeepSeek-V3æ¨¡å‹åœ¨2048ä¸ªNVIDIA H800 GPUä¸Šè®­ç»ƒï¼Œå±•ç¤ºäº†ç¡¬ä»¶æ„ŸçŸ¥æ¨¡å‹å…±åŒè®¾è®¡å¦‚ä½•æœ‰æ•ˆè§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è®ºæ–‡åˆ†æäº†DeepSeek-V3/R1æ¨¡å‹æ¶æ„åŠå…¶AIåŸºç¡€è®¾æ–½ï¼Œä»‹ç»äº†å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰ã€ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„å’ŒFP8æ··åˆç²¾åº¦è®­ç»ƒç­‰åˆ›æ–°ã€‚æœ€åï¼Œä½œè€…ä¸å­¦æœ¯ç•Œå’Œå·¥ä¸šç•ŒåŒè¡Œæ¢è®¨äº†æœªæ¥ç¡¬ä»¶çš„å‘å±•æ–¹å‘ï¼Œå¼ºè°ƒäº†ç¡¬ä»¶ä¸æ¨¡å‹å…±åŒè®¾è®¡åœ¨æ»¡è¶³AIå·¥ä½œè´Ÿè½½éœ€æ±‚ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.08455",
            "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
            "url": "https://huggingface.co/papers/2505.08455",
            "abstract": "Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.",
            "score": 1,
            "issue_id": 3773,
            "pub_date": "2025-05-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ",
                "en": "May 13",
                "zh": "5æœˆ13æ—¥"
            },
            "hash": "b7bc69bb40029690",
            "authors": [
                "Pritam Sarkar",
                "Ali Etemad"
            ],
            "affiliations": [
                "Queens University, Canada",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.08455.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#long_context",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VCRBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. VCRBench Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LVLM Ğ½Ğ° VCRBench Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¸Ñ… Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Recognition-Reasoning Decomposition (RRD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ» Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° VCRBench."
                },
                "en": {
                    "title": "Enhancing Video Causal Reasoning with RRD",
                    "desc": "This paper addresses the limitations of Large Video Language Models (LVLMs) in performing causal reasoning with videos. It introduces a new benchmark called Video-based long-form Causal Reasoning (VCRBench), which tests LVLMs on their ability to identify and sequence causal events in procedural videos. The benchmark is designed to challenge models by preventing them from using linguistic shortcuts and focuses on long-range causal dependencies. The authors propose a method called Recognition-Reasoning Decomposition (RRD) that improves the performance of LVLMs on VCRBench by separating the tasks of video recognition and causal reasoning, resulting in significant accuracy gains."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘å› æœæ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "å°½ç®¡è§†é¢‘ç†è§£æŠ€æœ¯æœ‰æ‰€è¿›æ­¥ï¼Œä½†å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†é¢‘åŸºç¡€çš„å› æœæ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºè§†é¢‘åŸºç¡€çš„é•¿å½¢å¼å› æœæ¨ç†ï¼ˆVCRBenchï¼‰ï¼Œé€šè¿‡å¯¹æ—¥å¸¸æ´»åŠ¨çš„è§†é¢‘è¿›è¡Œå¤„ç†ï¼Œæµ‹è¯•LVLMsèƒ½å¦è¯†åˆ«ã€æ¨ç†å¹¶æ­£ç¡®æ’åºå®ç°ç‰¹å®šç›®æ ‡æ‰€éœ€çš„äº‹ä»¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„æ–¹æ³•ï¼Œç§°ä¸ºè¯†åˆ«-æ¨ç†åˆ†è§£ï¼ˆRRDï¼‰ï¼Œå°†è§†é¢‘åŸºç¡€çš„å› æœæ¨ç†åˆ†ä¸ºè§†é¢‘è¯†åˆ«å’Œå› æœæ¨ç†ä¸¤ä¸ªå­ä»»åŠ¡ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLVLMsåœ¨å¤æ‚çš„é•¿å½¢å¼å› æœæ¨ç†ä»»åŠ¡ä¸­ä¸»è¦ä¾èµ–è¯­è¨€çŸ¥è¯†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-14.html",
    "link_next": "2025-05-16.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "14.05",
        "en": "05/14",
        "zh": "5æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.05",
        "en": "05/16",
        "zh": "5æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº†MiniMax-Speechï¼Œä¸€ç§åŸºäºTransformerçš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯å¯å­¦ä¹ çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œæ— éœ€è½¬å½•ã€‚è¿™ä½¿å¾—MiniMax-Speechèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆè¡¨ç°åŠ›å¼ºã€éŸ³è‰²ä¸€è‡´çš„è¯­éŸ³ï¼Œå¹¶æ”¯æŒä¸€æ ·æœ¬å£°éŸ³å…‹éš†ã€‚é€šè¿‡Flow-VAEï¼ŒåˆæˆéŸ³é¢‘çš„æ•´ä½“è´¨é‡å¾—åˆ°æå‡ã€‚æ¨¡å‹æ”¯æŒ32ç§è¯­è¨€ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚",
        "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
        "pinyin": "æˆ‘ä»¬ä»‹ç»äº†MiniMax-Speechï¼Œä¸€ç§åŸºäºTransformerçš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯å¯å­¦ä¹ çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œæ— éœ€è½¬å½•ã€‚è¿™ä½¿å¾—MiniMax-Speechèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆè¡¨ç°åŠ›å¼ºã€éŸ³è‰²ä¸€è‡´çš„è¯­éŸ³ï¼Œå¹¶æ”¯æŒä¸€æ ·æœ¬å£°éŸ³å…‹éš†ã€‚é€šè¿‡Flow-VAEï¼ŒåˆæˆéŸ³é¢‘çš„æ•´ä½“è´¨é‡å¾—åˆ°æå‡ã€‚æ¨¡å‹æ”¯æŒ32ç§è¯­è¨€ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚\n\nWÇ’men jiÃ¨shÃ o le MiniMax-Speech, yÄ« zhÇ’ng jÄ«yÃº Transformer de wÃ©nbÄ›n zhuÇn yÇ”yÄ«n mÃ³xÃ­ng. TÄ nÃ©ng shÄ“ngchÃ©ng gÄo zhÃ¬liÃ ng de yÇ”yÄ«n. MÃ³xÃ­ng de guÇnjiÃ n chuÃ ngxÄ«n shÃ¬ kÄ› xuÃ©xÃ­ de yÇnjiÇngzhÄ› biÄnmÇqÃ¬, cÃ³ng cÄnkÇo yÄ«npÃ­n zhÅng tÃ­quÄn yÄ«nsÃ¨ tÃ¨zhÄ“ng, wÃºxÅ« zhuÇnlÃ¹. ZhÃ¨ shÇ de MiniMax-Speech nÃ©ng zÃ i lÃ­ng yÃ ngbÄ›n qÃ­ngkuÃ ng xiÃ  shÄ“ngchÃ©ng biÇoxiÃ nlÃ¬ qiÃ¡ng, yÄ«nsÃ¨ yÄ«zhÃ¬ de yÇ”yÄ«n, bÃ¬ng zhÄ«chÃ­ yÄ« yÃ ngbÄ›n shÄ“ngyÄ«n kÃ¨lÃ³ng. TÅngguÃ² Flow-VAE, hÃ©chÃ©ng yÄ«npÃ­n de zhÄ›ngtÇ zhÃ¬liÃ ng dÃ©dÃ o tÃ­shÄ“ng. MÃ³xÃ­ng zhÄ«chÃ­ 32 zhÇ’ng yÇ”yÃ¡n, zÃ i duÅ zhÇ’ng pÃ­nggÅ« zhÇbiÄo shÃ ng biÇoxiÃ n chÅ«sÃ¨, dÃ¡dÃ o le zuÃ¬ xiÄnjÃ¬n de jiÃ©guÇ’.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"MiniMax-Speech\", \"pinyin\": \"MiniMax-Speech\", \"trans\": \"MiniMax-Speech\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Transformer\", \"trans\": \"Transformer\"},\n    {\"word\": \"æ–‡æœ¬è½¬è¯­éŸ³\", \"pinyin\": \"wÃ©n bÄ›n zhuÇn yÇ” yÄ«n\", \"trans\": \"text-to-speech\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"è¯­éŸ³\", \"pinyin\": \"yÇ” yÄ«n\", \"trans\": \"speech\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇn jiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"åˆ›æ–°\", \"pinyin\": \"chuÃ ng xÄ«n\", \"trans\": \"innovation\"},\n    {\"word\": \"å¯å­¦ä¹ \", \"pinyin\": \"kÄ› xuÃ© xÃ­\", \"trans\": \"learnable\"},\n    {\"word\": \"æ¼”è®²è€…\", \"pinyin\": \"yÇn jiÇng zhÄ›\", \"trans\": \"speaker\"},\n    {\"word\": \"ç¼–ç å™¨\", \"pinyin\": \"biÄn mÇ qÃ¬\", \"trans\": \"encoder\"},\n    {\"word\": \"ä»\", \"pinyin\": \"cÃ³ng\", \"trans\": \"from\"},\n    {\"word\": \"å‚è€ƒ\", \"pinyin\": \"cÄn kÇo\", \"trans\": \"reference\"},\n    {\"word\": \"éŸ³é¢‘\", \"pinyin\": \"yÄ«n pÃ­n\", \"trans\": \"audio\"},\n    {\"word\": \"æå–\", \"pinyin\": \"tÃ­ qu\", \"trans\": \"extract\"},\n    {\"word\": \"éŸ³è‰²\", \"pinyin\": \"yÄ«n sÃ¨\", \"trans\": \"timbre\"},\n    {\"word\": \"ç‰¹å¾\", \"pinyin\": \"tÃ¨ zhÄ“ng\", \"trans\": \"features\"},\n    {\"word\": \"æ— éœ€\", \"pinyin\": \"wÃº xÅ«\", \"trans\": \"no need\"},\n    {\"word\": \"è½¬å½•\", \"pinyin\": \"zhuÇn lÃ¹\", \"trans\": \"transcription\"},\n    {\"word\": \"é›¶æ ·æœ¬\", \"pinyin\": \"lÃ­ng yÃ ng bÄ›n\", \"trans\": \"zero-shot\"},\n    {\"word\": \"æƒ…å†µ\", \"pinyin\": \"qÃ­ng kuÃ ng\", \"trans\": \"situation\"},\n    {\"word\": \"è¡¨ç°åŠ›\", \"pinyin\": \"biÇo xiÃ n lÃ¬\", \"trans\": \"expressiveness\"},\n    {\"word\": \"å¼º\", \"pinyin\": \"qiÃ¡ng\", \"trans\": \"strong\"},\n    {\"word\": \"ä¸€è‡´\", \"pinyin\": \"yÄ« zhÃ¬\", \"trans\": \"consistent\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ« chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"ä¸€æ ·æœ¬\", \"pinyin\": \"yÄ« yÃ ng bÄ›n\", \"trans\": \"one-shot\"},\n    {\"word\": \"å£°éŸ³\", \"pinyin\": \"shÄ“ng yÄ«n\", \"trans\": \"voice\"},\n    {\"word\": \"å…‹éš†\", \"pinyin\": \"kÃ¨ lÃ³ng\", \"trans\": \"clone\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅng guÃ²\", \"trans\": \"through\"},\n    {\"word\": \"Flow-VAE\", \"pinyin\": \"Flow-VAE\", \"trans\": \"Flow-VAE\"},\n    {\"word\": \"åˆæˆ\", \"pinyin\": \"hÃ© chÃ©ng\", \"trans\": \"synthesis\"},\n    {\"word\": \"æ•´ä½“\", \"pinyin\": \"zhÄ›ng tÇ\", \"trans\": \"overall\"},\n    {\"word\": \"è´¨é‡\", \"pinyin\": \"zhÃ¬ liÃ ng\", \"trans\": \"quality\"},\n    {\"word\": \"å¾—åˆ°\", \"pinyin\": \"dÃ© dÃ o\", \"trans\": \"obtain\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­ shÄ“ng\", \"trans\": \"improvement\"},\n    {\"word\": \"å¤šç§\", \"pinyin\": \"duÅ zhÇ’ng\", \"trans\": \"various\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"æŒ‡æ ‡\", \"pinyin\": \"zhÇ biÄo\", \"trans\": \"metrics\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"è¾¾åˆ°\", \"pinyin\": \"dÃ¡ dÃ o\", \"trans\": \"achieve\"},\n    {\"word\": \"æœ€å…ˆè¿›\", \"pinyin\": \"zuÃ¬ xiÄn jÃ¬n\", \"trans\": \"state-of-the-art\"}\n]",
        "trans": "We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results.",
        "update_ts": "2025-05-14 09:12"
    }
}