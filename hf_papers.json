{
    "date": {
        "ru": "23 Ğ¸ÑĞ»Ñ",
        "en": "July 23",
        "zh": "7æœˆ23æ—¥"
    },
    "time_utc": "2025-07-23 02:59",
    "weekday": 2,
    "issue_id": 4959,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.16815",
            "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning",
            "url": "https://huggingface.co/papers/2507.16815",
            "abstract": "ThinkAct, a dual-system framework, uses reinforced visual latent planning to enable high-level reasoning and robust action execution in vision-language-action tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.",
            "score": 5,
            "issue_id": 4959,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ»Ñ",
                "en": "July 22",
                "zh": "7æœˆ22æ—¥"
            },
            "hash": "a0c0c8cc661a52b5",
            "authors": [
                "Chi-Pin Huang",
                "Yueh-Hua Wu",
                "Min-Hung Chen",
                "Yu-Chiang Frank Wang",
                "Fu-En Yang"
            ],
            "affiliations": [
                "NVIDIA",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16815.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#robotics",
                    "#training",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞ¹: Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "ThinkAct - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ThinkAct Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ñ‹ ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‚ÑÑ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ."
                },
                "en": {
                    "title": "ThinkAct: Bridging Reasoning and Action in AI Tasks",
                    "desc": "ThinkAct is a novel framework designed for vision-language-action tasks that combines high-level reasoning with low-level action execution. It utilizes reinforced visual latent planning to create effective plans that guide agents in dynamic environments. By training a multimodal large language model (LLM), ThinkAct generates reasoning plans that are optimized through visual rewards, ensuring actions align with goals. The framework shows significant improvements in few-shot adaptation and long-horizon planning, making it suitable for complex AI tasks like robot manipulation."
                },
                "zh": {
                    "title": "ThinkActï¼šé«˜æ•ˆæ¨ç†ä¸åŠ¨ä½œæ‰§è¡Œçš„åŒç³»ç»Ÿæ¡†æ¶",
                    "desc": "ThinkActæ˜¯ä¸€ä¸ªåŒç³»ç»Ÿæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–è§†è§‰æ½œåœ¨è§„åˆ’å®ç°é«˜æ°´å¹³æ¨ç†å’Œç¨³å¥çš„åŠ¨ä½œæ‰§è¡Œã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ä»»åŠ¡ï¼Œå¸®åŠ©æ™ºèƒ½ä½“ç†è§£å¤šæ¨¡æ€æŒ‡ä»¤å¹¶è¿›è¡Œé•¿è¿œè§„åˆ’ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒThinkActé€šè¿‡ç”Ÿæˆä¸åŠ¨ä½œå¯¹é½çš„è§†è§‰å¥–åŠ±æ¥æŒ‡å¯¼æ¨ç†è®¡åˆ’ï¼Œä»è€Œæé«˜äº†æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒThinkActåœ¨å°‘é‡æ ·æœ¬é€‚åº”ã€é•¿è¿œè§„åˆ’å’Œè‡ªæˆ‘ä¿®æ­£è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16812",
            "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning",
            "url": "https://huggingface.co/papers/2507.16812",
            "abstract": "MegaScience, a large-scale dataset of scientific reasoning questions, enhances the performance and training efficiency of AI models compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.",
            "score": 3,
            "issue_id": 4959,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ»Ñ",
                "en": "July 22",
                "zh": "7æœˆ22æ—¥"
            },
            "hash": "e6653e3f0a1b904f",
            "authors": [
                "Run-Ze Fan",
                "Zengzhi Wang",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University, SII, GAIR Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16812.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#data",
                    "#open_source",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MegaScience: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "MegaScience - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 1,25 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 7 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑƒÑ‡ĞµĞ±Ğ½Ğ¸ĞºĞ¾Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ñ‹ Ğ¸ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 15 ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµÑÑ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° MegaScience, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering AI with MegaScience for Superior Scientific Reasoning",
                    "desc": "This paper introduces MegaScience, a large-scale dataset designed to improve AI models' performance in scientific reasoning tasks. It addresses the lack of high-quality, open-source datasets in the scientific domain by providing 1.25 million instances of reasoning questions derived from university-level textbooks. The authors conducted systematic studies to select the best data subsets, ensuring that the dataset is both comprehensive and effective for training AI models. Their experiments show that models trained on MegaScience outperform existing models, highlighting its potential to enhance scientific reasoning capabilities in AI."
                },
                "zh": {
                    "title": "MegaScienceï¼šæ¨åŠ¨ç§‘å­¦æ¨ç†çš„æœªæ¥",
                    "desc": "MegaScienceæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ç§‘å­¦æ¨ç†é—®é¢˜æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»12000æœ¬å¤§å­¦çº§ç§‘å­¦æ•™ç§‘ä¹¦ä¸­æå–çš„çœŸå®å‚è€ƒç­”æ¡ˆï¼Œæ¶µç›–650000ä¸ªæ¨ç†é—®é¢˜ï¼Œæ¶‰åŠ7ä¸ªç§‘å­¦å­¦ç§‘ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬å¼€å‘äº†1.25ç™¾ä¸‡å®ä¾‹çš„é«˜è´¨é‡å¼€æ”¾æºæ•°æ®é›†ï¼Œå¹¶å»ºç«‹äº†å…¨é¢çš„è¯„ä¼°ç³»ç»Ÿï¼Œä»¥ç¡®ä¿å‡†ç¡®çš„è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMegaScienceåœ¨è®­ç»ƒæ•ˆç‡å’Œå“åº”é•¿åº¦ä¸Šä¼˜äºç°æœ‰çš„å¼€æ”¾æºç§‘å­¦æ•°æ®é›†ï¼Œç‰¹åˆ«é€‚åˆæ›´å¤§å’Œæ›´å¼ºçš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16746",
            "title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning",
            "url": "https://huggingface.co/papers/2507.16746",
            "abstract": "Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce Zebra-CoT, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT.",
            "score": 3,
            "issue_id": 4959,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ»Ñ",
                "en": "July 22",
                "zh": "7æœˆ22æ—¥"
            },
            "hash": "195867d3f8c130bf",
            "authors": [
                "Ang Li",
                "Charles Wang",
                "Kaiyu Yue",
                "Zikui Cai",
                "Ollie Liu",
                "Deqing Fu",
                "Peng Guo",
                "Wang Bill Zhu",
                "Vatsal Sharan",
                "Robin Jia",
                "Willie Neiswanger",
                "Furong Huang",
                "Tom Goldstein",
                "Micah Goldblum"
            ],
            "affiliations": [
                "Columbia University",
                "New York University",
                "University of Maryland",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16746.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#games",
                    "#optimization",
                    "#multimodal",
                    "#open_source",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ¦“",
                "ru": {
                    "title": "Zebra-CoT: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Zebra-CoT - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ°Ğ±Ğ¾Ñ€ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 182,384 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, 2D Ğ¸ 3D Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ³Ñ€Ñ‹. Ğ¤Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Anole-7B Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 12% Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸ Ğ´Ğ¾ 13% Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… VLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Zebra-CoT: Enhancing Visual Reasoning with a Rich Dataset",
                    "desc": "This paper presents Zebra-CoT, a large dataset designed to enhance multimodal models' ability to perform visual reasoning tasks. It addresses the challenges of poor performance in existing visual chain of thought (CoT) models and the scarcity of quality training data. The dataset includes 182,384 samples that combine text and images for various reasoning tasks, such as geometry and strategic games. Fine-tuning models like Anole-7B and Bagel-7B on this dataset significantly improves their accuracy and ability to generate coherent visual reasoning chains."
                },
                "zh": {
                    "title": "Zebra-CoTï¼šæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å…³é”®æ•°æ®é›†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºZebra-CoTçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«182,384ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨å¸®åŠ©å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œè§†è§‰æ¨ç†ã€‚è¯¥æ•°æ®é›†æä¾›äº†é€»è¾‘è¿è´¯çš„æ–‡æœ¬-å›¾åƒæ¨ç†é“¾ï¼Œé€‚ç”¨äºç§‘å­¦é—®é¢˜ã€2Då’Œ3Dæ¨ç†ä»»åŠ¡ä»¥åŠè§†è§‰é€»è¾‘é—®é¢˜ç­‰å¤šç§ä»»åŠ¡ã€‚é€šè¿‡å¯¹Anole-7Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡æé«˜äº†12%ï¼Œå¹¶åœ¨æ ‡å‡†VLMåŸºå‡†è¯„ä¼°ä¸­è·å¾—äº†é«˜è¾¾13%çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬å¼€æºäº†è¯¥æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒè§†è§‰æ¨ç†èƒ½åŠ›çš„å¼€å‘å’Œè¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16813",
            "title": "HOComp: Interaction-Aware Human-Object Composition",
            "url": "https://huggingface.co/papers/2507.16813",
            "abstract": "HOComp uses MLLMs and attention mechanisms to achieve seamless human-object interactions with consistent appearances in image compositing.  \t\t\t\t\tAI-generated summary \t\t\t\t While existing image-guided composition methods may help insert a foreground object onto a user-specified region of a background image, achieving natural blending inside the region with the rest of the image unchanged, we observe that these existing methods often struggle in synthesizing seamless interaction-aware compositions when the task involves human-object interactions. In this paper, we first propose HOComp, a novel approach for compositing a foreground object onto a human-centric background image, while ensuring harmonious interactions between the foreground object and the background person and their consistent appearances. Our approach includes two key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes MLLMs to identify the interaction region as well as the interaction type (e.g., holding and lefting) to provide coarse-to-fine constraints to the generated pose for the interaction while incorporating human pose landmarks to track action variations and enforcing fine-grained pose constraints; and (2) Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware attention modulation mechanism, a multi-view appearance loss, and a background consistency loss to ensure consistent shapes/textures of the foreground and faithful reproduction of the background human. We then propose the first dataset, named Interaction-aware Human-Object Composition (IHOC), for the task. Experimental results on our dataset show that HOComp effectively generates harmonious human-object interactions with consistent appearances, and outperforms relevant methods qualitatively and quantitatively.",
            "score": 1,
            "issue_id": 4959,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ»Ñ",
                "en": "July 22",
                "zh": "7æœˆ22æ—¥"
            },
            "hash": "5a513d80af038052",
            "authors": [
                "Dong Liang",
                "Jinyuan Jia",
                "Yuhao Liu",
                "Rynson W. H. Lau"
            ],
            "affiliations": [
                "CityUHK",
                "HKUST(GZ)",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16813.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#synthetic",
                    "#games",
                    "#dataset"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "HOComp - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ½Ğ° Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ° Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. HOComp Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IHOC Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Seamless Human-Object Interaction in Image Compositing",
                    "desc": "HOComp is a new method that enhances how foreground objects interact with people in images, ensuring they blend naturally into the scene. It uses Multi-Layered Language Models (MLLMs) to guide the placement and pose of objects based on the type of interaction, like holding or lifting. Additionally, it employs a technique called Detail-Consistent Appearance Preservation (DCAP) to maintain the visual consistency of shapes and textures between the foreground and background. The paper also introduces a new dataset, Interaction-aware Human-Object Composition (IHOC), to evaluate the effectiveness of HOComp, which shows significant improvements over existing methods."
                },
                "zh": {
                    "title": "æ— ç¼äººæœºäº’åŠ¨çš„å›¾åƒåˆæˆæ–°æ–¹æ³•",
                    "desc": "HOCompæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†å‰æ™¯ç‰©ä½“æ— ç¼åœ°åˆæˆåˆ°ä»¥äººä¸ºä¸­å¿ƒçš„èƒŒæ™¯å›¾åƒä¸­ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®ä¿å‰æ™¯ç‰©ä½“ä¸èƒŒæ™¯äººç‰©ä¹‹é—´çš„å’Œè°äº’åŠ¨åŠä¸€è‡´çš„å¤–è§‚ã€‚å…¶æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬åŸºäºåŒºåŸŸçš„å§¿æ€å¼•å¯¼å’Œç»†èŠ‚ä¸€è‡´çš„å¤–è§‚ä¿ç•™ï¼Œå‰è€…å¸®åŠ©è¯†åˆ«äº’åŠ¨åŒºåŸŸå’Œç±»å‹ï¼Œåè€…ç¡®ä¿å‰æ™¯å’ŒèƒŒæ™¯çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHOCompåœ¨ç”Ÿæˆè‡ªç„¶çš„äººç‰©ä¸ç‰©ä½“äº’åŠ¨æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç›¸å…³æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15245",
            "title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced\n  Academic Search",
            "url": "https://huggingface.co/papers/2507.15245",
            "abstract": "Recent advances in large language models (LLMs) have opened new opportunities for academic literature retrieval. However, existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR, a multi-agent framework that incorporates RefChain-based query decomposition and query evolution to enable more flexible and effective search. To facilitate systematic evaluation, we also construct SPARBench, a challenging benchmark with expert-annotated relevance labels. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. Together, SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval. Code and data will be available at: https://github.com/xiaofengShi/SPAR",
            "score": 0,
            "issue_id": 4959,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "a01817799751346b",
            "authors": [
                "Xiaofeng Shi",
                "Yuduo Li",
                "Qian Kou",
                "Longbin Yu",
                "Jinxin Xie",
                "Hua Zhou"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence (BAAI)",
                "Beijing Jiaotong University (BJTU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15245.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#survey",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "SPAR: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "SPAR - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RefChain Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SPARBench Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SPAR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ F1-Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ¾ 56% Ğ½Ğ° AutoScholar Ğ¸ 23% Ğ½Ğ° SPARBench."
                },
                "en": {
                    "title": "Revolutionizing Academic Search with SPAR",
                    "desc": "This paper presents SPAR, a novel multi-agent framework designed to enhance academic literature retrieval using large language models. SPAR utilizes RefChain-based query decomposition and evolution, allowing for more adaptable and effective search strategies compared to traditional rigid systems. The authors also introduce SPARBench, a benchmark with expert-annotated relevance labels to systematically evaluate retrieval performance. Experimental results show that SPAR significantly improves retrieval accuracy, outperforming existing methods by notable margins on both AutoScholar and SPARBench datasets."
                },
                "zh": {
                    "title": "SPARï¼šæå‡å­¦æœ¯æ£€ç´¢çš„çµæ´»æ€§ä¸æ•ˆæœ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSPARçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å­¦æœ¯æ–‡çŒ®æ£€ç´¢çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚SPARé€šè¿‡åŸºäºRefChainçš„æŸ¥è¯¢åˆ†è§£å’ŒæŸ¥è¯¢æ¼”å˜æŠ€æœ¯ï¼Œå…‹æœäº†ç°æœ‰ç³»ç»Ÿçš„å±€é™æ€§ã€‚ä¸ºäº†ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†SPARBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ä¸“å®¶æ ‡æ³¨ç›¸å…³æ€§æ ‡ç­¾çš„æŒ‘æˆ˜æ€§åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPARåœ¨AutoScholarå’ŒSPARBenchä¸Šåˆ†åˆ«æ¯”æœ€ä½³åŸºçº¿æé«˜äº†56%å’Œ23%çš„F1åˆ†æ•°ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-22.html",
    "link_next": "2025-07-24.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "22.07",
        "en": "07/22",
        "zh": "7æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.07",
        "en": "07/24",
        "zh": "7æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}