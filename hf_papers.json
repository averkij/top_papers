{
    "date": {
        "ru": "4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 4",
        "zh": "4æœˆ4æ—¥"
    },
    "time_utc": "2025-04-04 11:09",
    "weekday": 4,
    "issue_id": 3071,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.01990",
            "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
            "url": "https://huggingface.co/papers/2504.01990",
            "abstract": "The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.",
            "score": 59,
            "issue_id": 3069,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "f72a29b6411b97b1",
            "authors": [
                "Bang Liu",
                "Xinfeng Li",
                "Jiayi Zhang",
                "Jinlin Wang",
                "Tanjin He",
                "Sirui Hong",
                "Hongzhang Liu",
                "Shaokun Zhang",
                "Kaitao Song",
                "Kunlun Zhu",
                "Yuheng Cheng",
                "Suyuchen Wang",
                "Xiaoqiang Wang",
                "Yuyu Luo",
                "Haibo Jin",
                "Peiyan Zhang",
                "Ollie Liu",
                "Jiaqi Chen",
                "Huan Zhang",
                "Zhaoyang Yu",
                "Haochen Shi",
                "Boyan Li",
                "Dekun Wu",
                "Fengwei Teng",
                "Xiaojun Jia",
                "Jiawei Xu",
                "Jinyu Xiang",
                "Yizhang Lin",
                "Tianming Liu",
                "Tongliang Liu",
                "Yu Su",
                "Huan Sun",
                "Glen Berseth",
                "Jianyun Nie",
                "Ian Foster",
                "Logan Ward",
                "Qingyun Wu",
                "Yu Gu",
                "Mingchen Zhuge",
                "Xiangru Tang",
                "Haohan Wang",
                "Jiaxuan You",
                "Chi Wang",
                "Jian Pei",
                "Qiang Yang",
                "Xiaoliang Qi",
                "Chenglin Wu"
            ],
            "affiliations": [
                "Argonne National Laboratory",
                "Canada CIFAR AI Chair",
                "Duke University",
                "Google DeepMind",
                "King Abdullah University of Science and Technology",
                "MetaGPT",
                "Microsoft Research Asia",
                "Mila - Quebec AI Institute",
                "Nanyang Technological University",
                "Penn State University",
                "Stanford University",
                "The Hong Kong Polytechnic University",
                "The Hong Kong University of Science and Technology",
                "The Ohio State University",
                "University of Georgia",
                "University of Illinois at Urbana-Champaign",
                "University of Southern California",
                "University of Sydney",
                "UniversitÃ© de MontrÃ©al",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01990.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#survey",
                    "#security",
                    "#ethics",
                    "#agi",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ: Ğ¾Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ¼, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸, Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Building Intelligent Agents: From Brain-Inspired Design to Safe AI Deployment",
                    "desc": "This paper explores the development of large language models (LLMs) and their role in creating advanced intelligent agents that can reason, perceive, and act in various environments. It presents a modular architecture inspired by the human brain, detailing how cognitive, perceptual, and operational modules correspond to brain functions like memory and emotion. The paper also discusses how these agents can improve themselves through adaptive learning and optimization techniques, including AutoML. Finally, it emphasizes the importance of ensuring that AI systems are safe, ethical, and reliable for real-world applications."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä½“çš„æœªæ¥ï¼šä»å¤§è„‘å¯å‘åˆ°å®‰å…¨åº”ç”¨",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å˜é©æ€§å½±å“ï¼Œå¼ºè°ƒäº†æ™ºèƒ½ä½“çš„è®¾è®¡ã€è¯„ä¼°å’ŒæŒç»­æ”¹è¿›æ‰€é¢ä¸´çš„å¤æ‚æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†æ™ºèƒ½ä½“æ¡†æ¶ç½®äºæ¨¡å—åŒ–çš„ã€å—å¤§è„‘å¯å‘çš„æ¶æ„ä¸­ï¼Œç»“åˆäº†è®¤çŸ¥ç§‘å­¦ã€ç¥ç»ç§‘å­¦å’Œè®¡ç®—ç ”ç©¶çš„åŸåˆ™ã€‚æ–‡ç« åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼Œé¦–å…ˆåˆ†ææ™ºèƒ½ä½“çš„æ¨¡å—åŒ–åŸºç¡€ï¼Œæ˜ å°„å…¶è®¤çŸ¥ã€æ„ŸçŸ¥å’Œæ“ä½œæ¨¡å—ä¸äººç±»å¤§è„‘åŠŸèƒ½çš„ç›¸ä¼¼æ€§ã€‚æ¥ç€è®¨è®ºè‡ªæˆ‘å¢å¼ºå’Œé€‚åº”æ€§è¿›åŒ–æœºåˆ¶ï¼Œæœ€åå¼ºè°ƒæ„å»ºå®‰å…¨ã€å¯é å’Œæœ‰ç›Šçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02826",
            "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
            "url": "https://huggingface.co/papers/2504.02826",
            "abstract": "Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.",
            "score": 45,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "dbb1c07cd5a01838",
            "authors": [
                "Xiangyu Zhao",
                "Peiyuan Zhang",
                "Kexian Tang",
                "Hao Li",
                "Zicheng Zhang",
                "Guangtao Zhai",
                "Junchi Yan",
                "Hua Yang",
                "Xue Yang",
                "Haodong Duan"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02826.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RISEBench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "RISEBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼. Ğ¢ĞµÑÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ LLM-ÑÑƒĞ´ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "RISEBench: Advancing Reasoning in Visual Editing",
                    "desc": "This paper introduces RISEBench, a new benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) in Large Multi-modality Models (LMMs). It identifies challenges in visual editing, such as following complex instructions and maintaining appearance consistency. RISEBench categorizes reasoning into four types: Temporal, Causal, Spatial, and Logical, and provides a framework for assessing these reasoning types through both human and model evaluations. The findings indicate that even advanced models like GPT-4o-Native struggle with logical reasoning, suggesting a need for further research in this area."
                },
                "zh": {
                    "title": "æ¨ç†é©±åŠ¨çš„è§†è§‰ç¼–è¾‘æ–°åŸºå‡†",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éµå¾ªå¤æ‚æŒ‡ä»¤ã€ä¿æŒå¤–è§‚ä¸€è‡´æ€§å’Œæ”¯æŒçµæ´»è¾“å…¥æ ¼å¼æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RISEBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨ç†é©±åŠ¨è§†è§‰ç¼–è¾‘ï¼ˆRISEï¼‰çš„åŸºå‡†ã€‚RISEBenchä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ï¼šæ—¶é—´æ¨ç†ã€å› æœæ¨ç†ã€ç©ºé—´æ¨ç†å’Œé€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡GPT-4o-Nativeåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä½†åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ï¼Œæ˜¾ç¤ºå‡ºè¿™ä¸€é¢†åŸŸä»éœ€æ·±å…¥æ¢ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02782",
            "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
            "url": "https://huggingface.co/papers/2504.02782",
            "abstract": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.",
            "score": 28,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "5346697bd326eed4",
            "authors": [
                "Zhiyuan Yan",
                "Junyan Ye",
                "Weijia Li",
                "Zilong Huang",
                "Shenghai Yuan",
                "Xiangyang He",
                "Kaiqing Lin",
                "Jun He",
                "Conghui He",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Rabbitpre AI",
                "Shanghai AI Laboratory",
                "Shenzhen University",
                "Sun Yat-sen University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02782.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#architecture",
                    "#diffusion",
                    "#interpretability",
                    "#optimization",
                    "#hallucinations"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "GPT-4o: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº (GPT-ImgEval) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¾Ñ‚ OpenAI, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GPT-4o Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° GPT-4o Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ GPT-4o, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµĞµ Ñ Gemini 2.0 Flash Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unleashing the Power of GPT-4o in Image Generation and Editing",
                    "desc": "This paper evaluates the performance of OpenAI's GPT-4o model in image generation and editing using a new benchmark called GPT-ImgEval. The evaluation focuses on three key areas: the quality of generated images, the model's ability to edit images, and its understanding of semantic context. Results show that GPT-4o outperforms existing models in both image generation and editing, while also demonstrating strong reasoning capabilities. The paper also explores the model's architecture and limitations, providing insights for future research in image generation."
                },
                "zh": {
                    "title": "GPT-4oï¼šå›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†OpenAIçš„GPT-4oæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„æœ€æ–°çªç ´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºGPT-ImgEvalçš„è¯„ä¼°åŸºå‡†ï¼Œå®šé‡å’Œå®šæ€§åœ°åˆ†æäº†GPT-4oåœ¨ç”Ÿæˆè´¨é‡ã€ç¼–è¾‘èƒ½åŠ›å’ŒçŸ¥è¯†æ¨ç†ç­‰ä¸‰ä¸ªå…³é”®ç»´åº¦çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGPT-4oåœ¨å›¾åƒç”Ÿæˆæ§åˆ¶å’Œè¾“å‡ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å“è¶Šçš„çŸ¥è¯†æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†GPT-4oçš„æ¶æ„ï¼Œå¹¶è¯†åˆ«äº†å…¶åœ¨å›¾åƒç”Ÿæˆä¸­å¸¸è§çš„åˆæˆä¼ªå½±å’Œå±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02587",
            "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
            "url": "https://huggingface.co/papers/2504.02587",
            "abstract": "Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.",
            "score": 18,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "58300c3a6e30995f",
            "authors": [
                "Yan Ma",
                "Steffi Chern",
                "Xuyang Shen",
                "Yiran Zhong",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "Generative Artificial Intelligence Lab (GAIR)",
                "Minimax",
                "SII",
                "Shanghai Jiao Tong University (SJTU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02587.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ RL Ğ² VLM, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Vision-Language Models!",
                    "desc": "This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks."
                },
                "zh": {
                    "title": "å»ºç«‹å¯é‡å¤çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå¹¶æ­£åœ¨ç§¯ææ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLåº”ç”¨å¾€å¾€ä¾èµ–äºå¤æ‚çš„æ¡†æ¶ï¼Œé™åˆ¶äº†å¯é‡å¤æ€§å’Œå¯è®¿é—®æ€§ï¼ŒåŒæ—¶ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œä½¿å¾—ç»“æœæ¯”è¾ƒå’Œè®­ç»ƒåŠ¨æ€è§£é‡Šå˜å¾—å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€æ˜çš„ã€ä»é›¶å¼€å§‹çš„RLæ¡†æ¶ï¼Œæä¾›äº†ä¸€ä¸ªç»è¿‡å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†éªŒè¯çš„æœ€å°åŠŸèƒ½å››æ­¥æµç¨‹ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥è¯„ä¼°è®­ç»ƒåŠ¨æ€å’Œåæ€è¡Œä¸ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02436",
            "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2504.02436",
            "abstract": "This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.",
            "score": 14,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "86b46513a72dbd76",
            "authors": [
                "Zhengcong Fei",
                "Debang Li",
                "Di Qiu",
                "Jiahua Wang",
                "Yikun Dou",
                "Rui Wang",
                "Jingtao Xu",
                "Mingyuan Fan",
                "Guibin Chen",
                "Yang Li",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02436.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#video",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "SkyReels-A2 - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. SkyReels-A2 ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (E2V)."
                },
                "en": {
                    "title": "SkyReels-A2: Mastering Video Generation with Element Control",
                    "desc": "This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation."
                },
                "zh": {
                    "title": "SkyReels-A2ï¼šå¯æ§è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†SkyReels-A2ï¼Œä¸€ä¸ªå¯æ§çš„è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºå°†ä»»æ„è§†è§‰å…ƒç´ ï¼ˆå¦‚è§’è‰²ã€ç‰©ä½“ã€èƒŒæ™¯ï¼‰ç»„åˆæˆåˆæˆè§†é¢‘ï¼ŒåŒæ—¶ä¿æŒä¸æ¯ä¸ªå…ƒç´ çš„å‚è€ƒå›¾åƒçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†è¿™ä¸€ä»»åŠ¡ç§°ä¸ºå…ƒç´ åˆ°è§†é¢‘ï¼ˆE2Vï¼‰ï¼Œå…¶ä¸»è¦æŒ‘æˆ˜åœ¨äºä¿æŒæ¯ä¸ªå‚è€ƒå…ƒç´ çš„çœŸå®æ€§ï¼Œç¡®ä¿åœºæ™¯çš„è¿è´¯æ€§ï¼Œä»¥åŠå®ç°è‡ªç„¶çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®ç®¡é“ï¼Œä»¥æ„å»ºæç¤º-å‚è€ƒ-è§†é¢‘ä¸‰å…ƒç»„ç”¨äºæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è§†é¢‘ï¼Œå¹¶å®ç°ç²¾ç¡®çš„å…ƒç´ æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02398",
            "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
            "url": "https://huggingface.co/papers/2504.02398",
            "abstract": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.",
            "score": 11,
            "issue_id": 3067,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "c03d1b64b7e6e276",
            "authors": [
                "Gallil Maimon",
                "Michael Hassid",
                "Amit Roth",
                "Yossi Adi"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02398.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#audio",
                    "#open_source",
                    "#synthetic",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (SLM), Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾, Ñ‡ĞµĞ¼ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ»ĞµĞ¹Ğ²Ğ½Ñ‹Ñ… SLM, Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµÑÑÑ‚ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ² Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ñ‚Ğ°ĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ğ½Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Interleaved SLMs: Efficient Scaling for Speech Models!",
                    "desc": "This paper investigates the efficiency of interleaved Speech Language Models (SLMs) compared to traditional textless SLMs. It finds that interleaved SLMs, which leverage pre-trained Text Language Models (TextLMs), require less compute and data while achieving competitive performance on speech tasks. The authors conduct a scaling analysis that reveals distinct scaling dynamics, suggesting a need for more compute allocation towards model size rather than training data. Additionally, the study highlights the importance of synthetic data and various TextLM families in enhancing the performance of SLMs."
                },
                "zh": {
                    "title": "äº¤é”™SLMï¼šæ›´é«˜æ•ˆçš„æ‰©å±•ä¹‹è·¯",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†äº¤é”™è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ‰©å±•æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œäº¤é”™SLMåœ¨è®¡ç®—èµ„æºçš„ä½¿ç”¨ä¸Šæ¯”æ— æ–‡æœ¬SLMæ›´ä¸ºé«˜æ•ˆã€‚æˆ‘ä»¬å‘ç°ï¼Œäº¤é”™SLMçš„æ‰©å±•åŠ¨æ€ä¸æ— æ–‡æœ¬SLMæ˜¾è‘—ä¸åŒï¼Œå»ºè®®åœ¨å¢åŠ æ¨¡å‹è§„æ¨¡æ—¶åº”æ›´å¤šåœ°åˆ†é…è®¡ç®—é¢„ç®—ã€‚æœ€ç»ˆï¼Œç»è¿‡æ‰©å±•çš„æ¨¡å‹åœ¨è¯­éŸ³è¯­ä¹‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºä¸é¢†å…ˆæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨çš„è®¡ç®—å’Œæ•°æ®é‡æ›´å°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00502",
            "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
            "url": "https://huggingface.co/papers/2504.00502",
            "abstract": "Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV",
            "score": 10,
            "issue_id": 3067,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "1b236225c1d92fd7",
            "authors": [
                "Qianhao Yuan",
                "Qingyu Zhang",
                "Yanjiang Liu",
                "Jiawei Chen",
                "Yaojie Lu",
                "Hongyu Lin",
                "Jia Zheng",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00502.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ MLLM: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ° Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ÑĞ»Ğ¾ĞµĞ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Layer Contribution (LC). ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ MLLM Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ShortV, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ½Ğ¸Ñ…. ShortV Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 60% ÑĞ»Ğ¾ĞµĞ² MLLM, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Optimizing MLLMs: Freeze the Unnecessary Layers!",
                    "desc": "This paper addresses the high computational costs associated with Multimodal Large Language Models (MLLMs) by analyzing layer-wise redundancy. It introduces a new metric called Layer Contribution (LC) to measure how much each layer affects the processing of visual and text tokens. The findings indicate that many layers contribute little to the processing of visual tokens, allowing for optimization. The authors propose a method called ShortV, which identifies and freezes these ineffective layers, resulting in significant reductions in computational costs while preserving model performance."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹ï¼Œé™ä½è®¡ç®—æˆæœ¬",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºå…¶åºå¤§çš„è§„æ¨¡å’Œå¤§é‡çš„è§†è§‰æ ‡è®°ï¼Œé¢ä¸´ç€é«˜è®¡ç®—æˆæœ¬çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åº¦é‡æ ‡å‡†â€”â€”å±‚è´¡çŒ®ï¼ˆLayer Contributionï¼ŒLCï¼‰ï¼Œç”¨äºé‡åŒ–æ¨¡å‹ä¸­å„å±‚å¯¹è§†è§‰å’Œæ–‡æœ¬æ ‡è®°çš„å½±å“ã€‚é€šè¿‡è®¡ç®—å»é™¤æŸå±‚å˜æ¢åæ¨¡å‹è¾“å‡ºçš„å·®å¼‚ï¼ŒLCèƒ½å¤Ÿè¯„ä¼°è¯¥å±‚çš„è´¡çŒ®ã€‚å®éªŒè¡¨æ˜ï¼Œè®¸å¤šå±‚åœ¨å¤„ç†è§†è§‰æ ‡è®°æ—¶çš„è´¡çŒ®å¾ˆå°ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ShortVæ–¹æ³•ï¼Œèƒ½å¤Ÿè¯†åˆ«å¹¶å†»ç»“è¿™äº›æ— æ•ˆå±‚ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02542",
            "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
            "url": "https://huggingface.co/papers/2504.02542",
            "abstract": "Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.",
            "score": 8,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "fa93ea3aeacd0dbc",
            "authors": [
                "Fa-Ting Hong",
                "Zunnan Xu",
                "Zixiang Zhou",
                "Jun Zhou",
                "Xiu Li",
                "Qin Lin",
                "Qinglin Lu",
                "Dan Xu"
            ],
            "affiliations": [
                "HKUST",
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02542.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "ACTalker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ mamba Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²ĞµÑ‚Ğ²ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ACTalker Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "ACTalker: Multi-Signal Control for Natural Talking Head Synthesis",
                    "desc": "This paper presents ACTalker, a novel framework for generating talking head videos that can be controlled by multiple signals simultaneously. Unlike traditional methods that rely on a single control modality, ACTalker employs a parallel mamba structure with multiple branches, each dedicated to a specific facial region. A gate mechanism allows for flexible control, ensuring that different driving signals can manipulate facial features without interference. The introduction of a mask-drop strategy further enhances this capability, enabling independent control of facial regions and resulting in more natural and coordinated video outputs."
                },
                "zh": {
                    "title": "å¤šä¿¡å·æ§åˆ¶çš„å¯¹è¯å¤´åƒç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºACTalkerçš„ç«¯åˆ°ç«¯è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆè™šæ‹Ÿå¤´åƒçš„å¯¹è¯è§†é¢‘ã€‚è¯¥æ–¹æ³•æ”¯æŒå¤šä¿¡å·å’Œå•ä¿¡å·æ§åˆ¶ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡è®¾è®¡å¹¶è¡Œçš„mambaç»“æ„ï¼Œå…è®¸ä¸åŒçš„é©±åŠ¨ä¿¡å·æ§åˆ¶é¢éƒ¨çš„ç‰¹å®šåŒºåŸŸï¼Œå¹¶ä½¿ç”¨é—¨æ§æœºåˆ¶å®ç°çµæ´»æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACTalkerèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶çš„é¢éƒ¨è§†é¢‘ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ— å†²çªåœ°æ•´åˆå¤šç§é©±åŠ¨ä¿¡å·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02507",
            "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
            "url": "https://huggingface.co/papers/2504.02507",
            "abstract": "Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.",
            "score": 7,
            "issue_id": 3065,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "290019150fe5b4c9",
            "authors": [
                "Abhay Kumar",
                "Louis Owen",
                "Nilabhra Roy Chowdhury",
                "Fabian GÃ¼ra"
            ],
            "affiliations": [
                "BluOrion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02507.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ZClip: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ZClip Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ZClip Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ¾Ñ€Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ z-Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¾Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ZClip Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğµ Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ½Ğµ Ğ¼ĞµÑˆĞ°Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…."
                },
                "en": {
                    "title": "ZClip: Smart Gradient Clipping for Stable LLM Training",
                    "desc": "This paper addresses the challenges of training large language models (LLMs) by introducing ZClip, an adaptive gradient clipping algorithm. Traditional methods often fail to manage gradient instability and loss spikes effectively, leading to inefficient training and the need for manual adjustments. ZClip improves upon these methods by dynamically adjusting the clipping threshold based on the statistical behavior of gradient norms, allowing for a more responsive training process. By using z-score-based anomaly detection, ZClip prevents harmful loss spikes while maintaining the overall convergence of the model."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”æ¢¯åº¦è£å‰ªï¼Œæå‡è®­ç»ƒç¨³å®šæ€§",
                    "desc": "åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¸¸å¸¸ä¼šé‡åˆ°æ¢¯åº¦ä¸ç¨³å®šå’ŒæŸå¤±å³°å€¼ç­‰é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´ç¾éš¾æ€§çš„å‘æ•£ã€‚ä¼ ç»Ÿçš„æ¢¯åº¦è£å‰ªæŠ€æœ¯æ— æ³•æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºå›ºå®šçš„é˜ˆå€¼æˆ–å¯å‘å¼æ–¹æ³•ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºZClipçš„è‡ªé€‚åº”æ¢¯åº¦è£å‰ªç®—æ³•ï¼Œå®ƒæ ¹æ®æ¢¯åº¦èŒƒæ•°çš„ç»Ÿè®¡ç‰¹æ€§åŠ¨æ€è°ƒæ•´è£å‰ªé˜ˆå€¼ã€‚ZClipé€šè¿‡åŸºäºz-scoreçš„å¼‚å¸¸æ£€æµ‹æ¥è¯†åˆ«å’Œå‡è½»å¤§æ¢¯åº¦å³°å€¼ï¼Œä»è€Œé˜²æ­¢æŸå¤±å³°å€¼ï¼ŒåŒæ—¶ä¸å¹²æ‰°æ”¶æ•›è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02012",
            "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
            "url": "https://huggingface.co/papers/2504.02012",
            "abstract": "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.",
            "score": 5,
            "issue_id": 3065,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "cbdd586ccd2b682d",
            "authors": [
                "Soro Bedionita",
                "Bruno Andreis",
                "Song Chong",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai, South Korea",
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02012.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#cv",
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "IGPG: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€",
                    "desc": "IGPG - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° VQ-VAE Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ÑĞµÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸. IGPG Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸Ğ· ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IGPG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "IGPG: Unifying Neural Network Parameter Generation for Enhanced Adaptability",
                    "desc": "This paper introduces IGPG (Instruction Guided Parameter Generation), a new framework for generating neural network parameters based on task descriptions and architecture specifications. Unlike previous methods, IGPG uses an autoregressive approach that ensures coherence between layers and adapts efficiently to different models and datasets. By employing a VQ-VAE and generating weights at the token level, IGPG captures complex parameter distributions from various pretrained models. The results show that IGPG outperforms existing methods in scalability and efficiency, making it a valuable tool for model adaptation and fine-tuning."
                },
                "zh": {
                    "title": "IGPGï¼šçµæ´»çš„ç¥ç»ç½‘ç»œå‚æ•°ç”Ÿæˆå·¥å…·",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºIGPGï¼ˆæŒ‡ä»¤å¼•å¯¼å‚æ•°ç”Ÿæˆï¼‰çš„è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆç¥ç»ç½‘ç»œå‚æ•°ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡æè¿°å’Œæ¶æ„è§„èŒƒã€‚IGPGé€šè¿‡ç»“åˆVQ-VAEå’Œè‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡å’Œæ¶æ„ä¸­ç»Ÿä¸€å‚æ•°åˆæˆï¼Œç¡®ä¿å±‚é—´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆç¥ç»ç½‘ç»œæƒé‡æ—¶ï¼Œé‡‡ç”¨äº†åŸºäºtokençš„ç”Ÿæˆæ–¹å¼ï¼Œæœ‰æ•ˆæ•æ‰æ¥è‡ªå¤šç§é¢„è®­ç»ƒæ¨¡å‹çš„å¤æ‚å‚æ•°åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIGPGåœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡æ¶æ„çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡æ–¹é¢ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02119",
            "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
            "url": "https://huggingface.co/papers/2504.02119",
            "abstract": "Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.",
            "score": 4,
            "issue_id": 3063,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "7c31e20ce0a7813b",
            "authors": [
                "Wang Wei",
                "Tiankai Yang",
                "Hongjie Chen",
                "Ryan A. Rossi",
                "Yue Zhao",
                "Franck Dernoncourt",
                "Hoda Eldardiry"
            ],
            "affiliations": [
                "Adobe Research San Jose, CA, USA",
                "Adobe Research Seattle, WA, USA",
                "Department of Computer Science University of South California Los Angeles, CA, USA",
                "Department of Computer Science Virginia Tech Blacksburg, VA, USA",
                "Dolby Labs Atlanta, GA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02119.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ LLaMA, GPT Ğ¸ Gemini Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» LLM Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Model Selection with Large Language Models",
                    "desc": "This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ—¶é—´åºåˆ—é¢„æµ‹çš„æ¨¡å‹é€‰æ‹©",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ¨¡å‹é€‰æ‹©é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›çš„æ€§èƒ½è¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè½»é‡çº§æ›¿ä»£æ–¹æ¡ˆçš„æ–¹æ³•ï¼Œé¿å…äº†æ„å»ºæ˜‚è´µçš„æ€§èƒ½çŸ©é˜µã€‚é€šè¿‡ä¸LLaMAã€GPTå’ŒGeminiçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„å…ƒå­¦ä¹ æŠ€æœ¯å’Œå¯å‘å¼åŸºçº¿ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†LLMsåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­é«˜æ•ˆæ¨¡å‹é€‰æ‹©çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00891",
            "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
            "url": "https://huggingface.co/papers/2504.00891",
            "abstract": "Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.",
            "score": 4,
            "issue_id": 3066,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "b22a54f43f9d7a89",
            "authors": [
                "Jian Zhao",
                "Runze Liu",
                "Kaiyan Zhang",
                "Zhimu Zhou",
                "Junqi Gao",
                "Dong Li",
                "Jiafei Lyu",
                "Zhouyi Qian",
                "Biqing Qi",
                "Xiu Li",
                "Bowen Zhou"
            ],
            "affiliations": [
                "BUPT",
                "Harbin Institute of Technology",
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00891.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#math",
                    "#reasoning",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GenPRM: ĞĞ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenPRM - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GenPRM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PRM Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 23 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "GenPRM: Elevating LLMs with Generative Process Reward Models",
                    "desc": "This paper presents GenPRM, a generative process reward model designed to improve the performance of large language models (LLMs) by addressing key challenges faced by existing process reward models (PRMs). GenPRM utilizes Chain-of-Thought (CoT) reasoning and incorporates code verification to enhance the quality of its judgments at each reasoning step. The authors introduce a novel method called Relative Progress Estimation (RPE) to generate high-quality supervision labels and rationale data, leading to significant performance improvements on various reasoning tasks. Experimental results demonstrate that GenPRM outperforms previous PRMs and shows strong capabilities as a critic model for refining policy models, establishing a new approach for process supervision in LLMs."
                },
                "zh": {
                    "title": "ç”Ÿæˆæ€§è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼šæå‡LLMsçš„æ–°èŒƒå¼",
                    "desc": "æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºéªŒè¯å™¨å¯ä»¥æå‡LLMsçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„PRMsé¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæœ‰é™çš„è¿‡ç¨‹ç›‘ç£å’Œæ³›åŒ–èƒ½åŠ›ã€ä¾èµ–äºæ ‡é‡å€¼é¢„æµ‹è€Œæœªåˆ©ç”¨LLMsçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥åŠæ— æ³•æ‰©å±•PRMsçš„æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†GenPRMï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆæ€§è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡ä»£ç éªŒè¯è¿›è¡Œæ˜ç¡®çš„æ€ç»´é“¾æ¨ç†ï¼Œç„¶åå¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œåˆ¤æ–­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenPRMåœ¨å¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„PRMsï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºæ”¿ç­–æ¨¡å‹ç²¾ç‚¼çš„æ‰¹è¯„æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22444",
            "title": "Scaling Laws in Scientific Discovery with AI and Robot Scientists",
            "url": "https://huggingface.co/papers/2503.22444",
            "abstract": "Scientific discovery is poised for rapid advancement through advanced robotics and artificial intelligence. Current scientific practices face substantial limitations as manual experimentation remains time-consuming and resource-intensive, while multidisciplinary research demands knowledge integration beyond individual researchers' expertise boundaries. Here, we envision an autonomous generalist scientist (AGS) concept combines agentic AI and embodied robotics to automate the entire research lifecycle. This system could dynamically interact with both physical and virtual environments while facilitating the integration of knowledge across diverse scientific disciplines. By deploying these technologies throughout every research stage -- spanning literature review, hypothesis generation, experimentation, and manuscript writing -- and incorporating internal reflection alongside external feedback, this system aims to significantly reduce the time and resources needed for scientific discovery. Building on the evolution from virtual AI scientists to versatile generalist AI-based robot scientists, AGS promises groundbreaking potential. As these autonomous systems become increasingly integrated into the research process, we hypothesize that scientific discovery might adhere to new scaling laws, potentially shaped by the number and capabilities of these autonomous systems, offering novel perspectives on how knowledge is generated and evolves. The adaptability of embodied robots to extreme environments, paired with the flywheel effect of accumulating scientific knowledge, holds the promise of continually pushing beyond both physical and intellectual frontiers.",
            "score": 4,
            "issue_id": 3069,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 28",
                "zh": "3æœˆ28æ—¥"
            },
            "hash": "c2d75e49d08273c1",
            "authors": [
                "Pengsong Zhang",
                "Heng Zhang",
                "Huazhe Xu",
                "Renjun Xu",
                "Zhenting Wang",
                "Cong Wang",
                "Animesh Garg",
                "Zhibin Li",
                "Arash Ajoudani",
                "Xinyu Liu"
            ],
            "affiliations": [
                "Georgia Tech",
                "Harvard University",
                "Istituto Italiano di Tecnologia",
                "Rutgers University",
                "Tsinghua University",
                "Universita di Genova",
                "University College of London",
                "University of Toronto",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22444.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#robotics",
                    "#agi",
                    "#science"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ ÑƒÑ‡Ñ‘Ğ½Ñ‹Ğ¹-ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ñ‘Ğ½Ğ¾Ğ³Ğ¾-ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»Ğ° (AGS), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° AGS Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½. ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ AGS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸ĞµĞ¼ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½ÑÑ‚ÑŒÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğ¼ Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Revolutionizing Science with Autonomous Generalist Scientists",
                    "desc": "This paper introduces the concept of an Autonomous Generalist Scientist (AGS) that combines artificial intelligence and robotics to automate the entire scientific research process. The AGS can interact with both physical and virtual environments, facilitating knowledge integration across various scientific fields. By automating tasks such as literature review, hypothesis generation, experimentation, and manuscript writing, the AGS aims to significantly reduce the time and resources required for scientific discovery. The authors suggest that as these systems become more integrated into research, they could change how knowledge is generated and evolve, potentially leading to new scaling laws in scientific discovery."
                },
                "zh": {
                    "title": "è‡ªä¸»ç§‘å­¦å®¶ï¼šåŠ é€Ÿç§‘å­¦å‘ç°çš„æœªæ¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªä¸»é€šç”¨ç§‘å­¦å®¶ï¼ˆAGSï¼‰çš„æ¦‚å¿µï¼Œç»“åˆäº†æ™ºèƒ½ä»£ç†AIå’Œå…·èº«æœºå™¨äººï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–æ•´ä¸ªç ”ç©¶ç”Ÿå‘½å‘¨æœŸã€‚è¯¥ç³»ç»Ÿèƒ½å¤ŸåŠ¨æ€ä¸ç‰©ç†å’Œè™šæ‹Ÿç¯å¢ƒäº’åŠ¨ï¼Œå¹¶ä¿ƒè¿›ä¸åŒç§‘å­¦å­¦ç§‘ä¹‹é—´çš„çŸ¥è¯†æ•´åˆã€‚é€šè¿‡åœ¨æ–‡çŒ®å›é¡¾ã€å‡è®¾ç”Ÿæˆã€å®éªŒå’Œè®ºæ–‡å†™ä½œç­‰ç ”ç©¶é˜¶æ®µåº”ç”¨è¿™äº›æŠ€æœ¯ï¼ŒAGSå¸Œæœ›æ˜¾è‘—å‡å°‘ç§‘å­¦å‘ç°æ‰€éœ€çš„æ—¶é—´å’Œèµ„æºã€‚éšç€è¿™äº›è‡ªä¸»ç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°èå…¥ç ”ç©¶è¿‡ç¨‹ï¼Œç§‘å­¦å‘ç°å¯èƒ½ä¼šéµå¾ªæ–°çš„è§„æ¨¡æ³•åˆ™ï¼Œæä¾›å…³äºçŸ¥è¯†ç”Ÿæˆå’Œæ¼”å˜çš„æ–°è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01871",
            "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.01871",
            "abstract": "We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL",
            "score": 3,
            "issue_id": 3069,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "bf12d8cbe28bb942",
            "authors": [
                "Thomas Bush",
                "Stephen Chung",
                "Usman Anwar",
                "AdriÃ  Garriga-Alonso",
                "David Krueger"
            ],
            "affiliations": [
                "FAR AI",
                "Mila, University of Montreal",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01871.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#games",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¸Ğ³Ñ€Ğµ Sokoban. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ DRC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ° Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Unveiling Planning in Model-Free Reinforcement Learning Agents",
                    "desc": "This paper provides evidence that model-free reinforcement learning agents can learn to plan by using concept-based interpretability. The authors focus on a model-free agent called DRC, which learns to create internal plans that predict the outcomes of its actions in the Sokoban environment. They explore how the agent identifies relevant concepts, forms plans, and how these plans affect its behavior through interventions. The study reveals that as the agent develops planning capabilities, it also shows improved performance with additional computational resources, resembling advanced search algorithms."
                },
                "zh": {
                    "title": "æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„è§„åˆ’èƒ½åŠ›",
                    "desc": "æœ¬æ–‡é¦–æ¬¡æä¾›äº†æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä»£ç†èƒ½å¤Ÿå­¦ä¹ è§„åˆ’çš„æœºåˆ¶æ€§è¯æ®ã€‚æˆ‘ä»¬é€šè¿‡åœ¨Sokobanè¿™ä¸€å¸¸ç”¨åŸºå‡†ä¸Šåº”ç”¨åŸºäºæ¦‚å¿µçš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œå±•ç¤ºäº†DRCä»£ç†å¦‚ä½•åˆ©ç”¨å­¦ä¹ åˆ°çš„æ¦‚å¿µè¡¨ç¤ºæ¥å†…éƒ¨åˆ¶å®šè®¡åˆ’ã€‚å…·ä½“è€Œè¨€ï¼Œä»£ç†èƒ½å¤Ÿé¢„æµ‹è¡ŒåŠ¨å¯¹ç¯å¢ƒçš„é•¿æœŸå½±å“ï¼Œå¹¶å½±å“è¡ŒåŠ¨é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»£ç†çš„è®¡åˆ’å½¢æˆä¸å…¶è¡Œä¸ºä¹‹é—´å­˜åœ¨å› æœå…³ç³»ï¼Œå¹¶ä¸”è¿™ç§è®¡åˆ’çš„å‡ºç°ä¸ä»£ç†åœ¨æµ‹è¯•æ—¶åˆ©ç”¨é¢å¤–è®¡ç®—èƒ½åŠ›çš„èƒ½åŠ›ç›¸å»åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02821",
            "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
            "url": "https://huggingface.co/papers/2504.02821",
            "abstract": "Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.",
            "score": 2,
            "issue_id": 3069,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "d4568f020b5f2eca",
            "authors": [
                "Mateusz Pach",
                "Shyamgopal Karthik",
                "Quentin Bouniot",
                "Serge Belongie",
                "Zeynep Akata"
            ],
            "affiliations": [
                "Helmholtz Munich",
                "Munich Center of Machine Learning",
                "Munich Data Science Institute",
                "Technical University of Munich",
                "University of Copenhagen",
                "University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02821.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SAE) Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº (VLM). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SAE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² VLM Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ·Ñ€ĞµĞ½Ğ¸Ñ CLIP Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SAE. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ SAE Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ VLM."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Sparse Autoencoders",
                    "desc": "This paper explores the use of Sparse Autoencoders (SAEs) to improve the interpretability and steerability of Vision-Language Models (VLMs) like CLIP. The authors present a framework to assess how well these models represent single concepts, known as monosemanticity. Their experiments show that SAEs can enhance the clarity of individual neurons in VLMs and align these representations with established expert categories. Importantly, they demonstrate that SAEs can influence the output of multimodal language models without changing the original model architecture."
                },
                "zh": {
                    "title": "ç¨€ç–è‡ªç¼–ç å™¨æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸æ“æ§æ€§",
                    "desc": "ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰æœ€è¿‘è¢«è¯æ˜å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ã€‚æœ¬æ–‡å°†SAEsçš„åº”ç”¨æ‰©å±•åˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥è¯„ä¼°è§†è§‰è¡¨ç¤ºçš„å•ä¹‰æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨VLMsä¸Šè®­ç»ƒçš„SAEsæ˜¾è‘—å¢å¼ºäº†å•ä¸ªç¥ç»å…ƒçš„å•ä¹‰æ€§ï¼Œå¹¶å±•ç¤ºäº†ä¸ä¸“å®¶å®šä¹‰ç»“æ„ï¼ˆå¦‚iNaturaliståˆ†ç±»æ³•ï¼‰è‰¯å¥½å¯¹é½çš„å±‚æ¬¡è¡¨ç¤ºã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†å°†SAEsåº”ç”¨äºCLIPè§†è§‰ç¼–ç å™¨ï¼Œå¯ä»¥ç›´æ¥æ“æ§å¤šæ¨¡æ€LLMsï¼ˆå¦‚LLaVAï¼‰çš„è¾“å‡ºï¼Œè€Œæ— éœ€å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02495",
            "title": "Inference-Time Scaling for Generalist Reward Modeling",
            "url": "https://huggingface.co/papers/2504.02495",
            "abstract": "Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.",
            "score": 2,
            "issue_id": 3071,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "07408fa4b72ccb6c",
            "authors": [
                "Zijun Liu",
                "Peiyi Wang",
                "Runxin Xu",
                "Shirong Ma",
                "Chong Ruan",
                "Peng Li",
                "Yang Liu",
                "Yu Wu"
            ],
            "affiliations": [
                "DeepSeek-AI",
                "Dept. of Computer Sci. & Tech., Tsinghua University",
                "Institute for AI Industry Research (AIR), Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02495.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RM) Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Principled Critique Tuning (SPCT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ°-RM Ğ´Ğ»Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SPCT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (GRM), Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… RM."
                },
                "en": {
                    "title": "Enhancing Language Models with Scalable Reward Learning",
                    "desc": "This paper explores advancements in reinforcement learning (RL) for enhancing large language models (LLMs) by focusing on reward modeling (RM) for general queries. The authors introduce a novel approach called Self-Principled Critique Tuning (SPCT) that improves the generation of reward signals, enabling better inference-time scalability. They also propose a pointwise generative reward modeling (GRM) technique that allows flexibility in handling different input types. Empirical results demonstrate that SPCT significantly enhances the quality and scalability of GRMs, outperforming existing methods while addressing challenges in generalist reward systems."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åè®­ç»ƒä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ”¹è¿›å¥–åŠ±å»ºæ¨¡ï¼ˆRMï¼‰æ¥æé«˜ä¸€èˆ¬æŸ¥è¯¢çš„æ¨ç†æ—¶é—´å¯æ‰©å±•æ€§ï¼Œå¹¶æå‡ºäº†è‡ªæˆ‘åŸåˆ™æ‰¹è¯„è°ƒä¼˜ï¼ˆSPCTï¼‰æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›GRMä¸­çš„å¯æ‰©å±•å¥–åŠ±ç”Ÿæˆè¡Œä¸ºã€‚æˆ‘ä»¬é‡‡ç”¨ç‚¹å¯¹ç‚¹ç”Ÿæˆå¥–åŠ±å»ºæ¨¡ï¼ˆGRMï¼‰ï¼Œä»¥é€‚åº”ä¸åŒè¾“å…¥ç±»å‹å¹¶å®ç°æ¨ç†æ—¶é—´çš„å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPCTæ˜¾è‘—æé«˜äº†GRMçš„è´¨é‡å’Œå¯æ‰©å±•æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•å’Œæ¨¡å‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-03.html",
    "link_next": "2025-04-07.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "03.04",
        "en": "04/03",
        "zh": "4æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "07.04",
        "en": "04/07",
        "zh": "4æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 3,
        "#cv": 4,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºRISEBenchï¼Œç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç¼–è¾‘ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚RISEBenchä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ï¼šæ—¶é—´ã€å› æœã€ç©ºé—´å’Œé€»è¾‘æ¨ç†ã€‚é€šè¿‡é«˜è´¨é‡çš„æµ‹è¯•æ¡ˆä¾‹å’Œè¯„ä¼°æ¡†æ¶ï¼ŒRISEBenchèƒ½å¤Ÿè¯„ä¼°æŒ‡ä»¤æ¨ç†ã€å¤–è§‚ä¸€è‡´æ€§å’Œè§†è§‰åˆç†æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡GPT-4o-Nativeåœ¨æŸäº›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä½œè€…å¸Œæœ›RISEBenchèƒ½ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›åŸºç¡€æ€§çš„è§è§£ï¼Œå¹¶æ‰¿è¯ºä¸æ–­æ‰©å±•å’Œå®Œå–„è¿™ä¸€åŸºå‡†æµ‹è¯•ã€‚",
        "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
        "pinyin": "ZhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de jÄ« zhÇ”n cÃ¨ shÃ¬, chÄ“ng wÃ©i RISEBench, yÃ²ng yÃº pÃ­ng guÌ„ dÃ  xÃ­ng duÅ mÃ³ shÃ¹ mÃ³ xÃ­ng (LMMs) zÃ i shÃ¬ juÃ© biÄn jÃ­ zhÅng de tuÄ« lÇ nÃ©ng lÃ¬. RISEBench zhuÄn zhÃº yÃº sÃ¬ zhÇ’ng guÇn jiÃ n tuÄ« lÇ lÃ¨i xÃ­ng: shÃ­ jiÄn, yÄ«n guÇ’, kÃ²ng jiÄn hÃ© luÃ³ jiÌ€ tuÄ« lÇ. TÅng guÃ² gÄo zhÃ¬ lÃ¬ang de cÃ¨ shÃ¬ Ã n lÃ¬ hÃ© pÃ­ng guÌ„ kuÃ ng jiÃ , RISEBench nÃ©ng gÇ’u pÃ­ng guÌ„ zhÇ lÇng tuÄ« lÇ, wÃ i guÇn yÄ« zhÃ¬ xÃ­ng hÃ© shÃ¬ juÃ© hÃ© lÇ xÃ­ng. ShÃ­ yÃ n jiÃ© guÇ’ xiÇn shÃ¬, jÇn guÇn GPT-4o-Native zÃ i mÇ’u xiÄ“ fÄng miÃ n biÇo xiÃ n chÅ« sÃ¨, dÃ n zÃ i luÃ³ jiÌ€ tuÄ« lÇ rÃ¨n wÃ¹ shÃ ng rÃ©ng cÃºn zÃ i tiÇo zhÃ n. ZuÃ² zhÄ› xÄ« wÃ ng RISEBench nÃ©ng wÃ©i wÃ¨i lÃ¡i de yÃ¡n jiÅ« tÃ­ gÅng jÄ« chÇ” xÃ¬ng de jiÃ n shÃ¨, bÃ¬ng chÃ©ng nuÃ² dÃ n kuÃ² zhÇn hÃ© wÃ¡n shÃ n zhÃ¨ yÄ« jÄ« zhÇ”n cÃ¨ shÃ¬.",
        "vocab": "[{'word': 'åŸºå‡†', 'pinyin': 'jÄ«zhÇ”n', 'trans': 'benchmark'},\n{'word': 'æµ‹è¯•', 'pinyin': 'cÃ¨shÃ¬', 'trans': 'test'},\n{'word': 'ç§°ä¸º', 'pinyin': 'chÄ“ngwÃ©i', 'trans': 'called'},\n{'word': 'ç”¨äº', 'pinyin': 'yÃ²ngyÃº', 'trans': 'used for'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­nggÅ«', 'trans': 'evaluate'},\n{'word': 'å¤§å‹', 'pinyin': 'dÃ xÃ­ng', 'trans': 'large-scale'},\n{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅmÃ³shuÃ i', 'trans': 'multimodal'},\n{'word': 'æ¨¡å‹', 'pinyin': 'mÃ³xÃ­ng', 'trans': 'model'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬juÃ©', 'trans': 'visual'},\n{'word': 'ç¼–è¾‘', 'pinyin': 'biÄnjÃ­', 'trans': 'editing'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ«lÇ', 'trans': 'reasoning'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'ability'},\n{'word': 'ä¸“æ³¨', 'pinyin': 'zhuÄnzhÃ¹', 'trans': 'focus on'},\n{'word': 'å…³é”®', 'pinyin': 'guÇnjiÃ n', 'trans': 'key'},\n{'word': 'ç±»å‹', 'pinyin': 'lÃ¨ixÃ­ng', 'trans': 'type'},\n{'word': 'æ—¶é—´', 'pinyin': 'shÃ­jiÄn', 'trans': 'time'},\n{'word': 'å› æœ', 'pinyin': 'yÄ«nguÇ’', 'trans': 'causality'},\n{'word': 'ç©ºé—´', 'pinyin': 'kÅngjiÄn', 'trans': 'space'},\n{'word': 'é€»è¾‘', 'pinyin': 'luÃ³ji', 'trans': 'logic'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄozhÃ¬liÃ ng', 'trans': 'high-quality'},\n{'word': 'æ¡ˆä¾‹', 'pinyin': 'Ã nlÃ¬', 'trans': 'case'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æŒ‡ä»¤', 'pinyin': 'zhÇlÃ¬ng', 'trans': 'instruction'},\n{'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ«zhÃ¬xÃ¬ng', 'trans': 'consistency'},\n{'word': 'åˆç†æ€§', 'pinyin': 'hÃ©lÇxÃ¬ng', 'trans': 'reasonableness'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇnshÃ¬', 'trans': 'show'},\n{'word': 'å°½ç®¡', 'pinyin': 'jÇnguÇn', 'trans': 'although'},\n{'word': 'å‡ºè‰²', 'pinyin': 'chÅ«sÃ¨', 'trans': 'outstanding'},\n{'word': 'ä½†åœ¨', 'pinyin': 'dÃ nzÃ i', 'trans': 'but in'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'},\n{'word': 'ä»å­˜åœ¨', 'pinyin': 'rÃ©ngcÃºnzÃ i', 'trans': 'still exist'},\n{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇozhÃ n', 'trans': 'challenge'},\n{'word': 'ä½œè€…', 'pinyin': 'zuÃ²zhÄ›', 'trans': 'author'},\n{'word': 'å¸Œæœ›', 'pinyin': 'xÄ«wÃ ng', 'trans': 'hope'},\n{'word': 'æä¾›', 'pinyin': 'tÃ­gÅng', 'trans': 'provide'},\n{'word': 'åŸºç¡€æ€§', 'pinyin': 'jÄ«chÇ”xÃ¬ng', 'trans': 'foundational'},\n{'word': 'è§è§£', 'pinyin': 'jiÃ njiÄ›', 'trans': 'insight'},\n{'word': 'æ‰¿è¯º', 'pinyin': 'chÃ©ngnuÃ²', 'trans': 'promise'},\n{'word': 'ä¸æ–­', 'pinyin': 'bÃ¹duÃ n', 'trans': 'continuously'},\n{'word': 'æ‰©å±•', 'pinyin': 'kuÃ²zhÇn', 'trans': 'expand'},\n{'word': 'å®Œå–„', 'pinyin': 'wÃ¡nshÃ n', 'trans': 'perfect'}]",
        "trans": "This article introduces a new benchmark test called RISEBench, designed to evaluate the reasoning capabilities of large multimodal models (LMMs) in visual editing. RISEBench focuses on four key types of reasoning: temporal, causal, spatial, and logical reasoning. Through high-quality test cases and an evaluation framework, RISEBench can assess instruction reasoning, appearance consistency, and visual plausibility. Experimental results indicate that while GPT-4o-Native performs well in certain aspects, it still faces challenges in logical reasoning tasks. The authors hope that RISEBench will provide foundational insights for future research and are committed to continually expanding and refining this benchmark test.",
        "update_ts": "2025-04-04 09:11"
    }
}