{
    "date": {
        "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 5",
        "zh": "12æœˆ5æ—¥"
    },
    "time_utc": "2024-12-05 11:09",
    "weekday": 3,
    "issue_id": 965,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.02687",
            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
            "url": "https://huggingface.co/papers/2412.02687",
            "abstract": "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.",
            "score": 30,
            "issue_id": 961,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "d766bad745d5f322",
            "authors": [
                "Viet Nguyen",
                "Anh Nguyen",
                "Trung Dao",
                "Khoi Nguyen",
                "Cuong Pham",
                "Toan Tran",
                "Anh Tran"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech.",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02687.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SNOOPI - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ PG-SB Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ±ĞµÑĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ NASA Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ° HPSv2 Ğ² 31.08 Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "SNOOPI: Enhancing One-Step Diffusion Models with Robust Guidance",
                    "desc": "This paper introduces SNOOPI, a new framework that improves one-step text-to-image diffusion models by addressing issues with guidance stability and negative prompt support. The authors enhance training stability using Proper Guidance-SwiftBrush (PG-SB), which applies a random-scale classifier-free guidance method to diversify output distributions. Additionally, they present Negative-Away Steer Attention (NASA), a training-free technique that incorporates negative prompts to eliminate unwanted elements in generated images. Experimental results demonstrate that SNOOPI outperforms existing models, achieving a new state-of-the-art HPSv2 score of 31.08."
                },
                "zh": {
                    "title": "SNOOPIï¼šæå‡ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ€§ä¸ç”Ÿæˆè´¨é‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶SNOOPIï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ã€‚æˆ‘ä»¬é€šè¿‡Proper Guidance-SwiftBrush (PG-SB)æ–¹æ³•å¢å¼ºäº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œé‡‡ç”¨éšæœºå°ºåº¦çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•Negative-Away Steer Attention (NASA)ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å°†è´Ÿæç¤ºé›†æˆåˆ°ä¸€æ­¥æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥æŠ‘åˆ¶ç”Ÿæˆå›¾åƒä¸­çš„ä¸å¿…è¦å…ƒç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—æé«˜äº†åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œåˆ›é€ äº†ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„æ–°æ ‡æ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03552",
            "title": "Imagine360: Immersive 360 Video Generation from Perspective Anchor",
            "url": "https://huggingface.co/papers/2412.03552",
            "abstract": "360^circ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360^circ video format, we seek to lift standard perspective videos into 360^circ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360^circ video generation framework that creates high-quality 360^circ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360^circ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for 360^circ video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360^circ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360^circ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360^circ video creation.",
            "score": 16,
            "issue_id": 958,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "90dc986cabb575af",
            "authors": [
                "Jing Tan",
                "Shuai Yang",
                "Tong Wu",
                "Jingwen He",
                "Yuwei Guo",
                "Ziwei Liu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03552.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² 360Â°: Ğ¾Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ñ‹Ñ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Imagine360 - Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ñ‚Ğ¸Ğ¿Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ°ÑĞºÑƒ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ ÑƒĞ³Ğ»Ğ° Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming Perspective Videos into Immersive 360Â° Experiences",
                    "desc": "The paper presents Imagine360, a novel framework for converting standard perspective videos into immersive 360-degree equirectangular videos. It employs a dual-branch architecture that integrates local and global constraints to enhance video quality and motion coherence. Key innovations include an antipodal mask for capturing long-range motion dependencies and elevation-aware designs to adapt to varying perspectives. Extensive experiments demonstrate that Imagine360 outperforms existing methods in generating high-quality, dynamic 360-degree videos."
                },
                "zh": {
                    "title": "Imagine360ï¼šä¸ªæ€§åŒ–æ²‰æµ¸å¼360åº¦è§†é¢‘åˆ›ä½œçš„æœªæ¥",
                    "desc": "360åº¦è§†é¢‘æä¾›äº†ä¸€ç§è¶…æ²‰æµ¸å¼ä½“éªŒï¼Œè®©è§‚ä¼—å¯ä»¥ä»å…¨æ–¹ä½æ¢ç´¢åŠ¨æ€åœºæ™¯ã€‚ä¸ºå®ç°æ›´å‹å¥½å’Œä¸ªæ€§åŒ–çš„360åº¦è§†é¢‘å†…å®¹åˆ›ä½œï¼Œæˆ‘ä»¬æå‡ºäº†Imagine360ï¼Œè¿™æ˜¯é¦–ä¸ªå°†æ ‡å‡†è§†è§’è§†é¢‘è½¬æ¢ä¸º360åº¦è§†é¢‘çš„æ¡†æ¶ã€‚Imagine360é€šè¿‡æœ‰é™çš„360åº¦è§†é¢‘æ•°æ®å­¦ä¹ ç»†è‡´çš„çƒé¢è§†è§‰å’Œè¿åŠ¨æ¨¡å¼ï¼Œé‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡æ¥æä¾›å±€éƒ¨å’Œå…¨å±€çº¦æŸã€‚å®éªŒè¡¨æ˜ï¼ŒImagine360åœ¨å›¾å½¢è´¨é‡å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„360åº¦è§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03515",
            "title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion",
            "url": "https://huggingface.co/papers/2412.03515",
            "abstract": "Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5times) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.",
            "score": 16,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "6e733cf9c0a1b851",
            "authors": [
                "Shengyuan Zhang",
                "An Zhao",
                "Ling Yang",
                "Zejian Li",
                "Chenye Meng",
                "Haoran Xu",
                "Tianrun Chen",
                "AnYang Wei",
                "Perry Pengyun GU",
                "Lingyun Sun"
            ],
            "affiliations": [
                "Peking University",
                "Zhejiang Green Zhixing Technology co., ltd",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03515.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#3d",
                    "#open_source"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ 3D LiDAR-ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ 3D LiDAR-ÑÑ†ĞµĞ½ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ScoreLiDAR. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ĞŸĞ¾Ñ‚ĞµÑ€Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ 3D LiDAR-ÑÑ†ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ScoreLiDAR ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 5 Ñ€Ğ°Ğ· Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ 3D LiDAR-ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Accelerating 3D LiDAR Scene Completion with ScoreLiDAR",
                    "desc": "This paper introduces ScoreLiDAR, a new method for improving the efficiency of 3D LiDAR scene completion using diffusion models. The proposed distillation technique allows the model to generate high-quality scene completions in significantly fewer sampling steps, making it faster and more practical for real-time applications like autonomous vehicles. Additionally, a novel Structural Loss is introduced to enhance the model's ability to understand and replicate the geometric structure of the 3D scenes. Experimental results show that ScoreLiDAR reduces completion time dramatically while outperforming existing state-of-the-art models in quality."
                },
                "zh": {
                    "title": "é«˜æ•ˆ3D LiDARåœºæ™¯è¡¥å…¨çš„æ–°æ–¹æ³•",
                    "desc": "æ‰©æ•£æ¨¡å‹å› å…¶å¼ºå¤§çš„è®­ç»ƒç¨³å®šæ€§å’Œé«˜è´¨é‡çš„åœºæ™¯è¡¥å…¨è€Œè¢«åº”ç”¨äº3D LiDARåœºæ™¯è¡¥å…¨ã€‚ç„¶è€Œï¼Œæ…¢é€Ÿé‡‡æ ·é€Ÿåº¦é™åˆ¶äº†åŸºäºæ‰©æ•£çš„åœºæ™¯è¡¥å…¨æ¨¡å‹çš„å®é™…åº”ç”¨ï¼Œå› ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†éœ€è¦é«˜æ•ˆæ„ŸçŸ¥å‘¨å›´ç¯å¢ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è’¸é¦æ–¹æ³•ï¼Œç§°ä¸ºScoreLiDARï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆä¸”é«˜è´¨é‡çš„åœºæ™¯è¡¥å…¨ã€‚é€šè¿‡å¼•å…¥ç»“æ„æŸå¤±ï¼ŒScoreLiDARèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰3D LiDARåœºæ™¯çš„å‡ ä½•ç»“æ„ï¼ŒåŒæ—¶æ˜¾è‘—åŠ å¿«äº†æ¯å¸§çš„è¡¥å…¨æ—¶é—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03555",
            "title": "PaliGemma 2: A Family of Versatile VLMs for Transfer",
            "url": "https://huggingface.co/papers/2412.03555",
            "abstract": "PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.",
            "score": 11,
            "issue_id": 964,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "12d0d9bcc8060099",
            "authors": [
                "Andreas Steiner",
                "AndrÃ© Susano Pinto",
                "Michael Tschannen",
                "Daniel Keysers",
                "Xiao Wang",
                "Yonatan Bitton",
                "Alexey Gritsenko",
                "Matthias Minderer",
                "Anthony Sherbondy",
                "Shangbang Long",
                "Siyang Qin",
                "Reeve Ingle",
                "Emanuele Bugliarello",
                "Sahar Kazemzadeh",
                "Thomas Mesnard",
                "Ibrahim Alabdulmohsin",
                "Lucas Beyer",
                "Xiaohua Zhai"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03555.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#cv",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "PaliGemma 2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "PaliGemma 2 - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PaliGemma, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma 2. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ SigLIP-So400m Ñ Ñ€ÑĞ´Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma 2 Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ‚ 2B Ğ´Ğ¾ 27B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (224px, 448px Ğ¸ 896px) Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. PaliGemma 2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ½Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "PaliGemma 2: Advancing Vision-Language Understanding",
                    "desc": "PaliGemma 2 is an enhanced Vision-Language Model (VLM) that builds on the original PaliGemma framework by integrating the SigLIP-So400m vision encoder with various sizes of the Gemma 2 language models. The models are trained at three different image resolutions to improve their ability to transfer knowledge through fine-tuning. This upgrade allows researchers to explore how different factors, like learning rates and model sizes, affect performance on various tasks. PaliGemma 2 also expands its capabilities to include a wider range of tasks, achieving state-of-the-art results in areas such as optical character recognition and detailed captioning."
                },
                "zh": {
                    "title": "PaliGemma 2ï¼šè§†è§‰ä¸è¯­è¨€çš„å®Œç¾ç»“åˆ",
                    "desc": "PaliGemma 2 æ˜¯åŸºäº Gemma 2 è¯­è¨€æ¨¡å‹å®¶æ—çš„ PaliGemma å¼€æ”¾è§†è§‰è¯­è¨€æ¨¡å‹çš„å‡çº§ç‰ˆã€‚æˆ‘ä»¬ç»“åˆäº† SigLIP-So400m è§†è§‰ç¼–ç å™¨å’Œä¸åŒè§„æ¨¡çš„ Gemma 2 æ¨¡å‹ï¼Œè¿›è¡Œå¤šé˜¶æ®µè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„çŸ¥è¯†è¿ç§»èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸‰ç§åˆ†è¾¨ç‡ä¸‹è®­ç»ƒï¼Œæˆ‘ä»¬èƒ½å¤Ÿç ”ç©¶å½±å“è¿ç§»æ€§èƒ½çš„å› ç´ ï¼Œå¦‚å­¦ä¹ ç‡ï¼Œå¹¶åˆ†æä»»åŠ¡ç±»å‹ã€æ¨¡å‹å¤§å°å’Œåˆ†è¾¨ç‡ä¹‹é—´çš„å…³ç³»ã€‚PaliGemma 2 æ‰©å±•äº†è¿ç§»ä»»åŠ¡çš„æ•°é‡å’ŒèŒƒå›´ï¼Œæ¶µç›–äº†å¤šç§å…‰å­¦å­—ç¬¦è¯†åˆ«ç›¸å…³ä»»åŠ¡ï¼Œå¹¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03517",
            "title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images",
            "url": "https://huggingface.co/papers/2412.03517",
            "abstract": "Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.",
            "score": 10,
            "issue_id": 960,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "9d51bf0b60be344b",
            "authors": [
                "Lingen Li",
                "Zhaoyang Zhang",
                "Yaowei Li",
                "Jiale Xu",
                "Xiaoyu Li",
                "Wenbo Hu",
                "Weihao Cheng",
                "Jinwei Gu",
                "Tianfan Xue",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03517.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¾Ğ²",
                    "desc": "NVComposer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑÑ‚ĞµÑ€ĞµĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NVComposer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "NVComposer: Generating Novel Views Without External Alignment",
                    "desc": "This paper introduces NVComposer, a new method for generating novel views from multiple images without needing external alignment processes like pose estimation. NVComposer uses a dual-stream diffusion model that generates new views while also predicting camera poses, allowing for a more integrated approach. Additionally, it incorporates a geometry-aware feature alignment module that learns geometric information from stereo models during training. The results show that NVComposer outperforms existing methods, especially when there are many unaligned input views, making it a more flexible solution for novel view synthesis."
                },
                "zh": {
                    "title": "NVComposerï¼šæ— é¡»å¤–éƒ¨å¯¹é½çš„ç”Ÿæˆæ–°è§†å›¾åˆæˆ",
                    "desc": "æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤šè§†å›¾æ•°æ®çš„æ–°çš„è§†å›¾åˆæˆï¼ˆNVSï¼‰èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå¤–éƒ¨çš„å¤šè§†å›¾å¯¹é½è¿‡ç¨‹ï¼Œå¦‚æ˜¾å¼çš„å§¿æ€ä¼°è®¡æˆ–é¢„é‡å»ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„çµæ´»æ€§å’Œå¯è®¿é—®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è§†å›¾ä¹‹é—´é‡å ä¸è¶³æˆ–é®æŒ¡æ—¶å¯¹é½ä¸ç¨³å®šçš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†NVComposerï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ¶ˆé™¤äº†å¯¹æ˜¾å¼å¤–éƒ¨å¯¹é½çš„éœ€æ±‚ã€‚NVComposeré€šè¿‡å¼•å…¥ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œä½¿ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿéšå¼æ¨æ–­å¤šä¸ªæ¡ä»¶è§†å›¾ä¹‹é—´çš„ç©ºé—´å’Œå‡ ä½•å…³ç³»ï¼Œä»è€Œåœ¨ç”Ÿæˆå¤šè§†å›¾NVSä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03069",
            "title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2412.03069",
            "abstract": "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.",
            "score": 10,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "820e62e1bd498d55",
            "authors": [
                "Liao Qu",
                "Huichao Zhang",
                "Yiheng Liu",
                "Xu Wang",
                "Yi Jiang",
                "Yiming Gao",
                "Hu Ye",
                "Daniel K. Du",
                "Zehuan Yuan",
                "Xinglong Wu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03069.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "TokenFlow: ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "TokenFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. TokenFlow Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 7.2% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ LLaVA-1.5 13B. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "TokenFlow: Bridging Understanding and Generation in Image Processing",
                    "desc": "TokenFlow is a new image tokenizer that improves how machines understand and generate images by using a dual-codebook architecture. This approach separates the learning of high-level semantic features from fine-grained pixel-level details, allowing for better performance in both understanding and generation tasks. By aligning these two types of information through a shared mapping, TokenFlow can effectively utilize both granularities of visual data. The results show that TokenFlow outperforms previous models in understanding and image generation, achieving significant improvements in performance metrics."
                },
                "zh": {
                    "title": "TokenFlowï¼šå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æ¡¥æ¢",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å›¾åƒæ ‡è®°å™¨TokenFlowï¼Œå®ƒå¼¥åˆäº†å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç†è§£å’Œç”Ÿæˆä»»åŠ¡éœ€è¦ä¸åŒç²’åº¦çš„è§†è§‰ä¿¡æ¯ï¼Œä¼ ç»Ÿçš„å•ä¸€é‡å»ºç›®æ ‡å‘é‡é‡åŒ–ç¼–ç å™¨æ— æ³•æœ‰æ•ˆå¤„ç†è¿™ä¸€é—®é¢˜ã€‚TokenFlowé€šè¿‡åˆ›æ–°çš„åŒä»£ç æœ¬æ¶æ„ï¼Œè§£è€¦äº†è¯­ä¹‰å’Œåƒç´ çº§ç‰¹å¾å­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡å…±äº«æ˜ å°„æœºåˆ¶ä¿æŒå®ƒä»¬çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTokenFlowåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œé¦–æ¬¡è¯æ˜ç¦»æ•£è§†è§‰è¾“å…¥åœ¨ç†è§£æ€§èƒ½ä¸Šè¶…è¶Šäº†LLaVA-1.5 13Bã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00493",
            "title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding",
            "url": "https://huggingface.co/papers/2412.00493",
            "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models' learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.",
            "score": 8,
            "issue_id": 964,
            "pub_date": "2024-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "10c214b548697656",
            "authors": [
                "Duo Zheng",
                "Shijia Huang",
                "Liwei Wang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00493.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#3d",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Video-3D LLM: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Video-3D LLM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ 3D-ÑÑ†ĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D-Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Understanding with Video-3D LLM",
                    "desc": "This paper introduces a new model called Video-3D LLM, designed to improve understanding of 3D scenes by treating them like dynamic videos. The model incorporates 3D position encoding to better align video representations with real-world spatial contexts, addressing the limitations of existing Multimodal Large Language Models (MLLMs) that primarily learn from 2D data. To enhance efficiency, a maximum coverage sampling technique is used, balancing computational costs with performance. The results show that Video-3D LLM achieves state-of-the-art performance on multiple benchmarks for 3D scene understanding."
                },
                "zh": {
                    "title": "æå‡3Dåœºæ™¯ç†è§£çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç§°ä¸ºVideo-3D LLMï¼Œæ—¨åœ¨æé«˜3Dåœºæ™¯ç†è§£èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„MLLMä¸»è¦åŸºäº2Dæ•°æ®è®­ç»ƒï¼Œå¯¼è‡´å®ƒä»¬åœ¨å¤„ç†3Dç¯å¢ƒæ—¶å­˜åœ¨å±€é™æ€§ã€‚é€šè¿‡å°†3Dåœºæ™¯è§†ä¸ºåŠ¨æ€è§†é¢‘ï¼Œå¹¶å¼•å…¥3Dä½ç½®ç¼–ç ï¼ŒVideo-3D LLMèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å¯¹é½è§†é¢‘è¡¨ç¤ºä¸ç°å®ä¸–ç•Œçš„ç©ºé—´ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æœ€å¤§è¦†ç›–é‡‡æ ·æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œæ€§èƒ½æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19103",
            "title": "VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models",
            "url": "https://huggingface.co/papers/2411.19103",
            "abstract": "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B.",
            "score": 7,
            "issue_id": 964,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "4507a3a2ac0bc8b5",
            "authors": [
                "Jeongho Ju",
                "Daeyoung Kim",
                "SunYoung Park",
                "Youngjune Kim"
            ],
            "affiliations": [
                "NC Research, NCSOFT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19103.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#multimodal",
                    "#training",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "VARCO-VISION: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ VARCO-VISION Ğ´Ğ»Ñ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. VARCO-VISION Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞµĞµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "VARCO-VISION: Bridging Korean and English through Vision-Language Learning",
                    "desc": "This paper presents VARCO-VISION, an open-source vision-language model designed for Korean-English tasks. It employs a step-by-step training approach that effectively integrates linguistic and visual information while maintaining the foundational knowledge of the backbone model. The model excels in bilingual image-text understanding and generation, outperforming similar-sized models in various applications. Additionally, VARCO-VISION supports grounding, referring, and optical character recognition (OCR), and the authors provide five Korean evaluation datasets to facilitate further research in this area."
                },
                "zh": {
                    "title": "VARCO-VISIONï¼šåŒè¯­è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°é‡Œç¨‹ç¢‘",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¼€æºçš„éŸ©è‹±è§†è§‰è¯­è¨€æ¨¡å‹VARCO-VISIONã€‚æˆ‘ä»¬é‡‡ç”¨é€æ­¥è®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å­¦ä¹ è¯­è¨€å’Œè§†è§‰ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†ã€‚ä¸åŒç±»æ¨¡å‹ç›¸æ¯”ï¼ŒVARCO-VISIONåœ¨åŒè¯­å›¾åƒæ–‡æœ¬ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹è¿˜å…·å¤‡å®šä½ã€å¼•ç”¨å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰åŠŸèƒ½ï¼Œæ‰©å±•äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01106",
            "title": "One Shot, One Talk: Whole-body Talking Avatar from a Single Image",
            "url": "https://huggingface.co/papers/2412.01106",
            "abstract": "Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.",
            "score": 7,
            "issue_id": 957,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "13d96f9bb346e344",
            "authors": [
                "Jun Xiang",
                "Yudong Guo",
                "Leipeng Hu",
                "Boyang Guo",
                "Yancheng Yuan",
                "Juyong Zhang"
            ],
            "affiliations": [
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01106.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#multimodal",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¸ Ğ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ÑŒÑÑ, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞµ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞµÑ‚ĞºÑƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¶ĞµÑÑ‚Ñ‹ Ğ¸ Ğ¼Ğ¸Ğ¼Ğ¸ĞºÑƒ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "From One Image to a Lifelike Talking Avatar!",
                    "desc": "This paper presents a new method for creating realistic and animatable whole-body talking avatars using only a single image. The authors address two main challenges: modeling complex movements and ensuring the avatar can perform new gestures and expressions. They utilize pose-guided image-to-video diffusion models to generate video frames that serve as training data, despite being imperfect. To improve the quality of the avatar's animations, they introduce a hybrid representation that combines 3D mesh structures with regularization techniques to handle inconsistencies in the generated video frames."
                },
                "zh": {
                    "title": "ä»å•å¼ å›¾åƒç”Ÿæˆå…¨èº«ä¼šè¯´è¯çš„è™šæ‹Ÿå¤´åƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒæ„å»ºå…¨èº«ä¼šè¯´è¯çš„è™šæ‹Ÿå¤´åƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬è§£å†³äº†å¤æ‚åŠ¨æ€å»ºæ¨¡å’Œå¯¹æ–°æ‰‹åŠ¿ä¸è¡¨æƒ…çš„æ³›åŒ–è¿™ä¸¤ä¸ªå…³é”®é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å§¿æ€å¼•å¯¼çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸å®Œç¾çš„è§†é¢‘å¸§ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œä»¥å®ç°æ— ç¼æ³›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»å•å¼ å›¾åƒåˆ›å»ºå‡ºé€¼çœŸã€å¯ç²¾ç¡®åŠ¨ç”»å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„å…¨èº«è™šæ‹Ÿå¤´åƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03439",
            "title": "CleanDIFT: Diffusion Features without Noise",
            "url": "https://huggingface.co/papers/2412.03439",
            "abstract": "Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.",
            "score": 6,
            "issue_id": 963,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "cd474064bf17503a",
            "authors": [
                "Nick Stracke",
                "Stefan Andreas Baumann",
                "Kolja Bauer",
                "Frank Fundel",
                "BjÃ¶rn Ommer"
            ],
            "affiliations": [
                "CompVis @ LMU Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03439.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑˆÑƒĞ¼Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ±ĞµĞ· ÑˆÑƒĞ¼Ğ°. Ğ­Ñ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Noise-Free Semantic Features from Diffusion Models",
                    "desc": "This paper discusses how internal features from large pre-trained diffusion models can be used as effective semantic descriptors for various tasks. It highlights the problem that these models require added noise to generate useful features, which limits their effectiveness. The authors propose a new, lightweight, unsupervised fine-tuning method that allows these models to produce high-quality semantic features without the need for noise. Their approach significantly improves performance across multiple tasks compared to previous methods, including ensemble techniques, while being more efficient."
                },
                "zh": {
                    "title": "æ— å™ªå£°çš„é«˜è´¨é‡è¯­ä¹‰ç‰¹å¾æå–",
                    "desc": "æœ€è¿‘ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾è¢«ç¡®ç«‹ä¸ºå¼ºå¤§çš„è¯­ä¹‰æè¿°ç¬¦ï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚é€šå¸¸ï¼Œè¿™äº›ç‰¹å¾éœ€è¦åœ¨å›¾åƒä¸­æ·»åŠ å™ªå£°åæ‰èƒ½æå–ï¼Œå› ä¸ºæ¨¡å‹åœ¨å¤„ç†å‡ ä¹æ²¡æœ‰å™ªå£°çš„å›¾åƒæ—¶ï¼Œæä¾›çš„ç‰¹å¾æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬å‘ç°å™ªå£°å¯¹ç‰¹å¾çš„æœ‰æ•ˆæ€§æœ‰é‡è¦å½±å“ï¼Œä¸”é€šè¿‡ä¸åŒéšæœºå™ªå£°çš„é›†æˆæ— æ³•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ— ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæä¾›é«˜è´¨é‡ã€æ— å™ªå£°çš„è¯­ä¹‰ç‰¹å¾ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£ç‰¹å¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03558",
            "title": "MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation",
            "url": "https://huggingface.co/papers/2412.03558",
            "abstract": "This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.",
            "score": 6,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "5e1a4c1e1017e7af",
            "authors": [
                "Zehuan Huang",
                "Yuan-Chen Guo",
                "Xingqiao An",
                "Yunhan Yang",
                "Yangguang Li",
                "Zi-Xin Zou",
                "Ding Liang",
                "Xihui Liu",
                "Yan-Pei Cao",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "The University of Hong Kong",
                "Tsinghua University",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03558.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#synthetic",
                    "#diffusion",
                    "#training",
                    "#3d"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "MIDI: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MIDI - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. MIDI Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¸Ñ… Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. MIDI Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "MIDI: Revolutionizing 3D Scene Generation from Single Images",
                    "desc": "This paper presents MIDI, a new approach for creating 3D scenes from a single image. It improves upon traditional methods by using multi-instance diffusion models, allowing for the generation of multiple 3D objects at once while maintaining their spatial relationships. MIDI features a unique multi-instance attention mechanism that captures how objects interact and fit together in space, simplifying the generation process. The method is trained with a combination of scene-level and single-object data, ensuring high performance and generalization across various types of scenes."
                },
                "zh": {
                    "title": "MIDIï¼šä»å•å›¾åƒç”Ÿæˆ3Dåœºæ™¯çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMIDIçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å•å¼ å›¾åƒç”Ÿæˆç»„åˆ3Dåœºæ™¯ã€‚ä¸ç°æœ‰ä¾èµ–é‡å»ºæˆ–æ£€ç´¢æŠ€æœ¯çš„æ–¹æ³•ä¸åŒï¼ŒMIDIæ‰©å±•äº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°3Då¯¹è±¡ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨å¤šå®ä¾‹æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¤šä¸ª3Då®ä¾‹çš„åŒæ—¶ç”Ÿæˆã€‚MIDIçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šå®ä¾‹æ³¨æ„æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¯¹è±¡é—´çš„äº¤äº’å’Œç©ºé—´ä¸€è‡´æ€§ï¼Œç®€åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åœ¨å›¾åƒåˆ°åœºæ™¯ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç»è¿‡åˆæˆæ•°æ®ã€çœŸå®åœºæ™¯æ•°æ®å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é£æ ¼åŒ–åœºæ™¯å›¾åƒçš„è¯„ä¼°éªŒè¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03187",
            "title": "Weighted-Reward Preference Optimization for Implicit Model Fusion",
            "url": "https://huggingface.co/papers/2412.03187",
            "abstract": "While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex but also prone to introducing noise and errors. In this paper, we propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs. To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrate that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against GPT-4-0314 on Arena-Hard. Our code is available at https://github.com/SLIT-AI/WRPO.",
            "score": 4,
            "issue_id": 961,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "6da11fbf4e1ea7d9",
            "authors": [
                "Ziyi Yang",
                "Fanqi Wan",
                "Longguang Zhong",
                "Tianyuan Shi",
                "Xiaojun Quan"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Sun Yat-sen University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03187.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#open_source",
                    "#architecture",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "WRPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ - Weighted-Reward Preference Optimization (WRPO). WRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WRPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Effortless Fusion of LLMs with WRPO!",
                    "desc": "This paper introduces a new method called Weighted-Reward Preference Optimization (WRPO) for fusing different open-source large language models (LLMs). WRPO simplifies the fusion process by avoiding complex tasks like vocabulary alignment and distribution matrix merging, which often introduce errors. Instead, it uses a preference optimization approach to effectively transfer capabilities from source LLMs to a target LLM. The method also includes a progressive adaptation strategy to manage differences in distributions between models, leading to improved performance on various benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "åŠ æƒå¥–åŠ±åå¥½ä¼˜åŒ–ï¼šé«˜æ•ˆèåˆå¤šç§å¤§è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§éšå¼èåˆæ–¹æ³•ï¼Œç§°ä¸ºåŠ æƒå¥–åŠ±åå¥½ä¼˜åŒ–ï¼ˆWRPOï¼‰ï¼Œæ—¨åœ¨æœ‰æ•ˆæ•´åˆä¸åŒæ¶æ„å’Œè§„æ¨¡çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚WRPOé€šè¿‡ä¼˜åŒ–æºæ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„åå¥½ï¼Œé¿å…äº†è¯æ±‡å¯¹é½å’ŒçŸ©é˜µèåˆçš„å¤æ‚æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ¸è¿›é€‚åº”ç­–ç•¥ï¼Œé€æ­¥è°ƒæ•´å¯¹ç›®æ ‡æ¨¡å‹å’Œæºæ¨¡å‹çš„ä¾èµ–ï¼Œä»è€Œè§£å†³äº†åˆ†å¸ƒåå·®é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„çŸ¥è¯†èåˆæ–¹æ³•å’Œå¾®è°ƒåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03085",
            "title": "Mimir: Improving Video Diffusion Models for Precise Text Understanding",
            "url": "https://huggingface.co/papers/2412.03085",
            "abstract": "Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/",
            "score": 3,
            "issue_id": 957,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "a065164e5fdadf2c",
            "authors": [
                "Shuai Tan",
                "Biao Gong",
                "Yutong Feng",
                "Kecheng Zheng",
                "Dandan Zheng",
                "Shuwei Shi",
                "Yujun Shen",
                "Jingdong Chen",
                "Ming Yang"
            ],
            "affiliations": [
                "Ant Group",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03085.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Mimir: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mimir - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ Mimir - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ 'token fuser', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mimir ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Mimir: Bridging Text Understanding and Video Generation",
                    "desc": "This paper introduces Mimir, a new framework for text-to-video (T2V) generation that combines the strengths of text encoders and large language models (LLMs). It addresses the challenge of feature distribution gaps between these two text modeling approaches, which can hinder effective video generation. Mimir utilizes a specialized token fuser to integrate outputs from both models, enhancing text comprehension and video quality. The results show that Mimir excels in generating videos from short captions and effectively managing dynamic movements."
                },
                "zh": {
                    "title": "Mimirï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ™ºèƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMimirçš„ç«¯åˆ°ç«¯è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä»¤ç‰Œèåˆå™¨ï¼Œè§£å†³äº†æ–‡æœ¬ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´çš„ç‰¹å¾åˆ†å¸ƒå·®è·ã€‚Mimirèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å­¦ä¹ åˆ°çš„è§†é¢‘å…ˆéªŒï¼ŒåŒæ—¶å¢å¼ºLLMsåœ¨æ–‡æœ¬ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMimiråœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ—¶ï¼Œå°¤å…¶åœ¨å¤„ç†çŸ­æ–‡æœ¬å’ŒåŠ¨æ€å˜åŒ–æ—¶ï¼Œè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-04.html",
    "link_next": "2024-12-06.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "04.12",
        "en": "12/04",
        "zh": "12æœˆ4æ—¥"
    },
    "short_date_next": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12æœˆ6æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 8,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 5,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 7,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ SNOOPIï¼Œæ—¨åœ¨æ”¹è¿›å•æ­¥æ‰©æ•£æ¨¡å‹çš„æŒ‡å¯¼æœºåˆ¶ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒæ‰©æ•£æ¨¡å‹éª¨æ¶æ—¶è¡¨ç°ä¸ç¨³å®šï¼Œä¸”ä¸æ”¯æŒè´Ÿé¢æç¤ºæŒ‡å¯¼ã€‚SNOOPI é€šè¿‡ PG-SB å’Œ NASA ä¸¤ç§æ–¹æ³•è§£å†³äº†è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSNOOPI æ˜¾è‘—æå‡äº†åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ°´å¹³ã€‚",
        "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ SNOOPIï¼Œæ—¨åœ¨æ”¹è¿›å•æ­¥æ‰©æ•£æ¨¡å‹çš„æŒ‡å¯¼æœºåˆ¶ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒæ‰©æ•£æ¨¡å‹éª¨æ¶æ—¶è¡¨ç°ä¸ç¨³å®šï¼Œä¸”ä¸æ”¯æŒè´Ÿé¢æç¤ºæŒ‡å¯¼ã€‚SNOOPI é€šè¿‡ PG-SB å’Œ NASA ä¸¤ç§æ–¹æ³•è§£å†³äº†è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSNOOPI æ˜¾è‘—æå‡äº†åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ°´å¹³ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de kuÃ ngjiÃ  SNOOPI, zhÇ zÃ i gÇijÃ¬n dÄn bÃ¹ kuÃ²sÃ n mÃ³xÃ­ng de zhÇdÇo jÄ«zhÃ¬. XiÃ nyÇ’u fÄngfÇ zÃ i chÇ”lÇ bÃ¹tÃ³ng kuÃ²sÃ n mÃ³xÃ­ng gÇ”jiÃ  shÃ­ biÇoxiÃ n bÃ¹Ã­culai, qiÄ› bÃ¹ zhÄ«chÃ­ fÃ¹miÃ n tÃ­shÃ¬ zhÇdÇo. SNOOPI tÅngguÃ² PG-SB hÃ© NASA liÇng zhÇ’ng fÄngfÇ jiÄ›juÃ© le zhÃ¨xiÄ“ wÃ¨ntÃ­. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, SNOOPI xiÇnzhÃ¹ tÃ­shÄ“ng le jÄ«zhÇ”n mÃ³xÃ­ng de xÃ­ngnÃ©ng, dÃ¡ dÃ o le xÄ«n de zuÃ¬jiÄ shuÇpÃ­ng.",
        "vocab": "[{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'},\n{'word': 'æ”¹è¿›', 'pinyin': 'gÇijÃ¬n', 'trans': 'improve'},\n{'word': 'å•æ­¥', 'pinyin': 'dÄnbÃ¹', 'trans': 'single-step'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇdÇo', 'trans': 'guidance'},\n{'word': 'æœºåˆ¶', 'pinyin': 'jÄ«zhÃ¬', 'trans': 'mechanism'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ nyÇ’u', 'trans': 'existing'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'},\n{'word': 'å¤„ç†', 'pinyin': 'chÇ”lÇ', 'trans': 'handle'},\n{'word': 'ä¸åŒ', 'pinyin': 'bÃ¹tÃ³ng', 'trans': 'different'},\n{'word': 'éª¨æ¶', 'pinyin': 'gÇ”jiÃ ', 'trans': 'skeleton'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'},\n{'word': 'ä¸ç¨³å®š', 'pinyin': 'bÃ¹wÄ›ndÃ¬ng', 'trans': 'unstable'},\n{'word': 'ä¸”', 'pinyin': 'qiÄ›', 'trans': 'and'},\n{'word': 'ä¸æ”¯æŒ', 'pinyin': 'bÃ¹ zhÄ«chÃ­', 'trans': 'not support'},\n{'word': 'è´Ÿé¢', 'pinyin': 'fÃ¹miÃ n', 'trans': 'negative'},\n{'word': 'æç¤º', 'pinyin': 'tÃ­shÃ¬', 'trans': 'prompt'},\n{'word': 'é€šè¿‡', 'pinyin': 'tÅngguÃ²', 'trans': 'through'},\n{'word': 'PG-SB', 'pinyin': '', 'trans': 'PG-SB'},\n{'word': 'NASA', 'pinyin': '', 'trans': 'NASA'},\n{'word': 'è§£å†³', 'pinyin': 'jiÄ›juÃ©', 'trans': 'solve'},\n{'word': 'è¿™äº›', 'pinyin': 'zhÃ¨xiÄ“', 'trans': 'these'},\n{'word': 'é—®é¢˜', 'pinyin': 'wÃ¨ntÃ­', 'trans': 'problems'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇnshÃ¬', 'trans': 'show'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'æå‡', 'pinyin': 'tÃ­shÄ“ng', 'trans': 'enhance'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ«zhÇ”n', 'trans': 'benchmark'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ­ngnÃ©ng', 'trans': 'performance'},\n{'word': 'è¾¾åˆ°', 'pinyin': 'dÃ¡dÃ o', 'trans': 'reach'},\n{'word': 'æ–°çš„', 'pinyin': 'xÄ«n de', 'trans': 'new'},\n{'word': 'æœ€ä½³', 'pinyin': 'zuÃ¬jiÄ', 'trans': 'best'},\n{'word': 'æ°´å¹³', 'pinyin': 'shuÇpÃ­ng', 'trans': 'level'}]",
        "trans": "This article introduces a new framework called SNOOPI, aimed at improving the guidance mechanism of single-step diffusion models. Existing methods perform unstably when handling different diffusion model backbones and do not support negative prompt guidance. SNOOPI addresses these issues through the PG-SB and NASA methods. Experimental results demonstrate that SNOOPI significantly enhances the performance of benchmark models, achieving new best-in-class levels.",
        "update_ts": "2024-12-05 09:11"
    }
}