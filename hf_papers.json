{
    "date": {
        "ru": "25 февраля",
        "en": "February 25",
        "zh": "2月25日"
    },
    "time_utc": "2025-02-25 23:09",
    "weekday": 1,
    "issue_id": 2406,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.17129",
            "title": "Thus Spake Long-Context Large Language Model",
            "url": "https://huggingface.co/papers/2502.17129",
            "abstract": "Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the research on long-context LLMs has expanded from length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies.   Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend its mortality. In this survey, We will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to the research on long-context LLMs.",
            "score": 51,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "8b44dbb5e39d9b38",
            "authors": [
                "Xiaoran Liu",
                "Ruixiao Li",
                "Mianqiu Huang",
                "Zhigeng Liu",
                "Yuerong Song",
                "Qipeng Guo",
                "Siyang He",
                "Qiqi Wang",
                "Linlin Li",
                "Qun Liu",
                "Yaqian Zhou",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "School of Computer Science Fudan University",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17129.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#survey",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Преодолевая границы: путь к LLM с длинным контекстом",
                    "desc": "Данная статья представляет обзор исследований в области обработки длинного контекста в больших языковых моделях (LLM). Авторы рассматривают жизненный цикл LLM с длинным контекстом с точки зрения архитектуры, инфраструктуры, обучения и оценки. В работе проводится аналогия между стремлением расширить контекст LLM и попытками человека преодолеть свою смертность. Статья завершается 10 открытыми вопросами, стоящими перед LLM с длинным контекстом."
                },
                "en": {
                    "title": "Unlocking the Power of Long Context in Language Models",
                    "desc": "This paper discusses the importance of long context in Natural Language Processing (NLP) and its impact on Large Language Models (LLMs). It highlights the advancements in extending context length to millions of tokens, which enhances the models' capabilities. The authors explore the challenges faced by LLMs in balancing the need for longer context with their inherent limitations. Additionally, the paper provides a comprehensive overview of the lifecycle of long-context LLMs, covering aspects such as architecture, infrastructure, training, and evaluation, while also posing ten critical questions for future research."
                },
                "zh": {
                    "title": "长上下文：大型语言模型的核心竞争力",
                    "desc": "长上下文是自然语言处理（NLP）中的一个重要主题，对大型语言模型（LLMs）的发展具有重要意义。尽管追求长上下文面临许多挑战，但它仍然是LLMs的核心竞争优势。近年来，LLMs的上下文长度已突破到数百万个标记，研究也从长度延展扩展到架构、基础设施、训练和评估技术的全面关注。本文将从四个角度展示长上下文LLMs的生命周期，并提出当前面临的十个未解问题，以期为长上下文LLMs的研究提供系统性的介绍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17258",
            "title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing",
            "url": "https://huggingface.co/papers/2502.17258",
            "abstract": "Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model. To address these difficulties, we present VideoGrain, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content. We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention. Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention. Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios. Our code, data, and demos are available at https://knightyxp.github.io/VideoGrain_project_page/",
            "score": 45,
            "issue_id": 2389,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "aaaaf48d4594432f",
            "authors": [
                "Xiangpeng Yang",
                "Linchao Zhu",
                "Hehe Fan",
                "Yi Yang"
            ],
            "affiliations": [
                "ReLER Lab, AAII, University of Technology Sydney",
                "ReLER Lab, CCAI, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17258.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Точное редактирование видео с помощью искусственного интеллекта",
                    "desc": "VideoGrain - это новый подход к многоуровневому редактированию видео с использованием диффузионных моделей. Он решает проблемы семантического несоответствия в управлении текст-регион и связанности признаков путем модуляции механизмов внимания в пространстве-времени. Метод усиливает внимание локальных подсказок к соответствующим пространственно-разделенным регионам и улучшает разделение признаков. Эксперименты показывают, что VideoGrain достигает передовых результатов в реальных сценариях редактирования видео."
                },
                "en": {
                    "title": "Fine-Grained Control for Enhanced Video Editing with VideoGrain",
                    "desc": "This paper introduces VideoGrain, a novel approach to enhance video generation and editing using diffusion models. It tackles the challenges of multi-grained video editing by improving the control over different levels of video content, such as class, instance, and part modifications. The method employs advanced attention mechanisms to ensure that local prompts effectively target specific regions while minimizing irrelevant interactions. Experimental results show that VideoGrain outperforms existing methods, demonstrating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "VideoGrain：精细控制视频编辑的新方法",
                    "desc": "本论文介绍了一种名为VideoGrain的零-shot方法，旨在解决多粒度视频编辑中的挑战。该方法通过调节时空注意力机制，实现对视频内容的精细控制。我们增强了文本到区域的控制，确保每个局部提示的注意力集中在相应的空间区域，同时减少与无关区域的交互。此外，我们还通过提高区域内的意识和减少区域间的干扰，改善了特征分离。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17157",
            "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
            "url": "https://huggingface.co/papers/2502.17157",
            "abstract": "Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models.",
            "score": 40,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "f8790b30bb553397",
            "authors": [
                "Canyu Zhao",
                "Mingyu Liu",
                "Huanyi Zheng",
                "Muzhi Zhu",
                "Zhiyue Zhao",
                "Hao Chen",
                "Tong He",
                "Chunhua Shen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17157.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#diffusion",
                    "#cv",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Универсальное восприятие через диффузию текста в изображения",
                    "desc": "Статья представляет DICEPTION - универсальную модель восприятия, основанную на предобученных диффузионных моделях преобразования текста в изображение. DICEPTION эффективно решает множество задач восприятия, достигая производительности на уровне современных моделей, но используя значительно меньше данных и вычислительных ресурсов. Модель кодирует выходные данные различных задач восприятия с помощью цветового кодирования, что оказывается эффективным для сегментации объектов и семантической сегментации. DICEPTION может быть адаптирована к новым задачам путем дообучения на небольшом количестве изображений, что делает ее перспективным решением для создания универсальных моделей компьютерного зрения."
                },
                "en": {
                    "title": "DICEPTION: Efficient Generalist Perception with Minimal Data",
                    "desc": "This paper presents DICEPTION, a versatile perception model designed to perform multiple tasks efficiently while minimizing the need for extensive computational resources and training data. By leveraging pre-trained text-to-image diffusion models, DICEPTION achieves competitive performance on various perception tasks using only a fraction of the data required by traditional models. The innovative use of color encoding for output representation enhances the model's effectiveness in both entity and semantic segmentation. Overall, DICEPTION demonstrates that it is possible to create a powerful generalist model that requires minimal fine-tuning and can adapt quickly to new tasks."
                },
                "zh": {
                    "title": "DICEPTION：高效的通用感知模型",
                    "desc": "本文的主要目标是创建一个通用的感知模型，能够在计算资源和训练数据有限的情况下处理多种任务。我们使用了在数十亿张图像上预训练的文本到图像扩散模型。通过全面的评估指标，DICEPTION在多个感知任务上表现出色，达到了与最先进模型相当的性能。该模型在适应其他任务时，仅需对50张图像和1%的参数进行微调，显示出其高效性和潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15814",
            "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
            "url": "https://huggingface.co/papers/2502.15814",
            "abstract": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .",
            "score": 37,
            "issue_id": 2388,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 февраля",
                "en": "February 19",
                "zh": "2月19日"
            },
            "hash": "735714e237272170",
            "authors": [
                "Gallil Maimon",
                "Avishai Elmakies",
                "Yossi Adi"
            ],
            "affiliations": [
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15814.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#synthetic",
                    "#data",
                    "#optimization",
                    "#architecture",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Революция в обучении речевых моделей: качество и скорость на доступном оборудовании",
                    "desc": "Исследователи представляют Slam - метод обучения высококачественных речевых языковых моделей (SLM) на одном академическом GPU за 24 часа. Они проводят эмпирический анализ инициализации и архитектуры модели, синтетических обучающих данных и оптимизации предпочтений. Результаты показывают, что этот метод обучения хорошо масштабируется при увеличении вычислительных ресурсов, достигая результатов на уровне ведущих SLM при гораздо меньших затратах. Авторы надеются, что эти результаты сделают обучение и исследование SLM более доступными."
                },
                "en": {
                    "title": "Slam: Fast and Efficient Speech Language Model Training",
                    "desc": "This paper presents Slam, a method for efficiently training high-quality Speech Language Models (SLMs) using a single academic GPU within 24 hours. The authors analyze various factors such as model initialization, architecture, and synthetic training data to optimize the training process. Their empirical results show that Slam not only achieves competitive performance compared to leading SLMs but also does so at a significantly lower computational cost. This research aims to make SLM training more accessible and demonstrates that it can exceed expected performance based on scaling laws."
                },
                "zh": {
                    "title": "Slam：高效训练语音语言模型的新方法",
                    "desc": "我们介绍了一种名为Slam的训练高质量语音语言模型（SLM）的方案，该方案可以在24小时内使用单个学术GPU完成。通过对模型初始化、架构、合成训练数据和偏好优化等方面的实证分析，我们提出了这一训练方法。我们的实验证明，这种训练方案在计算资源增加时也能良好扩展，且在计算成本上远低于领先的SLM。我们希望这些见解能使SLM的训练和研究变得更加可及。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16584",
            "title": "Audio-FLAN: A Preliminary Release",
            "url": "https://huggingface.co/papers/2502.16584",
            "abstract": "Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.",
            "score": 25,
            "issue_id": 2388,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "67b5d61b3df1a4bc",
            "authors": [
                "Liumeng Xue",
                "Ziya Zhou",
                "Jiahao Pan",
                "Zixuan Li",
                "Shuai Fan",
                "Yinghao Ma",
                "Sitong Cheng",
                "Dongchao Yang",
                "Haohan Guo",
                "Yujia Xiao",
                "Xinsheng Wang",
                "Zixuan Shen",
                "Chuanbo Zhu",
                "Xinshen Zhang",
                "Tianchi Liu",
                "Ruibin Yuan",
                "Zeyue Tian",
                "Haohe Liu",
                "Emmanouil Benetos",
                "Ge Zhang",
                "Yike Guo",
                "Wei Xue"
            ],
            "affiliations": [
                "Beihang University",
                "Inner Mongolia University",
                "National University of Singapore",
                "Queen Mary University of London",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16584.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "Единая модель для понимания и генерации аудио",
                    "desc": "Статья представляет Audio-FLAN - масштабный датасет для обучения языковых моделей работе с аудио. Он охватывает 80 разнообразных задач в области речи, музыки и звука, содержит более 100 миллионов примеров. Audio-FLAN создает основу для унифицированных аудио-языковых моделей, способных решать задачи понимания и генерации аудио в различных областях. Датасет доступен на HuggingFace и GitHub и будет постоянно обновляться."
                },
                "en": {
                    "title": "Unifying Audio Understanding and Generation with Audio-FLAN",
                    "desc": "This paper presents Audio-FLAN, a large-scale dataset designed for instruction tuning in audio tasks. It addresses the challenge of integrating audio understanding and generation into unified audio-language models. By providing over 100 million instances across 80 diverse tasks, Audio-FLAN enables models to perform both comprehension and generation tasks in a zero-shot manner. The dataset aims to enhance the capabilities of large language models in handling various audio domains effectively."
                },
                "zh": {
                    "title": "音频理解与生成的统一之路",
                    "desc": "最近音频标记技术的进步显著提升了音频能力与大型语言模型的整合。然而，音频理解和生成通常被视为不同的任务，这阻碍了真正统一的音频语言模型的发展。尽管指令调优在文本和视觉领域的泛化和零样本学习中取得了显著成功，但在音频领域的应用仍然未被充分探索。为了解决这一问题，我们推出了Audio-FLAN，这是一个涵盖80个多样化任务的大规模指令调优数据集，包含超过1亿个实例，为统一的音频语言模型奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17435",
            "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
            "url": "https://huggingface.co/papers/2502.17435",
            "abstract": "Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in bi-directional evaluations. These results highlight our method's stability and generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile solution for real-world applications.",
            "score": 19,
            "issue_id": 2390,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "896a996c71c167eb",
            "authors": [
                "Chen-Wei Chang",
                "Cheng-De Fan",
                "Chia-Che Chang",
                "Yi-Chen Lo",
                "Yu-Chee Tseng",
                "Jiun-Long Huang",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "MediaTek Inc.",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17435.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная оценка освещения для любых камер",
                    "desc": "Статья представляет новый метод GCC для оценки освещения на изображениях с использованием диффузионных моделей. GCC применяет однократное детерминистическое предсказание для дорисовки цветовых шкал, отражающих освещение сцены. Метод использует разложение Лапласа для сохранения структуры шкалы при адаптации цвета к освещению. GCC демонстрирует высокую устойчивость при работе с разными камерами, достигая лучших результатов по ошибке в наихудших 25% случаев."
                },
                "en": {
                    "title": "GCC: Robust Color Constancy Across Cameras",
                    "desc": "This paper introduces GCC, a novel method for achieving color constancy in images taken with different camera sensors. It utilizes diffusion models to inpaint color checkers, which helps in estimating the illumination of the scene. Key innovations include a single-step inference process for inpainting, a technique to maintain the structure of checkers while adapting colors based on illumination, and a data augmentation strategy to improve the handling of color checker annotations. The results show that GCC outperforms existing methods in challenging cross-camera scenarios, demonstrating its robustness and ability to generalize without needing specific training for each camera."
                },
                "zh": {
                    "title": "GCC：跨相机场景中的颜色恒常性新方法",
                    "desc": "本文提出了一种新的颜色恒常性方法GCC，旨在解决不同相机传感器之间的泛化问题。GCC利用扩散模型将颜色棋盘格填充到图像中，以估计光照条件。我们的方法包括单步确定性推理、拉普拉斯分解技术和基于掩码的数据增强策略，确保了在不同光照下的颜色适应性。实验结果表明，GCC在跨相机场景中表现出色，具有优越的鲁棒性和泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16614",
            "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
            "url": "https://huggingface.co/papers/2502.16614",
            "abstract": "The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench.",
            "score": 19,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "c70e868ad4c1b726",
            "authors": [
                "Alexander Zhang",
                "Marcus Dong",
                "Jiaheng Liu",
                "Wei Zhang",
                "Yejie Wang",
                "Jian Yang",
                "Ge Zhang",
                "Tianyu Liu",
                "Zhongyuan Peng",
                "Yingshui Tan",
                "Yuanxing Zhang",
                "Zhexu Wang",
                "Weixun Wang",
                "Yancheng He",
                "Ken Deng",
                "Wangchunshu Zhou",
                "Wenhao Huang",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Alibaba",
                "BUAA",
                "BUPT",
                "CASIA",
                "Kuaishou",
                "M-A-P",
                "NJU",
                "OPPO"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16614.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "CodeCriticBench: комплексная оценка критических способностей LLM в задачах работы с кодом",
                    "desc": "Статья представляет новый бенчмарк CodeCriticBench для оценки способностей больших языковых моделей (LLM) критиковать код. Бенчмарк включает задачи генерации кода и ответов на вопросы о коде разной сложности. CodeCriticBench предлагает базовую и продвинутую оценку критических способностей с использованием детальных чек-листов. Авторы провели обширные эксперименты с существующими LLM, показавшие эффективность предложенного бенчмарка."
                },
                "en": {
                    "title": "Enhancing Code Critique with CodeCriticBench",
                    "desc": "This paper introduces CodeCriticBench, a new benchmark designed to evaluate the critique capacity of Large Language Models (LLMs) specifically in the context of code tasks. Unlike existing benchmarks that primarily focus on general reasoning tasks, CodeCriticBench encompasses both code generation and code question-answering tasks, offering a range of difficulties. The evaluation framework includes both basic and advanced critique assessments, utilizing detailed checklists to ensure comprehensive analysis. Experimental results demonstrate that CodeCriticBench effectively measures the critique abilities of various LLMs, highlighting its importance in enhancing model reasoning capabilities."
                },
                "zh": {
                    "title": "全面评估LLMs的代码批评能力",
                    "desc": "大型语言模型（LLMs）的批评能力对于推理能力至关重要，可以提供必要的建议，如详细分析和建设性反馈。为了评估LLMs的批评能力，研究者们提出了多个批评基准，但现有基准存在一些局限性，例如对代码任务的评估不足。为了解决这些问题，我们引入了一个全面的代码批评基准，称为CodeCriticBench，涵盖了代码生成和代码问答两种主流任务，并设计了细致的评估标准。通过对现有LLMs的广泛实验结果，我们验证了CodeCriticBench的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16894",
            "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
            "url": "https://huggingface.co/papers/2502.16894",
            "abstract": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose Great LoRA Mixture-of-Expert (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.",
            "score": 16,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "020c0f54f92a9238",
            "authors": [
                "Chenghao Fan",
                "Zhenyi Lu",
                "Sichen Liu",
                "Xiaoye Qu",
                "Wei Wei",
                "Chengfeng Gu",
                "Yu Cheng"
            ],
            "affiliations": [
                "School of Computer Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16894.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "🐐",
                "ru": {
                    "title": "GOAT: Эффективная низкоранговая адаптация на уровне полной тонкой настройки",
                    "desc": "Этa статья представляет новый метод под названием GOAT (Great LoRA Mixture-of-Expert) для улучшения эффективности низкоранговой адаптации (LoRA) в больших языковых моделях. GOAT использует адаптивную интеграцию релевантных приоров с помощью SVD-структурированной архитектуры Mixture-of-Experts и теоретически обоснованный масштабирующий фактор. Метод позволяет преодолеть ограничения стандартной LoRA и приблизиться по эффективности к полной тонкой настройке модели. Эксперименты на 25 наборах данных показали превосходство GOAT над существующими методами в различных задачах обработки естественного языка и компьютерного зрения."
                },
                "en": {
                    "title": "Boosting LoRA with GOAT: A New Path to Efficiency in LLMs",
                    "desc": "This paper introduces Great LoRA Mixture-of-Expert (GOAT), a new framework designed to enhance the performance of Low-Rank Adaptation (LoRA) for Large Language Models (LLMs). GOAT addresses the limitations of existing methods by integrating adaptive priors through a singular value decomposition (SVD)-structured Mixture-of-Experts (MoE) architecture. It also aligns the optimization process with that of fully fine-tuned MoE models by introducing a theoretical scaling factor. The results show that GOAT significantly improves efficiency and performance across various tasks, effectively bridging the gap between LoRA and Full Fine-Tuning."
                },
                "zh": {
                    "title": "提升LoRA性能的全新框架：GOAT",
                    "desc": "本文提出了一种名为Great LoRA Mixture-of-Expert（GOAT）的框架，旨在提高低秩适应（LoRA）在大型语言模型（LLMs）中的性能。GOAT通过自适应整合相关的先验知识，使用奇异值分解（SVD）结构的专家混合（MoE）来优化LoRA。该框架还通过推导理论缩放因子，使优化过程与完全微调（Full FT）的MoE对齐。实验结果表明，GOAT在25个数据集上的表现优于现有方法，缩小了与完全微调的差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17407",
            "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2502.17407",
            "abstract": "Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although \"thinking LLMs\" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.",
            "score": 14,
            "issue_id": 2388,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "f77ddcdca8181036",
            "authors": [
                "Guijin Son",
                "Jiwoo Hong",
                "Hyunwoo Ko",
                "James Thorne"
            ],
            "affiliations": [
                "KAIST AI",
                "OneLineAI",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17407.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#long_context",
                    "#multilingual",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Масштабирование ЯМ во время вывода: вызовы многоязычности",
                    "desc": "Исследование представляет MCLM - многоязычный математический бенчмарк с задачами соревновательного уровня на 55 языках. Авторы тестируют три метода масштабирования во время вывода на двух языковых моделях: Qwen2.5-1.5B Math и MR1-1.5B. Результаты показывают, что эффективность 'думающих' ЯМ сопоставима с традиционными методами масштабирования при одинаковом количестве вычислений. Обнаружено, что масштабирование во время вывода менее эффективно для многоязычных задач по сравнению с задачами на английском языке."
                },
                "en": {
                    "title": "Exploring Test-Time Scaling for Multilingual LLMs",
                    "desc": "This paper investigates the effectiveness of test-time scaling methods for multilingual large language models (LLMs) using a new benchmark called MCLM, which includes math problems in 55 languages. The authors evaluate three scaling techniques: Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF) on two LLMs, Qwen2.5-1.5B Math and MR1-1.5B. Results indicate that ORM with Qwen2.5-1.5B Math achieves the highest score, while BF shows limited improvement across languages compared to English. The findings suggest that traditional scaling methods may perform similarly to advanced LLMs when inference resources are equal, and that test-time scaling may not generalize well to multilingual tasks."
                },
                "zh": {
                    "title": "多语言数学基准与测试时间扩展的探索",
                    "desc": "本文介绍了一种名为MCLM的多语言数学基准，涵盖55种语言的竞争级问题。我们测试了三种测试时间扩展方法：结果奖励建模（ORM）、过程奖励建模（ORM）和预算强制（BF），并在两个多语言大语言模型上进行实验。实验结果显示，使用Qwen2.5-1.5B Math与ORM结合时，在MCLM上得分为35.8，而MR1-1.5B在BF下得分为35.2。尽管“思考型大语言模型”受到关注，但我们发现其性能与传统的扩展方法相当，且测试时间扩展在多语言任务上的效果不如预期。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16033",
            "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
            "url": "https://huggingface.co/papers/2502.16033",
            "abstract": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.",
            "score": 14,
            "issue_id": 2386,
            "pub_date": "2025-02-22",
            "pub_date_card": {
                "ru": "22 февраля",
                "en": "February 22",
                "zh": "2月22日"
            },
            "hash": "3e5fc69b8713e252",
            "authors": [
                "Qianqi Yan",
                "Yue Fan",
                "Hongquan Li",
                "Shan Jiang",
                "Yang Zhao",
                "Xinze Guan",
                "Ching-Chen Kuo",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "University of California, Santa Cruz",
                "eBay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16033.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый рубеж в мультимодальном анализе: оценка способности ИИ распознавать несоответствия",
                    "desc": "Статья представляет новый бенчмарк MMIR для оценки способности мультимодальных больших языковых моделей (MLLM) обнаруживать и анализировать семантические несоответствия в визуально-текстовом контенте. MMIR включает 534 сложных примера с синтетически внедренными ошибками в пяти категориях, требующих рассуждений. Авторы оценили шесть современных MLLM, показав, что модели с специализированными возможностями мультимодальных рассуждений значительно превосходят другие, в то время как модели с открытым исходным кодом особенно уязвимы к ошибкам несоответствия. Результаты исследования подчеркивают необходимость улучшения мультимодальных рассуждений в MLLM."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning: Tackling Inconsistencies in Real-World Content",
                    "desc": "This paper introduces the Multimodal Inconsistency Reasoning (MMIR) benchmark to evaluate how well Multimodal Large Language Models (MLLMs) can identify and reason about inconsistencies in complex visual-textual content. The benchmark consists of 534 samples with various types of semantic mismatches, such as factual contradictions and contextual mismatches. The study finds that models designed for multimodal reasoning perform significantly better than others, but many open-source models struggle with inconsistencies, especially those that span multiple modalities. The results indicate a need for improved cross-modal reasoning capabilities in MLLMs, as current methods show limited effectiveness in handling complex layouts and cross-modal conflicts."
                },
                "zh": {
                    "title": "提升多模态推理能力，解决现实世界的不一致性",
                    "desc": "现有的多模态大型语言模型（MLLMs）主要在一致的视觉-文本输入上进行训练和测试，尚不清楚它们能否处理现实世界中布局丰富内容的不一致性。为此，我们提出了多模态不一致性推理（MMIR）基准，以评估MLLMs在检测和推理语义不匹配方面的能力。MMIR包含534个具有挑战性的样本，涵盖五个推理密集型类别的合成错误。我们的研究表明，具备专门多模态推理能力的模型在处理不一致性时表现优异，而开源模型则特别容易受到不一致性错误的影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17055",
            "title": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam",
            "url": "https://huggingface.co/papers/2502.17055",
            "abstract": "This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featuring momentum reset and spike-aware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. To address these limitations, we propose Stable-SPAM, which incorporates enhanced gradient normalization and clipping techniques. In particular, Stable-SPAM (1) adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; (2) normalizes the entire gradient matrix based on its historical l_2-norm statistics; and (3) inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that Stable-SPAM effectively stabilizes gradient norms in 4-bit LLM training, delivering superior performance compared to Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM outperforms the BF16 LLaMA-1B trained with Adam by up to 2 perplexity. Furthermore, when both models are trained in 4-bit, Stable-SPAM achieves the same loss as Adam while requiring only about half the training steps. Code is available at https://github.com/TianjinYellow/StableSPAM.git.",
            "score": 11,
            "issue_id": 2394,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "7ba1b35f1ba59325",
            "authors": [
                "Tianjin Huang",
                "Haotian Hu",
                "Zhenyu Zhang",
                "Gaojie Jin",
                "Xiang Li",
                "Li Shen",
                "Tianlong Chen",
                "Lu Liu",
                "Qingsong Wen",
                "Zhangyang Wang",
                "Shiwei Liu"
            ],
            "affiliations": [
                "Department of Computer Science, The University of North Carolina at Chapel Hill",
                "Department of Computer Science, University of Exeter",
                "Department of Computer Science, University of Reading",
                "Department of Electrical and Computer Engineering, University of Texas at Austin",
                "Department of Mathematics and Computer Science, Eindhoven University of Technology",
                "Mathematical Institute, University of Oxford",
                "School of Cyber Science and Technology, Sun Yat-sen University",
                "School of the Gifted Young, University of Science and Technology of China",
                "Squirrel Ai Learning"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17055.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Стабильное обучение нейросетей с низкой битностью",
                    "desc": "Статья представляет всестороннюю оценку оптимизаторов для 4-битного обучения нейронных сетей. Авторы выявили, что низкобитная точность усиливает чувствительность к скорости обучения и часто вызывает нестабильность градиентных норм. Предложен новый оптимизатор Stable-SPAM, который включает улучшенные методы нормализации и отсечения градиентов. Эксперименты показывают, что Stable-SPAM эффективно стабилизирует градиентные нормы в 4-битном обучении больших языковых моделей, превосходя по производительности Adam и SPAM."
                },
                "en": {
                    "title": "Stable Training with 4-Bit Precision: Introducing Stable-SPAM",
                    "desc": "This paper evaluates various optimizers for training machine learning models with 4-bit precision, highlighting the challenges posed by low-bit training, such as sensitivity to learning rates and unstable gradient norms. The authors introduce Stable-SPAM, an improved version of the SPAM optimizer, which incorporates advanced techniques for gradient normalization and clipping to enhance stability. Stable-SPAM adaptively adjusts clipping thresholds based on historical gradient data and normalizes gradients using their historical l2-norm statistics, while also maintaining momentum reset to prevent gradient spikes. Experimental results demonstrate that Stable-SPAM not only stabilizes gradient norms but also outperforms traditional optimizers like Adam, achieving better performance with fewer training steps."
                },
                "zh": {
                    "title": "稳定的4位训练优化器：Stable-SPAM",
                    "desc": "本文全面评估了几种最近提出的4位训练优化器，发现低位精度对学习率的敏感性增强，常导致梯度范数不稳定，从而在较高学习率下出现发散。SPAM是一种新型优化器，具有动量重置和尖峰感知梯度裁剪，虽然在不同位数下表现最佳，但在稳定梯度范数方面仍存在困难，需要仔细调整学习率。为了解决这些问题，我们提出了Stable-SPAM，结合了增强的梯度归一化和裁剪技术，能够有效稳定4位LLM训练中的梯度范数。实验表明，Stable-SPAM在性能上优于Adam和SPAM，尤其是在4位LLaMA-1B模型训练中，表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15894",
            "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2502.15894",
            "abstract": "Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality 2times extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables 3times extrapolation by minimal fine-tuning without long videos. Project page and codes: https://riflex-video.github.io/{https://riflex-video.github.io/.}",
            "score": 11,
            "issue_id": 2388,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "3c5243aaae60e8eb",
            "authors": [
                "Min Zhao",
                "Guande He",
                "Yixiao Chen",
                "Hongzhou Zhu",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch ML Center, Tsinghua University",
                "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing Key Laboratory of Big Data Management and Analysis Methods",
                "Pazhou Laboratory (Huangpu)",
                "ShengShu",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15894.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#optimization",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "RIFLEx: Революция в генерации длинных видео без повторений",
                    "desc": "Статья представляет новый метод RIFLEx для генерации длинных видео с сохранением временной когерентности. Авторы анализируют роль частотных компонентов в позиционных эмбеддингах и выявляют ключевую внутреннюю частоту, влияющую на экстраполяцию. RIFLEx уменьшает эту частоту, подавляя повторения и сохраняя согласованность движения. Метод позволяет достичь двукратного увеличения длины видео без дополнительного обучения, а также трехкратного - с минимальной донастройкой."
                },
                "en": {
                    "title": "RIFLEx: Enhancing Video Length Extrapolation with Frequency Insights",
                    "desc": "This paper addresses the challenge of generating longer videos while maintaining temporal coherence. The authors analyze how frequency components in positional embeddings affect video extrapolation and identify a key frequency that influences this behavior. They introduce RIFLEx, a simple yet effective method that reduces this intrinsic frequency to minimize repetition and ensure consistent motion. RIFLEx achieves impressive results, allowing for 2x extrapolation without additional training and improving quality for 3x extrapolation with minimal fine-tuning."
                },
                "zh": {
                    "title": "RIFLEx：高效视频外推的新方法",
                    "desc": "最近视频生成技术的进步使得模型能够合成高质量的长达一分钟的视频。然而，生成更长的视频并保持时间一致性仍然是一个主要挑战，现有的长度外推方法往往导致时间重复或运动减速。在这项工作中，我们系统地分析了位置嵌入中频率成分的作用，并识别出一种主要影响外推行为的内在频率。基于这一见解，我们提出了RIFLEx，这是一种简单而有效的方法，通过降低内在频率来抑制重复，同时保持运动一致性，无需任何额外修改。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17110",
            "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2502.17110",
            "abstract": "The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks.",
            "score": 10,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "1900233d7723f824",
            "authors": [
                "Junyang Wang",
                "Haiyang Xu",
                "Xi Zhang",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Jitao Sang"
            ],
            "affiliations": [
                "Alibaba Group",
                "Beijing Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17110.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#agents"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "Видео-инструкции для ИИ: революция в автоматизации мобильных устройств",
                    "desc": "Mobile-Agent-V - это новая система автоматизации мобильных устройств, использующая видеоинструкции для обучения. Она применяет стратегию скользящего окна и включает видеоагента и агента глубокой рефлексии для выполнения задач в соответствии с указаниями пользователя. Система позволяет пользователям записывать процессы выполнения задач, что дает возможность автономного обучения и эффективного выполнения. Экспериментальные результаты показывают 30% улучшение производительности по сравнению с существующими фреймворками."
                },
                "en": {
                    "title": "Empowering Mobile Automation with Video Guidance",
                    "desc": "The paper presents Mobile-Agent-V, a novel framework designed to enhance task management on mobile devices through automation. It addresses the limitations of existing AI frameworks that lack sufficient operational knowledge by utilizing video guidance to provide rich, actionable insights. The framework employs a sliding window strategy along with a video agent and deep-reflection agent to ensure that the system's actions are in line with user instructions. Experimental results demonstrate that Mobile-Agent-V significantly improves performance by 30% over traditional methods, making it a more efficient solution for mobile automation."
                },
                "zh": {
                    "title": "移动自动化的新突破：Mobile-Agent-V",
                    "desc": "随着移动设备使用的快速增长，任务管理的自动化需求也在增加。许多基于人工智能的框架由于缺乏足够的操作知识而面临挑战。我们提出的Mobile-Agent-V框架利用视频指导提供丰富且经济的操作知识，从而改善移动自动化。通过集成滑动窗口策略和视频代理，Mobile-Agent-V能够高效地学习和执行任务，实验结果显示其性能比现有框架提高了30%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15987",
            "title": "Forecasting Open-Weight AI Model Growth on Hugging Face",
            "url": "https://huggingface.co/papers/2502.15987",
            "abstract": "As the open-weight AI landscape continues to proliferate-with model development, significant investment, and user interest-it becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics in scientific literature, we propose a framework to quantify how an open-weight model's influence evolves. Specifically, we adapt the model introduced by Wang et al. for scientific citations, using three key parameters-immediacy, longevity, and relative fitness-to track the cumulative number of fine-tuned models of an open-weight model. Our findings reveal that this citation-style approach can effectively capture the diverse trajectories of open-weight model adoption, with most models fitting well and outliers indicating unique patterns or abrupt jumps in usage.",
            "score": 9,
            "issue_id": 2388,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "4e2786a48c6c49ab",
            "authors": [
                "Kushal Raj Bhandari",
                "Pin-Yu Chen",
                "Jianxi Gao"
            ],
            "affiliations": [
                "Department of Computer Science, Network Science and Technology Center, Rensselaer Polytechnic Institute, Troy, NY, USA",
                "IBM Research, Yorktown Heights, NY, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15987.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "Измеряем влияние ИИ-моделей через призму научных цитирований",
                    "desc": "Статья предлагает framework для количественной оценки влияния open-weight моделей в сфере ИИ. Авторы адаптируют модель Ванга и др., изначально разработанную для анализа научных цитирований, к контексту машинного обучения. Ключевыми параметрами в этом подходе являются непосредственность, долговечность и относительная приспособленность модели. Исследование показывает, что данный метод эффективно отражает различные траектории adoption open-weight моделей."
                },
                "en": {
                    "title": "Tracking AI Model Influence: A Citation-Style Approach",
                    "desc": "This paper presents a framework to analyze the influence of open-weight AI models over time, similar to how citations are tracked in scientific research. It introduces three parameters: immediacy, longevity, and relative fitness, to measure the impact of these models based on the number of fine-tuned versions created from them. The study finds that this citation-style method can effectively illustrate the varying adoption patterns of open-weight models, highlighting both typical trends and unique outliers. This approach helps predict which models are likely to lead innovation in the AI field."
                },
                "zh": {
                    "title": "预测开放权重模型的影响力演变",
                    "desc": "本文提出了一种框架，用于预测开放权重模型在人工智能生态系统中的影响力演变。我们借鉴了科学文献中的引用动态，使用三个关键参数：即时性、持久性和相对适应性，来跟踪开放权重模型的微调模型数量。研究结果表明，这种引用风格的方法能够有效捕捉开放权重模型采用的多样化轨迹。大多数模型表现良好，而异常值则显示出独特的模式或使用的突然变化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16707",
            "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
            "url": "https://huggingface.co/papers/2502.16707",
            "abstract": "Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues. In this paper, we introduce a novel test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a \"reflection\" mechanism - it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at https://reflect-vlm.github.io.",
            "score": 8,
            "issue_id": 2389,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "e739378d26e8e7ab",
            "authors": [
                "Yunhai Feng",
                "Jiaming Han",
                "Zhuoran Yang",
                "Xiangyu Yue",
                "Sergey Levine",
                "Jianlan Luo"
            ],
            "affiliations": [
                "Cornell University",
                "The Chinese University of Hong Kong",
                "University of California, Berkeley",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16707.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#optimization",
                    "#robotics",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Рефлексивное улучшение VLM для продвинутой роботизированной манипуляции",
                    "desc": "Статья представляет новый подход к улучшению возможностей моделей компьютерного зрения и обработки естественного языка (VLM) для сложных задач роботизированной манипуляции. Авторы предлагают механизм 'рефлексии', который использует генеративную модель для прогнозирования будущих состояний и улучшения принятия решений. Метод включает итеративное улучшение предобученной VLM путем воображения возможных сценариев и анализа потенциальных недостатков. Эксперименты показывают, что предложенный подход значительно превосходит современные коммерческие VLM и другие методы постобработки."
                },
                "en": {
                    "title": "Enhancing VLMs for Better Robotic Manipulation through Reflection",
                    "desc": "This paper addresses the challenges of robotic manipulation over long time frames by enhancing vision-language models (VLMs) with improved physical reasoning. The authors propose a test-time computation framework that incorporates a 'reflection' mechanism, allowing the VLM to predict future states and refine its actions based on these predictions. This iterative process helps the model to better handle complex tasks by addressing potential errors before they occur. Experimental results show that this approach significantly outperforms existing VLMs and other advanced techniques like Monte Carlo Tree Search (MCTS)."
                },
                "zh": {
                    "title": "提升机器人操作的物理推理能力",
                    "desc": "本论文提出了一种新的计算框架，旨在提升视觉语言模型（VLMs）在多阶段机器人操作任务中的物理推理能力。该方法通过一种“反思”机制，迭代地改进预训练的VLM，利用生成模型想象未来的世界状态，并根据这些预测指导动作选择。通过反思潜在的次优决策，进一步优化推理过程。实验结果表明，我们的方法在性能上显著优于多种最先进的商业VLM和其他后训练方法，如蒙特卡洛树搜索（MCTS）。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16701",
            "title": "Beyond Release: Access Considerations for Generative AI Systems",
            "url": "https://huggingface.co/papers/2502.16701",
            "abstract": "Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy.",
            "score": 8,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "db3e8ec873d10ddb",
            "authors": [
                "Irene Solaiman",
                "Rishi Bommasani",
                "Dan Hendrycks",
                "Ariel Herbert-Voss",
                "Yacine Jernite",
                "Aviya Skowron",
                "Andrew Trask"
            ],
            "affiliations": [
                "Center for AI Safety",
                "EleutherAI",
                "Hugging Face",
                "OpenMined",
                "RunSybil",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16701.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "🔓",
                "ru": {
                    "title": "Доступ к ИИ: больше, чем просто релиз",
                    "desc": "Статья рассматривает вопросы доступа к компонентам систем искусственного интеллекта, выходящие за рамки простого решения о релизе. Авторы предлагают фреймворк для анализа доступности по трем осям: ресурсное обеспечение, техническая удобность использования и полезность. На примере четырех языковых моделей демонстрируются схожие соображения по переменным доступа. Исследуется, как масштаб доступа влияет на возможность управления рисками и вмешательства."
                },
                "en": {
                    "title": "Understanding Access: Key to Responsible AI Release",
                    "desc": "This paper discusses how the release of generative AI systems is not just about making components available, but also about how users can effectively engage with these systems. It introduces a framework that breaks down access into three main areas: resourcing, technical usability, and utility, highlighting the importance of each in utilizing AI components. The authors analyze four high-performance language models to illustrate that access variables impact both the risks and benefits associated with these systems. Ultimately, the framework aims to guide better decision-making in AI system releases by considering the broader implications of access."
                },
                "zh": {
                    "title": "解构生成性AI的访问与风险管理",
                    "desc": "这篇论文探讨了生成性人工智能发布决策对系统组件可用性的影响。作者指出，发布并不能解决用户和利益相关者与系统互动的所有问题，访问系统组件的方式也会影响潜在的风险和收益。论文将访问分为三个方面：资源、技术可用性和效用，并在每个类别中明确了不同变量的权衡。通过比较四种高性能语言模型的可访问性，作者展示了如何通过访问变量来评估和管理风险。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16922",
            "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
            "url": "https://huggingface.co/papers/2502.16922",
            "abstract": "Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth, and involve a limited range of temporal entities. To address these limitations, we introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. Extensive experimental results reveal the challenges posed by CTM and highlight potential avenues for improvement.",
            "score": 7,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "8e0389428d77685b",
            "authors": [
                "Zhenglin Wang",
                "Jialong Wu",
                "Pengfei LI",
                "Yong Jiang",
                "Deyu Zhou"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16922.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "⏳",
                "ru": {
                    "title": "CTM: Новый рубеж в оценке временных рассуждений языковых моделей",
                    "desc": "Статья представляет новый бенчмарк для оценки способностей больших языковых моделей (LLM) в области временных рассуждений на основе китайской династической хронологии. Бенчмарк Chinese Time Reasoning (CTM) фокусируется на отношениях между временными сущностями, попарном временном выравнивании и контекстуализированных рассуждениях с учетом культурных особенностей. CTM преодолевает ограничения существующих бенчмарков, которые часто основаны на правилах и имеют ограниченный диапазон временных сущностей. Экспериментальные результаты выявляют сложности, связанные с CTM, и указывают на потенциальные направления для улучшения временных рассуждений в LLM."
                },
                "en": {
                    "title": "Enhancing Temporal Reasoning with Chinese Time Benchmark",
                    "desc": "This paper presents the Chinese Time Reasoning (CTM) benchmark, which aims to enhance the evaluation of Large Language Models (LLMs) in the area of temporal reasoning. Unlike existing benchmarks that are rule-based and limited in scope, CTM focuses on the rich context of Chinese dynastic history, allowing for a deeper assessment of temporal relationships. It emphasizes the importance of cross-entity relationships and pairwise temporal alignment, ensuring that the reasoning is both contextualized and culturally relevant. The results from extensive experiments indicate significant challenges for LLMs in this domain, suggesting areas for future research and improvement."
                },
                "zh": {
                    "title": "中文时间推理：提升机器学习的时间理解能力",
                    "desc": "时间推理是人类认知的基础，对许多实际应用至关重要。尽管大型语言模型在时间推理方面取得了进展，但现有基准主要依赖于基于规则的构建，缺乏上下文深度，并且涉及的时间实体范围有限。为了解决这些问题，我们引入了中文时间推理（CTM），这是一个旨在评估大型语言模型在中国历史时间推理方面的基准。CTM强调跨实体关系、成对时间对齐以及上下文化和文化基础的推理，提供了全面的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15823",
            "title": "InductionBench: LLMs Fail in the Simplest Complexity Class",
            "url": "https://huggingface.co/papers/2502.15823",
            "abstract": "Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.",
            "score": 6,
            "issue_id": 2401,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "59e261b599c661c4",
            "authors": [
                "Wenyue Hua",
                "Tyler Wong",
                "Sun Fei",
                "Liangming Pan",
                "Adam Jardine",
                "William Yang Wang"
            ],
            "affiliations": [
                "Independent Researcher",
                "Rutgers University, New Brunswick",
                "University of Arizona",
                "University of California, Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15823.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#science",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Индуктивное мышление: ахиллесова пята современных языковых моделей",
                    "desc": "Статья представляет новый бенчмарк InductionBench для оценки способностей больших языковых моделей (LLM) к индуктивному мышлению. Авторы отмечают, что существующие бенчмарки в основном фокусируются на дедуктивном мышлении, в то время как индуктивное мышление менее изучено. Эксперименты показали, что даже самые продвинутые LLM испытывают трудности с простейшими классами сложности в подрегулярной иерархии функций. Это указывает на существенный недостаток в способностях современных LLM к индуктивному мышлению."
                },
                "en": {
                    "title": "Unveiling the Inductive Reasoning Gap in LLMs",
                    "desc": "This paper discusses the limitations of large language models (LLMs) in performing inductive reasoning, which is crucial for scientific discovery. While LLMs have excelled in deductive reasoning tasks, such as mathematical and coding challenges, they struggle with tasks that require inferring rules from data. To evaluate this inductive reasoning ability, the authors introduce a new benchmark called InductionBench. Their findings indicate that even the most advanced LLMs have difficulty with basic complexity classes, revealing a significant gap in their reasoning capabilities."
                },
                "zh": {
                    "title": "评估大型语言模型的归纳推理能力",
                    "desc": "大型语言模型（LLMs）在推理方面取得了显著进展，但大多数现有基准主要集中在演绎推理上，例如数学和编程任务。与此不同，归纳推理则是从观察到的数据中推断出潜在规则，这在科学发现中至关重要。为了评估LLMs的归纳推理能力，我们引入了InductionBench，这是一个新的基准测试。实验结果表明，即使是最先进的模型在处理简单的复杂性类别时也存在困难，显示出当前LLMs在归纳推理能力上的不足。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15425",
            "title": "TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning",
            "url": "https://huggingface.co/papers/2502.15425",
            "abstract": "Hierarchical organization is fundamental to biological systems and human societies, yet artificial intelligence systems often rely on monolithic architectures that limit adaptability and scalability. Current hierarchical reinforcement learning (HRL) approaches typically restrict hierarchies to two levels or require centralized training, which limits their practical applicability. We introduce TAME Agent Framework (TAG), a framework for constructing fully decentralized hierarchical multi-agent systems.TAG enables hierarchies of arbitrary depth through a novel LevelEnv concept, which abstracts each hierarchy level as the environment for the agents above it. This approach standardizes information flow between levels while preserving loose coupling, allowing for seamless integration of diverse agent types. We demonstrate the effectiveness of TAG by implementing hierarchical architectures that combine different RL agents across multiple levels, achieving improved performance over classical multi-agent RL baselines on standard benchmarks. Our results show that decentralized hierarchical organization enhances both learning speed and final performance, positioning TAG as a promising direction for scalable multi-agent systems.",
            "score": 6,
            "issue_id": 2394,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "8c7cfe64bc678e90",
            "authors": [
                "Giuseppe Paolo",
                "Abdelhakim Benechehab",
                "Hamza Cherkaoui",
                "Albert Thomas",
                "Balázs Kégl"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15425.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#games",
                    "#architecture",
                    "#agents",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "Децентрализованная иерархия для масштабируемых мультиагентных систем",
                    "desc": "Статья представляет TAME Agent Framework (TAG) - новый подход к созданию полностью децентрализованных иерархических мультиагентных систем. TAG позволяет строить иерархии произвольной глубины с помощью концепции LevelEnv, которая абстрагирует каждый уровень иерархии как окружение для агентов выше. Этот метод стандартизирует поток информации между уровнями, сохраняя при этом слабую связность. Авторы демонстрируют эффективность TAG, реализуя иерархические архитектуры, комбинирующие различные RL-агенты на нескольких уровнях."
                },
                "en": {
                    "title": "Unlocking Scalable Intelligence with Decentralized Hierarchical Learning",
                    "desc": "The paper presents the TAME Agent Framework (TAG), which allows for the creation of decentralized hierarchical multi-agent systems. Unlike traditional hierarchical reinforcement learning methods that are limited to two levels or require centralized training, TAG supports hierarchies of any depth. It introduces the LevelEnv concept, which treats each level of the hierarchy as an environment for the agents above it, facilitating better information flow and integration of various agent types. The results indicate that TAG improves learning speed and performance compared to standard multi-agent reinforcement learning approaches."
                },
                "zh": {
                    "title": "去中心化的层次多智能体系统新框架",
                    "desc": "本文提出了一种新的框架，称为TAME Agent Framework (TAG)，用于构建完全去中心化的层次多智能体系统。TAG通过引入LevelEnv概念，使得层次结构可以达到任意深度，每个层次都被抽象为上层智能体的环境。这种方法标准化了层次之间的信息流，同时保持了松耦合，允许不同类型的智能体无缝集成。实验结果表明，去中心化的层次组织提高了学习速度和最终性能，TAG在可扩展的多智能体系统中展现出良好的前景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16810",
            "title": "Grounded Persuasive Language Generation for Automated Marketing",
            "url": "https://huggingface.co/papers/2502.16810",
            "abstract": "This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin. Our findings suggest a promising LLM-based agentic framework to automate large-scale targeted marketing while ensuring responsible generation using only facts.",
            "score": 5,
            "issue_id": 2401,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "69148b881675400e",
            "authors": [
                "Jibang Wu",
                "Chenghao Yang",
                "Simon Mahns",
                "Chaoqi Wang",
                "Hao Zhu",
                "Fei Fang",
                "Haifeng Xu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Stanford University",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16810.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#optimization",
                    "#agents",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🏠",
                "ru": {
                    "title": "ИИ превосходит человека в написании рекламных текстов для недвижимости",
                    "desc": "Статья представляет агентный фреймворк, использующий большие языковые модели (LLM) для автоматизации создания убедительного маркетингового контента в сфере недвижимости. Фреймворк состоит из трех модулей: модуля обоснования, модуля персонализации и маркетингового модуля. Эксперименты показали, что описания, сгенерированные этим подходом, предпочтительнее описаний, написанных экспертами-людьми. Результаты демонстрируют перспективность использования LLM для автоматизации таргетированного маркетинга с обеспечением ответственной генерации на основе фактов."
                },
                "en": {
                    "title": "Automating Persuasive Real Estate Marketing with LLMs",
                    "desc": "This paper introduces a framework that uses large language models (LLMs) to create effective marketing content, specifically for real estate listings. The framework includes three main components: a Grounding Module that identifies key marketable features, a Personalization Module that tailors content to user preferences, and a Marketing Module that ensures factual accuracy. Through experiments with potential home buyers, the study shows that the LLM-generated descriptions are preferred over those crafted by human experts. The results indicate that this approach can automate targeted marketing while maintaining a focus on factual information."
                },
                "zh": {
                    "title": "智能代理框架：自动化房地产营销内容生成",
                    "desc": "本文提出了一种代理框架，利用大型语言模型（LLMs）自动生成有说服力且基于事实的营销内容，重点应用于房地产列表描述。该方法旨在使生成的内容与用户偏好相一致，同时突出有用的事实属性。代理框架包括三个关键模块：基础模块模拟专家行为以预测可销售特征；个性化模块使内容与用户偏好对齐；营销模块确保事实准确性并包含本地化特征。通过对潜在购房者进行系统的人类实验，结果表明我们的方法生成的营销描述明显优于人类专家撰写的内容。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14132",
            "title": "Can Community Notes Replace Professional Fact-Checkers?",
            "url": "https://huggingface.co/papers/2502.14132",
            "abstract": "Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking.",
            "score": 5,
            "issue_id": 2393,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 февраля",
                "en": "February 19",
                "zh": "2月19日"
            },
            "hash": "b46d43e5e12c38f4",
            "authors": [
                "Nadav Borenstein",
                "Greta Warren",
                "Desmond Elliott",
                "Isabelle Augenstein"
            ],
            "affiliations": [
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14132.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#ethics",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Фактчекинг - ключ к эффективной пользовательской модерации",
                    "desc": "Это исследование анализирует взаимосвязь между профессиональным фактчекингом и пользовательской модерацией в социальных сетях. Используя языковые модели для аннотирования корпуса заметок сообщества Twitter/X, авторы обнаружили, что пользователи ссылаются на источники фактчекинга до пяти раз чаще, чем предполагалось ранее. Особенно важен фактчекинг для заметок о постах, связанных с более широкими дезинформационными нарративами. Результаты показывают, что успешная модерация сообществом сильно зависит от профессионального фактчекинга."
                },
                "en": {
                    "title": "Community Moderation Thrives on Fact-Checking Support",
                    "desc": "This paper investigates the relationship between community moderation and professional fact-checking in combating misinformation on social media platforms like Twitter/X and Meta. Using language models, the authors analyze community notes to identify their attributes, including topics and cited sources. The findings indicate that community notes frequently reference fact-checking sources, particularly when addressing broader misinformation narratives. Ultimately, the study highlights the importance of integrating professional fact-checking into community-driven moderation efforts to enhance their effectiveness."
                },
                "zh": {
                    "title": "社区管理依赖于专业事实核查",
                    "desc": "本文探讨了社交媒体上对抗虚假信息的两种策略：专业机构的事实核查和用户的社区管理。研究发现，社区笔记在引用事实核查来源时，频率比之前报告的高出五倍，尤其是在与更广泛的虚假叙事相关的帖子中。我们的分析表明，成功的社区管理在很大程度上依赖于专业的事实核查。政策变化显示，社交媒体平台正在逐渐转向依赖用户生成的内容，但事实核查仍然至关重要。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17414",
            "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
            "url": "https://huggingface.co/papers/2502.17414",
            "abstract": "We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes.",
            "score": 5,
            "issue_id": 2389,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "ef6013182ae4065e",
            "authors": [
                "Zeyuan Chen",
                "Hongyi Xu",
                "Guoxian Song",
                "You Xie",
                "Chenxu Zhang",
                "Xin Chen",
                "Chao Wang",
                "Di Chang",
                "Linjie Luo"
            ],
            "affiliations": [
                "ByteDance",
                "UC San Diego",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17414.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "💃",
                "ru": {
                    "title": "Танцующие изображения: ИИ оживляет фото под музыку",
                    "desc": "X-Dancer - это новая система для создания анимированных видео танцев из статичного изображения под музыку без предварительного обучения. Она использует объединенную архитектуру трансформера и диффузионной модели для синтеза последовательностей токенов поз тела, головы и рук, которые затем управляют генерацией реалистичных кадров видео. X-Dancer моделирует широкий спектр 2D движений танца, захватывая их нюансированное соответствие музыкальным битам. Экспериментальные результаты показывают, что X-Dancer способна создавать разнообразные и характерные танцевальные видео, превосходя современные методы по разнообразию, выразительности и реалистичности."
                },
                "en": {
                    "title": "Transforming Static Images into Dynamic Dance Videos with Music",
                    "desc": "X-Dancer is a new system that creates realistic dance videos from just one image, using music as a guide. It combines a transformer model that generates dance poses with a diffusion model that turns these poses into video frames. This approach allows it to produce a wide variety of 2D dance movements that match the rhythm of the music, overcoming limitations of traditional 3D methods. The results show that X-Dancer excels in creating diverse and expressive dance videos, outperforming existing techniques."
                },
                "zh": {
                    "title": "X-Dancer：从静态图像生成音乐驱动的舞蹈视频",
                    "desc": "X-Dancer是一种新颖的零样本音乐驱动图像动画管道，可以从单一静态图像生成多样化且长时间的逼真人类舞蹈视频。它的核心是一个统一的变换器-扩散框架，利用自回归变换器模型合成与音乐同步的2D身体、头部和手部姿势的扩展序列，指导扩散模型生成连贯且真实的舞蹈视频帧。与传统方法主要生成3D人类运动不同，X-Dancer通过建模广泛的2D舞蹈动作，捕捉与音乐节拍的细微对齐，解决了数据限制并增强了可扩展性。实验结果表明，X-Dancer在多样性、表现力和真实感方面显著优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15799",
            "title": "Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models",
            "url": "https://huggingface.co/papers/2502.15799",
            "abstract": "Large Language Models (LLMs) have emerged as powerful tools for addressing modern challenges and enabling practical applications. However, their computational expense remains a significant barrier to widespread adoption. Quantization has emerged as a promising technique to democratize access and enable low resource device deployment. Despite these advancements, the safety and trustworthiness of quantized models remain underexplored, as prior studies often overlook contemporary architectures and rely on overly simplistic benchmarks and evaluations. To address this gap, we introduce OpenSafetyMini, a novel open-ended safety dataset designed to better distinguish between models. We evaluate 4 state-of-the-art quantization techniques across LLaMA and Mistral models using 4 benchmarks, including human evaluations. Our findings reveal that the optimal quantization method varies for 4-bit precision, while vector quantization techniques deliver the best safety and trustworthiness performance at 2-bit precision, providing foundation for future research.",
            "score": 3,
            "issue_id": 2400,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "5ac439725f2eb54b",
            "authors": [
                "Artyom Kharinaev",
                "Viktor Moskvoretskii",
                "Egor Shvetsov",
                "Kseniia Studenikina",
                "Bykov Mikhail",
                "Evgeny Burnaev"
            ],
            "affiliations": [
                "Artificial Intelligence Research Institute",
                "HSE University",
                "Skolkovo Institute of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15799.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#security",
                    "#open_source",
                    "#inference",
                    "#dataset"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Безопасное квантование: путь к доступным и надежным LLM",
                    "desc": "Эта статья исследует влияние квантования на безопасность и надежность больших языковых моделей (LLM). Авторы представляют новый набор данных OpenSafetyMini для лучшей оценки моделей. Они сравнивают 4 современных метода квантования на моделях LLaMA и Mistral, используя 4 бенчмарка, включая оценку людьми. Результаты показывают, что оптимальный метод квантования варьируется для 4-битной точности, а методы векторного квантования дают лучшие результаты при 2-битной точности."
                },
                "en": {
                    "title": "Democratizing LLMs: Ensuring Safety in Quantization",
                    "desc": "This paper discusses the challenges of using Large Language Models (LLMs) due to their high computational costs. It introduces quantization as a method to make these models more accessible for low-resource devices. The authors present OpenSafetyMini, a new safety dataset that helps evaluate the safety and trustworthiness of quantized models more effectively. Their experiments with various quantization techniques on LLaMA and Mistral models show that different methods perform best at different precision levels, highlighting the need for careful evaluation in model deployment."
                },
                "zh": {
                    "title": "量化技术助力安全可信的语言模型",
                    "desc": "大型语言模型（LLMs）在解决现代挑战和实际应用中展现出强大的能力，但其计算成本仍然是广泛应用的主要障碍。量化技术被认为是一种有前景的方法，可以降低资源需求并促进低资源设备的部署。尽管如此，量化模型的安全性和可信度仍然未得到充分研究，之前的研究往往忽视了现代架构，并依赖过于简单的基准和评估。为了解决这一问题，我们引入了OpenSafetyMini，一个新颖的开放式安全数据集，以更好地区分模型，并评估了四种最先进的量化技术在LLaMA和Mistral模型上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14429",
            "title": "Early-Exit and Instant Confidence Translation Quality Estimation",
            "url": "https://huggingface.co/papers/2502.14429",
            "abstract": "Quality estimation is omnipresent in machine translation, for both evaluation and generation. Unfortunately, quality estimation models are often opaque and computationally expensive, making them impractical to be part of large-scale pipelines. In this work, we tackle two connected challenges: (1) reducing the cost of quality estimation at scale, and (2) developing an inexpensive uncertainty estimation method for quality estimation. To address the latter, we introduce Instant Confidence COMET, an uncertainty-aware quality estimation model that matches the performance of previous approaches at a fraction of their costs. We extend this to Early-Exit COMET, a quality estimation model that can compute quality scores and associated confidences already at early model layers, allowing us to early-exit computations and reduce evaluation costs. We also apply our model to machine translation reranking. We combine Early-Exit COMET with an upper confidence bound bandit algorithm to find the best candidate from a large pool without having to run the full evaluation model on all candidates. In both cases (evaluation and reranking) our methods reduce the required compute by 50% with very little degradation in performance.",
            "score": 2,
            "issue_id": 2401,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "27764bf1a3fe36b3",
            "authors": [
                "Vilém Zouhar",
                "Maike Züfle",
                "Beni Egressy",
                "Julius Cheng",
                "Jan Niehues"
            ],
            "affiliations": [
                "ETH Zurich",
                "Heidelberg Institute for Theoretical Studies",
                "Karlsruhe Institute of Technology",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14429.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#data",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Быстрая и эффективная оценка качества машинного перевода",
                    "desc": "В этой статье представлен новый подход к оценке качества машинного перевода под названием Instant Confidence COMET. Модель способна эффективно оценивать качество перевода и уровень уверенности в оценке, при этом требуя меньше вычислительных ресурсов по сравнению с существующими методами. Авторы также предлагают Early-Exit COMET - модификацию, позволяющую получать предварительные оценки на ранних слоях нейронной сети. Применение этих подходов позволяет сократить вычислительные затраты на 50% при минимальном снижении точности оценки качества перевода."
                },
                "en": {
                    "title": "Efficient Quality Estimation in Machine Translation",
                    "desc": "This paper addresses the challenges of quality estimation in machine translation, focusing on making it more efficient and cost-effective. The authors introduce Instant Confidence COMET, a model that provides uncertainty-aware quality estimates while significantly reducing computational costs. They further develop Early-Exit COMET, which allows for early evaluation of quality scores, enabling quicker decisions without full model evaluations. By integrating these models with a bandit algorithm for reranking, they achieve a 50% reduction in computational requirements with minimal impact on performance."
                },
                "zh": {
                    "title": "降低机器翻译质量估计的计算成本",
                    "desc": "本研究关注机器翻译中的质量估计问题，旨在降低大规模质量估计的计算成本。我们提出了Instant Confidence COMET模型，它在保持性能的同时，显著减少了计算开销。进一步地，我们开发了Early-Exit COMET模型，能够在模型的早期层计算质量分数和置信度，从而提前结束计算，降低评估成本。通过结合上置信界限算法，我们的模型在评估和重排序中都能将计算需求减少50%，且性能几乎没有下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15122",
            "title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
            "url": "https://huggingface.co/papers/2502.15122",
            "abstract": "We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the datasets in these benchmarks are small, with median sizes of 217 and 255 examples, respectively. In consequence they favour a narrow subspace of models that are optimised to achieve low classification error on a wide variety of smaller datasets, that is, models that minimise variance, and give little weight to computational issues such as scalability. Our hope is to diversify the field by introducing benchmarks using larger datasets. We believe that there is enormous potential for new progress in the field by engaging with the theoretical and practical challenges of learning effectively from larger quantities of data.",
            "score": 2,
            "issue_id": 2389,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "0befd2d9ae63b48e",
            "authors": [
                "Angus Dempster",
                "Navid Mohammadi Foumani",
                "Chang Wei Tan",
                "Lynn Miller",
                "Amish Mishra",
                "Mahsa Salehi",
                "Charlotte Pelletier",
                "Daniel F. Schmidt",
                "Geoffrey I. Webb"
            ],
            "affiliations": [
                "Monash University, Melbourne, Australia",
                "Universite Bretagne Sud, IRISA, Vannes, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15122.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🦖",
                "ru": {
                    "title": "Большие данные для больших прорывов в классификации временных рядов",
                    "desc": "MONSTER - это новый набор крупных датасетов для классификации временных рядов. Он создан в противовес существующим бенчмаркам UCR и UEA, которые содержат небольшие наборы данных. MONSTER призван расширить область исследований, стимулируя разработку моделей, эффективных на больших объемах данных. Авторы надеются, что это откроет новые теоретические и практические возможности в сфере машинного обучения на временных рядах."
                },
                "en": {
                    "title": "Unlocking Potential with Large Datasets in Time Series Classification",
                    "desc": "This paper presents MONSTER, a new repository designed for evaluating time series classification using large datasets. Current benchmarks, like UCR and UEA, consist of small datasets that limit the types of models that can be effectively tested. By providing larger datasets, MONSTER aims to encourage the development of models that can handle scalability and learn from more extensive data. The authors believe that this will lead to significant advancements in the field of time series classification."
                },
                "zh": {
                    "title": "引入大规模数据集，推动时间序列分类进步",
                    "desc": "我们介绍了MONSTER——蒙纳士可扩展时间序列评估库，这是一个包含大量时间序列分类数据集的集合。现有的时间序列分类基准（如UCR和UEA）中的数据集较小，限制了模型的多样性。MONSTER旨在通过引入更大的数据集来丰富这一领域，促进模型在处理大规模数据时的有效学习。我们相信，面对更大数据量的理论和实践挑战，将为时间序列分类领域带来新的进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14247",
            "title": "Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and Texture Generation",
            "url": "https://huggingface.co/papers/2502.14247",
            "abstract": "This report presents a comprehensive framework for generating high-quality 3D shapes and textures from diverse input prompts, including single images, multi-view images, and text descriptions. The framework consists of 3D shape generation and texture generation. (1). The 3D shape generation pipeline employs a Variational Autoencoder (VAE) to encode implicit 3D geometries into a latent space and a diffusion network to generate latents conditioned on input prompts, with modifications to enhance model capacity. An alternative Artist-Created Mesh (AM) generation approach is also explored, yielding promising results for simpler geometries. (2). Texture generation involves a multi-stage process starting with frontal images generation followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. A consistency scheduler is plugged into every stage, to enforce pixel-wise consistency among multi-view textures during inference, ensuring seamless integration.   The pipeline demonstrates effective handling of diverse input formats, leveraging advanced neural architectures and novel methodologies to produce high-quality 3D content. This report details the system architecture, experimental results, and potential future directions to improve and expand the framework. The source code and pretrained weights are released at: https://github.com/Tencent/Tencent-XR-3DGen.",
            "score": 1,
            "issue_id": 2405,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "bdddc27023097d3b",
            "authors": [
                "Jiayu Yang",
                "Taizhang Shang",
                "Weixuan Sun",
                "Xibin Song",
                "Ziang Cheng",
                "Senbo Wang",
                "Shenzhou Chen",
                "Weizhe Liu",
                "Hongdong Li",
                "Pan Ji"
            ],
            "affiliations": [
                "Tencent XR Vision Labs",
                "The Australian National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14247.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в 3D-генерации: от изображений и текста к реалистичным объектам",
                    "desc": "Статья представляет комплексную систему для генерации высококачественных 3D-форм и текстур на основе различных входных данных, включая одиночные изображения, многоракурсные изображения и текстовые описания. Для генерации 3D-форм используется вариационный автоэнкодер (VAE) и диффузионная сеть, а также исследуется подход с использованием сеток, созданных художниками. Генерация текстур включает многоэтапный процесс, начиная с создания фронтальных изображений и заканчивая уточнением многоракурсных текстур высокого разрешения. Система демонстрирует эффективную обработку различных форматов входных данных, используя передовые нейронные архитектуры и новые методологии для создания высококачественного 3D-контента."
                },
                "en": {
                    "title": "Transforming Inputs into Stunning 3D Creations!",
                    "desc": "This paper introduces a framework for creating high-quality 3D shapes and textures from various input types, such as images and text. It utilizes a Variational Autoencoder (VAE) for 3D shape generation, which encodes geometries into a latent space and employs a diffusion network for generating shapes based on input prompts. For texture generation, the framework follows a multi-stage process that includes generating frontal and multi-view images, converting RGB to PBR textures, and refining textures to high resolution, all while maintaining pixel-wise consistency. The report discusses the architecture, results, and future improvements for this innovative system, which is designed to handle diverse input formats effectively."
                },
                "zh": {
                    "title": "高质量3D生成的全面框架",
                    "desc": "本报告提出了一个全面的框架，用于从多种输入提示生成高质量的3D形状和纹理。该框架包括3D形状生成和纹理生成两个部分。3D形状生成使用变分自编码器（VAE）将隐式3D几何体编码到潜在空间，并通过扩散网络根据输入提示生成潜在变量，同时对模型能力进行了增强。纹理生成则采用多阶段过程，确保多视图纹理在推理过程中保持像素级一致性，从而实现无缝集成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15920",
            "title": "Self-Taught Agentic Long Context Understanding",
            "url": "https://huggingface.co/papers/2502.15920",
            "abstract": "Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.",
            "score": 1,
            "issue_id": 2401,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "30be0556f4cf3d68",
            "authors": [
                "Yufan Zhuang",
                "Xiaodong Yu",
                "Jialian Wu",
                "Ximeng Sun",
                "Ze Wang",
                "Jiang Liu",
                "Yusheng Su",
                "Jingbo Shang",
                "Zicheng Liu",
                "Emad Barsoum"
            ],
            "affiliations": [
                "AMD",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15920.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#agents",
                    "#multimodal",
                    "#training",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AgenticLU: Улучшение понимания длинных контекстов через самоуточнение и контекстуальное заземление",
                    "desc": "Статья представляет AgenticLU - фреймворк для улучшения понимания длинных контекстов языковыми моделями. В основе лежит метод Chain-of-Clarifications, где модель уточняет понимание через самогенерируемые вопросы. Авторы применяют двухэтапное дообучение модели: supervised finetuning и direct preference optimization. Эксперименты на семи задачах показывают, что AgenticLU превосходит современные методы и специализированные модели для длинных контекстов."
                },
                "en": {
                    "title": "Enhancing Long-Context Understanding with Self-Clarification",
                    "desc": "The paper introduces Agentic Long-Context Understanding (AgenticLU), a framework aimed at improving large language models' ability to answer complex questions by using self-clarification and contextual grounding. Central to this framework is the Chain-of-Clarifications (CoC), which allows models to ask and answer their own clarification questions while retrieving relevant context. The approach employs a tree search strategy to optimize the inference process, achieving high answer recall rates on long-context tasks. Additionally, the framework incorporates a two-stage finetuning process to enhance the model's reasoning capabilities and efficiency in generating clarifications and retrieving context."
                },
                "zh": {
                    "title": "提升长上下文理解的智能框架",
                    "desc": "本文提出了一种名为Agentic Long-Context Understanding（AgenticLU）的框架，旨在提高大型语言模型（LLM）对复杂问题的理解能力。该框架通过自我澄清和上下文检索的结合，帮助模型更好地处理长上下文问题。核心方法是链式澄清（Chain-of-Clarifications，CoC），模型通过生成澄清问题来逐步完善理解。实验结果表明，AgenticLU在多个长上下文任务中表现优于现有的最先进方法，能够有效进行多跳推理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16622",
            "title": "Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures",
            "url": "https://huggingface.co/papers/2502.16622",
            "abstract": "The COVID-19 pandemic strained healthcare resources and prompted discussion about how machine learning can alleviate physician burdens and contribute to diagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but few studies predict the severity of a patient's condition from CXRs. In this study, we produce a large COVID severity dataset by merging three sources and investigate the efficacy of transfer learning using ImageNet- and CXR-pretrained models and vision transformers (ViTs) in both severity regression and classification tasks. A pretrained DenseNet161 model performed the best on the three class severity prediction problem, reaching 80% accuracy overall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases, respectively. The ViT had the best regression results, with a mean absolute error of 0.5676 compared to radiologist-predicted severity scores. The project's source code is publicly available.",
            "score": 1,
            "issue_id": 2401,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "00166601e278b4b2",
            "authors": [
                "Luis Lara",
                "Lucia Eve Berger",
                "Rajesh Raju",
                "Shawn Whitfield"
            ],
            "affiliations": [
                "Mila Quebec AI Institute, Canada",
                "Universite de Montreal, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16622.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#dataset",
                    "#training",
                    "#cv",
                    "#transfer_learning",
                    "#open_source"
                ],
                "emoji": "🫁",
                "ru": {
                    "title": "ИИ оценивает тяжесть COVID-19 по рентгену не хуже врачей",
                    "desc": "Исследование посвящено применению машинного обучения для оценки тяжести COVID-19 по рентгеновским снимкам грудной клетки. Авторы создали большой набор данных, объединив три источника, и изучили эффективность трансферного обучения с использованием моделей, предобученных на ImageNet и рентгеновских снимках, а также визуальных трансформеров (ViT). Наилучшие результаты в задаче классификации тяжести на три класса показала предобученная модель DenseNet161, достигнув общей точности 80%. В задаче регрессии лучший результат продемонстрировал ViT, со средней абсолютной ошибкой 0,5676 по сравнению с оценками рентгенологов."
                },
                "en": {
                    "title": "Enhancing COVID-19 Severity Prediction with Machine Learning",
                    "desc": "This paper addresses the challenge of predicting COVID-19 severity from chest x-rays (CXRs) using machine learning techniques. It creates a comprehensive dataset by combining data from three different sources to enhance the model's training. The study evaluates the performance of various pretrained models, including DenseNet161 and vision transformers (ViTs), for both classification and regression tasks related to severity prediction. The results show that DenseNet161 achieved the highest accuracy for classifying severity levels, while ViTs excelled in providing precise regression outcomes."
                },
                "zh": {
                    "title": "利用机器学习提升COVID-19诊断效率",
                    "desc": "本研究探讨了机器学习在COVID-19诊断中的应用，特别是通过胸部X光片（CXR）预测患者病情的严重程度。我们合并了三个数据源，创建了一个大型COVID严重性数据集，并使用迁移学习方法进行研究。经过实验，预训练的DenseNet161模型在三类严重性预测中表现最佳，整体准确率达到80%。此外，视觉变换器（ViT）在回归任务中表现出色，平均绝对误差为0.5676，优于放射科医生的预测结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15167",
            "title": "M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment",
            "url": "https://huggingface.co/papers/2502.15167",
            "abstract": "The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehensive framework for AGI quality assessment that is Multimodal, Multi-Round, and Multi-Aspect. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) as joint text and image encoders and distills advanced captioning capabilities from online MLLMs into a local model via Low-Rank Adaptation (LoRA) fine-tuning. The framework includes a structured multi-round evaluation mechanism, where intermediate image descriptions are generated to provide deeper insights into the quality, correspondence, and authenticity aspects. To align predictions with human perceptual judgments, a predictor constructed by an xLSTM and a regression head is incorporated to process sequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments conducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves state-of-the-art performance, effectively capturing nuanced aspects of AGI quality. Furthermore, cross-dataset validation confirms its strong generalizability. The code is available at https://github.com/strawhatboy/M3-AGIQA.",
            "score": 1,
            "issue_id": 2392,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "457cffd47ddda6bb",
            "authors": [
                "Chuan Cui",
                "Kejiang Chen",
                "Zhihua Wei",
                "Wen Shen",
                "Weiming Zhang",
                "Nenghai Yu"
            ],
            "affiliations": [
                "Anhui Province Key Laboratory of Cyberspace Security Situation Awareness and Evaluation",
                "Hefei High-Dimensional Data Technology Co.,Ltd.",
                "School and Technology, Tongji University, Shanghai, of Computer Science China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15167.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#optimization",
                    "#benchmark",
                    "#interpretability",
                    "#agi"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Комплексная оценка качества ИИ-изображений с помощью мультимодального анализа",
                    "desc": "В статье представлена новая система оценки качества изображений, созданных искусственным интеллектом - M3-AGIQA. Эта система использует мультимодальные языковые модели для анализа изображений и текста, применяя многоэтапный подход оценки. M3-AGIQA учитывает различные аспекты качества, включая визуальное восприятие, соответствие запросу и аутентичность. Эксперименты показали, что предложенный метод превосходит существующие подходы и хорошо обобщается на разные наборы данных."
                },
                "en": {
                    "title": "M3-AGIQA: A New Standard for Evaluating AI-Generated Images",
                    "desc": "This paper presents M3-AGIQA, a new framework designed to evaluate the quality of AI-generated images by considering various factors like perceptual quality and authenticity. It utilizes Multimodal Large Language Models (MLLMs) to analyze both text and images, enhancing the evaluation process through Low-Rank Adaptation (LoRA) fine-tuning. The framework features a multi-round evaluation system that generates intermediate descriptions to provide detailed insights into the quality of the images. The results show that M3-AGIQA outperforms existing methods and is highly adaptable across different datasets, making it a robust tool for assessing AGI quality."
                },
                "zh": {
                    "title": "全面评估AGI质量的M3-AGIQA框架",
                    "desc": "随着人工智能生成图像（AGI）模型的快速发展，评估其质量面临着多维度的挑战，包括感知质量、提示对应性和真实性。为了解决这些问题，我们提出了M3-AGIQA，这是一个全面的AGI质量评估框架，具有多模态、多轮次和多方面的特点。该方法利用多模态大型语言模型（MLLMs）作为文本和图像的联合编码器，并通过低秩适应（LoRA）微调将在线MLLMs的高级描述能力提炼到本地模型中。我们的框架包括一个结构化的多轮评估机制，通过生成中间图像描述来深入洞察质量、对应性和真实性的各个方面。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15919",
            "title": "Mind the Gap! Static and Interactive Evaluations of Large Audio Models",
            "url": "https://huggingface.co/papers/2502.15919",
            "abstract": "As AI chatbots become ubiquitous, voice interaction presents a compelling way to enable rapid, high-bandwidth communication for both semantic and social signals. This has driven research into Large Audio Models (LAMs) to power voice-native experiences. However, aligning LAM development with user goals requires a clear understanding of user needs and preferences to establish reliable progress metrics. This study addresses these challenges by introducing an interactive approach to evaluate LAMs and collecting 7,500 LAM interactions from 484 participants. Through topic modeling of user queries, we identify primary use cases for audio interfaces. We then analyze user preference rankings and qualitative feedback to determine which models best align with user needs. Finally, we evaluate how static benchmarks predict interactive performance - our analysis reveals no individual benchmark strongly correlates with interactive results (tau leq 0.33 for all benchmarks). While combining multiple coarse-grained features yields modest predictive power (R^2=0.30), only two out of twenty datasets on spoken question answering and age prediction show significantly positive correlations. This suggests a clear need to develop LAM evaluations that better correlate with user preferences.",
            "score": 0,
            "issue_id": 2405,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "365bc5c397c701f6",
            "authors": [
                "Minzhi Li",
                "William Barr Held",
                "Michael J Ryan",
                "Kunat Pipatanakul",
                "Potsawee Manakul",
                "Hao Zhu",
                "Diyi Yang"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Institute for Infocomm Research (I2R), A*STAR",
                "National University of Singapore",
                "SCB 10X, SCBX Group",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15919.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#alignment",
                    "#dataset",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Голос пользователя важнее бенчмарков в оценке аудио ИИ",
                    "desc": "Статья исследует оценку Больших Аудио Моделей (LAM) для голосовых интерфейсов. Авторы собрали 7500 взаимодействий с LAM от 484 участников для определения основных сценариев использования и предпочтений пользователей. Анализ показал слабую корреляцию между статическими бенчмарками и интерактивной производительностью моделей. Исследование подчеркивает необходимость разработки новых методов оценки LAM, лучше отражающих пользовательские предпочтения."
                },
                "en": {
                    "title": "Enhancing Voice Interaction: Aligning LAMs with User Needs",
                    "desc": "This paper explores the development of Large Audio Models (LAMs) for enhancing voice interactions in AI chatbots. It emphasizes the importance of understanding user needs to create effective evaluation metrics for these models. The authors collected data from 484 participants, analyzing their interactions to identify key use cases and preferences for audio interfaces. The findings indicate that current static benchmarks do not reliably predict interactive performance, highlighting the need for improved evaluation methods that align with user preferences."
                },
                "zh": {
                    "title": "提升语音交互体验，满足用户需求！",
                    "desc": "随着人工智能聊天机器人的普及，语音交互成为一种快速、高效的沟通方式。本文研究了大型音频模型（LAMs），旨在提升语音原生体验。我们通过收集484名参与者的7500个LAM交互数据，分析用户需求和偏好，以评估LAM的表现。研究发现，现有的静态基准测试与交互性能之间的相关性较低，强调了需要开发更符合用户偏好的LAM评估方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17237",
            "title": "MegaLoc: One Retrieval to Place Them All",
            "url": "https://huggingface.co/papers/2502.17237",
            "abstract": "Retrieving images from the same location as a given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However, existing solutions are built to specifically work for one of these tasks, and are known to fail when the requirements slightly change or when they meet out-of-distribution data. In this paper we combine a variety of existing methods, training techniques, and datasets to train a retrieval model, called MegaLoc, that is performant on multiple tasks. We find that MegaLoc (1) achieves state of the art on a large number of Visual Place Recognition datasets, (2) impressive results on common Landmark Retrieval datasets, and (3) sets a new state of the art for Visual Localization on the LaMAR datasets, where we only changed the retrieval method to the existing localization pipeline. The code for MegaLoc is available at https://github.com/gmberton/MegaLoc",
            "score": 0,
            "issue_id": 2397,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "7eada0ecd3a7e714",
            "authors": [
                "Gabriele Berton",
                "Carlo Masone"
            ],
            "affiliations": [
                "Polytechnic of Turin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17237.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "MegaLoc: универсальный инструмент для визуальной навигации и локализации",
                    "desc": "MegaLoc - это модель для извлечения изображений, которая эффективна в различных задачах компьютерного зрения. Она объединяет существующие методы, техники обучения и наборы данных для достижения универсальности. MegaLoc показывает высокие результаты в задачах визуального распознавания мест, поиска ориентиров и визуальной локализации. Модель устанавливает новый стандарт производительности на нескольких наборах данных, демонстрируя свою универсальность и эффективность."
                },
                "en": {
                    "title": "MegaLoc: One Model to Rule Multiple Vision Tasks",
                    "desc": "This paper presents MegaLoc, a versatile retrieval model designed to improve performance across various computer vision tasks such as Visual Place Recognition, Landmark Retrieval, and Visual Localization. Unlike existing models that are tailored for specific tasks, MegaLoc integrates multiple methods and training techniques to adapt to different requirements and out-of-distribution data. The authors demonstrate that MegaLoc achieves state-of-the-art results on numerous Visual Place Recognition datasets and excels in Landmark Retrieval, while also setting a new benchmark for Visual Localization on the LaMAR datasets. This approach highlights the importance of flexibility in model design for effective image retrieval in diverse applications."
                },
                "zh": {
                    "title": "MegaLoc：多任务图像检索的新突破",
                    "desc": "本论文提出了一种名为MegaLoc的检索模型，旨在解决多个计算机视觉任务中的图像检索问题。现有的解决方案通常只针对特定任务，难以适应不同的需求或分布外数据。MegaLoc结合了多种现有方法、训练技术和数据集，能够在多个任务上表现出色。研究表明，MegaLoc在多个视觉位置识别数据集上达到了最先进的水平，并在常见的地标检索数据集上也取得了令人印象深刻的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13074",
            "title": "The snake in the Brownian sphere",
            "url": "https://huggingface.co/papers/2502.13074",
            "abstract": "The Brownian sphere is a random metric space, homeomorphic to the two-dimensional sphere, which arises as the universal scaling limit of many types of random planar maps. The direct construction of the Brownian sphere is via a continuous analogue of the Cori--Vauquelin--Schaeffer (CVS) bijection. The CVS bijection maps labeled trees to planar maps, and the continuous version maps Aldous' continuum random tree with Brownian labels (the Brownian snake) to the Brownian sphere. In this work, we describe the inverse of the continuous CVS bijection, by constructing the Brownian snake as a measurable function of the Brownian sphere. Special care is needed to work with the orientation of the Brownian sphere.",
            "score": 0,
            "issue_id": 2392,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "1723a9c12f61fd5f",
            "authors": [
                "Omer Angel",
                "Emmanuel Jacob",
                "Brett Kolesnik",
                "Grégory Miermont"
            ],
            "affiliations": [
                "Institut Universitaire de France",
                "University of British Columbia, Department of Mathematics",
                "University of Warwick, Department of Statistics",
                "École Normale Supérieure de Lyon, Unité de Mathématiques Pures et Appliquées"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13074.jpg",
            "data": {
                "categories": [
                    "#math"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Обратное отображение броуновской сферы в случайное дерево",
                    "desc": "Статья описывает обратное отображение непрерывной версии биекции Кори-Вокелена-Шеффера (CVS) для броуновской сферы. Броуновская сфера - это случайное метрическое пространство, гомеоморфное двумерной сфере, возникающее как универсальный скейлинговый предел многих типов случайных планарных карт. Непрерывная версия биекции CVS отображает континуальное случайное дерево Олдоса с броуновскими метками (броуновская змея) на броуновскую сферу. Авторы конструируют броуновскую змею как измеримую функцию от броуновской сферы, уделяя особое внимание ориентации броуновской сферы."
                },
                "en": {
                    "title": "Connecting Random Trees to the Brownian Sphere",
                    "desc": "The paper introduces the Brownian sphere, a random metric space that resembles a two-dimensional sphere and serves as a scaling limit for various random planar maps. It details the continuous version of the Cori--Vauquelin--Schaeffer (CVS) bijection, which connects labeled trees to planar maps. The authors construct the Brownian snake, a continuous object derived from the Brownian sphere, as a measurable function, emphasizing the importance of orientation in this context. This work provides a deeper understanding of the relationship between random trees and planar maps through the lens of stochastic processes."
                },
                "zh": {
                    "title": "布朗球：随机平面图的极限",
                    "desc": "布朗球是一个随机度量空间，与二维球面同胚，是许多随机平面图的普遍缩放极限。布朗球的直接构造是通过Cori-Vauquelin-Schaeffer (CVS)双射的连续类比实现的。CVS双射将标记树映射到平面图，而连续版本则将Aldous的连续随机树与布朗标签（布朗蛇）映射到布朗球。本文描述了连续CVS双射的逆过程，通过将布朗蛇构造为布朗球的可测函数来实现，特别注意布朗球的方向性。"
                }
            }
        }
    ],
    "link_prev": "2025-02-24.html",
    "link_next": "2025-02-26.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "24.02",
        "en": "02/24",
        "zh": "2月24日"
    },
    "short_date_next": {
        "ru": "26.02",
        "en": "02/26",
        "zh": "2月26日"
    },
    "categories": {
        "#dataset": 11,
        "#data": 3,
        "#benchmark": 13,
        "#agents": 5,
        "#cv": 4,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 1,
        "#audio": 3,
        "#video": 4,
        "#multimodal": 8,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 6,
        "#healthcare": 1,
        "#training": 11,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 7,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 1,
        "#optimization": 11,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 5,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章的主要目标是创建一个计算资源和训练数据有限的多任务通用感知模型。作者使用了预训练的文本到图像扩散模型。实验结果表明，DICEPTION 在多个感知任务上表现优异，仅使用了 SAM-vit-h 模型 0.06% 的数据。DICEPTION 通过颜色编码统一了各种感知任务，使得预训练的文本到图像模型可以被充分利用。在适应其他任务时，模型只需在少量图像和参数上进行微调。",
        "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
        "pinyin": "这篇文章的主要目标是创建一个计算资源和训练数据有限的多任务通用感知模型。\nZhè piān wénzhāng de zhǔyào mùbiāo shì chuàngjiàn yīgè jìsuàn zīyuán hé xùnliàn shùjù yǒu xiàn de duō rènwù tōngyòng gǎnjué móxíng.\n\n作者使用了预训练的文本到图像扩散模型。\nZuòzhě shǐyòngle yù xùnliàn de wénběn dào túxiàng kuòsàn móxíng.\n\n实验结果表明，DICEPTION 在多个感知任务上表现优异，仅使用了 SAM-vit-h 模型 0.06% 的数据。\nShíyàn jiéguǒ biǎomíng, DICEPTION zài duō gè gǎnjué rènwù shàng biǎoxiàn yōuyì, jǐn shǐyòngle SAM-vit-h móxíng 0.06% de shùjù.\n\nDICEPTION 通过颜色编码统一了各种感知任务，使得预训练的文本到图像模型可以被充分利用。\nDICEPTION tōngguò yánsè biānmǎ tǒngyīle gèzhǒng gǎnjué rènwù, shǐdé yù xùnliàn de wénběn dào túxiàng móxíng kěyǐ bèi chōngfēn lìyòng.\n\n在适应其他任务时，模型只需在少量图像和参数上进行微调。\nZài shìyìng qítā rènwù shí, móxíng zhǐ xū zài shǎoliàng túxiàng hé cānshù shàng jìnxíng wēitiáo.",
        "vocab": "[\n    {\"word\": \"创建\", \"pinyin\": \"chuàng jiàn\", \"trans\": \"create\"},\n    {\"word\": \"计算资源\", \"pinyin\": \"jì suàn zī yuán\", \"trans\": \"computational resources\"},\n    {\"word\": \"训练数据\", \"pinyin\": \"xùn liàn shù jù\", \"trans\": \"training data\"},\n    {\"word\": \"有限\", \"pinyin\": \"yǒu xiàn\", \"trans\": \"limited\"},\n    {\"word\": \"多任务\", \"pinyin\": \"duō rèn wù\", \"trans\": \"multi-task\"},\n    {\"word\": \"通用\", \"pinyin\": \"tōng yòng\", \"trans\": \"general-purpose\"},\n    {\"word\": \"感知\", \"pinyin\": \"gǎn zhī\", \"trans\": \"perception\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-trained\"},\n    {\"word\": \"文本到图像\", \"pinyin\": \"wén běn dào tú xiàng\", \"trans\": \"text-to-image\"},\n    {\"word\": \"扩散\", \"pinyin\": \"kuò sàn\", \"trans\": \"diffusion\"},\n    {\"word\": \"表明\", \"pinyin\": \"biǎo míng\", \"trans\": \"indicate\"},\n    {\"word\": \"优异\", \"pinyin\": \"yōu yì\", \"trans\": \"excellent\"},\n    {\"word\": \"仅\", \"pinyin\": \"jǐn\", \"trans\": \"only\"},\n    {\"word\": \"统一\", \"pinyin\": \"tǒng yī\", \"trans\": \"unify\"},\n    {\"word\": \"编码\", \"pinyin\": \"biān mǎ\", \"trans\": \"encoding\"},\n    {\"word\": \"充分利用\", \"pinyin\": \"chōng fèn lì yòng\", \"trans\": \"fully utilize\"},\n    {\"word\": \"适应\", \"pinyin\": \"shì yìng\", \"trans\": \"adapt\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tune\"}\n]",
        "trans": "The primary objective of this article is to create a general-purpose multi-task perception model with limited computational resources and training data. The authors utilized a pre-trained text-to-image diffusion model. Experimental results demonstrate that DICEPTION performs exceptionally well on multiple perception tasks, using only 0.06% of the data from the SAM-vit-h model. DICEPTION unifies various perception tasks through color coding, enabling the full utilization of the pre-trained text-to-image model. When adapting to other tasks, the model requires only fine-tuning on a small number of images and parameters.",
        "update_ts": "2025-02-25 09:11"
    }
}