{
    "date": {
        "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 24",
        "zh": "12æœˆ24æ—¥"
    },
    "time_utc": "2025-12-24 12:44",
    "weekday": 2,
    "issue_id": 222,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.20619",
            "title": "SemanticGen: Video Generation in Semantic Space",
            "url": "https://huggingface.co/papers/2512.20619",
            "abstract": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.",
            "score": 72,
            "issue_id": 214,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "5437eab0e96a3d62",
            "authors": [
                "Jianhong Bai",
                "Xiaoshi Wu",
                "Xintao Wang",
                "Fu Xiao",
                "Yuanxing Zhang",
                "Qinghe Wang",
                "Xiaoyu Shi",
                "Menghan Xia",
                "Zuozhu Liu",
                "Haoji Hu",
                "Pengfei Wan",
                "Kun Gai"
            ],
            "affiliations": [
                "CUHK",
                "DLUT",
                "HUST",
                "Kling Team, Kuaishou Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20619.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#diffusion",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "SemanticGen Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ VAE Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ñ‹, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğ¾Ñ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğº Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ¿Ñ€ÑĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ VAE Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Faster Video Generation through Semantic Features",
                    "desc": "SemanticGen is a novel video generation model that improves the speed and efficiency of creating videos by using a two-stage diffusion approach. In the first stage, it generates semantic features that outline the video's overall structure, allowing for better global planning. The second stage then creates VAE latents based on these semantic features, which helps in adding detailed elements to the video. This method not only accelerates convergence but also maintains high-quality output, especially for longer videos, outperforming existing models."
                },
                "zh": {
                    "title": "è¯­ä¹‰ç”Ÿæˆï¼Œå¿«é€Ÿé«˜æ•ˆçš„è§†é¢‘åˆ›ä½œ",
                    "desc": "SemanticGen æ˜¯ä¸€ç§æ–°é¢–çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶çš„æ”¶æ•›é€Ÿåº¦æ…¢å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œé¦–å…ˆç”Ÿæˆè¯­ä¹‰ç‰¹å¾ï¼Œç„¶åç”Ÿæˆå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ½œå˜é‡ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ”¶æ•›å’Œé«˜è´¨é‡çš„ç»“æœã€‚é€šè¿‡åœ¨è¯­ä¹‰ç©ºé—´ä¸­è¿›è¡Œç”Ÿæˆï¼ŒSemanticGen èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è§†é¢‘çš„å†—ä½™æ€§ï¼Œè¿›è¡Œå…¨å±€è§„åˆ’å¹¶æ·»åŠ é«˜é¢‘ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSemanticGen åœ¨è§†é¢‘ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19673",
            "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
            "url": "https://huggingface.co/papers/2512.19673",
            "abstract": "The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
            "score": 47,
            "issue_id": 213,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "4a2108b598a67e98",
            "authors": [
                "Yuqiao Tan",
                "Minzheng Wang",
                "Shizhu He",
                "Huanxuan Liao",
                "Chengfeng Zhao",
                "Qiunan Lu",
                "Tian Liang",
                "Jun Zhao",
                "Kang Liu"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "Tencent AI Lab",
                "University of Chinese Academy of Sciences",
                "University of Electronic Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19673.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#optimization",
                    "#interpretability",
                    "#open_source",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° ÑĞ½Ğ¸Ğ·Ñƒ Ğ²Ğ²ĞµÑ€Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ñ‘Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°, Ğ° Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Bottom-up Policy Optimization (BuPO) â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Unlocking Layered Reasoning in Language Models with BuPO",
                    "desc": "This paper explores the internal workings of large language models (LLMs) by breaking down their policy into internal layer and modular policies. It highlights how different layers contribute to reasoning, with early layers focusing on exploration and later layers refining outputs. The authors introduce Bottom-up Policy Optimization (BuPO), a new approach that optimizes these internal policies during training to improve reasoning capabilities. Experiments show that this method enhances performance on complex reasoning tasks compared to traditional methods."
                },
                "zh": {
                    "title": "åˆ†å±‚ä¼˜åŒ–ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "æœ¬æ–‡å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç­–ç•¥åˆ†è§£ä¸ºå†…éƒ¨å±‚ç­–ç•¥å’Œæ¨¡å—åŒ–ç­–ç•¥ï¼Œæ­ç¤ºäº†ä¸åŒå±‚æ¬¡ä¹‹é—´çš„æ¨ç†æ¨¡å¼ã€‚é€šè¿‡åˆ†æTransformerçš„æ®‹å·®æµå’Œéšè—çŠ¶æ€çš„ç»„åˆï¼Œæå‡ºäº†åº•å±‚æ”¿ç­–ä¼˜åŒ–ï¼ˆBuPOï¼‰æ–¹æ³•ï¼Œä»¥æå‡å¤æ‚æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œæ—©æœŸå±‚ä¿æŒé«˜ç†µä»¥è¿›è¡Œæ¢ç´¢ï¼Œè€Œé¡¶å±‚åˆ™è¶‹è¿‘äºé›¶ç†µä»¥è¿›è¡Œç²¾ç»†åŒ–ï¼Œä¸”ä¸åŒæ¨¡å‹ç³»åˆ—çš„æ”¶æ•›æ¨¡å¼å„å¼‚ã€‚é€šè¿‡å¯¹å†…éƒ¨ç­–ç•¥çš„ä¼˜åŒ–ï¼ŒBuPOé‡å»ºäº†åŸºç¡€æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤æ‚æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20618",
            "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
            "url": "https://huggingface.co/papers/2512.20618",
            "abstract": "A multi-agent framework, involving a master LLM, grounding agent, and vision agent, enhances long-video QA by improving temporal grounding and leveraging visual and textual data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
            "score": 30,
            "issue_id": 218,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "3f9db9f2dd3dd95c",
            "authors": [
                "Runtao Liu",
                "Ziyi Liu",
                "Jiaqi Tang",
                "Yue Ma",
                "Renjie Pi",
                "Jipeng Zhang",
                "Qifeng Chen"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20618.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#rl",
                    "#video",
                    "#reasoning",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³Ğ´Ğµ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ LLM ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ, Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ…, Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… LongTVQA Ğ¸ LongTVQA+ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing Long-Video QA with Multi-Agent Collaboration",
                    "desc": "This paper introduces a multi-agent framework designed to improve question answering (QA) for long videos by enhancing temporal grounding. It features a master large language model (LLM) that coordinates a grounding agent to identify relevant video segments and a vision agent to extract specific visual information. The framework employs reinforcement learning to optimize the agents' cooperation, ensuring that the master agent efficiently focuses on pertinent clips while integrating visual details with textual data. The proposed system demonstrates significant performance improvements on the LongTVQA datasets compared to traditional methods, highlighting the effectiveness of multi-agent collaboration in long-video QA tasks."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“æ¡†æ¶æå‡é•¿è§†é¢‘é—®ç­”èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æå‡é•¿è§†é¢‘é—®ç­”ï¼ˆQAï¼‰çš„æ•ˆæœã€‚è¯¥æ¡†æ¶ç”±ä¸€ä¸ªä¸»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€ä¸€ä¸ªå®šä½æ™ºèƒ½ä½“å’Œä¸€ä¸ªè§†è§‰æ™ºèƒ½ä½“ç»„æˆï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œæ—¶é—´å®šä½å¹¶åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬æ•°æ®ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œä¸»æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåè°ƒå„ä¸ªæ™ºèƒ½ä½“çš„åˆä½œï¼Œä¸“æ³¨äºä¸é—®é¢˜ç›¸å…³çš„ç‰‡æ®µï¼Œå¹¶ç»“åˆè§†è§‰ç»†èŠ‚è¡¥å……å­—å¹•ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨é•¿è§†é¢‘é—®ç­”æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†å¤šæ™ºèƒ½ä½“åˆä½œçš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20617",
            "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
            "url": "https://huggingface.co/papers/2512.20617",
            "abstract": "SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.  \t\t\t\t\tAI-generated summary \t\t\t\t Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
            "score": 30,
            "issue_id": 213,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "4a93e02fa033e681",
            "authors": [
                "Yuxi Xiao",
                "Longfei Li",
                "Shen Yan",
                "Xinhang Liu",
                "Sida Peng",
                "Yunchao Wei",
                "Xiaowei Zhou",
                "Bingyi Kang"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "ByteDance",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20617.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ğ¾Ñ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ SpatialTree â€” Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ÑƒĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ: Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ§ĞµÑ€ĞµĞ· Ğ¾Ñ†ĞµĞ½ĞºÑƒ 27 Ğ¿Ğ¾Ğ´ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹: Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹, Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ auto-think Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Spatial Intelligence in MLLMs with SpatialTree",
                    "desc": "This paper introduces SpatialTree, a hierarchical framework inspired by cognitive science that categorizes spatial abilities in multimodal large language models (MLLMs) into four levels: perception, mental mapping, simulation, and agentic competence. The authors evaluate various MLLMs using a new benchmark that assesses 27 sub-abilities, revealing that lower-level skills are independent while higher-level skills are interdependent. They discover a unique transfer dynamic where low-level skills can negatively impact performance, but higher-level skills benefit from low-level training. To enhance performance across all levels, they propose an auto-think strategy that minimizes unnecessary cognitive deliberation, leading to consistent improvements in MLLM capabilities."
                },
                "zh": {
                    "title": "SpatialTreeï¼šæå‡ç©ºé—´èƒ½åŠ›çš„å±‚æ¬¡åŒ–ç­–ç•¥",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSpatialTreeçš„å±‚æ¬¡ç»“æ„ï¼Œçµæ„Ÿæ¥æºäºè®¤çŸ¥ç§‘å­¦ï¼Œç”¨äºè¯„ä¼°å’Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç©ºé—´èƒ½åŠ›ã€‚SpatialTreeå°†ç©ºé—´èƒ½åŠ›åˆ†ä¸ºå››ä¸ªå±‚æ¬¡ï¼šä½çº§æ„ŸçŸ¥ï¼ˆL1ï¼‰ã€å¿ƒç†æ˜ å°„ï¼ˆL2ï¼‰ã€æ¨¡æ‹Ÿï¼ˆL3ï¼‰å’Œä»£ç†èƒ½åŠ›ï¼ˆL4ï¼‰ã€‚ç ”ç©¶å‘ç°ï¼ŒL1æŠ€èƒ½ä¹‹é—´ç›¸å¯¹ç‹¬ç«‹ï¼Œè€Œé«˜å±‚æŠ€èƒ½ä¹‹é—´åˆ™å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œè¡¨æ˜æŠ€èƒ½ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§å¢å¼ºã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„è‡ªæˆ‘æ€è€ƒç­–ç•¥ï¼Œä»¥æé«˜å„å±‚æ¬¡çš„æ•´ä½“è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.18746",
            "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
            "url": "https://huggingface.co/papers/2512.18746",
            "abstract": "MemEvolve, a meta-evolutionary framework, enhances self-evolving memory systems by jointly evolving agents' experiential knowledge and memory architecture, leading to improved performance and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to 17.06%; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
            "score": 17,
            "issue_id": 216,
            "pub_date": "2025-12-21",
            "pub_date_card": {
                "ru": "21 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 21",
                "zh": "12æœˆ21æ—¥"
            },
            "hash": "67c64602fd3a4037",
            "authors": [
                "Guibin Zhang",
                "Haotian Ren",
                "Chong Zhan",
                "Zhenhong Zhou",
                "Junhao Wang",
                "He Zhu",
                "Wangchunshu Zhou",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "LV-NUS lab",
                "OPPO AI Agent Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.18746.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#training",
                    "#transfer_learning",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ¸Ñ… Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼",
                    "desc": "MemEvolve â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ°-ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ±Ñ‹Ğ»Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹, ÑÑ‚Ğ¾Ñ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ EvolveLab â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ Ğ±Ğ°Ğ·Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ (ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MemEvolve ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 17%, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Evolving Memory for Smarter Agents",
                    "desc": "MemEvolve is a meta-evolutionary framework that enhances self-evolving memory systems by evolving both the agents' knowledge and their memory architecture. This approach allows agents to not only learn from their experiences but also adapt their memory systems to better suit different tasks. Unlike previous methods that relied on fixed memory architectures, MemEvolve enables dynamic adaptation, leading to improved performance and generalization across various benchmarks. The framework is supported by EvolveLab, a codebase that standardizes memory systems and facilitates fair experimentation."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–è®°å¿†çš„å…ƒè¿›åŒ–æ–°æ¡†æ¶",
                    "desc": "MemEvolveæ˜¯ä¸€ä¸ªå…ƒè¿›åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè‡ªæˆ‘è¿›åŒ–çš„è®°å¿†ç³»ç»Ÿã€‚å®ƒé€šè¿‡å…±åŒè¿›åŒ–ä»£ç†çš„ç»éªŒçŸ¥è¯†å’Œè®°å¿†æ¶æ„ï¼Œæå‡äº†æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä»¥å¾€çš„è®°å¿†ç³»ç»Ÿå¾€å¾€æ˜¯é™æ€çš„ï¼Œé™åˆ¶äº†ä»£ç†åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚MemEvolveé€šè¿‡å¼•å…¥EvolveLabï¼Œæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–çš„è‡ªæˆ‘è¿›åŒ–è®°å¿†ä»£ç åº“ï¼Œä¿ƒè¿›äº†æœªæ¥ç³»ç»Ÿçš„å¼€æ”¾æ€§å’Œæ ‡å‡†åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20491",
            "title": "Step-DeepResearch Technical Report",
            "url": "https://huggingface.co/papers/2512.20491",
            "abstract": "Step-DeepResearch, an end-to-end agent enhanced with a data synthesis strategy and progressive training, achieves expert-level capabilities in deep research scenarios, outperforming established models.  \t\t\t\t\tAI-generated summary \t\t\t\t As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
            "score": 11,
            "issue_id": 213,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "a8636818baa93e22",
            "authors": [
                "Chen Hu",
                "Haikuo Du",
                "Heng Wang",
                "Lin Lin",
                "Mingrui Chen",
                "Peng Liu",
                "Ruihang Miao",
                "Tianchi Yue",
                "Wang You",
                "Wei Ji",
                "Wei Yuan",
                "Wenjin Deng",
                "Xiaojian Yuan",
                "Xiaoyun Zhang",
                "Xiangyu Liu",
                "Xikai Liu",
                "Yanming Xu",
                "Yicheng Cao",
                "Yifei Zhang",
                "Yongyao Wang",
                "Yubo Shu",
                "Yurong Zhang",
                "Yuxiang Zhang",
                "Zheng Gong",
                "Zhichao Chang",
                "Binyan Li",
                "Dan Ma",
                "Furong Jia",
                "Hongyuan Wang",
                "Jiayu Liu",
                "Jing Bai",
                "Junlan Liu",
                "Manjiao Liu",
                "Na Wang",
                "Qiuping Wu",
                "Qinxin Du",
                "Shiwei Li",
                "Wen Sun",
                "Yifeng Gong",
                "Yonglin Chen",
                "Yuling Zhao",
                "Yuxuan Lin",
                "Ziqi Ren",
                "Zixuan Wang",
                "Aihu Zhang",
                "Brian Li",
                "Buyun Ma",
                "Kang An",
                "Li Xie",
                "Mingliang Li",
                "Pan Li",
                "Shidong Yang",
                "Xi Chen",
                "Xiaojia Liu",
                "Yuchu Luo",
                "Yuan Song",
                "YuanHao Ding",
                "Yuanwei Liang",
                "Zexi Li",
                "Zhaoning Zhang",
                "Zixin Zhang",
                "Binxing Jiao",
                "Daxin Jiang",
                "Jiansheng Chen",
                "Jing Li",
                "Xiangyu Zhang",
                "Yibo Zhu"
            ],
            "affiliations": [
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20491.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#rlhf",
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#open_source",
                    "#multilingual",
                    "#science",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ€ĞµĞ´Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Step-DeepResearch â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ supervised fine-tuning Ğ¸ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¸ÑĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 32B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ OpenAI Ğ¸ Gemini, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Step-DeepResearch: Redefining Deep Research with Expert-Level AI Efficiency",
                    "desc": "Step-DeepResearch is an advanced AI agent designed for deep research tasks, utilizing a unique data synthesis strategy and progressive training methods. It excels in critical areas such as intent recognition, long-term decision-making, and verifying information from multiple sources. By implementing a Checklist-style Judger, the model enhances its robustness and effectiveness in real-world applications. The results demonstrate that this model not only surpasses existing benchmarks but also competes with top-tier closed-source models, showcasing its efficiency and capability in achieving expert-level performance."
                },
                "zh": {
                    "title": "æ·±åº¦ç ”ç©¶çš„æ™ºèƒ½åŒ–æ–°çªç ´",
                    "desc": "Step-DeepResearch æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“ï¼Œé‡‡ç”¨æ•°æ®åˆæˆç­–ç•¥å’Œæ¸è¿›å¼è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨æ·±åº¦ç ”ç©¶åœºæ™¯ä¸­å±•ç°å‡ºä¸“å®¶çº§çš„èƒ½åŠ›ã€‚å®ƒè§£å†³äº†ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•æ»¡è¶³å¼€æ”¾å¼ç ”ç©¶éœ€æ±‚çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ„å›¾è¯†åˆ«ã€é•¿æœŸå†³ç­–å’Œè·¨æºéªŒè¯æ–¹é¢ã€‚é€šè¿‡åŸºäºåŸå­èƒ½åŠ›çš„æ•°æ®åˆæˆç­–ç•¥ï¼ŒStep-DeepResearch å¼ºåŒ–äº†è§„åˆ’å’ŒæŠ¥å‘Šå†™ä½œçš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ¸è¿›å¼è®­ç»ƒè·¯å¾„æå‡äº†æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStep-DeepResearch åœ¨å¤šä¸ªè¯„ä¼°æ ‡å‡†ä¸Šè¶…è¶Šäº†åŒç±»æ¨¡å‹ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§ä»·æ¯”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.18099",
            "title": "SAM Audio: Segment Anything in Audio",
            "url": "https://huggingface.co/papers/2512.18099",
            "abstract": "SAM Audio, a diffusion transformer-based foundation model, achieves superior performance in general audio separation using unified text, visual, and temporal span prompts across various audio types.  \t\t\t\t\tAI-generated summary \t\t\t\t General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.",
            "score": 8,
            "issue_id": 213,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "effc12ca0ce9232b",
            "authors": [
                "Bowen Shi",
                "Andros Tjandra",
                "John Hoffman",
                "Helin Wang",
                "Yi-Chiao Wu",
                "Luya Gao",
                "Julius Richter",
                "Matt Le",
                "Apoorv Vyas",
                "Sanyuan Chen",
                "Christoph Feichtenhofer",
                "Piotr DollÃ¡r",
                "Wei-Ning Hsu",
                "Ann Lee"
            ],
            "affiliations": [
                "Meta Superintelligence Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.18099.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#audio",
                    "#diffusion"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸",
                    "desc": "SAM Audio â€” ÑÑ‚Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ flow matching Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ°Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµÑ‡ÑŒ, Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. SAM Audio Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°."
                },
                "en": {
                    "title": "Unified Audio Separation with SAM Audio",
                    "desc": "SAM Audio is a cutting-edge foundation model designed for general audio source separation, utilizing a diffusion transformer architecture. It stands out by integrating multiple prompting modalities, including text, visual cues, and temporal spans, allowing for flexible and precise audio separation. Trained on a vast dataset that includes various audio types like speech and music, SAM Audio demonstrates superior performance on a range of benchmarks, outperforming both specialized and general-purpose models. Additionally, the introduction of a new benchmark with human-labeled prompts enhances the evaluation of the model's effectiveness in real-world scenarios."
                },
                "zh": {
                    "title": "SAM Audioï¼šç»Ÿä¸€å¤šæ¨¡æ€éŸ³é¢‘åˆ†ç¦»çš„åŸºç¡€æ¨¡å‹",
                    "desc": "SAM Audioæ˜¯ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨çš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§éŸ³é¢‘ç±»å‹ä¸­å®ç°ä¼˜è¶Šçš„éŸ³é¢‘åˆ†ç¦»æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„æ–‡æœ¬ã€è§†è§‰å’Œæ—¶é—´è·¨åº¦æç¤ºï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸå’Œæ§åˆ¶èƒ½åŠ›ä¸Šçš„å±€é™ã€‚SAM Audioåœ¨å¤§è§„æ¨¡éŸ³é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿçµæ´»åœ°åˆ†ç¦»ç”±è¯­è¨€ã€è§†è§‰æ©ç æˆ–æ—¶é—´è·¨åº¦æè¿°çš„ç›®æ ‡éŸ³æºã€‚å®ƒåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„é€šç”¨å’Œä¸“ä¸šç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17102",
            "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
            "url": "https://huggingface.co/papers/2512.17102",
            "abstract": "A novel RL framework, SAGE, enhances LLM-based agents' self-improvement capabilities by systematically incorporating skills from a skill library, leading to better performance and efficiency in new environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.",
            "score": 6,
            "issue_id": 213,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "5c21964c90bb1ac1",
            "authors": [
                "Jiongxiao Wang",
                "Qiaojing Yan",
                "Yawei Wang",
                "Yijun Tian",
                "Soumya Smruti Mishra",
                "Zhichao Xu",
                "Megha Gandhi",
                "Panpan Xu",
                "Lin Lee Cheong"
            ],
            "affiliations": [
                "AWS Agentic AI",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17102.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#agents",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñæ¡†æ¶ SAGE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Sequential Rollout, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒÑÑ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğµ Skill-integrated Reward Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SAGE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ° 8,9% Ğ²Ñ‹ÑˆĞµ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ½Ğ° 26% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ½Ğ° 59% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering LLM Agents with Skill-Driven Self-Improvement",
                    "desc": "The paper introduces SAGE, a novel Reinforcement Learning framework designed to improve the self-improvement capabilities of Large Language Model (LLM)-based agents. By systematically integrating skills from a skill library, SAGE allows agents to learn and apply new skills more effectively in various environments. The framework employs a Sequential Rollout method, where agents tackle a series of related tasks, accumulating skills that enhance their performance in future tasks. Experimental results show that SAGE significantly boosts goal completion rates while reducing the number of interactions and tokens generated, outperforming traditional methods."
                },
                "zh": {
                    "title": "SAGEï¼šæå‡ä»£ç†è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶SAGEï¼Œæ—¨åœ¨å¢å¼ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ã€‚é€šè¿‡ç³»ç»Ÿåœ°å°†æŠ€èƒ½åº“ä¸­çš„æŠ€èƒ½èå…¥å­¦ä¹ è¿‡ç¨‹ï¼ŒSAGEèƒ½å¤Ÿåœ¨æ–°ç¯å¢ƒä¸­æé«˜ä»£ç†çš„è¡¨ç°å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶çš„å…³é”®ç»„ä»¶æ˜¯é¡ºåºå›æ”¾ï¼Œä»£ç†åœ¨ä¸€ç³»åˆ—ç›¸ä¼¼ä»»åŠ¡ä¸­è¿­ä»£éƒ¨ç½²ï¼Œä»è€Œç§¯ç´¯æŠ€èƒ½å¹¶åœ¨åç»­ä»»åŠ¡ä¸­ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAGEåœ¨å®Œæˆç›®æ ‡å’Œå‡å°‘äº¤äº’æ­¥éª¤æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16144",
            "title": "INTELLECT-3: Technical Report",
            "url": "https://huggingface.co/papers/2512.16144",
            "abstract": "INTELLECT-3, a large Mixture-of-Experts model trained with reinforcement learning, achieves top performance across various benchmarks and is supported by an open-source RL infrastructure framework.  \t\t\t\t\tAI-generated summary \t\t\t\t We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",
            "score": 6,
            "issue_id": 216,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "fbf844349d0482c4",
            "authors": [
                "Prime Intellect Team",
                "Mika Senghaas",
                "Fares Obeid",
                "Sami Jaghouar",
                "William Brown",
                "Jack Min Ong",
                "Daniel Auras",
                "Matej Sirovatka",
                "Jannik Straube",
                "Andrew Baker",
                "Sebastian MÃ¼ller",
                "Justus Mattern",
                "Manveer Basra",
                "Aiman Ismail",
                "Dominik Scherm",
                "Cooper Miller",
                "Ameen Patel",
                "Simon Kirsten",
                "Mario Sieg",
                "Christian Reetz",
                "Kemal Erdem",
                "Vincent Weisser",
                "Johannes Hagemann"
            ],
            "affiliations": [
                "Prime Intellect, Inc.",
                "ellamind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16144.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#rl",
                    "#math",
                    "#plp",
                    "#science",
                    "#training",
                    "#agents",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° INTELLECT-3 â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mixture-of-Experts Ñ 106 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ reinforcement learning Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚ĞµĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°ÑƒĞºĞµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ¸Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº prime-rl Ğ´Ğ»Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ·Ğ»Ğ° Ğ´Ğ¾ Ñ‚Ñ‹ÑÑÑ‡ GPU. Ğ˜Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ multi-turn Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… tools."
                },
                "en": {
                    "title": "INTELLECT-3: Reinforcement Learning Redefined for Expert Models",
                    "desc": "INTELLECT-3 is a large Mixture-of-Experts model with 106 billion parameters, where only 12 billion are active at any time, designed to excel in various tasks through reinforcement learning. It demonstrates superior performance on benchmarks related to math, coding, science, and reasoning, surpassing even larger models. The model is accompanied by an open-source reinforcement learning infrastructure, which includes tools for training and evaluation, making it accessible for further research and development. Additionally, the introduction of prime-rl provides a scalable framework for asynchronous reinforcement learning, enhancing the model's capabilities in multi-turn interactions and tool usage."
                },
                "zh": {
                    "title": "INTELLECT-3ï¼šå¼ºåŒ–å­¦ä¹ çš„æ··åˆä¸“å®¶æ–°æ ‡æ†",
                    "desc": "INTELLECT-3 æ˜¯ä¸€ä¸ªå¤§å‹çš„æ··åˆä¸“å®¶æ¨¡å‹ï¼Œæ‹¥æœ‰1060äº¿ä¸ªå‚æ•°ï¼ˆå…¶ä¸­120äº¿ä¸ªä¸ºæ´»è·ƒå‚æ•°ï¼‰ï¼Œé€šè¿‡å¤§è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹åœ¨æ•°å­¦ã€ä»£ç ã€ç§‘å­¦å’Œæ¨ç†ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†è®¸å¤šæ›´å¤§çš„å‰æ²¿æ¨¡å‹ã€‚æˆ‘ä»¬å¼€æºäº†è¯¥æ¨¡å‹åŠå…¶å®Œæ•´çš„åŸºç¡€è®¾æ–½ï¼ŒåŒ…æ‹¬å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€è®­ç»ƒå’Œè¯„ä¼°çš„ç¯å¢ƒé›†åˆã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€åŠªåŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº† prime-rlï¼Œä¸€ä¸ªç”¨äºå¤§è§„æ¨¡å¼‚æ­¥å¼ºåŒ–å­¦ä¹ çš„å¼€æ”¾æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•èŠ‚ç‚¹æ— ç¼æ‰©å±•åˆ°æ•°åƒä¸ªGPUã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20182",
            "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
            "url": "https://huggingface.co/papers/2512.20182",
            "abstract": "FaithLens, a cost-efficient faithfulness hallucination detection model using advanced LLMs for training data synthesis and rule-based reinforcement learning, outperforms models like GPT-4.1 and o3 on 12 tasks with high-quality explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
            "score": 5,
            "issue_id": 216,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "2f671e75c44f8081",
            "authors": [
                "Shuzheng Si",
                "Qingyi Wang",
                "Haozhe Zhao",
                "Yuzhuo Bai",
                "Guanqiao Chen",
                "Kangyang Luo",
                "Gang Chen",
                "Fanchao Qi",
                "Minjia Zhang",
                "Baobao Chang",
                "Maosong Sun"
            ],
            "affiliations": [
                "DeepLang AI",
                "Fudan University",
                "Peking University",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20182.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#synthetic",
                    "#rag",
                    "#rlhf",
                    "#training",
                    "#hallucinations"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœĞ°Ğ»Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ‡ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ: Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚",
                    "desc": "FaithLens â€” ÑÑ‚Ğ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ LLM Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ fine-tuning Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ÑÑĞ½ĞµĞ½Ğ¸Ğ¹. ĞĞ° 12 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 8-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ FaithLens Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4.1 Ğ¸ o3."
                },
                "en": {
                    "title": "FaithLens: Trustworthy AI through Effective Hallucination Detection",
                    "desc": "FaithLens is a novel model designed to detect faithfulness hallucinations in outputs from large language models (LLMs). It utilizes advanced LLMs to synthesize training data that includes explanations, ensuring high-quality and diverse examples for training. The model is fine-tuned using a cold start approach and optimized through rule-based reinforcement learning, focusing on both prediction accuracy and explanation quality. Results demonstrate that FaithLens outperforms existing models like GPT-4.1 and o3 across 12 tasks, providing a reliable and efficient solution for enhancing trustworthiness in AI-generated content."
                },
                "zh": {
                    "title": "FaithLensï¼šé«˜æ•ˆçš„ä¿¡å®å¹»è§‰æ£€æµ‹æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFaithLensçš„æ¨¡å‹ï¼Œç”¨äºæ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¾“å‡ºä¸­çš„ä¿¡å®å¹»è§‰ã€‚è¯¥æ¨¡å‹é€šè¿‡å…ˆè¿›çš„LLMsåˆæˆè®­ç»ƒæ•°æ®ï¼Œå¹¶é‡‡ç”¨è§„åˆ™åŸºç¡€çš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„æ£€æµ‹ã€‚FaithLensåœ¨12ä¸ªä¸åŒä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºGPT-4.1å’Œo3ç­‰å…ˆè¿›æ¨¡å‹ï¼Œå¹¶èƒ½å¤Ÿæä¾›é«˜è´¨é‡çš„è§£é‡Šï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¯ä¿¡åº¦ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒFaithLensåœ¨ä¿¡å®æ€§ã€æ•ˆç‡å’Œæœ‰æ•ˆæ€§ä¹‹é—´è¾¾åˆ°äº†è‰¯å¥½çš„å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19526",
            "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
            "url": "https://huggingface.co/papers/2512.19526",
            "abstract": "QuantiPhy is a benchmark that quantitatively assesses state-of-the-art vision perception models' ability to reason about physical properties such as size, velocity, and acceleration from video observations, revealing gaps between qualitative plausibility and numerical correctness.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
            "score": 2,
            "issue_id": 213,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "1dabb3a40df87e2f",
            "authors": [
                "Li Puyin",
                "Tiange Xiang",
                "Ella Mao",
                "Shirley Wei",
                "Xinye Chen",
                "Adnan Masood",
                "Li Fei-fei",
                "Ehsan Adeli"
            ],
            "affiliations": [
                "Stanford University",
                "UST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19526.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#multimodal",
                    "#reasoning",
                    "#cv",
                    "#science"
                ],
                "emoji": "ğŸ“¹",
                "ru": {
                    "title": "ĞÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸",
                    "desc": "QuantiPhy â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 3.3K Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ, Ğ° Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²."
                },
                "en": {
                    "title": "Bridging the Gap: Quantifying Physical Reasoning in Vision Models",
                    "desc": "QuantiPhy is a new benchmark designed to evaluate how well vision perception models can understand and reason about physical properties like size, velocity, and acceleration from video data. It consists of over 3,300 video-text pairs with numerical ground truth, allowing for a quantitative assessment of a model's performance. The benchmark highlights a significant gap between the models' qualitative assessments and their actual numerical accuracy when estimating kinematic properties. By standardizing prompts and scoring, QuantiPhy enables fair comparisons across different models and encourages improvements in their ability to reason about the physical world."
                },
                "zh": {
                    "title": "QuantiPhyï¼šä»å®šæ€§åˆ°å®šé‡çš„ç‰©ç†ç†è§£",
                    "desc": "QuantiPhyæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å®šé‡è¯„ä¼°æœ€å…ˆè¿›çš„è§†è§‰æ„ŸçŸ¥æ¨¡å‹åœ¨ä»è§†é¢‘è§‚å¯Ÿä¸­æ¨ç†ç‰©ç†å±æ€§ï¼ˆå¦‚å¤§å°ã€é€Ÿåº¦å’ŒåŠ é€Ÿåº¦ï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡3300ä¸ªè§†é¢‘-æ–‡æœ¬å®ä¾‹ï¼Œå¹¶æä¾›æ•°å€¼çœŸå€¼ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šæ—¶é—´æˆ³ä¸‹å¯¹ç‰©ä½“å±æ€§çš„ä¼°è®¡èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å®šæ€§ä¸Šçœ‹ä¼¼åˆç†ï¼Œä½†åœ¨å®é™…æ•°å€¼å‡†ç¡®æ€§ä¸Šå­˜åœ¨æ˜æ˜¾å·®è·ã€‚QuantiPhyä¸ºæ¨åŠ¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†ç†è§£æ–¹é¢çš„è¿›æ­¥æä¾›äº†ä¸€ä¸ªä¸¥æ ¼ä¸”å¯æ‰©å±•çš„æµ‹è¯•å¹³å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17648",
            "title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
            "url": "https://huggingface.co/papers/2512.17648",
            "abstract": "A new open-source framework, simulstream, is introduced for evaluating and demonstrating streaming speech-to-text translation systems, supporting both incremental decoding and re-translation methods with a focus on long-form audio processing.  \t\t\t\t\tAI-generated summary \t\t\t\t Streaming Speech-to-Text Translation (StreamST) requires producing translations concurrently with incoming speech, imposing strict latency constraints and demanding models that balance partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in a demo. As a solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool.",
            "score": 2,
            "issue_id": 221,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "147a97b70a1f9f93",
            "authors": [
                "Marco Gaido",
                "Sara Papi",
                "Mauro Cettolo",
                "Matteo Negri",
                "Luisa Bentivogli"
            ],
            "affiliations": [
                "Fondazione Bruno Kessler, Trento, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17648.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#machine_translation",
                    "#multimodal",
                    "#benchmark",
                    "#audio",
                    "#long_context",
                    "#dataset"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ open-source Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº simulstream Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ° Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞµĞ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° SimulEval, simulstream Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ĞµĞ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Streamline Your Speech Translation with Simulstream!",
                    "desc": "The paper introduces simulstream, an open-source framework designed for evaluating and demonstrating streaming speech-to-text translation systems. It addresses the limitations of previous tools by supporting both incremental decoding and re-translation methods, specifically for long-form audio processing. This framework allows researchers to compare different translation approaches in terms of quality and latency, which is crucial for real-time applications. Additionally, simulstream features an interactive web interface that enables users to showcase their systems effectively."
                },
                "zh": {
                    "title": "æµå¼è¯­éŸ³ç¿»è¯‘çš„å…¨æ–°è¯„ä¼°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¼€æºæ¡†æ¶simulstreamï¼Œç”¨äºè¯„ä¼°å’Œæ¼”ç¤ºæµå¼è¯­éŸ³è½¬æ–‡æœ¬ç¿»è¯‘ç³»ç»Ÿã€‚è¯¥æ¡†æ¶æ”¯æŒå¢é‡è§£ç å’Œé‡ç¿»è¯‘æ–¹æ³•ï¼Œç‰¹åˆ«å…³æ³¨é•¿éŸ³é¢‘å¤„ç†ã€‚ä¸ä¹‹å‰çš„SimulEvalåº“ç›¸æ¯”ï¼Œsimulstreamèƒ½å¤Ÿå¤„ç†é•¿éŸ³é¢‘æµï¼Œå¹¶å…è®¸ç³»ç»Ÿä¿®æ­£å…¶è¾“å‡ºã€‚å®ƒè¿˜æä¾›äº†ä¸€ä¸ªäº’åŠ¨ç½‘é¡µç•Œé¢ï¼Œæ–¹ä¾¿å±•ç¤ºåœ¨è¯¥å·¥å…·ä¸­æ„å»ºçš„ä»»ä½•ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20615",
            "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
            "url": "https://huggingface.co/papers/2512.20615",
            "abstract": "ORCA, a framework for goal-directed planning in video avatars, uses an internal world model and dual-system architecture to enable autonomous task completion in stochastic environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",
            "score": 1,
            "issue_id": 214,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "7f171669a021ff2b",
            "authors": [
                "Xuanhua He",
                "Tianyu Yang",
                "Ke Cao",
                "Ruiqi Wu",
                "Cheng Meng",
                "Yong Zhang",
                "Zhuoliang Kang",
                "Xiaoming Wei",
                "Qifeng Chen"
            ],
            "affiliations": [
                "Meituan",
                "The Hong Kong University of Science and Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20615.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ: Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ORCA â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» OTAR (ĞĞ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ-Ğ Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ-Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ-Ğ ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ²ÑƒÑ…ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ³Ğ´Ğµ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ORCA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°."
                },
                "en": {
                    "title": "Empowering Video Avatars with Autonomous Goal-Directed Intelligence",
                    "desc": "The paper introduces ORCA, a framework designed to enhance video avatars with goal-directed planning capabilities. It utilizes an Internal World Model (IWM) and a dual-system architecture to allow avatars to autonomously complete tasks in unpredictable environments. The framework employs a closed-loop OTAR cycle for continuous state tracking and a hierarchical system where one part focuses on strategic reasoning while the other translates plans into actions. Experimental results show that ORCA outperforms traditional methods, demonstrating its effectiveness in enabling active intelligence in video avatars."
                },
                "zh": {
                    "title": "ORCAï¼šè§†é¢‘åŒ–èº«çš„è‡ªä¸»æ™ºèƒ½è§„åˆ’æ¡†æ¶",
                    "desc": "ORCAæ˜¯ä¸€ä¸ªç”¨äºè§†é¢‘åŒ–èº«çš„ç›®æ ‡å¯¼å‘è§„åˆ’æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨éšæœºç¯å¢ƒä¸­å®ç°è‡ªä¸»ä»»åŠ¡å®Œæˆã€‚å®ƒé€šè¿‡å†…éƒ¨ä¸–ç•Œæ¨¡å‹å’ŒåŒç³»ç»Ÿæ¶æ„ï¼Œä½¿å¾—è§†é¢‘åŒ–èº«èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œé•¿æœŸç›®æ ‡çš„è‡ªä¸»è¿½æ±‚ã€‚ORCAçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶é—­ç¯OTARå¾ªç¯å’Œå±‚æ¬¡åŒ–çš„åŒç³»ç»Ÿæ¶æ„ï¼Œå‰è€…ç¡®ä¿åœ¨ç”Ÿæˆä¸ç¡®å®šæ€§ä¸‹çš„çŠ¶æ€è·Ÿè¸ªï¼Œåè€…åˆ™å°†æŠ½è±¡è®¡åˆ’è½¬åŒ–ä¸ºå…·ä½“çš„è¡ŒåŠ¨æŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORCAåœ¨ä»»åŠ¡æˆåŠŸç‡å’Œè¡Œä¸ºä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨åŠ¨äº†è§†é¢‘åŒ–èº«æ™ºèƒ½çš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20352",
            "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
            "url": "https://huggingface.co/papers/2512.20352",
            "abstract": "A multi-perspective validation framework using LLMs for thematic analysis combines ensemble validation with Cohen's Kappa and cosine similarity to enhance reliability and extract consensus themes from qualitative data.  \t\t\t\t\tAI-generated summary \t\t\t\t Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa (Îº) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability (Îº= 0.907, cosine=95.3%), followed by GPT-4o (Îº= 0.853, cosine=92.6%) and Claude (Îº= 0.842, cosine=92.1%). All three models achieve a high agreement (Îº> 0.80), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
            "score": 1,
            "issue_id": 216,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "be550aa8213bbc8b",
            "authors": [
                "Nilesh Jain",
                "Seyi Adeyinka",
                "Leor Roseman",
                "Aza Allsop"
            ],
            "affiliations": [
                "Center for Collective Healing",
                "University of Exeter",
                "Yale School of Medicine"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20352.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#open_source"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞĞ°Ğ´Ñ‘Ğ¶Ğ½Ğ°Ñ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ÑŒ LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ĞµĞ²ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ ĞšĞ¾ÑĞ½Ğ° Ğ¸ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ñ‹Ğ¼ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ LLM Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ Ğ¾ Ğ°Ñ€Ñ‚-Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Gemini Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (Îº=0,907), Ğ¾Ğ¿ĞµÑ€ĞµĞ¶Ğ°Ñ GPT-4o Ğ¸ Claude. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ½Ñ‹Ğµ Ñ‚ĞµĞ¼Ñ‹ Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸. ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Qualitative Research Reliability with AI",
                    "desc": "This paper introduces a new framework for validating thematic analysis in qualitative research using large language models (LLMs). It combines ensemble validation with two reliability metrics: Cohen's Kappa for measuring agreement between coders and cosine similarity for assessing semantic consistency. The framework allows researchers to customize analysis parameters and extract consensus themes from qualitative data in various formats. The results show that the framework enhances reliability and provides a structured approach for AI-assisted qualitative research, with Gemini 2.5 Pro achieving the highest reliability among the tested models."
                },
                "zh": {
                    "title": "å¤šè§†è§’éªŒè¯æ¡†æ¶ï¼šæå‡å®šæ€§ç ”ç©¶çš„å¯é æ€§",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šè§†è§’éªŒè¯æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œä¸»é¢˜åˆ†æï¼Œæ—¨åœ¨æé«˜å®šæ€§ç ”ç©¶çš„å¯é æ€§ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é›†æˆéªŒè¯ã€Cohen's Kappaå’Œä½™å¼¦ç›¸ä¼¼åº¦ä¸¤ç§å¯é æ€§æŒ‡æ ‡ï¼Œä»¥æå–ä¸€è‡´çš„ä¸»é¢˜ã€‚é€šè¿‡å¯¹ä¸‰ç§é¢†å…ˆçš„LLMè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºGeminiæ¨¡å‹åœ¨å¯é æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬çš„å¼€æºå®ç°ä¸ºç ”ç©¶äººå‘˜æä¾›äº†é€æ˜çš„å¯é æ€§æŒ‡æ ‡å’Œçµæ´»çš„é…ç½®é€‰é¡¹ï¼Œå¥ å®šäº†å¯é çš„AIè¾…åŠ©å®šæ€§ç ”ç©¶çš„æ–¹æ³•åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15031",
            "title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
            "url": "https://huggingface.co/papers/2512.15031",
            "abstract": "A novel framework using Large Language Models and a two-step prompting pipeline effectively predicts and prevents conversational derailment in open source software communities on GitHub.  \t\t\t\t\tAI-generated summary \t\t\t\t Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.   We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate Summaries of Conversation Dynamics (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the likelihood of derailment. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.",
            "score": 1,
            "issue_id": 220,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "4b01f5e39d17ee18",
            "authors": [
                "Mia Mohammad Imran",
                "Robert Zita",
                "Rahat Rizvi Rahman",
                "Preetha Chatterjee",
                "Kostadin Damevski"
            ],
            "affiliations": [
                "Drexel University",
                "Elmhurst University",
                "Missouri University of Science and Technology",
                "Virginia Commonwealth University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15031.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#ethics"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸ÑĞºÑƒÑÑĞ¸Ğ¹ Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ĞŸĞ Ğ½Ğ° GitHub. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 159 Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 207 Ğ½ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ°Ğ¼ Ğ½Ğ°Ğ¿Ñ€ÑĞ¶ĞµĞ½Ğ¸Ñ, ÑĞ´Ğ²Ğ¸Ğ³Ğ°Ğ¼ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑĞ²Ğ¾Ğ´ĞºĞ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Least-to-Most, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ ÑĞ²Ğ¾Ğ´ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen Ğ¸ Llama Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ F1-score 0.901 Ğ¸ 0.852 ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ NLP-Ğ±Ğ°Ğ·Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Predicting Toxicity in Open Source Conversations with LLMs",
                    "desc": "This paper introduces a new framework that uses Large Language Models (LLMs) to predict and prevent toxic conversations in open source software communities on GitHub. By analyzing conversation dynamics, the framework identifies tension triggers and sentiment shifts that indicate potential derailment. It employs a two-step prompting pipeline to generate summaries of conversations and assess the likelihood of toxicity. The results show that this approach significantly outperforms traditional methods, providing a scalable solution for community maintainers to manage interactions effectively."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹å¼€æºç¤¾åŒºå¯¹è¯åç¦»",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä¸¤æ­¥æç¤ºç®¡é“ï¼Œæœ‰æ•ˆé¢„æµ‹å’Œé˜²æ­¢å¼€æºè½¯ä»¶ç¤¾åŒºä¸­çš„å¯¹è¯åç¦»ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¯’æ€§äº’åŠ¨ä¼šé™ä½è´¡çŒ®è€…çš„å‚ä¸åº¦ï¼Œå¨èƒé¡¹ç›®çš„å¯æŒç»­æ€§ï¼Œå› æ­¤éœ€è¦åœ¨æ¯’æ€§å‡ºç°ä¹‹å‰è¿›è¡Œé¢„é˜²ã€‚æˆ‘ä»¬åˆ†æäº†159ä¸ªåç¦»çš„æ¯’æ€§çº¿ç¨‹å’Œ207ä¸ªéæ¯’æ€§çº¿ç¨‹ï¼Œå‘ç°æ¯’æ€§å¯ä»¥é€šè¿‡ç´§å¼ è§¦å‘ã€æƒ…æ„Ÿå˜åŒ–å’Œç‰¹å®šå¯¹è¯æ¨¡å¼è¿›è¡Œé¢„æµ‹ã€‚é€šè¿‡åœ¨Qwenå’ŒLlamaæ¨¡å‹ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„ç­–ç•¥åœ¨å¯¹è¯åç¦»çš„F1åˆ†æ•°ä¸Šè¶…è¿‡äº†ç°æœ‰çš„è‡ªç„¶è¯­è¨€å¤„ç†åŸºçº¿ï¼Œè¯æ˜äº†ç»“æ„åŒ–LLMæç¤ºåœ¨å¼€æºè½¯ä»¶ä¸­æ—©æœŸæ£€æµ‹å¯¹è¯åç¦»çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-23.html",
    "link_next": "2025-12-25.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "23.12",
        "en": "12/23",
        "zh": "12æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 9,
        "#agents": 6,
        "#cv": 1,
        "#rl": 5,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 2,
        "#video": 4,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 9,
        "#small_models": 0,
        "#science": 4,
        "#low_resource": 0
    }
}