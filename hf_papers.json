{
    "date": {
        "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 25",
        "zh": "3æœˆ25æ—¥"
    },
    "time_utc": "2025-03-25 03:26",
    "weekday": 1,
    "issue_id": 2875,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.17359",
            "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
            "url": "https://huggingface.co/papers/2503.17359",
            "abstract": "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.",
            "score": 24,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 21",
                "zh": "3æœˆ21æ—¥"
            },
            "hash": "0046c940a41d8637",
            "authors": [
                "Jiwen Yu",
                "Yiran Qin",
                "Haoxuan Che",
                "Quande Liu",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Kuaishou",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17359.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸Ğ³Ñ€: Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ”Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² (GGE), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ’Ğ¸Ğ´ĞµĞ¾ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ (IGV). GGE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° IGV Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ GGE Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ·Ñ€ĞµĞ»Ğ¾ÑÑ‚Ğ¸ (L0-L4) Ğ´Ğ»Ñ ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ³Ñ€ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Game Development with AI-Driven Generative Engines",
                    "desc": "This paper discusses the limitations of traditional game engines that rely on fixed content, which can hinder creativity and increase costs in game development. It introduces Interactive Generative Video (IGV) as a new approach to create Generative Game Engines (GGE), which can produce endless unique game content. GGE utilizes advanced features like high-quality content synthesis, physics-aware modeling, and user interactivity to enhance the gaming experience. The authors also outline a framework and roadmap for the development of GGE, aiming to transform the future of game creation through AI technologies."
                },
                "zh": {
                    "title": "AIé©±åŠ¨çš„æ¸¸æˆåˆ›ä½œæ–°çºªå…ƒ",
                    "desc": "ç°ä»£æ¸¸æˆå¼€å‘é¢ä¸´ç€åˆ›é€ åŠ›å’Œæˆæœ¬çš„é‡å¤§æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ¸¸æˆå¼•æ“çš„å†…å®¹é¢„è®¾é™åˆ¶äº†åˆ›æ–°ã€‚æœ€è¿‘ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„çªç ´ä½¿å¾—åˆæˆé€¼çœŸä¸”äº’åŠ¨çš„è™šæ‹Ÿç¯å¢ƒæˆä¸ºå¯èƒ½ï¼Œè¿™ä¸ºæ¸¸æˆåˆ›ä½œå¸¦æ¥äº†é©å‘½æ€§çš„æœºä¼šã€‚æˆ‘ä»¬æå‡ºäº†äº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰ä½œä¸ºç”Ÿæˆæ¸¸æˆå¼•æ“ï¼ˆGGEï¼‰çš„åŸºç¡€ï¼Œèƒ½å¤Ÿåœ¨ä¸‹ä¸€ä»£æ¸¸æˆä¸­å®ç°æ— é™çš„æ–°å†…å®¹ç”Ÿæˆã€‚GGEåˆ©ç”¨IGVåœ¨é«˜è´¨é‡å†…å®¹åˆæˆã€ç‰©ç†æ„ŸçŸ¥ä¸–ç•Œå»ºæ¨¡ã€ç”¨æˆ·æ§åˆ¶äº’åŠ¨ã€é•¿æœŸè®°å¿†èƒ½åŠ›å’Œå› æœæ¨ç†ç­‰æ–¹é¢çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18940",
            "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
            "url": "https://huggingface.co/papers/2503.18940",
            "abstract": "Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video resolution. While existing acceleration methods often compromise output quality or necessitate costly retraining, we observe that most diffusion models are pre-trained at lower resolutions, presenting an opportunity to exploit these low-resolution priors for more efficient inference without degrading performance. In this work, we introduce Bottleneck Sampling, a training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. Bottleneck Sampling follows a high-low-high denoising workflow: it performs high-resolution denoising in the initial and final stages while operating at lower resolutions in intermediate steps. To mitigate aliasing and blurring artifacts, we further refine the resolution transition points and adaptively shift the denoising timesteps at each stage. We evaluate Bottleneck Sampling on both image and video generation tasks, where extensive experiments demonstrate that it accelerates inference by up to 3times for image generation and 2.5times for video generation, all while maintaining output quality comparable to the standard full-resolution sampling process across multiple evaluation metrics. Code is available at: https://github.com/tyfeld/Bottleneck-Sampling",
            "score": 5,
            "issue_id": 2875,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "83ffcf1c20f5d4db",
            "authors": [
                "Ye Tian",
                "Xin Xia",
                "Yuxi Ren",
                "Shanchuan Lin",
                "Xing Wang",
                "Xuefeng Xiao",
                "Yunhai Tong",
                "Ling Yang",
                "Bin Cui"
            ],
            "affiliations": [
                "Bytedance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18940.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#inference",
                    "#video"
                ],
                "emoji": "â±ï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Bottleneck Sampling. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ…ĞµĞ¼Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ-Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ-Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ² 2.5-3 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Bottleneck Sampling Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Speeding Up Diffusion Models with Bottleneck Sampling",
                    "desc": "This paper presents Bottleneck Sampling, a new method to speed up diffusion models used for generating images and videos. Traditional diffusion models are slow because they use a complex self-attention mechanism that increases with image resolution. Bottleneck Sampling takes advantage of low-resolution training data to reduce the computational load during inference without sacrificing quality. By using a high-low-high denoising approach, it achieves significant speed improvementsâ€”up to 3 times faster for images and 2.5 times for videosâ€”while still producing high-quality outputs."
                },
                "zh": {
                    "title": "ç“¶é¢ˆé‡‡æ ·ï¼šé«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹æ¨ç†",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ¨ç†æ—¶ç”±äºè®¡ç®—æˆæœ¬é«˜è€Œéš¾ä»¥éƒ¨ç½²ã€‚ä¸»è¦çš„è®¡ç®—è´Ÿæ‹…æ¥è‡ªäºè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾åƒæˆ–è§†é¢‘åˆ†è¾¨ç‡ä¸Šçš„äºŒæ¬¡å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºç“¶é¢ˆé‡‡æ ·çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡çš„å…ˆéªŒçŸ¥è¯†æ¥å‡å°‘è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºè´¨é‡ã€‚é€šè¿‡åœ¨é«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡ä¹‹é—´è¿›è¡Œé«˜ä½é«˜çš„å»å™ªå·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒç”Ÿæˆä¸­åŠ é€Ÿæ¨ç†é€Ÿåº¦å¯è¾¾3å€ï¼Œåœ¨è§†é¢‘ç”Ÿæˆä¸­å¯è¾¾2.5å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17489",
            "title": "Judge Anything: MLLM as a Judge Across Any Modality",
            "url": "https://huggingface.co/papers/2503.17489",
            "abstract": "Evaluating generative foundation models on open-ended multimodal understanding (MMU) and generation (MMG) tasks across diverse modalities (e.g., images, audio, video) poses significant challenges due to the complexity of cross-modal interactions. To this end, the idea of utilizing Multimodal LLMs (MLLMs) as automated judges has emerged, with encouraging results in assessing vision-language understanding tasks. Moving further, this paper extends MLLM-as-a-Judge across modalities to a unified manner by introducing two benchmarks, TaskAnything and JudgeAnything, to respectively evaluate the overall performance and judging capabilities of MLLMs across any-to-any modality tasks. Specifically, TaskAnything evaluates the MMU and MMG capabilities across 15 any-to-any modality categories, employing 1,500 queries curated from well-established benchmarks. Furthermore, JudgeAnything evaluates the judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from the perspectives of Pair Comparison and Score Evaluation, providing a standardized testbed that incorporates human judgments and detailed rubrics. Our extensive experiments reveal that while these MLLMs show promise in assessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting and 42.79% in Score Evaluation setting), they encounter significant challenges with MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and 30.05% in Score Evaluation setting), exposing cross-modality biases and hallucination issues. To address this, we present OmniArena, an automated platform for evaluating omni-models and multimodal reward models. Our work highlights the need for fairer evaluation protocols and stronger alignment with human preferences. The source code and dataset are publicly available at: https://urrealhero.github.io/judgeanythingweb/.",
            "score": 5,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 21",
                "zh": "3æœˆ21æ—¥"
            },
            "hash": "bb040618997e1b0a",
            "authors": [
                "Shu Pu",
                "Yaochen Wang",
                "Dongping Chen",
                "Yuhang Chen",
                "Guohao Wang",
                "Qi Qin",
                "Zhongyi Zhang",
                "Zhiyuan Zhang",
                "Zetong Zhou",
                "Shuang Gong",
                "Yi Gui",
                "Yao Wan",
                "Philip S. Yu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17489.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#hallucinations",
                    "#benchmark",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ TaskAnything Ğ¸ JudgeAnything Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. TaskAnything Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ² 15 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 1500 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². JudgeAnything Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ»Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLM Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡ĞµĞ¼ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Multimodal Evaluation with MLLMs",
                    "desc": "This paper discusses the challenges of evaluating generative foundation models in tasks that involve multiple types of data, like images and audio. It introduces Multimodal LLMs (MLLMs) as automated judges to assess these models' understanding and generation capabilities across different modalities. The authors present two benchmarks, TaskAnything and JudgeAnything, to systematically evaluate MLLMs' performance and judging abilities. The findings reveal that while MLLMs perform reasonably well in understanding tasks, they struggle with generation tasks, highlighting the need for improved evaluation methods and alignment with human preferences."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€è¯„ä¼°çš„æ–°è§†è§’",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨å¤šæ¨¡æ€ç†è§£ï¼ˆMMUï¼‰å’Œç”Ÿæˆï¼ˆMMGï¼‰ä»»åŠ¡ä¸­è¯„ä¼°ç”ŸæˆåŸºç¡€æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯è·¨æ¨¡æ€äº¤äº’çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºè‡ªåŠ¨è¯„ä¼°è€…çš„æƒ³æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªåŸºå‡†ï¼šTaskAnythingå’ŒJudgeAnythingï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°MLLMsåœ¨ä»»ä½•æ¨¡æ€ä»»åŠ¡ä¸­çš„æ•´ä½“æ€§èƒ½å’Œåˆ¤æ–­èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡MLLMsåœ¨MMUä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸€å®šçš„æ½œåŠ›ï¼Œä½†åœ¨MMGä»»åŠ¡ä¸­é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œæš´éœ²äº†è·¨æ¨¡æ€åè§å’Œå¹»è§‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OmniArenaï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹çš„è‡ªåŠ¨åŒ–å¹³å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17439",
            "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
            "url": "https://huggingface.co/papers/2503.17439",
            "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines.",
            "score": 2,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 21",
                "zh": "3æœˆ21æ—¥"
            },
            "hash": "946d486485fedb03",
            "authors": [
                "Zhuoshi Pan",
                "Yu Li",
                "Honglin Lin",
                "Qizhi Pei",
                "Zinan Tang",
                "Wei Wu",
                "Chenlin Ming",
                "H. Vicky Zhao",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17439.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#reasoning",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ LEMMA Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…. LEMMA ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ²ÑĞ·ÑĞ¼Ğ¸ Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LEMMA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering LLMs: Learning from Errors to Enhance Reasoning",
                    "desc": "This paper introduces a novel approach called Learning from Errors for Mathematical Advancement (LEMMA) to improve the reasoning capabilities of large language models (LLMs) in solving mathematical problems. Unlike traditional methods that focus solely on enhancing correct training data, LEMMA leverages the value of error data by constructing a dataset that includes incorrect solutions paired with reflections on correct solutions. The method systematically analyzes error types and employs a mistake augmentation technique to gather diverse errors, allowing the model to learn from its mistakes. By fine-tuning on this enriched dataset, LEMMA enables LLMs to autonomously correct their errors during the generation process, leading to significant performance gains compared to existing methods."
                },
                "zh": {
                    "title": "ä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œæå‡æ•°å­¦æ¨ç†èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶å±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨æé«˜æ­£ç¡®è®­ç»ƒæ•°æ®çš„è´¨é‡ï¼Œè€Œå¿½è§†äº†é”™è¯¯æ•°æ®çš„ä»·å€¼ï¼Œè¿™å¯èƒ½ä¼šå¦¨ç¢æ¨¡å‹çš„åæ€èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡å­¦ä¹ é”™è¯¯æ¥æå‡æ•°å­¦æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œç§°ä¸ºLEMMAã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºåŒ…å«é”™è¯¯æ­¥éª¤çš„é”™è¯¯è§£å’Œä¸æ­£ç¡®è§£çš„åæ€è¿æ¥çš„æ•°æ®é›†ï¼Œæ¥è¿›è¡Œæ¨¡å‹çš„å¾®è°ƒï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è‡ªä¸»çº æ­£é”™è¯¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18769",
            "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
            "url": "https://huggingface.co/papers/2503.18769",
            "abstract": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet.",
            "score": 1,
            "issue_id": 2875,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "e92ee9df78b66019",
            "authors": [
                "Alan Dao",
                "Dinh Bach Vu",
                "Bui Quang Huy"
            ],
            "affiliations": [
                "Menlo Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18769.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#reasoning",
                    "#3d"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "AlphaSpace: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "AlphaSpace - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ 3D Ğ´ĞµĞºĞ°Ñ€Ñ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²Ñ‹ÑĞ¾Ñ‚Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼ [x, y, z]. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AlphaSpace Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 66,67%."
                },
                "en": {
                    "title": "Enhancing 3D Navigation in Language Models with AlphaSpace",
                    "desc": "This paper introduces AlphaSpace, a new method aimed at improving how large language models (LLMs) understand and navigate 3D spaces. It uses a unique tokenization method that incorporates height information through special semantic tokens, allowing for better spatial reasoning. By combining this with symbolic reasoning data, AlphaSpace enables LLMs to effectively manipulate objects in a 3D environment by placing them at precise coordinates. The results show that AlphaSpace achieves a notable accuracy of 66.67% in manipulation tasks, outperforming other models like GPT-4o and Claude 3.5 Sonnet."
                },
                "zh": {
                    "title": "AlphaSpaceï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•AlphaSpaceï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸‰ç»´ç¬›å¡å°”ç©ºé—´å¯¼èˆªä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚AlphaSpaceé‡‡ç”¨åŸºäºè¯­ä¹‰çš„æ ‡è®°åŒ–ç­–ç•¥ï¼Œé€šè¿‡ä¸“é—¨çš„è¯­ä¹‰æ ‡è®°ç¼–ç é«˜åº¦ä¿¡æ¯ï¼Œå¹¶ä¸»è¦æ•´åˆç¬¦å·åˆæˆæ¨ç†æ•°æ®ã€‚è¯¥æ–¹æ³•ä½¿å¾—LLMsèƒ½å¤Ÿå‡†ç¡®åœ°é€šè¿‡ç‰¹å®šçš„[x, y, z]åæ ‡æ¥æ“ä½œç‰©ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlphaSpaceåœ¨æ“ä½œå­ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ€»å‡†ç¡®ç‡è¾¾åˆ°66.67%ï¼Œè€ŒGPT-4oä¸º37.5%ï¼ŒClaude 3.5 Sonnetä¸º29.17%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14428",
            "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
            "url": "https://huggingface.co/papers/2503.14428",
            "abstract": "Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we propose MagicComp, a training-free method that enhances compositional T2V generation through dual-phase refinement. Specifically, (1) During the Conditioning Stage: We introduce the Semantic Anchor Disambiguation to reinforces subject-specific semantics and resolve inter-subject ambiguity by progressively injecting the directional vectors of semantic anchors into original text embedding; (2) During the Denoising Stage: We propose Dynamic Layout Fusion Attention, which integrates grounding priors and model-adaptive spatial perception to flexibly bind subjects to their spatiotemporal regions through masked attention modulation. Furthermore, MagicComp is a model-agnostic and versatile approach, which can be seamlessly integrated into existing T2V architectures. Extensive experiments on T2V-CompBench and VBench demonstrate that MagicComp outperforms state-of-the-art methods, highlighting its potential for applications such as complex prompt-based and trajectory-controllable video generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
            "score": 1,
            "issue_id": 2875,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "1cd532518024f266",
            "authors": [
                "Hongyu Zhang",
                "Yufan Deng",
                "Shenghai Yuan",
                "Peng Jin",
                "Zesen Cheng",
                "Yian Zhao",
                "Chang Liu",
                "Jie Chen"
            ],
            "affiliations": [
                "Peng Cheng Laboratory, Shenzhen, China",
                "School of Electronic and Computer Engineering, Peking University, Shenzhen, China",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14428.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#architecture",
                    "#games",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "MagicComp: Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "MagicComp - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ², Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. MagicComp Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Text-to-Video Generation with MagicComp",
                    "desc": "This paper presents MagicComp, a novel method for improving text-to-video (T2V) generation using diffusion models. It addresses challenges in accurately linking attributes and understanding spatial relationships between subjects in videos. The method consists of two main phases: the Conditioning Stage, which clarifies subject semantics using Semantic Anchor Disambiguation, and the Denoising Stage, which employs Dynamic Layout Fusion Attention to enhance spatial binding. MagicComp is designed to be adaptable and can be integrated into existing T2V systems, showing superior performance in various benchmarks."
                },
                "zh": {
                    "title": "MagicCompï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMagicCompçš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å±æ€§ç»‘å®šã€ç©ºé—´å…³ç³»ç¡®å®šå’Œå¤æ‚åŠ¨ä½œäº¤äº’æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•é€šè¿‡åŒé˜¶æ®µçš„ç²¾ç‚¼è¿‡ç¨‹æ¥å¢å¼ºç»„åˆå¼T2Vç”Ÿæˆï¼Œé¦–å…ˆåœ¨æ¡ä»¶é˜¶æ®µå¼•å…¥è¯­ä¹‰é”šç‚¹æ¶ˆæ­§ï¼Œä»¥å¼ºåŒ–ç‰¹å®šä¸»é¢˜çš„è¯­ä¹‰å¹¶è§£å†³ä¸»é¢˜é—´çš„æ­§ä¹‰ã€‚å…¶æ¬¡ï¼Œåœ¨å»å™ªé˜¶æ®µï¼Œæå‡ºåŠ¨æ€å¸ƒå±€èåˆæ³¨æ„åŠ›ï¼Œé€šè¿‡æ©è”½æ³¨æ„åŠ›è°ƒåˆ¶çµæ´»ç»‘å®šä¸»é¢˜ä¸å…¶æ—¶ç©ºåŒºåŸŸã€‚MagicCompæ˜¯ä¸€ç§ä¸æ¨¡å‹æ— å…³çš„é€šç”¨æ–¹æ³•ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„T2Væ¶æ„ä¸­ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-24.html",
    "link_next": "2025-03-26.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "24.03",
        "en": "03/24",
        "zh": "3æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "26.03",
        "en": "03/26",
        "zh": "3æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤šæ¨¡æ€ç§‘å­¦é—®é¢˜ï¼ˆMSPsï¼‰åŠå…¶æŒ‘æˆ˜ã€‚MSPséœ€è¦æ•´åˆå¤šç§æ¨¡æ€ï¼Œå¦‚æ–‡æœ¬å’Œå›¾è¡¨ã€‚ç›®å‰ï¼ŒMSPsé¢ä¸´ä¸¤å¤§é—®é¢˜ï¼šå¤šæ¨¡æ€æ¨ç†å’Œç¼ºä¹åæ€èƒ½åŠ›ã€‚ä½œè€…æå‡ºäº†åŸºäºå¤§ä¸ƒäººæ ¼å’Œè‹æ ¼æ‹‰åº•æŒ‡å¯¼çš„å¤šä»£ç†æ¡†æ¶ï¼ˆMAPSï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€ä½³æ¨¡å‹ã€‚",
        "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤šæ¨¡æ€ç§‘å­¦é—®é¢˜ï¼ˆMSPsï¼‰åŠå…¶æŒ‘æˆ˜ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le duÅ mÃ³shuÃ i kÄ“xuÃ© wÃ¨ntÃ­ (MSPs) jÃ­ qÃ­ tiÇozhÃ n.\n\nMSPséœ€è¦æ•´åˆå¤šç§æ¨¡æ€ï¼Œå¦‚æ–‡æœ¬å’Œå›¾è¡¨ã€‚\nMSPs xÅ«yÃ o zhÄ›nghÃ© duÅ zhÇ’ng mÃ³shuÃ i, rÃº wÃ©nbÄ›n hÃ© tÃºbiÇo.\n\nç›®å‰ï¼ŒMSPsé¢ä¸´ä¸¤å¤§é—®é¢˜ï¼šå¤šæ¨¡æ€æ¨ç†å’Œç¼ºä¹åæ€èƒ½åŠ›ã€‚\nMÃ¹qiÃ¡n, MSPs miÃ nlÃ­n liÇng dÃ  wÃ¨ntÃ­: duÅ mÃ³shuÃ i tuÄ«lÇ hÃ© quÄ“fÃ¡ fÇnsÄ« nÃ©nglÃ¬.\n\nä½œè€…æå‡ºäº†åŸºäºå¤§ä¸ƒäººæ ¼å’Œè‹æ ¼æ‹‰åº•æŒ‡å¯¼çš„å¤šä»£ç†æ¡†æ¶ï¼ˆMAPSï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚\nZuÃ²zhÄ› tÃ­chÅ« le jÄ«yÃº dÃ  qÄ« rÃ©ngÃ© hÃ© SÅ«gÃ©lÄdÇ zhÇdÇo de duÅ dÃ ilÇ kuÃ ngjiÃ  (MAPS) lÃ¡i jiÄ›juÃ© zhÃ¨xiÄ“ wÃ¨ntÃ­.\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€ä½³æ¨¡å‹ã€‚\nShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, gÇi kuÃ ngjiÃ  zÃ i duÅ gÃ¨ shÃ¹jÃ¹jÃ­ shÃ ng biÇoxiÃ n chÅ«sÃ¨, chÄoyuÃ¨ le xiÃ nyÇ’u de zuÃ¬jiÄ mÃ³xÃ­ng.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"ç§‘å­¦é—®é¢˜\", \"pinyin\": \"kÄ“ xuÃ© wÃ¨n tÃ­\", \"trans\": \"scientific problems\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenges\"},\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›ng hÃ©\", \"trans\": \"integrate\"},\n    {\"word\": \"æ¨¡æ€\", \"pinyin\": \"mÃ³ tÃ i\", \"trans\": \"modality\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©n bÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"å›¾è¡¨\", \"pinyin\": \"tÃº biÇo\", \"trans\": \"charts\"},\n    {\"word\": \"é¢ä¸´\", \"pinyin\": \"miÃ n lÃ­n\", \"trans\": \"face\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"åæ€\", \"pinyin\": \"fÇn sÄ«\", \"trans\": \"reflection\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"å¤§ä¸ƒäººæ ¼\", \"pinyin\": \"dÃ  qÄ« rÃ©n gÃ©\", \"trans\": \"Big Five personality traits\"},\n    {\"word\": \"è‹æ ¼æ‹‰åº•\", \"pinyin\": \"sÅ« gÃ© lÄ dÇ\", \"trans\": \"Socrates\"},\n    {\"word\": \"æŒ‡å¯¼\", \"pinyin\": \"zhÇ dÇo\", \"trans\": \"guidance\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"results\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ n yÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"æœ€ä½³\", \"pinyin\": \"zuÃ¬ jiÄ\", \"trans\": \"best\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"}\n]",
        "trans": "This article discusses multimodal scientific problems (MSPs) and their challenges. MSPs require the integration of multiple modalities, such as text and graphs. Currently, MSPs face two major issues: multimodal reasoning and a lack of reflective capabilities. The authors propose a multi-agent framework (MAPS) based on the Big Five personality traits and Socratic guidance to address these issues. Experimental results show that this framework performs excellently on multiple datasets, surpassing existing state-of-the-art models.",
        "update_ts": "2025-03-24 09:12"
    }
}