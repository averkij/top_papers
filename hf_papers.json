{
    "date": {
        "ru": "27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 27",
        "zh": "10æœˆ27æ—¥"
    },
    "time_utc": "2025-10-27 13:27",
    "weekday": 0,
    "issue_id": 6632,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.21618",
            "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
            "url": "https://huggingface.co/papers/2510.21618",
            "abstract": "DeepAgent, an end-to-end deep reasoning agent, autonomously performs thinking, tool discovery, and action execution using memory folding and reinforcement learning, outperforming baselines in various tool-use and application tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
            "score": 53,
            "issue_id": 6621,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "58d13c7e0c138478",
            "authors": [
                "Xiaoxi Li",
                "Wenxiang Jiao",
                "Jiarui Jin",
                "Guanting Dong",
                "Jiajie Jin",
                "Yinuo Wang",
                "Hao Wang",
                "Yutao Zhu",
                "Ji-Rong Wen",
                "Yuan Lu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Renmin University of China",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21618.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "DeepAgent: Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ reasoning Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ DeepAgent â€” end-to-end Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ. Ğ”Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (memory folding), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ² ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ñ€Ğ°Ğ±Ğ¾Ñ‡ÑƒÑ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ reinforcement learning ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ToolPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ LLM API Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ credit assignment Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DeepAgent Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² open-set ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "DeepAgent: Autonomous Reasoning and Tool Discovery for Real-World Tasks",
                    "desc": "DeepAgent is a novel deep reasoning agent designed to autonomously think, discover tools, and execute actions in a unified process. It addresses the challenges of long-horizon interactions by implementing a memory folding mechanism that compresses past interactions, thus minimizing errors while retaining essential information. The agent is trained using an end-to-end reinforcement learning approach called ToolPO, which effectively assigns credit to tool usage based on simulated API interactions. Extensive testing shows that DeepAgent outperforms existing frameworks in various tool-use and application tasks, paving the way for more capable AI agents in real-world scenarios."
                },
                "zh": {
                    "title": "DeepAgentï¼šè‡ªä¸»æ€è€ƒä¸å·¥å…·å‘ç°çš„æ™ºèƒ½ä»£ç†",
                    "desc": "DeepAgentæ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„æ·±åº¦æ¨ç†ä»£ç†ï¼Œèƒ½å¤Ÿè‡ªä¸»è¿›è¡Œæ€è€ƒã€å·¥å…·å‘ç°å’Œè¡ŒåŠ¨æ‰§è¡Œã€‚å®ƒé€šè¿‡è®°å¿†æŠ˜å å’Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œè§£å†³äº†é•¿æ—¶é—´äº¤äº’ä¸­çš„ä¸Šä¸‹æ–‡é•¿åº¦çˆ†ç‚¸é—®é¢˜ã€‚DeepAgentåœ¨å¤šä¸ªå·¥å…·ä½¿ç”¨å’Œåº”ç”¨ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºæ›´é€šç”¨å’Œå¼ºå¤§çš„ç°å®ä¸–ç•Œä»£ç†è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20888",
            "title": "Video-As-Prompt: Unified Semantic Control for Video Generation",
            "url": "https://huggingface.co/papers/2510.20888",
            "abstract": "Video-As-Prompt (VAP) uses a reference video to guide a frozen Video Diffusion Transformer via a Mixture-of-Transformers expert, achieving state-of-the-art results in semantic-controlled video generation with strong zero-shot generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation.",
            "score": 28,
            "issue_id": 6621,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "519377609a2a0255",
            "authors": [
                "Yuxuan Bian",
                "Xin Chen",
                "Zenan Li",
                "Tiancheng Zhi",
                "Shen Sang",
                "Linjie Luo",
                "Qiang Xu"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20888.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#open_source",
                    "#dataset",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Video-As-Prompt (VAP) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³Ğ´Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Video Diffusion Transformer. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Mixture-of-Transformers ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ zero-shot Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ VAP-Data â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100K Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ 100 ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ open-source Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ 38.7% Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Contextual Prompts",
                    "desc": "Video-As-Prompt (VAP) is a novel approach in video generation that utilizes a reference video to guide a pre-trained Video Diffusion Transformer, enhancing semantic control without the need for extensive fine-tuning. This method employs a Mixture-of-Transformers expert to ensure robust context retrieval while avoiding common pitfalls like catastrophic forgetting and inappropriate pixel-wise priors. VAP introduces a temporally biased position embedding to improve the model's ability to generate coherent and contextually relevant videos. With the creation of VAP-Data, a comprehensive dataset of over 100,000 paired videos, VAP achieves state-of-the-art results in user preference and demonstrates strong zero-shot generalization capabilities."
                },
                "zh": {
                    "title": "è§†é¢‘ç”Ÿæˆçš„æ–°èŒƒå¼ï¼šè§†é¢‘ä½œä¸ºæç¤º",
                    "desc": "Video-As-Prompt (VAP) æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å‚è€ƒè§†é¢‘æ¥å¼•å¯¼ä¸€ä¸ªå†»ç»“çš„Video Diffusion Transformerï¼Œä»è€Œå®ç°è¯­ä¹‰æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆã€‚è¯¥æ–¹æ³•ä½¿ç”¨æ··åˆå˜æ¢å™¨ä¸“å®¶ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¸¸è§çš„ä¼ªå½±é—®é¢˜ï¼Œå¹¶ä¸”ä¸éœ€è¦ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒã€‚VAP é€šè¿‡æ—¶é—´åç½®ä½ç½®åµŒå…¥æ¥å¢å¼ºä¸Šä¸‹æ–‡æ£€ç´¢çš„é²æ£’æ€§ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚VAP-Data æ•°æ®é›†æ˜¯ç›®å‰æœ€å¤§çš„è¯­ä¹‰æ§åˆ¶è§†é¢‘ç”Ÿæˆæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10ä¸‡å¯¹è§†é¢‘ï¼Œæ¨åŠ¨äº†è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19871",
            "title": "From Denoising to Refining: A Corrective Framework for Vision-Language\n  Diffusion Model",
            "url": "https://huggingface.co/papers/2510.19871",
            "abstract": "ReDiff, a refining-enhanced diffusion framework, addresses train-inference discrepancies in discrete diffusion models by enabling the model to identify and correct its own errors, improving coherence and factual accuracy in generated content.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete diffusion models have emerged as a promising direction for vision-language tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by a train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering a chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, a refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features a two-stage training process: first, we instill a foundational revision capability by training the model to revise synthetic errors; second, we implement a novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an expert's corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at https://rediff-hku.github.io/.",
            "score": 23,
            "issue_id": 6622,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "5e3206cbeb5b995b",
            "authors": [
                "Yatai Ji",
                "Teng Wang",
                "Yuying Ge",
                "Zhiheng Liu",
                "Sidi Yang",
                "Ying Shan",
                "Ping Luo"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19871.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#optimization",
                    "#diffusion",
                    "#rl",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸",
                    "desc": "Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞºĞ°ÑĞºĞ°Ğ´Ñƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ReDiff Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ ÑˆÑƒĞ¼, Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¸, ÑƒÑ‡Ğ°ÑÑŒ Ğ½Ğ° Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "Refining Errors for Better AI Generation",
                    "desc": "ReDiff is a new framework designed to improve discrete diffusion models by addressing the errors that occur during the generation of content. It focuses on correcting mistakes made during the initial token generation, which can lead to a series of compounding errors. The framework employs a two-stage training process that first teaches the model to revise synthetic errors and then allows it to learn from its own mistakes through an online self-correction loop. This approach enhances the model's ability to produce coherent and factually accurate outputs, making it more effective than traditional methods."
                },
                "zh": {
                    "title": "ä¸»åŠ¨ä¿®æ­£ï¼Œæå‡ç”Ÿæˆè´¨é‡çš„ReDiffæ¡†æ¶",
                    "desc": "ReDiffæ˜¯ä¸€ç§å¢å¼ºç²¾ç‚¼çš„æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„å·®å¼‚ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸»åŠ¨ä¿®æ­£è‡ªèº«é”™è¯¯ï¼Œæé«˜ç”Ÿæˆå†…å®¹çš„ä¸€è‡´æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒè¿‡ç¨‹ï¼Œé¦–å…ˆè®­ç»ƒæ¨¡å‹ä¿®æ­£åˆæˆé”™è¯¯ï¼Œç„¶åé€šè¿‡åœ¨çº¿è‡ªæˆ‘ä¿®æ­£å¾ªç¯ï¼Œè®©æ¨¡å‹å­¦ä¹ ä¸“å®¶çš„ä¿®æ­£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReDiffæ˜¾è‘—æå‡äº†ç”Ÿæˆå†…å®¹çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿå»å™ªæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21583",
            "title": "Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image\n  Generation",
            "url": "https://huggingface.co/papers/2510.21583",
            "abstract": "Chunk-GRPO, a chunk-level optimization approach for text-to-image generation, improves preference alignment and image quality by addressing inaccurate advantage attribution and neglecting temporal dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods.",
            "score": 22,
            "issue_id": 6625,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "de5b9cebb6c64fcc",
            "authors": [
                "Yifu Luo",
                "Penghui Du",
                "Bo Li",
                "Sinan Du",
                "Tiantian Zhang",
                "Yongzhe Chang",
                "Kai Wu",
                "Kun Gai",
                "Xueqian Wang"
            ],
            "affiliations": [
                "Kolors Team, Kuaishou Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21583.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#cv",
                    "#rl"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚ÑĞ¼: ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Chunk-GRPO â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğµ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸, Ğ° Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² (Â«Ñ‡Ğ°Ğ½ĞºĞ°Ğ¼Ğ¸Â»). ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ GRPO: Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒÑ ÑˆĞ°Ğ³Ğ¸ Ğ² ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ flow matching Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Chunk-GRPO ĞºĞ°Ğº Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Generation with Chunk-Level Optimization",
                    "desc": "Chunk-GRPO is a novel approach for improving text-to-image generation by optimizing at the chunk level rather than the individual step level. This method addresses two main challenges: inaccurate advantage attribution and the neglect of temporal dynamics during the generation process. By grouping consecutive steps into coherent 'chunks', it captures the flow of generation more effectively and enhances policy optimization. The introduction of a weighted sampling strategy further boosts performance, leading to better preference alignment and higher image quality in the generated outputs."
                },
                "zh": {
                    "title": "å—çº§ä¼˜åŒ–æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè´¨é‡",
                    "desc": "Chunk-GRPOæ˜¯ä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å—çº§ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åå¥½å¯¹é½å’Œå›¾åƒè´¨é‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼˜åŠ¿å½’å› ä¸å‡†ç¡®å’Œå¿½è§†ç”Ÿæˆæ—¶é—´åŠ¨æ€çš„é—®é¢˜ã€‚é€šè¿‡å°†ä¼˜åŒ–èŒƒå¼ä»æ­¥éª¤çº§åˆ«è½¬ç§»åˆ°å—çº§åˆ«ï¼ŒChunk-GRPOèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æµåŒ¹é…çš„å†…åœ¨æ—¶é—´åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChunk-GRPOåœ¨åå¥½å¯¹é½å’Œå›¾åƒè´¨é‡æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å—çº§ä¼˜åŒ–åœ¨GRPOæ–¹æ³•ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18212",
            "title": "A Definition of AGI",
            "url": "https://huggingface.co/papers/2510.18212",
            "abstract": "A quantifiable framework based on Cattell-Horn-Carroll theory evaluates AI systems across ten cognitive domains, revealing significant gaps in foundational cognitive abilities like long-term memory.  \t\t\t\t\tAI-generated summary \t\t\t\t The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly \"jagged\" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify both rapid progress and the substantial gap remaining before AGI.",
            "score": 17,
            "issue_id": 6621,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "be8269623d74c8cf",
            "authors": [
                "Dan Hendrycks",
                "Dawn Song",
                "Christian Szegedy",
                "Honglak Lee",
                "Yarin Gal",
                "Erik Brynjolfsson",
                "Sharon Li",
                "Andy Zou",
                "Lionel Levine",
                "Bo Han",
                "Jie Fu",
                "Ziwei Liu",
                "Jinwoo Shin",
                "Kimin Lee",
                "Mantas Mazeika",
                "Long Phan",
                "George Ingebretsen",
                "Adam Khoja",
                "Cihang Xie",
                "Olawale Salaudeen",
                "Matthias Hein",
                "Kevin Zhao",
                "Alexander Pan",
                "David Duvenaud",
                "Bo Li",
                "Steve Omohundro",
                "Gabriel Alfour",
                "Max Tegmark",
                "Kevin McGrew",
                "Gary Marcus",
                "Jaan Tallinn",
                "Eric Schmidt",
                "Yoshua Bengio"
            ],
            "affiliations": [
                "Beneficial AI Research",
                "CSER",
                "Carnegie Mellon University",
                "Center for AI Safety",
                "Conjecture",
                "Cornell University",
                "Gray Swan AI",
                "HKUST",
                "Hong Kong Baptist University",
                "Institute for Applied Psychometrics",
                "KAIST",
                "LG AI Research",
                "LawZero",
                "Massachusetts Institute of Technology",
                "Morph Labs",
                "Nanyang Technological University",
                "New York University",
                "Stanford University",
                "University of California, Berkeley",
                "University of California, Santa Cruz",
                "University of Chicago",
                "University of Michigan",
                "University of Oxford",
                "University of Toronto",
                "University of TÃ¼bingen",
                "University of Washington",
                "University of Wisconsin-Madison",
                "UniversitÃ© de MontrÃ©al",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18212.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#long_context",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ÑÑ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ´Ğ¾ AGI Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ĞšĞµÑ‚Ñ‚ĞµĞ»Ğ»Ğ°-Ğ¥Ğ¾Ñ€Ğ½Ğ°-ĞšÑÑ€Ñ€Ğ¾Ğ»Ğ»Ğ°, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ Ğ´ĞµÑÑÑ‚ÑŒ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ \"Ğ·ÑƒĞ±Ñ‡Ğ°Ñ‚Ñ‹Ğ¹\" Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ğ½Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¸Ğ¼ĞµÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞŸĞ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ğµ GPT-4 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 27% Ğ½Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğº AGI, Ğ° GPT-5 - 58%, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ³Ğ»ÑĞ´Ğ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸Ğ¹ÑÑ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ´Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ."
                },
                "en": {
                    "title": "Bridging the Gap to Artificial General Intelligence",
                    "desc": "This paper presents a framework to evaluate AI systems based on the Cattell-Horn-Carroll theory of intelligence, which breaks down general intelligence into ten cognitive domains. It aims to define Artificial General Intelligence (AGI) as the ability to perform across these domains like a well-educated adult. The evaluation shows that while current AI models excel in knowledge-heavy tasks, they significantly lack in foundational cognitive abilities, especially in long-term memory. The framework provides quantifiable AGI scores, highlighting both the advancements made and the considerable gaps that still exist in achieving true AGI."
                },
                "zh": {
                    "title": "é‡åŒ–æ¡†æ¶æ­ç¤ºAIè®¤çŸ¥èƒ½åŠ›çš„å·®è·",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºCattell-Horn-Carrollç†è®ºçš„é‡åŒ–æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨åä¸ªè®¤çŸ¥é¢†åŸŸçš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„AIç³»ç»Ÿåœ¨åŸºç¡€è®¤çŸ¥èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æœŸè®°å¿†æ–¹é¢ã€‚é€šè¿‡å°†ä¸€èˆ¬æ™ºåŠ›åˆ†è§£ä¸ºæ¨ç†ã€è®°å¿†å’Œæ„ŸçŸ¥ç­‰æ ¸å¿ƒè®¤çŸ¥é¢†åŸŸï¼Œæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°AIçš„è®¤çŸ¥èƒ½åŠ›ã€‚æœ€ç»ˆçš„AGIè¯„åˆ†æ˜¾ç¤ºï¼Œå°½ç®¡AIåœ¨çŸ¥è¯†å¯†é›†å‹é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å®ç°ä¸äººç±»ç›¸å½“çš„è®¤çŸ¥çµæ´»æ€§å’Œç†Ÿç»ƒåº¦æ–¹é¢ä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21270",
            "title": "Sparser Block-Sparse Attention via Token Permutation",
            "url": "https://huggingface.co/papers/2510.21270",
            "abstract": "Permuted Block-Sparse Attention improves computational efficiency in large language models by enhancing block-level sparsity in the self-attention mechanism, achieving significant speedups without compromising accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose O(N^2) complexity with respect to sequence length presents a major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (PBS-Attn), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to 2.75times in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn",
            "score": 16,
            "issue_id": 6622,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "37b575b4da897688",
            "authors": [
                "Xinghao Wang",
                "Pengyu Wang",
                "Dong Zhang",
                "Chenkun Tan",
                "Shaojun Zhou",
                "Zhaoxiang Liu",
                "Shiguo Lian",
                "Fangxu Liu",
                "Kai Song",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "ByteDance",
                "China Unicom",
                "Fudan University",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21270.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#training",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Permuted Block-Sparse Attention (PBS-Attn) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° self-attention, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ÑƒĞ·ĞºĞ¸Ğ¼ Ğ¼ĞµÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. PBS-Attn Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ†ĞµĞ»Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 2.75 Ñ€Ğ°Ğ· Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ prefilling Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ attention."
                },
                "en": {
                    "title": "Boosting Efficiency with Permuted Block-Sparse Attention",
                    "desc": "The paper introduces Permuted Block-Sparse Attention (PBS-Attn), a method designed to improve the efficiency of large language models (LLMs) by optimizing the self-attention mechanism. It addresses the computational challenges posed by the O(N^2) complexity of traditional attention methods, particularly for long sequences. By enhancing block-level sparsity through permutation properties, PBS-Attn reduces unnecessary computations while maintaining model accuracy. Experimental results show that PBS-Attn significantly speeds up processing times, achieving up to 2.75 times faster performance compared to existing block-sparse attention techniques."
                },
                "zh": {
                    "title": "æå‡å¤§è¯­è¨€æ¨¡å‹æ•ˆç‡çš„å—ç¨€ç–æ³¨æ„åŠ›",
                    "desc": "Permuted Block-Sparse Attentionï¼ˆPBS-Attnï¼‰æ˜¯ä¸€ç§æé«˜å¤§è¯­è¨€æ¨¡å‹è®¡ç®—æ•ˆç‡çš„æ–¹æ³•ï¼Œé€šè¿‡å¢å¼ºè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å—çº§ç¨€ç–æ€§æ¥å®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼Œè€Œä¸å½±å“å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹åºåˆ—è¿›è¡Œåˆ†å—å¤„ç†ï¼Œè·³è¿‡éƒ¨åˆ†å—çš„è®¡ç®—ï¼Œä»è€Œä¼˜åŒ–äº†è®¡ç®—è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPBS-Attnåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†æ—¶ï¼Œèƒ½å¤Ÿåœ¨æ¨¡å‹å‡†ç¡®æ€§ä¸Šè¶…è¶Šç°æœ‰çš„å—ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œå¹¶ä¸”ä¸å®Œæ•´æ³¨æ„åŠ›åŸºçº¿ç›¸è¿‘ã€‚é€šè¿‡å®šåˆ¶çš„permute-FlashAttentionå†…æ ¸ï¼ŒPBS-Attnåœ¨é•¿ä¸Šä¸‹æ–‡é¢„å¡«å……ä¸­å®ç°äº†æœ€é«˜2.75å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œè¯æ˜äº†å…¶å®é™…å¯è¡Œæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20286",
            "title": "UI-Ins: Enhancing GUI Grounding with Multi-Perspective\n  Instruction-as-Reasoning",
            "url": "https://huggingface.co/papers/2510.20286",
            "abstract": "The Instruction-as-Reasoning paradigm enhances GUI grounding by treating instructions as dynamic pathways, improving performance through multi-perspective reasoning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.",
            "score": 16,
            "issue_id": 6621,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "7a550042831907d9",
            "authors": [
                "Liangyu Chen",
                "Hanzhang Zhou",
                "Chenglin Cai",
                "Jianan Zhang",
                "Panrong Tong",
                "Quyu Kong",
                "Xu Zhang",
                "Chen Liu",
                "Yuqi Liu",
                "Wenxuan Wang",
                "Yue Wang",
                "Qin Jin",
                "Steven Hoi"
            ],
            "affiliations": [
                "CUHK",
                "Renmin University of China",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20286.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#open_source",
                    "#benchmark",
                    "#agents",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Instruction-as-Reasoning Ğ´Ğ»Ñ GUI grounding - Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿ÑƒÑ‚Ğ¸, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…, Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ UI-Ins-7B Ğ¸ UI-Ins-32B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ UI-Ins-32B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° 87.3% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° UI-I2E-Bench Ğ¸ 74.1% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° AndroidWorld."
                },
                "en": {
                    "title": "Dynamic Pathways for Enhanced GUI Grounding",
                    "desc": "This paper introduces the Instruction-as-Reasoning paradigm, which enhances GUI grounding by treating instructions as dynamic pathways for reasoning. It highlights the flaws in existing grounding datasets and demonstrates that leveraging instruction diversity can significantly improve performance. The authors propose a two-stage training framework that combines supervised fine-tuning and reinforcement learning to optimize the selection of effective reasoning pathways. Their models achieve state-of-the-art results on multiple benchmarks, showcasing improved grounding accuracy and emergent reasoning capabilities."
                },
                "zh": {
                    "title": "æŒ‡ä»¤ä½œä¸ºæ¨ç†ï¼šæå‡GUIåŸºç¡€æ€§èƒ½çš„åŠ¨æ€è·¯å¾„",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†\"æŒ‡ä»¤ä½œä¸ºæ¨ç†\"çš„èŒƒå¼ï¼Œé€šè¿‡å°†æŒ‡ä»¤è§†ä¸ºåŠ¨æ€åˆ†æè·¯å¾„ï¼Œæ¥å¢å¼ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åŸºç¡€çš„æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ•°æ®é›†ä¸­æŒ‡ä»¤çš„å¤šæ ·æ€§å’Œè´¨é‡å¯¹åŸºç¡€æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œä¸”åœ¨æ¨ç†æ—¶åˆ©ç”¨æŒ‡ä»¤å¤šæ ·æ€§å¯ä»¥æé«˜æ€§èƒ½è¾¾76%ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥åŸ¹å…»å¤šè§’åº¦æ¨ç†ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–è·¯å¾„é€‰æ‹©ã€‚æœ€ç»ˆæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨UI-I2E-Benchä¸Šè¾¾åˆ°äº†87.3%çš„æœ€ä½³å‡†ç¡®ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14901",
            "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
            "url": "https://huggingface.co/papers/2510.14901",
            "abstract": "An iterative sampling algorithm enhances reasoning capabilities in base models without additional training, matching or outperforming reinforcement learning on single-shot tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.",
            "score": 16,
            "issue_id": 6624,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "eab418eb30e48c40",
            "authors": [
                "Aayush Karan",
                "Yilun Du"
            ],
            "affiliations": [
                "Harvard University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14901.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#math",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ¸Ğ»Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… LLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ñ… Markov chain Monte Carlo Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ reinforcement learning Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ¸Ğ¿Ğ° MATH500, HumanEval Ğ¸ GPQA. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ â€” Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¸Ğ»Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Boosting Reasoning with Iterative Sampling!",
                    "desc": "This paper presents an iterative sampling algorithm that enhances the reasoning abilities of base models without the need for additional training. The approach leverages the models' own likelihoods, inspired by Markov chain Monte Carlo techniques, to generate improved reasoning outputs. The results demonstrate that this method can match or even surpass the performance of reinforcement learning on various single-shot tasks. Importantly, the algorithm maintains diversity in its outputs, avoiding common pitfalls associated with reinforcement learning post-training."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒå¢å¼ºæ¨ç†èƒ½åŠ›çš„è¿­ä»£é‡‡æ ·ç®—æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è¿­ä»£é‡‡æ ·ç®—æ³•ï¼Œå¯ä»¥åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å¢å¼ºåŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥ç®—æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹è‡ªèº«çš„å¯èƒ½æ€§ï¼Œé€šè¿‡çº¯é‡‡æ ·åœ¨æ¨ç†æ—¶å¼•å‘ç±»ä¼¼äºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªå•æ¬¡ä»»åŠ¡ä¸Šï¼Œæ¨ç†èƒ½åŠ›å‡ ä¹å¯ä»¥ä¸å¼ºåŒ–å­¦ä¹ ç›¸åª²ç¾ï¼Œç”šè‡³è¶…è¶Šå…¶è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é¿å…äº†å¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­æ ·æœ¬å¤šæ ·æ€§ä¸‹é™çš„é—®é¢˜ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21697",
            "title": "Visual Diffusion Models are Geometric Solvers",
            "url": "https://huggingface.co/papers/2510.21697",
            "abstract": "Visual diffusion models can solve geometric problems by transforming noisy images into valid solutions, demonstrating a novel approach to geometric reasoning through image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem.   Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation.   Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks.",
            "score": 11,
            "issue_id": 6621,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "ca1111516b2085c3",
            "authors": [
                "Nir Goren",
                "Shai Yehezkel",
                "Omer Dahary",
                "Andrey Voynov",
                "Or Patashnik",
                "Daniel Cohen-Or"
            ],
            "affiliations": [
                "Google DeepMind",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21697.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#reasoning",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ”·",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸ Ğ² Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ visual diffusion Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ñ‘Ğ½ Ğº Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ²Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ°, Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¨Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ°."
                },
                "en": {
                    "title": "Transforming Noise into Geometry: A New Approach with Visual Diffusion Models",
                    "desc": "This paper explores how visual diffusion models can effectively solve geometric problems by transforming noisy images into valid solutions. It demonstrates this capability through the Inscribed Square Problem, the Steiner Tree Problem, and the Simple Polygon Problem, treating each problem as an image. The model learns to convert Gaussian noise into images that represent approximate solutions, thereby framing geometric reasoning as a process of image generation. This approach simplifies the application of diffusion models to geometric tasks, suggesting a new paradigm for addressing complex geometric challenges using standard visual models."
                },
                "zh": {
                    "title": "è§†è§‰æ‰©æ•£æ¨¡å‹ï¼šå‡ ä½•é—®é¢˜çš„æ–°è§£æ³•",
                    "desc": "æœ¬æ–‡å±•ç¤ºäº†è§†è§‰æ‰©æ•£æ¨¡å‹å¦‚ä½•é€šè¿‡å°†å™ªå£°å›¾åƒè½¬åŒ–ä¸ºæœ‰æ•ˆè§£æ¥è§£å†³å‡ ä½•é—®é¢˜ã€‚è¿™ç§æ–¹æ³•åœ¨åƒç´ ç©ºé—´ä¸­ç›´æ¥æ¨ç†å‡ ä½•é—®é¢˜ï¼Œé¦–æ¬¡åº”ç”¨äºè‘—åçš„å†…åˆ‡æ­£æ–¹å½¢é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ¯ä¸ªé—®é¢˜å®ä¾‹è§†ä¸ºå›¾åƒï¼Œå¹¶è®­ç»ƒæ ‡å‡†çš„è§†è§‰æ‰©æ•£æ¨¡å‹ï¼Œå°†é«˜æ–¯å™ªå£°è½¬åŒ–ä¸ºæ¥è¿‘ç²¾ç¡®è§£çš„æœ‰æ•ˆè¿‘ä¼¼è§£ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå›¾åƒç©ºé—´çš„æ“ä½œä¸ºè¿‘ä¼¼è§£å†³å¤æ‚å‡ ä½•é—®é¢˜æä¾›äº†ä¸€ä¸ªé€šç”¨ä¸”å®ç”¨çš„æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21682",
            "title": "WorldGrow: Generating Infinite 3D World",
            "url": "https://huggingface.co/papers/2510.21682",
            "abstract": "WorldGrow, a hierarchical framework, generates large, continuous 3D environments with coherent geometry and realistic appearance using pre-trained 3D models and a coarse-to-fine generation strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We tackle the challenge of generating the infinitely extendable 3D world -- large, continuous environments with coherent geometry and realistic appearance. Existing methods face key challenges: 2D-lifting approaches suffer from geometric and appearance inconsistencies across views, 3D implicit representations are hard to scale up, and current 3D foundation models are mostly object-centric, limiting their applicability to scene-level generation. Our key insight is leveraging strong generation priors from pre-trained 3D models for structured scene block generation. To this end, we propose WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our method features three core components: (1) a data curation pipeline that extracts high-quality scene blocks for training, making the 3D structured latent representations suitable for scene generation; (2) a 3D block inpainting mechanism that enables context-aware scene extension; and (3) a coarse-to-fine generation strategy that ensures both global layout plausibility and local geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs. These results highlight its capability for constructing large-scale virtual environments and potential for building future world models.",
            "score": 11,
            "issue_id": 6621,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "c8660a9a47c4dbe3",
            "authors": [
                "Sikuang Li",
                "Chen Yang",
                "Jiemin Fang",
                "Taoran Yi",
                "Jia Lu",
                "Jiazhong Cen",
                "Lingxi Xie",
                "Wei Shen",
                "Qi Tian"
            ],
            "affiliations": [
                "Huawei Inc.",
                "Huazhong University of Science and Technology",
                "MoE Key Lab of Artificial Intelligence, School of Computer Science, SJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21682.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#games"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ±Ğ»Ğ¾Ğº Ğ·Ğ° Ğ±Ğ»Ğ¾ĞºĞ¾Ğ¼",
                    "desc": "WorldGrow - ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ñ… 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ†ĞµĞ½Ñ‹ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ inpainting Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ - ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ 3D-FRONT Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°."
                },
                "en": {
                    "title": "Infinite 3D Worlds Made Real with WorldGrow",
                    "desc": "WorldGrow is a hierarchical framework designed to create large, continuous 3D environments that maintain coherent geometry and realistic appearances. It addresses the limitations of existing methods, such as inconsistencies in 2D-lifting approaches and the challenges of scaling 3D implicit representations. By utilizing pre-trained 3D models, WorldGrow employs a coarse-to-fine generation strategy that enhances both the global layout and local details of the generated scenes. The framework demonstrates state-of-the-art performance in geometry reconstruction and supports the generation of infinite, photorealistic environments, making it a significant advancement in 3D scene synthesis."
                },
                "zh": {
                    "title": "æ— é™æ‰©å±•çš„3Dä¸–ç•Œç”Ÿæˆæ¡†æ¶",
                    "desc": "WorldGrowæ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆå¤§å‹ã€è¿ç»­çš„3Dç¯å¢ƒï¼Œå…·æœ‰ä¸€è‡´çš„å‡ ä½•å½¢çŠ¶å’ŒçœŸå®çš„å¤–è§‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„3Dæ¨¡å‹å’Œç²—åˆ°ç»†çš„ç”Ÿæˆç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å‡ ä½•å’Œå¤–è§‚ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚WorldGrowçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬é«˜è´¨é‡åœºæ™¯å—çš„æ•°æ®æ•´ç†ç®¡é“ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„3Då—ä¿®å¤æœºåˆ¶ï¼Œä»¥åŠç¡®ä¿å…¨å±€å¸ƒå±€åˆç†æ€§å’Œå±€éƒ¨å‡ ä½•/çº¹ç†ä¿çœŸåº¦çš„ç”Ÿæˆç­–ç•¥ã€‚ç»è¿‡åœ¨å¤§è§„æ¨¡3D-FRONTæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼ŒWorldGrowåœ¨å‡ ä½•é‡å»ºæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ”¯æŒæ— é™åœºæ™¯ç”Ÿæˆï¼Œå±•ç¤ºäº†å…¶æ„å»ºå¤§è§„æ¨¡è™šæ‹Ÿç¯å¢ƒçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20479",
            "title": "RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via\n  Hierarchical Model Merging",
            "url": "https://huggingface.co/papers/2510.20479",
            "abstract": "RECALL is a representation-aware framework for continual learning in large language models that merges models without historical data, preserving domain-general features and adapting to task-specific knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs.",
            "score": 10,
            "issue_id": 6622,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "0c4b0e43a28cdf2a",
            "authors": [
                "Bowen Wang",
                "Haiyuan Wan",
                "Liwen Shi",
                "Chen Yang",
                "Peng He",
                "Yue Ma",
                "Haochen Han",
                "Wenhao Li",
                "Tiao Tan",
                "Yongjian Li",
                "Fangming Liu",
                "Yifan Gong",
                "Sheng Zhang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Peng Cheng Laboratory",
                "School of Biomedical Engineering, Tsinghua University",
                "Shenzhen International Graduate School, Tsinghua University",
                "The Hong Kong University of Science and Technology, Guangzhou",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20479.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RECALL â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ continual learning Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ LLM ĞºĞ°Ğº Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… ÑĞµÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ¾Ğ´ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºÑƒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ RECALL ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ catastrophic forgetting Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑƒĞ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Seamless Learning with RECALL: Merging Knowledge Without History",
                    "desc": "RECALL is a framework designed for continual learning in large language models (LLMs) that allows merging of models without needing past data. It leverages internal representations as indicators of learned knowledge and computes similarities between models based on their hidden layers. The framework uses adaptive parameter fusion to maintain general features while adapting to specific tasks. This approach not only prevents performance loss but also enhances knowledge retention and generalization across various natural language processing tasks."
                },
                "zh": {
                    "title": "RECALLï¼šæ— å†å²æ•°æ®çš„æŒç»­å­¦ä¹ æ–°æ¡†æ¶",
                    "desc": "RECALLæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å†å²æ•°æ®çš„æƒ…å†µä¸‹åˆå¹¶æ¨¡å‹ï¼Œä¿ç•™é¢†åŸŸé€šç”¨ç‰¹å¾å¹¶é€‚åº”ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å†…éƒ¨è¡¨ç¤ºä½œä¸ºå­¦ä¹ çŸ¥è¯†çš„å¯é ä»£ç†ï¼Œé€šè¿‡è®¡ç®—å±‚çº§éšè—è¡¨ç¤ºçš„æ¨¡å‹é—´ç›¸ä¼¼æ€§ï¼Œè¿›è¡Œè‡ªé€‚åº”çš„å±‚æ¬¡å‚æ•°èåˆã€‚RECALLçš„è®¾è®¡ä½¿å¾—åœ¨æµ…å±‚ä¿ç•™é¢†åŸŸé€šç”¨ç‰¹å¾çš„åŒæ—¶ï¼Œåœ¨æ·±å±‚è¿›è¡Œç‰¹å®šä»»åŠ¡çš„é€‚åº”ã€‚ä¸ä»¥å¾€éœ€è¦ä»»åŠ¡æ ‡ç­¾æˆ–å­˜åœ¨æ€§èƒ½æŠ˜è¡·çš„æ–¹æ³•ä¸åŒï¼ŒRECALLå®ç°äº†æ— ç¼çš„å¤šé¢†åŸŸé›†æˆï¼Œå¹¶å¯¹ç¾éš¾æ€§é—å¿˜å…·æœ‰å¼ºå¤§çš„æŠµæŠ—åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20206",
            "title": "RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via\n  Data Alignment and Test-Time Scaling",
            "url": "https://huggingface.co/papers/2510.20206",
            "abstract": "RAPO++ enhances text-to-video generation by optimizing user prompts through retrieval, iterative refinement, and LLM fine-tuning, improving semantic alignment, compositionality, and temporal coherence.  \t\t\t\t\tAI-generated summary \t\t\t\t Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present RAPO++, a cross-stage prompt optimization framework that unifies training-data--aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In Stage 1, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. Stage 2 introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback -- including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow -- yielding progressively improved video generation quality. Stage 3 leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at https://github.com/Vchitect/RAPO.",
            "score": 10,
            "issue_id": 6622,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "0715013010b8fa21",
            "authors": [
                "Bingjie Gao",
                "Qianli Ma",
                "Xiaoxue Wu",
                "Shuai Yang",
                "Guanzhou Lan",
                "Haonan Zhao",
                "Jiaxuan Chen",
                "Qingyang Liu",
                "Yu Qiao",
                "Xinyuan Chen",
                "Yaohui Wang",
                "Li Niu"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory, Shanghai 201112, China",
                "Shanghai Jiao Tong University, Shanghai 200240, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20206.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#optimization",
                    "#rag",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¢Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ text-to-video Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "RAPO++ â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· retrieval Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ñ… Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñƒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ñ€ĞµÑ‚Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ LLM Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ ĞµÑ‰Ñ‘ Ğ´Ğ¾ inference Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Text-to-Video Generation with RAPO++",
                    "desc": "RAPO++ is a framework designed to improve text-to-video (T2V) generation by optimizing user prompts. It uses a three-stage process that includes retrieving relevant modifiers, refining prompts iteratively, and fine-tuning a large language model (LLM) to enhance the quality of generated videos. The framework focuses on improving semantic alignment, compositionality, and temporal coherence, which are crucial for creating realistic and coherent videos. Extensive testing shows that RAPO++ significantly outperforms existing methods, making it a versatile and efficient solution for T2V generation."
                },
                "zh": {
                    "title": "RAPO++ï¼šæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ–°æ ‡å‡†",
                    "desc": "RAPO++ æ˜¯ä¸€ç§å¢å¼ºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–ç”¨æˆ·æç¤ºæ¥æé«˜ç”Ÿæˆæ•ˆæœã€‚å®ƒç»“åˆäº†æ£€ç´¢ã€è¿­ä»£ç²¾ç‚¼å’Œå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œæ”¹å–„äº†è¯­ä¹‰å¯¹é½ã€ç»„åˆæ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œé¦–å…ˆé€šè¿‡æ£€ç´¢ç›¸å…³ä¿®é¥°ç¬¦æ¥ä¸°å¯Œç”¨æˆ·æç¤ºï¼Œç„¶ååˆ©ç”¨å¤šæºåé¦ˆè¿­ä»£ä¼˜åŒ–æç¤ºï¼Œæœ€åå¾®è°ƒé‡å†™çš„ LLM ä»¥å®ç°é«˜æ•ˆçš„æç¤ºç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAPO++ åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ï¼Œæˆä¸ºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13251",
            "title": "Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs",
            "url": "https://huggingface.co/papers/2510.13251",
            "abstract": "Video Large Language Models (VideoLLMs) perform video question answering by initiating temporal reasoning through cross-frame interactions, followed by video-language integration, and generate answers using effective information pathways while suppressing unnecessary attention edges.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Large Language Models (VideoLLMs) extend the capabilities of vision-language models to spatiotemporal inputs, enabling tasks such as video question answering (VideoQA). Despite recent advances in VideoLLMs, their internal mechanisms on where and how they extract and propagate video and textual information remain less explored. In this study, we investigate the internal information flow of VideoLLMs using mechanistic interpretability techniques. Our analysis reveals consistent patterns across diverse VideoQA tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame interactions in early-to-middle layers, (2) followed by progressive video-language integration in middle layers. This is facilitated by alignment between video representations and linguistic embeddings containing temporal concepts. (3) Upon completion of this integration, the model is ready to generate correct answers in middle-to-late layers. (4) Based on our analysis, we show that VideoLLMs can retain their VideoQA performance by selecting these effective information pathways while suppressing a substantial amount of attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a blueprint on how VideoLLMs perform temporal reasoning and offer practical insights for improving model interpretability and downstream generalization. Our project page with the source code is available at https://map-the-flow.github.io",
            "score": 9,
            "issue_id": 6623,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "acfa1fde05b861ca",
            "authors": [
                "Minji Kim",
                "Taekyung Kim",
                "Bohyung Han"
            ],
            "affiliations": [
                "ECE & IPAI, Seoul National University",
                "NAVER AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13251.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#video",
                    "#reasoning",
                    "#interpretability",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšĞ°Ğº Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ²Ñ€ĞµĞ¼Ñ: ĞºĞ°Ñ€Ñ‚Ğ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM (Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾) Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ°Ñ…. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ…, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ…, Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ½ĞµÑ† Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ…. Ğ’Ğ°Ğ¶Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 42% ÑĞ²ÑĞ·ĞµĞ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ°Ğº Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ĞºĞ°Ğº AI Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑÑ… Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "Unlocking the Secrets of Video Question Answering with VideoLLMs",
                    "desc": "This paper explores how Video Large Language Models (VideoLLMs) handle video question answering by analyzing their internal information flow. It reveals that these models use cross-frame interactions for temporal reasoning, integrating video and language representations effectively. The study shows that VideoLLMs can maintain performance by focusing on key information pathways while ignoring unnecessary attention connections. These insights enhance our understanding of VideoLLMs and suggest ways to improve their interpretability and generalization in various tasks."
                },
                "zh": {
                    "title": "è§†é¢‘é—®ç­”çš„æ–°è§†è§’ï¼šæ—¶é—´æ¨ç†ä¸ä¿¡æ¯æµåŠ¨",
                    "desc": "è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰é€šè¿‡è·¨å¸§äº¤äº’å¯åŠ¨æ—¶é—´æ¨ç†ï¼Œè¿›è€Œè¿›è¡Œè§†é¢‘é—®ç­”ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒVideoLLMsåœ¨å¤„ç†è§†é¢‘å’Œæ–‡æœ¬ä¿¡æ¯æ—¶ï¼Œé‡‡ç”¨äº†æœ‰æ•ˆçš„ä¿¡æ¯æµåŠ¨è·¯å¾„ï¼Œå¹¶æŠ‘åˆ¶äº†ä¸å¿…è¦çš„æ³¨æ„åŠ›è¾¹ç¼˜ã€‚åˆ†ææ˜¾ç¤ºï¼Œæ—¶é—´æ¨ç†åœ¨æ¨¡å‹çš„æ—©ä¸­å±‚é€šè¿‡æ´»è·ƒçš„è·¨å¸§äº¤äº’å¼€å§‹ï¼Œéšååœ¨ä¸­å±‚è¿›è¡Œè§†é¢‘ä¸è¯­è¨€çš„é€æ­¥æ•´åˆã€‚æœ€ç»ˆï¼Œæ¨¡å‹åœ¨ä¸­åå±‚ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆï¼Œä¿æŒäº†è‰¯å¥½çš„è§†é¢‘é—®ç­”æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21223",
            "title": "Model Merging with Functional Dual Anchors",
            "url": "https://huggingface.co/papers/2510.21223",
            "abstract": "Functional Dual Anchors (FDAs) enhance model merging by aligning gradients with task vectors in the input-representation space, offering robustness and flexibility compared to parameter-space methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Model merging is an efficient post-training strategy for integrating knowledge from multiple finetuned checkpoints of a shared foundation model. Existing methods operate in the parameter space, combining task vectors to mitigate conflicts, but remain constrained by parameter inconsistencies. We propose Functional Dual Anchors (FDAs), a framework that instead models the input-representation space. FDAs are synthetic inputs whose induced gradients align with task vectors, capturing task-specific functional shifts relative to the pretrained model. This perspective bridges joint multi-task training and post-hoc merging, offering both robustness and flexibility. We further introduce a principled initialization scheme and show that FDAs are complementary to parameter-space model merging. Comprehensive experiments demonstrate the effectiveness of FDAs in model merging.",
            "score": 8,
            "issue_id": 6621,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "029e5f4701cf63dd",
            "authors": [
                "Kexuan Shi",
                "Yandong Wen",
                "Weiyang Liu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21223.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "âš“",
                "ru": {
                    "title": "Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºĞ¾Ñ€Ñ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ fine-tuned Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Functional Dual Anchors (FDA). Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğº Â«ÑĞºĞ¾Ñ€ÑÂ». Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ ÑÑ‚Ğ¸Ñ… ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ task vectors, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Model Merging with Functional Dual Anchors",
                    "desc": "Functional Dual Anchors (FDAs) improve the process of model merging by focusing on the input-representation space rather than the parameter space. This approach aligns gradients with task vectors, which helps to manage conflicts that arise when integrating knowledge from different finetuned models. FDAs act as synthetic inputs that reflect task-specific changes, enhancing the robustness and flexibility of the merging process. Our experiments show that FDAs work well alongside traditional parameter-space methods, making them a valuable addition to model merging techniques."
                },
                "zh": {
                    "title": "åŠŸèƒ½åŒé”šï¼šæå‡æ¨¡å‹åˆå¹¶çš„é²æ£’æ€§ä¸çµæ´»æ€§",
                    "desc": "åŠŸèƒ½åŒé”šï¼ˆFDAsï¼‰é€šè¿‡åœ¨è¾“å…¥è¡¨ç¤ºç©ºé—´ä¸­å¯¹é½æ¢¯åº¦ä¸ä»»åŠ¡å‘é‡ï¼Œå¢å¼ºäº†æ¨¡å‹åˆå¹¶çš„èƒ½åŠ›ï¼Œç›¸æ¯”äºå‚æ•°ç©ºé—´çš„æ–¹æ³•ï¼Œæä¾›äº†æ›´å¼ºçš„é²æ£’æ€§å’Œçµæ´»æ€§ã€‚ç°æœ‰çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ä¸»è¦åœ¨å‚æ•°ç©ºé—´ä¸­æ“ä½œï¼Œè¯•å›¾é€šè¿‡ç»„åˆä»»åŠ¡å‘é‡æ¥å‡å°‘å†²çªï¼Œä½†å—åˆ°å‚æ•°ä¸ä¸€è‡´æ€§çš„é™åˆ¶ã€‚FDAsä½œä¸ºåˆæˆè¾“å…¥ï¼Œå…¶è¯±å¯¼çš„æ¢¯åº¦ä¸ä»»åŠ¡å‘é‡å¯¹é½ï¼Œèƒ½å¤Ÿæ•æ‰ç›¸å¯¹äºé¢„è®­ç»ƒæ¨¡å‹çš„ä»»åŠ¡ç‰¹å®šåŠŸèƒ½å˜åŒ–ã€‚è¿™ç§æ–¹æ³•å°†è”åˆå¤šä»»åŠ¡è®­ç»ƒä¸åæœŸåˆå¹¶ç›¸ç»“åˆï¼Œå±•ç¤ºäº†FDAsåœ¨æ¨¡å‹åˆå¹¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21652",
            "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research\n  Suite",
            "url": "https://huggingface.co/papers/2510.21652",
            "abstract": "AstaBench provides a comprehensive benchmark suite for evaluating AI agents in scientific research, revealing that while progress has been made, AI still falls short in fully assisting scientific research.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose \"deep research\" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance.",
            "score": 2,
            "issue_id": 6621,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "e8b1cd801c13a100",
            "authors": [
                "Jonathan Bragg",
                "Mike D'Arcy",
                "Nishant Balepur",
                "Dan Bareket",
                "Bhavana Dalvi",
                "Sergey Feldman",
                "Dany Haddad",
                "Jena D. Hwang",
                "Peter Jansen",
                "Varsha Kishore",
                "Bodhisattwa Prasad Majumder",
                "Aakanksha Naik",
                "Sigal Rahamimov",
                "Kyle Richardson",
                "Amanpreet Singh",
                "Harshit Surana",
                "Aryeh Tiktinsky",
                "Rosni Vasu",
                "Guy Wiener",
                "Chloe Anastasiades",
                "Stefan Candra",
                "Jason Dunkelberger",
                "Dan Emery",
                "Rob Evans",
                "Malachi Hamada",
                "Regan Huff",
                "Rodney Kinney",
                "Matt Latzke",
                "Jaron Lochner",
                "Ruben Lozano-Aguilera",
                "Cecile Nguyen",
                "Smita Rao",
                "Amber Tanaka",
                "Brooke Vlahos",
                "Peter Clark",
                "Doug Downey",
                "Yoav Goldberg",
                "Ashish Sabharwal",
                "Daniel S. Weld"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Bar-Ilan University",
                "University of Arizona",
                "University of Maryland",
                "University of Washington",
                "University of Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21652.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#science"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "AI Ğ² Ğ½Ğ°ÑƒĞºĞµ: ĞµÑ‰Ñ‘ Ğ½Ğµ Ğ·Ğ°Ğ¼ĞµĞ½Ğ° ÑƒÑ‡Ñ‘Ğ½Ñ‹Ğ¼",
                    "desc": "AstaBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…. ĞĞ½ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ, AI Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ ÑƒÑ‡Ñ‘Ğ½Ñ‹Ñ…. AstaBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 2400 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²ĞµÑÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ."
                },
                "en": {
                    "title": "AstaBench: Elevating AI's Role in Scientific Research Evaluation",
                    "desc": "AstaBench is a new benchmark suite designed to evaluate AI agents specifically in the context of scientific research. It addresses the shortcomings of existing benchmarks by providing a comprehensive set of over 2400 problems that reflect real-world scientific tasks. The suite includes tools for reproducible evaluations and accounts for various confounding factors, ensuring a more accurate assessment of AI capabilities. Despite advancements in AI, the evaluation shows that these agents still struggle to fully assist in scientific research, highlighting the need for further development."
                },
                "zh": {
                    "title": "AstaBenchï¼šç§‘å­¦ç ”ç©¶ä¸­çš„ AI ä»£ç†è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "AstaBench æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œç”¨äºè¯„ä¼° AI ä»£ç†åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„è¡¨ç°ã€‚å°½ç®¡ AI åœ¨æŸäº›æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä»æœªèƒ½å®Œå…¨æ”¯æŒç§‘å­¦ç ”ç©¶ã€‚è¯¥å¥—ä»¶æä¾›äº†2400å¤šä¸ªé—®é¢˜ï¼Œæ¶µç›–æ•´ä¸ªç§‘å­¦å‘ç°è¿‡ç¨‹ï¼Œå¹¶åŒ…æ‹¬å¤šä¸ªç§‘å­¦é¢†åŸŸçš„é—®é¢˜ã€‚é€šè¿‡ä¸¥æ ¼çš„è¯„ä¼°ï¼ŒAstaBench æ—¨åœ¨ä¸ºç§‘å­¦ç ”ç©¶æä¾›æ›´æœ‰æ•ˆçš„ AI ä»£ç†è¯„ä¼°å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21553",
            "title": "Document Understanding, Measurement, and Manipulation Using Category\n  Theory",
            "url": "https://huggingface.co/papers/2510.21553",
            "abstract": "Category theory is used to develop information-theoretic measures, summarization, and self-supervised improvement of large pretrained models through a mathematical framework of question-answer pairs and orthogonalization.  \t\t\t\t\tAI-generated summary \t\t\t\t We apply category theory to extract multimodal document structure which leads us to develop information theoretic measures, content summarization and extension, and self-supervised improvement of large pretrained models. We first develop a mathematical representation of a document as a category of question-answer pairs. Second, we develop an orthogonalization procedure to divide the information contained in one or more documents into non-overlapping pieces. The structures extracted in the first and second steps lead us to develop methods to measure and enumerate the information contained in a document. We also build on those steps to develop new summarization techniques, as well as to develop a solution to a new problem viz. exegesis resulting in an extension of the original document. Our question-answer pair methodology enables a novel rate distortion analysis of summarization techniques. We implement our techniques using large pretrained models, and we propose a multimodal extension of our overall mathematical framework. Finally, we develop a novel self-supervised method using RLVR to improve large pretrained models using consistency constraints such as composability and closure under certain operations that stem naturally from our category theoretic framework.",
            "score": 2,
            "issue_id": 6621,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "aa27a9cf9e256732",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#rl",
                    "#multimodal",
                    "#survey",
                    "#data",
                    "#optimization",
                    "#math"
                ],
                "emoji": "ğŸ”·",
                "ru": {
                    "title": "Ğ¢ĞµĞ¾Ñ€Ğ¸Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ¿ĞµÑ€ĞµÑĞµĞºĞ°ÑÑ‰Ğ¸ĞµÑÑ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½ rate-distortion Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ self-supervised Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RLVR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹Ñ‚ĞµĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¸Ğ· ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°."
                },
                "en": {
                    "title": "Harnessing Category Theory for Enhanced Document Understanding and Model Improvement",
                    "desc": "This paper uses category theory to create new ways to measure and summarize information in documents. It represents documents as categories of question-answer pairs, allowing for a structured analysis of their content. The authors introduce an orthogonalization method to separate information into distinct parts, which aids in developing effective summarization techniques. Additionally, they propose a self-supervised learning approach to enhance large pretrained models by applying consistency constraints derived from their mathematical framework."
                },
                "zh": {
                    "title": "åˆ©ç”¨èŒƒç•´ç†è®ºæå‡é¢„è®­ç»ƒæ¨¡å‹çš„æ‘˜è¦ä¸æ‰©å±•èƒ½åŠ›",
                    "desc": "æœ¬æ–‡åˆ©ç”¨èŒƒç•´ç†è®ºæ¥æå–å¤šæ¨¡æ€æ–‡æ¡£ç»“æ„ï¼Œä»è€Œå¼€å‘ä¿¡æ¯ç†è®ºåº¦é‡ã€å†…å®¹æ‘˜è¦å’Œæ‰©å±•ï¼Œä»¥åŠå¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„è‡ªç›‘ç£æ”¹è¿›ã€‚æˆ‘ä»¬é¦–å…ˆå°†æ–‡æ¡£æ•°å­¦è¡¨ç¤ºä¸ºé—®é¢˜-ç­”æ¡ˆå¯¹çš„èŒƒç•´ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼€å‘äº†æ­£äº¤åŒ–ç¨‹åºï¼Œå°†ä¸€ä¸ªæˆ–å¤šä¸ªæ–‡æ¡£ä¸­çš„ä¿¡æ¯åˆ’åˆ†ä¸ºä¸é‡å çš„éƒ¨åˆ†ã€‚è¿™äº›ç»“æ„çš„æå–ä½¿æˆ‘ä»¬èƒ½å¤Ÿæµ‹é‡å’Œæšä¸¾æ–‡æ¡£ä¸­åŒ…å«çš„ä¿¡æ¯ï¼Œå¹¶æå‡ºæ–°çš„æ‘˜è¦æŠ€æœ¯å’Œæ–‡æ¡£æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21447",
            "title": "PhysWorld: From Real Videos to World Models of Deformable Objects via\n  Physics-Aware Demonstration Synthesis",
            "url": "https://huggingface.co/papers/2510.21447",
            "abstract": "PhysWorld uses a simulator to generate diverse demonstrations for training a GNN-based world model, enabling accurate and fast predictions for deformable objects with competitive performance and faster inference speeds.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive world models that simulate object dynamics are crucial for robotics, VR, and AR. However, it remains a significant challenge to learn physics-consistent dynamics models from limited real-world video data, especially for deformable objects with spatially-varying physical properties. To overcome the challenge of data scarcity, we propose PhysWorld, a novel framework that utilizes a simulator to synthesize physically plausible and diverse demonstrations to learn efficient world models. Specifically, we first construct a physics-consistent digital twin within MPM simulator via constitutive model selection and global-to-local optimization of physical properties. Subsequently, we apply part-aware perturbations to the physical properties and generate various motion patterns for the digital twin, synthesizing extensive and diverse demonstrations. Finally, using these demonstrations, we train a lightweight GNN-based world model that is embedded with physical properties. The real video can be used to further refine the physical properties. PhysWorld achieves accurate and fast future predictions for various deformable objects, and also generalizes well to novel interactions. Experiments show that PhysWorld has competitive performance while enabling inference speeds 47 times faster than the recent state-of-the-art method, i.e., PhysTwin.",
            "score": 2,
            "issue_id": 6621,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "aee1bcbeeb33e55c",
            "authors": [
                "Yu Yang",
                "Zhilu Zhang",
                "Xiang Zhang",
                "Yihan Zeng",
                "Hui Li",
                "Wangmeng Zuo"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21447.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#robotics",
                    "#synthetic",
                    "#graphs",
                    "#inference"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¸Ñ€: ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "PhysWorld â€” ÑÑ‚Ğ¾ framework Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… world models, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ MPM-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ GNN-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ ĞµĞ³Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ inference Ğ² 47 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡ĞµĞ¼ state-of-the-art Ğ¼ĞµÑ‚Ğ¾Ğ´ PhysTwin."
                },
                "en": {
                    "title": "Revolutionizing Predictions for Deformable Objects with PhysWorld",
                    "desc": "PhysWorld is a framework designed to improve the training of Graph Neural Network (GNN)-based world models for predicting the behavior of deformable objects. It addresses the challenge of limited real-world data by using a simulator to create diverse and physically plausible demonstrations. By constructing a digital twin and applying part-aware perturbations, PhysWorld generates a wide range of motion patterns that enhance the learning process. The resulting model not only achieves accurate predictions but also operates significantly faster than previous methods, making it highly effective for applications in robotics and virtual environments."
                },
                "zh": {
                    "title": "PhysWorldï¼šå¿«é€Ÿå‡†ç¡®çš„å¯å˜å½¢ç‰©ä½“é¢„æµ‹",
                    "desc": "PhysWorldæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ¨¡æ‹Ÿå™¨ç”Ÿæˆå¤šæ ·åŒ–çš„æ¼”ç¤ºæ•°æ®ï¼Œä»¥è®­ç»ƒåŸºäºå›¾ç¥ç»ç½‘ç»œçš„ä¸–ç•Œæ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®å¿«é€Ÿåœ°é¢„æµ‹å¯å˜å½¢ç‰©ä½“çš„åŠ¨æ€ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚é€šè¿‡æ„å»ºç‰©ç†ä¸€è‡´çš„æ•°å­—åŒèƒèƒï¼Œå¹¶å¯¹ç‰©ç†å±æ€§è¿›è¡Œå±€éƒ¨ä¼˜åŒ–ï¼ŒPhysWorldèƒ½å¤Ÿåˆæˆä¸°å¯Œçš„è¿åŠ¨æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒPhysWorldåœ¨æ€§èƒ½ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦æ¯”æœ€æ–°çš„PhysTwinæ–¹æ³•å¿«47å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20780",
            "title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and\n  Performance Boost",
            "url": "https://huggingface.co/papers/2510.20780",
            "abstract": "Calibrating large reasoning models with synthetic human-like thinking trajectories improves their efficiency and performance in machine translation evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large reasoning models (LRMs) have introduced an intermediate \"thinking\" process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to \"overthink\" simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation.",
            "score": 2,
            "issue_id": 6623,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "0e1d847c372418a9",
            "authors": [
                "Runzhe Zhan",
                "Zhihong Huang",
                "Xinyi Yang",
                "Lidia S. Chao",
                "Min Yang",
                "Derek F. Wong"
            ],
            "affiliations": [
                "NLP2CT Lab, Department of Computer and Information Science, University of Macau",
                "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20780.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#synthetic",
                    "#training",
                    "#machine_translation",
                    "#multimodal"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞšĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ€ÑĞ´ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Â«Ğ¿ĞµÑ€ĞµÑƒÑĞ»Ğ¾Ğ¶Ğ½ÑÑ‚ÑŒÂ» Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ·Ğ°Ğ²Ñ‹ÑˆĞ°Ñ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ LRM Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WMT24 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² 35 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 7B Ğ´Ğ¾ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ R1-Distill-Qwen-7B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° 8.7 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… LRM Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Calibrating LRMs for Smarter Machine Translation Evaluation",
                    "desc": "This paper explores how calibrating large reasoning models (LRMs) with synthetic human-like thinking processes can enhance their efficiency and performance in evaluating machine translation (MT). The authors identify challenges faced by LRMs, such as the tendency to overthink simpler tasks and issues with scoring accuracy. By training LRMs on tailored evaluation materials that mimic human reasoning, they demonstrate a significant reduction in the computational resources needed for evaluation while improving the correlation of MT quality assessments. The results indicate that well-calibrated LRMs can effectively contribute to more nuanced and accurate automatic MT evaluation."
                },
                "zh": {
                    "title": "æ ¡å‡†æ¨ç†æ¨¡å‹ï¼Œæå‡æœºå™¨ç¿»è¯‘è¯„ä¼°æ•ˆç‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡åˆæˆçš„äººç±»æ€ç»´è½¨è¿¹æ¥æ ¡å‡†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼Œä»¥æé«˜å…¶åœ¨æœºå™¨ç¿»è¯‘è¯„ä¼°ä¸­çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼ŒLRMsåœ¨è¯„ä¼°æœºå™¨ç¿»è¯‘è´¨é‡æ—¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éœ€è¦å®šåˆ¶çš„è¯„ä¼°ææ–™ã€å¯¹ç®€å•å®ä¾‹çš„è¿‡åº¦æ€è€ƒä»¥åŠè¯„åˆ†æœºåˆ¶çš„é—®é¢˜ã€‚é€šè¿‡åœ¨åˆæˆçš„æ€ç»´è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ ¡å‡†LRMæ€ç»´çš„æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜è¿™ç§æ–¹æ³•å¯ä»¥å°†æ€ç»´é¢„ç®—å‡å°‘çº¦35å€ï¼ŒåŒæ—¶æé«˜è¯„ä¼°æ€§èƒ½ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†é«˜æ•ˆæ ¡å‡†çš„LRMsåœ¨ç»†ç²’åº¦è‡ªåŠ¨æœºå™¨ç¿»è¯‘è¯„ä¼°ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20535",
            "title": "ARC-Encoder: learning compressed text representations for large language\n  models",
            "url": "https://huggingface.co/papers/2510.20535",
            "abstract": "An ARC-Encoder compresses context into continuous representations for LLMs, improving inference efficiency and performance across various scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs x-times fewer continuous representations (typically x!in!{4,8}) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .",
            "score": 2,
            "issue_id": 6621,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "38daae856d0502b4",
            "authors": [
                "Hippolyte Pilchen",
                "Edouard Grave",
                "Patrick PÃ©rez"
            ],
            "affiliations": [
                "Kyutai, Paris, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20535.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#long_context",
                    "#architecture",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ inference",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ARC-Encoder â€” ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ decoder LLM, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² 4-8 Ñ€Ğ°Ğ·. Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ inference Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ fine-tuning Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞµÑ‘ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM. ARC-Encoder Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ â€” Ğ¾Ñ‚ in-context learning Ğ´Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ â€” Ğ¾Ğ´Ğ¸Ğ½ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ decoder LLM, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Efficient Context Compression for Enhanced LLM Performance",
                    "desc": "The ARC-Encoder is a novel approach that compresses context into continuous representations for large language models (LLMs), enhancing both inference efficiency and performance. Unlike traditional methods that require fine-tuning or architectural changes, ARC-Encoder replaces token embeddings with fewer continuous representations, typically reducing the number of outputs by a factor of 4 to 8. This method has been systematically studied to optimize training strategies and architecture, resulting in state-of-the-art performance across various benchmarks. Additionally, ARC-Encoder is adaptable, allowing it to work with multiple decoder LLMs, making it a versatile solution for improving computational efficiency in diverse applications."
                },
                "zh": {
                    "title": "ARC-Encoderï¼šé«˜æ•ˆçš„ä¸Šä¸‹æ–‡å‹ç¼©è§£å†³æ–¹æ¡ˆ",
                    "desc": "ARC-Encoderæ˜¯ä¸€ç§å°†ä¸Šä¸‹æ–‡å‹ç¼©ä¸ºè¿ç»­è¡¨ç¤ºçš„ç¼–ç å™¨ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ•ˆç‡å’Œæ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯ä¸åŒï¼ŒARC-Encoderä¸éœ€è¦å¯¹ç›®æ ‡æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–æ¶æ„ä¿®æ”¹ï¼Œä»è€Œé¿å…äº†å¯¹æ¨¡å‹é€šç”¨èƒ½åŠ›çš„å½±å“ã€‚é€šè¿‡ç³»ç»Ÿç ”ç©¶è®­ç»ƒç­–ç•¥å’Œæ¶æ„é€‰æ‹©ï¼ŒARC-Encoderèƒ½å¤Ÿè¾“å‡ºæ¯”æ–‡æœ¬æ ‡è®°å°‘xå€çš„è¿ç»­è¡¨ç¤ºï¼Œæ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒARC-Encoderåœ¨å¤šç§ä½¿ç”¨åœºæ™¯ä¸‹è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„è§£ç å™¨ï¼Œæˆä¸ºä¸€ç§é«˜æ•ˆçš„å¯ç§»æ¤ç¼–ç å™¨è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17234",
            "title": "Taming Modality Entanglement in Continual Audio-Visual Segmentation",
            "url": "https://huggingface.co/papers/2510.17234",
            "abstract": "A novel Continual Audio-Visual Segmentation (CAVS) task addresses challenges in multi-modal continual learning through a Collision-based Multi-modal Rehearsal (CMR) framework, improving performance over single-modal methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, significant progress has been made in multi-modal continual learning, aiming to learn new tasks sequentially in multi-modal settings while preserving performance on previously learned ones. However, existing methods mainly focus on coarse-grained tasks, with limitations in addressing modality entanglement in fine-grained continual learning settings. To bridge this gap, we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to continuously segment new classes guided by audio. Through comprehensive analysis, two critical challenges are identified: 1) multi-modal semantic drift, where a sounding objects is labeled as background in sequential tasks; 2) co-occurrence confusion, where frequent co-occurring classes tend to be confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework is designed to address these challenges. Specifically, for multi-modal semantic drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select samples with high modal consistency for rehearsal. Meanwhile, for co-occurence confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed, allowing for the increase of rehearsal sample frequency of those confusable classes during training process. Moreover, we construct three audio-visual incremental scenarios to verify effectiveness of our method. Comprehensive experiments demonstrate that our method significantly outperforms single-modal continual learning methods.",
            "score": 2,
            "issue_id": 6622,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 20",
                "zh": "10æœˆ20æ—¥"
            },
            "hash": "7a6c5466d3f58bd9",
            "authors": [
                "Yuyang Hong",
                "Qi Yang",
                "Tao Zhang",
                "Zili Wang",
                "Zhaojin Fu",
                "Kun Ding",
                "Bin Fan",
                "Shiming Xiang"
            ],
            "affiliations": [
                "School of Intelligent Science and Technology, University of Science and Technology Beijing, Beijing 100083, China",
                "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China",
                "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17234.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ”Š",
                "ru": {
                    "title": "ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (CAVS), Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ²ÑƒĞºĞ°, Ğ½Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ñ€ĞµĞ¹Ñ„, ĞºĞ¾Ğ³Ğ´Ğ° Ğ·Ğ²ÑƒÑ‡Ğ°Ñ‰Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ñ„Ğ¾Ğ½Ğ¾Ğ¼, Ğ¸ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñƒ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ ĞºĞ»Ğ°ÑÑĞ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº CMR Ñ ÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¹ ĞºĞ»Ğ°ÑÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ continual learning."
                },
                "en": {
                    "title": "Enhancing Multi-Modal Learning with Collision-based Rehearsal",
                    "desc": "This paper introduces a new task called Continual Audio-Visual Segmentation (CAVS) that focuses on learning from both audio and visual data over time. It addresses the challenges of multi-modal continual learning, particularly the issues of multi-modal semantic drift and co-occurrence confusion. To tackle these challenges, the authors propose a Collision-based Multi-modal Rehearsal (CMR) framework, which includes strategies for selecting samples that maintain modal consistency and increasing the frequency of rehearsal for confusing classes. Experimental results show that this approach significantly improves performance compared to traditional single-modal methods."
                },
                "zh": {
                    "title": "æŒç»­éŸ³è§†é¢‘åˆ†å‰²ï¼šå¤šæ¨¡æ€å­¦ä¹ çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŒç»­éŸ³è§†é¢‘åˆ†å‰²ï¼ˆCAVSï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æŒç»­å­¦ä¹ ä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥åŸºäºç¢°æ’çš„å¤šæ¨¡æ€é‡æ¼”ï¼ˆCMRï¼‰æ¡†æ¶ï¼Œæœ¬æ–‡æé«˜äº†åœ¨æ–°ä»»åŠ¡å­¦ä¹ æ—¶å¯¹å·²å­¦ä»»åŠ¡æ€§èƒ½çš„ä¿æŒã€‚ç ”ç©¶ä¸­è¯†åˆ«äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå¤šæ¨¡æ€è¯­ä¹‰æ¼‚ç§»å’Œå…±ç°æ··æ·†ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå•æ¨¡æ€æŒç»­å­¦ä¹ æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21581",
            "title": "Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video",
            "url": "https://huggingface.co/papers/2510.21581",
            "abstract": "Foley Control uses a small cross-attention bridge to synchronize video and audio without retraining, achieving competitive alignment with fewer parameters and maintaining modularity.  \t\t\t\t\tAI-generated summary \t\t\t\t Foley Control is a lightweight approach to video-guided Foley that keeps pretrained single-modality models frozen and learns only a small cross-attention bridge between them. We connect V-JEPA2 video embeddings to a frozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact video cross-attention after the model's existing text cross-attention, so prompts set global semantics while video refines timing and local dynamics. The frozen backbones retain strong marginals (video; audio given text) and the bridge learns the audio-video dependency needed for synchronization -- without retraining the audio prior. To cut memory and stabilize training, we pool video tokens before conditioning. On curated video-audio benchmarks, Foley Control delivers competitive temporal and semantic alignment with far fewer trainable parameters than recent multi-modal systems, while preserving prompt-driven controllability and production-friendly modularity (swap/upgrade encoders or the T2A backbone without end-to-end retraining). Although we focus on Video-to-Foley, the same bridge design can potentially extend to other audio modalities (e.g., speech).",
            "score": 1,
            "issue_id": 6630,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "5e57c85ed10db64d",
            "authors": [
                "Ciara Rowles",
                "Varun Jampani",
                "Simon DonnÃ©",
                "Shimon Vainer",
                "Julian Parker",
                "Zach Evans"
            ],
            "affiliations": [
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21581.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#small_models",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Foley Control â€” ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾ÑÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ cross-attention Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ video embeddings Ğ¸Ğ· V-JEPA2 Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ audio diffusion Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Stable Audio Open Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ cross-attention, Ğ³Ğ´Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ·Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ, Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ñ‚Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾ÑÑ‚Ğ°, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ÑĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Efficient Video-Audio Synchronization with Minimal Parameters",
                    "desc": "Foley Control is a novel method that synchronizes video and audio using a small cross-attention bridge, allowing for effective alignment without the need for retraining existing models. It leverages pretrained single-modality models, keeping them frozen while learning only the necessary connections for audio-video synchronization. By integrating video embeddings with a text-to-audio model, it ensures that video enhances the timing and dynamics of audio based on global semantic prompts. This approach not only reduces the number of trainable parameters but also maintains modularity, enabling easy upgrades and swaps of components without full retraining."
                },
                "zh": {
                    "title": "è½»é‡çº§è§†é¢‘éŸ³é¢‘åŒæ­¥çš„æ–°æ–¹æ³•",
                    "desc": "Foley Controlæ˜¯ä¸€ç§è½»é‡çº§çš„è§†é¢‘å¼•å¯¼Foleyæ–¹æ³•ï¼Œå®ƒé€šè¿‡å°å‹äº¤å‰æ³¨æ„åŠ›æ¡¥æ¥æ¥åŒæ­¥è§†é¢‘å’ŒéŸ³é¢‘ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•ä¿æŒäº†é¢„è®­ç»ƒçš„å•æ¨¡æ€æ¨¡å‹ä¸å˜ï¼Œä»…å­¦ä¹ è§†é¢‘å’ŒéŸ³é¢‘ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚é€šè¿‡åœ¨ç°æœ‰æ–‡æœ¬äº¤å‰æ³¨æ„åŠ›åæ’å…¥ç´§å‡‘çš„è§†é¢‘äº¤å‰æ³¨æ„åŠ›ï¼ŒFoley Controlèƒ½å¤Ÿåœ¨å…¨å±€è¯­ä¹‰çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨è§†é¢‘æ¥ç»†åŒ–æ—¶åºå’Œå±€éƒ¨åŠ¨æ€ã€‚è¯¥æ–¹æ³•åœ¨è§†é¢‘-éŸ³é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›çš„æ—¶é—´å’Œè¯­ä¹‰å¯¹é½ï¼ŒåŒæ—¶å‚æ•°é‡è¿œä½äºæœ€è¿‘çš„å¤šæ¨¡æ€ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21057",
            "title": "Soft Instruction De-escalation Defense",
            "url": "https://huggingface.co/papers/2510.21057",
            "abstract": "SIC, an iterative prompt sanitization method, enhances the security of tool-augmented LLM agents by repeatedly inspecting and cleaning incoming data to prevent prompt injection attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment; this makes them susceptible to prompt injections when dealing with untrusted data. To overcome this limitation, we propose SIC (Soft Instruction Control)-a simple yet effective iterative prompt sanitization loop designed for tool-augmented LLM agents. Our method repeatedly inspects incoming data for instructions that could compromise agent behavior. If such content is found, the malicious content is rewritten, masked, or removed, and the result is re-evaluated. The process continues until the input is clean or a maximum iteration limit is reached; if imperative instruction-like content remains, the agent halts to ensure security. By allowing multiple passes, our approach acknowledges that individual rewrites may fail but enables the system to catch and correct missed injections in later steps. Although immediately useful, worst-case analysis shows that SIC is not infallible; strong adversary can still get a 15% ASR by embedding non-imperative workflows. This nonetheless raises the bar.",
            "score": 1,
            "issue_id": 6630,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "e29c4154928b821d",
            "authors": [
                "Nils Philipp Walter",
                "Chawin Sitawarin",
                "Jamie Hayes",
                "David Stutz",
                "Ilia Shumailov"
            ],
            "affiliations": [
                "AI Sequrity Company",
                "CISPA Helmholtz Center for Information Security",
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21057.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#security",
                    "#data",
                    "#inference"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SIC (Soft Instruction Control) Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ prompt injection Ğ°Ñ‚Ğ°Ğº Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» ÑĞ°Ğ½Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚, Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ»Ğ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ¸Ñ…. Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ğ¾Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ, Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¼ - Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ non-imperative workflows Ğ²ÑÑ‘ ĞµÑ‰Ñ‘ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 15% ÑƒÑĞ¿ĞµÑ…Ğ°."
                },
                "en": {
                    "title": "SIC: Securing LLM Agents Against Prompt Injections",
                    "desc": "The paper introduces SIC, an iterative prompt sanitization method aimed at improving the security of tool-augmented Large Language Model (LLM) agents against prompt injection attacks. SIC works by continuously inspecting and cleaning incoming data to identify and neutralize potentially harmful instructions that could alter the agent's behavior. The method allows for multiple iterations of data evaluation, ensuring that any missed malicious content can be addressed in subsequent passes. While SIC significantly enhances security, it acknowledges that determined adversaries may still find ways to exploit the system, achieving a 15% attack success rate under certain conditions."
                },
                "zh": {
                    "title": "æå‡LLMä»£ç†å®‰å…¨æ€§çš„è¿­ä»£æ¸…ç†æ–¹æ³•",
                    "desc": "SICï¼ˆè½¯æŒ‡ä»¤æ§åˆ¶ï¼‰æ˜¯ä¸€ç§è¿­ä»£çš„æç¤ºæ¸…ç†æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå·¥å…·å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åå¤æ£€æŸ¥å’Œæ¸…ç†è¾“å…¥æ•°æ®ï¼Œé˜²æ­¢æç¤ºæ³¨å…¥æ”»å‡»ï¼Œä»è€Œä¿æŠ¤ä»£ç†çš„è¡Œä¸ºã€‚SICä¼šè¯†åˆ«å¹¶å¤„ç†å¯èƒ½å±å®³ä»£ç†è¡Œä¸ºçš„æ¶æ„å†…å®¹ï¼Œç¡®ä¿è¾“å…¥æ•°æ®çš„å®‰å…¨æ€§ã€‚å°½ç®¡åœ¨æœ€åæƒ…å†µä¸‹ä»å­˜åœ¨ä¸€å®šçš„é£é™©ï¼Œä½†SICæ˜¾è‘—æé«˜äº†ç³»ç»Ÿçš„å®‰å…¨æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20708",
            "title": "ALICE-LRI: A General Method for Lossless Range Image Generation for\n  Spinning LiDAR Sensors without Calibration Metadata",
            "url": "https://huggingface.co/papers/2510.20708",
            "abstract": "ALICE-LRI is a sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds, preserving all points and maintaining geometric accuracy in real-time.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation.",
            "score": 1,
            "issue_id": 6632,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "2267826ebc8fe420",
            "authors": [
                "Samuel Soutullo",
                "Miguel Yermo",
                "David L. VilariÃ±o",
                "Ã“scar G. Lorenzo",
                "JosÃ© C. Cabaleiro",
                "Francisco F. Rivera"
            ],
            "affiliations": [
                "Centro Singular de InvestigaciÃ³n en TecnoloxÃ­as Intelixentes (CiTIUS), RÃºa de Jenaro de la Fuente DomÃ­nguez, Santiago de Compostela, 15782, CoruÃ±a, Spain",
                "Departamento de ElectrÃ³nica ComputaciÃ³n, Universidade de Santiago de Compostela, RÃºa Lope GÃ³mez de Marzoa, Santiago de Compostela, 15782, CoruÃ±a, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20708.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸ“¡",
                "ru": {
                    "title": "Ğ˜Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ LiDAR Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "ALICE-LRI â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ 2D range-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ LiDAR-ÑĞµĞ½ÑĞ¾Ñ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ LiDAR-Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ»Ğ°Ğ·ĞµÑ€Ğ½Ñ‹Ñ… Ğ»ÑƒÑ‡ĞµĞ¹ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… KITTI Ğ¸ DurLAR Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… Ğ¿Ğ¾Ğ³Ñ€ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞµĞ½ÑĞ¾Ñ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Achieving Lossless LiDAR Projections for Precision Mapping",
                    "desc": "ALICE-LRI is a novel method designed to generate lossless range images from spinning LiDAR point clouds, ensuring that no data points are lost during the process. This approach is sensor-agnostic, meaning it can work with any spinning LiDAR sensor without needing specific calibration data. By automatically determining the intrinsic parameters of the LiDAR sensor, ALICE-LRI maintains high geometric accuracy and allows for real-time processing of point clouds. The method has been validated on extensive datasets, showing perfect point preservation and significant improvements in the quality of downstream applications."
                },
                "zh": {
                    "title": "æ— æŸLiDARèŒƒå›´å›¾åƒç”Ÿæˆçš„é©å‘½æ€§æ–¹æ³•",
                    "desc": "ALICE-LRIæ˜¯ä¸€ç§ä¼ æ„Ÿå™¨æ— å…³çš„æ–¹æ³•ï¼Œå¯ä»¥ä»æ—‹è½¬çš„LiDARç‚¹äº‘ä¸­ç”Ÿæˆæ— æŸçš„èŒƒå›´å›¾åƒï¼Œå®æ—¶ä¿æŒæ‰€æœ‰ç‚¹å’Œå‡ ä½•ç²¾åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨æ–­æ¿€å…‰æŸé…ç½®ã€è§’åº¦åˆ†å¸ƒå’Œæ¯æŸæ¿€å…‰çš„æ ¡æ­£ç­‰å…³é”®å‚æ•°ï¼Œè‡ªåŠ¨é€†å‘å·¥ç¨‹ä»»ä½•æ—‹è½¬LiDARä¼ æ„Ÿå™¨çš„å†…åœ¨å‡ ä½•ç‰¹æ€§ã€‚ALICE-LRIåœ¨KITTIå’ŒDurLARæ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æœ‰ç‚¹äº‘ä¸­æ²¡æœ‰ç‚¹ä¸¢å¤±ï¼Œå‡ ä½•ç²¾åº¦ä¿æŒåœ¨ä¼ æ„Ÿå™¨ç²¾åº¦èŒƒå›´å†…ã€‚è¯¥æ–¹æ³•çš„æ— æŸæŠ•å½±ä¸ºé«˜ç²¾åº¦é¥æ„Ÿåº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œç¡®ä¿äº†å®Œæ•´çš„å‡ ä½•ä¿ç•™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21111",
            "title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language\n  Models in Physical Environments",
            "url": "https://huggingface.co/papers/2510.21111",
            "abstract": "Active Visual Reasoning (AVR) extends visual reasoning to interactive, partially observable environments, requiring agents to sequentially gather and integrate information for coherent decision-making, as evaluated by the CLEVR-AVR benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual reasoning in multimodal large language models (MLLMs) has primarily been studied in static, fully observable settings, limiting their effectiveness in real-world environments where information is often incomplete due to occlusion or limited field of view. Humans, in contrast, actively explore and interact with their environment-moving, examining, and manipulating objects-to gather information through a closed-loop process integrating perception, reasoning, and action. Inspired by this human capability, we introduce the Active Visual Reasoning (AVR) task, extending visual reasoning to partially observable, interactive environments. AVR necessitates agents to: (1) actively acquire information via sequential physical actions, (2) integrate observations across multiple steps for coherent reasoning, and (3) dynamically adjust decisions based on evolving visual feedback. To rigorously evaluate AVR, we introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive environments designed to assess both reasoning correctness and information-gathering efficiency. We present AVR-152k, a large-scale dataset that offers rich Chain-of-Thought (CoT) annotations detailing iterative reasoning for uncertainty identification, action-conditioned information gain prediction, and information-maximizing action selection, crucial for training agents in a higher-order Markov Decision Process. Building on this, we develop PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR, embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath, Geometry30K). Our analysis also reveals that current embodied MLLMs, despite detecting information incompleteness, struggle to actively acquire and integrate new information through interaction, highlighting a fundamental gap in active reasoning capabilities.",
            "score": 0,
            "issue_id": 6627,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "c9c37d0f923c45b0",
            "authors": [
                "Weijie Zhou",
                "Xuantang Xiong",
                "Yi Peng",
                "Manli Tao",
                "Chaoyang Zhao",
                "Honghui Dong",
                "Ming Tang",
                "Jinqiao Wang"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences",
                "ObjectEye Inc.",
                "Tencent Robotics & Futian Laboratory, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21111.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#agents",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ¸Ñ‚Ğµ AI Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ¸ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Active Visual Reasoning (AVR), Ğ³Ğ´Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ Ğ»ÑĞ´ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CLEVR-AVR Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AVR-152k Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Chain-of-Thought Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ±Ğ¾Ñ€Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PhysVLM-AVR, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ°Ñ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ multimodal LLM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹."
                },
                "en": {
                    "title": "Empowering AI with Active Visual Reasoning",
                    "desc": "Active Visual Reasoning (AVR) is a new approach that allows AI agents to make decisions in environments where they cannot see everything at once. Unlike traditional visual reasoning, which works in fully observable settings, AVR requires agents to actively explore and gather information through actions. This process involves integrating observations over time and adjusting decisions based on what they see, similar to how humans interact with their surroundings. The paper introduces a benchmark called CLEVR-AVR to evaluate these capabilities and presents a dataset and model that improve the performance of AI in these complex scenarios."
                },
                "zh": {
                    "title": "ä¸»åŠ¨è§†è§‰æ¨ç†ï¼šæ™ºèƒ½ä½“çš„äº¤äº’å¼å†³ç­–æ–°å¢ƒç•Œ",
                    "desc": "ä¸»åŠ¨è§†è§‰æ¨ç†ï¼ˆAVRï¼‰å°†è§†è§‰æ¨ç†æ‰©å±•åˆ°äº¤äº’å¼ã€éƒ¨åˆ†å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸­ï¼Œè¦æ±‚æ™ºèƒ½ä½“é€šè¿‡é¡ºåºæ”¶é›†å’Œæ•´åˆä¿¡æ¯æ¥è¿›è¡Œè¿è´¯çš„å†³ç­–ã€‚ä¸é™æ€ã€å®Œå…¨å¯è§‚å¯Ÿçš„è®¾ç½®ä¸åŒï¼ŒAVR éœ€è¦æ™ºèƒ½ä½“é€šè¿‡ç‰©ç†åŠ¨ä½œä¸»åŠ¨è·å–ä¿¡æ¯ï¼Œå¹¶åœ¨å¤šä¸ªæ­¥éª¤ä¸­æ•´åˆè§‚å¯Ÿç»“æœä»¥è¿›è¡Œåˆç†æ¨ç†ã€‚ä¸ºè¯„ä¼° AVR çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº† CLEVR-AVR åŸºå‡†æµ‹è¯•ï¼Œè®¾è®¡äº†å¤šè½®äº¤äº’ç¯å¢ƒä»¥è¯„ä¼°æ¨ç†çš„æ­£ç¡®æ€§å’Œä¿¡æ¯æ”¶é›†çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº† PhysVLM-AVRï¼Œè¿™æ˜¯ä¸€ç§åœ¨ CLEVR-AVR ä¸Šè¡¨ç°å‡ºè‰²çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11370",
            "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers",
            "url": "https://huggingface.co/papers/2510.11370",
            "abstract": "Rollout Routing Replay (R3) stabilizes reinforcement learning training in Mixture-of-Experts models by reducing discrepancies between training and inference routing behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models.",
            "score": 0,
            "issue_id": 6630,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "bffa313f30aff879",
            "authors": [
                "Wenhan Ma",
                "Hailin Zhang",
                "Liang Zhao",
                "Yifan Song",
                "Yudong Wang",
                "Zhifang Sui",
                "Fuli Luo"
            ],
            "affiliations": [
                "LLM-Core Xiaomi",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11370.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MoE Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾ÑƒÑ‚Ğ¸Ğ½Ğ³Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Rollout Routing Replay (R3) Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Mixture-of-Experts Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ñ€Ğ¾ÑƒÑ‚Ğ¸Ğ½Ğ³Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğº ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. R3 Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾ÑƒÑ‚Ğ¸Ğ½Ğ³Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ RL-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ‚Ğ¸Ğ¿Ğ° GSPO Ğ¸ TIS."
                },
                "en": {
                    "title": "Stabilizing RL Training in Mixture-of-Experts with R3",
                    "desc": "The paper introduces Rollout Routing Replay (R3), a method designed to stabilize reinforcement learning (RL) training in Mixture-of-Experts (MoE) models. It addresses the issue of inconsistencies between the routing behaviors during training and inference, which can lead to training failures. By recording and replaying routing distributions from the inference phase during training, R3 minimizes the KL divergence between training and inference policies. The results show that R3 effectively prevents training collapse and outperforms existing methods, providing a promising solution for enhancing RL in MoE architectures."
                },
                "zh": {
                    "title": "ç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRollout Routing Replayï¼ˆR3ï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç¨³å®šæ··åˆä¸“å®¶æ¨¡å‹ä¸­çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚é€šè¿‡å‡å°‘è®­ç»ƒå’Œæ¨ç†é˜¶æ®µä¹‹é—´çš„è·¯ç”±è¡Œä¸ºå·®å¼‚ï¼ŒR3æœ‰æ•ˆåœ°è§£å†³äº†åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚æˆ‘ä»¬åˆ†æäº†æ··åˆä¸“å®¶æ¨¡å‹çš„è®­ç»ƒä¸æ¨ç†ä¸€è‡´æ€§ï¼Œå¹¶å‘ç°ä¸¤è€…ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è·¯ç”±è¡Œä¸ºå·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR3èƒ½å¤Ÿæ˜¾è‘—é™ä½è®­ç»ƒä¸æ¨ç†ç­–ç•¥ä¹‹é—´çš„KLæ•£åº¦ï¼Œä»è€Œé˜²æ­¢è®­ç»ƒå´©æºƒï¼Œå¹¶åœ¨å¤šç§è®¾ç½®ä¸­ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-24.html",
    "link_next": "2025-10-28.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "24.10",
        "en": "10/24",
        "zh": "10æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "28.10",
        "en": "10/28",
        "zh": "10æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 7,
        "#cv": 4,
        "#rl": 7,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 5,
        "#3d": 2,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 8,
        "#math": 3,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 13,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 9,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 13,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 3,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}