{
    "date": {
        "ru": "4 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 4",
        "zh": "11æœˆ4æ—¥"
    },
    "time_utc": "2024-11-04 02:50",
    "weekday": 0,
    "issue_id": 407,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.00412",
            "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation",
            "url": "https://huggingface.co/papers/2411.00412",
            "abstract": "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but often produce hallucinations for complex ones. While integrating LLMs with tools can increase reliability, this approach typically results in over-reliance on tools, diminishing the model's ability to solve simple problems through basic reasoning. In contrast, human experts first assess problem complexity using domain knowledge before choosing an appropriate solution approach. Inspired by this human problem-solving process, we propose a novel two-component fine-tuning method. In the first component World Knowledge Distillation (WKD), LLMs learn directly from solutions generated using tool's information to internalize domain knowledge. In the second component Tool Usage Adaptation (TUA), we partition problems into easy and hard categories based on the model's direct answering accuracy. While maintaining the same alignment target for easy problems as in WKD, we train the model to intelligently switch to tool usage for more challenging problems. We validate our method on six scientific benchmark datasets, spanning mathematics, climate science and epidemiology. On average, our models demonstrate a 28.18% improvement in answer accuracy and a 13.89% increase in tool usage precision across all datasets, surpassing state-of-the-art models including GPT-4o and Claude-3.5.",
            "score": 2,
            "issue_id": 407,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 1",
                "zh": "11æœˆ1æ—¥"
            },
            "hash": "27e4deefc7d09df0",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… - Ğ¿Ñ€Ğ¸Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLMs: Smart Tool Use for Complex Problems",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) in solving complex scientific problems, which often lead to inaccuracies or 'hallucinations'. The authors propose a two-component fine-tuning method that mimics human problem-solving strategies by first assessing problem complexity. The first component, World Knowledge Distillation (WKD), allows LLMs to learn from solutions that utilize external tools, while the second component, Tool Usage Adaptation (TUA), helps the model categorize problems as easy or hard and decide when to use tools. The proposed method shows significant improvements in accuracy and tool usage precision across various scientific datasets, outperforming existing models."
                },
                "zh": {
                    "title": "æ™ºèƒ½åˆ‡æ¢ï¼Œæå‡æ¨¡å‹è§£å†³é—®é¢˜çš„èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³ç®€å•ç§‘å­¦é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚é—®é¢˜ä¸Šå¸¸å¸¸å‡ºç°å¹»è§‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒç»„ä»¶å¾®è°ƒæ–¹æ³•ï¼Œæ¨¡ä»¿äººç±»ä¸“å®¶çš„è§£å†³é—®é¢˜è¿‡ç¨‹ã€‚ç¬¬ä¸€ä¸ªç»„ä»¶æ˜¯ä¸–ç•ŒçŸ¥è¯†è’¸é¦ï¼ˆWKDï¼‰ï¼Œä½¿LLMsä»å·¥å…·ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆä¸­å­¦ä¹ é¢†åŸŸçŸ¥è¯†ã€‚ç¬¬äºŒä¸ªç»„ä»¶æ˜¯å·¥å…·ä½¿ç”¨é€‚åº”ï¼ˆTUAï¼‰ï¼Œæ ¹æ®æ¨¡å‹çš„ç›´æ¥å›ç­”å‡†ç¡®æ€§å°†é—®é¢˜åˆ†ä¸ºç®€å•å’Œå›°éš¾ä¸¤ç±»ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸Šçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.23775",
            "title": "In-Context LoRA for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2410.23775",
            "abstract": "Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20sim 100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA",
            "score": 1,
            "issue_id": 407,
            "pub_date": "2024-10-31",
            "pub_date_card": {
                "ru": "31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 31",
                "zh": "10æœˆ31æ—¥"
            },
            "hash": "748dab03a37a21a4",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° DiT Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DiT Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ pipeline, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ LoRA-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ IC-LoRA, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼."
                },
                "en": {
                    "title": "Unlocking In-Context Generation with IC-LoRA",
                    "desc": "This paper investigates the use of diffusion transformers (DiTs) for generating images without being tied to specific tasks. The authors propose that DiTs can generate images effectively with minimal adjustments, leveraging their inherent in-context generation capabilities. They introduce a new method called In-Context LoRA (IC-LoRA), which simplifies the process by concatenating images and using joint captioning, along with small dataset tuning. This approach enhances the quality of generated images while maintaining a flexible architecture that can adapt to various tasks without extensive retraining."
                },
                "zh": {
                    "title": "æ¿€æ´»ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰åœ¨æ— ä»»åŠ¡ç‰¹å®šçš„å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„DiTsæœ¬èº«å…·å¤‡ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œåªéœ€å°‘é‡è°ƒæ•´å³å¯æ¿€æ´»ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒDiTsèƒ½å¤Ÿåœ¨ä¸è°ƒæ•´çš„æƒ…å†µä¸‹æœ‰æ•ˆè¿›è¡Œä¸Šä¸‹æ–‡ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æµç¨‹ï¼Œåˆ©ç”¨DiTsçš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œç”Ÿæˆé«˜ä¿çœŸåº¦çš„å›¾åƒé›†ï¼Œä¸”ä¸éœ€è¦å¯¹åŸå§‹æ¨¡å‹è¿›è¡Œä¿®æ”¹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-01.html",
    "link_next": "2024-11-05.html",
    "short_date_prev": {
        "ru": "01.11",
        "en": "11/01",
        "zh": "11æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "05.11",
        "en": "11/05",
        "zh": "11æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ã€‚SAEså¯ä»¥å°†ä¸æ˜“è§£é‡Šçš„ä¸­é—´è¡¨ç¤ºåˆ†è§£æˆæ˜“è§£é‡Šçš„ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°æ§åˆ¶å’Œåˆ†æã€‚ç„¶è€Œï¼Œç±»ä¼¼çš„åˆ†æå’Œæ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹ä¸­å°šç¼ºä¹ç ”ç©¶ã€‚ä½œè€…ç ”ç©¶äº†åœ¨å°‘æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚SDXL Turboï¼‰ä¸­ä½¿ç”¨SAEså­¦ä¹ å¯è§£é‡Šç‰¹å¾çš„å¯èƒ½æ€§ã€‚ç»“æœå‘ç°ï¼ŒSAEså­¦åˆ°çš„ç‰¹å¾å¯è§£é‡Šï¼Œå¹¶å¯¹ç”Ÿæˆè¿‡ç¨‹äº§ç”Ÿå› æœå½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹å†…éƒ¨çš„ä¸“ä¸šåŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªæ¨¡å—ä¸»è¦å¤„ç†å›¾åƒç»„åˆï¼Œä¸€ä¸ªè´Ÿè´£æ·»åŠ ç»†èŠ‚ï¼Œå¦ä¸€ä¸ªè´Ÿè´£é¢œè‰²ã€å…‰ç…§å’Œé£æ ¼ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œæ˜¯ç†è§£ç”Ÿæˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å†…éƒ¨çš„é‡è¦ä¸€æ­¥ï¼Œå±•ç¤ºäº†SAEsåœ¨è§†è§‰é¢†åŸŸçš„æ½œåŠ›ã€‚",
        "title": "Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ã€‚SAEså¯ä»¥å°†ä¸æ˜“è§£é‡Šçš„ä¸­é—´è¡¨ç¤ºåˆ†è§£æˆæ˜“è§£é‡Šçš„ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°æ§åˆ¶å’Œåˆ†æã€‚ç„¶è€Œï¼Œç±»ä¼¼çš„åˆ†æå’Œæ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹ä¸­å°šç¼ºä¹ç ”ç©¶ã€‚ä½œè€…ç ”ç©¶äº†åœ¨å°‘æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚SDXL Turboï¼‰ä¸­ä½¿ç”¨SAEså­¦ä¹ å¯è§£é‡Šç‰¹å¾çš„å¯èƒ½æ€§ã€‚ç»“æœå‘ç°ï¼ŒSAEså­¦åˆ°çš„ç‰¹å¾å¯è§£é‡Šï¼Œå¹¶å¯¹ç”Ÿæˆè¿‡ç¨‹äº§ç”Ÿå› æœå½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹å†…éƒ¨çš„ä¸“ä¸šåŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªæ¨¡å—ä¸»è¦å¤„ç†å›¾åƒç»„åˆï¼Œä¸€ä¸ªè´Ÿè´£æ·»åŠ ç»†èŠ‚ï¼Œå¦ä¸€ä¸ªè´Ÿè´£é¢œè‰²ã€å…‰ç…§å’Œé£æ ¼ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œæ˜¯ç†è§£ç”Ÿæˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å†…éƒ¨çš„é‡è¦ä¸€æ­¥ï¼Œå±•ç¤ºäº†SAEsåœ¨è§†è§‰é¢†åŸŸçš„æ½œåŠ›ã€‚\n\nPinyin transcription:\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le xÄ«shÅ« zÃ¬biÇnmÇqÃ¬ (SAEs) zÃ i dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zhÅng de yÃ¬ngyÃ²ng. SAEs kÄ›yÇ jiÄng bÃ¹yÃ¬ jiÄ›shÃ¬ de zhÅngjiÄn biÇoshÃ¬ fÄ“njiÄ› chÃ©ng yÃ¬ jiÄ›shÃ¬ de tÃ¨zhÄ“ng, cÃ³ng'Ã©r gÃ¨ng hÇo de kÃ²ngzhÃ¬ hÃ© fÄ“nxÄ«. RÃ¡n'Ã©r, lÃ¨isÃ¬ de fÄ“nxÄ« hÃ© fÄngfÇ zÃ i wÃ©nbÄ›n dÃ o tÃºxiÃ ng de mÃ³xÃ­ng zhÅng shÃ ng quÄ“fÃ¡ yÃ¡njiÅ«. ZuÃ²zhÄ› yÃ¡njiÅ« le zÃ i shÇo bÃ¹ wÃ©nbÄ›n dÃ o tÃºxiÃ ng kuÃ²sÃ n mÃ³xÃ­ng (rÃº SDXL Turbo) zhÅng shÇyÃ²ng SAEs xuÃ©xÃ­ kÄ› jiÄ›shÃ¬ tÃ¨zhÄ“ng de kÄ›nÃ©ngxÃ¬ng. JiÃ©guÇ’ fÄxiÃ n, SAEs xuÃ© dÃ o de tÃ¨zhÄ“ng kÄ› jiÄ›shÃ¬, bÃ¬ng duÃ¬ shÄ“ngchÃ©ng guÃ²chÃ©ng chÇnshÄ“ng yÄ«nguÇ’ yÇngxiÇng, jiÄ“shÃ¬ le mÃ³xÃ­ng nÃ¨ibÃ¹ de zhuÄnmÃ©nhuÃ . JÃ¹tÇ lÃ¡i shuÅ, yÇ’u yÄ«gÃ¨ mÃ³kuÃ i zhÇ”yÃ o chÇ”lÇ tÃºxiÃ ng zÇ”hÃ©, yÄ«gÃ¨ fÃ¹zÃ© tiÄnjiÇ xÃ¬jiÄ›, lÃ¬ng yÄ«gÃ¨ fÃ¹zÃ© yÃ¡nsÃ¨, guÄngzhÃ o hÃ© fÄ“nggÃ©. YÄ«ncÇ, zhÃ¨ xiÃ ng gÅngzuÃ² shÃ¬ liÇojiÄ› shÄ“ngchÃ©ng wÃ©nbÄ›n dÃ o tÃºxiÃ ng mÃ³xÃ­ng nÃ¨ibÃ¹ de zhÃ²ngyÃ o yÄ« bÃ¹, zhÇnshÃ¬ le SAEs zÃ i shÃ¬juÃ© lÇngyÃ¹ de qiÃ¡nlÃ¬.",
        "vocab": "[\n    {\"word\": \"ç¨€ç–è‡ªç¼–ç å™¨\", \"pinyin\": \"xÄ« shÅ« zÃ¬ biÄn mÇ qÃ¬\", \"trans\": \"Sparse Autoencoder\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"ä¸­é—´è¡¨ç¤º\", \"pinyin\": \"zhÅng jiÄn biÇo shÃ¬\", \"trans\": \"Intermediate Representation\"},\n    {\"word\": \"ç‰¹å¾\", \"pinyin\": \"tÃ¨ zhÄ“ng\", \"trans\": \"Feature\"},\n    {\"word\": \"åˆ†è§£\", \"pinyin\": \"fÄ“n jiÄ›\", \"trans\": \"Decompose\"},\n    {\"word\": \"æ§åˆ¶\", \"pinyin\": \"kÃ²ng zhÃ¬\", \"trans\": \"Control\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“n xÄ«\", \"trans\": \"Analyze\"},\n    {\"word\": \"æ–‡æœ¬åˆ°å›¾åƒ\", \"pinyin\": \"wÃ©n bÄ›n dÃ o tÃº xiÃ ng\", \"trans\": \"Text-to-Image\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"Model\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"Research\"},\n    {\"word\": \"å°‘æ­¥\", \"pinyin\": \"shÇo bÃ¹\", \"trans\": \"Few-Step\"},\n    {\"word\": \"æ‰©æ•£æ¨¡å‹\", \"pinyin\": \"kuÃ² sÃ n mÃ³ xÃ­ng\", \"trans\": \"Diffusion Model\"},\n    {\"word\": \"å¯è§£é‡Š\", \"pinyin\": \"kÄ› jiÄ› shÃ¬\", \"trans\": \"Interpretable\"},\n    {\"word\": \"å› æœå½±å“\", \"pinyin\": \"yÄ«n guÇ’ yÇng xiÇng\", \"trans\": \"Causal Effect\"},\n    {\"word\": \"æ­ç¤º\", \"pinyin\": \"jiÄ“ shÃ¬\", \"trans\": \"Reveal\"},\n    {\"word\": \"ä¸“ä¸šåŒ–\", \"pinyin\": \"zhuÄn yÃ¨ huÃ \", \"trans\": \"Specialization\"},\n    {\"word\": \"æ¨¡å—\", \"pinyin\": \"mÃ³ kuÃ i\", \"trans\": \"Module\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"Process\"},\n    {\"word\": \"å›¾åƒç»„åˆ\", \"pinyin\": \"tÃº xiÃ ng zÇ” hÃ©\", \"trans\": \"Image Composition\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬ jiÄ›\", \"trans\": \"Detail\"},\n    {\"word\": \"é¢œè‰²\", \"pinyin\": \"yÃ¡n sÃ¨\", \"trans\": \"Color\"},\n    {\"word\": \"å…‰ç…§\", \"pinyin\": \"guÄng zhÃ o\", \"trans\": \"Lighting\"},\n    {\"word\": \"é£æ ¼\", \"pinyin\": \"fÄ“ng gÄ“\", \"trans\": \"Style\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"Generate\"},\n    {\"word\": \"è§†è§‰é¢†åŸŸ\", \"pinyin\": \"shÃ¬ juÃ© lÇng yÃ¹\", \"trans\": \"Visual Domain\"},\n    {\"word\": \"æ½œåŠ›\", \"pinyin\": \"qiÃ¡n lÃ¬\", \"trans\": \"Potential\"}\n]",
        "trans": "This article discusses the application of Sparse Autoencoders (SAEs) in large language models (LLMs). SAEs can decompose hard-to-interpret intermediate representations into interpretable features, thereby enabling better control and analysis. However, similar analyses and methods in text-to-image models are still lacking in research. The authors investigated the possibility of using SAEs to learn interpretable features in few-step text-to-image diffusion models (such as SDXL Turbo). The results showed that the features learned by SAEs are interpretable and have a causal impact on the generation process, revealing specialization within the model. Specifically, one module mainly handles image composition, another is responsible for adding details, and another deals with color, lighting, and style. Therefore, this work is an important step towards understanding the internal workings of text-to-image generation models and demonstrates the potential of SAEs in the visual domain.",
        "update_ts": "2024-11-03 09:12"
    }
}