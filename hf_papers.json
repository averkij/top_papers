{
    "date": {
        "ru": "8 мая",
        "en": "May 8",
        "zh": "5月8日"
    },
    "time_utc": "2025-05-08 08:15",
    "weekday": 3,
    "issue_id": 3653,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.04588",
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "url": "https://huggingface.co/papers/2505.04588",
            "abstract": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.",
            "score": 12,
            "issue_id": 3647,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "24edc7c3c5e5e23d",
            "authors": [
                "Hao Sun",
                "Zile Qiao",
                "Jiayan Guo",
                "Xuanbo Fan",
                "Yingyan Hou",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Yan Zhang"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04588.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ZeroSearch: обучение LLM эффективному поиску без реальных поисковых систем",
                    "desc": "Статья представляет ZeroSearch - новую систему обучения с подкреплением для улучшения поисковых возможностей больших языковых моделей (LLM). В отличие от предыдущих подходов, ZeroSearch не требует взаимодействия с реальными поисковыми системами, что решает проблемы неконтролируемого качества документов и высоких затрат на API. Метод использует легковесную предобученную модель в качестве модуля поиска и стратегию постепенного ухудшения качества генерируемых документов во время обучения. Эксперименты показывают, что ZeroSearch эффективно улучшает поисковые способности LLM, причем модели с 14 миллиардами параметров даже превосходят реальные поисковые системы."
                },
                "en": {
                    "title": "ZeroSearch: Enhancing LLM Search Without Real Engines",
                    "desc": "This paper presents ZeroSearch, a novel reinforcement learning framework designed to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses two significant challenges: the unpredictable quality of documents from search engines and the high costs associated with frequent API calls during RL training. ZeroSearch utilizes a supervised fine-tuning approach to create a retrieval module that can generate both relevant and noisy documents, followed by a curriculum-based strategy that gradually increases the difficulty of retrieval tasks. Experimental results show that ZeroSearch can effectively improve LLM search performance, with larger models outperforming traditional search engines."
                },
                "zh": {
                    "title": "提升LLMs搜索能力的创新框架",
                    "desc": "有效的信息搜索对于提升大型语言模型（LLMs）的推理和生成能力至关重要。本文提出了一种名为ZeroSearch的强化学习框架，旨在提高LLMs的搜索能力，而无需与真实搜索引擎互动。该方法通过轻量级的监督微调，将LLM转变为一个检索模块，并在训练过程中逐步降低生成文档的质量，以激发模型的推理能力。实验结果表明，ZeroSearch能够有效提升LLMs的搜索能力，并在不同参数规模的模型中表现出良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04622",
            "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
            "url": "https://huggingface.co/papers/2505.04622",
            "abstract": "Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io",
            "score": 6,
            "issue_id": 3652,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "8205883cc18835a6",
            "authors": [
                "Jingwen Ye",
                "Yuze He",
                "Yanning Zhou",
                "Yiqin Zhu",
                "Kaiwen Xiao",
                "Yong-Jin Liu",
                "Wei Yang",
                "Xiao Han"
            ],
            "affiliations": [
                "Tencent AIPD, China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04622.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#cv",
                    "#games"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Универсальная абстракция 3D-форм с помощью ИИ",
                    "desc": "Статья представляет PrimitiveAnything - новый фреймворк для абстракции 3D-форм с помощью примитивов. Он использует трансформер, обученный на масштабных данных человеческих абстракций, для автоматической генерации сборок примитивов. PrimitiveAnything применяет унифицированную параметризацию для разных типов примитивов и генерирует высококачественные абстракции, соответствующие человеческому восприятию. Фреймворк демонстрирует хорошую обобщающую способность на разнообразных категориях форм и имеет потенциал для применения в играх и других 3D-приложениях."
                },
                "en": {
                    "title": "Revolutionizing 3D Shape Understanding with PrimitiveAnything",
                    "desc": "This paper introduces PrimitiveAnything, a new framework for breaking down complex 3D shapes into simpler geometric parts, which is important for both human understanding and computer applications. Unlike previous methods that either optimize geometry without understanding or rely on small datasets, PrimitiveAnything learns from large-scale human-created examples to improve its generalization across different shape types. The framework uses a shape-conditioned primitive transformer for generating these parts in a structured way, ensuring clarity in how different primitives are represented. The results show that PrimitiveAnything produces high-quality assemblies that align well with human perception, making it useful for various 3D applications, including user-generated content in games."
                },
                "zh": {
                    "title": "形状抽象的新突破：PrimitiveAnything",
                    "desc": "形状原始抽象是将复杂的3D形状分解为简单几何元素的过程，这对人类视觉认知至关重要，并在计算机视觉和图形学中有广泛应用。现有的原始抽象方法通常依赖于几何优化，缺乏语义理解，或者仅从小规模、特定类别的数据集中学习，难以在多样的形状类别中进行泛化。我们提出了PrimitiveAnything，一个将形状原始抽象重新定义为原始组装生成任务的新框架。该框架通过大规模人类创作的抽象学习原始组装过程，从而能够更好地捕捉人类如何将复杂形状分解为原始元素。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04364",
            "title": "Benchmarking LLMs' Swarm intelligence",
            "url": "https://huggingface.co/papers/2505.04364",
            "abstract": "Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.",
            "score": 6,
            "issue_id": 3648,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "4b0575d2194aee20",
            "authors": [
                "Kai Ruan",
                "Mowen Huang",
                "Ji-Rong Wen",
                "Hao Sun"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04364.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🐝",
                "ru": {
                    "title": "SwarmBench: Тестирование роевого интеллекта языковых моделей",
                    "desc": "Статья представляет SwarmBench - новый бенчмарк для оценки способностей больших языковых моделей (LLM) к роевому интеллекту в многоагентных системах. SwarmBench включает пять задач координации в 2D-сетке, где агенты ограничены локальным восприятием и коммуникацией. Результаты экспериментов показывают значительные различия в производительности LLM между задачами, выявляя сложности планирования в условиях неопределенности. Авторы предоставляют открытый инструментарий для воспроизводимых исследований координации на основе LLM в многоагентных системах."
                },
                "en": {
                    "title": "Unlocking Swarm Intelligence in Language Models",
                    "desc": "This paper explores how Large Language Models (LLMs) can coordinate in Multi-Agent Systems (MAS) under strict constraints, similar to natural swarms. It introduces SwarmBench, a new benchmark that evaluates the swarm intelligence of LLMs by simulating decentralized coordination tasks in a 2D grid environment. The study highlights the challenges of local perception and communication, revealing significant performance variations among LLMs when faced with limited information. The findings emphasize the need for further research into LLMs' capabilities in decentralized scenarios to unlock their potential in future systems."
                },
                "zh": {
                    "title": "探索大型语言模型的群体智能潜力",
                    "desc": "大型语言模型（LLMs）在复杂推理方面显示出潜力，但它们在多智能体系统（MAS）中在严格约束下的协调能力仍然未被充分探索，尤其是在群体智能的细微差别方面。现有基准测试往往无法完全捕捉到在不完整时空信息下，智能体进行去中心化协调所面临的独特挑战。为此，我们引入了SwarmBench，这是一个新颖的基准，旨在系统评估LLMs作为去中心化智能体的群体智能能力。通过评估多个领先的LLMs，我们发现它们在任务中的表现差异显著，突显了在局部信息限制下的协调困难。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04512",
            "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
            "url": "https://huggingface.co/papers/2505.04512",
            "abstract": "Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.",
            "score": 5,
            "issue_id": 3652,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "82e5839ef846d9d8",
            "authors": [
                "Teng Hu",
                "Zhentao Yu",
                "Zhengguang Zhou",
                "Sen Liang",
                "Yuan Zhou",
                "Qin Lin",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04512.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Мультимодальная генерация персонализированных видео с сохранением идентичности",
                    "desc": "HunyuanCustom - это мультимодальная система для генерации персонализированных видео, поддерживающая условия в виде изображений, аудио, видео и текста. Она использует модуль слияния текста и изображений на основе LLaVA для улучшенного мультимодального понимания, а также модуль усиления идентификации изображений для сохранения согласованности личности в кадрах. Система включает специальные механизмы для внедрения аудио- и видеоусловий, такие как AudioNet и сеть выравнивания признаков на основе патчей. Эксперименты показывают, что HunyuanCustom превосходит современные методы по согласованности идентичности, реалистичности и соответствию текста видео."
                },
                "en": {
                    "title": "HunyuanCustom: Consistent and Multi-Modal Video Generation",
                    "desc": "This paper introduces HunyuanCustom, a framework for generating customized videos that maintain subject consistency while accommodating various input types like images, audio, and text. It enhances multi-modal understanding through a text-image fusion module and reinforces identity features across video frames with an image ID enhancement module. Additionally, it incorporates specialized mechanisms for audio and video conditioning, ensuring effective alignment and integration of different modalities. The results show that HunyuanCustom outperforms existing methods in terms of identity consistency, realism, and alignment with text, proving its effectiveness in controllable video generation."
                },
                "zh": {
                    "title": "多模态定制视频生成的创新之路",
                    "desc": "定制视频生成旨在根据用户定义的条件生成特定主题的视频，但现有方法在身份一致性和输入模态方面常常面临挑战。本文提出了HunyuanCustom，一个多模态定制视频生成框架，强调主题一致性，并支持图像、音频、视频和文本条件。我们的模型通过引入基于LLaVA的文本-图像融合模块和图像ID增强模块，解决了图像-文本条件生成任务，从而增强多模态理解。实验结果表明，HunyuanCustom在身份一致性、真实感和文本-视频对齐方面显著优于现有的最先进方法，验证了多模态条件和身份保持策略在可控视频生成中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04528",
            "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
            "url": "https://huggingface.co/papers/2505.04528",
            "abstract": "As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
            "score": 3,
            "issue_id": 3652,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "0e9e0d509e4b4624",
            "authors": [
                "Qi Liu",
                "Xinhao Zheng",
                "Renqiu Xia",
                "Xingzhi Qi",
                "Qinxiang Cao",
                "Junchi Yan"
            ],
            "affiliations": [
                "Sch. of Computer Science & Sch. of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04528.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#benchmark",
                    "#alignment",
                    "#interpretability",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Формальная верификация процесса решения задач искусственным интеллектом",
                    "desc": "Статья представляет новый подход к формализации решения задач как марковского процесса принятия решений. Авторы предлагают фреймворк FPS (Formal Problem-Solving), использующий среды формального доказательства теорем для верификации процесса решения задач. Также представлен D-FPS (Deductive FPS), разделяющий решение и проверку ответа для лучшего соответствия человеческому подходу. Созданы три новых набора данных для оценки систем решения задач, а также предложен метод RPE для формальной верификации корректности ответов."
                },
                "en": {
                    "title": "Revolutionizing Problem-Solving with Formal Frameworks",
                    "desc": "This paper addresses the challenge of formalizing problem-solving in science and engineering by proposing a new framework called FPS (Formal Problem-Solving). It treats problem-solving as a deterministic Markov decision process, allowing for process-level verifiability in AI-based agents. The authors introduce D-FPS (Deductive FPS) to separate the solving process from answer verification, enhancing alignment with human reasoning. They also present benchmarks for evaluating problem-solving capabilities and a novel method, RPE (Restricted Propositional Equivalence), for verifying the correctness of solutions through formal methods."
                },
                "zh": {
                    "title": "形式化问题解决的新框架",
                    "desc": "这篇论文探讨了问题解决的形式化，提出了一种将问题解决视为确定性马尔可夫决策过程的框架。作者介绍了FPS（正式问题解决）框架，利用现有的正式定理证明环境进行过程验证的问题解决。为了提高人类对齐，论文还提出了D-FPS（演绎FPS），将求解与答案验证解耦。最后，作者构建了三个基准测试，并提出了一种符号方法RPE来评估答案的正确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03912",
            "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
            "url": "https://huggingface.co/papers/2505.03912",
            "abstract": "Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.",
            "score": 1,
            "issue_id": 3652,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "f7347c1b093f9488",
            "authors": [
                "Can Cui",
                "Pengxiang Ding",
                "Wenxuan Song",
                "Shuanghao Bai",
                "Xinyang Tong",
                "Zirui Ge",
                "Runze Suo",
                "Wanqi Zhou",
                "Yang Liu",
                "Bofang Jia",
                "Han Zhao",
                "Siteng Huang",
                "Donglin Wang"
            ],
            "affiliations": [
                "HKUST(GZ)",
                "Westlake University",
                "Xian Jiaotong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03912.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Открытая платформа для исследования двухсистемных VLA архитектур",
                    "desc": "Статья посвящена двухсистемным архитектурам VLA (Vision-Language-Action) в области воплощенного интеллекта. Авторы анализируют и сравнивают существующие архитектуры, проводя систематическую эмпирическую оценку их ключевых элементов. Целью работы является создание открытой модели с низкими вычислительными затратами для дальнейших исследований. Проект планирует регулярно обновляться новыми экспериментальными выводами и улучшенными открытыми моделями."
                },
                "en": {
                    "title": "Empowering Embodied Intelligence with Open-Source VLA Models",
                    "desc": "This paper focuses on dual-system Vision-Language-Action (VLA) architectures, which are important for developing embodied intelligence. It highlights the current lack of open-source resources that allow for thorough performance analysis and optimization of these architectures. The authors summarize and compare existing designs and conduct empirical evaluations on their core elements. The goal is to provide a low-cost open-source model that can be continuously updated with new findings and improved performance options for researchers."
                },
                "zh": {
                    "title": "推动双系统VLA架构的开源探索",
                    "desc": "本文探讨了双系统视觉-语言-行动（VLA）架构在具身智能研究中的重要性，并指出目前缺乏足够的开源工作来进行性能分析和优化。作者总结并比较了现有双系统架构的结构设计，并对其核心设计元素进行了系统的实证评估。最终，本文将提供一个低成本的开源模型，以便进一步探索和研究。该项目将持续更新，提供更多实验结论和性能改进的开源模型供大家选择。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03418",
            "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
            "url": "https://huggingface.co/papers/2505.03418",
            "abstract": "Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.",
            "score": 1,
            "issue_id": 3652,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "8417799a01a2ecc2",
            "authors": [
                "Da Zheng",
                "Lun Du",
                "Junwei Su",
                "Yuchen Tian",
                "Yuqi Zhu",
                "Jintian Zhang",
                "Lanning Wei",
                "Ningyu Zhang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Ant Group, China",
                "The University of Hong Kong, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03418.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#survey",
                    "#math",
                    "#training",
                    "#reasoning",
                    "#science",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LLM: Новый рубеж в решении сложных задач",
                    "desc": "Эта статья исследует возможности и ограничения больших языковых моделей (LLM) в решении сложных задач. Авторы рассматривают такие техники, как рассуждения по цепочке мыслей (Chain-of-Thought), расширение знаний и различные методы верификации на основе LLM и инструментов. В статье обсуждаются проблемы применения LLM в различных областях, включая разработку программного обеспечения, математические рассуждения и доказательства, анализ данных и научные исследования. Также рассматриваются фундаментальные ограничения текущих решений на основе LLM и будущие направления развития в контексте многоступенчатых рассуждений, интеграции доменных знаний и верификации результатов."
                },
                "en": {
                    "title": "Unlocking Complex Problem-Solving with Large Language Models",
                    "desc": "This paper surveys the role of Large Language Models (LLMs) in solving complex problems across various fields. It highlights how LLMs combine computational power with human-like reasoning to generate solutions and make inferences. The paper addresses challenges such as multi-step reasoning, integrating domain knowledge, and verifying results when applying LLMs in real-world scenarios. It also discusses specific challenges in areas like software engineering and scientific research, while outlining future directions for improving LLM capabilities in complex problem-solving."
                },
                "zh": {
                    "title": "大型语言模型：复杂问题解决的新工具",
                    "desc": "本论文探讨了大型语言模型（LLMs）在复杂问题解决中的能力和局限性。与传统计算系统不同，LLMs结合了强大的计算能力和人类推理的近似，能够生成解决方案和进行推理。尽管LLMs在多步骤推理、领域知识整合和结果验证方面面临挑战，但它们在软件工程、数学推理、数据分析和科学研究等领域的应用潜力巨大。本文还讨论了当前LLM解决方案的基本局限性以及未来在复杂问题解决中的发展方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02393",
            "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
            "url": "https://huggingface.co/papers/2505.02393",
            "abstract": "Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.",
            "score": 1,
            "issue_id": 3651,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 мая",
                "en": "May 5",
                "zh": "5月5日"
            },
            "hash": "b5c708abbb25e1ce",
            "authors": [
                "Sungheon Jeong",
                "Jihong Park",
                "Mohsen Imani"
            ],
            "affiliations": [
                "MOLOCO",
                "University of California, Irvine"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02393.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Синтез событий из RGB для точного обнаружения видеоаномалий",
                    "desc": "В этой статье представлен метод IEF-VAD для обнаружения аномалий в видео, который объединяет RGB-кадры с синтезированными событийными представлениями. Система моделирует шум датчика, применяет покадровые обновления в стиле фильтра Калмана и итеративно уточняет слитое латентное состояние. IEF-VAD достигает нового уровня производительности на нескольких реальных тестовых наборах данных для обнаружения аномалий. Метод подчеркивает важность синтетических событийных представлений для выделения ключевых признаков движения в задачах анализа видео."
                },
                "en": {
                    "title": "Enhancing Video Anomaly Detection with Image-Event Fusion",
                    "desc": "The paper introduces a new method called Image-Event Fusion for Video Anomaly Detection (IEF-VAD) that improves the detection of unusual events in videos. Traditional methods rely only on RGB frames, which can miss important motion details. IEF-VAD combines RGB video data with synthetic event representations to enhance the detection process, using advanced techniques to manage noise and improve accuracy. This approach achieves state-of-the-art results in various benchmarks without needing special sensors or labeled data."
                },
                "zh": {
                    "title": "图像与事件融合，提升视频异常检测的准确性",
                    "desc": "现有的视频异常检测器主要依赖RGB帧，但这些帧缺乏捕捉突发或瞬态运动线索的时间分辨率。为了解决这个问题，我们提出了一种图像-事件融合的视频异常检测框架（IEF-VAD），该框架直接从RGB视频合成事件表示，并通过一种基于不确定性的过程将其与图像特征融合。该系统通过拉普拉斯近似建模重尾传感器噪声，应用卡尔曼风格的逐帧更新来平衡时间上的模态，并迭代优化融合的潜在状态以消除残余的跨模态噪声。IEF-VAD在多个真实世界的异常检测基准上设定了新的最先进水平，展示了合成事件表示在强调RGB帧中常被低估的运动线索方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00358",
            "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
            "url": "https://huggingface.co/papers/2505.00358",
            "abstract": "Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&B matches or exceeds the performance of state-of-the-art data mixing strategies.",
            "score": 1,
            "issue_id": 3652,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 мая",
                "en": "May 1",
                "zh": "5月1日"
            },
            "hash": "74b251baea8510bd",
            "authors": [
                "Albert Ge",
                "Tzu-Heng Huang",
                "John Cooper",
                "Avi Trost",
                "Ziyi Chu",
                "Satya Sai Srinath Namburi GNVV",
                "Ziyang Cai",
                "Kendall Park",
                "Nicholas Roberts",
                "Frederic Sala"
            ],
            "affiliations": [
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00358.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "R&B: Умное смешивание данных для эффективного обучения языковых моделей",
                    "desc": "Статья представляет новый фреймворк R&B для оптимизации стратегий смешивания данных при обучении языковых моделей. R&B перегруппирует обучающие данные на основе семантического сходства и эффективно оптимизирует состав данных, используя матрицу Грама, полученную из градиентов доменов. Этот метод устраняет необходимость в дополнительных вычислениях для получения оценочной информации. Теоретический и эмпирический анализ показывает эффективность R&B по сравнению с неадаптивными подходами к смешиванию данных."
                },
                "en": {
                    "title": "R&B: Smarter Data Mixing for Language Models",
                    "desc": "This paper introduces R&B, a novel framework for improving data mixing strategies in training language models. R&B addresses two main issues: the reliance on fixed data domains and the high computational cost associated with scaling these domains. By regrouping training data based on semantic similarity and optimizing data composition using domain gradients, R&B creates more effective and efficient training domains. The authors provide theoretical insights and empirical evidence showing that R&B can achieve superior performance with minimal additional computational overhead compared to existing methods."
                },
                "zh": {
                    "title": "R&B：高效的数据混合新策略",
                    "desc": "本文提出了一种新的数据混合策略R&B，旨在解决现有方法的两个主要缺陷。首先，R&B通过语义相似性重新划分训练数据，创建更细粒度的数据域，从而捕捉到重要的语义细节。其次，该框架通过利用训练过程中获得的领域梯度的Gram矩阵，优化数据组合，避免了额外的计算开销。实验结果表明，R&B在多种数据集上表现优异，能够以极小的计算成本超越现有的最先进数据混合策略。"
                }
            }
        }
    ],
    "link_prev": "2025-05-07.html",
    "link_next": "2025-05-09.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "07.05",
        "en": "05/07",
        "zh": "5月7日"
    },
    "short_date_next": {
        "ru": "09.05",
        "en": "05/09",
        "zh": "5月9日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 3,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的多模态奖励模型，称为 UnifiedReward-Think。它能够进行长链条的推理，以提高视觉理解和生成任务的奖励信号准确性。模型通过三个步骤进行训练：首先，使用少量图像生成偏好数据蒸馏 GPT-4 的推理过程；然后，利用模型的先验知识和泛化能力，准备大规模的多模态偏好数据；最后，通过群体相对策略优化进行增强微调。实验结果证明了该模型的优越性。",
        "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
        "pinyin": "Zhè piān wén zhāng jiè shào le yī zhǒng xīn de duō mó tài jiǎng lì mó xíng, chēng wéi UnifiedReward-Think. Tā néng gòu jìn xíng cháng liàn tiáo de tuí lǐ, yǐ tí gāo shì jué lǐ jiě hé shēng chéng rèn wù de jiǎng lì xìn hào zhùn què xìng. Mó xíng tōng guò sān gè bù zhòu jìn xíng xùn liàn: shǒu xiān, shǐ yòng shǎo liàng tú xiàng shēng chéng piàn hào shù jùn zhèng GPT-4 de tuí lǐ guò chéng; rán hòu, lì yòng mó xíng de xiān yán zhī shì hé fàn huà néng lì, zhǔn bèi dà guī mó de duō mó tài piàn hào shù jù; zùi hòu, tōng guò qún tǐ xiāng duì cè lüè yōu huà jìn xíng zēng qiáng wēi tiáo. Shí yàn jié guǒ zhèng míng le gāi mó xíng de yōu yuè xìng.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"奖励\", \"pinyin\": \"jiǎng lì\", \"trans\": \"reward\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generation\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wu\", \"trans\": \"task\"},\n    {\"word\": \"准确性\", \"pinyin\": \"zhǔn què xìng\", \"trans\": \"accuracy\"},\n    {\"word\": \"蒸馏\", \"pinyin\": \"zhēng liú\", \"trans\": \"distill\"},\n    {\"word\": \"先验\", \"pinyin\": \"xiān yàn\", \"trans\": \"prior\"},\n    {\"word\": \"知识\", \"pinyin\": \"zhī shi\", \"trans\": \"knowledge\"},\n    {\"word\": \"泛化\", \"pinyin\": \"fàn huà\", \"trans\": \"generalization\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"证明\", \"pinyin\": \"zhèng míng\", \"trans\": \"prove\"},\n    {\"word\": \"优越性\", \"pinyin\": \"yōu yuè xìng\", \"trans\": \"superiority\"}\n]",
        "trans": "This article introduces a new multimodal reward model called UnifiedReward-Think. It is capable of performing long-chain reasoning to enhance the accuracy of reward signals for visual understanding and generation tasks. The model is trained through three steps: first, using a small amount of image generation preference data to distill the reasoning process of GPT-4; then, leveraging the model's prior knowledge and generalization capabilities to prepare large-scale multimodal preference data; finally, performing reinforcement fine-tuning through population-based relative strategy optimization. Experimental results demonstrate the superiority of this model.",
        "update_ts": "2025-05-07 09:12"
    }
}