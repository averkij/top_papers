{
    "date": {
        "ru": "8 Ğ¼Ğ°Ñ",
        "en": "May 8",
        "zh": "5æœˆ8æ—¥"
    },
    "time_utc": "2025-05-08 02:29",
    "weekday": 3,
    "issue_id": 3647,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.04588",
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "url": "https://huggingface.co/papers/2505.04588",
            "abstract": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.",
            "score": 1,
            "issue_id": 3647,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 Ğ¼Ğ°Ñ",
                "en": "May 7",
                "zh": "5æœˆ7æ—¥"
            },
            "hash": "24edc7c3c5e5e23d",
            "authors": [
                "Hao Sun",
                "Zile Qiao",
                "Jiayan Guo",
                "Xuanbo Fan",
                "Yingyan Hou",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Yan Zhang"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04588.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ZeroSearch: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ZeroSearch - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ZeroSearch Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° API. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ZeroSearch ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "ZeroSearch: Enhancing LLM Search Without Real Engines",
                    "desc": "This paper presents ZeroSearch, a novel reinforcement learning framework designed to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses two significant challenges: the unpredictable quality of documents from search engines and the high costs associated with frequent API calls during RL training. ZeroSearch utilizes a supervised fine-tuning approach to create a retrieval module that can generate both relevant and noisy documents, followed by a curriculum-based strategy that gradually increases the difficulty of retrieval tasks. Experimental results show that ZeroSearch can effectively improve LLM search performance, with larger models outperforming traditional search engines."
                },
                "zh": {
                    "title": "æå‡LLMsæœç´¢èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ‰æ•ˆçš„ä¿¡æ¯æœç´¢å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºZeroSearchçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMsçš„æœç´¢èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¸çœŸå®æœç´¢å¼•æ“äº’åŠ¨ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§çš„ç›‘ç£å¾®è°ƒï¼Œå°†LLMè½¬å˜ä¸ºä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥é™ä½ç”Ÿæˆæ–‡æ¡£çš„è´¨é‡ï¼Œä»¥æ¿€å‘æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZeroSearchèƒ½å¤Ÿæœ‰æ•ˆæå‡LLMsçš„æœç´¢èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-07.html",
    "link_next": "2025-05-09.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "07.05",
        "en": "05/07",
        "zh": "5æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "09.05",
        "en": "05/09",
        "zh": "5æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œç§°ä¸º UnifiedReward-Thinkã€‚å®ƒèƒ½å¤Ÿè¿›è¡Œé•¿é“¾æ¡çš„æ¨ç†ï¼Œä»¥æé«˜è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„å¥–åŠ±ä¿¡å·å‡†ç¡®æ€§ã€‚æ¨¡å‹é€šè¿‡ä¸‰ä¸ªæ­¥éª¤è¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆï¼Œä½¿ç”¨å°‘é‡å›¾åƒç”Ÿæˆåå¥½æ•°æ®è’¸é¦ GPT-4 çš„æ¨ç†è¿‡ç¨‹ï¼›ç„¶åï¼Œåˆ©ç”¨æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå‡†å¤‡å¤§è§„æ¨¡çš„å¤šæ¨¡æ€åå¥½æ•°æ®ï¼›æœ€åï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–è¿›è¡Œå¢å¼ºå¾®è°ƒã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚",
        "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
        "pinyin": "ZhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de duÅ mÃ³ tÃ i jiÇng lÃ¬ mÃ³ xÃ­ng, chÄ“ng wÃ©i UnifiedReward-Think. TÄ nÃ©ng gÃ²u jÃ¬n xÃ­ng chÃ¡ng liÃ n tiÃ¡o de tuÃ­ lÇ, yÇ tÃ­ gÄo shÃ¬ juÃ© lÇ jiÄ› hÃ© shÄ“ng chÃ©ng rÃ¨n wÃ¹ de jiÇng lÃ¬ xÃ¬n hÃ o zhÃ¹n quÃ¨ xÃ¬ng. MÃ³ xÃ­ng tÅng guÃ² sÄn gÃ¨ bÃ¹ zhÃ²u jÃ¬n xÃ­ng xÃ¹n liÃ n: shÇ’u xiÄn, shÇ yÃ²ng shÇo liÃ ng tÃº xiÃ ng shÄ“ng chÃ©ng piÃ n hÃ o shÃ¹ jÃ¹n zhÃ¨ng GPT-4 de tuÃ­ lÇ guÃ² chÃ©ng; rÃ¡n hÃ²u, lÃ¬ yÃ²ng mÃ³ xÃ­ng de xiÄn yÃ¡n zhÄ« shÃ¬ hÃ© fÃ n huÃ  nÃ©ng lÃ¬, zhÇ”n bÃ¨i dÃ  guÄ« mÃ³ de duÅ mÃ³ tÃ i piÃ n hÃ o shÃ¹ jÃ¹; zÃ¹i hÃ²u, tÅng guÃ² qÃºn tÇ xiÄng duÃ¬ cÃ¨ lÃ¼Ã¨ yÅu huÃ  jÃ¬n xÃ­ng zÄ“ng qiÃ¡ng wÄ“i tiÃ¡o. ShÃ­ yÃ n jiÃ© guÇ’ zhÃ¨ng mÃ­ng le gÄi mÃ³ xÃ­ng de yÅu yuÃ¨ xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å¥–åŠ±\", \"pinyin\": \"jiÇng lÃ¬\", \"trans\": \"reward\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generation\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wu\", \"trans\": \"task\"},\n    {\"word\": \"å‡†ç¡®æ€§\", \"pinyin\": \"zhÇ”n quÃ¨ xÃ¬ng\", \"trans\": \"accuracy\"},\n    {\"word\": \"è’¸é¦\", \"pinyin\": \"zhÄ“ng liÃº\", \"trans\": \"distill\"},\n    {\"word\": \"å…ˆéªŒ\", \"pinyin\": \"xiÄn yÃ n\", \"trans\": \"prior\"},\n    {\"word\": \"çŸ¥è¯†\", \"pinyin\": \"zhÄ« shi\", \"trans\": \"knowledge\"},\n    {\"word\": \"æ³›åŒ–\", \"pinyin\": \"fÃ n huÃ \", \"trans\": \"generalization\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅu huÃ \", \"trans\": \"optimization\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“i tiÃ¡o\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"è¯æ˜\", \"pinyin\": \"zhÃ¨ng mÃ­ng\", \"trans\": \"prove\"},\n    {\"word\": \"ä¼˜è¶Šæ€§\", \"pinyin\": \"yÅu yuÃ¨ xÃ¬ng\", \"trans\": \"superiority\"}\n]",
        "trans": "This article introduces a new multimodal reward model called UnifiedReward-Think. It is capable of performing long-chain reasoning to enhance the accuracy of reward signals for visual understanding and generation tasks. The model is trained through three steps: first, using a small amount of image generation preference data to distill the reasoning process of GPT-4; then, leveraging the model's prior knowledge and generalization capabilities to prepare large-scale multimodal preference data; finally, performing reinforcement fine-tuning through population-based relative strategy optimization. Experimental results demonstrate the superiority of this model.",
        "update_ts": "2025-05-07 09:12"
    }
}