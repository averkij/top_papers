{
    "date": {
        "ru": "24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 24",
        "zh": "9æœˆ24æ—¥"
    },
    "time_utc": "2025-09-24 16:14",
    "weekday": 2,
    "issue_id": 6066,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.18174",
            "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR",
            "url": "https://huggingface.co/papers/2509.18174",
            "abstract": "Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.  \t\t\t\t\tAI-generated summary \t\t\t\t Arabic document OCR remains a challenging task due to the language's cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, a vision-language model fine- tuned specifically for Arabic document OCR. Leveraging a large-scale dataset combining synthetic and real-world documents, Baseer is trained using a decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, a high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving a WER of 0.25 and establishing a new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish a strong baseline for high-accuracy OCR on morphologically rich languages like Arabic.",
            "score": 79,
            "issue_id": 6056,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "e92f83a2d009cb0f",
            "authors": [
                "Khalil Hennara",
                "Muhammad Hreden",
                "Mohamed Motasim Hamed",
                "Ahmad Bastati",
                "Zeina Aldallal",
                "Sara Chrouf",
                "Safwan AlModhayan"
            ],
            "affiliations": [
                "Misraj.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18174.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#cv",
                    "#low_resource",
                    "#multimodal",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "ğŸ“œ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: Baseer ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ OCR",
                    "desc": "Baseer - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Baseer Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ² (WER) 0,25. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ğ¹."
                },
                "en": {
                    "title": "Baseer: Revolutionizing Arabic Document OCR with State-of-the-Art Performance",
                    "desc": "Baseer is a vision-language model specifically designed for Optical Character Recognition (OCR) of Arabic documents. It utilizes a decoder-only fine-tuning strategy on a large-scale dataset that includes both synthetic and real-world documents. This model achieves a remarkable Word Error Rate (WER) of 0.25, surpassing existing OCR solutions for Arabic. Additionally, the introduction of the Misraj-DocOCR benchmark provides a robust framework for evaluating the performance of Arabic OCR systems."
                },
                "zh": {
                    "title": "Baseerï¼šé˜¿æ‹‰ä¼¯æ–‡æ¡£OCRçš„æ–°æ ‡æ†",
                    "desc": "Baseeræ˜¯ä¸€ç§ä¸“é—¨ä¸ºé˜¿æ‹‰ä¼¯æ–‡æ¡£OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰å¾®è°ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å®ƒé‡‡ç”¨è§£ç å™¨ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†é˜¿æ‹‰ä¼¯æ–‡æ¡£çš„è¯†åˆ«å‡†ç¡®ç‡ï¼Œè¾¾åˆ°äº†0.25çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚è¯¥æ¨¡å‹åœ¨å¤„ç†é˜¿æ‹‰ä¼¯è¯­çš„è¿å†™ã€ä¸åŒå­—ä½“å’Œå³åˆ°å·¦çš„ä¹¦å†™æ–¹å‘ç­‰æŒ‘æˆ˜æ—¶è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒå¯ä»¥æ˜¾è‘—æå‡é€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯æ–‡OCRä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19249",
            "title": "Reinforcement Learning on Pre-Training Data",
            "url": "https://huggingface.co/papers/2509.19249",
            "abstract": "Reinforcement Learning on Pre-Training data (RLPT) optimizes large language models by autonomously exploring meaningful trajectories in pre-training data, improving generalizable reasoning skills without human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1, 6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
            "score": 39,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "7ee5ca9be200b064",
            "authors": [
                "Siheng Li",
                "Kejiao Li",
                "Zenan Xu",
                "Guanhua Huang",
                "Evander Yang",
                "Kun Li",
                "Haoyuan Wu",
                "Jiajia Wu",
                "Zihao Zheng",
                "Chenchen Zhang",
                "Kun Shi",
                "Kyrierl Deng",
                "Qi Yi",
                "Ruibin Xiong",
                "Tingqiang Xu",
                "Yuhao Jiang",
                "Jianfeng Yan",
                "Yuyuan Zeng",
                "Guanghui Xu",
                "Jinbao Xue",
                "Zhijiang Xu",
                "Zheng Fang",
                "Shuai Li",
                "Qibin Liu",
                "Xiaoxue Li",
                "Zhuoyu Li",
                "Yangyu Tao",
                "Fei Gao",
                "Cheng Jiang",
                "Bo Chao Wang",
                "Kai Liu",
                "Jianchen Zhu",
                "Wai Lam",
                "Wayyt Wang",
                "Bo Zhou",
                "Di Wang"
            ],
            "affiliations": [
                "HunYuan Infra Team",
                "LLM Department, Tencent",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19249.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RLPT: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (RLPT) Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. RLPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğµ, Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Autonomous Learning for Enhanced Reasoning in Language Models",
                    "desc": "Reinforcement Learning on Pre-Training data (RLPT) is a novel approach that enhances large language models (LLMs) by allowing them to learn from pre-training data without needing human annotations. It uses reinforcement learning to autonomously explore meaningful data trajectories, which helps improve the model's reasoning abilities. Unlike traditional methods that rely on supervised learning, RLPT derives reward signals directly from the data, focusing on predicting subsequent text segments based on prior context. This method not only boosts performance on various reasoning tasks but also shows promise for scaling with increased computational resources."
                },
                "zh": {
                    "title": "è‡ªä¸»æ¢ç´¢ï¼Œæå‡æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "å¼ºåŒ–å­¦ä¹ åœ¨é¢„è®­ç»ƒæ•°æ®ä¸Šçš„åº”ç”¨ï¼ˆRLPTï¼‰é€šè¿‡è‡ªä¸»æ¢ç´¢é¢„è®­ç»ƒæ•°æ®ä¸­çš„æœ‰æ„ä¹‰è½¨è¿¹ï¼Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå‡å…¶é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒRLPTå…è®¸ç­–ç•¥ä»é¢„è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹åç»­æ–‡æœ¬æ®µè½æ¥æ„å»ºå¥–åŠ±ä¿¡å·ï¼Œé¼“åŠ±åœ¨æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¸­æ¢ç´¢æ›´ä¸°å¯Œçš„è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLPTåœ¨å¤šä¸ªæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18644",
            "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
            "url": "https://huggingface.co/papers/2509.18644",
            "abstract": "A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.  \t\t\t\t\tAI-generated summary \t\t\t\t Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\\% to 85\\% in height generalization and from 6\\% to 64\\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.",
            "score": 39,
            "issue_id": 6056,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "9beb129b0b2f10ec",
            "authors": [
                "Juntu Zhao",
                "Wenbo Lu",
                "Di Zhang",
                "Yufeng Liu",
                "Yushen Liang",
                "Tianluo Zhang",
                "Yifeng Cao",
                "Junyuan Xie",
                "Yingdong Hu",
                "Shengjie Wang",
                "Junliang Guo",
                "Dequan Wang",
                "Yang Gao"
            ],
            "affiliations": [
                "New York University Shanghai",
                "Shanghai Jiao Tong University",
                "Spirit AI",
                "Tongji University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18644.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#robotics",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ—Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ±ĞµĞ· ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ²ÑƒÑ… ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€ Ğ½Ğ° Ğ·Ğ°Ğ¿ÑÑÑ‚ÑŒĞµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ñ‹ÑĞ¾Ñ‚Ğµ Ğ¸ Ğ² Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Visual-Only Policies: Better Generalization in Robot Manipulation!",
                    "desc": "This paper introduces a State-free Policy for robot manipulation that relies solely on visual observations instead of combining them with proprioceptive states. The authors argue that traditional state-based policies can lead to overfitting and poor generalization across different spatial contexts. By focusing on visual inputs, the State-free Policy demonstrates improved spatial generalization and data efficiency in various tasks, such as pick-and-place and shirt-folding. Empirical results show significant improvements in success rates for height and horizontal generalization, making this approach more effective for real-world applications."
                },
                "zh": {
                    "title": "æ— çŠ¶æ€ç­–ç•¥ï¼šæå‡æœºå™¨äººæ“ä½œçš„ç©ºé—´æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— çŠ¶æ€ç­–ç•¥ï¼Œä»…ä¾èµ–è§†è§‰è§‚å¯Ÿæ¥è¿›è¡Œæœºå™¨äººæ“ä½œä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºçŠ¶æ€çš„ç­–ç•¥ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•åœ¨ç©ºé—´æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®æ•ˆç‡ä¸Šè¡¨ç°æ›´ä½³ã€‚é€šè¿‡å»é™¤å¯¹æœ¬ä½“çŠ¶æ€è¾“å…¥çš„ä¾èµ–ï¼Œé¿å…äº†è¿‡æ‹Ÿåˆè®­ç»ƒè½¨è¿¹çš„é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ— çŠ¶æ€ç­–ç•¥åœ¨å¤šç§å®é™…ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æˆåŠŸç‡ï¼Œè¯æ˜äº†å…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18154",
            "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe",
            "url": "https://huggingface.co/papers/2509.18154",
            "abstract": "MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) are undergoing rapid progress and represent the frontier of AI development. However, their training and inference efficiency have emerged as a core bottleneck in making MLLMs more accessible and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B parameter model designed for high efficiency and strong performance. We introduce three core improvements in model architecture, data strategy and training method: a unified 3D-Resampler model architecture for highly compact encoding over images and videos, a unified learning paradigm for document knowledge and text recognition without heavy data engineering, and a hybrid reinforcement learning strategy for proficiency in both short and long reasoning modes. Comprehensive experimental results in OpenCompass evaluation show that MiniCPM-V 4.5 surpasses widely used proprietary models such as GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL 72B. Notably, the strong performance is achieved with remarkable efficiency. For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves state-of-the-art performance among models under 30B size, using just 46.7\\% GPU memory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
            "score": 29,
            "issue_id": 6052,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "e46263baf17f8869",
            "authors": [
                "Tianyu Yu",
                "Zefan Wang",
                "Chongyi Wang",
                "Fuwei Huang",
                "Wenshuo Ma",
                "Zhihui He",
                "Tianchi Cai",
                "Weize Chen",
                "Yuxiang Huang",
                "Yuanqian Zhao",
                "Bokai Xu",
                "Junbo Cui",
                "Yingjing Xu",
                "Liqing Ruan",
                "Luoyuan Zhang",
                "Hanyu Liu",
                "Jingkun Tang",
                "Hongyuan Liu",
                "Qining Guo",
                "Wenhao Hu",
                "Bingxiang He",
                "Jie Zhou",
                "Jie Cai",
                "Ji Qi",
                "Zonghao Guo",
                "Chi Chen",
                "Guoyang Zeng",
                "Yuxuan Li",
                "Ganqu Cui",
                "Ning Ding",
                "Xu Han",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "MiniCPM-V Team, OpenBMB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18154.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#rl",
                    "#agi",
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "MiniCPM-V 4.5: ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "MiniCPM-V 4.5 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ 3D-Resampler, ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. MiniCPM-V 4.5 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4 Ğ¸ Qwen2.5-VL 72B, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VideoMME ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficiency Meets Performance in Multimodal AI",
                    "desc": "MiniCPM-V 4.5 is an advanced multimodal large language model with 8 billion parameters, designed to enhance both performance and efficiency. It utilizes a novel 3D-Resampler architecture that allows for compact encoding of images and videos, streamlining the processing of multimodal data. The model also incorporates a unified learning paradigm that simplifies document knowledge and text recognition, reducing the need for extensive data engineering. Additionally, a hybrid reinforcement learning strategy enables the model to excel in both short and long reasoning tasks, achieving superior results while using significantly less computational resources compared to larger models."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æœªæ¥",
                    "desc": "MiniCPM-V 4.5 æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 80 äº¿å‚æ•°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„ 3D-é‡é‡‡æ ·æ¶æ„ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€å­¦ä¹ èŒƒå¼å’Œæ··åˆå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„æ•ˆç‡ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMiniCPM-V 4.5 åœ¨ OpenCompass è¯„ä¼°ä¸­è¶…è¶Šäº†è®¸å¤šçŸ¥åæ¨¡å‹ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚å°¤å…¶æ˜¯åœ¨ VideoMME åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨ 30B ä»¥ä¸‹çš„æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½äº† GPU å†…å­˜å’Œæ¨ç†æ—¶é—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18849",
            "title": "MAPO: Mixed Advantage Policy Optimization",
            "url": "https://huggingface.co/papers/2509.18849",
            "abstract": "Mixed Advantage Policy Optimization (MAPO) dynamically reweights the advantage function to improve trajectory ranking in reinforcement learning for foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
            "score": 16,
            "issue_id": 6053,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "9416ec88d3b85956",
            "authors": [
                "Wenke Huang",
                "Quan Zhang",
                "Yiyang Fang",
                "Jian Liang",
                "Xuankun Rong",
                "Huanjin Yao",
                "Guancheng Wan",
                "Ke Liang",
                "Wenwen He",
                "Mingjun Li",
                "Leszek Rutkowski",
                "Mang Ye",
                "Bo Du",
                "Dacheng Tao"
            ],
            "affiliations": [
                "ByteDance",
                "Nanyang Technological University",
                "National University of Defense Technology",
                "The AGH University of Krakow",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18849.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "MAPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. MAPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€ĞµĞ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Dynamic Advantage Reweighting for Enhanced Trajectory Ranking",
                    "desc": "Mixed Advantage Policy Optimization (MAPO) enhances reinforcement learning by dynamically adjusting the advantage function to better rank trajectories in foundation models. It addresses issues like advantage reversion and advantage mirror problems that affect how advantages are distributed among different query samples. By introducing the concept of advantage percent deviation, MAPO focuses on samples with high-certainty trajectories, allowing for a more tailored advantage allocation. The effectiveness of MAPO is demonstrated through comparisons with existing methods and detailed ablation studies."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´ä¼˜åŠ¿ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆæœ",
                    "desc": "æ··åˆä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ï¼ˆMAPOï¼‰æ˜¯ä¸€ç§åœ¨å¼ºåŒ–å­¦ä¹ ä¸­åŠ¨æ€è°ƒæ•´ä¼˜åŠ¿å‡½æ•°çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„åŸºç¡€æ¨¡å‹çš„è½¨è¿¹æ’åã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯ä¸­å­˜åœ¨çš„ä¼˜åŠ¿åè½¬å’Œä¼˜åŠ¿é•œåƒé—®é¢˜ï¼Œä»è€Œå®ç°æ›´åˆç†çš„ä¼˜åŠ¿åˆ†é…ã€‚MAPOé€šè¿‡å¼•å…¥é«˜ç¡®å®šæ€§è½¨è¿¹çš„ä¼˜åŠ¿ç™¾åˆ†æ¯”åå·®ï¼Œæ¥é€‚åº”ä¸åŒæ ·æœ¬çš„ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAPOåœ¨ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18824",
            "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal\n  Understanding and Generation",
            "url": "https://huggingface.co/papers/2509.18824",
            "abstract": "Hyper-Bagel accelerates multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving significant speedups while maintaining high-quality outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous.",
            "score": 16,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "6de809558bad8c90",
            "authors": [
                "Yanzuo Lu",
                "Xin Xia",
                "Manlin Zhang",
                "Huafeng Kuang",
                "Jianbin Zheng",
                "Yuxi Ren",
                "Xuefeng Xiao"
            ],
            "affiliations": [
                "ByteDance AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18824.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Hyper-Bagel: Ğ¡Ğ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Hyper-Bagel - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Hyper-Bagel Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ: Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ 22 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ 1-NFE Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Hyper-Bagel: Speeding Up Multimodal Tasks with Smart Techniques",
                    "desc": "Hyper-Bagel is a new framework that speeds up tasks involving multiple types of data, like text and images, by using advanced techniques. It combines speculative decoding, which predicts the next piece of data quickly, with a multi-stage distillation process to reduce noise in the data. This approach allows for more than double the speed in understanding multimodal content and significantly faster generation of images from text. The framework also includes a model that can edit and generate content in real-time, making it efficient and responsive while maintaining high-quality results."
                },
                "zh": {
                    "title": "Hyper-Bagelï¼šåŠ é€Ÿå¤šæ¨¡æ€ä»»åŠ¡çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "Hyper-Bagel æ˜¯ä¸€ä¸ªåŠ é€Ÿå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ¡†æ¶ï¼Œé‡‡ç”¨äº†æ¨æµ‹è§£ç å’Œå¤šé˜¶æ®µè’¸é¦çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è¾“å‡ºã€‚è¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­å®ç°äº†è¶…è¿‡2å€çš„åŠ é€Ÿï¼Œè€Œåœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é€Ÿåº¦æå‡è¾¾åˆ°äº†16.67å€ï¼Œå›¾åƒç¼–è¾‘çš„é€Ÿåº¦æå‡è¾¾åˆ°äº†22å€ã€‚é€šè¿‡ç»“åˆå¯¹æŠ—è’¸é¦å’Œäººç±»åé¦ˆå­¦ä¹ ï¼ŒHyper-Bagel ä½¿å¾—å¤æ‚çš„å¤šæ¨¡æ€äº¤äº’å˜å¾—æ— ç¼ä¸”å³æ—¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19297",
            "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\n  Voxel-Aligned Prediction",
            "url": "https://huggingface.co/papers/2509.19297",
            "abstract": "VolSplat, a voxel-aligned Gaussian prediction method, improves novel view synthesis by overcoming pixel alignment limitations and enhancing 3D reconstruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
            "score": 13,
            "issue_id": 6053,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "708db1d702c58d65",
            "authors": [
                "Weijie Wang",
                "Yeqing Chen",
                "Zeyu Zhang",
                "Hengyu Liu",
                "Haoxiao Wang",
                "Zhiyuan Feng",
                "Wenkang Qin",
                "Zheng Zhu",
                "Donny Y. Chen",
                "Bohan Zhuang"
            ],
            "affiliations": [
                "GigaAI",
                "Monash University",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "University of Electronic Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19297.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#benchmark",
                    "#3d",
                    "#games"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "VolSplat: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²",
                    "desc": "VolSplat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾-Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾. ĞĞ½ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸. VolSplat Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ 3D-ÑÑ†ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with Voxel-Aligned Gaussians",
                    "desc": "VolSplat is a novel method for synthesizing new views in 3D reconstruction by using voxel-aligned Gaussian predictions instead of the traditional pixel-aligned approach. This new technique addresses limitations such as dependency on input views, view-biased density distributions, and alignment errors caused by occlusions or low texture in source views. By predicting Gaussians directly from a 3D voxel grid, VolSplat enhances multi-view consistency and adapts Gaussian density based on scene complexity. Experiments show that it outperforms existing methods, providing more accurate and consistent 3D reconstructions, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "ä½“ç´ å¯¹é½ï¼Œé‡å¡‘3Dé‡å»ºçš„æœªæ¥",
                    "desc": "VolSplatæ˜¯ä¸€ç§åŸºäºä½“ç´ å¯¹é½çš„é«˜æ–¯é¢„æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„æ–°è§†è§’åˆæˆï¼Œå…‹æœåƒç´ å¯¹é½çš„å±€é™æ€§ï¼Œå¹¶æå‡3Dé‡å»ºè´¨é‡ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºåƒç´ å¯¹é½çš„é«˜æ–¯é¢„æµ‹ï¼Œè¿™å¯¼è‡´é‡å»ºçš„3Dæ¨¡å‹å¯¹è¾“å…¥è§†å›¾æ•°é‡é«˜åº¦ä¾èµ–ï¼Œå¹¶ä¸”åœ¨è§†å›¾åå·®å’Œé®æŒ¡æƒ…å†µä¸‹å®¹æ˜“å‡ºç°å¯¹é½é”™è¯¯ã€‚VolSplaté€šè¿‡ç›´æ¥ä»é¢„æµ‹çš„3Dä½“ç´ ç½‘æ ¼ä¸­é¢„æµ‹é«˜æ–¯ï¼Œé¿å…äº†å¯¹é”™è¯¯æ˜“æ„Ÿçš„2Dç‰¹å¾åŒ¹é…ï¼Œä»è€Œç¡®ä¿äº†å¤šè§†å›¾çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVolSplatåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†æ›´çœŸå®å’Œä¸€è‡´çš„é«˜æ–¯é‡å»ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19296",
            "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
            "url": "https://huggingface.co/papers/2509.19296",
            "abstract": "A self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.",
            "score": 9,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "763d3ecf06625fdf",
            "authors": [
                "Sherwin Bahmani",
                "Tianchang Shen",
                "Jiawei Ren",
                "Jiahui Huang",
                "Yifeng Jiang",
                "Haithem Turki",
                "Andrea Tagliasacchi",
                "David B. Lindell",
                "Zan Gojcic",
                "Sanja Fidler",
                "Huan Ling",
                "Jun Gao",
                "Xuanchi Ren"
            ],
            "affiliations": [
                "NVIDIA",
                "Simon Fraser University",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19296.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#robotics",
                    "#games",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "3D-Ğ¼Ğ¸Ñ€Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ 3D-Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² ÑĞ²Ğ½Ğ¾Ğµ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Gaussian Splatting. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ RGB-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¸ 3DGS-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Transforming 2D Imagination into 3D Reality",
                    "desc": "This paper introduces a self-distillation framework that transforms implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation. This method allows for the generation of 3D scenes from text or images without needing extensive multi-view training data. By enhancing the RGB decoder with a 3DGS decoder, the framework can be trained solely on synthetic data produced by video diffusion models. The results demonstrate superior performance in generating both static and dynamic 3D scenes, making it valuable for applications in gaming and robotics."
                },
                "zh": {
                    "title": "è‡ªè’¸é¦æ¡†æ¶ï¼šä»è§†é¢‘ç”Ÿæˆ3Dåœºæ™¯çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œå°†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„éšå¼3DçŸ¥è¯†è½¬åŒ–ä¸ºæ˜¾å¼çš„3Dé«˜æ–¯ç‚¹äº‘è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•ä½¿å¾—ä»æ–‡æœ¬æˆ–å›¾åƒç”Ÿæˆ3Dåœºæ™¯æˆä¸ºå¯èƒ½ï¼Œé¿å…äº†å¯¹å¤šè§†è§’è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚æˆ‘ä»¬é€šè¿‡å¢å¼ºå…¸å‹çš„RGBè§£ç å™¨ï¼ŒåŠ å…¥3Dé«˜æ–¯ç‚¹äº‘è§£ç å™¨ï¼Œå¹¶åˆ©ç”¨RGBè§£ç å™¨çš„è¾“å‡ºè¿›è¡Œç›‘ç£è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€3Dåœºæ™¯ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19284",
            "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and\n  Structure of CoT",
            "url": "https://huggingface.co/papers/2509.19284",
            "abstract": "Effective chain-of-thoughts in large reasoning models are characterized by fewer failed steps and better structural quality, not necessarily by length or review.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy.   As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
            "score": 9,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "4e8ddb8ed978283e",
            "authors": [
                "Yunzhen Feng",
                "Julia Kempe",
                "Cheng Zhang",
                "Parag Jain",
                "Anthony Hartshorn"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19284.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#rl",
                    "#reasoning",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ° Ğ½Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾: ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought, CoT) Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Large Reasoning Models, LRM) Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ½Ğµ Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ°, Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. Ğ’Ğ²ĞµĞ´ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ - Ğ´Ğ¾Ğ»Ñ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² (Failed-Step Fraction, FSF), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ğ° CoT. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CoT Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ CoT Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Less Failure, More Structure: The Key to Effective Reasoning in AI",
                    "desc": "This paper investigates what makes chain-of-thought (CoT) reasoning effective in large reasoning models (LRMs). It challenges the idea that longer CoTs are always better, showing that both longer CoTs and increased review can lead to lower accuracy. The authors introduce a new metric called the Failed-Step Fraction (FSF), which measures the proportion of steps in abandoned reasoning paths, and find it to be a better predictor of correctness than length or review. Their findings suggest that effective CoTs are characterized by fewer failures and emphasize the importance of structure in reasoning processes."
                },
                "zh": {
                    "title": "æœ‰æ•ˆæ€ç»´é“¾ï¼šå‡å°‘å¤±è´¥æ­¥éª¤ï¼Œæå‡æ¨ç†è´¨é‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä¸­æœ‰æ•ˆçš„æ€ç»´é“¾ï¼ˆCoTï¼‰çš„ç‰¹å¾ã€‚ç ”ç©¶å‘ç°ï¼Œæ€ç»´é“¾çš„æœ‰æ•ˆæ€§ä¸å¤±è´¥æ­¥éª¤çš„æ•°é‡å’Œç»“æ„è´¨é‡æœ‰å…³ï¼Œè€Œä¸æ˜¯ç®€å•çš„é•¿åº¦æˆ–å¤å®¡æ¬¡æ•°ã€‚é€šè¿‡å¯¹åä¸ªLRMsè¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œç®€å•åœ°å»¶é•¿æ€ç»´é“¾æˆ–å¢åŠ å¤å®¡ä¼šå¯¼è‡´å‡†ç¡®ç‡é™ä½ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾å½¢è§†è§’æ¥æå–æ€ç»´é“¾çš„ç»“æ„ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç»Ÿè®¡é‡â€”â€”å¤±è´¥æ­¥éª¤æ¯”ä¾‹ï¼ˆFSFï¼‰ï¼Œè¯¥æ¯”ä¾‹èƒ½å¤Ÿæ›´å¥½åœ°é¢„æµ‹æ¨¡å‹çš„æ­£ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13835",
            "title": "Large Language Models Discriminate Against Speakers of German Dialects",
            "url": "https://huggingface.co/papers/2509.13835",
            "abstract": "Large language models exhibit significant dialect naming and usage bias against German dialect speakers, which is amplified when linguistic demographics are explicitly labeled.  \t\t\t\t\tAI-generated summary \t\t\t\t Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: an association task and a decision task. To assess a model's dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics--German dialect speakers--amplifies bias more than implicit cues like dialect usage.",
            "score": 5,
            "issue_id": 6057,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "b9343322b02e9713",
            "authors": [
                "Minh Duc Bui",
                "Carolin Holtermann",
                "Valentin Hofmann",
                "Anne Lauscher",
                "Katharina von der Wense"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Data Science Group, University of Hamburg, Germany",
                "Johannes Gutenberg University Mainz, Germany",
                "University of Colorado Boulder, USA",
                "University of Washington, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13835.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğº Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑĞ¼ Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¸Ñ… Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ² Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¸Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ½Ğ¾Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½ĞµĞµ, Ñ‡ĞµĞ¼ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM."
                },
                "en": {
                    "title": "Unmasking Bias: Language Models and German Dialects",
                    "desc": "This paper investigates the biases present in large language models (LLMs) against speakers of German dialects. It highlights that these models not only associate negative traits with dialect speakers but also exhibit biased decision-making when processing dialect-related content. The research utilizes a novel evaluation corpus to measure dialect naming and usage biases through specific tasks. The findings reveal that explicitly labeling dialect speakers increases bias, contradicting previous studies that suggested minimal bias with demographic mentions."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¾·è¯­æ–¹è¨€ä½¿ç”¨è€…çš„åè§ç ”ç©¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹å¾·è¯­æ–¹è¨€ä½¿ç”¨è€…çš„åè§ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨æ–¹è¨€å‘½åå’Œä½¿ç”¨ä¸Šå­˜åœ¨æ˜¾è‘—çš„åè§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ–¹è¨€ä½¿ç”¨è€…å¸¸å¸¸é¢ä¸´è´Ÿé¢çš„ç¤¾ä¼šåˆ»æ¿å°è±¡ï¼Œè€Œè¿™äº›åˆ»æ¿å°è±¡åœ¨LLMsä¸­å¾—åˆ°äº†åæ˜ ã€‚é€šè¿‡å…³è”ä»»åŠ¡å’Œå†³ç­–ä»»åŠ¡ï¼Œä½œè€…è¯„ä¼°äº†æ¨¡å‹çš„æ–¹è¨€å‘½ååè§å’Œä½¿ç”¨åè§ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°è¯­æ–™åº“ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ˜ç¡®æ ‡è®°è¯­è¨€äººå£ç»Ÿè®¡ä¿¡æ¯ä¼šåŠ å‰§åè§ï¼Œåæ˜ å‡ºå¯¹å¾·è¯­æ–¹è¨€ä½¿ç”¨è€…çš„è´Ÿé¢è”æƒ³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17321",
            "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation",
            "url": "https://huggingface.co/papers/2509.17321",
            "abstract": "OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.  \t\t\t\t\tAI-generated summary \t\t\t\t Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately 70% of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at github.com/budzianowski/opengvl{OpenGVL}.",
            "score": 3,
            "issue_id": 6056,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "3469469f2dba37f5",
            "authors": [
                "PaweÅ‚ Budzianowski",
                "Emilia WiÅ›nios",
                "Gracjan GÃ³ral",
                "Igor Kulakov",
                "Viktor Petrenko",
                "Krzysztof Walas"
            ],
            "affiliations": [
                "IDEAS Research Institute",
                "Poznan University of Technology",
                "Simple Automation",
                "University of Warsaw"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17321.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#data",
                    "#robotics",
                    "#dataset",
                    "#science"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "OpenGVL: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ",
                    "desc": "OpenGVL - ÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼. OpenGVL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ°Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸."
                },
                "en": {
                    "title": "OpenGVL: Bridging the Gap in Robotics Task Prediction",
                    "desc": "OpenGVL is a benchmark designed to improve task progress prediction in robotics by utilizing vision-language models (VLMs). It highlights the performance gap between open-source and closed-source models, with open-source models achieving only about 70% of the effectiveness of their closed-source counterparts. The benchmark aims to facilitate automated data curation, allowing for better annotation and quality assessment of large robotics datasets. By leveraging the Generative Value Learning (GVL) approach, OpenGVL provides a structured way to evaluate and enhance the capabilities of models in predicting task completion from visual inputs."
                },
                "zh": {
                    "title": "OpenGVLï¼šæå‡æœºå™¨äººä»»åŠ¡é¢„æµ‹çš„åŸºå‡†",
                    "desc": "OpenGVLæ˜¯ä¸€ä¸ªç”¨äºæœºå™¨äººä»»åŠ¡è¿›åº¦é¢„æµ‹çš„åŸºå‡†ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œç ”ç©¶ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œå¼€æºæ¨¡å‹åœ¨ä»»åŠ¡è¿›åº¦é¢„æµ‹æ–¹é¢çš„è¡¨ç°æ˜æ˜¾ä½äºé—­æºæ¨¡å‹ï¼Œä»…è¾¾åˆ°åè€…æ€§èƒ½çš„70%å·¦å³ã€‚OpenGVLä¸ä»…æä¾›äº†å¤šæ ·åŒ–çš„æŒ‘æˆ˜æ€§æ“ä½œä»»åŠ¡çš„è¯„ä¼°ï¼Œè¿˜èƒ½ä½œä¸ºè‡ªåŠ¨åŒ–æ•°æ®æ•´ç†å’Œè¿‡æ»¤çš„å®ç”¨å·¥å…·ï¼Œå¸®åŠ©é«˜æ•ˆè¯„ä¼°å¤§è§„æ¨¡æœºå™¨äººæ•°æ®é›†çš„è´¨é‡ã€‚é€šè¿‡åˆ©ç”¨ä¸æ–­å¢é•¿çš„æœºå™¨äººæ•°æ®ï¼ŒOpenGVLä¸ºæœºå™¨äººé¢†åŸŸçš„è¿›æ­¥æä¾›äº†æ–°çš„æœºä¼šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17083",
            "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\n  View Synthesis",
            "url": "https://huggingface.co/papers/2509.17083",
            "abstract": "Hybrid Radiance Fields combine explicit Gaussians and neural fields to achieve high-quality rendering with reduced memory usage and real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.",
            "score": 3,
            "issue_id": 6053,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 21",
                "zh": "9æœˆ21æ—¥"
            },
            "hash": "4a06acbb1d75ae4a",
            "authors": [
                "Zipeng Wang",
                "Dan Xu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17083.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸŒˆ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² 3D Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ",
                    "desc": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ (HyRF) Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ ÑĞ²Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ñƒ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ²Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ², Ñ…Ñ€Ğ°Ğ½ÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ¸ ÑĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. HyRF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ°, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HyRF Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 20 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ 3DGS, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Rendering with Hybrid Radiance Fields",
                    "desc": "Hybrid Radiance Fields (HyRF) introduce a new way to represent 3D scenes by merging explicit Gaussians with neural fields. This approach allows for high-quality rendering while significantly reducing memory usage and enabling real-time performance. HyRF uses a compact set of explicit Gaussians to capture essential high-frequency details and employs grid-based neural fields for other properties. The innovative architecture separates geometry and color modeling, leading to improved scene representation and rendering quality compared to previous methods."
                },
                "zh": {
                    "title": "æ··åˆè¾å°„åœºï¼šé«˜æ•ˆæ¸²æŸ“çš„æ–°æ–¹æ³•",
                    "desc": "æ··åˆè¾å°„åœºï¼ˆHyRFï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œç»“åˆäº†æ˜¾å¼é«˜æ–¯å’Œç¥ç»åœºçš„ä¼˜ç‚¹ï¼Œä»¥å®ç°é«˜è´¨é‡æ¸²æŸ“ï¼ŒåŒæ—¶å‡å°‘å†…å­˜ä½¿ç”¨å¹¶ä¿æŒå®æ—¶æ€§èƒ½ã€‚HyRFå°†åœºæ™¯åˆ†è§£ä¸ºä¸€ç»„ç´§å‡‘çš„æ˜¾å¼é«˜æ–¯ï¼Œä»…å­˜å‚¨å…³é”®çš„é«˜é¢‘å‚æ•°ï¼Œä»¥åŠåŸºäºç½‘æ ¼çš„ç¥ç»åœºï¼Œç”¨äºé¢„æµ‹å…¶ä½™å±æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§£è€¦çš„ç¥ç»åœºæ¶æ„ï¼Œåˆ†åˆ«å»ºæ¨¡å‡ ä½•å½¢çŠ¶ï¼ˆå°ºåº¦ã€ä¸é€æ˜åº¦ã€æ—‹è½¬ï¼‰å’Œè§†è§’ä¾èµ–çš„é¢œè‰²ã€‚å®éªŒè¡¨æ˜ï¼ŒHyRFåœ¨æ¸²æŸ“è´¨é‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒåŒæ—¶æ¨¡å‹å¤§å°æ¯”3Dé«˜æ–¯ç‚¹äº‘å‡å°‘äº†20å€ä»¥ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19300",
            "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target\n  for Better Flow Matching",
            "url": "https://huggingface.co/papers/2509.19300",
            "abstract": "Condition-Aware Reparameterization for Flow Matching (CAR-Flow) enhances conditional generative modeling by repositioning distributions, leading to faster training and improved performance on image data.  \t\t\t\t\tAI-generated summary \t\t\t\t Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport and conditional injection. To ease the demand on the model, we propose Condition-Aware Reparameterization for Flow Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6% additional parameters.",
            "score": 2,
            "issue_id": 6055,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "5053f16cc1621faa",
            "authors": [
                "Chen Chen",
                "Pengsheng Guo",
                "Liangchen Song",
                "Jiasen Lu",
                "Rui Qian",
                "Xinze Wang",
                "Tsu-Jui Fu",
                "Wei Liu",
                "Yinfei Yang",
                "Alex Schwing"
            ],
            "affiliations": [
                "Apple Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19300.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#training",
                    "#synthetic",
                    "#diffusion"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "CAR-Flow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. CAR-Flow ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ. ĞĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet-256 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ CAR-Flow Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SiT-XL/2 ÑĞ½Ğ¸Ğ·Ğ¸Ğ»Ğ¾ FID Ñ 2.07 Ğ´Ğ¾ 1.68, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ² Ğ¼ĞµĞ½ĞµĞµ 0.6% Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Streamlining Conditional Generative Modeling with CAR-Flow",
                    "desc": "The paper introduces Condition-Aware Reparameterization for Flow Matching (CAR-Flow), which improves conditional generative modeling by adjusting the positioning of data distributions. This method allows for more efficient training by reducing the complexity of the probability paths that the model needs to learn. By conditioning either the source, the target, or both distributions, CAR-Flow enhances the performance of flow-based models on image data. The results show significant improvements in image quality metrics, such as a reduction in FID scores, with minimal increase in model parameters."
                },
                "zh": {
                    "title": "æ¡ä»¶æ„ŸçŸ¥é‡å‚æ•°åŒ–ï¼Œæå‡ç”Ÿæˆæ¨¡å‹æ•ˆç‡",
                    "desc": "æ¡ä»¶ç”Ÿæˆå»ºæ¨¡æ—¨åœ¨ä»åŒ…å«æ•°æ®-æ¡ä»¶å¯¹çš„æ ·æœ¬ä¸­å­¦ä¹ æ¡ä»¶æ•°æ®åˆ†å¸ƒã€‚ä¸ºæ­¤ï¼Œæ‰©æ•£å’ŒåŸºäºæµçš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚è¿™äº›æ–¹æ³•ä½¿ç”¨å­¦ä¹ åˆ°çš„æµæ¨¡å‹å°†åˆå§‹çš„æ ‡å‡†é«˜æ–¯å™ªå£°ä¼ è¾“åˆ°æ¡ä»¶æ•°æ®åˆ†å¸ƒã€‚æˆ‘ä»¬æå‡ºçš„æ¡ä»¶æ„ŸçŸ¥é‡å‚æ•°åŒ–ï¼ˆCAR-Flowï¼‰é€šè¿‡é‡æ–°å®šä½åˆ†å¸ƒï¼Œç®€åŒ–äº†æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€ŒåŠ å¿«äº†è®­ç»ƒé€Ÿåº¦å¹¶æé«˜äº†åœ¨å›¾åƒæ•°æ®ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19087",
            "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal\n  Gemini 2.5 Model for Remote Sensing Applications",
            "url": "https://huggingface.co/papers/2509.19087",
            "abstract": "A training-free method enables generalist multimodal models to process multi-spectral imagery in a zero-shot manner, enhancing performance on remote sensing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing a lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals.   To address this, we propose a training-free approach which introduces new multi-spectral data in a Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models' understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data.",
            "score": 1,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "fbe226390b6ea231",
            "authors": [
                "Ganesh Mallya",
                "Yotam Gigi",
                "Dahun Kim",
                "Maxim Neumann",
                "Genady Beryozkin",
                "Tomer Shekel",
                "Anelia Angelova"
            ],
            "affiliations": [
                "Google DeepMind",
                "Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19087.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot, Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Gemini2.5 Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·ĞµĞ¼Ğ»ĞµĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Multimodal Models for Multi-Spectral Imagery Without Training",
                    "desc": "This paper presents a novel training-free method that allows generalist multimodal models to process multi-spectral imagery without prior training, enhancing their performance in remote sensing tasks. Multi-spectral images, which contain additional spectral bands, are crucial for accurately identifying physical materials on the ground. The proposed approach enables these models, typically trained only on RGB images, to adapt to multi-spectral data in a zero-shot manner by injecting domain-specific instructions. The results demonstrate significant performance improvements on remote sensing benchmarks, showcasing the potential for geospatial professionals to utilize advanced multimodal models effectively."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œè½»æ¾å¤„ç†å¤šå…‰è°±å›¾åƒ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œä½¿é€šç”¨å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿä»¥é›¶æ ·æœ¬çš„æ–¹å¼å¤„ç†å¤šå…‰è°±å›¾åƒï¼Œä»è€Œæé«˜é¥æ„Ÿä»»åŠ¡çš„æ€§èƒ½ã€‚å¤šå…‰è°±å›¾åƒåœ¨åœŸåœ°åˆ©ç”¨åˆ†ç±»ã€ç¯å¢ƒç›‘æµ‹å’ŒåŸå¸‚è§„åˆ’ç­‰é¥æ„Ÿåº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™äº›å›¾åƒéœ€è¦ä¸“é—¨è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè‡ªåŠ¨åˆ†æï¼Œä½†è¿™ç§æ–¹æ³•æˆæœ¬é«˜ä¸”ä¸å¤Ÿçµæ´»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é€šç”¨å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿè½»æ¾é€‚åº”æ–°çš„å¤šå…‰è°±è¾“å…¥ï¼Œå±•ç¤ºäº†åœ¨é¥æ„ŸåŸºå‡†æµ‹è¯•ä¸­çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19002",
            "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via\n  Travel Video Itinerary Reconstruction",
            "url": "https://huggingface.co/papers/2509.19002",
            "abstract": "VIR-Bench, a new benchmark for travel videos, evaluates and enhances MLLMs' geospatial-temporal intelligence, improving itinerary recommendations in real-world applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.",
            "score": 1,
            "issue_id": 6057,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "110499782ca8d1ee",
            "authors": [
                "Hao Wang",
                "Eiki Murata",
                "Lingfang Zhang",
                "Ayako Sato",
                "So Fukuda",
                "Ziqi Yin",
                "Wentao Hu",
                "Keisuke Nakao",
                "Yusuke Nakamura",
                "Sebastian Zwirner",
                "Yi-Chia Chen",
                "Hiroyuki Otomo",
                "Hiroki Ouchi",
                "Daisuke Kawahara"
            ],
            "affiliations": [
                "AI Shift, Inc.",
                "CyberAgent, Inc.",
                "Nara Institute of Science and Technology",
                "Waseda University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19002.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#science",
                    "#games",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "VIR-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "VIR-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸ÑÑ…. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 200 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ insights Ğ¸Ğ· VIR-Bench Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ°Ğ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Travel Itinerary Recommendations with VIR-Bench",
                    "desc": "VIR-Bench is a new benchmark designed to assess and improve the geospatial-temporal intelligence of multimodal large language models (MLLMs) using travel videos. It consists of 200 videos that focus on long-distance travel, a topic that has been largely overlooked in existing benchmarks. The study shows that even advanced MLLMs struggle with the complexities of itinerary reconstruction from these videos. By developing a travel-planning agent based on insights from VIR-Bench, the research demonstrates significant improvements in itinerary recommendations, highlighting the practical benefits of this evaluation framework."
                },
                "zh": {
                    "title": "æå‡æ—…è¡Œè§†é¢‘ç†è§£çš„åŸºå‡†æŒ‘æˆ˜",
                    "desc": "VIR-Benchæ˜¯ä¸€ä¸ªæ–°çš„æ—…è¡Œè§†é¢‘åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åœ°ç†æ—¶ç©ºæ™ºèƒ½ï¼Œä»è€Œæ”¹å–„ç°å®åº”ç”¨ä¸­çš„è¡Œç¨‹æ¨èã€‚å½“å‰çš„è§†é¢‘åŸºå‡†ä¸»è¦é›†ä¸­åœ¨å®¤å†…åœºæ™¯æˆ–çŸ­é€”æˆ·å¤–æ´»åŠ¨ä¸Šï¼Œé•¿é€”æ—…è¡Œçš„æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æŒæ¡æ‰©å±•çš„åœ°ç†æ—¶ç©ºè½¨è¿¹å¯¹äºä¸‹ä¸€ä»£MLLMsè‡³å…³é‡è¦ï¼Œè¿™æ”¯æŒäº†è¯¸å¦‚å…·èº«äººå·¥æ™ºèƒ½è§„åˆ’å’Œå¯¼èˆªç­‰ç°å®ä»»åŠ¡ã€‚é€šè¿‡VIR-Benchï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªåŒ…å«200ä¸ªæ—…è¡Œè§†é¢‘çš„åŸºå‡†ï¼Œå°†è¡Œç¨‹é‡å»ºä½œä¸ºä¸€é¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œä»¥è¯„ä¼°å’Œæ¨åŠ¨MLLMsçš„åœ°ç†æ—¶ç©ºæ™ºèƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18090",
            "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface\n  Reconstruction",
            "url": "https://huggingface.co/papers/2509.18090",
            "abstract": "GeoSVR, a voxel-based framework, improves surface reconstruction accuracy and detail using sparse voxels with depth constraints and surface regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR.",
            "score": 1,
            "issue_id": 6063,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "d2fdd6191d2fb77a",
            "authors": [
                "Jiahe Li",
                "Jiawei Zhang",
                "Youmin Zhang",
                "Xiao Bai",
                "Jin Zheng",
                "Xiaohan Yu",
                "Lin Gu"
            ],
            "affiliations": [
                "Macquarie University",
                "RIKEN AIP",
                "Rawmantic AI",
                "School of Computer Science and Engineering, State Key Laboratory of Complex Critical Software Environment, Jiangxi Research Institute, Beihang University",
                "State Key Laboratory of Virtual Reality Technology and Systems, Beijing",
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18090.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ĞºÑĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ GeoSVR - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Gaussian Splatting. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğµ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Surface Reconstruction with Sparse Voxel Precision",
                    "desc": "GeoSVR is a novel voxel-based framework designed to enhance the accuracy and detail of surface reconstruction using sparse voxels. It addresses limitations found in traditional methods, particularly those relying on Gaussian Splatting, by introducing a Voxel-Uncertainty Depth Constraint that leverages monocular depth cues while managing voxel uncertainty. Additionally, Sparse Voxel Surface Regularization is implemented to improve geometric consistency and support the creation of sharp surfaces. The framework demonstrates superior performance in various challenging scenarios, achieving high geometric accuracy and detail preservation efficiently."
                },
                "zh": {
                    "title": "GeoSVRï¼šæå‡è¡¨é¢é‡å»ºçš„å‡†ç¡®æ€§ä¸ç»†èŠ‚",
                    "desc": "GeoSVRæ˜¯ä¸€ç§åŸºäºä½“ç´ çš„æ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–ä½“ç´ å’Œæ·±åº¦çº¦æŸæ¥æé«˜è¡¨é¢é‡å»ºçš„å‡†ç¡®æ€§å’Œç»†èŠ‚ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿé«˜æ–¯ç‚¹äº‘æ–¹æ³•çš„å±€é™æ€§ï¼Œåˆ©ç”¨ç¨€ç–ä½“ç´ çš„ä¼˜åŠ¿æ¥å®ç°æ›´å®Œæ•´å’Œæ¸…æ™°çš„å‡ ä½•é‡å»ºã€‚æˆ‘ä»¬æå‡ºäº†ä½“ç´ ä¸ç¡®å®šæ€§æ·±åº¦çº¦æŸï¼Œä»¥æœ€å¤§åŒ–å•ç›®æ·±åº¦çº¿ç´¢çš„æ•ˆæœï¼ŒåŒæ—¶é¿å…è´¨é‡ä¸‹é™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoSVRåœ¨å‡ ä½•å‡†ç¡®æ€§ã€ç»†èŠ‚ä¿ç•™å’Œé‡å»ºå®Œæ•´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17349",
            "title": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous\n  Speech-to-Text Translation",
            "url": "https://huggingface.co/papers/2509.17349",
            "abstract": "The paper analyzes SimulST latency metrics, identifies segmentation bias, and introduces YAAL and LongYAAL for more accurate latency evaluation, along with SoftSegmenter for improved alignment quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency--the delay between speech input and the translated output. While quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented. In this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes. We uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more accurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose SoftSegmenter, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while SoftSegmenter enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.",
            "score": 1,
            "issue_id": 6064,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "c98b194f176b93cf",
            "authors": [
                "Peter PolÃ¡k",
                "Sara Papi",
                "Luisa Bentivogli",
                "OndÅ™ej Bojar"
            ],
            "affiliations": [
                "Charles University, Czech Republic",
                "Fondazione Bruno Kessler, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17349.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#data",
                    "#machine_translation",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "â±ï¸",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚ (SimulST), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½ÑƒÑ Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµÑ‡Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ YAAL Ğ¸ LongYAAL, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ SoftSegmenter Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ² Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑÑÑ…."
                },
                "en": {
                    "title": "Enhancing Latency Evaluation in SimulST Systems",
                    "desc": "This paper focuses on improving the evaluation of latency in simultaneous speech-to-text translation (SimulST) systems. It identifies a segmentation bias in existing latency metrics that can lead to inaccurate comparisons between different systems and languages. To address this issue, the authors introduce YAAL and LongYAAL, new metrics that provide more reliable latency measurements, especially in short-form and unsegmented audio contexts. Additionally, they present SoftSegmenter, a tool that enhances alignment quality, ultimately leading to better assessments of translation performance."
                },
                "zh": {
                    "title": "æå‡SimulSTç³»ç»Ÿè¯„ä¼°çš„å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡åˆ†æäº†åŒæ—¶è¯­éŸ³ç¿»è¯‘ç³»ç»Ÿï¼ˆSimulSTï¼‰çš„å»¶è¿ŸæŒ‡æ ‡ï¼Œè¯†åˆ«äº†åˆ†æ®µåå·®ï¼Œå¹¶å¼•å…¥äº†YAALå’ŒLongYAALä»¥å®ç°æ›´å‡†ç¡®çš„å»¶è¿Ÿè¯„ä¼°ï¼ŒåŒæ—¶æå‡ºäº†SoftSegmenterä»¥æé«˜å¯¹é½è´¨é‡ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„å»¶è¿Ÿæµ‹é‡æŒ‡æ ‡åœ¨çŸ­æ ¼å¼è®¾ç½®ä¸­å¸¸å¸¸äº§ç”Ÿä¸ä¸€è‡´æˆ–è¯¯å¯¼æ€§çš„ç»“æœï¼Œå½±å“äº†å…¬å¹³çš„æ¯”è¾ƒã€‚YAALï¼ˆYet Another Average Laggingï¼‰æ˜¯ä¸€ç§æ”¹è¿›çš„å»¶è¿ŸæŒ‡æ ‡ï¼Œèƒ½å¤Ÿåœ¨çŸ­æ ¼å¼ä¸­æä¾›æ›´å‡†ç¡®çš„è¯„ä¼°ï¼Œè€ŒLongYAALåˆ™é€‚ç”¨äºæœªåˆ†æ®µçš„éŸ³é¢‘ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†YAALå’ŒLongYAALåœ¨å»¶è¿Ÿè¯„ä¼°ä¸­ä¼˜äºæµè¡Œçš„æŒ‡æ ‡ï¼ŒåŒæ—¶SoftSegmenteræå‡äº†é•¿æ ¼å¼è¯„ä¼°ä¸­çš„å¯¹é½è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19274",
            "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language\n  Models' Understanding on Indian Culture",
            "url": "https://huggingface.co/papers/2509.19274",
            "abstract": "DRISHTIKON is a multimodal and multilingual benchmark for evaluating generative AI systems' cultural understanding across India's diverse regions and languages.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual benchmark centered exclusively on Indian culture, designed to evaluate the cultural understanding of generative AI systems. Unlike existing benchmarks with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage across India's diverse regions, spanning 15 languages, covering all states and union territories, and incorporating over 64,000 aligned text-image pairs. The dataset captures rich cultural themes including festivals, attire, cuisines, art forms, and historical heritage amongst many more. We evaluate a wide range of vision-language models (VLMs), including open-source small and large models, proprietary systems, reasoning-specialized VLMs, and Indic-focused models, across zero-shot and chain-of-thought settings. Our results expose key limitations in current models' ability to reason over culturally grounded, multimodal inputs, particularly for low-resource languages and less-documented traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a robust testbed to advance culturally aware, multimodally competent language technologies.",
            "score": 0,
            "issue_id": 6063,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "ffb921e6e33421e6",
            "authors": [
                "Arijit Maji",
                "Raghvendra Kumar",
                "Akash Ghosh",
                "Anushka",
                "Nemil Shah",
                "Abhilekh Borah",
                "Vanshika Shah",
                "Nishant Mishra",
                "Sriparna Saha"
            ],
            "affiliations": [
                "Banasthali Vidyapeeth University, Rajasthan, India",
                "Dwarkadas J. Sanghvi College of Engineering, India",
                "Indian Institute of Technology Patna, India",
                "Manipal University Jaipur, India",
                "Pandit Deendayal Energy University, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19274.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#low_resource",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#multilingual",
                    "#games"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ AI Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ˜Ğ½Ğ´Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ DRISHTIKON - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¾Ğ¹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ AI ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 15 ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ²ÑĞµ ÑˆÑ‚Ğ°Ñ‚Ñ‹ Ğ˜Ğ½Ğ´Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 64,000 Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞ¼Ğ°Ğ¼Ğ¸: Ñ„ĞµÑÑ‚Ğ¸Ğ²Ğ°Ğ»ÑĞ¼Ğ¸, Ğ¾Ğ´ĞµĞ¶Ğ´Ğ¾Ğ¹, ĞºÑƒÑ…Ğ½ĞµĞ¹ Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾Ğ¼. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². DRISHTIKON Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ñ‹Ñ… AI Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Evaluating AI's Cultural Understanding with DRISHTIKON",
                    "desc": "DRISHTIKON is a unique benchmark designed to assess how well generative AI systems understand Indian culture through multiple languages and types of data. It includes a vast collection of over 64,000 text-image pairs that represent various cultural aspects from all regions of India, covering 15 languages. The benchmark evaluates different vision-language models (VLMs) to identify their strengths and weaknesses in processing culturally rich and multimodal information. This initiative aims to improve AI's cultural awareness, especially for underrepresented languages and traditions, by providing a comprehensive testing framework."
                },
                "zh": {
                    "title": "DRISHTIKONï¼šè¯„ä¼°ç”Ÿæˆæ€§AIçš„æ–‡åŒ–ç†è§£åŠ›",
                    "desc": "DRISHTIKONæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å’Œå¤šè¯­è¨€çš„åŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç³»ç»Ÿå¯¹å°åº¦æ–‡åŒ–çš„ç†è§£ã€‚å®ƒæ¶µç›–äº†å°åº¦15ç§è¯­è¨€å’Œ64,000å¤šä¸ªæ–‡æœ¬-å›¾åƒå¯¹ï¼Œæ·±å…¥åæ˜ äº†å„åœ°åŒºçš„æ–‡åŒ–ä¸»é¢˜ï¼Œå¦‚èŠ‚æ—¥ã€æœé¥°ã€ç¾é£Ÿå’Œè‰ºæœ¯å½¢å¼ç­‰ã€‚é€šè¿‡è¯„ä¼°å¤šç§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†æ–‡åŒ–ç›¸å…³çš„å¤šæ¨¡æ€è¾“å…¥æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€å’Œè¾ƒå°‘æ–‡çŒ®çš„ä¼ ç»Ÿæ–¹é¢ã€‚DRISHTIKONä¸ºåŒ…å®¹æ€§äººå·¥æ™ºèƒ½ç ”ç©¶å¡«è¡¥äº†é‡è¦ç©ºç™½ï¼Œæä¾›äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„æµ‹è¯•å¹³å°ï¼Œä»¥æ¨åŠ¨æ–‡åŒ–æ„è¯†å’Œå¤šæ¨¡æ€èƒ½åŠ›çš„è¯­è¨€æŠ€æœ¯å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16506",
            "title": "CommonForms: A Large, Diverse Dataset for Form Field Detection",
            "url": "https://huggingface.co/papers/2509.16506",
            "abstract": "A web-scale dataset and models for form field detection are introduced, achieving high precision and supporting diverse languages and domains.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces CommonForms, a web-scale dataset for form field detection. It casts the problem of form field detection as object detection: given an image of a page, predict the location and type (Text Input, Choice Button, Signature) of form fields. The dataset is constructed by filtering Common Crawl to find PDFs that have fillable elements. Starting with 8 million documents, the filtering process is used to arrive at a final dataset of roughly 55k documents that have over 450k pages. Analysis shows that the dataset contains a diverse mixture of languages and domains; one third of the pages are non-English, and among the 14 classified domains, no domain makes up more than 25% of the dataset.   In addition, this paper presents a family of form field detectors, FFDNet-Small and FFDNet-Large, which attain a very high average precision on the CommonForms test set. Each model cost less than $500 to train. Ablation results show that high-resolution inputs are crucial for high-quality form field detection, and that the cleaning process improves data efficiency over using all PDFs that have fillable fields in Common Crawl. A qualitative analysis shows that they outperform a popular, commercially available PDF reader that can prepare forms. Unlike the most popular commercially available solutions, FFDNet can predict checkboxes in addition to text and signature fields. This is, to our knowledge, the first large scale dataset released for form field detection, as well as the first open source models. The dataset, models, and code will be released at https://github.com/jbarrow/commonforms",
            "score": 0,
            "issue_id": 6066,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "8711c5ce068d185e",
            "pdf_title_img": "assets/pdf/title_img/2509.16506.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#cv"
                ],
                "emoji": "ğŸ“‹",
                "ru": {
                    "title": "ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ…",
                    "desc": "Ğ’ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ CommonForms - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ¹ Ñ„Ğ¾Ñ€Ğ¼, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 55 Ñ‚Ñ‹ÑÑÑ‡ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 450 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº object detection: Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚Ğ¸Ğ¿ Ğ¿Ğ¾Ğ»ĞµĞ¹ (Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ, ĞºĞ½Ğ¾Ğ¿ĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°, Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ FFDNet-Small Ğ¸ FFDNet-Large, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ĞµĞµ 500 Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ğ°Ñ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ - Ñ‚Ñ€ĞµÑ‚ÑŒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ° Ğ½Ğµ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ¹ Ñ„Ğ¾Ñ€Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Form Field Detection with CommonForms",
                    "desc": "This paper presents CommonForms, a large dataset specifically designed for detecting form fields in documents. It treats form field detection as an object detection task, where the goal is to identify the location and type of fields like text inputs and buttons in images of pages. The dataset is derived from filtering 8 million documents from Common Crawl, resulting in about 55,000 documents with diverse languages and domains. Additionally, the authors introduce two models, FFDNet-Small and FFDNet-Large, which achieve high precision in detecting form fields and outperform existing commercial solutions."
                },
                "zh": {
                    "title": "è¡¨å•å­—æ®µæ£€æµ‹çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCommonFormsçš„ç½‘ç»œè§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºè¡¨å•å­—æ®µæ£€æµ‹ã€‚è¯¥ç ”ç©¶å°†è¡¨å•å­—æ®µæ£€æµ‹è§†ä¸ºç›®æ ‡æ£€æµ‹é—®é¢˜ï¼Œæ—¨åœ¨é¢„æµ‹é¡µé¢ä¸­å¯å¡«å†™å­—æ®µçš„ä½ç½®å’Œç±»å‹ï¼ˆå¦‚æ–‡æœ¬è¾“å…¥ã€é€‰æ‹©æŒ‰é’®ã€ç­¾åï¼‰ã€‚æ•°æ®é›†é€šè¿‡è¿‡æ»¤Common Crawlä¸­çš„PDFæ–‡æ¡£æ„å»ºï¼Œæœ€ç»ˆå¾—åˆ°çº¦55,000ä¸ªæ–‡æ¡£ï¼ŒåŒ…å«è¶…è¿‡450,000ä¸ªé¡µé¢ï¼Œæ¶µç›–å¤šç§è¯­è¨€å’Œé¢†åŸŸã€‚ç ”ç©¶è¿˜æå‡ºäº†FFDNet-Smallå’ŒFFDNet-Largeä¸¤ç§è¡¨å•å­—æ®µæ£€æµ‹æ¨¡å‹ï¼Œå…·æœ‰é«˜ç²¾åº¦ï¼Œå¹¶ä¸”è®­ç»ƒæˆæœ¬ä½äº500ç¾å…ƒã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-23.html",
    "link_next": "2025-09-25.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "23.09",
        "en": "09/23",
        "zh": "9æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "25.09",
        "en": "09/25",
        "zh": "9æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 4,
        "#benchmark": 10,
        "#agents": 2,
        "#cv": 5,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 4,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 2,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 3,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 3
    }
}