{
    "date": {
        "ru": "14 Ğ¸ÑĞ»Ñ",
        "en": "July 14",
        "zh": "7æœˆ14æ—¥"
    },
    "time_utc": "2025-07-14 09:19",
    "weekday": 0,
    "issue_id": 4798,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.01951",
            "title": "Test-Time Scaling with Reflective Generative Model",
            "url": "https://huggingface.co/papers/2507.01951",
            "abstract": "MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3's performance via the self-supervised process reward model (SPRM). Through sharing the backbone network and using task-specific heads for next token prediction and process scoring respectively, SPRM successfully integrates the policy model and process reward model(PRM) into a unified interface without extra process annotation, reducing over 99% PRM parameters for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable for test time scaling (TTS), and we provide three reasoning effort modes (low, medium, and high), based on the controllable thinking length. Moreover, we empirically establish a scaling law that reveals the relationship between total thinking computation and TTS performance. Experiments demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with only 32B parameter size. To support the research community, we have open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
            "score": 56,
            "issue_id": 4792,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "46a6ab7d22e05401",
            "authors": [
                "Zixiao Wang",
                "Yuxin Wang",
                "Xiaorui Wang",
                "Mengting Xing",
                "Jie Gao",
                "Jianjun Xu",
                "Guangcan Liu",
                "Chenhui Jin",
                "Zhuo Wang",
                "Shengzhuo Zhang",
                "Hongtao Xie"
            ],
            "affiliations": [
                "MetaStone-AI",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01951.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#architecture",
                    "#rl",
                    "#small_models",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸: MetaStone-S1 Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "MetaStone-S1 - ÑÑ‚Ğ¾ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (SPRM). ĞĞ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. SPRM Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ 99% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. MetaStone-S1 Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° ÑƒÑĞ¸Ğ»Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ TTS."
                },
                "en": {
                    "title": "Efficient Reasoning with Fewer Parameters: Introducing MetaStone-S1",
                    "desc": "MetaStone-S1 is a new generative model that uses a self-supervised process reward model (SPRM) to enhance reasoning capabilities while maintaining a smaller parameter size. By integrating the policy model and process reward model into a single framework, it significantly reduces the number of parameters needed for effective reasoning. The model offers different reasoning effort modes, allowing users to control the depth of thinking during tasks. Experiments show that MetaStone-S1 performs comparably to larger models while being more efficient, and it is available for the research community to explore."
                },
                "zh": {
                    "title": "MetaStone-S1ï¼šé«˜æ•ˆæ¨ç†çš„æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹",
                    "desc": "MetaStone-S1æ˜¯ä¸€ç§åæ€ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨è‡ªç›‘ç£è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆSPRMï¼‰ï¼Œåœ¨å‚æ•°æ›´å°‘çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆæ¨ç†å’Œå¯æ‰©å±•æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡å…±äº«ä¸»å¹²ç½‘ç»œï¼Œå¹¶ä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„å¤´éƒ¨è¿›è¡Œä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œè¿‡ç¨‹è¯„åˆ†ï¼ŒæˆåŠŸå°†ç­–ç•¥æ¨¡å‹å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ•´åˆä¸ºç»Ÿä¸€æ¥å£ï¼Œå‡å°‘äº†99%ä»¥ä¸Šçš„å‚æ•°ã€‚MetaStone-S1é€‚åˆæµ‹è¯•æ—¶é—´æ‰©å±•ï¼ˆTTSï¼‰ï¼Œå¹¶æä¾›ä½ã€ä¸­ã€é«˜ä¸‰ç§æ¨ç†åŠªåŠ›æ¨¡å¼ï¼ŒåŸºäºå¯æ§çš„æ€è€ƒé•¿åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒMetaStone-S1åœ¨ä»…32Bå‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸OpenAI-o3-miniç³»åˆ—ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08776",
            "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering",
            "url": "https://huggingface.co/papers/2507.08776",
            "abstract": "A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper proposes a neural rendering approach that represents a scene as \"compressed light-field tokens (CLiFTs)\", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view ``condenser'' compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed.",
            "score": 37,
            "issue_id": 4792,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "9361d4738ec618fe",
            "authors": [
                "Zhengqing Wang",
                "Yuefan Wu",
                "Jiacheng Chen",
                "Fuyang Zhang",
                "Yasutaka Furukawa"
            ],
            "affiliations": [
                "Simon Fraser University",
                "Wayve"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08776.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ (CLiFTs) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¼ Ğ²Ğ¸Ğ´Ğµ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµÑ€ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… RealEstate10K Ğ¸ DL3DV Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°."
                },
                "en": {
                    "title": "Efficient Scene Representation with Compressed Light-Field Tokens",
                    "desc": "This paper introduces a novel neural rendering technique that utilizes compressed light-field tokens (CLiFTs) to efficiently depict scenes and generate new views. By employing a multi-view encoder, the method tokenizes images based on their camera positions, allowing for effective representation of both appearance and geometry. The approach leverages latent-space K-means to select key rays as cluster centroids, which are then condensed into CLiFTs for streamlined rendering. The system adapts to different compute budgets by varying the number of tokens used, demonstrating significant data reduction while maintaining high rendering quality across various datasets."
                },
                "zh": {
                    "title": "é«˜æ•ˆç¥ç»æ¸²æŸ“ï¼šå‹ç¼©å…‰åœºæ ‡è®°çš„åº”ç”¨",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œä½¿ç”¨å‹ç¼©å…‰åœºæ ‡è®°ï¼ˆCLiFTsï¼‰æ¥é«˜æ•ˆè¡¨ç¤ºåœºæ™¯å¹¶æ¸²æŸ“æ–°è§†å›¾ã€‚CLiFTé€šè¿‡å‹ç¼©æ ‡è®°å®ç°è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶èƒ½å¤Ÿæ ¹æ®éœ€è¦è°ƒæ•´æ ‡è®°æ•°é‡ä»¥è¡¨ç¤ºåœºæ™¯æˆ–æ¸²æŸ“æ–°è§†å›¾ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ç»„å›¾åƒï¼Œå¤šè§†è§’ç¼–ç å™¨å°†å›¾åƒä¸ç›¸æœºå§¿æ€è¿›è¡Œæ ‡è®°ã€‚é€šè¿‡æ½œåœ¨ç©ºé—´Kå‡å€¼é€‰æ‹©ä¸€ç»„å‡å°‘çš„å…‰çº¿ä½œä¸ºèšç±»ä¸­å¿ƒï¼Œæœ€ç»ˆæ„å»ºå‡ºCLiFTsã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08800",
            "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models",
            "url": "https://huggingface.co/papers/2507.08800",
            "abstract": "NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.",
            "score": 21,
            "issue_id": 4792,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "97c49e854df6a19d",
            "authors": [
                "Luke Rivard",
                "Sun Sun",
                "Hongyu Guo",
                "Wenhu Chen",
                "Yuntian Deng"
            ],
            "affiliations": [
                "National Research Council Canada",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08800.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#diffusion",
                    "#dataset",
                    "#agents",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ GUI: ÑˆĞ°Ğ³ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾",
                    "desc": "NeuralOS - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² ÑĞºÑ€Ğ°Ğ½Ğ° Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ (RNN) Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ° Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞºÑ€Ğ°Ğ½Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ubuntu XFCE, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ ĞºĞ°Ğº ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NeuralOS ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ GUI-Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ñ…Ğ¾Ñ‚Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»Ğ°Ğ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹."
                },
                "en": {
                    "title": "NeuralOS: Predicting GUIs with RNNs and Diffusion Rendering",
                    "desc": "NeuralOS is a neural framework designed to simulate operating system graphical user interfaces (GUIs) by predicting screen frames based on user inputs like mouse movements and keyboard events. It utilizes a recurrent neural network (RNN) to keep track of the computer's state and a diffusion-based renderer to create realistic screen images. The model is trained on a comprehensive dataset of Ubuntu XFCE recordings, which include both random and realistic user interactions. While it excels at rendering GUI sequences and predicting state transitions, accurately modeling detailed keyboard interactions remains a challenge, marking a significant advancement in generative neural interfaces for human-computer interaction."
                },
                "zh": {
                    "title": "NeuralOSï¼šæœªæ¥äººæœºäº¤äº’çš„æ™ºèƒ½ç•Œé¢",
                    "desc": "NeuralOS æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡é¢„æµ‹ç”¨æˆ·è¾“å…¥ï¼ˆå¦‚é¼ æ ‡ç§»åŠ¨ã€ç‚¹å‡»å’Œé”®ç›˜äº‹ä»¶ï¼‰æ¥æ¨¡æ‹Ÿæ“ä½œç³»ç»Ÿçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ã€‚å®ƒç»“åˆäº†é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’ŒåŸºäºæ‰©æ•£çš„ç¥ç»æ¸²æŸ“å™¨ï¼Œèƒ½å¤Ÿç”Ÿæˆå±å¹•å›¾åƒå¹¶è·Ÿè¸ªè®¡ç®—æœºçŠ¶æ€ã€‚è¯¥æ¨¡å‹åœ¨å¤§è§„æ¨¡çš„ Ubuntu XFCE å½•åˆ¶æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«éšæœºç”Ÿæˆçš„äº¤äº’å’Œ AI ä»£ç†ç”Ÿæˆçš„çœŸå®äº¤äº’ã€‚å°½ç®¡ç²¾ç¡®å»ºæ¨¡ç»†ç²’åº¦çš„é”®ç›˜äº¤äº’ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒNeuralOS ä¸ºæœªæ¥äººæœºäº¤äº’ç³»ç»Ÿåˆ›å»ºå®Œå…¨è‡ªé€‚åº”çš„ç”Ÿæˆç¥ç»æ¥å£è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05397",
            "title": "Neural-Driven Image Editing",
            "url": "https://huggingface.co/papers/2507.05397",
            "abstract": "LoongX uses multimodal neurophysiological signals and diffusion models for hands-free image editing, achieving performance comparable to text-driven methods and outperforming them when combined with speech.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. Datasets and code will be released to support future work and foster progress in this emerging area.",
            "score": 15,
            "issue_id": 4796,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ»Ñ",
                "en": "July 7",
                "zh": "7æœˆ7æ—¥"
            },
            "hash": "37985f3d38738096",
            "authors": [
                "Pengfei Zhou",
                "Jie Xia",
                "Xiaopeng Peng",
                "Wangbo Zhao",
                "Zilong Ye",
                "Zekai Li",
                "Suorong Yang",
                "Jiadong Pan",
                "Yuanxiang Chen",
                "Ziqiao Wang",
                "Kai Wang",
                "Qian Zheng",
                "Xiaojun Chang",
                "Gang Pan",
                "Shurong Dong",
                "Kaipeng Zhang",
                "Yang You"
            ],
            "affiliations": [
                "MBZUAI",
                "NJU",
                "NUS",
                "RIT",
                "SII",
                "Shanghai AI Lab",
                "USTC",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05397.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑ‹ÑĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "LoongX - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ÑƒĞº, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ñ„Ğ¸Ğ·Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ñ… Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ­Ğ­Ğ“, Ñ„ĞĞ˜Ğ Ğ¡, Ğ¤ĞŸĞ“ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹. LoongX Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ñ€ĞµÑ‡ÑŒÑ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Hands-Free Image Editing with Brain Signals!",
                    "desc": "LoongX is a novel hands-free image editing system that utilizes multimodal neurophysiological signals, such as EEG and fNIRS, combined with advanced diffusion models. This approach allows users with limited motor control or language abilities to edit images intuitively, without manual input. By integrating a cross-scale state space module and a dynamic gated fusion module, LoongX effectively processes diverse signals to capture user intent and align it with image editing semantics. Experimental results show that LoongX performs comparably to traditional text-driven methods and even surpasses them when incorporating speech signals, demonstrating its potential for accessible creative technologies."
                },
                "zh": {
                    "title": "LoongXï¼šæ— éšœç¢å›¾åƒç¼–è¾‘çš„æ–°æ–¹å¼",
                    "desc": "LoongXæ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€ç¥ç»ç”Ÿç†ä¿¡å·çš„å…æ‰‹åŠ¨å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°å›¾åƒç¼–è¾‘ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è„‘æœºæ¥å£ï¼ˆBCIï¼‰å’Œç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œèƒ½å¤Ÿå¸®åŠ©è¿åŠ¨èƒ½åŠ›æˆ–è¯­è¨€èƒ½åŠ›æœ‰é™çš„ç”¨æˆ·è¿›è¡Œå›¾åƒç¼–è¾‘ã€‚LoongXé€šè¿‡è·¨å°ºåº¦çŠ¶æ€ç©ºé—´ï¼ˆCS3ï¼‰æ¨¡å—å’ŒåŠ¨æ€é—¨æ§èåˆï¼ˆDGFï¼‰æ¨¡å—æœ‰æ•ˆå¤„ç†ä¸åŒç±»å‹çš„ä¿¡å·ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoongXåœ¨æ€§èƒ½ä¸Šä¸åŸºäºæ–‡æœ¬çš„æ–¹æ³•ç›¸å½“ï¼Œå¹¶åœ¨ç»“åˆè¯­éŸ³æ—¶è¡¨ç°æ›´ä½³ï¼Œå±•ç¤ºäº†ç¥ç»é©±åŠ¨ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08799",
            "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
            "url": "https://huggingface.co/papers/2507.08799",
            "abstract": "Cache steering improves reasoning in language models through a single intervention in the key-value cache, enhancing both reasoning structure and task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation.",
            "score": 13,
            "issue_id": 4796,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "368d77b32f2df891",
            "authors": [
                "Max Belitsky",
                "Dawid J. Kopiczko",
                "Michael Dorkenwald",
                "M. Jehanzeb Mirza",
                "Cees G. M. Snoek",
                "Yuki M. Asano"
            ],
            "affiliations": [
                "CSAIL MIT",
                "FunAI Lab University of Technology Nuremberg",
                "VIS Lab University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08799.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ² ĞºÑÑˆ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºÑÑˆ-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ² ĞºÑÑˆ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ»ĞµĞ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ GPT-4. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Cache Steering: Enhancing Reasoning in Language Models Efficiently",
                    "desc": "This paper introduces cache steering, a novel technique that enhances the reasoning capabilities of language models by modifying the key-value cache in a single step. By using this method, the authors demonstrate how to encourage chain-of-thought reasoning in smaller language models without the need for extensive fine-tuning or altering prompts. The approach utilizes reasoning traces generated by GPT-4o to create steering vectors that guide the model towards more structured and multi-step reasoning. Experimental results show that cache steering not only improves the quality of reasoning but also boosts performance on various reasoning tasks, offering a more efficient and stable alternative to previous methods."
                },
                "zh": {
                    "title": "ç¼“å­˜å¼•å¯¼ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†çš„æœ‰æ•ˆæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç¼“å­˜å¼•å¯¼çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹å…³é”®å€¼ç¼“å­˜è¿›è¡Œä¸€æ¬¡æ€§å¹²é¢„ï¼Œéšå¼åœ°å¼•å¯¼è¯­è¨€æ¨¡å‹çš„æ¨ç†ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æé«˜å°å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ¨ç†èƒ½åŠ›ï¼Œåˆ©ç”¨GPT-4ç”Ÿæˆçš„æ¨ç†è½¨è¿¹æ„å»ºå¼•å¯¼å‘é‡ï¼Œä»è€Œä½¿æ¨¡å‹è¡Œä¸ºå‘æ›´æ˜ç¡®çš„å¤šæ­¥éª¤æ¨ç†è½¬å˜ï¼Œè€Œæ— éœ€è¿›è¡Œå¾®è°ƒæˆ–ä¿®æ”¹æç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¼“å­˜å¼•å¯¼ä¸ä»…æ”¹å–„äº†æ¨¡å‹æ¨ç†çš„å®šæ€§ç»“æ„ï¼Œè¿˜æé«˜äº†å®šé‡ä»»åŠ¡æ€§èƒ½ã€‚ä¸éœ€è¦æŒç»­å¹²é¢„çš„å…ˆå‰æ¿€æ´»å¼•å¯¼æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ä¸€æ¬¡æ€§ç¼“å­˜å¼•å¯¼åœ¨è¶…å‚æ•°ç¨³å®šæ€§ã€æ¨ç†æ—¶é—´æ•ˆç‡å’Œé›†æˆä¾¿åˆ©æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œæˆä¸ºä¸€ç§æ›´ç¨³å¥å’Œå®ç”¨çš„å—æ§ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08801",
            "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
            "url": "https://huggingface.co/papers/2507.08801",
            "abstract": "Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.",
            "score": 12,
            "issue_id": 4792,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "d53fd2bb25db02a0",
            "authors": [
                "Hangjie Yuan",
                "Weihua Chen",
                "Jun Cen",
                "Hu Yu",
                "Jingyun Liang",
                "Shuning Chang",
                "Zhihui Lin",
                "Tao Feng",
                "Pengwei Liu",
                "Jiazheng Xing",
                "Hao Luo",
                "Jiasheng Tang",
                "Fan Wang",
                "Yi Yang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08801.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#architecture",
                    "#video",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Lumos-1 - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM). ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ MM-RoPE Ğ¸ AR-DF Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Lumos-1 ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ LLM Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Lumos-1: Efficient Video Generation with LLM Architecture",
                    "desc": "Lumos-1 is an innovative autoregressive video generator that enhances the traditional large language model (LLM) architecture to effectively handle video data. It introduces a new method called MM-RoPE, which improves the model's ability to understand spatiotemporal correlations while addressing issues of frame-wise loss imbalance. The model employs a token dependency strategy that respects both intra-frame and inter-frame relationships, ensuring coherent video generation. By utilizing efficient training techniques, Lumos-1 achieves competitive performance with fewer computational resources compared to existing models."
                },
                "zh": {
                    "title": "Lumos-1ï¼šé«˜æ•ˆè‡ªå›å½’è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "Lumos-1æ˜¯ä¸€ç§è‡ªå›å½’è§†é¢‘ç”Ÿæˆå™¨ï¼Œé‡‡ç”¨äº†ç»è¿‡ä¿®æ”¹çš„LLMæ¶æ„ï¼Œç»“åˆäº†MM-RoPEå’ŒAR-DFæŠ€æœ¯ï¼Œä»¥è§£å†³æ—¶ç©ºç›¸å…³æ€§å’Œå¸§é—´æŸå¤±ä¸å¹³è¡¡çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åœ¨ä¿æŒLLMæ¶æ„çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨3D RoPEæ¥å¢å¼ºæ—¶ç©ºç›¸å…³æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„RoPEæ–¹æ¡ˆMM-RoPEï¼Œä»¥æ”¯æŒå¤šæ¨¡æ€æ—¶ç©ºæ•°æ®å»ºæ¨¡ã€‚Lumos-1è¿˜é‡‡ç”¨äº†ä¸€ç§ä»¤ç‰Œä¾èµ–ç­–ç•¥ï¼Œç¡®ä¿å¸§å†…åŒå‘æ€§å’Œå¸§é—´æ—¶é—´å› æœå…³ç³»ï¼Œä»è€Œè§£å†³äº†ç©ºé—´ä¿¡æ¯å†—ä½™å¯¼è‡´çš„å¸§é—´æŸå¤±ä¸å¹³è¡¡é—®é¢˜ã€‚é€šè¿‡é«˜æ•ˆçš„è®­ç»ƒæŠ€æœ¯ï¼ŒLumos-1åœ¨ä»…ä½¿ç”¨48ä¸ªGPUçš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08772",
            "title": "From One to More: Contextual Part Latents for 3D Generation",
            "url": "https://huggingface.co/papers/2507.08772",
            "abstract": "A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.",
            "score": 8,
            "issue_id": 4792,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "13e80f68dc4965b1",
            "authors": [
                "Shaocong Dong",
                "Lihe Ding",
                "Xiao Chen",
                "Yaokun Li",
                "Yuxin Wang",
                "Yucheng Wang",
                "Qi Wang",
                "Jaehyeok Kim",
                "Chenjian Gao",
                "Zhanpeng Huang",
                "Zibin Wang",
                "Tianfan Xue",
                "Dan Xu"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SenseTime Research",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08772.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "CoPart - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. CoPart Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Partverse Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Generation with Part-Aware Diffusion",
                    "desc": "The paper introduces CoPart, a part-aware diffusion framework designed to improve 3D object generation by breaking down objects into contextual parts. This approach addresses limitations in existing models, such as the inability to capture complex geometries and the lack of part independence in holistic representations. CoPart enhances the modeling of relationships between parts and allows for fine-grained control over part-level conditioning. Additionally, the authors present a new dataset, Partverse, to support large-scale training and demonstrate CoPart's effectiveness in tasks like part-level editing and scene composition."
                },
                "zh": {
                    "title": "éƒ¨ä»¶æ„è¯†çš„3Dç”Ÿæˆæ–°æ¡†æ¶",
                    "desc": "CoPartæ˜¯ä¸€ä¸ªå…³æ³¨éƒ¨ä»¶çš„æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†3Då¯¹è±¡åˆ†è§£ä¸ºä¸Šä¸‹æ–‡ç›¸å…³çš„éƒ¨ä»¶æ¥å¢å¼º3Dç”Ÿæˆã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å¤æ‚å¤šéƒ¨ä»¶å‡ ä½•å½¢çŠ¶æ—¶çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å»ºæ¨¡éƒ¨ä»¶ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡éƒ¨ä»¶åˆ†è§£ï¼ŒCoParté™ä½äº†ç¼–ç å¤æ‚æ€§ï¼Œå¹¶æ”¯æŒç²¾ç»†çš„éƒ¨ä»¶çº§æ¡ä»¶æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoPartåœ¨éƒ¨ä»¶çº§ç¼–è¾‘ã€å…³èŠ‚å¯¹è±¡ç”Ÿæˆå’Œåœºæ™¯ç»„åˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å‰æ‰€æœªæœ‰çš„å¯æ§æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08794",
            "title": "One Token to Fool LLM-as-a-Judge",
            "url": "https://huggingface.co/papers/2507.08794",
            "abstract": "Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning openers like \"Thought process:\" and \"Let's solve this problem step by step.\" can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM.",
            "score": 6,
            "issue_id": 4793,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "c5951de5379e6612",
            "authors": [
                "Yulai Zhao",
                "Haolin Liu",
                "Dian Yu",
                "S. Y. Kung",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Princeton University",
                "Tencent AI Lab",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08794.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#data",
                    "#dataset",
                    "#optimization",
                    "#hallucinations",
                    "#synthetic",
                    "#rlhf"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ±Ğ¼Ğ°Ğ½ÑƒÑ‚Ñ‹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ñ„Ñ€Ğ°Ğ·-Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM."
                },
                "en": {
                    "title": "Strengthening LLMs: Combatting Superficial Manipulations in Reward Models",
                    "desc": "This paper discusses the vulnerabilities of generative reward models, which use large language models (LLMs) to assess the quality of answers in reinforcement learning scenarios. The authors reveal that these models can be easily tricked by superficial changes in the input, such as adding non-word symbols or specific phrases, leading to incorrect evaluations. To address this issue, they propose a data augmentation strategy that enhances the robustness of the generative reward models against such manipulations. The study emphasizes the importance of developing more reliable evaluation methods for LLMs in reinforcement learning applications."
                },
                "zh": {
                    "title": "æå‡ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§",
                    "desc": "ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆLLMsä½œä¸ºè¯„åˆ¤è€…ï¼‰åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ç­”æ¡ˆè´¨é‡æ—¶ï¼Œå®¹æ˜“å—åˆ°è¡¨é¢æ“æ§çš„å½±å“ã€‚å°½ç®¡è¿™ç§æ¯”è¾ƒä»»åŠ¡çœ‹ä¼¼ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°ç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨é¢å¯¹éå•è¯ç¬¦å·æˆ–æ¨ç†å¼€å¤´è¯æ—¶ï¼Œå¸¸å¸¸ä¼šäº§ç”Ÿé”™è¯¯çš„æ­£å‘å¥–åŠ±ã€‚è¿™ç§è„†å¼±æ€§åœ¨ä¸åŒçš„LLMã€æ•°æ®é›†å’Œæç¤ºæ ¼å¼ä¸­æ™®éå­˜åœ¨ï¼Œå¨èƒåˆ°ä¾èµ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„æ ¸å¿ƒç®—æ³•èŒƒå¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè®­ç»ƒå‡ºä¸€ç§å…·æœ‰æ˜¾è‘—æ”¹è¿›é²æ£’æ€§çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08441",
            "title": "Vision Foundation Models as Effective Visual Tokenizers for\n  Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2507.08441",
            "abstract": "A novel image tokenizer built on pre-trained vision foundation models improves image reconstruction, generation quality, and token efficiency, enhancing autoregressive generation and class-conditional synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging the powerful representations of pre-trained vision foundation models -- traditionally used for visual comprehension -- we explore a novel direction: building an image tokenizer directly atop such models, a largely underexplored area. Specifically, we employ a frozen vision foundation model as the encoder of our tokenizer. To enhance its effectiveness, we introduce two key components: (1) a region-adaptive quantization framework that reduces redundancy in the pre-trained features on regular 2D grids, and (2) a semantic reconstruction objective that aligns the tokenizer's outputs with the foundation model's representations to preserve semantic fidelity. Based on these designs, our proposed image tokenizer, VFMTok, achieves substantial improvements in image reconstruction and generation quality, while also enhancing token efficiency. It further boosts autoregressive (AR) generation -- achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model convergence by three times, and enabling high-fidelity class-conditional synthesis without the need for classifier-free guidance (CFG). The code will be released publicly to benefit the community.",
            "score": 3,
            "issue_id": 4797,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "700a23d0c8771ece",
            "authors": [
                "Anlin Zheng",
                "Xin Wen",
                "Xuanyang Zhang",
                "Chuofan Ma",
                "Tiancai Wang",
                "Gang Yu",
                "Xiangyu Zhang",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "Dexmal",
                "MEGVII Technology",
                "StepFun",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08441.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "VFMTok: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ VFMTok, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. VFMTok Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½ÑƒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ†ĞµĞ»ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. VFMTok Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ classifier-free guidance."
                },
                "en": {
                    "title": "Enhancing Image Generation with VFMTok: A New Tokenizer Approach",
                    "desc": "This paper introduces VFMTok, a new image tokenizer that utilizes pre-trained vision foundation models to improve image reconstruction and generation. By employing a frozen vision model as its encoder, VFMTok incorporates a region-adaptive quantization framework to minimize redundancy in feature representation. Additionally, it uses a semantic reconstruction objective to ensure that the outputs maintain semantic accuracy aligned with the original model. The results show significant enhancements in token efficiency and autoregressive generation performance, achieving a gFID of 2.07 on ImageNet and tripling model convergence speed."
                },
                "zh": {
                    "title": "åŸºäºè§†è§‰æ¨¡å‹çš„é«˜æ•ˆå›¾åƒæ ‡è®°å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›¾åƒæ ‡è®°å™¨ï¼ŒåŸºäºé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å›¾åƒé‡å»ºå’Œç”Ÿæˆçš„è´¨é‡ä»¥åŠæ ‡è®°æ•ˆç‡ã€‚æˆ‘ä»¬ä½¿ç”¨å†»ç»“çš„è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºæ ‡è®°å™¨çš„ç¼–ç å™¨ï¼Œå¹¶å¼•å…¥äº†åŒºåŸŸè‡ªé€‚åº”é‡åŒ–æ¡†æ¶å’Œè¯­ä¹‰é‡å»ºç›®æ ‡ï¼Œä»¥å‡å°‘å†—ä½™å¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒVFMTokåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆè´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åŠ é€Ÿäº†è‡ªå›å½’ç”Ÿæˆçš„æ”¶æ•›é€Ÿåº¦ã€‚è¯¥æ–¹æ³•åœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†2.07çš„gFIDï¼Œå¹¶æ”¯æŒé«˜ä¿çœŸåº¦çš„ç±»åˆ«æ¡ä»¶åˆæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.08771",
            "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
            "url": "https://huggingface.co/papers/2507.08771",
            "abstract": "To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67times speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN).",
            "score": 2,
            "issue_id": 4794,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "27bac3ede0d76a2a",
            "authors": [
                "Chenyang Song",
                "Weilin Zhao",
                "Xu Han",
                "Chaojun Xiao",
                "Yingfa Chen",
                "Yuxuan Li",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.08771.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#optimization",
                    "#low_resource",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "BlockFFN: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ BlockFFN Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). BlockFFN Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¸ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ ReLU Ğ¸ RMSNorm. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ BlockFFN Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ MoE, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ 80% Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ 70% Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ 8-Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "BlockFFN: Efficient Sparsity for Faster Language Models",
                    "desc": "This paper presents a new architecture called BlockFFN that improves the efficiency of mixture-of-experts (MoE) models by addressing issues with routing and sparsity. The authors introduce a router that uses ReLU activation and RMSNorm, allowing for more flexible and differentiable routing of parameters. They also propose training objectives that enhance both token-level and chunk-level sparsity, making the model more suitable for low-resource environments. Experimental results show that BlockFFN outperforms existing MoE models, achieving significant speedups on end-side devices while maintaining high levels of sparsity."
                },
                "zh": {
                    "title": "æå‡ç¨€ç–æ¿€æ´»æ¨¡å‹çš„æ€§èƒ½ä¸åŠ é€Ÿ",
                    "desc": "ä¸ºäº†å‡è½»å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¡ç®—è´Ÿæ‹…ï¼Œç¨€ç–æ¿€æ´»æ¶æ„ï¼ˆå¦‚ä¸“å®¶æ··åˆæ¨¡å‹MoEï¼‰å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œä¼ ç»ŸMoEçš„éå¯å¾®å’Œä¸çµæ´»çš„è·¯ç”±æ–¹å¼ä¼šå½±å“æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè™½ç„¶æ¯ä¸ªtokenåªæ¿€æ´»å°‘é‡å‚æ•°ï¼Œä½†è¿™äº›ç¨€ç–æ¿€æ´»æ¶æ„åœ¨å—çº§ç¨€ç–æ€§ä¸Šè¡¨ç°è¾ƒä½ï¼Œè¿™ä½¿å¾—åœ¨ä½èµ„æºæ¡ä»¶ä¸‹åŠ é€Ÿå˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„MoEæ¶æ„BlockFFNï¼Œå¹¶è®¾è®¡äº†é«˜æ•ˆçš„è®­ç»ƒå’Œéƒ¨ç½²æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06952",
            "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models",
            "url": "https://huggingface.co/papers/2507.06952",
            "abstract": "Foundation models, despite excelling in training tasks, often fail to generalize to new tasks due to task-specific heuristics rather than capturing underlying world models.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models are premised on the idea that sequence prediction can uncover deeper domain understanding, much like how Kepler's predictions of planetary motion later led to the discovery of Newtonian mechanics. However, evaluating whether these models truly capture deeper structure remains a challenge. We develop a technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model. Our technique measures whether the foundation model's inductive bias aligns with the world model, and so we refer to it as an inductive bias probe. Across multiple domains, we find that foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks. We particularly find that foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks. Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize.",
            "score": 2,
            "issue_id": 4795,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "d203424a90614c2b",
            "authors": [
                "Keyon Vafa",
                "Peter G. Chang",
                "Ashesh Rambachan",
                "Sendhil Mullainathan"
            ],
            "affiliations": [
                "Harvard University",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06952.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑƒÑĞ¿ĞµÑ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸Ñ… ÑƒÑĞ¿ĞµÑ…Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ¼ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ñ€Ğ±Ğ¸Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…, Ğ½Ğµ ÑĞ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ ĞÑŒÑÑ‚Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼."
                },
                "en": {
                    "title": "Unveiling the Limits of Foundation Models: Task-Specific Heuristics vs. General Understanding",
                    "desc": "This paper discusses the limitations of foundation models in machine learning, particularly their inability to generalize to new tasks. The authors introduce a method called the inductive bias probe, which evaluates how well these models adapt to synthetic datasets based on a theoretical world model. Their findings indicate that while foundation models perform well on training tasks, they often rely on task-specific heuristics instead of understanding the underlying principles. Specifically, models trained on orbital trajectories struggle to apply Newtonian mechanics in new physics tasks, highlighting the need for better generalization capabilities."
                },
                "zh": {
                    "title": "åŸºç¡€æ¨¡å‹çš„å½’çº³åå·®æ¢æµ‹ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "åŸºç¡€æ¨¡å‹åœ¨è®­ç»ƒä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ–°ä»»åŠ¡ä¸Šå¸¸å¸¸æ— æ³•æ³›åŒ–ï¼Œè¿™æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„å¯å‘å¼æ–¹æ³•ï¼Œè€Œä¸æ˜¯æ•æ‰åˆ°æ›´æ·±å±‚æ¬¡çš„ä¸–ç•Œæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯„ä¼°åŸºç¡€æ¨¡å‹çš„æ–°æŠ€æœ¯ï¼Œæ—¨åœ¨æ£€æŸ¥å®ƒä»¬å¦‚ä½•é€‚åº”ä»å‡è®¾çš„ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®é›†ã€‚è¯¥æŠ€æœ¯æµ‹é‡åŸºç¡€æ¨¡å‹çš„å½’çº³åå·®æ˜¯å¦ä¸ä¸–ç•Œæ¨¡å‹ä¸€è‡´ï¼Œå› æ­¤æˆ‘ä»¬ç§°ä¹‹ä¸ºå½’çº³åå·®æ¢æµ‹å™¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå°½ç®¡åŸºç¡€æ¨¡å‹åœ¨è®­ç»ƒä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬å¾€å¾€æœªèƒ½å‘å±•å‡ºå¯¹åº•å±‚ä¸–ç•Œæ¨¡å‹çš„å½’çº³åå·®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07151",
            "title": "Robust Multimodal Large Language Models Against Modality Conflict",
            "url": "https://huggingface.co/papers/2507.07151",
            "abstract": "Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.",
            "score": 2,
            "issue_id": 4792,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "2c4bd34981d368dd",
            "authors": [
                "Zongmeng Zhang",
                "Wengang Zhou",
                "Jie Zhao",
                "Houqiang Li"
            ],
            "affiliations": [
                "Department of Electronic Engineering and Information Science, University of Science and Technology of China",
                "Huawei Technologies Co., Ltd.",
                "School of Artificial Intelligence and Data Science, University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07151.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#dataset",
                    "#rl",
                    "#hallucinations",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM) Ğ¸Ğ·-Ğ·Ğ° ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MMMC Ğ´Ğ»Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ‘Ñ‹Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Tackling Hallucinations in MLLMs: The Power of Reinforcement Learning",
                    "desc": "This paper explores how multimodal large language models (MLLMs) can experience hallucinations due to conflicts between different types of input data, known as modality conflict. The authors define modality conflict and create a dataset called Multimodal Modality Conflict (MMMC) to study this issue in vision-language tasks. They propose three strategies to reduce hallucinations: prompt engineering, supervised fine-tuning, and reinforcement learning. Among these, reinforcement learning is found to be the most effective method for addressing hallucinations caused by modality conflict, while supervised fine-tuning also shows reliable results."
                },
                "zh": {
                    "title": "æ­ç¤ºæ¨¡æ€å†²çªï¼Œå‡è½»å¹»è§‰çš„æœ‰æ•ˆç­–ç•¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„æ¨¡æ€å†²çªç°è±¡ï¼Œå‘ç°å®ƒæ˜¯å¯¼è‡´å¹»è§‰çš„ä¸€ä¸ªé‡è¦åŸå› ã€‚ä¸ä»¥å¾€ç ”ç©¶ä¸åŒï¼Œæœ¬æ–‡å…³æ³¨çš„æ˜¯æ¥è‡ªä¸åŒæ¨¡æ€çš„è¾“å…¥ä¹‹é—´çš„å†…åœ¨å†²çªï¼Œè¿™ç§å†²çªä½¿å¾—MLLMsé¢ä¸´å›°å¢ƒå¹¶ç›´æ¥å¯¼è‡´å¹»è§‰ã€‚æˆ‘ä»¬æ­£å¼å®šä¹‰äº†æ¨¡æ€å†²çªï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåä¸ºå¤šæ¨¡æ€æ¨¡æ€å†²çªï¼ˆMMMCï¼‰çš„æ•°æ®é›†ï¼Œä»¥æ¨¡æ‹Ÿè¿™ä¸€ç°è±¡ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•åœ¨å‡è½»æ¨¡æ€å†²çªå¼•èµ·çš„å¹»è§‰æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œç›‘ç£å¾®è°ƒæ–¹æ³•åˆ™å±•ç°å‡ºè‰¯å¥½ä¸”ç¨³å®šçš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-11.html",
    "link_next": "2025-07-15.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "11.07",
        "en": "07/11",
        "zh": "7æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "15.07",
        "en": "07/15",
        "zh": "7æœˆ15æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 3,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 1
    }
}