{
    "date": {
        "ru": "18 –∏—é–Ω—è",
        "en": "June 18",
        "zh": "6Êúà18Êó•"
    },
    "time_utc": "2025-06-18 08:16",
    "weekday": 2,
    "issue_id": 4353,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.14429",
            "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
            "url": "https://huggingface.co/papers/2506.14429",
            "abstract": "This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textit{stable perplexity} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textit{local perception} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.",
            "score": 23,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "d0032538675516d6",
            "authors": [
                "Xiaoran Liu",
                "Zhigeng Liu",
                "Zengfeng Huang",
                "Qipeng Guo",
                "Ziwei He",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "School of Computer Science, Fudan University",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14429.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#architecture",
                    "#benchmark",
                    "#diffusion",
                    "#rl"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
                    "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é –ø–µ—Ä–ø–ª–µ–∫—Å–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ–±–ª–∞–¥–∞—é—Ç —Ñ–µ–Ω–æ–º–µ–Ω–æ–º '–ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è'. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ LongLLaDA –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ –∑–∞–¥–∞—á–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
                },
                "en": {
                    "title": "Unlocking Long Contexts in Diffusion LLMs with LongLLaDA",
                    "desc": "This paper explores how diffusion large language models (LLMs) perform with long contexts compared to traditional auto-regressive LLMs. It highlights that diffusion LLMs maintain stable perplexity when extending context, unlike their auto-regressive counterparts, which struggle with longer inputs. The authors introduce LongLLaDA, a method that allows for context window extension without additional training, leveraging insights from Rotary Position Embedding (RoPE) scaling. The findings reveal specific tasks where diffusion LLMs excel and others where they do not, paving the way for future research in long-context applications."
                },
                "zh": {
                    "title": "Êâ©Êï£Ê®°ÂûãÁöÑÈïø‰∏ä‰∏ãÊñáÊñ∞ÊñπÊ≥ïÔºöLongLLaDA",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàdiffusion LLMsÔºâ‰∏éËá™ÂõûÂΩíÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàauto-regressive LLMsÔºâÂú®Èïø‰∏ä‰∏ãÊñáÊÄßËÉΩÊñπÈù¢ÁöÑÊØîËæÉÔºåËØÜÂà´‰∫ÜÂÆÉ‰ª¨ÁöÑÁã¨ÁâπÁâπÊÄßÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊñπÊ≥ïLongLLaDAÊù•Êâ©Â±ï‰∏ä‰∏ãÊñáÁ™óÂè£„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊâ©Êï£LLMsÂú®Áõ¥Êé•‰∏ä‰∏ãÊñáÂ§ñÊé®Êó∂‰øùÊåÅ‰∫ÜÊòæËëóÁ®≥ÂÆöÁöÑÂõ∞ÊÉëÂ∫¶ÔºåËÄåËá™ÂõûÂΩíÊ®°ÂûãÂú®‰∏ä‰∏ãÊñáË∂ÖÂá∫È¢ÑËÆ≠ÁªÉÈïøÂ∫¶Êó∂ÂàôË°®Áé∞‰∏ç‰Ω≥„ÄÇÊâ©Êï£LLMsÂ±ïÁé∞Âá∫Áã¨ÁâπÁöÑÂ±ÄÈÉ®ÊÑüÁü•Áé∞Ë±°Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊàêÂäü‰ªéÊúÄËøëÁöÑ‰∏ä‰∏ãÊñáÁâáÊÆµ‰∏≠Ê£ÄÁ¥¢‰ø°ÊÅØ„ÄÇÈÄöËøáÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÁº©ÊîæÁêÜËÆ∫ÔºåÊàë‰ª¨Ëß£Èáä‰∫ÜËøô‰∫õÁé∞Ë±°ÔºåÂπ∂È™åËØÅ‰∫ÜÊâ©Êï£LLMsÁöÑ‰∏ä‰∏ãÊñáÂ§ñÊé®ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14234",
            "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team",
            "url": "https://huggingface.co/papers/2506.14234",
            "abstract": "Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.",
            "score": 16,
            "issue_id": 4348,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "70ebdc96484832ea",
            "authors": [
                "Md Tanzib Hosain",
                "Salman Rahman",
                "Md Kishor Morol",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "American International University-Bangladesh",
                "Cornell University",
                "Qatar Computing Research Institute",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14234.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#agi",
                    "#open_source",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "Xolver: –û–ø—ã—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "desc": "Xolver - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∑–∞ —Å—á–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –æ–ø—ã—Ç–∞. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, –ø–æ–¥–æ–±–Ω–æ —ç–∫—Å–ø–µ—Ä—Ç–∞–º-—Ä–µ—à–∞—Ç–µ–ª—è–º –∑–∞–¥–∞—á. Xolver –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –æ–ø—ã—Ç–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ –≤–Ω–µ—à–Ω–∏–π –∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ. –î–∞–∂–µ —Å –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ Xolver –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤—ã—Ö –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö."
                },
                "en": {
                    "title": "Empowering Language Models with Experience-Aware Reasoning",
                    "desc": "Xolver is a multi-agent reasoning framework designed to enhance large language models (LLMs) by incorporating persistent memory and diverse experience modalities. Unlike traditional LLMs that treat each problem independently, Xolver allows agents to accumulate knowledge from past experiences, similar to expert problem solvers. This framework integrates various methods such as self-retrieval, tool usage, and collaborative interactions to refine reasoning and improve performance on complex tasks. As a result, Xolver consistently outperforms specialized reasoning agents, achieving state-of-the-art results on several benchmarks, demonstrating the importance of experience-aware learning in AI."
                },
                "zh": {
                    "title": "XolverÔºöÁªèÈ™åÈ©±Âä®ÁöÑÊé®ÁêÜÊ°ÜÊû∂",
                    "desc": "XolverÊòØ‰∏Ä‰∏™Â§öÊô∫ËÉΩ‰ΩìÊé®ÁêÜÊ°ÜÊû∂ÔºåÈÄöËøáÊåÅ‰πÖËÆ∞ÂøÜÂíåÂ§öÊ†∑ÂåñÁöÑÁªèÈ™åÊ®°ÂºèÂ¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÔºå‰ªéËÄåÊèêÈ´òÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°ÁöÑË°®Áé∞„ÄÇ‰∏é‰º†ÁªüÁöÑLLMÂ≠§Á´ãÂ§ÑÁêÜÊØè‰∏™ÈóÆÈ¢ò‰∏çÂêåÔºåXolverËÉΩÂ§üÊï¥ÂêàÂíåÁßØÁ¥ØÁªèÈ™åÁü•ËØÜÔºåÊ®°Êãü‰∏ìÂÆ∂ÈóÆÈ¢òËß£ÂÜ≥ËÄÖÁöÑÊÄùÁª¥ÊñπÂºè„ÄÇÂÆÉÈÄöËøáÂ§ñÈÉ®ÂíåËá™ÊàëÊ£ÄÁ¥¢„ÄÅÂ∑•ÂÖ∑‰ΩøÁî®„ÄÅÂçè‰Ωú‰∫íÂä®Á≠âÂ§öÁßçÁªèÈ™åÊ®°ÂºèÔºåÈÅøÂÖç‰ªéÂ§¥ÁîüÊàêËß£ÂÜ≥ÊñπÊ°à„ÄÇXolverÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ±ïÁ§∫‰∫ÜÊï¥‰ΩìÁªèÈ™åÂ≠¶‰π†Âú®ÂÆûÁé∞ÈÄöÁî®Êô∫ËÉΩ‰ΩìÊñπÈù¢ÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13642",
            "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
            "url": "https://huggingface.co/papers/2506.13642",
            "abstract": "Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.",
            "score": 16,
            "issue_id": 4347,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "0d0624980a111254",
            "authors": [
                "Shaolei Zhang",
                "Shoutao Guo",
                "Qingkai Fang",
                "Yan Zhou",
                "Yang Feng"
            ],
            "affiliations": [
                "Key Laboratory of AI Safety, Chinese Academy of Sciences",
                "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13642.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#audio",
                    "#transfer_learning",
                    "#cv",
                    "#benchmark",
                    "#agi"
                ],
                "emoji": "üîÄ",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –º–æ—â–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π",
                    "desc": "Stream-Omni - —ç—Ç–æ –∫—Ä—É–ø–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ä–µ—á—å. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é –ø–æ –∏–∑–º–µ—Ä–µ–Ω–∏—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –∏–∑–º–µ—Ä–µ–Ω–∏—é —Å–ª–æ–µ–≤ –¥–ª—è —Ä–µ—á–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏. Stream-Omni –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è, —Ä–µ—á–µ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ —Ä–µ—á–µ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Stream-Omni: Efficient Multimodal Integration for Enhanced Interaction",
                    "desc": "Stream-Omni is a large multimodal model that effectively integrates text, vision, and speech by using innovative alignment techniques. It employs sequence-dimension concatenation for aligning vision with text and a layer-dimension mapping for aligning speech with text, which allows for more efficient learning of modality relationships. This approach reduces the reliance on large datasets, particularly for speech, while still achieving strong performance across various multimodal tasks. The model's design enables it to provide intermediate outputs during speech interactions, enhancing the overall user experience in multimodal applications."
                },
                "zh": {
                    "title": "Stream-OmniÔºöÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÊï¥ÂêàÊ®°Âûã",
                    "desc": "Stream-OmniÊòØ‰∏ÄÁßçÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåËÉΩÂ§üÊúâÊïàÊï¥ÂêàÊñáÊú¨„ÄÅËßÜËßâÂíåËØ≠Èü≥„ÄÇÂÆÉÈÄöËøáÂ∫èÂàóÁª¥Â∫¶ËøûÊé•ÂÆûÁé∞ËßÜËßâ‰∏éÊñáÊú¨ÁöÑÂØπÈΩêÔºåÂπ∂ÈÄöËøáÂü∫‰∫éCTCÁöÑÂ±ÇÁª¥Â∫¶Êò†Â∞ÑÂÆûÁé∞ËØ≠Èü≥‰∏éÊñáÊú¨ÁöÑÂØπÈΩêÔºå‰ªéËÄåÂú®Êï∞ÊçÆËæÉÂ∞ëÁöÑÊÉÖÂÜµ‰∏ã‰πüËÉΩËææÂà∞ËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÊîØÊåÅÂ§öÁßçÊ®°ÊÄÅÁªÑÂêàÁöÑ‰∫§‰∫íÔºåËÉΩÂ§üÂú®ËßÜËßâÁêÜËß£„ÄÅËØ≠Èü≥‰∫§‰∫íÂíåËßÜËßâÂºïÂØºÁöÑËØ≠Èü≥‰∫§‰∫í‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇStream-OmniÁöÑËÆæËÆ°‰ΩøÂæóÁî®Êà∑Âú®ËØ≠Èü≥‰∫§‰∫íÊó∂ÂèØ‰ª•ÂêåÊó∂Ëé∑Âæó‰∏≠Èó¥ÊñáÊú¨ËæìÂá∫ÔºåÊèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑÂ§öÊ®°ÊÄÅ‰ΩìÈ™å„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12928",
            "title": "Scaling Test-time Compute for LLM Agents",
            "url": "https://huggingface.co/papers/2506.12928",
            "abstract": "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.",
            "score": 16,
            "issue_id": 4351,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 –∏—é–Ω—è",
                "en": "June 15",
                "zh": "6Êúà15Êó•"
            },
            "hash": "39c8f3e831e90d93",
            "authors": [
                "King Zhu",
                "Hanhao Li",
                "Siwei Wu",
                "Tianshun Xing",
                "Dehua Ma",
                "Xiangru Tang",
                "Minghao Liu",
                "Jian Yang",
                "Jiaheng Liu",
                "Yuchen Eleanor Jiang",
                "Changwang Zhang",
                "Chenghua Lin",
                "Jun Wang",
                "Ge Zhang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.12928.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–∫–∞–∑–∞–ª–æ —É–ª—É—á—à–µ–Ω–∏–µ –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –ë—ã–ª–∏ –∏–∑—É—á–µ–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –≤–∫–ª—é—á–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –≤—ã–±–æ—Ä–∫—É, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫—É, –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤. –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –æ–∫–∞–∑–∞–ª–∏—Å—å –º–µ—Ç–æ–¥—ã —Å–ø–∏—Å–æ—á–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–π."
                },
                "en": {
                    "title": "Boosting Language Agents with Test-Time Scaling",
                    "desc": "This paper investigates how increasing computational resources at test time can enhance the performance of large language models (LLMs). It systematically examines various test-time scaling methods, such as parallel sampling, sequential revisions, and verification techniques. The findings indicate that scaling up computation not only boosts reasoning capabilities but also highlights the importance of strategic reflection and diverse rollouts. Notably, the study reveals that the list-wise verification method yields the best results among different merging approaches."
                },
                "zh": {
                    "title": "ÊµãËØïÊó∂Èó¥Êâ©Â±ïÊèêÂçáËØ≠Ë®Ä‰ª£ÁêÜÊÄßËÉΩ",
                    "desc": "Êú¨ÊñáÁ≥ªÁªüÊé¢ËÆ®‰∫ÜÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠Â∫îÁî®ÊµãËØïÊó∂Èó¥Êâ©Â±ïÊñπÊ≥ïÁöÑÊïàÊûú„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËÆ°ÁÆóÊâ©Â±ïËÉΩÂ§üÊòæËëóÊèêÂçáËØ≠Ë®Ä‰ª£ÁêÜÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÈÄöËøáÂπ∂Ë°åÈááÊ†∑„ÄÅÈ°∫Â∫è‰øÆËÆ¢„ÄÅÊúâÊïàÈ™åËØÅÂíåÂ¢ûÂä†Â§öÊ†∑ÂåñÁöÑÂõûÊªöÁ≠ñÁï•„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫Ü‰∏çÂêåËÆæËÆ°Á≠ñÁï•ÂØπËØ≠Ë®Ä‰ª£ÁêÜÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÂπ∂ÂèëÁé∞ÊµãËØïÊó∂Èó¥ËÆ°ÁÆóÁöÑÊâ©Â±ïÁ°ÆÂÆûËÉΩÊèêÈ´ò‰ª£ÁêÜÁöÑË°®Áé∞„ÄÇÁâπÂà´ÊòØÔºåÈááÁî®ÂàóË°®ÂºèÈ™åËØÅÊñπÊ≥ïÊïàÊûúÊúÄ‰Ω≥ÔºåËÄåÂ§öÊ†∑ÂåñÁöÑÂõûÊªöÁ≠ñÁï•‰πüÂØπ‰ªªÂä°Ë°®Áé∞ÊúâÁßØÊûÅÂΩ±Âìç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14758",
            "title": "Reasoning with Exploration: An Entropy Perspective",
            "url": "https://huggingface.co/papers/2506.14758",
            "abstract": "Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.",
            "score": 15,
            "issue_id": 4349,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "14595ff25bf8a37c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠–Ω—Ç—Ä–æ–ø–∏—è –∫–∞–∫ –∫–ª—é—á –∫ –≥–ª—É–±–æ–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã–π —á–ª–µ–Ω –≤ —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞, —á—Ç–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é –º–æ–¥–µ–ª–∏. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ –º–µ—Ç—Ä–∏–∫–µ Pass@K. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏, —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏ —Ä–µ–¥–∫–∏–º–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Enhancing Language Model Reasoning through Entropy-Driven Exploration",
                    "desc": "This paper introduces a new approach to enhance exploratory reasoning in language models (LMs) by modifying the advantage function in reinforcement learning (RL) with an entropy-based term. The authors highlight that traditional methods often focus on exploitation, leading to performance plateaus, and argue that incorporating entropy can promote better exploration. Their empirical analysis shows that high-entropy regions correlate with key reasoning actions, such as pivotal tokens and reflective behaviors. The proposed method not only encourages deeper reasoning chains but also significantly improves performance on the Pass@K metric, demonstrating its effectiveness in advancing LM reasoning capabilities."
                },
                "zh": {
                    "title": "Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÁöÑÊé¢Á¥¢ÊÄß",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁÜµÁöÑÊúØËØ≠ÔºåÂ∫îÁî®‰∫éÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑ‰ºòÂäøÂáΩÊï∞Ôºå‰ª•Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé¢Á¥¢ÊÄßÊé®ÁêÜËÉΩÂäõ„ÄÇËøôÁßçÊñπÊ≥ïÈÄöËøáÂºïÂÖ•ÁÜµ‰ø°Âè∑Ôºå‰øÉËøõ‰∫ÜÊé¢Á¥¢‰∏éÂà©Áî®‰πãÈó¥ÁöÑÂπ≥Ë°°ÔºåËß£ÂÜ≥‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÊÄßËÉΩÂÅúÊªûÁöÑÈóÆÈ¢ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈ´òÁÜµÂå∫Âüü‰∏é‰∏âÁßçÊé¢Á¥¢ÊÄßÊé®ÁêÜË°å‰∏∫‰πãÈó¥Â≠òÂú®Âº∫Ê≠£Áõ∏ÂÖ≥ÔºåÂåÖÊã¨ÂÖ≥ÈîÆÊ†áËÆ∞„ÄÅÂèçÊÄùÊÄßË°å‰∏∫ÂíåÁ®ÄÊúâË°å‰∏∫„ÄÇÈÄöËøáÁÆÄÂçïÁöÑ‰ª£Á†Å‰øÆÊîπÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂ∞§ÂÖ∂Âú®Pass@KÊåáÊ†á‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12278",
            "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure",
            "url": "https://huggingface.co/papers/2506.12278",
            "abstract": "TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.",
            "score": 12,
            "issue_id": 4348,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 –∏—é–Ω—è",
                "en": "June 13",
                "zh": "6Êúà13Êó•"
            },
            "hash": "c852db550c523453",
            "authors": [
                "Zheyuan Yang",
                "Zexi Kuang",
                "Xue Xia",
                "Yilun Zhao"
            ],
            "affiliations": [
                "HKUST",
                "Northeastern University",
                "Tongji University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12278.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üß™",
                "ru": {
                    "title": "TestCase-Eval: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –Ø–ú –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤",
                    "desc": "TestCase-Eval - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ø–ú) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 500 –∑–∞–¥–∞—á –∏ 100 000 —Ä–µ—à–µ–Ω–∏–π —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã Codeforces. –ë–µ–Ω—á–º–∞—Ä–∫ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –¥–≤—É—Ö –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö: –ø–æ–∫—Ä—ã—Ç–∏–µ –æ—à–∏–±–æ–∫ –∏ –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ—Ü–µ–Ω–∫—É 19 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ø–ú –Ω–∞ TestCase-Eval, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–≤ –∞–Ω–∞–ª–∏–∑ –∏—Ö —Å–∏–ª—å–Ω—ã—Ö –∏ —Å–ª–∞–±—ã—Ö —Å—Ç–æ—Ä–æ–Ω –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤."
                },
                "en": {
                    "title": "Evaluating LLMs for Effective Test Case Generation",
                    "desc": "TestCase-Eval is a benchmark designed to assess the performance of large language models (LLMs) in generating effective test cases for algorithmic problems. It consists of 500 algorithm problems paired with 100,000 human-created solutions sourced from the Codeforces platform. The evaluation focuses on two main aspects: Fault Coverage, which checks how well the generated test cases explore various input scenarios, and Fault Exposure, which determines the ability of LLMs to create specific test inputs that can uncover flaws in code implementations. The study evaluates 19 different LLMs, providing valuable insights into their capabilities and limitations in this area."
                },
                "zh": {
                    "title": "ËØÑ‰º∞LLMÁîüÊàêÊµãËØïÁî®‰æãÁöÑÊñ∞Âü∫ÂáÜ",
                    "desc": "TestCase-EvalÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁîüÊàêÁÆóÊ≥ïÈóÆÈ¢òÊµãËØïÁî®‰æãÁöÑÊñ∞Âü∫ÂáÜ„ÄÇÂÆÉÂåÖÂê´500‰∏™ÁÆóÊ≥ïÈóÆÈ¢òÂíåÊù•Ëá™CodeforcesÂπ≥Âè∞ÁöÑ100,000‰∏™‰∫∫Â∑•Ëß£ÂÜ≥ÊñπÊ°à„ÄÇËØ•Âü∫ÂáÜÂÖ≥Ê≥®‰∏§‰∏™ÂÖ≥ÈîÆ‰ªªÂä°ÔºöÊïÖÈöúË¶ÜÁõñÊÄßÔºåËØÑ‰º∞LLMÁîüÊàêÁöÑÊµãËØïÈõÜÊòØÂê¶ËÉΩÂ§üÊé¢ÊµãÂ§öÊ†∑ÁöÑËæìÂÖ•Âú∫ÊôØÔºõÊïÖÈöúÊö¥Èú≤ÊÄßÔºåËØÑ‰º∞LLMÊòØÂê¶ËÉΩÂ§üÁîüÊàêÁâπÂÆöÁöÑÊµãËØïËæìÂÖ•‰ª•Êè≠Á§∫‰ª£Á†ÅÂÆûÁé∞‰∏≠ÁöÑÈîôËØØ„ÄÇÊàë‰ª¨ÂØπ19‰∏™ÊúÄÂÖàËøõÁöÑÂºÄÊ∫êÂíå‰∏ìÊúâLLMÂú®TestCase-Eval‰∏äÁöÑË°®Áé∞ËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÊèê‰æõ‰∫ÜÂÆÉ‰ª¨Âú®ÁîüÊàêÊúâÊïàÊµãËØïÁî®‰æãÊñπÈù¢ÁöÑ‰ºòÁº∫ÁÇπÁöÑËßÅËß£„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14245",
            "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs",
            "url": "https://huggingface.co/papers/2506.14245",
            "abstract": "RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the Pass@K metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the Pass@K metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, CoT-Pass@K, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using CoT-Pass@K, we observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.",
            "score": 11,
            "issue_id": 4348,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "c78cc63a970ea4e9",
            "authors": [
                "Xumeng Wen",
                "Zihan Liu",
                "Shun Zheng",
                "Zhijian Xu",
                "Shengyu Ye",
                "Zhirong Wu",
                "Xiao Liang",
                "Yang Wang",
                "Junjie Li",
                "Ziming Miao",
                "Jiang Bian",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Peking University",
                "The Chinese University of Hong Kong",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14245.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "RLVR: –ø—É—Ç—å –∫ –ª–æ–≥–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - Reinforcement Learning with Verifiable Rewards (RLVR). –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –æ—Ü–µ–Ω–∫–∏ CoT-Pass@K, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–∞–∫ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Ç–∞–∫ –∏ –∫–æ–Ω–µ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ RLVR –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –æ–±–æ–±—â–µ–Ω–∏—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π K. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª RLVR –¥–ª—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –º–∞—à–∏–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Enhancing Machine Reasoning with RLVR and CoT-Pass@K",
                    "desc": "Reinforcement Learning with Verifiable Rewards (RLVR) enhances the reasoning abilities of Large Language Models (LLMs) by promoting logical thought processes. The study identifies a flaw in the existing evaluation metric, Pass@K, which inaccurately rewards correct answers that may stem from faulty reasoning paths. To improve this, the authors propose a new metric, CoT-Pass@K, that ensures both the reasoning chain and the final answer are accurate. The findings demonstrate that RLVR can effectively encourage correct reasoning from the early stages of training, leading to better generalization across various scenarios."
                },
                "zh": {
                    "title": "RLVRÔºöÊé®Âä®Êú∫Âô®Êé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "RLVRÔºàÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºâÈÄöËøáÊøÄÂä±Ê≠£Á°ÆÂíåÈÄªËæëÁöÑÊÄùÁª¥ÈìæÔºåÊé®Âä®‰∫ÜÊú∫Âô®Êé®ÁêÜÁöÑÂèëÂ±ï„ÄÇÁ†îÁ©∂ÂèëÁé∞Ôºå‰º†ÁªüÁöÑËØÑ‰º∞ÊåáÊ†áPass@KÂ≠òÂú®Áº∫Èô∑ÔºåÂèØËÉΩ‰ºöÈîôËØØÂú∞ËÆ§ÂèØ‰∏çÂÆåÊï¥ÁöÑÊÄùÁª¥ÈìæÊâÄÂæóÂà∞ÁöÑÊ≠£Á°ÆÁ≠îÊ°à„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÊõ¥Á≤æÁ°ÆÁöÑËØÑ‰º∞ÊåáÊ†áCoT-Pass@KÔºåË¶ÅÊ±ÇÊé®ÁêÜË∑ØÂæÑÂíåÊúÄÁªàÁ≠îÊ°àÈÉΩÂøÖÈ°ªÊ≠£Á°Æ„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åËØÅÊòéÔºåRLVRËÉΩÂ§üÊúâÊïàÊøÄÂä±Ê≠£Á°ÆÊé®ÁêÜÁöÑÊ≥õÂåñÔºåÂπ∂‰∏îËøôÁßçÂ¢ûÂº∫ÁöÑÊé®ÁêÜËÉΩÂäõÂú®ËÆ≠ÁªÉÊó©ÊúüÂ∞±ËÉΩÊòæÁé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13363",
            "title": "Efficient Medical VIE via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.13363",
            "abstract": "An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE.",
            "score": 11,
            "issue_id": 4348,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "29de6bf10e7470ad",
            "authors": [
                "Lijun Liu",
                "Ruiyang Li",
                "Zhaocheng Liu",
                "Chenglin Zhu",
                "Chong Li",
                "Jiehan Cheng",
                "Qiang Ju",
                "Jian Xie"
            ],
            "affiliations": [
                "Baichuan Inc.",
                "Peking University",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13363.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#training",
                    "#optimization",
                    "#healthcare",
                    "#reasoning",
                    "#rl",
                    "#multimodal"
                ],
                "emoji": "üè•",
                "ru": {
                    "title": "RLVR: –ü—Ä–æ—Ä—ã–≤ –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ RLVR –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen2.5-VL-7B –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. RLVR —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥—Ä—É–≥–∏—Ö –¥–æ–º–µ–Ω–æ–≤."
                },
                "en": {
                    "title": "Revolutionizing Medical VIE with Limited Data and Enhanced Reasoning",
                    "desc": "This paper presents a Reinforcement Learning with Verifiable Rewards (RLVR) framework that utilizes a fine-tuned Qwen2.5-VL-7B model to enhance Visual Information Extraction (VIE) in medical contexts. By leveraging only 100 annotated samples, the framework effectively balances precision and recall, addressing the challenges posed by limited annotated data and high annotation costs. The approach incorporates innovative sampling strategies and a balanced reward mechanism to improve reasoning capabilities and reduce hallucinations in the output. The results show significant improvements in F1 score, precision, and recall, although the model's performance varies with the similarity of the tasks to the training data."
                },
                "zh": {
                    "title": "ÂåªÁñóËßÜËßâ‰ø°ÊÅØÊèêÂèñÁöÑÂàõÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÂèØÈ™åËØÅÂ•ñÂä±ÔºàRLVRÔºâÊ°ÜÊû∂ÁöÑÊñπÊ≥ïÔºåÂà©Áî®ÂæÆË∞ÉÁöÑQwen2.5-VL-7BÊ®°ÂûãÔºåÂú®ÂåªÁñóËßÜËßâ‰ø°ÊÅØÊèêÂèñÔºàVIEÔºâ‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ï‰ªÖ‰ΩøÁî®100‰∏™Ê†áÊ≥®Ê†∑Êú¨ÔºåËß£ÂÜ≥‰∫Ü‰º†ÁªüÊñπÊ≥ïÂú®ÂåªÁñóÈ¢ÜÂüüÈù¢‰∏¥ÁöÑÈ´òÊ†áÊ≥®ÊàêÊú¨ÂíåÈ¢ÜÂüüÁâπÂÆöÊ®°ÂºèÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÁ°Æ‰øùÊï∞ÊçÆÈõÜÁöÑÂ§öÊ†∑ÊÄßÂíåÂπ≥Ë°°ÁöÑÁ≤æÁ°ÆÁéá-Âè¨ÂõûÁéáÂ•ñÂä±Êú∫Âà∂ÔºåÂáèÂ∞ë‰∫ÜÊ®°ÂûãÁöÑÂπªËßâÁé∞Ë±°ÔºåÂπ∂ÊèêÈ´ò‰∫ÜÈ¢ÜÂüüË¶ÜÁõñÁéá„ÄÇÊ°à‰æãÁ†îÁ©∂Ëøõ‰∏ÄÊ≠•ËØÅÊòé‰∫ÜÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠Êé®ÁêÜËÉΩÂäõÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14606",
            "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
            "url": "https://huggingface.co/papers/2506.14606",
            "abstract": "A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.",
            "score": 10,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "c414a1f31e0417da",
            "authors": [
                "Ahmed Heakl",
                "Sarim Hashmi",
                "Chaimaa Abi",
                "Celine Lee",
                "Abdulrahman Mahmoud"
            ],
            "affiliations": [
                "Cornell University",
                "MBZUAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14606.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#data",
                    "#science"
                ],
                "emoji": "üîÑ",
                "ru": {
                    "title": "–Ø–ú–ë –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–∏–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç—Ä–∞–Ω—Å–ø–∏–ª—è—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–æ–¥–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ GG (Guaranteed Guess) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–µ—Ä–µ–≤–æ–¥–∞ –∫–æ–¥–∞ —Å –æ–¥–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–∞ –¥—Ä—É–≥—É—é —Å –ø–æ–º–æ—â—å—é –Ø–ú–ë –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Ö –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–≤–æ–¥–∞ (99% –¥–ª—è HumanEval –∏ 49% –¥–ª—è BringupBench) –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø–∞–º—è—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø–ª–∞–Ω–∏—Ä—É—é—Ç –æ—Ç–∫—Ä—ã—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥, –¥–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏."
                },
                "en": {
                    "title": "Efficient ISA Translation with Guaranteed Guess",
                    "desc": "This paper presents a new transpilation pipeline called GG (Guaranteed Guess) that focuses on translating programs between complex (CISC) and reduced (RISC) instruction set architectures (ISAs). By leveraging large language models (LLMs) for generating translation candidates, the pipeline integrates software testing to ensure high correctness and efficiency. The authors demonstrate that their approach achieves over 99% functional correctness on specific benchmarks and outperforms the existing Rosetta 2 framework in terms of runtime speed, energy efficiency, and memory usage. The research aims to enhance the portability of code across different hardware architectures and will provide open-source resources for further exploration in ISA-level code translation."
                },
                "zh": {
                    "title": "È´òÊïàÂáÜÁ°ÆÁöÑISAËΩ¨ËØëÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑISA‰∏≠ÂøÉÁöÑËΩ¨ËØëÁÆ°ÈÅìÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåËΩØ‰ª∂ÊµãËØïÊäÄÊúØÔºåÂÆûÁé∞‰∫ÜÂú®Â§çÊùÇÂíåÁÆÄÂåñÁ°¨‰ª∂Êû∂ÊûÑ‰πãÈó¥ÁöÑÈ´òÊïà‰∏îÊ≠£Á°ÆÁöÑ‰ª£Á†ÅËΩ¨Êç¢„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáLLMÁîüÊàêÂÄôÈÄâÁøªËØëÔºåÂπ∂Â∞ÜÂÖ∂ÂµåÂÖ•ËΩØ‰ª∂ÊµãËØïÊ°ÜÊû∂‰∏≠Ôºå‰ª•ÊèêÈ´òÁøªËØëÁöÑÂèØÈù†ÊÄß„ÄÇÊàë‰ª¨Âú®‰∏§‰∏™‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫ÜËØ•ÊñπÊ≥ïÔºåËææÂà∞‰∫Ü99%ÁöÑÂäüËÉΩÂíåËØ≠‰πâÊ≠£Á°ÆÁéáÔºåÂπ∂‰∏îÂú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑRosetta 2Ê°ÜÊû∂„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Â∞ÜÂºÄÊ∫ê‰ª£Á†Å„ÄÅÊï∞ÊçÆ„ÄÅÊ®°ÂûãÂíåÂü∫ÂáÜÔºå‰ª•Êé®Âä®ISAÁ∫ß‰ª£Á†ÅÁøªËØëÁ†îÁ©∂ÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12860",
            "title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning",
            "url": "https://huggingface.co/papers/2506.12860",
            "abstract": "Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios.",
            "score": 10,
            "issue_id": 4348,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 –∏—é–Ω—è",
                "en": "June 15",
                "zh": "6Êúà15Êó•"
            },
            "hash": "4e8d6c1da3d2fdd1",
            "authors": [
                "Wanlong Liu",
                "Junxiao Xu",
                "Fei Yu",
                "Yukang Lin",
                "Ke Ji",
                "Wenyu Chen",
                "Yan Xu",
                "Yasheng Wang",
                "Lifeng Shang",
                "Benyou Wang"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong, Shenzhen",
                "University of Electronic Science and Technology of China, Chengdu, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12860.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#long_context",
                    "#reasoning",
                    "#low_resource"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "QFFT: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò –≥–∏–±–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - Question-Free Fine-Tuning (QFFT). QFFT –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–∏–µ, —Ç–∞–∫ –∏ –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Å–æ–∫—Ä–∞—â–∞–µ—Ç —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 50%, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º. QFFT —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å —à—É–º–æ–º, –≤–Ω–µ –¥–æ–º–µ–Ω–∞ –∏ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö."
                },
                "en": {
                    "title": "Efficient Reasoning with Question-Free Fine-Tuning",
                    "desc": "This paper introduces Question-Free Fine-Tuning (QFFT), a method that enhances cognitive models by combining short and long chain-of-thought reasoning patterns. QFFT addresses the issue of overthinking in Long Chain-of-Thought models, which can lead to unnecessary complexity in responses. By training models without input questions, QFFT allows them to learn from Long CoT responses while primarily using Short CoT patterns for efficiency. The results show that QFFT significantly reduces response length and performs well across various challenging scenarios, outperforming traditional Supervised Fine-Tuning methods in specific contexts."
                },
                "zh": {
                    "title": "Êó†ÈóÆÂæÆË∞ÉÔºöÈ´òÊïàÈÄÇÂ∫îÁöÑÊé®ÁêÜÊñ∞ÊñπÊ≥ï",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂæÆË∞ÉÊñπÊ≥ïÔºåÁß∞‰∏∫Êó†ÈóÆÂæÆË∞ÉÔºàQFFTÔºâÔºåÊó®Âú®ÊèêÈ´òËÆ§Áü•Ê®°ÂûãÁöÑÊïàÁéáÂíåÈÄÇÂ∫îÊÄß„ÄÇÈÄöËøáÁªìÂêàÁü≠ÈìæÂíåÈïøÈìæÊé®ÁêÜÊ®°ÂºèÔºåQFFTËÉΩÂ§üÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÂáèÂ∞ëÂìçÂ∫îÈïøÂ∫¶„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁü≠ÈìæÊé®ÁêÜÂú®ÁÆÄÂçïÈóÆÈ¢ò‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËÄåÈïøÈìæÊé®ÁêÜÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Êõ¥ÂÖ∑‰ºòÂäø„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåQFFTÂú®Â§ö‰∏™Êï∞Â≠¶Êï∞ÊçÆÈõÜ‰∏äÂπ≥ÂùáÂìçÂ∫îÈïøÂ∫¶ÂáèÂ∞ëË∂ÖËøá50%ÔºåÂπ∂Âú®Âô™Â£∞„ÄÅÂüüÂ§ñÂíå‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14603",
            "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
            "url": "https://huggingface.co/papers/2506.14603",
            "abstract": "Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.",
            "score": 6,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "c235653dff87ea28",
            "authors": [
                "Amirmojtaba Sabour",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14603.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#diffusion",
                    "#small_models"
                ],
                "emoji": "üåä",
                "ru": {
                    "title": "Flow maps: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'flow maps'. –≠—Ç–∞ —Ç–µ—Ö–Ω–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–µ–¥–∏–Ω—è—Ç—å –ª—é–±—ã–µ –¥–≤–∞ —É—Ä–æ–≤–Ω—è —à—É–º–∞ –∑–∞ –æ–¥–∏–Ω —à–∞–≥, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —à–∞–≥–æ–≤. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Ü–µ–ª–µ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è flow maps. –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤."
                },
                "en": {
                    "title": "Flow Maps: Efficient Few-Step Generative Modeling",
                    "desc": "This paper introduces flow maps, a new approach in generative modeling that connects different noise levels in a single step, allowing for efficient image and text-to-image generation. Unlike traditional diffusion and consistency models, which require many sampling steps and degrade in performance with increased steps, flow maps maintain effectiveness across all step counts. The authors propose two continuous-time training objectives and novel techniques that enhance the training of flow maps, including autoguidance and adversarial finetuning. The results demonstrate that their method, called Align Your Flow, achieves state-of-the-art performance in few-step generation tasks on various benchmarks, outperforming existing models in both image and text-conditioned synthesis."
                },
                "zh": {
                    "title": "ÊµÅÂõæÊ®°ÂûãÔºöÈ´òÊïàÁöÑÂ∞ëÊ≠•È™§ÁîüÊàêÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊµÅÂõæÊ®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÂõæÂÉèÂíåÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊïàÁéá„ÄÇÊµÅÂõæÈÄöËøáÂú®Âçï‰∏ÄÊ≠•È™§‰∏≠ËøûÊé•‰ªªÊÑè‰∏§‰∏™Âô™Â£∞Ê∞¥Âπ≥ÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊâ©Êï£ÂíåÊµÅÊ®°ÂûãÂú®Â§öÊ≠•È™§ÈááÊ†∑‰∏≠ÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÊñ∞ÁöÑËøûÁª≠Êó∂Èó¥ÁõÆÊ†áÂíåËÆ≠ÁªÉÊäÄÊúØÔºåËøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÊµÅÂõæÁöÑËÆ≠ÁªÉËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊµÅÂõæÊ®°ÂûãÂú®ÂõæÂÉèÁîüÊàêÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â∞ëÊ≠•È™§ÁîüÊàê‰ªªÂä°‰∏≠ÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13977",
            "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios",
            "url": "https://huggingface.co/papers/2506.13977",
            "abstract": "A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at https://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.",
            "score": 5,
            "issue_id": 4351,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 –∏—é–Ω—è",
                "en": "June 11",
                "zh": "6Êúà11Êó•"
            },
            "hash": "d72fbcfec0e28e92",
            "authors": [
                "Shiting Huang",
                "Zhen Fang",
                "Zehui Chen",
                "Siyu Yuan",
                "Junjie Ye",
                "Yu Zeng",
                "Lin Chen",
                "Qi Mao",
                "Feng Zhao"
            ],
            "affiliations": [
                "Communication University of China",
                "Fudan University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13977.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "üõ†Ô∏è",
                "ru": {
                    "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏",
                    "desc": "CRITICTOOL - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—à–∏–±–æ–∫ –≤–æ –≤—Ä–µ–º—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –û–Ω –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∏–ø—ã –æ—à–∏–±–æ–∫, –≤–æ–∑–Ω–∏–∫–∞—é—â–∏—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–π, –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. CRITICTOOL –ø—Ä–æ–≤–æ–¥–∏—Ç –æ–±—à–∏—Ä–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –≠—Ç–æ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ–±–ª–∞—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Enhancing LLM Robustness with CRITICTOOL",
                    "desc": "This paper introduces CRITICTOOL, a benchmark designed to assess and improve the robustness of large language models (LLMs) when using external tools. It identifies and categorizes various errors that can occur during the function-calling process, especially as tasks become more complex. The benchmark employs an innovative evolutionary strategy for dataset construction, ensuring a wide range of tool-use errors that mimic real-world challenges. Through extensive experiments, the authors demonstrate the effectiveness of CRITICTOOL in enhancing the error-handling capabilities of LLMs and provide insights into their tool reflection abilities."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ∑•ÂÖ∑‰ΩøÁî®ÁöÑÈ≤ÅÊ£íÊÄß",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜCRITICTOOLÔºå‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØïÂ∑•ÂÖ∑ÔºåÁî®‰∫éËØÑ‰º∞ÂíåÂ¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰ΩøÁî®Â∑•ÂÖ∑Êó∂Â§ÑÁêÜÈîôËØØÁöÑËÉΩÂäõ„ÄÇÈöèÁùÄ‰ªªÂä°ÁöÑÂ§çÊùÇÊÄßÂ¢ûÂä†ÔºåÂ∑•ÂÖ∑‰ΩøÁî®ËøáÁ®ã‰∏≠ÂèØËÉΩ‰ºöÂá∫Áé∞ÂêÑÁßçÊÑèÂ§ñÈîôËØØÔºåÂõ†Ê≠§ÊúâÊïàÂ§ÑÁêÜËøô‰∫õÈîôËØØÊàê‰∏∫‰∫Ü‰∏Ä‰∏™ÈáçË¶ÅÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜÂú®Â§ö‰∏™Á´û‰∫âÊÄßÂ∑•ÂÖ∑ËØÑ‰º∞Âü∫ÂáÜ‰∏≠ÈÅáÂà∞ÁöÑÈîôËØØÁ±ªÂûãÔºåÂπ∂Âü∫‰∫éÊ≠§ÊûÑÂª∫‰∫ÜCRITICTOOLÔºå‰∏ìÊ≥®‰∫éÂ∑•ÂÖ∑Â≠¶‰π†ÁöÑÊâπÂà§ÊÄßËØÑ‰º∞„ÄÇÈÄöËøáÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÊàë‰ª¨È™åËØÅ‰∫ÜËØ•Âü∫ÂáÜÁ≠ñÁï•ÁöÑÊúâÊïàÊÄßÔºåÂπ∂Êèê‰æõ‰∫ÜÂØπ‰∏çÂêåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ∑•ÂÖ∑ÂèçÂ∫îËÉΩÂäõÁöÑÊ∑±ÂÖ•ÂàÜÊûê„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14002",
            "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
            "url": "https://huggingface.co/papers/2506.14002",
            "abstract": "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically prove that this algorithm correctly recovers all monosemantic features when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.",
            "score": 4,
            "issue_id": 4347,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "f27daadffcce7200",
            "authors": [
                "Siyu Chen",
                "Heejune Sheen",
                "Xuyuan Xiong",
                "Tianhao Wang",
                "Zhuoran Yang"
            ],
            "affiliations": [
                "Antai College of Economics and Management, Shanghai Jiao Tong University",
                "Department of Statistics and Data Science, Yale University",
                "Toyota Technological Institute at Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14002.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#interpretability",
                    "#math",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü—Ä–æ—Ä—ã–≤ –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Group Bias Adaptation –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ (Sparse Autoencoders). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å –º–æ–Ω–æ—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Large Language Models) —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –≥–∞—Ä–∞–Ω—Ç–∏—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤–æ–µ –ø–æ–Ω—è—Ç–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –º–æ–¥–µ–ª–∏—Ä—É—è –ø–æ–ª–∏—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–∞–∫ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ —Å–º–µ—Å–∏ –º–æ–Ω–æ—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –Ω–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –¥–æ 1,5 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Enhancing Feature Recovery in Language Models with Group Bias Adaptation",
                    "desc": "This paper introduces a new method called Group Bias Adaptation (GBA) to improve Sparse Autoencoders (SAEs) for extracting clear features from Large Language Models (LLMs). The authors address the limitations of existing SAE training methods, which often lack solid mathematical backing and can be unstable. They propose a statistical framework that models complex features as combinations of simpler, clear concepts, ensuring better feature recovery. The new training algorithm not only provides theoretical guarantees for recovering these features but also shows better performance compared to traditional methods when tested on large models."
                },
                "zh": {
                    "title": "Áæ§‰ΩìÂÅèÂ∑ÆÈÄÇÂ∫îÔºöÊèêÂçáÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÁöÑÂçï‰πâÁâπÂæÅÊÅ¢Â§çËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁªüËÆ°Ê°ÜÊû∂ÂíåËÆ≠ÁªÉÁÆóÊ≥ïÔºåÁß∞‰∏∫Áæ§‰ΩìÂÅèÂ∑ÆÈÄÇÂ∫îÔºàGroup Bias AdaptationÔºâÔºåÊó®Âú®Â¢ûÂº∫Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSparse AutoencodersÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂçï‰πâÁâπÂæÅÊÅ¢Â§çËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ËÆ≠ÁªÉÁÆóÊ≥ïÁº∫‰πè‰∏•Ê†ºÁöÑÊï∞Â≠¶‰øùËØÅÔºåÂπ∂‰∏îÂú®Ë∂ÖÂèÇÊï∞ÊïèÊÑüÊÄßÂíå‰∏çÁ®≥ÂÆöÊÄßÊñπÈù¢Â≠òÂú®ÂÆûÈôÖÈôêÂà∂„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ•ÁâπÂæÅÂèØËØÜÂà´ÊÄßÁöÑÊñ∞Ê¶ÇÂøµÔºåËß£ÂÜ≥‰∫ÜÁâπÂæÅÊÅ¢Â§çÈóÆÈ¢òÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂÅèÂ∑ÆÈÄÇÂ∫îÁöÑÊñ∞ËÆ≠ÁªÉÁÆóÊ≥ï„ÄÇÁêÜËÆ∫ËØÅÊòéËØ•ÁÆóÊ≥ïËÉΩÂ§üÂú®ÁâπÂÆöÁªüËÆ°Ê®°Âûã‰∏ãÊ≠£Á°ÆÊÅ¢Â§çÊâÄÊúâÂçï‰πâÁâπÂæÅÔºå‰ªéËÄå‰∏∫Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÁöÑËÆ≠ÁªÉÊèê‰æõ‰∫ÜÁêÜËÆ∫ÊîØÊåÅ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13651",
            "title": "xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations",
            "url": "https://huggingface.co/papers/2506.13651",
            "abstract": "We introduce xbench, a dynamic, profession-aligned evaluation suite designed to bridge the gap between AI agent capabilities and real-world productivity. While existing benchmarks often focus on isolated technical skills, they may not accurately reflect the economic value agents deliver in professional settings. To address this, xbench targets commercially significant domains with evaluation tasks defined by industry professionals. Our framework creates metrics that strongly correlate with productivity value, enables prediction of Technology-Market Fit (TMF), and facilitates tracking of product capabilities over time. As our initial implementations, we present two benchmarks: Recruitment and Marketing. For Recruitment, we collect 50 tasks from real-world headhunting business scenarios to evaluate agents' abilities in company mapping, information retrieval, and talent sourcing. For Marketing, we assess agents' ability to match influencers with advertiser needs, evaluating their performance across 50 advertiser requirements using a curated pool of 836 candidate influencers. We present initial evaluation results for leading contemporary agents, establishing a baseline for these professional domains. Our continuously updated evalsets and evaluations are available at https://xbench.org.",
            "score": 4,
            "issue_id": 4351,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "ce4f94d367671b84",
            "authors": [
                "Kaiyuan Chen",
                "Yixin Ren",
                "Yang Liu",
                "Xiaobo Hu",
                "Haotong Tian",
                "Tianbao Xie",
                "Fangfu Liu",
                "Haoye Zhang",
                "Hongzhang Liu",
                "Yuan Gong",
                "Chen Sun",
                "Han Hou",
                "Hui Yang",
                "James Pan",
                "Jianan Lou",
                "Jiayi Mao",
                "Jizheng Liu",
                "Jinpeng Li",
                "Kangyi Liu",
                "Kenkun Liu",
                "Rui Wang",
                "Run Li",
                "Tong Niu",
                "Wenlong Zhang",
                "Wenqi Yan",
                "Xuanzheng Wang",
                "Yuchen Zhang",
                "Yi-Hsin Hung",
                "Yuan Jiang",
                "Zexuan Liu",
                "Zihan Yin",
                "Zijian Ma",
                "Zhiwen Mo"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Fudan University",
                "Imperial College London",
                "Massachusetts Institute of Technology",
                "National University of Singapore",
                "Peking University",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "The Chinese University of Hong Kong (Shenzhen)",
                "The Ohio State University",
                "Tsinghua University",
                "University of Chinese Academy of Sciences",
                "University of Oxford",
                "University of Pennsylvania",
                "University of Science and Technology of China",
                "University of Sydney",
                "University of Toronto",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13651.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "üìä",
                "ru": {
                    "title": "xbench: –æ—Ü–µ–Ω–∫–∞ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω xbench - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, xbench —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö —Å –∑–∞–¥–∞—á–∞–º–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∞–º–∏ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Å–æ–∑–¥–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏, –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ —Å –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é, –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Ä—ã–Ω–∫—É –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–≤–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞: –¥–ª—è —Ä–µ–∫—Ä—É—Ç–∏–Ω–≥–∞ –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-—Å—Ü–µ–Ω–∞—Ä–∏—è—Ö."
                },
                "en": {
                    "title": "Bridging AI Performance with Real-World Productivity",
                    "desc": "The paper introduces xbench, a new evaluation suite aimed at assessing AI agents in real-world professional contexts. Unlike traditional benchmarks that focus on isolated skills, xbench emphasizes the economic impact of AI agents in industries like recruitment and marketing. It includes tasks defined by industry experts to ensure relevance and creates metrics that correlate with productivity value. The initial benchmarks evaluate agents' performance in real-world scenarios, providing a baseline for future assessments and tracking improvements over time."
                },
                "zh": {
                    "title": "xbenchÔºöËøûÊé•AIËÉΩÂäõ‰∏éÁúüÂÆûÁîü‰∫ßÂäõÁöÑÊ°•Ê¢Å",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜxbenchÔºåËøôÊòØ‰∏Ä‰∏™Âä®ÊÄÅÁöÑ„ÄÅ‰∏éËÅå‰∏öÁõ∏ÂÖ≥ÁöÑËØÑ‰º∞Â•ó‰ª∂ÔºåÊó®Âú®Âº•Âêà‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜËÉΩÂäõ‰∏éÁé∞ÂÆû‰∏ñÁïåÁîü‰∫ßÂäõ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØïÈÄöÂ∏∏ÂÖ≥Ê≥®Â≠§Á´ãÁöÑÊäÄÊúØÊäÄËÉΩÔºå‰ΩÜÂèØËÉΩÊó†Ê≥ïÂáÜÁ°ÆÂèçÊò†‰ª£ÁêÜÂú®‰∏ì‰∏öÁéØÂ¢É‰∏≠ÊâÄÂ∏¶Êù•ÁöÑÁªèÊµé‰ª∑ÂÄº„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåxbenchÈíàÂØπÂïÜ‰∏ö‰∏äÈáçË¶ÅÁöÑÈ¢ÜÂüüÔºåËØÑ‰º∞‰ªªÂä°Áî±Ë°å‰∏ö‰∏ì‰∏ö‰∫∫Â£´ÂÆö‰πâ„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÂàõÂª∫‰∫Ü‰∏éÁîü‰∫ßÂäõ‰ª∑ÂÄºÈ´òÂ∫¶Áõ∏ÂÖ≥ÁöÑÊåáÊ†áÔºåËÉΩÂ§üÈ¢ÑÊµãÊäÄÊúØÂ∏ÇÂú∫Â•ëÂêàÂ∫¶ÔºàTMFÔºâÔºåÂπ∂‰æø‰∫éË∑üË∏™‰∫ßÂìÅËÉΩÂäõÁöÑÂèòÂåñ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10100",
            "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2506.10100",
            "abstract": "EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.",
            "score": 4,
            "issue_id": 4348,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 –∏—é–Ω—è",
                "en": "June 11",
                "zh": "6Êúà11Êó•"
            },
            "hash": "6a877c4c5f1d1f72",
            "authors": [
                "Yantai Yang",
                "Yuhao Wang",
                "Zichen Wen",
                "Luo Zhongwei",
                "Chang Zou",
                "Zhipeng Zhang",
                "Chuan Wen",
                "Linfeng Zhang"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "School of Artificial Intelligence, Shanghai Jiao Tong University",
                "University of Electronic Science and Technology of China",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10100.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#architecture",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "EfficientVLA: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π VLA –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "EfficientVLA - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–µ–π Vision-Language-Action (VLA). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: –æ–±—Ä–µ–∑–∞–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤ –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥—É–ª–µ, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –æ—Ç–±–æ—Ä–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –≥–æ–ª–æ–≤–µ –¥–µ–π—Å—Ç–≤–∏–π. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ EfficientVLA –∫ –º–æ–¥–µ–ª–∏ CogACT –ø–æ–∑–≤–æ–ª–∏–ª–æ –¥–æ—Å—Ç–∏—á—å —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –≤ 1,93 —Ä–∞–∑–∞ –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è FLOP –Ω–∞ 71,1% –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É—Å—Ç—Ä–∞–Ω–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –ø–∞–º—è—Ç—å –±–∞—Ä—å–µ—Ä—ã –≤ –º–æ–¥–µ–ª—è—Ö VLA."
                },
                "en": {
                    "title": "Accelerating VLA Models with EfficientVLA",
                    "desc": "EfficientVLA is a framework designed to speed up Vision-Language-Action (VLA) models by addressing their high computational and memory requirements. It achieves this by pruning unnecessary language layers, optimizing the selection of visual tokens, and caching important features during the action generation process. This approach not only reduces the overall processing time but also minimizes the number of floating-point operations (FLOPs) needed for inference. As a result, EfficientVLA significantly enhances the efficiency of VLA models while maintaining a high level of performance."
                },
                "zh": {
                    "title": "È´òÊïàÂä†ÈÄüËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑËß£ÂÜ≥ÊñπÊ°à",
                    "desc": "EfficientVLAÊòØ‰∏ÄÁßçÂä†ÈÄüËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑÊ°ÜÊû∂ÔºåÈÄöËøá‰øÆÂâ™ËØ≠Ë®ÄÂ±Ç„ÄÅ‰ºòÂåñËßÜËßâÊ†áËÆ∞ÈÄâÊã©ÂíåÁºìÂ≠ò‰∏≠Èó¥ÁâπÂæÅÊù•ÊèêÈ´òÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÁ≥ªÁªüÊÄßÂú∞Ê∂àÈô§‰∫ÜËÆ°ÁÆóÂíåÂÜÖÂ≠òÁì∂È¢àÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÂä†ÈÄüÊñπÊ≥ïÊó†Ê≥ïÂÖ®Èù¢Â∫îÂØπÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂàÜÊûêÂ±ÇÈó¥ÂÜó‰ΩôÔºåEfficientVLAÂéªÈô§‰∫ÜÂäüËÉΩ‰∏çÈáçË¶ÅÁöÑËØ≠Ë®ÄÊ®°ÂùóÂ±ÇÔºåÂπ∂ÈááÁî®‰ªªÂä°ÊÑüÁü•Á≠ñÁï•‰ºòÂåñËßÜËßâÂ§ÑÁêÜË∑ØÂæÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∫îÁî®EfficientVLAÂêéÔºåÊ†áÂáÜVLAÊ®°ÂûãCogACTÁöÑÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü1.93ÂÄçÔºåFLOPsÂáèÂ∞ëËá≥28.9%ÔºåÊàêÂäüÁéá‰ªÖ‰∏ãÈôç0.6%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10038",
            "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
            "url": "https://huggingface.co/papers/2506.10038",
            "abstract": "Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.",
            "score": 3,
            "issue_id": 4349,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 –∏—é–Ω—è",
                "en": "June 10",
                "zh": "6Êúà10Êó•"
            },
            "hash": "f746e9dd50fb7b78",
            "authors": [
                "Giannis Daras",
                "Adrian Rodriguez-Munoz",
                "Adam Klivans",
                "Antonio Torralba",
                "Constantinos Daskalakis"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10038.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#dataset",
                    "#synthetic",
                    "#diffusion",
                    "#data"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª—å–∑—ã –∏–∑ —à—É–º–∞: —É–ª—É—á—à–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Ambient Diffusion Omni, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –æ—Ç–±—Ä–∞–∫–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Å–≤–æ–π—Å—Ç–≤–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç–∞–∫–∏–µ –∫–∞–∫ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Å—Ç–µ–ø–µ–Ω–Ω–æ–π –∑–∞–∫–æ–Ω –∑–∞—Ç—É—Ö–∞–Ω–∏—è –∏ –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å. –§—Ä–µ–π–º–≤–æ—Ä–∫ —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è FID –Ω–∞ ImageNet –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ —Å–º–µ—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Unlocking Potential: Enhancing Diffusion Models with Low-Quality Images",
                    "desc": "The Ambient Diffusion Omni framework enhances diffusion models by effectively utilizing low-quality images, which are often overlooked. It demonstrates that these lower-quality images can significantly improve model performance on tasks like text-to-image generation. The framework leverages natural image properties, such as spectral power law decay and locality, to extract valuable signals during training. By validating its approach with various synthetic corruptions, the framework achieves state-of-the-art results in ImageNet FID, showcasing improved image quality and diversity."
                },
                "zh": {
                    "title": "Âà©Áî®‰ΩéË¥®ÈáèÂõæÂÉèÊèêÂçáÊâ©Êï£Ê®°ÂûãÁöÑË¥®Èáè",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ambient Diffusion OmniÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®‰ΩéË¥®ÈáèÂõæÂÉèÊù•ÊèêÂçáÊâ©Êï£Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄöÂ∏∏Ë¢´‰∏¢ÂºÉÁöÑ‰ΩéË¥®ÈáèÂõæÂÉèÂÆûÈôÖ‰∏äÂÖ∑ÊúâÂæàÂ§ßÁöÑ‰ª∑ÂÄºÔºåÂèØ‰ª•ÊîπÂñÑÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÊûú„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®Ëá™ÁÑ∂ÂõæÂÉèÁöÑ‰∏§‰∏™ÁâπÊÄß‚Äî‚ÄîË∞±ÂäüÁéáÊ≥ïÂàôË°∞ÂáèÂíåÂ±ÄÈÉ®ÊÄßÔºåÊàêÂäüÂú∞‰ªéÂêàÊàêÊ®°Á≥ä„ÄÅJPEGÂéãÁº©ÂíåËøêÂä®Ê®°Á≥äÁöÑÂõæÂÉè‰∏≠ÊèêÂèñ‰ø°Âè∑„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Âú®ImageNet FID‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÂπ∂ÊòæËëóÊèêÈ´ò‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ®°ÂûãÁöÑÂõæÂÉèË¥®ÈáèÂíåÂ§öÊ†∑ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05336",
            "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
            "url": "https://huggingface.co/papers/2506.05336",
            "abstract": "VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at https://github.com/mbzuai-oryx/VideoMolmo.",
            "score": 3,
            "issue_id": 4348,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 –∏—é–Ω—è",
                "en": "June 5",
                "zh": "6Êúà5Êó•"
            },
            "hash": "def5ee56ea3b6157",
            "authors": [
                "Ghazi Shazan Ahmad",
                "Ahmed Heakl",
                "Hanan Gani",
                "Abdelrahman Shaker",
                "Zhiqiang Shen",
                "Ranjay Krishna",
                "Fahad Shahbaz Khan",
                "Salman Khan"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Australian National University",
                "Link√∂ping University",
                "Mohamed Bin Zayed University of Artificial Intelligence",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05336.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#interpretability",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–¢–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò",
                    "desc": "VideoMolmo - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–æ–¥—É–ª—å —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç SAM2 –¥–ª—è –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ—á–µ–∫, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–∞—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö. VideoMolmo –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ —É–∫–∞–∑–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞."
                },
                "en": {
                    "title": "Enhancing Spatio-Temporal Reasoning with VideoMolmo",
                    "desc": "VideoMolmo is a multimodal model designed to improve spatio-temporal pointing accuracy and reasoning in various real-world applications. It combines a temporal attention mechanism with a novel mask fusion technique called SAM2, which enhances the coherence of video sequences. By generating precise pointing coordinates through a large language model and then refining them with a mask-fusion module, VideoMolmo simplifies the task and improves interpretability. The model is evaluated on a newly curated dataset and a challenging benchmark, demonstrating significant advancements over existing video-based approaches."
                },
                "zh": {
                    "title": "VideoMolmoÔºöÊèêÂçáÊó∂Á©∫ÊåáÂêë‰∏éÊé®ÁêÜËÉΩÂäõÁöÑÂ§öÊ®°ÊÄÅÊ®°Âûã",
                    "desc": "VideoMolmoÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÁªìÂêà‰∫ÜÊó∂Èó¥Ê≥®ÊÑèÊú∫Âà∂ÂíåSAM2ËøõË°åÊé©ËÜúËûçÂêàÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊó∂Á©∫ÊåáÂêëÁöÑÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê®°Âûã‰∏ì‰∏∫Âü∫‰∫éÊñáÊú¨ÊèèËø∞ÁöÑÁªÜÁ≤íÂ∫¶Êó∂Á©∫ÊåáÂêëËÄåËÆæËÆ°ÔºåËÉΩÂ§üÂú®‰∏çÂêåÁöÑÁúüÂÆûÂú∫ÊôØ‰∏≠ËøõË°åÁ≤æÁ°Æ‰∫§‰∫í„ÄÇÈÄöËøáÂºïÂÖ•Êó∂Èó¥Ê®°ÂùóÂíåÂèåÂêëÁÇπ‰º†Êí≠ÁöÑÊé©ËÜúËûçÂêàÁÆ°ÈÅìÔºåVideoMolmoÁ°Æ‰øù‰∫ÜËßÜÈ¢ëÂ∫èÂàóÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíåËøûË¥ØÊÄß„ÄÇÊàë‰ª¨ËøòÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´72,000‰∏™ËßÜÈ¢ë-Â≠óÂπïÂØπÁöÑÊï∞ÊçÆÈõÜÔºå‰ª•ËØÑ‰º∞Ê®°ÂûãÂú®Â§öÁßçÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14755",
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "url": "https://huggingface.co/papers/2506.14755",
            "abstract": "LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as \"invalid thinking\" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.",
            "score": 1,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "837a56d067dd6e74",
            "authors": [
                "Zhengxiang Cheng",
                "Dongping Chen",
                "Mingyang Fu",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14755.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "‚úÇÔ∏è",
                "ru": {
                    "title": "LC-R1: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "LC-R1 - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏. –û–Ω –Ω–∞—Ü–µ–ª–µ–Ω –Ω–∞ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≤—ã–≤–æ–¥–∞—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. LC-R1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –Ω–∞–≥—Ä–∞–¥ –∑–∞ –¥–ª–∏–Ω—É –∏ —Å–∂–∞—Ç–∏–µ –≤ —Ä–∞–º–∫–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥—Ä—É–ø–ø–æ–≤–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (GRPO). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 50% –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ –Ω–∞ 2%."
                },
                "en": {
                    "title": "Streamlining Reasoning: LC-R1 for Efficient Large Models",
                    "desc": "The paper introduces LC-R1, a post-training method aimed at improving Large Reasoning Models (LRMs) by reducing unnecessary reasoning while maintaining accuracy. It identifies 'invalid thinking' as a key issue where models redundantly verify correct answers, leading to verbosity. To combat this, the authors propose two principles: Brevity, which focuses on cutting out redundant reasoning, and Sufficiency, which ensures essential reasoning steps are retained. Through experiments, LC-R1 demonstrates a significant reduction in reasoning sequence length by about 50% with only a slight accuracy drop of around 2%, showcasing an effective balance between compression and performance."
                },
                "zh": {
                    "title": "ÁÆÄÂåñÊé®ÁêÜÔºåÊèêÂçáÊïàÁéáÔºÅ",
                    "desc": "LC-R1ÊòØ‰∏ÄÁßçÂêéËÆ≠ÁªÉÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÁÆÄÊ¥ÅÊÄßÂíåÂÖÖÂàÜÊÄßÂéüÂàôÊù•ÂáèÂ∞ëÂ§ßÂûãÊé®ÁêÜÊ®°Âûã‰∏≠ÁöÑ‰∏çÂøÖË¶ÅÊé®ÁêÜÔºåÂêåÊó∂‰øùÊåÅËæÉÂ∞èÁöÑÂáÜÁ°ÆÊÄßÊçüÂ§±„ÄÇËØ•ÊñπÊ≥ïËØÜÂà´Âá∫Ê®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Â≠òÂú®ÁöÑ‚ÄúÊó†ÊïàÊÄùÁª¥‚ÄùÈóÆÈ¢òÔºåÂç≥Ê®°ÂûãÂú®ÂæóÂá∫Ê≠£Á°ÆÁ≠îÊ°àÂêé‰ªçÁÑ∂ÂèçÂ§çÊ£ÄÊü•„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏Ä‰ΩéÊïàÈóÆÈ¢òÔºåLC-R1ÊèêÂá∫‰∫Ü‰∏§‰∏™Êñ∞ÂéüÂàôÔºöÁÆÄÊ¥ÅÊÄßÔºåÂº∫Ë∞ÉÊ∂àÈô§ÂÜó‰ΩôÔºõÂÖÖÂàÜÊÄßÔºåÁ°Æ‰øùÂÖ≥ÈîÆÊé®ÁêÜÊ≠•È™§Âæó‰ª•‰øùÁïô„ÄÇÈÄöËøáÂØπÂ§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÁöÑÂπøÊ≥õÂÆûÈ™åÔºåLC-R1ÂÆûÁé∞‰∫ÜÂ∫èÂàóÈïøÂ∫¶ÁöÑÊòæËëóÂáèÂ∞ëÔºàÁ∫¶50%ÔºâÔºåËÄåÂáÜÁ°ÆÊÄß‰ªÖ‰∏ãÈôçÁ∫¶2%ÔºåÂú®È´òÂéãÁº©ÁéáÂíåÂáÜÁ°ÆÊÄß‰πãÈó¥ËææÊàê‰∫ÜËâØÂ•ΩÁöÑÂπ≥Ë°°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14731",
            "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
            "url": "https://huggingface.co/papers/2506.14731",
            "abstract": "Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.",
            "score": 1,
            "issue_id": 4350,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "7c1c5a66d6e8f898",
            "authors": [
                "Ring Team",
                "Bin Hu",
                "Cai Chen",
                "Deng Zhao",
                "Ding Liu",
                "Dingnan Jin",
                "Feng Zhu",
                "Hao Dai",
                "Hongzhi Luan",
                "Jia Guo",
                "Jiaming Liu",
                "Jiewei Wu",
                "Jun Mei",
                "Jun Zhou",
                "Junbo Zhao",
                "Junwu Xiong",
                "Kaihong Zhang",
                "Kuan Xu",
                "Lei Liang",
                "Liang Jiang",
                "Liangcheng Fu",
                "Longfei Zheng",
                "Qiang Gao",
                "Qing Cui",
                "Quan Wan",
                "Shaomian Zheng",
                "Shuaicheng Li",
                "Tongkai Yang",
                "Wang Ren",
                "Xiaodong Yan",
                "Xiaopei Wan",
                "Xiaoyun Feng",
                "Xin Zhao",
                "Xinxing Yang",
                "Xinyu Kong",
                "Xuemin Yang",
                "Yang Li",
                "Yingting Wu",
                "Yongkang Liu",
                "Zhankai Xu",
                "Zhenduo Zhang",
                "Zhenglei Zhou",
                "Zhenyu Huang",
                "Zhiqiang Zhang",
                "Zihao Wang",
                "Zujie Wen"
            ],
            "affiliations": [
                "Ring Team, Inclusion AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14731.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#open_source",
                    "#rl",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –º–µ–Ω—å—à–∏–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏",
                    "desc": "Ring-lite - —ç—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts (MoE), –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞–∫—Ç–∏–≤–∏—Ä—É—è –ª–∏—à—å —Ç—Ä–µ—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ C3PO –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –û–Ω–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤."
                },
                "en": {
                    "title": "Efficient Reasoning with Fewer Parameters: Introducing Ring-lite",
                    "desc": "Ring-lite is a large language model that uses a Mixture-of-Experts (MoE) architecture combined with reinforcement learning (RL) to enhance reasoning capabilities while minimizing parameter activation. It builds on the Ling-lite model, achieving state-of-the-art performance on various reasoning benchmarks with only a fraction of the parameters activated compared to similar models. The paper introduces a novel training method called Constrained Contextual Computation Policy Optimization (C3PO) to improve stability during RL training and optimize computational efficiency. Additionally, it highlights the importance of selecting distillation checkpoints based on entropy loss for better performance in RL training and proposes a two-stage training approach to manage domain conflicts in mixed datasets."
                },
                "zh": {
                    "title": "È´òÊïàÊé®ÁêÜÔºåÊøÄÊ¥ªÊõ¥Â∞ëÂèÇÊï∞ÁöÑRing-lite",
                    "desc": "Ring-liteÊòØ‰∏ÄÁßçÂü∫‰∫é‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâÊû∂ÊûÑÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËøõË°å‰ºòÂåñÔºå‰ª•ÂÆûÁé∞È´òÊïà‰∏îÁ®≥ÂÅ•ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê®°ÂûãÂú®Ling-liteÁöÑÂü∫Á°Ä‰∏äÊûÑÂª∫ÔºåÂÖ∑Êúâ168‰∫ø‰∏™ÂèÇÊï∞Ôºå‰ΩÜ‰ªÖÊøÄÊ¥ª2.75‰∫ø‰∏™ÂèÇÊï∞ÔºåËÉΩÂ§üÂú®Â§ö‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂü∫ÂáÜÊµãËØï‰∏≠‰∏éÂ∞èËßÑÊ®°ÁöÑÊúÄÂÖàËøõÊé®ÁêÜÊ®°ÂûãÁõ∏ÂåπÈÖç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËÅîÂêàËÆ≠ÁªÉÊµÅÁ®ãÔºåÂ∞ÜËí∏È¶è‰∏éÂº∫ÂåñÂ≠¶‰π†ÁªìÂêàÔºåËß£ÂÜ≥‰∫ÜMoEÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∏≠ÁöÑ‰∏Ä‰∫õÊú™ËÆ∞ÂΩïÁöÑÊåëÊàò„ÄÇÈÄöËøáÂºïÂÖ•ÂèóÈôê‰∏ä‰∏ãÊñáËÆ°ÁÆóÁ≠ñÁï•‰ºòÂåñÔºàC3POÔºâÔºåÊàë‰ª¨ÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÔºåÂπ∂ÈÄöËøáÁÆóÊ≥ï‰∏éÁ≥ªÁªüÁöÑÂÖ±ÂêåËÆæËÆ°ÊñπÊ≥ïÊîπÂñÑ‰∫ÜËÆ°ÁÆóÂêûÂêêÈáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14702",
            "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers",
            "url": "https://huggingface.co/papers/2506.14702",
            "abstract": "A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: \"Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?\" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.",
            "score": 1,
            "issue_id": 4350,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "2fbff0f4b562f92e",
            "authors": [
                "Daniel D'souza",
                "Julia Kreutzer",
                "Adrien Morisot",
                "Ahmet √úst√ºn",
                "Sara Hooker"
            ],
            "affiliations": [
                "Cohere",
                "Cohere Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14702.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#optimization",
                    "#training"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–¢–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ–¥–∫–∏—Ö —Å–ª—É—á–∞–µ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∏ –Ω–µ–¥–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è –∑–∞–¥–∞—á –¥–ª—è —è–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —ç—Ç–∏ –º–∞—Ä–∫–µ—Ä—ã, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ö–≤–æ—Å—Ç–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Optimizing Model Performance for Rare Use Cases",
                    "desc": "This paper addresses the challenge of improving machine learning model performance on rare and underrepresented use cases, often referred to as the long tail. It proposes a method for fine-tuning models that enhances both controllability and performance by automatically inferring generation attributes during inference. The authors introduce a taxonomy of data characteristics to help guide the model's output, allowing for better adaptation to specific tasks without relying heavily on prompt engineering. Their approach demonstrates significant performance improvements, particularly in underrepresented domains, achieving notable gains in generation quality and task-specific evaluations."
                },
                "zh": {
                    "title": "‰ºòÂåñÊ®°Âûã‰ª•ÊèêÂçáÁ®ÄÊúâÁî®‰æãÁöÑÊÄßËÉΩ‰∏éÂèØÊéßÊÄß",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ≥ªÁªüÁöÑÊñπÊ≥ïÔºåÈÄöËøáËá™Âä®Êé®Êñ≠ÁîüÊàêÂ±ûÊÄßÊù•ÂæÆË∞ÉÊ®°ÂûãÔºå‰ª•ÊèêÈ´òÂú®Á®ÄÊúâÂíåÊú™ÂÖÖÂàÜ‰ª£Ë°®ÁöÑÁî®‰æã‰∏äÁöÑÊÄßËÉΩÂíåÂèØÊéßÊÄß„ÄÇÁé∞‰ª£Êú∫Âô®Â≠¶‰π†Èù¢‰∏¥ÁöÑ‰∏Ä‰∏™‰∏ªË¶ÅÊåëÊàòÊòØÂ¶Ç‰ΩïÂú®ÈïøÂ∞æÁâπÂæÅ‰∏äË°®Áé∞ËâØÂ•ΩÔºåÂ∞§ÂÖ∂ÊòØÂú®ËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ËæÉÂ∞ëÂá∫Áé∞ÁöÑÁâπÂæÅ„ÄÇÊàë‰ª¨ÈáçÊñ∞ÂÆ°ËßÜËÆ≠ÁªÉÂíåÊé®ÁêÜÊäÄÊúØ‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºå‰ª•ÊîπÂñÑÈïøÂ∞æÊÄßËÉΩÔºåÂπ∂‰∏∫Áî®Êà∑Êèê‰æõ‰∏ÄÁªÑÂèØÊéßÁöÑÁîüÊàêÂ±ûÊÄß„ÄÇÈÄöËøáÂØπÂü∫Á°ÄÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåÊàë‰ª¨ÂÆûÁé∞‰∫ÜÂú®Êé®ÁêÜÊó∂Ëá™Âä®Êé®Êñ≠Ëøô‰∫õÊ†áËÆ∞Ôºå‰ªéËÄåÂú®Êú™ÂÖÖÂàÜ‰ª£Ë°®ÁöÑÈ¢ÜÂüü‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13901",
            "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations",
            "url": "https://huggingface.co/papers/2506.13901",
            "abstract": "A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.   To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.   Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.",
            "score": 1,
            "issue_id": 4351,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "5492fc2feb0ae2f3",
            "authors": [
                "Abhilekh Borah",
                "Chhavi Sharma",
                "Danush Khanna",
                "Utkarsh Bhatt",
                "Gurpreet Singh",
                "Hasnat Md Abdullah",
                "Raghav Kaushik Ravi",
                "Vinija Jain",
                "Jyoti Patel",
                "Shubham Singh",
                "Vasu Sharma",
                "Arpita Vats",
                "Rahul Raja",
                "Aman Chadha",
                "Amitava Das"
            ],
            "affiliations": [
                "Amazon AI",
                "BITS Goa, India",
                "Evalueserve",
                "IIIT Guwahati, India",
                "IIT Kharagpur, India",
                "LinkedIn",
                "Manipal University Jaipur, India",
                "Meta AI",
                "New York University, USA",
                "Texas A&M University, USA",
                "Vellore Institute of Technology, Chennai, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13901.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#rlhf",
                    "#alignment",
                    "#benchmark",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "AQI: –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (alignment) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - –ò–Ω–¥–µ–∫—Å –ö–∞—á–µ—Å—Ç–≤–∞ –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (AQI). AQI –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏, –æ—Ü–µ–Ω–∏–≤–∞—è –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –∏ —Ñ–∞–ª—å—à–∏–≤–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è. –ú–µ—Ç—Ä–∏–∫–∞ –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–æ–∫—Å–∏-–º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —á–∞—Å—Ç–æ—Ç–∞ –æ—Ç–∫–∞–∑–æ–≤ –∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å. AQI —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å —Ä–∞–Ω–Ω–∏–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –ø–æ–ø—ã—Ç–æ–∫ –æ–±—Ö–æ–¥–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –º–æ–¥–µ–ª–∏, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –Ω–∞–¥–µ–∂–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞—É–¥–∏—Ç–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Ensuring True Alignment in Language Models with AQI",
                    "desc": "The paper introduces a new evaluation metric called the Alignment Quality Index (AQI) to assess the alignment of large language models (LLMs). AQI analyzes latent space activations to measure clustering quality, helping to identify misalignments and instances of alignment faking that traditional behavioral proxies may overlook. By utilizing established clustering metrics like the Davies-Bouldin Score and Dunn Index, AQI provides a more reliable assessment of model safety and alignment in high-stakes applications. The authors also present the LITMUS dataset to support rigorous evaluation, demonstrating AQI's effectiveness in revealing vulnerabilities that other metrics fail to detect."
                },
                "zh": {
                    "title": "ÂØπÈΩêË¥®ÈáèÊåáÊï∞ÔºöÁ°Æ‰øùÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®‰∏éÂèØÈù†",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞ÊåáÊ†áÔºåÁß∞‰∏∫ÂØπÈΩêË¥®ÈáèÊåáÊï∞ÔºàAQIÔºâÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂØπÈΩêÊÉÖÂÜµ„ÄÇAQIÈÄöËøáÂàÜÊûêÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑÊøÄÊ¥ªÂàÜÁ¶ªÔºåÊçïÊçâËÅöÁ±ªË¥®ÈáèÔºå‰ª•Ê£ÄÊµãÊ®°ÂûãÁöÑÈîôËØØÂØπÈΩêÂíå‰º™ÂØπÈΩêÁé∞Ë±°„ÄÇ‰∏éÁé∞ÊúâÁöÑË°å‰∏∫‰ª£ÁêÜÁõ∏ÊØîÔºåAQIËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËØÜÂà´Ê®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÂíåÊΩúÂú®È£éÈô©„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜLITMUSÊï∞ÊçÆÈõÜÔºå‰ª•ÊîØÊåÅÂú®Â§çÊùÇÊù°‰ª∂‰∏ãÁöÑÁ®≥ÂÅ•ËØÑ‰º∞ÔºåÂπ∂Â±ïÁ§∫‰∫ÜAQI‰∏éÂ§ñÈÉ®ËØÑÂÆ°ËÄÖÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13599",
            "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
            "url": "https://huggingface.co/papers/2506.13599",
            "abstract": "CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose CityGPT-Powered Agentic framework for Mobility Simulation (CAMS), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. CAMS comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that CAMS achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, CAMS generates more realistic and plausible trajectories. In general, CAMS establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.",
            "score": 1,
            "issue_id": 4348,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "0b0a6282d1310e1b",
            "authors": [
                "Yuwei Du",
                "Jie Feng",
                "Jian Yuan",
                "Yong Li"
            ],
            "affiliations": [
                "Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13599.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#synthetic",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "üèôÔ∏è",
                "ru": {
                    "title": "–£–º–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥—Å–∫–æ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò",
                    "desc": "CAMS (CityGPT-Powered Agentic framework for Mobility Simulation) - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤ –≥–æ—Ä–æ–¥—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –≥–æ—Ä–æ–¥—Å–∫–æ–π —Å—Ä–µ–¥—ã –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –∏ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–µ—Ä–µ–¥–≤–∏–∂–µ–Ω–∏—è. CAMS –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–¥—É–ª—è: MobExtractor –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —à–∞–±–ª–æ–Ω–æ–≤ –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏, GeoGenerator –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ—á–µ–∫, –∏ TrajEnhancer –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CAMS –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –±–æ–ª–µ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Revolutionizing Urban Mobility Simulation with CAMS",
                    "desc": "CAMS introduces a novel framework that combines agent-based modeling with large language models to enhance the simulation of human mobility in urban environments. It addresses the limitations of traditional methods by integrating individual and collective mobility patterns, allowing for more realistic trajectory generation. The framework consists of three main components: MobExtractor for mobility pattern extraction, GeoGenerator for urban geospatial knowledge generation, and TrajEnhancer for trajectory refinement. Experiments demonstrate that CAMS outperforms existing approaches by generating plausible mobility trajectories without needing external geospatial data."
                },
                "zh": {
                    "title": "ÂüéÂ∏ÇÁßªÂä®Ê®°ÊãüÁöÑÊñ∞ËåÉÂºè",
                    "desc": "CAMSÊòØ‰∏Ä‰∏™ÁªìÂêà‰∫Ü‰ª£ÁêÜÊ°ÜÊû∂ÂíåÂüéÂ∏ÇÁü•ËØÜÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÁî®‰∫éÊõ¥ÁúüÂÆûÂú∞Ê®°Êãü‰∫∫Á±ªÁöÑÁßªÂä®Ë°å‰∏∫„ÄÇÂÆÉÈÄöËøá‰∏â‰∏™Ê†∏ÂøÉÊ®°ÂùóÊù•ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÔºöMobExtractorÊèêÂèñÂíåÂêàÊàêÁî®Êà∑ÁöÑÁßªÂä®Ê®°ÂºèÔºåGeoGeneratorÁîüÊàêËÄÉËôëÈõÜ‰ΩìÁü•ËØÜÁöÑÂüéÂ∏ÇÂú∞ÁêÜ‰ø°ÊÅØÔºåTrajEnhancerÊ†πÊçÆÁßªÂä®Ê®°ÂºèÁîüÊàêÁ¨¶ÂêàÁúüÂÆûÂÅèÂ•ΩÁöÑËΩ®Ëøπ„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåCAMSÂú®‰∏ç‰æùËµñÂ§ñÈÉ®Âú∞ÁêÜ‰ø°ÊÅØÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Âª∫Ê®°‰∏™‰ΩìÂíåÈõÜ‰ΩìÁöÑÁßªÂä®Ê®°Âºè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCAMSÁîüÊàêÁöÑËΩ®ËøπÊõ¥Âä†ÁúüÂÆûÂèØ‰ø°ÔºåÂºÄÂàõ‰∫Ü‰∫∫Á±ªÁßªÂä®Ê®°ÊãüÁöÑÊñ∞ËåÉÂºè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13387",
            "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
            "url": "https://huggingface.co/papers/2506.13387",
            "abstract": "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2M's great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: https://github.com/BeileiCui/TR2M)",
            "score": 0,
            "issue_id": 4347,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "6d0fc497ae4dcfd0",
            "authors": [
                "Beilei Cui",
                "Yiming Huang",
                "Long Bai",
                "Hongliang Ren"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13387.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
                    "desc": "TR2M - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤ –º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. TR2M –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, —Ç–∞–∫ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –¥–≤–µ –∫–∞—Ä—Ç—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤ –º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –Ω–∞ –∑–Ω–∞–∫–æ–º—ã—Ö, —Ç–∞–∫ –∏ –Ω–∞ –Ω–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞–∑—ã–≤–∞—è –±–æ–ª—å—à–æ–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤ –ø–æ–ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –≥–ª—É–±–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Transforming Relative Depth to Metric Depth with TR2M",
                    "desc": "The paper introduces TR2M, a framework that effectively converts relative depth information into metric depth using multimodal inputs, specifically images and text. It addresses the limitations of existing monocular depth estimation methods by combining the strengths of metric and relative depth estimation. TR2M employs cross-modality attention to enhance feature fusion and utilizes contrastive learning to improve scale alignment. The framework demonstrates strong performance across various datasets, including impressive zero-shot capabilities on unseen data, showcasing its versatility and effectiveness in depth estimation tasks."
                },
                "zh": {
                    "title": "TR2MÔºöÁõ∏ÂØπÊ∑±Â∫¶Âà∞Â∫¶ÈáèÊ∑±Â∫¶ÁöÑÊô∫ËÉΩËΩ¨Êç¢",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫TR2MÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â∞ÜÁõ∏ÂØπÊ∑±Â∫¶ËΩ¨Êç¢‰∏∫Â∫¶ÈáèÊ∑±Â∫¶ÔºåÂà©Áî®Â§öÊ®°ÊÄÅËæìÂÖ•ÊèêÂçáÂú®‰∏çÂêåÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞„ÄÇÂΩìÂâçÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ï‰∏ªË¶ÅÂàÜ‰∏∫Â∫¶ÈáèÊ∑±Â∫¶‰º∞ËÆ°ÂíåÁõ∏ÂØπÊ∑±Â∫¶‰º∞ËÆ°ÔºåÂâçËÄÖÂú®ÁâπÂÆöÈ¢ÜÂüüË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂ±ÄÈôêÊÄßËæÉÂ§ßÔºåËÄåÂêéËÄÖÂú®‰∏çÂêåÈ¢ÜÂüüÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩÜÂ≠òÂú®Â∞∫Â∫¶‰∏çÁ°ÆÂÆöÊÄßÁöÑÈóÆÈ¢ò„ÄÇTR2MÈÄöËøáËûçÂêàÊñáÊú¨ÊèèËø∞ÂíåÂõæÂÉèËæìÂÖ•ÔºåÂà©Áî®‰∫§ÂèâÊ®°ÊÄÅÊ≥®ÊÑèÂäõÊ®°ÂùóÂíåÂØπÊØîÂ≠¶‰π†Á≠ñÁï•ÔºåÊûÑÂª∫‰∫Ü‰∏§‰∏™ÈáçÊ†áÂÆöÂõæ‰ª•Âú®ÂÉèÁ¥†Á∫ßÂà´‰∏äËøõË°åÊ∑±Â∫¶ËΩ¨Êç¢„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTR2MÂú®Â∑≤Áü•Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂπ∂Âú®‰∫î‰∏™Êú™Áü•Êï∞ÊçÆÈõÜ‰∏äÂ±ïÁé∞Âá∫ÂçìË∂äÁöÑÈõ∂Ê†∑Êú¨ËÉΩÂäõÔºåÊòæÁ§∫Âá∫Âú®ÂÉèÁ¥†Á∫ßÂà´‰∏äÂà©Áî®ËØ≠Ë®ÄËæÖÂä©ËøõË°åÊ∑±Â∫¶ËΩ¨Êç¢ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-06-17.html",
    "link_next": "2025-06-19.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "17.06",
        "en": "06/17",
        "zh": "6Êúà17Êó•"
    },
    "short_date_next": {
        "ru": "19.06",
        "en": "06/19",
        "zh": "6Êúà19Êó•"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 11,
        "#agents": 4,
        "#cv": 4,
        "#rl": 5,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 7,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 1,
        "#training": 13,
        "#robotics": 0,
        "#agi": 2,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 10,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 11,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 1
    }
}