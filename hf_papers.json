{
    "date": {
        "ru": "26 –º–∞—è",
        "en": "May 26",
        "zh": "5Êúà26Êó•"
    },
    "time_utc": "2025-05-26 23:10",
    "weekday": 0,
    "issue_id": 3965,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.18125",
            "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
            "url": "https://huggingface.co/papers/2505.18125",
            "abstract": "TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees (GBDTs). However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. We introduce TabSTAR: a Foundation Tabular Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements.",
            "score": 95,
            "issue_id": 3949,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "e6debd33931f5a78",
            "authors": [
                "Alan Arazi",
                "Eilam Shapira",
                "Roi Reichart"
            ],
            "affiliations": [
                "Technion - IIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18125.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#transfer_learning",
                    "#architecture",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "üìä",
                "ru": {
                    "title": "TabSTAR: –£–º–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "TabSTAR - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö. TabSTAR –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ü–µ–ª–µ–≤—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º —Ç–µ–∫—Å—Ç–∞. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "TabSTAR: Revolutionizing Tabular Learning with Contextual Text Understanding",
                    "desc": "TabSTAR is a new model designed to improve classification tasks that involve tabular data with text features. It uses a unique approach called semantically target-aware representations, which helps the model understand the context of the data better. Unlike previous methods, TabSTAR does not rely on dataset-specific parameters, allowing it to generalize across different datasets effectively. This model achieves top performance on various benchmarks, demonstrating its potential for enhancing tabular learning tasks."
                },
                "zh": {
                    "title": "TabSTARÔºöË°®Ê†ºÊï∞ÊçÆÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "TabSTARÊòØ‰∏ÄÁßçÊñ∞ÁöÑË°®Ê†ºÂü∫Á°ÄÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫éÂ§ÑÁêÜÂ∏¶ÊúâÊñáÊú¨ÁâπÂæÅÁöÑÂàÜÁ±ª‰ªªÂä°„ÄÇÂÆÉÈÄöËøáËΩ¨ÁßªÂ≠¶‰π†ÂÆûÁé∞‰∫ÜÊó†Êï∞ÊçÆÈõÜÁâπÂÆöÂèÇÊï∞ÁöÑÈ´òÊïàÊÄßËÉΩÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÂ±ÄÈôê„ÄÇËØ•Ê®°ÂûãÂà©Áî®ËØ≠‰πâÁõÆÊ†áÊÑüÁü•ÁöÑË°®Á§∫ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£‰ªªÂä°‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÁîüÊàêÊõ¥ÊúâÊïàÁöÑÁâπÂæÅÂµåÂÖ•„ÄÇTabSTARÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÂú®‰∏≠ÂûãÂíåÂ§ßÂûãÊï∞ÊçÆÈõÜ‰∏äÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17667",
            "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.17667",
            "abstract": "A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments.",
            "score": 59,
            "issue_id": 3948,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "943935a610a2c31e",
            "authors": [
                "Fanqi Wan",
                "Weizhou Shen",
                "Shengyi Liao",
                "Yingcheng Shi",
                "Chenliang Li",
                "Ziyi Yang",
                "Ji Zhang",
                "Fei Huang",
                "Jingren Zhou",
                "Ming Yan"
            ],
            "affiliations": [
                "Qwen-Doc Team, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17667.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "QwenLong-L1: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "desc": "QwenLong-L1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, —É–ª—É—á—à–∞—é—â–∏–π –º–æ–¥–µ–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –≤–∫–ª—é—á–∞—é—â–∏–π –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –∫—É—Ä–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. QwenLong-L1 —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ QwenLong-L1-32B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤–µ–¥—É—â–∏–µ LRM –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
                },
                "en": {
                    "title": "Empowering Long-Context Reasoning with QwenLong-L1",
                    "desc": "The paper introduces QwenLong-L1, a framework designed to enhance large reasoning models (LRMs) for long-context reasoning tasks using reinforcement learning (RL). It addresses the challenges of training efficiency and optimization stability that arise when adapting LRMs from short-context to long-context scenarios. The framework employs a warm-up supervised fine-tuning stage to create a strong initial policy, followed by a curriculum-guided RL approach to ensure stable policy updates. Experimental results show that QwenLong-L1 significantly outperforms existing LRMs on long-context document question-answering benchmarks, marking a significant advancement in the field."
                },
                "zh": {
                    "title": "QwenLong-L1ÔºöÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "QwenLong-L1ÊòØ‰∏Ä‰∏™Â¢ûÂº∫Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÂú®Èïø‰∏ä‰∏ãÊñáËæìÂÖ•‰∏≠ËøõË°åÊúâÊïàÊé®ÁêÜÁöÑÂÖ≥ÈîÆÊåëÊàòÔºåÂåÖÊã¨ËÆ≠ÁªÉÊïàÁéá‰Ωé‰∏ãÂíå‰ºòÂåñËøáÁ®ã‰∏çÁ®≥ÂÆö„ÄÇÈÄöËøáÈÄêÊ≠•‰∏ä‰∏ãÊñáÊâ©Â±ïÂíåÊ∏©ÊöñÂêØÂä®ÁöÑÁõëÁù£ÂæÆË∞ÉÈò∂ÊÆµÔºåQwenLong-L1Âª∫Á´ã‰∫ÜÁ®≥ÂÅ•ÁöÑÂàùÂßãÁ≠ñÁï•ÔºåÂπ∂ÈááÁî®ËØæÁ®ãÂºïÂØºÁöÑÈò∂ÊÆµÊÄßÂº∫ÂåñÂ≠¶‰π†ÊäÄÊúØÊù•Á®≥ÂÆöÁ≠ñÁï•ÊºîÂèò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQwenLong-L1Âú®Èïø‰∏ä‰∏ãÊñáÊñáÊ°£ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÈ¢ÜÂÖàÁöÑÊé®ÁêÜÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14669",
            "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
            "url": "https://huggingface.co/papers/2505.14669",
            "abstract": "Quartet, a hardware-supported FP4 training approach for large language models, demonstrates state-of-the-art accuracy while significantly reducing computational costs compared to standard or FP8 precision.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of large language models (LLMs) has been paralleled by unprecedented increases in computational demands, with training costs for state-of-the-art models doubling every few months. Training models directly in low-precision arithmetic offers a solution, by improving both computational throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell architecture facilitates extremely low-precision operations, specifically FP4 variants, promising substantial efficiency gains. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we systematically investigate hardware-supported FP4 training and introduce Quartet, a new approach enabling accurate, end-to-end FP4 training with all the major computations (in e.g. linear layers) being performed in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across varying bit-widths and allows us to identify a \"near-optimal\" low-precision training technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve state-of-the-art accuracy for FP4 precision, successfully training billion-scale models. Our method demonstrates that fully FP4-based training is a competitive alternative to standard-precision and FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.",
            "score": 53,
            "issue_id": 3953,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "10d7639a81c5e992",
            "authors": [
                "Roberto L. Castro",
                "Andrei Panferov",
                "Soroush Tabesh",
                "Oliver Sieberling",
                "Jiale Chen",
                "Mahdi Nikdan",
                "Saleh Ashkboos",
                "Dan Alistarh"
            ],
            "affiliations": [
                "ETH Z√ºrich",
                "ISTA",
                "ISTA & Red Hat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14669.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é 4-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Quartet - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 4-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ (FP4). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∏–ª–∏ 8-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å FP4 –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ CUDA-—è–¥—Ä–∞ –¥–ª—è GPU NVIDIA Blackwell. Quartet –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é FP4-–æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–π –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ FP8."
                },
                "en": {
                    "title": "Quartet: Revolutionizing LLM Training with FP4 Precision",
                    "desc": "The paper presents Quartet, a novel training method for large language models (LLMs) that utilizes hardware-supported FP4 precision to enhance training efficiency. By leveraging NVIDIA's Blackwell architecture, Quartet achieves state-of-the-art accuracy while significantly lowering computational costs compared to traditional FP8 and standard precision methods. The authors introduce a new low-precision scaling law that helps balance performance and accuracy across different bit-widths, allowing for effective training of billion-scale models. This approach demonstrates that fully FP4 training can be a viable alternative to existing precision techniques, making it a promising advancement in the field of machine learning."
                },
                "zh": {
                    "title": "QuartetÔºöÈ´òÊïàÁöÑFP4ËÆ≠ÁªÉÊñ∞ÊñπÊ≥ï",
                    "desc": "QuartetÊòØ‰∏ÄÁßçÁ°¨‰ª∂ÊîØÊåÅÁöÑFP4ËÆ≠ÁªÉÊñπÊ≥ïÔºå‰∏ì‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÆæËÆ°„ÄÇÂÆÉÂú®‰øùÊåÅÈ´òÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåÁõ∏ÊØî‰∫éÊ†áÂáÜÊàñFP8Á≤æÂ∫¶ÊúâÊòéÊòæ‰ºòÂäø„ÄÇÈÄöËøáÂØπLlamaÁ±ªÂûãÊ®°ÂûãÁöÑÂπøÊ≥õËØÑ‰º∞ÔºåQuartetÊè≠Á§∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ΩéÁ≤æÂ∫¶Áº©ÊîæÊ≥ïÂàôÔºåÂ∏ÆÂä©Êàë‰ª¨ÊâæÂà∞Âú®ÂáÜÁ°ÆÊÄß‰∏éËÆ°ÁÆó‰πãÈó¥ÁöÑÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ÈíàÂØπNVIDIA Blackwell GPU‰ºòÂåñÁöÑCUDAÂÜÖÊ†∏ÔºåÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑFP4ËÆ≠ÁªÉÔºåËØÅÊòé‰∫ÜFP4ËÆ≠ÁªÉÊòØÊ†áÂáÜÁ≤æÂ∫¶ÂíåFP8ËÆ≠ÁªÉÁöÑÊúâÂäõÊõø‰ª£ÊñπÊ°à„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18129",
            "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.18129",
            "abstract": "A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at https://github.com/MiniMax-AI.",
            "score": 49,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "e690c5668b7f4cd0",
            "authors": [
                "Yan Ma",
                "Linge Du",
                "Xuyang Shen",
                "Shaoxiang Chen",
                "Pengfei Li",
                "Qibing Ren",
                "Lizhuang Ma",
                "Yuchao Dai",
                "Pengfei Liu",
                "Junjie Yan"
            ],
            "affiliations": [
                "Google DeepMind",
                "MiniMax-AI",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18129.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ò–ò",
                    "desc": "V-Triune - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –∑–∞–¥–∞—á–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤—ã–±–æ—Ä–∫–∏, –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥ –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞. –í–≤–µ–¥–µ–Ω–∞ –Ω–æ–≤–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞–≥—Ä–∞–¥–∞ IoU –¥–ª—è –∑–∞–¥–∞—á –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è. –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∞—è –º–æ–¥–µ–ª—å Orsta –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞–∫ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫ –∏ –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è."
                },
                "en": {
                    "title": "Unifying Visual Reasoning and Perception in One RL System",
                    "desc": "The paper introduces V-Triune, a unified reinforcement learning system designed to enhance vision-language models (VLMs) by integrating visual reasoning and perception tasks into a single training framework. It features three key components: Sample-Level Data Formatting for input unification, Verifier-Level Reward Computation for tailored reward systems, and Source-Level Metric Monitoring for data diagnostics. A novel Dynamic IoU reward mechanism is also proposed, providing adaptive feedback for perception tasks. The resulting model, Orsta, shows significant performance improvements across various reasoning and perception benchmarks, demonstrating the effectiveness of this unified approach."
                },
                "zh": {
                    "title": "Áªü‰∏ÄÂº∫ÂåñÂ≠¶‰π†ÔºåÊèêÂçáËßÜËßâÊé®ÁêÜ‰∏éÊÑüÁü•ËÉΩÂäõ",
                    "desc": "V-TriuneÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂº∫ÂåñÂ≠¶‰π†Á≥ªÁªüÔºåÊó®Âú®ÈÄöËøáÂçï‰∏ÄÁöÑËÆ≠ÁªÉÊµÅÁ®ãÁªìÂêàËßÜËßâÊé®ÁêÜÂíåÊÑüÁü•‰ªªÂä°„ÄÇËØ•Á≥ªÁªüÂåÖÂê´‰∏â‰∏™‰∫íË°•ÁöÑÁªÑ‰ª∂ÔºåÂàÜÂà´ÊòØÊ†∑Êú¨Á∫ßÊï∞ÊçÆÊ†ºÂºèÂåñ„ÄÅÈ™åËØÅÂô®Á∫ßÂ•ñÂä±ËÆ°ÁÆóÂíåÊ∫êÁ∫ßÊåáÊ†áÁõëÊéßÔºå‰ª•ÊîØÊåÅÂ§öÊ†∑ÂåñÁöÑ‰ªªÂä°ËæìÂÖ•ÂíåÂÆöÂà∂ÂåñÁöÑÂ•ñÂä±ÂèçÈ¶à„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂä®ÊÄÅIoUÂ•ñÂä±Ôºå‰∏∫ÊÑüÁü•‰ªªÂä°Êèê‰æõÈÄÇÂ∫îÊÄßÂíåÊ∏êËøõÊÄßÁöÑÂèçÈ¶à„ÄÇÈÄöËøáÂú®Â§öÊ†∑ÂåñÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåV-TriuneÊòæËëóÊèêÂçá‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜÂíåÊÑüÁü•‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17225",
            "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
            "url": "https://huggingface.co/papers/2505.17225",
            "abstract": "A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks. However, they frequently exhibit a problematic reliance on familiar reasoning patterns, a phenomenon we term reasoning rigidity. Despite explicit instructions from users, these models often override clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect conclusions. This behavior presents significant challenges, particularly in domains such as mathematics and logic puzzle, where precise adherence to specified constraints is critical. To systematically investigate reasoning rigidity, a behavior largely unexplored in prior work, we introduce a expert-curated diagnostic set, . Our dataset includes specially modified variants of existing mathematical benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately redesigned to require deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination patterns that occur when models default to ingrained reasoning. Specifically, we categorize this contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention, each causing models to ignore or distort provided instructions. We publicly release our diagnostic set to facilitate future research on mitigating reasoning rigidity in language models.",
            "score": 48,
            "issue_id": 3945,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "41036303d3b75082",
            "authors": [
                "Doohyuk Jang",
                "Yoonjeon Kim",
                "Chanjae Park",
                "Hyun Ryu",
                "Eunho Yang"
            ],
            "affiliations": [
                "AITRICS",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17225.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#dataset",
                    "#data",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∂–µ—Å—Ç–∫–æ—Å—Ç–∏ –º—ã—à–ª–µ–Ω–∏—è –≤ –ò–ò: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –∂–µ—Å—Ç–∫–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è–≤–∏–ª–∏ —Ç–µ–Ω–¥–µ–Ω—Ü–∏—é –º–æ–¥–µ–ª–µ–π –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞–∫–æ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ë—ã–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Ç—Ä–∏ —Ä–µ–∂–∏–º–∞ –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏: –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, –Ω–µ–¥–æ–≤–µ—Ä–∏–µ –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∏ –≥–æ–ª–æ–≤–æ–ª–æ–º–æ–∫, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç –ø—Ä–∏–≤—ã—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Unraveling Reasoning Rigidity in Language Models",
                    "desc": "This paper investigates a phenomenon called reasoning rigidity in large language models, where these models often ignore user instructions and revert to familiar reasoning patterns. The authors introduce a diagnostic set designed to identify and categorize this behavior, which can lead to incorrect conclusions in tasks requiring precise adherence to instructions. They highlight three specific modes of contamination: Interpretation Overload, Input Distrust, and Partial Instruction Attention, which describe how models distort or overlook given instructions. By releasing this diagnostic set, the authors aim to support further research aimed at reducing reasoning rigidity in language models."
                },
                "zh": {
                    "title": "Êè≠Á§∫ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÂÉµÂåñÁé∞Ë±°",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊé®ÁêÜÂÉµÂåñÁé∞Ë±°ÔºåÂç≥Ê®°ÂûãÂú®Èù¢ÂØπÊòéÁ°ÆÊåá‰ª§Êó∂Ôºå‰ªçÁÑ∂ÂÄæÂêë‰∫é‰ΩøÁî®ÁÜüÊÇâÁöÑÊé®ÁêÜÊ®°Âºè„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™‰∏ìÂÆ∂Á≠ñÂàíÁöÑËØäÊñ≠ÈõÜÔºå‰ª•Á≥ªÁªüÂú∞Á†îÁ©∂Ëøô‰∏ÄË°å‰∏∫ÔºåÁâπÂà´ÊòØÂú®Êï∞Â≠¶ÂíåÈÄªËæëÈöæÈ¢òÁ≠âÈ¢ÜÂüü„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´ÁªèËøá‰øÆÊîπÁöÑÊï∞Â≠¶Âü∫ÂáÜÂíåÈáçÊñ∞ËÆæËÆ°ÁöÑÈöæÈ¢òÔºåÊó®Âú®‰øÉ‰ΩøÊ®°ÂûãÂÅèÁ¶ªÂ∏∏ËßÑÊé®ÁêÜÁ≠ñÁï•„ÄÇÈÄöËøáÂàÜÊûêÔºåÊàë‰ª¨ËØÜÂà´Âá∫‰∏âÁßç‰∏ªË¶ÅÁöÑÊé®ÁêÜÂÉµÂåñÊ®°ÂºèÔºåÂ∏ÆÂä©Êú™Êù•ÁöÑÁ†îÁ©∂Êõ¥Â•ΩÂú∞Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17612",
            "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
            "url": "https://huggingface.co/papers/2505.17612",
            "abstract": "Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.",
            "score": 47,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "258cfb9a5b51fa42",
            "authors": [
                "Minki Kang",
                "Jongwon Jeong",
                "Seanie Lee",
                "Jaewoong Cho",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "KRAFTON"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17612.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#small_models",
                    "#agents",
                    "#transfer_learning",
                    "#training",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü–µ—Ä–µ–¥–∞—á–∞ –Ω–∞–≤—ã–∫–æ–≤ –∞–≥–µ–Ω—Ç–∞: –æ—Ç –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–∞–ª—ã–º",
                    "desc": "–ú–µ—Ç–æ–¥ Agent Distillation –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–µ–Ω—å—à–∏–º –º–æ–¥–µ–ª—è–º. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –∏ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–µ–π—Å—Ç–≤–∏–π. Agent Distillation –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–º –º–æ–¥–µ–ª—è–º –¥–æ—Å—Ç–∏–≥–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –±—ã–ª –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –≤–æ—Å—å–º–∏ –∑–∞–¥–∞—á–∞—Ö –≤ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö."
                },
                "en": {
                    "title": "Empowering Small Models with Big Model Intelligence",
                    "desc": "Agent Distillation is a method that helps smaller language models (sLMs) learn reasoning and task-solving skills from larger language models (LLMs). It uses improved prompts and self-consistent actions to enhance the performance of sLMs on reasoning tasks, making them competitive with larger models. The approach addresses challenges like hallucination in sLMs when faced with rare facts or complex computations. By evaluating on various reasoning tasks, the study shows that even small models can perform well, paving the way for more efficient AI applications."
                },
                "zh": {
                    "title": "‰ª£ÁêÜËí∏È¶èÔºöÂ∞èÂûãÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÊèêÂçá",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‰ª£ÁêÜËí∏È¶èÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜÂíå‰ªªÂä°Ëß£ÂÜ≥ËÉΩÂäõËΩ¨ÁßªÂà∞ËæÉÂ∞èÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàsLMÔºâ‰∏≠„ÄÇÈÄöËøá‰ΩøÁî®Â¢ûÂº∫ÁöÑÊèêÁ§∫ÂíåËá™‰∏ÄËá¥ÊÄßÂä®‰ΩúÔºå‰ª£ÁêÜËí∏È¶èËÉΩÂ§üÂú®Â§ö‰∏™Êé®ÁêÜ‰ªªÂä°‰∏äÂÆûÁé∞‰∏éÂ§ßÂûãÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂåÖÊã¨ÂºïÂÖ•‰∏ÄÁßçÊñ∞ÁöÑÊèêÁ§∫ÊñπÊ≥ïÂíåÊîπËøõÂ∞èÂûã‰ª£ÁêÜÂú®ÊµãËØïÊó∂ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂèÇÊï∞Èáè‰∏∫0.5BÂà∞3BÁöÑÂ∞èÂûãÊ®°ÂûãÂèØ‰ª•Âú®‰∫ãÂÆûÂíåÊï∞Â≠¶È¢ÜÂüüÁöÑÊé®ÁêÜ‰ªªÂä°‰∏≠‰∏éÊõ¥Â§ßÁöÑÊ®°ÂûãÁ´û‰∫â„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15929",
            "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
            "url": "https://huggingface.co/papers/2505.15929",
            "abstract": "A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy respectively-performance gaps exceeding 29\\% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation.",
            "score": 38,
            "issue_id": 3950,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "995e1fe73d1e1cef",
            "authors": [
                "Hui Shen",
                "Taiqiang Wu",
                "Qi Han",
                "Yunta Hsieh",
                "Jizhou Wang",
                "Yuyue Zhang",
                "Yuxin Cheng",
                "Zijian Hao",
                "Yuansheng Ni",
                "Xin Wang",
                "Zhongwei Wan",
                "Kai Zhang",
                "Wendong Xu",
                "Jing Xiong",
                "Ping Luo",
                "Wenhu Chen",
                "Chaofan Tao",
                "Zhuoqing Mao",
                "Ngai Wong"
            ],
            "affiliations": [
                "Independent",
                "The Ohio State University",
                "The University of Hong Kong",
                "University of Michigan",
                "University of Toronto",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15929.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "PhyX: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò",
                    "desc": "–ù–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç PhyX –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –¢–µ—Å—Ç –≤–∫–ª—é—á–∞–µ—Ç 3000 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ 6 —Ç–∏–ø–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ 25 –ø–æ–¥–¥–æ–º–µ–Ω–∞—Ö –∏ 6 –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö —Ñ–∏–∑–∏–∫–∏. –î–∞–∂–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4 –∏ Claude, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏-–ª—é–¥—å–º–∏. –ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–∏–ª –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è —á—Ä–µ–∑–º–µ—Ä–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∑–∞–ø–æ–º–Ω–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–º–µ—Å—Ç–æ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è."
                },
                "en": {
                    "title": "PhyX: Bridging the Gap in Physics-Grounded Reasoning for AI",
                    "desc": "The paper introduces PhyX, a new benchmark for evaluating models' abilities in physics-grounded reasoning within visual contexts. It highlights that existing benchmarks do not adequately assess this crucial aspect of intelligence, which combines domain knowledge and real-world constraints. PhyX consists of 3,000 carefully curated multimodal questions across various physics domains, revealing that even advanced models like GPT-4o and Claude3.7-Sonnet perform poorly compared to human experts. The study identifies key limitations in current models, such as reliance on memorized knowledge and superficial visual pattern recognition, and provides a robust evaluation framework for future research."
                },
                "zh": {
                    "title": "PhyXÔºöËØÑ‰º∞Áâ©ÁêÜÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜ",
                    "desc": "PhyXÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Ê®°ÂûãÂú®ËßÜËßâÂú∫ÊôØ‰∏≠ÁöÑÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÁöÑÊ®°ÂûãÂú®Áâ©ÁêÜÁêÜËß£ÊñπÈù¢Â≠òÂú®ÊòæËëóÂ±ÄÈôêÔºåËøú‰∏çÂèä‰∫∫Á±ª‰∏ìÂÆ∂„ÄÇPhyXÂåÖÂê´3000‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑÂ§öÊ®°ÊÄÅÈóÆÈ¢òÔºåÊ∂µÁõñ25‰∏™Â≠êÈ¢ÜÂüüÂíå6‰∏™Ê†∏ÂøÉÁâ©ÁêÜÈ¢ÜÂüü„ÄÇÈÄöËøáÂÖ®Èù¢ËØÑ‰º∞ÔºåÂèëÁé∞Âç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®Áâ©ÁêÜÊé®ÁêÜ‰∏ä‰πüÈù¢‰∏¥ÈáçÂ§ßÊåëÊàòÔºåÂáÜÁ°ÆÁéáËøú‰Ωé‰∫é‰∫∫Á±ª‰∏ìÂÆ∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18092",
            "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
            "url": "https://huggingface.co/papers/2505.18092",
            "abstract": "QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.  \t\t\t\t\tAI-generated summary \t\t\t\t This technical report presents QwenLong-CPRS, a context compression framework designed for explicit long-context optimization, addressing prohibitive computation overhead during the prefill stage and the \"lost in the middle\" performance degradation of large language models (LLMs) during long sequence processing. Implemented through a novel dynamic context optimization mechanism, QwenLong-CPRS enables multi-granularity context compression guided by natural language instructions, achieving both efficiency gains and improved performance.   Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key innovations: (1) Natural language-guided dynamic optimization, (2) Bidirectional reasoning layers for enhanced boundary awareness, (3) Token critic mechanisms with language modeling heads, and (4) Window-parallel inference.   Comprehensive evaluations across five benchmarks (4K-2M word contexts) demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority over other context management methods like RAG and sparse attention in both accuracy and efficiency. (2) Architecture-agnostic integration with all flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3, and Qwen2.5-max, achieves 21.59times context compression alongside 19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct, QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on Ruler-128K and InfiniteBench, establishing new SOTA performance.",
            "score": 37,
            "issue_id": 3949,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "59626d27805a1427",
            "authors": [
                "Weizhou Shen",
                "Chenliang Li",
                "Fanqi Wan",
                "Shengyi Liao",
                "Shaopeng Lai",
                "Bo Zhang",
                "Yingcheng Shi",
                "Yuning Wu",
                "Gang Fu",
                "Zhansheng Li",
                "Bin Yang",
                "Ji Zhang",
                "Fei Huang",
                "Jingren Zhou",
                "Ming Yan"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18092.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#training",
                    "#long_context"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö",
                    "desc": "QwenLong-CPRS - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —Å–∂–∞—Ç–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ. QwenLong-CPRS —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –¢–µ—Å—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ QwenLong-CPRS –Ω–∞–¥ –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∫–∞–∫ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏, —Ç–∞–∫ –∏ –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Revolutionizing Long Context Management in LLMs",
                    "desc": "QwenLong-CPRS is a framework that improves large language models (LLMs) by optimizing how they handle long contexts. It uses a dynamic optimization method that is guided by natural language, allowing for better context compression at multiple levels. This approach not only enhances the efficiency of processing long sequences but also improves the overall performance of the models. The framework has been tested against various benchmarks, showing significant gains in accuracy and efficiency compared to existing context management techniques."
                },
                "zh": {
                    "title": "QwenLong-CPRSÔºöÈ´òÊïàÁöÑ‰∏ä‰∏ãÊñáÂéãÁº©‰∏é‰ºòÂåñ",
                    "desc": "QwenLong-CPRS ÊòØ‰∏ÄÁßçÁî®‰∫éÈïø‰∏ä‰∏ãÊñá‰ºòÂåñÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ÁöÑËÆ°ÁÆóÂºÄÈîÄÂíåÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÊåáÂØºÁöÑÂä®ÊÄÅ‰∏ä‰∏ãÊñá‰ºòÂåñÊú∫Âà∂ÔºåÂÆûÁé∞‰∫ÜÂ§öÁ≤íÂ∫¶ÁöÑ‰∏ä‰∏ãÊñáÂéãÁº©Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊïàÁéáÂíåÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÂºïÂÖ•‰∫ÜÂõõÈ°πÂÖ≥ÈîÆÂàõÊñ∞ÔºåÂåÖÊã¨ÂèåÂêëÊé®ÁêÜÂ±ÇÂíåÂü∫‰∫éËØ≠Ë®ÄÂª∫Ê®°ÁöÑ‰ª§ÁâåËØÑ‰º∞Êú∫Âà∂„ÄÇÁªºÂêàËØÑ‰º∞ÊòæÁ§∫ÔºåQwenLong-CPRS Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñ‰∏ä‰∏ãÊñáÁÆ°ÁêÜÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17618",
            "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
            "url": "https://huggingface.co/papers/2505.17618",
            "abstract": "EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.  \t\t\t\t\tAI-generated summary \t\t\t\t As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose Evolutionary Search (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch.",
            "score": 31,
            "issue_id": 3949,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "dfe3bcf9e6ec80f9",
            "authors": [
                "Haoran He",
                "Jiajun Liang",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai",
                "Ling Pan"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17618.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#cv",
                    "#inference",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "üß¨",
                "ru": {
                    "title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "EvoSearch - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–µ—Å—Ç-—Ç–∞–π–º —Å–∫–µ–π–ª–∏–Ω–≥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –ø–æ—Ç–æ–∫–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. EvoSearch –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —ç—Ç–∞–ø–µ –≤—ã–≤–æ–¥–∞. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö –æ—Ü–µ–Ω–∫–∏."
                },
                "en": {
                    "title": "EvoSearch: Evolving Image and Video Generation at Test Time",
                    "desc": "EvoSearch is an innovative evolutionary search method designed to improve test-time scaling (TTS) for generative models, specifically diffusion and flow-based models. It addresses the limitations of existing TTS strategies by reformulating the scaling process as an evolutionary problem, allowing for efficient exploration of denoising trajectories. By utilizing selection and mutation mechanisms inspired by biological evolution, EvoSearch enhances the quality and diversity of generated images and videos without the need for additional training. Our extensive evaluations show that EvoSearch consistently outperforms current methods, demonstrating superior generalizability and diversity in generative tasks."
                },
                "zh": {
                    "title": "ËøõÂåñÊêúÁ¥¢ÔºöÊèêÂçáÁîüÊàêÊ®°ÂûãÁöÑÊµãËØïÊó∂Êâ©Â±ïËÉΩÂäõ",
                    "desc": "EvoSearchÊòØ‰∏ÄÁßçËøõÂåñÊêúÁ¥¢ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊâ©Êï£ÂíåÊµÅÂºèÁîüÊàêÊ®°ÂûãÂú®ÊµãËØïÊó∂ÁöÑÊâ©Â±ïËÉΩÂäõÔºå‰ªéËÄåÊîπÂñÑÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÁöÑË¥®Èáè„ÄÅÂ§öÊ†∑ÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÈöèÁùÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉÊúüÈó¥ËÆ°ÁÆóÊàêÊú¨ÁöÑÊòæËëóÂ¢ûÂä†ÔºåÊµãËØïÊó∂Êâ©Â±ïÔºàTTSÔºâÊàê‰∏∫ÊèêÂçáÁîüÊàêÊ®°ÂûãÊÄßËÉΩÁöÑ‰∏Ä‰∏™ÊúâÂâçÊôØÁöÑÊñπÂêë„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÊñπÊ≥ïÂú®ËØ≠Ë®Ä‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÊµãËØïÊó∂Êâ©Â±ïË°å‰∏∫‰∏ä‰ªçÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇEvoSearchÈÄöËøáÂ∞ÜÊµãËØïÊó∂Êâ©Â±ïÈáçÊñ∞ÊûÑÈÄ†Êàê‰∏Ä‰∏™ËøõÂåñÊêúÁ¥¢ÈóÆÈ¢òÔºåÂà©Áî®ÁîüÁâ©ËøõÂåñÁöÑÂéüÁêÜÈ´òÊïàÊé¢Á¥¢Âíå‰ºòÂåñÂéªÂô™ËΩ®ËøπÔºåÊúÄÁªàÂÆûÁé∞‰∫ÜÊõ¥È´òË¥®ÈáèÁöÑÁîüÊàêÁªìÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17873",
            "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated\n  Experimental Feedback",
            "url": "https://huggingface.co/papers/2505.17873",
            "abstract": "A novel simulator and experiment-guided ranking method improve hypothesis prioritization in scientific discovery by incorporating simulated experimental outcomes.  \t\t\t\t\tAI-generated summary \t\t\t\t Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet-lab experiments are costly and throughput-limited. Existing approaches focus on pre-experiment ranking, relying solely on large language model's internal reasoning without incorporating empirical outcomes from experiments. We introduce the task of experiment-guided ranking, which aims to prioritize candidate hypotheses based on the results of previously tested ones. However, developing such strategies is challenging due to the impracticality of repeatedly conducting real experiments in natural science domains. To address this, we propose a simulator grounded in three domain-informed assumptions, modeling hypothesis performance as a function of similarity to a known ground truth hypothesis, perturbed by noise. We curate a dataset of 124 chemistry hypotheses with experimentally reported outcomes to validate the simulator. Building on this simulator, we develop a pseudo experiment-guided ranking method that clusters hypotheses by shared functional characteristics and prioritizes candidates based on insights derived from simulated experimental feedback. Experiments show that our method outperforms pre-experiment baselines and strong ablations.",
            "score": 24,
            "issue_id": 3951,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "45be71b3061e9e57",
            "authors": [
                "Wanhao Liu",
                "Zonglin Yang",
                "Jue Wang",
                "Lidong Bing",
                "Di Zhang",
                "Dongzhan Zhou",
                "Yuqiang Li",
                "Houqiang Li",
                "Erik Cambria",
                "Wanli Ouyang"
            ],
            "affiliations": [
                "MiroMind",
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17873.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#optimization",
                    "#data",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "üß™",
                "ru": {
                    "title": "–°–∏–º—É–ª—è—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —É–ª—É—á—à–∞–µ—Ç —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏–º—É–ª—è—Ç–æ—Ä, –º–æ–¥–µ–ª–∏—Ä—É—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–∏–ø–æ—Ç–µ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∏—Å—Ç–∏–Ω–Ω–æ–π –≥–∏–ø–æ—Ç–µ–∑–æ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –≥–∏–ø–æ—Ç–µ–∑ –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Revolutionizing Hypothesis Ranking with Simulation-Driven Insights",
                    "desc": "This paper presents a new method for ranking scientific hypotheses by using simulated experimental outcomes, which helps prioritize hypotheses before actual experiments are conducted. Traditional methods rely on large language models for pre-experiment ranking, but they do not consider real experimental results. The authors introduce an experiment-guided ranking approach that uses a simulator to model how hypotheses perform based on their similarity to known successful hypotheses, while accounting for noise. Their method, validated with a dataset of chemistry hypotheses, shows improved performance over existing pre-experiment ranking techniques."
                },
                "zh": {
                    "title": "ÂÆûÈ™åÂºïÂØºÁöÑÂÅáËÆæ‰ºòÂÖàÁ∫ßÊéíÂ∫èÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ®°ÊãüÂô®ÂíåÂÆûÈ™åÂºïÂØºÁöÑÊéíÂêçÊñπÊ≥ïÔºå‰ª•ÊîπÂñÑÁßëÂ≠¶ÂèëÁé∞‰∏≠ÁöÑÂÅáËÆæ‰ºòÂÖàÁ∫ßÊéíÂ∫è„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÁªìÂêàÊ®°ÊãüÂÆûÈ™åÁªìÊûúÔºå‰ºòÂÖàËÄÉËôëÂü∫‰∫éÂÖàÂâçÊµãËØïÁªìÊûúÁöÑÂÄôÈÄâÂÅáËÆæ„ÄÇÊàë‰ª¨ÂºÄÂèëÁöÑÊ®°ÊãüÂô®Âü∫‰∫é‰∏â‰∏™È¢ÜÂüüÁõ∏ÂÖ≥ÁöÑÂÅáËÆæÔºåËÉΩÂ§üÊ®°ÊãüÂÅáËÆæÊÄßËÉΩÔºåÂπ∂ÈÄöËøáÂô™Â£∞ËøõË°åÊâ∞Âä®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂÅáËÆæÊéíÂêç‰∏ä‰ºò‰∫é‰º†ÁªüÁöÑÈ¢ÑÂÆûÈ™åÂü∫Á∫øÂíåÂº∫Ê∂àËûçÂÆûÈ™å„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17561",
            "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
            "url": "https://huggingface.co/papers/2505.17561",
            "abstract": "ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t The choice of initial noise significantly affects the quality and prompt alignment of video diffusion models, where different noise seeds for the same prompt can lead to drastically different generations. While recent methods rely on externally designed priors such as frequency filters or inter-frame smoothing, they often overlook internal model signals that indicate which noise seeds are inherently preferable. To address this, we propose ANSE (Active Noise Selection for Generation), a model-aware framework that selects high-quality noise seeds by quantifying attention-based uncertainty. At its core is BANSA (Bayesian Active Noise Selection via Attention), an acquisition function that measures entropy disagreement across multiple stochastic attention samples to estimate model confidence and consistency. For efficient inference-time deployment, we introduce a Bernoulli-masked approximation of BANSA that enables score estimation using a single diffusion step and a subset of attention layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video quality and temporal coherence with only an 8% and 13% increase in inference time, respectively, providing a principled and generalizable approach to noise selection in video diffusion. See our project page: https://anse-project.github.io/anse-project/",
            "score": 24,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "224b15e182587a84",
            "authors": [
                "Kwanyoung Kim",
                "Sanghyun Kim"
            ],
            "affiliations": [
                "Samsung Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17561.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä —à—É–º–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –≤–∏–¥–µ–æ-—Å–∏–Ω—Ç–µ–∑–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ANSE - –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —à—É–º–æ–≤—ã—Ö —Å–∏–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í –æ—Å–Ω–æ–≤–µ –ª–µ–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏—è BANSA, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—é —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏–π –º–µ–∂–¥—É —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤—ã–±–æ—Ä–∫–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è BANSA —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –ë–µ—Ä–Ω—É–ª–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞."
                },
                "en": {
                    "title": "Smart Noise Selection for Better Video Generation",
                    "desc": "The paper introduces ANSE, a method that enhances video diffusion models by intelligently selecting noise seeds based on the model's confidence. It highlights the importance of initial noise in generating high-quality videos, as different seeds can lead to varying results. ANSE utilizes an acquisition function called BANSA, which measures uncertainty through attention-based entropy to identify the best noise seeds. This approach improves video quality and temporal coherence while only slightly increasing the time needed for inference."
                },
                "zh": {
                    "title": "‰∏ªÂä®ÈÄâÊã©Âô™Â£∞ÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàêË¥®Èáè",
                    "desc": "ANSEÔºà‰∏ªÂä®Âô™Â£∞ÈÄâÊã©ÁîüÊàêÔºâÈÄöËøáÂü∫‰∫éÊ®°Âûã‰ø°ÂøÉÈÄâÊã©Âô™Â£∞ÁßçÂ≠êÔºåÂ¢ûÂº∫‰∫ÜËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Ê≥®ÊÑèÂäõÊú∫Âà∂ÈáèÂåñ‰∏çÁ°ÆÂÆöÊÄßÔºå‰ªéËÄåÈÄâÊã©È´òË¥®ÈáèÁöÑÂô™Â£∞ÁßçÂ≠êÔºåÊòæËëóÊèêÈ´òËßÜÈ¢ëË¥®ÈáèÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÊ†∏ÂøÉÁÆóÊ≥ïBANSAÔºàÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑË¥ùÂè∂ÊñØ‰∏ªÂä®Âô™Â£∞ÈÄâÊã©ÔºâÈÄöËøáÊµãÈáèÂ§ö‰∏™ÈöèÊú∫Ê≥®ÊÑèÂäõÊ†∑Êú¨‰πãÈó¥ÁöÑÁÜµ‰∏ç‰∏ÄËá¥ÊÄßÊù•‰º∞ËÆ°Ê®°ÂûãÁöÑ‰ø°ÂøÉÂíå‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåANSEÂú®Êé®ÁêÜÊó∂Èó¥‰ªÖÂ¢ûÂä†8%Âíå13%ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÊîπÂñÑ‰∫ÜËßÜÈ¢ëÁîüÊàêÁöÑË¥®ÈáèÂíå‰∏ÄËá¥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17941",
            "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
            "url": "https://huggingface.co/papers/2505.17941",
            "abstract": "VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker",
            "score": 22,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "cfc0e5dae345ea81",
            "authors": [
                "Zigeng Chen",
                "Xinyin Ma",
                "Gongfan Fang",
                "Ruonan Yu",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17941.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏",
                    "desc": "VeriThinker - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∂–∞—Ç–∏—é —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ LRM –Ω–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–æ–π –∑–∞–¥–∞—á–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –ª—É—á—à–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —à–∞–≥–æ–≤ —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VeriThinker –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –¥–ª–∏–Ω—É —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –∏–ª–∏ –¥–∞–∂–µ –Ω–µ–º–Ω–æ–≥–æ —É–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å. –ü–æ–¥—Ö–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–æ–±—â–µ–Ω–∏—è –Ω–∞ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Streamlining Reasoning with VeriThinker",
                    "desc": "VeriThinker is a method designed to enhance Large Reasoning Models (LRMs) by reducing the length of their reasoning chains. It achieves this by fine-tuning the models on a verification task instead of directly on the original reasoning tasks. This approach helps the models become more efficient by minimizing unnecessary steps in their reasoning process, which lowers inference costs. Experimental results show that VeriThinker not only shortens reasoning chains but also improves accuracy in various tasks, demonstrating its effectiveness in optimizing LRM performance."
                },
                "zh": {
                    "title": "VeriThinkerÔºö‰ºòÂåñÊé®ÁêÜÈìæÔºåÊèêÂçáÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß",
                    "desc": "VeriThinkerÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøáÂú®È™åËØÅ‰ªªÂä°‰∏äÂæÆË∞ÉÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÔºåÂáèÂ∞ëÂ§çÊùÇÊé®ÁêÜÈìæÁöÑÈïøÂ∫¶Ôºå‰ªéËÄåÈôç‰ΩéÊé®ÁêÜÊàêÊú¨ËÄå‰∏çÊòæËëóÁâ∫Áâ≤ÂáÜÁ°ÆÊÄß„ÄÇ‰º†ÁªüÊñπÊ≥ïÁõ¥Êé•Âú®ÂéüÂßãÊé®ÁêÜ‰ªªÂä°‰∏äÂæÆË∞ÉÊ®°ÂûãÔºåËÄåVeriThinkerÂàôÂàõÊñ∞ÊÄßÂú∞‰ªÖÈÄöËøáËæÖÂä©È™åËØÅ‰ªªÂä°ËøõË°åÂæÆË∞É„ÄÇÈÄöËøáËÆ≠ÁªÉLRMsÂáÜÁ°ÆÈ™åËØÅÊé®ÁêÜËß£ÂÜ≥ÊñπÊ°àÁöÑÊ≠£Á°ÆÊÄßÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà§Êñ≠ÂêéÁª≠Ëá™ÊàëÂèçÊÄùÊ≠•È™§ÁöÑÂøÖË¶ÅÊÄßÔºåÊúâÊïàÊäëÂà∂ËøáÂ∫¶ÊÄùËÄÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVeriThinkerÊòæËëóÂáèÂ∞ë‰∫ÜÊé®ÁêÜÈìæÁöÑÈïøÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅÊàñÁï•ÂæÆÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16211",
            "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
            "url": "https://huggingface.co/papers/2505.16211",
            "abstract": "AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust.",
            "score": 17,
            "issue_id": 3946,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "1849951d3588375e",
            "authors": [
                "Kai Li",
                "Can Shen",
                "Yile Liu",
                "Jirui Han",
                "Kelong Zheng",
                "Xuechao Zou",
                "Zhe Wang",
                "Xingjian Du",
                "Shun Zhang",
                "Hanjun Luo",
                "Yingbin Jin",
                "Xinxin Xing",
                "Ziyang Ma",
                "Yue Liu",
                "Xiaojun Jia",
                "Yifan Zhang",
                "Junfeng Fang",
                "Kun Wang",
                "Yibo Yan",
                "Haoyang Li",
                "Yiming Li",
                "Xiaobin Zhuang",
                "Yang Liu",
                "Haibo Hu",
                "Zhuo Chen",
                "Zhizheng Wu",
                "Xiaolin Hu",
                "Eng-Siong Chng",
                "XiaoFeng Wang",
                "Wenyuan Xu",
                "Wei Dong",
                "Xinfeng Li"
            ],
            "affiliations": [
                "ACM Member",
                "BJTU",
                "BNBU",
                "Bytedance",
                "CAS",
                "HUST",
                "Hong Kong Polytechnic University",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Independent Researcher",
                "Nanyang Technological University",
                "National University of Singapore",
                "QHU",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong (Shenzhen)",
                "Tsinghua University",
                "University of Rochester",
                "Waseda University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16211.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#benchmark",
                    "#security",
                    "#ethics",
                    "#open_source",
                    "#dataset",
                    "#audio"
                ],
                "emoji": "üéôÔ∏è",
                "ru": {
                    "title": "AudioTrust: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ –ò–ò",
                    "desc": "AudioTrust - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ê—É–¥–∏–æ –ë–æ–ª—å—à–∏—Ö –Ø–∑—ã–∫–æ–≤—ã—Ö –ú–æ–¥–µ–ª–µ–π (–ê–ë–õ–ú). –û–Ω–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ê–ë–õ–ú –ø–æ —à–µ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º: —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å, –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª–µ–µ —á–µ–º 4420 –∞—É–¥–∏–æ/—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏ 9 —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤—ã—è–≤–ª—è—é—Ç –≥—Ä–∞–Ω–∏—Ü—ã –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ê–ë–õ–ú –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤—ã—Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤—ã—Ö –∞—É–¥–∏–æ—Å—Ü–µ–Ω–∞—Ä–∏—è—Ö."
                },
                "en": {
                    "title": "Evaluating Trust in Audio Large Language Models with AudioTrust",
                    "desc": "AudioTrust is a novel framework designed to evaluate the trustworthiness of Audio Large Language Models (ALLMs) across multiple dimensions. It addresses the unique challenges and risks associated with audio data, which are often overlooked in existing evaluation methods that focus primarily on text. The framework includes a comprehensive dataset of over 4,420 audio/text samples and employs 18 experimental setups to assess six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. By utilizing nine audio-specific metrics and an automated scoring pipeline, AudioTrust provides insights into the limitations of current ALLMs, guiding their secure deployment in real-world applications."
                },
                "zh": {
                    "title": "Èü≥È¢ëÊ®°Âûã‰ø°‰ªªËØÑ‰º∞Êñ∞Ê†áÂáÜ",
                    "desc": "AudioTrustÊòØ‰∏Ä‰∏™‰∏ìÈó®‰∏∫Èü≥È¢ëÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàALLMsÔºâËÆæËÆ°ÁöÑÂ§öÁª¥‰ø°‰ªªËØÑ‰º∞Ê°ÜÊû∂„ÄÇÂÆÉÈÄöËøá‰∏Ä‰∏™ÂåÖÂê´4420Â§ö‰∏™Èü≥È¢ë/ÊñáÊú¨Ê†∑Êú¨ÁöÑÊï∞ÊçÆÈõÜÔºåËØÑ‰º∞Ê®°ÂûãÂú®ÂÖ¨Âπ≥ÊÄß„ÄÅÂπªËßâ„ÄÅÂÆâÂÖ®ÊÄß„ÄÅÈöêÁßÅ„ÄÅÈ≤ÅÊ£íÊÄßÂíåËÆ§ËØÅÁ≠âÂÖ≠‰∏™ÂÖ≥ÈîÆÁª¥Â∫¶ÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∫Ü18Áßç‰∏çÂêåÁöÑÂÆûÈ™åËÆæÁΩÆÂíå9‰∏™Èü≥È¢ëÁâπÂÆöÁöÑËØÑ‰º∞ÊåáÊ†áÔºå‰ª•Á°Æ‰øùÂÖ®Èù¢ËØÑ‰º∞ALLMsÁöÑ‰ø°‰ªªworthiness„ÄÇÂÆûÈ™åÁªìÊûúÊè≠Á§∫‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÂºÄÊ∫êÂíåÈó≠Ê∫êALLMsÂú®È´òÈ£éÈô©Èü≥È¢ëÂú∫ÊôØ‰∏ãÁöÑ‰ø°‰ªªËæπÁïåÂíåÂ±ÄÈôêÊÄßÔºå‰∏∫Êú™Êù•Èü≥È¢ëÊ®°ÂûãÁöÑÂÆâÂÖ®ÂíåÂèØ‰ø°ÈÉ®ÁΩ≤Êèê‰æõ‰∫ÜÈáçË¶ÅËßÅËß£„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17558",
            "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
            "url": "https://huggingface.co/papers/2505.17558",
            "abstract": "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks.",
            "score": 13,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "9faa21418742a88c",
            "authors": [
                "Shrey Pandit",
                "Ashwin Vinod",
                "Liu Leqi",
                "Ying Ding"
            ],
            "affiliations": [
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17558.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#rlhf",
                    "#training",
                    "#hallucinations"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–û–±—É—á–µ–Ω–∏–µ LLM —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é —Å–∞–º–∏—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ —Å–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ DPO-–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ–±–Ω—ã–º –ø–ª–∞–Ω–æ–º, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥—è –æ—Ç –ª–µ–≥–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –∫ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ HaluCheck, –æ–±—É—á–µ–Ω–Ω—ã–µ —ç—Ç–∏–º –º–µ—Ç–æ–¥–æ–º, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."
                },
                "en": {
                    "title": "Enhancing Hallucination Detection in LLMs through Curriculum Learning",
                    "desc": "This paper presents a novel approach to improve large language models' (LLMs) ability to detect hallucinations by using specially designed negative examples in a curriculum learning framework. The authors recognize that hallucinated texts are often more deceptive than standard negative samples, and they leverage this insight in the DPO alignment procedure. By gradually increasing the difficulty of training samples, the method ensures that LLMs learn to identify hallucinations more effectively over time. Experimental results show that the proposed HaluCheck models achieve significant performance gains, particularly on challenging benchmarks, and demonstrate strong performance even in zero-shot scenarios."
                },
                "zh": {
                    "title": "Âà©Áî®ËØæÁ®ãÂ≠¶‰π†ÊèêÂçáÂπªËßâÊ£ÄÊµãËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú®DPOÂØπÈΩêËøáÁ®ã‰∏≠‰ΩøÁî®Á≤æÂøÉËÆæËÆ°ÁöÑÂπªËßâÊ†∑Êú¨ÁöÑËØæÁ®ãÂ≠¶‰π†ÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂØπÂπªËßâÁöÑÊ£ÄÊµãËÉΩÂäõ„ÄÇÊàë‰ª¨ËÆ§ËØÜÂà∞ÔºåÂπªËßâÊ†∑Êú¨ÈÄöÂ∏∏ÊØî‰º†ÁªüÁöÑË¥üÊ†∑Êú¨ÂÖ∑ÊúâÊõ¥È´òÁöÑÊ¨∫È™óÊÄßÔºåÂõ†Ê≠§Â∞ÜËøô‰∫õÂπªËßâÊ†∑Êú¨‰Ωú‰∏∫Ë¥ü‰æã‰ΩøÁî®„ÄÇÈÄöËøáÈÄêÊ≠•ÂºïÂÖ•Êõ¥ÈöæÁöÑÊ†∑Êú¨ÔºåÊàë‰ª¨ÁöÑËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•Á°Æ‰øù‰∫ÜÁ®≥ÂÆöÁöÑÂ¢ûÈáèÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®ËØæÁ®ãDPOÊñπÊ≥ïÂíåÈ´òË¥®ÈáèË¥üÊ†∑Êú¨ËÆ≠ÁªÉÁöÑHaluCheckÊ®°ÂûãÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåÂ∞§ÂÖ∂Âú®MedHalluÂíåHaluEvalÁ≠âÂõ∞ÈöæÂü∫ÂáÜ‰∏äÊèêÂçá‰∫ÜÂ§öËææ24%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17412",
            "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
            "url": "https://huggingface.co/papers/2505.17412",
            "abstract": "A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating high resolution 3D shapes using volumetric representations such as Signed Distance Functions presents substantial computational and memory challenges. We introduce Direct3D S2, a scalable 3D generation framework based on sparse volumes that achieves superior output quality with dramatically reduced training costs. Our key innovation is the Spatial Sparse Attention mechanism, which greatly enhances the efficiency of Diffusion Transformer computations on sparse volumetric data. SSA allows the model to effectively process large token sets within sparse volumes, significantly reducing computational overhead and achieving a 3.9x speedup in the forward pass and a 9.6x speedup in the backward pass. Our framework also includes a variational autoencoder that maintains a consistent sparse volumetric format across input, latent, and output stages. Compared to previous methods with heterogeneous representations in 3D VAE, this unified design significantly improves training efficiency and stability. Our model is trained on public available datasets, and experiments demonstrate that Direct3D S2 not only surpasses state-of-the-art methods in generation quality and efficiency, but also enables training at 1024 resolution using only 8 GPUs, a task typically requiring at least 32 GPUs for volumetric representations at 256 resolution, thus making gigascale 3D generation both practical and accessible. Project page: https://nju3dv.github.io/projects/Direct3D-S2/.",
            "score": 13,
            "issue_id": 3948,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "e0f74abb880208f9",
            "authors": [
                "Shuang Wu",
                "Youtian Lin",
                "Feihu Zhang",
                "Yifei Zeng",
                "Yikang Yang",
                "Yajie Bao",
                "Jiachen Qian",
                "Siyu Zhu",
                "Philip Torr",
                "Xun Cao",
                "Yao Yao"
            ],
            "affiliations": [
                "DreamTech",
                "Fudan University",
                "Nanjing University",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17412.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#training",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "üßä",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Direct3D S2 - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–º—ã –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ. –ö–ª—é—á–µ–≤–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–µ–π —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º Spatial Sparse Attention, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –µ–¥–∏–Ω—ã–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–º–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –Ω–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Direct3D S2 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø–æ–∑–≤–æ–ª—è—è –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 1024 –Ω–∞ –≤—Å–µ–≥–æ 8 GPU."
                },
                "en": {
                    "title": "Efficient High-Resolution 3D Shape Generation with Sparse Volumes",
                    "desc": "This paper presents Direct3D S2, a framework for generating high-resolution 3D shapes using sparse volumetric representations. It introduces a Spatial Sparse Attention mechanism that enhances the efficiency of computations in Diffusion Transformers, allowing for significant reductions in training time and resource usage. The framework employs a variational autoencoder to maintain a consistent format across different stages of processing, improving training stability. Overall, Direct3D S2 achieves superior generation quality while drastically lowering the computational requirements, making high-resolution 3D shape generation more accessible."
                },
                "zh": {
                    "title": "È´òÊïàÁîüÊàêÈ´òÂàÜËæ®Áéá3DÂΩ¢Áä∂ÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑ3DÂΩ¢Áä∂ÁîüÊàêÊ°ÜÊû∂ÔºåÂêç‰∏∫Direct3D S2ÔºåÂà©Áî®Á®ÄÁñè‰ΩìÁßØÂíåÁ©∫Èó¥Á®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§ü‰ª•ËæÉ‰ΩéÁöÑËÆ°ÁÆóÈúÄÊ±ÇÁîüÊàêÈ´òÂàÜËæ®ÁéáÁöÑ3DÂΩ¢Áä∂„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁ®ÄÁñè‰ΩìÁßØÁöÑËÆæËÆ°ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêË¥®ÈáèÂπ∂Èôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨„ÄÇÁ©∫Èó¥Á®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÊèêÂçá‰∫ÜÊâ©Êï£ÂèòÊç¢Âô®Âú®Á®ÄÁñè‰ΩìÁßØÊï∞ÊçÆ‰∏äÁöÑËÆ°ÁÆóÊïàÁéáÔºåÂÆûÁé∞‰∫ÜÂâçÂêë‰º†Êí≠ÈÄüÂ∫¶ÊèêÈ´ò3.9ÂÄçÂíåÂèçÂêë‰º†Êí≠ÈÄüÂ∫¶ÊèêÈ´ò9.6ÂÄç„ÄÇ‰∏é‰º†ÁªüÁöÑ3DÂèòÂàÜËá™ÁºñÁ†ÅÂô®Áõ∏ÊØîÔºåDirect3D S2Âú®ËÆ≠ÁªÉÊïàÁéáÂíåÁ®≥ÂÆöÊÄß‰∏äÊúâ‰∫ÜÊòæËëóÊîπÂñÑÔºå‰ΩøÂæóÂú®‰ªÖ‰ΩøÁî®8‰∏™GPUÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞1024ÂàÜËæ®ÁéáÁöÑËÆ≠ÁªÉÊàê‰∏∫ÂèØËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17399",
            "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering\n  Workflow",
            "url": "https://huggingface.co/papers/2505.17399",
            "abstract": "FullFront is a benchmark evaluating Multimodal Large Language Models across conceptualization, comprehension, and implementation phases in front-end engineering.  \t\t\t\t\tAI-generated summary \t\t\t\t Front-end engineering involves a complex workflow where engineers conceptualize designs, translate them into code, and iteratively refine the implementation. While recent benchmarks primarily focus on converting visual designs to code, we present FullFront, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) across the full front-end development pipeline. FullFront assesses three fundamental tasks that map directly to the front-end engineering pipeline: Webpage Design (conceptualization phase), Webpage Perception QA (comprehension of visual organization and elements), and Webpage Code Generation (implementation phase). Unlike existing benchmarks that use either scraped websites with bloated code or oversimplified LLM-generated HTML, FullFront employs a novel, two-stage process to transform real-world webpages into clean, standardized HTML while maintaining diverse visual designs and avoiding copyright issues. Extensive testing of state-of-the-art MLLMs reveals significant limitations in page perception, code generation (particularly for image handling and layout), and interaction implementation. Our results quantitatively demonstrate performance disparities across models and tasks, and highlight a substantial gap between current MLLM capabilities and human expert performance in front-end engineering. The FullFront benchmark and code are available in https://github.com/Mikivishy/FullFront.",
            "score": 13,
            "issue_id": 3951,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "da19bb4affb160fc",
            "authors": [
                "Haoyu Sun",
                "Huichen Will Wang",
                "Jiawei Gu",
                "Linjie Li",
                "Yu Cheng"
            ],
            "affiliations": [
                "Microsoft",
                "Sun Yat-sen University",
                "The Chinese University of Hong Kong",
                "Tongji University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17399.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#multimodal",
                    "#optimization",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "üñ•Ô∏è",
                "ru": {
                    "title": "FullFront: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ MLLM –≤ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ",
                    "desc": "FullFront - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –æ–±–ª–∞—Å—Ç–∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏. –û–Ω –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏: –¥–∏–∑–∞–π–Ω –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü, –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞. –ë–µ–Ω—á–º–∞—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ HTML –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö MLLM –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "FullFront: Bridging the Gap in Front-End Engineering with MLLMs",
                    "desc": "FullFront is a benchmark that evaluates Multimodal Large Language Models (MLLMs) in the context of front-end engineering, which includes design, comprehension, and implementation tasks. It focuses on three key areas: conceptualizing webpage designs, understanding visual elements through QA, and generating clean HTML code. Unlike previous benchmarks, FullFront uses a unique two-stage process to create standardized HTML from real-world webpages, ensuring diverse designs without copyright issues. The findings reveal significant performance gaps in MLLMs compared to human experts, particularly in page perception and code generation tasks."
                },
                "zh": {
                    "title": "ÂÖ®Èù¢ËØÑ‰º∞ÂâçÁ´ØÂ∑•Á®ãÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "FullFrontÊòØ‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂâçÁ´ØÂ∑•Á®ã‰∏≠ÁöÑÊ¶ÇÂøµÂåñ„ÄÅÁêÜËß£ÂíåÂÆûÁé∞Èò∂ÊÆµÁöÑË°®Áé∞„ÄÇÂÆÉÊ∂µÁõñ‰∫ÜÁΩëÈ°µËÆæËÆ°„ÄÅÁΩëÈ°µÊÑüÁü•ÈóÆÁ≠îÂíåÁΩëÈ°µ‰ª£Á†ÅÁîüÊàê‰∏â‰∏™Âü∫Êú¨‰ªªÂä°ÔºåÂèçÊò†‰∫ÜÂâçÁ´ØÂºÄÂèëÁöÑÂÆåÊï¥ÊµÅÁ®ã„ÄÇ‰∏éÁé∞ÊúâÂü∫ÂáÜ‰∏çÂêåÔºåFullFrontÈááÁî®Êñ∞È¢ñÁöÑ‰∏§Èò∂ÊÆµËøáÁ®ãÔºåÂ∞ÜÁúüÂÆûÁΩëÈ°µËΩ¨Âåñ‰∏∫Âπ≤ÂáÄ„ÄÅÊ†áÂáÜÂåñÁöÑHTMLÔºåÂêåÊó∂‰øùÊåÅÂ§öÊ†∑ÁöÑËßÜËßâËÆæËÆ°„ÄÇÊµãËØïÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®È°µÈù¢ÊÑüÁü•„ÄÅ‰ª£Á†ÅÁîüÊàêÂíå‰∫§‰∫íÂÆûÁé∞ÊñπÈù¢Â≠òÂú®ÊòæËëóÁöÑÂ±ÄÈôêÊÄßÔºåËøú‰Ωé‰∫é‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17955",
            "title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply",
            "url": "https://huggingface.co/papers/2505.17955",
            "abstract": "A study of diffusion classifiers across multiple datasets and tasks reveals their compositional understanding, highlighting domain-specific performance effects and timestep weighting importance.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.",
            "score": 12,
            "issue_id": 3954,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "ca791ea943cccda5",
            "authors": [
                "Yujin Jeong",
                "Arnas Uselis",
                "Seong Joon Oh",
                "Anna Rohrbach"
            ],
            "affiliations": [
                "TU Darmstadt & hessian.AI",
                "T√ºbingen AI Center & University of T√ºbingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17955.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–∏—Ñ—Ñ—É–∑–∏–∏: –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å —Å —É—Å–ª–æ–≤–∏—è–º–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –≤—ã—è–≤–ª—è–µ—Ç –∏—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –û–Ω–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –≤–∞–∂–Ω–æ—Å—Ç—å –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç —Ç—Ä–∏ –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ (SD 1.5, 2.0 –∏ 3-m) –Ω–∞ 10 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –±–æ–ª–µ–µ —á–µ–º 30 –∑–∞–¥–∞—á–∞—Ö. –û–Ω–∏ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ Self-Bench –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –¥–æ–º–µ–Ω–∞."
                },
                "en": {
                    "title": "Unlocking Compositional Understanding in Diffusion Classifiers",
                    "desc": "This paper investigates how diffusion classifiers perform on various datasets and tasks, focusing on their ability to understand complex compositions. It highlights the importance of the specific domains of datasets and how they affect the classifiers' performance. The study introduces a new benchmark called Self-Bench to evaluate the classifiers using images generated by diffusion models. Additionally, it examines the significance of timestep weighting and its relationship with domain sensitivity, particularly in the latest diffusion model SD3-m."
                },
                "zh": {
                    "title": "Êâ©Êï£ÂàÜÁ±ªÂô®ÔºöÁªÑÂêàÁêÜËß£ÁöÑÊñ∞ËßÜËßí",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÊâ©Êï£ÂàÜÁ±ªÂô®Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜÂíå‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÊè≠Á§∫‰∫ÜÂÆÉ‰ª¨ÂØπÁªÑÂêàÁêÜËß£ÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁâπÂÆöÈ¢ÜÂüüÁöÑÊÄßËÉΩÂΩ±ÂìçÂíåÊó∂Èó¥Ê≠•Âä†ÊùÉÁöÑÈáçË¶ÅÊÄßÂØπÊ®°ÂûãÁöÑÊàêÂäüËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫Ü‰∏âÁßçÊâ©Êï£Ê®°ÂûãÂú®ÂçÅ‰∏™Êï∞ÊçÆÈõÜÂíå‰∏âÂçÅÂ§ö‰∏™‰ªªÂä°‰∏äÁöÑÂà§Âà´ËÉΩÂäõÔºåÈ¶ñÊ¨°ÂºïÂÖ•‰∫ÜËá™ÊàëÂü∫ÂáÜÊµãËØï‰ª•ÈöîÁ¶ªÈ¢ÜÂüüÊïàÂ∫î„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÊâ©Êï£ÂàÜÁ±ªÂô®Âú®ÁªÑÂêàÊÄßÁêÜËß£ÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂÖ∂ÊïàÊûúÂèóÁâπÂÆöÊù°‰ª∂ÁöÑÂΩ±Âìç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15692",
            "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
            "url": "https://huggingface.co/papers/2505.15692",
            "abstract": "A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance (\"thought patterns\"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.",
            "score": 12,
            "issue_id": 3946,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "5b1bedaf6be49ffa",
            "authors": [
                "Jinyang Wu",
                "Chonghua Liao",
                "Mingkuan Feng",
                "Shuai Zhang",
                "Zhengqi Wen",
                "Pengpeng Shao",
                "Huazhe Xu",
                "Jianhua Tao"
            ],
            "affiliations": [
                "Beijing National Research Center for Information Science and Technology",
                "Department of Automation, Tsinghua University",
                "Institution for Interdisciplinary Information Sciences, Tsinghua University",
                "Shanghai AI Lab",
                "Shanghai Qi Zhi Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15692.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#interpretability",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "TAPO: –£—Å–∏–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏–º–∏ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏",
                    "desc": "TAPO - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –≤–Ω–µ—à–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, TAPO –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä—É—è –º–µ–∂–¥—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ TAPO –Ω–∞–¥ GRPO –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–Ω–∞–ª–∏–∑ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª, —á—Ç–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –≤–Ω–µ—à–Ω–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–æ—â–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å—é –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å—é –≤—ã–≤–æ–¥–æ–≤."
                },
                "en": {
                    "title": "Enhancing Reinforcement Learning with Thought Patterns",
                    "desc": "The paper introduces TAPO (Thought-Augmented Policy Optimization), a new reinforcement learning framework that enhances model performance by integrating external guidance. Traditional RL methods often limit exploration by focusing solely on reward-maximizing paths, which restricts the reasoning capabilities of the models. TAPO addresses this issue by incorporating structured thought patterns during training, allowing for a better balance between internal exploration and external guidance. Experimental results demonstrate that TAPO significantly outperforms existing methods, leading to improved reasoning models that are more explainable and readable."
                },
                "zh": {
                    "title": "TAPOÔºöÂ¢ûÂº∫Êé¢Á¥¢‰∏éÊé®ÁêÜÁöÑÊñ∞Ê°ÜÊû∂",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂TAPOÔºåÈÄöËøáÊï¥ÂêàÂ§ñÈÉ®ÊåáÂØºÊù•ÊèêÂçáÊ®°ÂûãÊÄßËÉΩÂíåÊé¢Á¥¢ËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂæÄÂæÄÂè™ÂÖ≥Ê≥®ÊúÄÂ§ßÂåñÂ•ñÂä±Ë∑ØÂæÑÔºåÁº∫‰πèÂ§ñÈÉ®Áü•ËØÜÁöÑÂºïÂÖ•ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõ„ÄÇTAPOÈÄöËøáÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÈÄÇÂ∫îÊÄßÂú∞Êï¥ÂêàÁªìÊûÑÂåñÊÄùÁª¥ÔºåÂπ≥Ë°°‰∫ÜÊ®°ÂûãÂÜÖÈÉ®ÁöÑÊé¢Á¥¢‰∏éÂ§ñÈÉ®ÊåáÂØºÁöÑÂà©Áî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTAPOÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂπøÊ≥õÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16479",
            "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
            "url": "https://huggingface.co/papers/2505.16479",
            "abstract": "A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.  \t\t\t\t\tAI-generated summary \t\t\t\t Restoring nighttime images affected by multiple adverse weather conditions is a practical yet under-explored research problem, as multiple weather conditions often coexist in the real world alongside various lighting effects at night. This paper first explores the challenging multi-weather nighttime image restoration task, where various types of weather degradations are intertwined with flare effects. To support the research, we contribute the AllWeatherNight dataset, featuring large-scale high-quality nighttime images with diverse compositional degradations, synthesized using our introduced illumination-aware degradation generation. Moreover, we present ClearNight, a unified nighttime image restoration framework, which effectively removes complex degradations in one go. Specifically, ClearNight extracts Retinex-based dual priors and explicitly guides the network to focus on uneven illumination regions and intrinsic texture contents respectively, thereby enhancing restoration effectiveness in nighttime scenarios. In order to better represent the common and unique characters of multiple weather degradations, we introduce a weather-aware dynamic specific-commonality collaboration method, which identifies weather degradations and adaptively selects optimal candidate units associated with specific weather types. Our ClearNight achieves state-of-the-art performance on both synthetic and real-world images. Comprehensive ablation experiments validate the necessity of AllWeatherNight dataset as well as the effectiveness of ClearNight. Project page: https://henlyta.github.io/ClearNight/mainpage.html",
            "score": 11,
            "issue_id": 3950,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "e02bace9da76256b",
            "authors": [
                "Yuetong Liu",
                "Yunqiu Xu",
                "Yang Wei",
                "Xiuli Bi",
                "Bin Xiao"
            ],
            "affiliations": [
                "Chongqing University of Posts and Telecommunications",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16479.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset"
                ],
                "emoji": "üåô",
                "ru": {
                    "title": "–ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Å–ª–æ–∂–Ω—ã—Ö –ø–æ–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç AllWeatherNight —Å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –Ω–æ—á–Ω—ã–º–∏ —Å–Ω–∏–º–∫–∞–º–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ClearNight –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–æ–π–Ω—ã–µ –∞–ø—Ä–∏–æ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–æ—Ä–∏–∏ –†–µ—Ç–∏–Ω–µ–∫—Å–∞ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∏—Å–∫–∞–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–∫ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö, —Ç–∞–∫ –∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö."
                },
                "en": {
                    "title": "ClearNight: Mastering Nighttime Image Restoration Across Weather Conditions",
                    "desc": "This paper addresses the challenge of restoring nighttime images that are affected by various weather conditions and lighting effects. It introduces the AllWeatherNight dataset, which contains high-quality nighttime images with different types of weather degradations. The authors propose a framework called ClearNight that utilizes Retinex-based dual priors to enhance image restoration by focusing on illumination and texture. Additionally, a weather-aware collaboration method is introduced to adaptively handle different weather conditions, resulting in state-of-the-art performance in image restoration tasks."
                },
                "zh": {
                    "title": "Áªü‰∏ÄÊ°ÜÊû∂ÔºåÊ∏ÖÊô∞Â§úÊôØ",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®Â§öÁßçÊÅ∂Âä£Â§©Ê∞îÊù°‰ª∂‰∏ãÊÅ¢Â§çÂ§úÈó¥ÂõæÂÉèÁöÑÊåëÊàòÊÄß‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜAllWeatherNightÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Â§öÊ†∑ÂåñÁöÑÂ§úÈó¥ÂõæÂÉèÔºåÂ∏ÆÂä©Á†îÁ©∂‰∏çÂêåÂ§©Ê∞îÈÄÄÂåñÁöÑÂΩ±Âìç„ÄÇClearNightÊòØÊàë‰ª¨ÊèêÂá∫ÁöÑÁªü‰∏ÄÂ§úÈó¥ÂõæÂÉèÊÅ¢Â§çÊ°ÜÊû∂ÔºåËÉΩÂ§üÊúâÊïàÂéªÈô§Â§çÊùÇÁöÑÈÄÄÂåñÁé∞Ë±°„ÄÇÈÄöËøáÂºïÂÖ•Â§©Ê∞îÊÑüÁü•ÁöÑÂä®ÊÄÅÁâπÂÆö-ÂÖ±ÊÄßÂçè‰ΩúÊñπÊ≥ïÔºåClearNightÂú®ÂêàÊàêÂíåÁúüÂÆûÂõæÂÉè‰∏äÂùáÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16134",
            "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in\n  Large Language Models",
            "url": "https://huggingface.co/papers/2505.16134",
            "abstract": "Large language models exhibit positional bias -- systematic neglect of information at specific context positions -- yet its interplay with linguistic diversity remains poorly understood. We present a cross-linguistic study across five typologically distinct languages (English, Russian, German, Hindi, Vietnamese), examining how positional bias interacts with model uncertainty, syntax, and prompting. Key findings: (1) Positional bias is model-driven, with language-specific variations -- Qwen2.5-7B favors late positions, challenging assumptions of early-token bias; (2) Explicit positional guidance (e.g., correct context is at position X) reduces accuracy across languages, undermining prompt-engineering practices; (3) Aligning context with positional bias increases entropy, yet minimal entropy does not predict accuracy. (4) We further uncover that LLMs differently impose dominant word order in free-word-order languages like Hindi.",
            "score": 11,
            "issue_id": 3957,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "5849eec94fecbfa3",
            "authors": [
                "Menschikov Mikhail",
                "Alexander Kharitonov",
                "Maiia Kotyga",
                "Vadim Porvatov",
                "Anna Zhukovskaya",
                "David Kagramanyan",
                "Egor Shvetsov",
                "Evgeny Burnaev"
            ],
            "affiliations": [
                "AIRI, Moscow, Russia",
                "HSE University, Moscow, Russia",
                "ITMO, Saint-Petersburg, Russia",
                "Lomonosov MSU, Moscow, Russia",
                "MIPT, Moscow, Russia",
                "Sber, Moscow, Russia",
                "Skoltech, Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16134.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#alignment",
                    "#multilingual"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö: –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –ø—è—Ç–∏ —Ç–∏–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏, –∞ –Ω–µ –æ—Ç —è–∑—ã–∫–∞, –ø—Ä–∏—á–µ–º –º–æ–¥–µ–ª—å Qwen2.5-7B –æ—Ç–¥–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –ø–æ–∑–¥–Ω–∏–º –ø–æ–∑–∏—Ü–∏—è–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —è–≤–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —É–∫–∞–∑–∞–Ω–∏—è —Å–Ω–∏–∂–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º —Å–º–µ—â–µ–Ω–∏–µ–º —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é. –¢–∞–∫–∂–µ –±—ã–ª–æ –≤—ã—è–≤–ª–µ–Ω–æ, —á—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –ø–æ-—Ä–∞–∑–Ω–æ–º—É –Ω–∞–≤—è–∑—ã–≤–∞—é—Ç –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤ –≤ —è–∑—ã–∫–∞—Ö —Å–æ —Å–≤–æ–±–æ–¥–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º —Å–ª–æ–≤."
                },
                "en": {
                    "title": "Understanding Positional Bias in Multilingual Contexts",
                    "desc": "This paper investigates how large language models (LLMs) show positional bias, which means they often ignore information based on where it appears in a sentence. The study looks at five different languages to see how this bias interacts with factors like model uncertainty and syntax. It finds that the bias varies by language and that giving explicit positional hints can actually lower accuracy. Additionally, the research reveals that LLMs handle word order differently in languages that allow more flexibility, such as Hindi."
                },
                "zh": {
                    "title": "Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑ‰ΩçÁΩÆÂÅèËßÅ‰∏éËØ≠Ë®ÄÂ§öÊ†∑ÊÄßÁöÑÂÖ≥Á≥ª",
                    "desc": "ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑ‰ΩçÁΩÆÂÅèËßÅÔºåÂç≥Âú®ÁâπÂÆö‰∏ä‰∏ãÊñá‰ΩçÁΩÆ‰∏äÁ≥ªÁªüÊÄßÂøΩËßÜ‰ø°ÊÅØÁöÑÁé∞Ë±°„ÄÇÁ†îÁ©∂Ê∂µÁõñ‰∫Ü‰∫îÁßç‰∏çÂêåÁ±ªÂûãÁöÑËØ≠Ë®ÄÔºàËã±ËØ≠„ÄÅ‰øÑËØ≠„ÄÅÂæ∑ËØ≠„ÄÅÂç∞Âú∞ËØ≠ÂíåË∂äÂçóËØ≠ÔºâÔºåÊé¢ËÆ®‰∫Ü‰ΩçÁΩÆÂÅèËßÅ‰∏éÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄß„ÄÅÂè•Ê≥ïÂíåÊèêÁ§∫‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇ‰∏ªË¶ÅÂèëÁé∞ÂåÖÊã¨Ôºö‰ΩçÁΩÆÂÅèËßÅÊòØÁî±Ê®°ÂûãÈ©±Âä®ÁöÑÔºåÂπ∂‰∏îÂú®‰∏çÂêåËØ≠Ë®Ä‰∏≠Â≠òÂú®ÁâπÂÆöÁöÑÂèòÂåñÔºõÊòéÁ°ÆÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØÊåáÂØº‰ºöÈôç‰ΩéÂêÑËØ≠Ë®ÄÁöÑÂáÜÁ°ÆÊÄßÔºåÊåëÊàò‰∫ÜÊèêÁ§∫Â∑•Á®ãÁöÑÂÅáËÆæ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13508",
            "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2505.13508",
            "abstract": "A novel framework, Time-R1, enhances moderate-sized LLMs with comprehensive temporal abilities through a reinforcement learning curriculum, outperforming larger models on future event prediction and creative scenario generation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce Time-R1, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a reinforcement learning (RL) curriculum driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release Time-Bench, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of Time-R1 checkpoints.",
            "score": 11,
            "issue_id": 3951,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 –º–∞—è",
                "en": "May 16",
                "zh": "5Êúà16Êó•"
            },
            "hash": "bd70d09511fa2ae4",
            "authors": [
                "Zijia Liu",
                "Peixuan Han",
                "Haofei Yu",
                "Haoru Li",
                "Jiaxuan You"
            ],
            "affiliations": [
                "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13508.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "‚è≥",
                "ru": {
                    "title": "–ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º —á—É–≤—Å—Ç–≤–æ–º –≤—Ä–µ–º–µ–Ω–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Time-R1 - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ò—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, Time-R1 –ø–æ—ç—Ç–∞–ø–Ω–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Time-R1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±—É–¥—É—â–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Time-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò-—Å–∏—Å—Ç–µ–º."
                },
                "en": {
                    "title": "Empowering Smaller Models with Temporal Intelligence",
                    "desc": "The paper introduces Time-R1, a new framework that enhances moderate-sized large language models (LLMs) with advanced temporal reasoning capabilities. It utilizes a reinforcement learning curriculum to develop skills in understanding past events, predicting future occurrences, and generating creative scenarios. Unlike existing methods that focus on isolated temporal tasks, Time-R1 enables comprehensive temporal abilities, allowing the model to generalize well beyond its training data. Experimental results show that Time-R1 outperforms much larger models in challenging benchmarks, demonstrating the effectiveness of its structured approach to temporal learning."
                },
                "zh": {
                    "title": "Time-R1ÔºöÂ∞èÊ®°ÂûãÁöÑÊó∂Èó¥Êô∫ËÉΩÈù©ÂëΩ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂Time-R1ÔºåÊó®Âú®Â¢ûÂº∫‰∏≠Á≠âËßÑÊ®°ÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Êó∂Èó¥Êé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËØæÁ®ãÔºåTime-R1ËÉΩÂ§üÁêÜËß£ÂéÜÂè≤‰∫ã‰ª∂„ÄÅÈ¢ÑÊµãÊú™Êù•‰∫ã‰ª∂ÔºåÂπ∂ÁîüÊàêÂàõÊÑèÂú∫ÊôØÔºåË∂ÖË∂ä‰∫ÜÊõ¥Â§ßÊ®°ÂûãÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∏âÈò∂ÊÆµÁöÑÂèëÂ±ïË∑ØÂæÑÔºåÈÄêÊ≠•ÊûÑÂª∫Êó∂Èó¥ÁêÜËß£„ÄÅÊú™Êù•‰∫ã‰ª∂È¢ÑÊµãÂíåÂàõÊÑèÁîüÊàêËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTime-R1Âú®Êú™Êù•‰∫ã‰ª∂È¢ÑÊµãÂíåÂàõÊÑèÂú∫ÊôØÁîüÊàêÁöÑÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é200ÂÄç‰ª•‰∏äÁöÑÂ§ßÂûãÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16483",
            "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.16483",
            "abstract": "CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to improve the faithfulness of LLMs in both short-form and long-form generation tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different downstream tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
            "score": 10,
            "issue_id": 3945,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "3bd78fedbb109d9a",
            "authors": [
                "Shuzheng Si",
                "Haozhe Zhao",
                "Cheng Gao",
                "Yuzhuo Bai",
                "Zhitong Wang",
                "Bofei Gao",
                "Kangyang Luo",
                "Wenhao Li",
                "Yufei Huang",
                "Gang Chen",
                "Fanchao Qi",
                "Minjia Zhang",
                "Baobao Chang",
                "Maosong Sun"
            ],
            "affiliations": [
                "DeepLang AI",
                "Peking University",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16483.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "üõ∂",
                "ru": {
                    "title": "–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏: CANOE —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏",
                    "desc": "CANOE - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö –ø–∞—Ä–∞—Ö –∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É Dual-GRPO. CANOE –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç—Ä–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ CANOE –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ 11 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥–∞–∂–µ —Å–∞–º—ã–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤—Ä–æ–¥–µ GPT-4."
                },
                "en": {
                    "title": "Enhancing LLM Faithfulness with CANOE and Synthetic Data",
                    "desc": "The paper presents CANOE, a framework designed to enhance the faithfulness of large language models (LLMs) in generating text. It achieves this by creating synthetic question-answering (QA) data, which serves as high-quality training material without requiring human annotations. The framework employs a novel reinforcement learning approach called Dual-GRPO, which uses rule-based rewards to optimize both short-form and long-form text generation. Experimental results demonstrate that CANOE significantly improves LLM performance across various tasks, surpassing even state-of-the-art models like GPT-4o."
                },
                "zh": {
                    "title": "CANOEÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØ‰ø°Â∫¶",
                    "desc": "CANOEÊòØ‰∏Ä‰∏™Á≥ªÁªüÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁîüÊàê‰ªªÂä°‰∏≠ÁöÑÂèØ‰ø°Â∫¶ÔºåËÄåÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂêàÊàêÁü≠ÂΩ¢ÂºèÈóÆÁ≠îÔºàQAÔºâÊï∞ÊçÆÔºåÊûÑÂª∫È´òË¥®ÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî®ÂèåÈáçGRPOÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÊù•‰ºòÂåñÁîüÊàêËøáÁ®ã„ÄÇÂèåÈáçGRPOÁªìÂêà‰∫ÜÂü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåÁ°Æ‰øùÂú®Áü≠ÂΩ¢ÂºèÂíåÈïøÂΩ¢ÂºèÁîüÊàê‰ªªÂä°‰∏≠ÈÉΩËÉΩÊúâÊïàÊèêÂçáÊ®°ÂûãÁöÑË°®Áé∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCANOEÂú®11‰∏™‰∏çÂêåÁöÑ‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜLLMsÁöÑÂèØ‰ø°Â∫¶ÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂ¶ÇGPT-4oÂíåOpenAI o1„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17417",
            "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
            "url": "https://huggingface.co/papers/2505.17417",
            "abstract": "The rapid growth of voice assistants powered by large language models (LLM) has highlighted a need for speech instruction data to train these systems. Despite the abundance of speech recognition data, there is a notable scarcity of speech instruction data, which is essential for fine-tuning models to understand and execute spoken commands. Generating high-quality synthetic speech requires a good text-to-speech (TTS) model, which may not be available to low resource languages. Our novel approach addresses this challenge by halting synthesis at the semantic representation level, bypassing the need for TTS. We achieve this by aligning synthetic semantic representations with the pre-trained Whisper encoder, enabling an LLM to be fine-tuned on text instructions while maintaining the ability to understand spoken instructions during inference. This simplified training process is a promising approach to building voice assistant for low-resource languages.",
            "score": 9,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "9d72b20aca0789ee",
            "authors": [
                "Alan Dao",
                "Dinh Bach Vu",
                "Huy Hoang Ha",
                "Tuan Le Duc Anh",
                "Shreyas Gopal",
                "Yue Heng Yeo",
                "Warren Keng Hoong Low",
                "Eng Siong Chng",
                "Jia Qi Yip"
            ],
            "affiliations": [
                "CCDS, Nanyang Technological University, Singapore",
                "Menlo Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17417.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#data",
                    "#synthetic",
                    "#training",
                    "#audio"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "–ì–æ–ª–æ—Å–æ–≤—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–ª—è —Ä–µ–¥–∫–∏—Ö —è–∑—ã–∫–æ–≤: –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ TTS",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –æ–±–æ–π—Ç–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ text-to-speech, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è —Å–∏–Ω—Ç–µ–∑ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –ø—É—Ç–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º —ç–Ω–∫–æ–¥–µ—Ä–æ–º Whisper. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–Ω–∏–º–∞—Ç—å —É—Å—Ç–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ."
                },
                "en": {
                    "title": "Empowering Voice Assistants for Low-Resource Languages",
                    "desc": "This paper addresses the challenge of training voice assistants in low-resource languages, where there is a lack of speech instruction data. It proposes a novel method that generates synthetic speech by stopping at the semantic representation level, eliminating the need for a text-to-speech (TTS) model. By aligning these semantic representations with the pre-trained Whisper encoder, the approach allows for fine-tuning large language models (LLMs) on text instructions while still being able to process spoken commands. This method simplifies the training process and enhances the development of voice assistants for languages with limited resources."
                },
                "zh": {
                    "title": "‰∏∫‰ΩéËµÑÊ∫êËØ≠Ë®ÄÊûÑÂª∫ËØ≠Èü≥Âä©ÊâãÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏∫ËØ≠Èü≥Âä©ÊâãËÆ≠ÁªÉÊâÄÈúÄÁöÑËØ≠Èü≥Êåá‰ª§Êï∞ÊçÆÁöÑ‰∏çË∂≥ÈóÆÈ¢ò„ÄÇÂ∞ΩÁÆ°ËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆ‰∏∞ÂØåÔºå‰ΩÜËØ≠Èü≥Êåá‰ª§Êï∞ÊçÆÂç¥Áõ∏ÂØπÁ®ÄÁº∫ÔºåËøôÂØπÊ®°ÂûãÁêÜËß£ÂíåÊâßË°åÂè£Â§¥ÂëΩ‰ª§Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøáÂú®ËØ≠‰πâË°®Á§∫Â±ÇÈù¢ÂÅúÊ≠¢ÂêàÊàêÔºåÈÅøÂÖç‰∫ÜÂØπÊñáÊú¨Âà∞ËØ≠Èü≥ÔºàTTSÔºâÊ®°ÂûãÁöÑ‰æùËµñ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÂêàÊàêÁöÑËØ≠‰πâË°®Á§∫‰∏éÈ¢ÑËÆ≠ÁªÉÁöÑWhisperÁºñÁ†ÅÂô®ÂØπÈΩêÔºå‰ΩøÂæóÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÉΩÂ§üÂú®ÊñáÊú¨Êåá‰ª§‰∏äËøõË°åÂæÆË∞ÉÔºåÂêåÊó∂Âú®Êé®ÁêÜËøáÁ®ã‰∏≠ÁêÜËß£Âè£Â§¥Êåá‰ª§„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17295",
            "title": "ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic\n  Systems",
            "url": "https://huggingface.co/papers/2505.17295",
            "abstract": "The ScanBot dataset, focusing on instruction-conditioned high-precision robotic surface scanning, showcases challenges for vision-language action models in achieving precise scanning trajectories under real-world constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce ScanBot, a novel dataset designed for instruction-conditioned, high-precision surface scanning in robotic systems. In contrast to existing robot learning datasets that focus on coarse tasks such as grasping, navigation, or dialogue, ScanBot targets the high-precision demands of industrial laser scanning, where sub-millimeter path continuity and parameter stability are critical. The dataset covers laser scanning trajectories executed by a robot across 12 diverse objects and 6 task types, including full-surface scans, geometry-focused regions, spatially referenced parts, functionally relevant structures, defect inspection, and comparative analysis. Each scan is guided by natural language instructions and paired with synchronized RGB, depth, and laser profiles, as well as robot pose and joint states. Despite recent progress, existing vision-language action (VLA) models still fail to generate stable scanning trajectories under fine-grained instructions and real-world precision demands. To investigate this limitation, we benchmark a range of multimodal large language models (MLLMs) across the full perception-planning-execution loop, revealing persistent challenges in instruction-following under realistic constraints.",
            "score": 9,
            "issue_id": 3960,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "212d967802dd2390",
            "authors": [
                "Zhiling Chen",
                "Yang Zhang",
                "Fardin Jalil Piran",
                "Qianyu Zhou",
                "Jiong Tang",
                "Farhad Imani"
            ],
            "affiliations": [
                "University of Connecticut"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17295.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#robotics",
                    "#multimodal"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "ScanBot: –≤—ã–∑–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ScanBot –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–º—É —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ù–∞–±–æ—Ä –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ª–∞–∑–µ—Ä–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è 12 –æ–±—ä–µ–∫—Ç–æ–≤ —Å 6 —Ç–∏–ø–∞–º–∏ –∑–∞–¥–∞—á, —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ–º—ã–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ-—è–∑—ã–∫–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏. –≠—Ç–æ –≤—ã—è–≤–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–ª–µ–º—ã –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö."
                },
                "en": {
                    "title": "ScanBot: Precision Scanning Meets Natural Language Instructions",
                    "desc": "The ScanBot dataset is a new resource aimed at improving robotic surface scanning by using natural language instructions. It focuses on high-precision tasks that require robots to follow detailed scanning paths with sub-millimeter accuracy. The dataset includes various scanning tasks across different objects, providing rich multimodal data such as RGB images, depth information, and laser profiles. Despite advancements in vision-language action models, the study shows that these models struggle to produce stable scanning trajectories when faced with complex, real-world instructions."
                },
                "zh": {
                    "title": "È´òÁ≤æÂ∫¶Êú∫Âô®‰∫∫Êâ´ÊèèÁöÑÊñ∞ÊåëÊàò",
                    "desc": "ScanBotÊï∞ÊçÆÈõÜ‰∏ìÊ≥®‰∫éÂü∫‰∫éÊåá‰ª§ÁöÑÈ´òÁ≤æÂ∫¶Êú∫Âô®‰∫∫Ë°®Èù¢Êâ´ÊèèÔºåÂ±ïÁ§∫‰∫ÜËßÜËßâ-ËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÂú®Áé∞ÂÆûÁ∫¶Êùü‰∏ãÂÆûÁé∞Á≤æÁ°ÆÊâ´ÊèèËΩ®ËøπÁöÑÊåëÊàò„ÄÇ‰∏éÁé∞ÊúâÁöÑÊú∫Âô®‰∫∫Â≠¶‰π†Êï∞ÊçÆÈõÜ‰∏çÂêåÔºåScanBotÈíàÂØπÂ∑•‰∏öÊøÄÂÖâÊâ´ÊèèÁöÑÈ´òÁ≤æÂ∫¶ÈúÄÊ±ÇÔºåÂº∫Ë∞É‰∫öÊØ´Á±≥Á∫ßÁöÑË∑ØÂæÑËøûÁª≠ÊÄßÂíåÂèÇÊï∞Á®≥ÂÆöÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜÊ∂µÁõñ‰∫ÜÊú∫Âô®‰∫∫Âú®12Áßç‰∏çÂêåÁâ©‰ΩìÂíå6Áßç‰ªªÂä°Á±ªÂûã‰∏ãÊâßË°åÁöÑÊøÄÂÖâÊâ´ÊèèËΩ®ËøπÔºåÂåÖÊã¨ÂÖ®Ë°®Èù¢Êâ´Êèè„ÄÅÂá†‰ΩïÈáçÁÇπÂå∫Âüü„ÄÅÁ©∫Èó¥ÂèÇËÄÉÈÉ®ÂàÜÁ≠â„ÄÇÂ∞ΩÁÆ°Â∑≤ÊúâËøõÂ±ïÔºåÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÂú®ÁªÜÁ≤íÂ∫¶Êåá‰ª§ÂíåÁé∞ÂÆûÁ≤æÂ∫¶Ë¶ÅÊ±Ç‰∏ã‰ªçÁÑ∂Êó†Ê≥ïÁîüÊàêÁ®≥ÂÆöÁöÑÊâ´ÊèèËΩ®Ëøπ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16770",
            "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs",
            "url": "https://huggingface.co/papers/2505.16770",
            "abstract": "A benchmark called RBench-V evaluates multi-modal models' vision-indispensable reasoning through image manipulation and auxiliary line construction, demonstrating that current models struggle with multi-modal outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of native multi-modal models and omni-models, exemplified by GPT-4o, Gemini, and o3, with their capability to process and generate content across modalities such as text and images, marks a significant milestone in the evolution of intelligence. Systematic evaluation of their multi-modal output capabilities in visual thinking processes (also known as multi-modal chain of thought, M-CoT) becomes critically important. However, existing benchmarks for evaluating multi-modal models primarily focus on assessing multi-modal inputs and text-only reasoning while neglecting the importance of reasoning through multi-modal outputs. In this paper, we present a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable reasoning abilities. To construct RBench-V, we carefully hand-pick 803 questions covering math, physics, counting, and games. Unlike previous benchmarks that typically specify certain input modalities, RBench-V presents problems centered on multi-modal outputs, which require image manipulation such as generating novel images and constructing auxiliary lines to support the reasoning process. We evaluate numerous open- and closed-source models on RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below the human score of 82.3%, highlighting that current models struggle to leverage multi-modal reasoning. Data and code are available at https://evalmodels.github.io/rbenchv",
            "score": 9,
            "issue_id": 3951,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "3e6a52b7ac905c1d",
            "authors": [
                "Meng-Hao Guo",
                "Xuanyu Chu",
                "Qianrui Yang",
                "Zhe-Han Mo",
                "Yiqing Shen",
                "Pei-lin Li",
                "Xinjie Lin",
                "Jinnian Zhang",
                "Xin-Sheng Chen",
                "Yi Zhang",
                "Kiyohiro Nakayama",
                "Zhengyang Geng",
                "Houwen Peng",
                "Han Hu",
                "Shi-Min Hu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Stanford University",
                "Tencent Hunyuan X",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16770.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –∑—Ä–µ–Ω–∏—è",
                    "desc": "RBench-V - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑—Ä–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 803 –≤–æ–ø—Ä–æ—Å–∞ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, —Ñ–∏–∑–∏–∫–µ, –ø–æ–¥—Å—á–µ—Ç—É –∏ –∏–≥—Ä–∞–º, —Ç—Ä–µ–±—É—é—â–∏–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –ª–∏–Ω–∏–π. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ o3 –∏ Gemini 2.5 Pro, –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ 25.8% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å 82.3% —É –ª—é–¥–µ–π. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–º –º–æ–¥–µ–ª—è–º —Å–ª–æ–∂–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è."
                },
                "en": {
                    "title": "RBench-V: Bridging the Gap in Multi-Modal Reasoning",
                    "desc": "The paper introduces RBench-V, a new benchmark designed to evaluate the reasoning capabilities of multi-modal models, particularly in tasks that require visual thinking and manipulation. It highlights that existing benchmarks often overlook the importance of assessing multi-modal outputs, focusing instead on inputs and text-only reasoning. RBench-V includes 803 carefully selected questions that require models to perform tasks like image generation and auxiliary line construction. The evaluation shows that even the top-performing models struggle significantly, achieving only 25.8% accuracy compared to a human benchmark of 82.3%, indicating a gap in current multi-modal reasoning abilities."
                },
                "zh": {
                    "title": "ËØÑ‰º∞Â§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Âü∫ÂáÜRBench-V",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫RBench-VÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÊ®°ÂûãÂú®ËßÜËßâÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÂΩìÂâçÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Â§ÑÁêÜÂõæÂÉèÂíåÊñáÊú¨Êó∂Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Â§öÊ®°ÊÄÅËæìÂá∫ÁöÑÊé®ÁêÜ‰∏ä‰ªçÁÑ∂Â≠òÂú®Âõ∞Èöæ„ÄÇRBench-VÈÄöËøá803‰∏™Ê∂µÁõñÊï∞Â≠¶„ÄÅÁâ©ÁêÜ„ÄÅËÆ°Êï∞ÂíåÊ∏∏ÊàèÁöÑÈóÆÈ¢òÔºå‰∏ìÊ≥®‰∫éÈúÄË¶ÅÂõæÂÉèÊìç‰ΩúÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÊµãËØïÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØË°®Áé∞ÊúÄÂ•ΩÁöÑÊ®°Âûão3ÔºåÂÖ∂ÂáÜÁ°ÆÁéá‰πü‰ªÖ‰∏∫25.8%ÔºåËøú‰Ωé‰∫é‰∫∫Á±ªÁöÑ82.3%ÔºåË°®ÊòéÁé∞ÊúâÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14146",
            "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL",
            "url": "https://huggingface.co/papers/2505.14146",
            "abstract": "A lightweight, model-agnostic framework decouples the retrieval and generation processes in RAG systems, enhancing performance with minimal training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.",
            "score": 9,
            "issue_id": 3953,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "75be81201c7c18e4",
            "authors": [
                "Pengcheng Jiang",
                "Xueqiang Xu",
                "Jiacheng Lin",
                "Jinfeng Xiao",
                "Zifeng Wang",
                "Jimeng Sun",
                "Jiawei Han"
            ],
            "affiliations": [
                "Amazon",
                "University of Illinois"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14146.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#rag",
                    "#rl",
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è RAG —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º",
                    "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è, –º–æ–¥–µ–ª—å–Ω–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ s3 –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö. –û–Ω–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—ã –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ–±—É—á–∞—è –ø–æ–∏—Å–∫–æ–≤—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º RAG. s3 —Ç—Ä–µ–±—É–µ—Ç –≤—Å–µ–≥–æ 2400 –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ –≤ 70 —Ä–∞–∑ –±–æ–ª—å—à–µ–º –æ–±—ä–µ–º–µ –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —à–µ—Å—Ç–∏ –æ–±—â–∏—Ö –∏ –ø—è—Ç–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã."
                },
                "en": {
                    "title": "Decoupling Retrieval and Generation for Enhanced Performance",
                    "desc": "This paper introduces a new framework called s3 that separates the retrieval and generation processes in retrieval-augmented generation (RAG) systems. By doing this, it allows for better performance with significantly less training data compared to traditional methods. The framework uses a novel reward system, Gain Beyond RAG, to enhance the searcher's effectiveness without needing to fine-tune the entire language model. As a result, s3 achieves superior results on various question-answering benchmarks while requiring only a fraction of the training samples."
                },
                "zh": {
                    "title": "Ëß£ËÄ¶Ê£ÄÁ¥¢‰∏éÁîüÊàêÔºåÊèêÂçáRAGÁ≥ªÁªüÊÄßËÉΩ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑ„ÄÅ‰∏éÊ®°ÂûãÊó†ÂÖ≥ÁöÑÊ°ÜÊû∂s3ÔºåÊó®Âú®Ëß£ËÄ¶Ê£ÄÁ¥¢ÂíåÁîüÊàêËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òRAGÁ≥ªÁªüÁöÑÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøá‰ΩøÁî®Ë∂ÖË∂äRAGÂ•ñÂä±ÁöÑÂ¢ûÁõäÊù•ËÆ≠ÁªÉÊ£ÄÁ¥¢Âô®Ôºå‰∏ìÊ≥®‰∫éÁîüÊàêÂáÜÁ°ÆÊÄßÁöÑÊèêÂçá„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºås3Âè™ÈúÄ2.4ÂçÉ‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÔºåÂ∞±ËÉΩË∂ÖË∂äÂü∫‰∫é70ÂÄç‰ª•‰∏äÊï∞ÊçÆËÆ≠ÁªÉÁöÑÂü∫Á∫øÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºås3Âú®ÂÖ≠‰∏™ÈÄöÁî®ÈóÆÁ≠îÂíå‰∫î‰∏™ÂåªÂ≠¶ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂßãÁªàÊèê‰æõÊõ¥Âº∫ÁöÑ‰∏ãÊ∏∏ÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17826",
            "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
            "url": "https://huggingface.co/papers/2505.17826",
            "abstract": "Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.  \t\t\t\t\tAI-generated summary \t\t\t\t Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework.",
            "score": 8,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "6f6fdf1b20859c44",
            "authors": [
                "Xuchen Pan",
                "Yanxi Chen",
                "Yushuo Chen",
                "Yuchang Sun",
                "Daoyuan Chen",
                "Wenhao Zhang",
                "Yuexiang Xie",
                "Yilun Huang",
                "Yilei Zhang",
                "Dawei Gao",
                "Yaliang Li",
                "Bolin Ding",
                "Jingren Zhou"
            ],
            "affiliations": [
                "alibaba-inc.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17826.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#agi",
                    "#optimization",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "Trinity-RFT: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "Trinity-RFT - —ç—Ç–æ –≥–∏–±–∫–∞—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ–∂–∏–º—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ/–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ, on-policy/off-policy –∏ –æ–Ω–ª–∞–π–Ω/–æ—Ñ–ª–∞–π–Ω –ø–æ–¥—Ö–æ–¥—ã. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Trinity-RFT —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —è–¥—Ä–∞ RFT, –º–æ–¥—É–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞ –∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω–≤–µ–π–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –ª–µ–≥–∫–æ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ä–∞–¥–∏–≥–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º."
                },
                "en": {
                    "title": "Empowering Language Models with Flexible Reinforcement Fine-Tuning",
                    "desc": "Trinity-RFT is a versatile framework designed for reinforcement fine-tuning (RFT) of large language models. It features a decoupled architecture that supports various modes of RFT, including synchronous and asynchronous, as well as on-policy and off-policy approaches. The framework ensures efficient and robust interactions between agents and environments, while also providing optimized data pipelines for RFT tasks. This makes Trinity-RFT adaptable to a wide range of applications, serving as a comprehensive platform for exploring advanced reinforcement learning techniques."
                },
                "zh": {
                    "title": "Trinity-RFTÔºöÁÅµÊ¥ªÁöÑÂº∫ÂåñÂæÆË∞ÉÊ°ÜÊû∂",
                    "desc": "Trinity-RFTÊòØ‰∏Ä‰∏™ÁÅµÊ¥ª‰∏îÂèØÊâ©Â±ïÁöÑÊ°ÜÊû∂Ôºå‰∏ìÈó®Áî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂº∫ÂåñÂæÆË∞É„ÄÇÂÆÉÈááÁî®Ëß£ËÄ¶ËÆæËÆ°ÔºåÂåÖÂê´‰∏Ä‰∏™RFTÊ†∏ÂøÉÔºåËÉΩÂ§üÁªü‰∏ÄÂíåÊ¶ÇÊã¨ÂêåÊ≠•/ÂºÇÊ≠•„ÄÅÂú®Á∫ø/Á¶ªÁ∫øÁ≠âÂ§öÁßçÂº∫ÂåñÂæÆË∞ÉÊ®°Âºè„ÄÇËØ•Ê°ÜÊû∂ÊîØÊåÅÈ´òÊïà‰∏îÁ®≥ÂÅ•ÁöÑÊô∫ËÉΩ‰Ωì‰∏éÁéØÂ¢ÉÁöÑ‰∫§‰∫íÔºåÂπ∂‰ºòÂåñ‰∫ÜÊï∞ÊçÆÁÆ°ÈÅì‰ª•ÈÄÇÂ∫îÂº∫ÂåñÂæÆË∞ÉÁöÑÈúÄÊ±Ç„ÄÇTrinity-RFTÊòì‰∫éÈÄÇÂ∫î‰∏çÂêåÁöÑÂ∫îÁî®Âú∫ÊôØÔºåÊòØÊé¢Á¥¢ÂÖàËøõÂº∫ÂåñÂ≠¶‰π†ËåÉÂºèÁöÑÁªü‰∏ÄÂπ≥Âè∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15389",
            "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
            "url": "https://huggingface.co/papers/2505.15389",
            "abstract": "VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations rely on artificial images. This study asks: How safe are current VLMs when confronted with meme images that ordinary users share? To investigate this question, we introduce MemeSafetyBench, a 50,430-instance benchmark pairing real meme images with both harmful and benign instructions. Using a comprehensive safety taxonomy and LLM-based instruction generation, we assess multiple VLMs across single and multi-turn interactions. We investigate how real-world memes influence harmful outputs, the mitigating effects of conversational context, and the relationship between model scale and safety metrics. Our findings demonstrate that VLMs show greater vulnerability to meme-based harmful prompts than to synthetic or typographic images. Memes significantly increase harmful responses and decrease refusals compared to text-only inputs. Though multi-turn interactions provide partial mitigation, elevated vulnerability persists. These results highlight the need for ecologically valid evaluations and stronger safety mechanisms.",
            "score": 7,
            "issue_id": 3945,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "102a2cdaf1ccf7d5",
            "authors": [
                "DongGeon Lee",
                "Joonwon Jang",
                "Jihae Jeong",
                "Hwanjo Yu"
            ],
            "affiliations": [
                "LG AI Research",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15389.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#ethics",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "üõ°Ô∏è",
                "ru": {
                    "title": "–ú–µ–º—ã vs –ò–ò: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —É–≥—Ä–æ–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (VLM) –±–æ–ª–µ–µ —É—è–∑–≤–∏–º—ã –∫ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–º –º–µ–º–∞–º, —á–µ–º –∫ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MemeSafetyBench - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 50 430 –º–µ–º–æ–≤ —Å –≤—Ä–µ–¥–Ω—ã–º–∏ –∏ –±–µ–∑–æ–±–∏–¥–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ VLM. –ú–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–∞—Å—Ç–∏—á–Ω–æ —Å–Ω–∏–∂–∞—é—Ç —Ä–∏—Å–∫–∏, –Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —É—è–∑–≤–∏–º–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –≤–∞–ª–∏–¥–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –∏ —É—Å–∏–ª–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è VLM."
                },
                "en": {
                    "title": "Meme Vulnerability: A Call for Safer VLMs",
                    "desc": "This paper investigates the safety of vision-language models (VLMs) when exposed to real-world meme images, which are often shared by users. The authors introduce a benchmark called MemeSafetyBench, consisting of over 50,000 instances of meme images paired with harmful and benign instructions. The study finds that VLMs are more susceptible to harmful prompts from memes compared to synthetic images, and while multi-turn interactions can offer some protection, vulnerabilities remain significant. The results emphasize the importance of realistic evaluations and the need for improved safety measures in VLMs."
                },
                "zh": {
                    "title": "ÊÅ∂ÊêûÂõæÂÉèÂØπËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®Â®ÅËÉÅ",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Èù¢ÂØπÁî®Êà∑ÂàÜ‰∫´ÁöÑÊÅ∂ÊêûÂõæÂÉèÊó∂ÁöÑÂÆâÂÖ®ÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜMemeSafetyBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´50,430‰∏™ÂÆû‰æãÁöÑÂü∫ÂáÜÔºåÁªìÂêà‰∫ÜÁúüÂÆûÁöÑÊÅ∂ÊêûÂõæÂÉèÂíåÊúâÂÆ≥‰∏éÊó†ÂÆ≥ÁöÑÊåá‰ª§„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåVLMsÂØπÊÅ∂ÊêûÂõæÂÉèÁöÑÊúâÂÆ≥ÊèêÁ§∫ÊØîÂØπÂêàÊàêÂõæÂÉèÊõ¥ËÑÜÂº±Ôºå‰∏îÂ§öËΩÆÂØπËØùËôΩÁÑ∂Êèê‰æõ‰∫Ü‰∏ÄÂÆöÁöÑ‰øùÊä§Ôºå‰ΩÜ‰ªçÁÑ∂Â≠òÂú®ÊòæËëóÁöÑËÑÜÂº±ÊÄß„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÂº∫Ë∞É‰∫ÜÈúÄË¶ÅËøõË°åÁîüÊÄÅÊúâÊïàÁöÑËØÑ‰º∞ÂíåÊõ¥Âº∫ÁöÑÂÆâÂÖ®Êú∫Âà∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17063",
            "title": "Synthetic Data RL: Task Definition Is All You Need",
            "url": "https://huggingface.co/papers/2505.17063",
            "abstract": "Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
            "score": 7,
            "issue_id": 3947,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 –º–∞—è",
                "en": "May 18",
                "zh": "5Êúà18Êó•"
            },
            "hash": "a46b0456dc458ebe",
            "authors": [
                "Yiduo Guo",
                "Zhen Guo",
                "Chuanwei Huang",
                "Zi-Ang Wang",
                "Zekai Zhang",
                "Haofei Yu",
                "Huishuai Zhang",
                "Yikang Shen"
            ],
            "affiliations": [
                "MIT",
                "MIT-IBM",
                "Peking University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17063.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–£—Å–∏–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Synthetic Data RL, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –∏—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. Synthetic Data RL –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Reinforcement Learning with Synthetic Data: A Game Changer for Model Training!",
                    "desc": "Synthetic Data RL is a novel approach that enhances foundation models using reinforcement learning (RL) without the need for extensive human-labeled data. It generates synthetic question and answer pairs based on task definitions, allowing for effective model fine-tuning. The method adapts question difficulty according to the model's performance, optimizing the training process. Results show significant performance improvements on various benchmarks, demonstrating that this approach can achieve results comparable to traditional RL methods that rely on human data."
                },
                "zh": {
                    "title": "ÂêàÊàêÊï∞ÊçÆÂº∫ÂåñÂ≠¶‰π†ÔºöÈ´òÊïàÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑÂàõÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÂêàÊàêÊï∞ÊçÆÂº∫ÂåñÂ≠¶‰π†ÔºàSynthetic Data RLÔºâÁöÑÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÈÄöËøá‰ΩøÁî®ÂêàÊàêÊï∞ÊçÆÊù•Â¢ûÂº∫Âü∫Á°ÄÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÁîüÊàê‰∏é‰ªªÂä°ÂÆö‰πâÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÂíåÁ≠îÊ°àÂØπÔºåÂπ∂Ê†πÊçÆÊ®°ÂûãÁöÑËß£ÂÜ≥ËÉΩÂäõË∞ÉÊï¥ÈóÆÈ¢òÁöÑÈöæÂ∫¶„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåSynthetic Data RLÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÁîöËá≥Êé•Ëøë‰∫é‰ΩøÁî®ÂÖ®‰∫∫Á±ªÊ†áÊ≥®Êï∞ÊçÆÁöÑÂº∫ÂåñÂ≠¶‰π†Ê®°Âûã„ÄÇÊ≠§ÊñπÊ≥ïÂáèÂ∞ë‰∫ÜÂØπ‰∫∫Á±ªÊï∞ÊçÆÊ†áÊ≥®ÁöÑ‰æùËµñÔºå‰ΩøÂæóÊ®°ÂûãÈÄÇÂ∫îÂèòÂæóÊõ¥Âä†È´òÊïàÂíåÂèØÊâ©Â±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16270",
            "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
            "url": "https://huggingface.co/papers/2505.16270",
            "abstract": "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability.",
            "score": 6,
            "issue_id": 3945,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "242b4420fd3d9c4f",
            "authors": [
                "Jiaru Zou",
                "Yikun Ban",
                "Zihao Li",
                "Yunzhe Qi",
                "Ruizhong Qiu",
                "Ling Yang",
                "Jingrui He"
            ],
            "affiliations": [
                "Princeton University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16270.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#architecture",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "Transformer Copilot: –£—á–∏–º—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer Copilot, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏-–≤—Ç–æ—Ä–æ–≥–æ –ø–∏–ª–æ—Ç–∞ (Copilot), –∫–æ—Ç–æ—Ä–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –ª–æ–≥–∏—Ç—ã –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ (Pilot) –Ω–∞ –æ—Å–Ω–æ–≤–µ –∂—É—Ä–Ω–∞–ª–∞ –æ—à–∏–±–æ–∫ (Mistake Log). –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ —Å–≤–æ–∏—Ö –ø—Ä–æ—à–ª—ã—Ö –æ—à–∏–±–∫–∞—Ö, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç–æ–º—É, –∫–∞–∫ —É—á–∞—Ç—Å—è –ª—é–¥–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 12 —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ 34.5% –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö."
                },
                "en": {
                    "title": "Enhancing Language Models with Reflective Learning",
                    "desc": "The Transformer Copilot framework improves the performance of large language models by using a Copilot model that refines the Pilot's outputs based on a Mistake Log. This Mistake Log tracks the model's errors during fine-tuning, allowing the Copilot to learn from these mistakes, similar to how humans learn. The framework includes a novel design for the Copilot, a joint training approach where both models learn together, and a fused inference method that enhances the Pilot's predictions. Experiments show that this approach can boost performance by up to 34.5% across various tasks with minimal additional computational cost."
                },
                "zh": {
                    "title": "ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÂâØÈ©æÈ©∂Ê°ÜÊû∂",
                    "desc": "Transformer CopilotÊ°ÜÊû∂ÈÄöËøá‰∏Ä‰∏™ÂâØÈ©æÈ©∂Ê®°ÂûãÊù•ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËøô‰∏™ÂâØÈ©æÈ©∂Ê®°ÂûãÊ†πÊçÆÈîôËØØÊó•ÂøóÊù•‰ºòÂåñ‰∏ªÊ®°ÂûãÁöÑËæìÂá∫Ôºå‰ªéËÄåÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊåÅÁª≠ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÈîôËØØÊó•ÂøóÁöÑÊ¶ÇÂøµÔºå‰ª•Á≥ªÁªüÂú∞Ë∑üË∏™Ê®°ÂûãÁöÑÂ≠¶‰π†Ë°å‰∏∫ÂíåÈáçÂ§çÈîôËØØ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂâØÈ©æÈ©∂Ê®°ÂûãËÉΩÂ§üÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠‰∏çÊñ≠Â≠¶‰π†Ôºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁöÑÂáÜÁ°ÆÊÄßÂíåË¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17540",
            "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.17540",
            "abstract": "RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results.",
            "score": 5,
            "issue_id": 3950,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "0b4459388e7a7ca1",
            "authors": [
                "Mingrui Wu",
                "Lu Wang",
                "Pu Zhao",
                "Fangkai Yang",
                "Jianjin Zhang",
                "Jianfeng Liu",
                "Yuefeng Zhan",
                "Weihao Han",
                "Hao Sun",
                "Jiayi Ji",
                "Xiaoshuai Sun",
                "Qingwei Lin",
                "Weiwei Deng",
                "Dongmei Zhang",
                "Feng Sun",
                "Qi Zhang",
                "Rongrong Ji"
            ],
            "affiliations": [
                "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17540.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#rl",
                    "#rag",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "–£–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "RePrompt - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ü–µ–Ω–∫–∏. RePrompt –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –∫–æ–º–ø–æ–Ω–æ–≤–∫—É –∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "RePrompt: Enhancing Text-to-Image Generation with Reinforcement Learning",
                    "desc": "RePrompt is a new framework that uses reinforcement learning to improve text-to-image generation by focusing on the quality of the generated images. It addresses the common issue where existing models fail to accurately interpret short prompts, often leading to unrealistic outputs. By incorporating explicit reasoning into the prompt enhancement process, RePrompt generates structured prompts that are better aligned with user intentions. The framework trains a language model to optimize prompts based on image-level outcomes, achieving significant improvements in spatial layout and compositional generalization without needing human-annotated data."
                },
                "zh": {
                    "title": "RePromptÔºö‰ºòÂåñÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "RePromptÊòØ‰∏Ä‰∏™‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁöÑÈáçÊñ∞ÊèêÁ§∫Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊïàÊûú„ÄÇÂÆÉÈÄöËøá‰ºòÂåñÂõæÂÉèÁ∫ßÁªìÊûúÔºåÊòæËëóÊîπÂñÑ‰∫ÜÁ©∫Èó¥Â∏ÉÂ±ÄÂíåÁªÑÂêàÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåRePrompt‰∏ç‰æùËµñÊâãÂ∑•ËßÑÂàôÔºåËÄåÊòØËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁªìÊûÑÂåñÁöÑËá™ÂèçÊèêÁ§∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRePromptÂú®Â§ö‰∏™ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊàêÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17508",
            "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.17508",
            "abstract": "A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG.",
            "score": 5,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "6ae63edcb7127847",
            "authors": [
                "Yifan Zhang",
                "Yifeng Liu",
                "Huizhuo Yuan",
                "Yang Yuan",
                "Quanquan Gu",
                "Andrew C Yao"
            ],
            "affiliations": [
                "IIIS, Tsinghua University",
                "Shanghai Qi Zhi Institute",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17508.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RPG (regularized policy gradient) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–≤—ã—à–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–∏–ª—å–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ RPG –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ KL-—Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Regularized Policy Gradients",
                    "desc": "This paper introduces a regularized policy gradient framework that utilizes Kullback-Leibler (KL) divergence to improve the reasoning abilities of large language models (LLMs) in online reinforcement learning (RL). It systematically explores various KL divergence formulations to enhance training stability and performance through surrogate loss functions. The authors derive policy gradients for both forward and reverse KL divergences, accommodating different types of policy distributions. Extensive experiments demonstrate that their proposed methods achieve better or comparable results against established baselines in RL tasks involving LLMs."
                },
                "zh": {
                    "title": "Ê≠£ÂàôÂåñÁ≠ñÁï•Ê¢ØÂ∫¶ÔºöÊèêÂçáLLMÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ≠£ÂàôÂåñÁ≠ñÁï•Ê¢ØÂ∫¶Ê°ÜÊû∂ÔºåÁî®‰∫éÊé¢Á¥¢KLÊï£Â∫¶ÁöÑ‰∏çÂêåÂΩ¢ÂºèÔºå‰ª•Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨Á≥ªÁªüÂú∞ÂàÜÊûê‰∫ÜÂ¶Ç‰ΩïÂ∞Ü‰∏çÂêåÁöÑKLÊï£Â∫¶‰º∞ËÆ°Êï¥ÂêàÂà∞Êõø‰ª£ÊçüÂ§±ÂáΩÊï∞‰∏≠Ôºå‰ªéËÄåÊèêÈ´òËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ„ÄÇÈÄöËøáÂØπÊ≠£ÂêëÂíåÂèçÂêëKLÊï£Â∫¶ÁöÑÊ≠£ÂàôÂåñÁõÆÊ†áÔºåÊàë‰ª¨Êé®ÂØº‰∫ÜÁõ∏Â∫îÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶ÂíåÊçüÂ§±ÂáΩÊï∞ÔºåÂπ∂ËÄÉËôë‰∫ÜÊ†áÂáÜÂåñÂíåÈùûÊ†áÂáÜÂåñÁöÑÁ≠ñÁï•ÂàÜÂ∏É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÁé∞ÊúâÁöÑÂº∫Âü∫Á∫øÁÆóÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ‰∏äÈÉΩÊúâÊòæËëóÊèêÂçá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15182",
            "title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State\n  Reflection",
            "url": "https://huggingface.co/papers/2505.15182",
            "abstract": "ReflAct, a new reasoning backbone for LLM agents, improves goal alignment and reduces hallucinations by continuously reflecting on the agent's state, surpassing ReAct and other enhanced variants.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.",
            "score": 5,
            "issue_id": 3955,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "b595495608e9ea19",
            "authors": [
                "Jeonghye Kim",
                "Sojeong Rhee",
                "Minbeom Kim",
                "Dohyung Kim",
                "Sangmook Lee",
                "Youngchul Sung",
                "Kyomin Jung"
            ],
            "affiliations": [
                "KAIST",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15182.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#reasoning",
                    "#rl",
                    "#hallucinations",
                    "#agents"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "ReflAct: –†–µ—Ñ–ª–µ–∫—Å–∏—è –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "ReflAct - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω —É–ª—É—á—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å —Ü–µ–ª—å—é –∏ —É–º–µ–Ω—å—à–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ø—É—Ç–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∞–≥–µ–Ω—Ç–∞. ReflAct –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç ReAct –∏ –¥—Ä—É–≥–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, —Å–º–µ—â–∞—è –∞–∫—Ü–µ–Ω—Ç —Å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –µ–≥–æ —Ü–µ–ª–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ReflAct –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å ReAct, –¥–æ—Å—Ç–∏–≥–∞—è 93.3% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤ ALFWorld."
                },
                "en": {
                    "title": "ReflAct: Grounded Reasoning for Reliable LLM Agents",
                    "desc": "ReflAct is a new reasoning framework designed for large language model (LLM) agents that enhances their ability to align with goals and minimizes errors known as hallucinations. Unlike its predecessor, ReAct, which often leads to inconsistent reasoning and misalignment, ReflAct focuses on continuously reflecting on the agent's current state in relation to its objectives. This approach ensures that decisions are grounded in accurate internal beliefs, thereby improving the reliability of the agent's strategic actions. Empirical results show that ReflAct significantly outperforms ReAct and its enhanced versions, achieving a high success rate in complex environments."
                },
                "zh": {
                    "title": "ReflActÔºöÊèêÂçá‰ª£ÁêÜÊé®ÁêÜÁöÑÊ†∏ÂøÉÂäõÈáè",
                    "desc": "ReflActÊòØ‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÁöÑÁõÆÊ†á‰∏ÄËá¥ÊÄßÂπ∂ÂáèÂ∞ëÂπªËßâÁé∞Ë±°„ÄÇ‰∏é‰º†ÁªüÁöÑReActÊñπÊ≥ïÁõ∏ÊØîÔºåReflActÈÄöËøáÊåÅÁª≠ÂèçÊÄù‰ª£ÁêÜÁöÑÁä∂ÊÄÅÊù•ÊîπÂñÑÊé®ÁêÜËøáÁ®ãÔºåÈÅøÂÖç‰∫Ü‰∏ç‰∏ÄËá¥ÁöÑÂÜÖÈÉ®‰ø°ÂøµÂíåÁõÆÊ†áÂØπÈΩêÈóÆÈ¢ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåReflActÂú®ALFWorld‰∏≠Âπ≥ÂùáË∂ÖË∂äReAct 27.7%ÔºåÊàêÂäüÁéáËææÂà∞93.3%„ÄÇËøô‰∏ÄËÆæËÆ°Âº∫Ë∞É‰∫ÜÂä†Âº∫Ê†∏ÂøÉÊé®ÁêÜÊ°ÜÊû∂ÂØπ‰∫éÊèêÈ´ò‰ª£ÁêÜÊÄßËÉΩÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17091",
            "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
            "url": "https://huggingface.co/papers/2505.17091",
            "abstract": "Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.",
            "score": 5,
            "issue_id": 3945,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "7bb07d5dfb6680e6",
            "authors": [
                "Prateek Verma",
                "Mert Pilanci"
            ],
            "affiliations": [
                "Department of Electrical Engineering Stanford University Stanford, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17091.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#optimization",
                    "#architecture",
                    "#transfer_learning",
                    "#audio"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—Ä–µ—Ç–∞—é—Ç –∑—Ä–µ–Ω–∏–µ –∏ —Å–ª—É—Ö —á–µ—Ä–µ–∑ —á—Ç–µ–Ω–∏–µ",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ø–æ—Å–æ–±–Ω—ã —Ä–∞–∑–≤–∏–≤–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∞—É–¥–∏–æ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö. –î–∞–Ω–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –º–æ—â–Ω—ã—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Ö–µ–º–∞—Ö, —Ñ–æ—Ä–º–∏—Ä—É–µ–º—ã—Ö —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∏ –∏—Ö –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è."
                },
                "en": {
                    "title": "Unlocking Multi-Modal Understanding with Text LLMs",
                    "desc": "This paper explores how auto-regressive language models (LLMs) trained solely on text can develop the ability to understand and classify images and audio without needing additional fine-tuning. The authors demonstrate that these text-based models can process inputs like image patches and audio waveforms, producing embeddings or category labels similar to those used in traditional classification tasks. They validate their findings by applying the model to audio classification tasks on datasets like FSD-50K and GTZAN, as well as image classification on CIFAR-10 and Fashion-MNIST. This research highlights the potential of leveraging text LLMs' internal capabilities for multi-modal applications, reducing the need for training separate models for each modality."
                },
                "zh": {
                    "title": "ÊñáÊú¨Ê®°ÂûãÁöÑË∑®Ê®°ÊÄÅÁêÜËß£ËÉΩÂäõ",
                    "desc": "ËøôÁØáËÆ∫ÊñáÂ±ïÁ§∫‰∫Ü‰∏Ä‰∏™ÊúâË∂£ÁöÑÂèëÁé∞ÔºöÈÄöËøáÂØπÊñáÊú¨ËøõË°åËÆ≠ÁªÉÁöÑËá™ÂõûÂΩíËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂÜÖÂú®Âú∞ÂèëÂ±ïÂá∫ÁêÜËß£ÂõæÂÉèÂíåÈü≥È¢ëÁöÑËÉΩÂäõ„ÄÇËøôÊ†∑ÔºåÊ®°ÂûãÂú®Ê≤°ÊúâÂæÆË∞ÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞±ËÉΩËøõË°åË∑®Ê®°ÊÄÅÁöÑÂàÜÁ±ª‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáËæìÂÖ•ÂõæÂÉèÂùó„ÄÅÈü≥È¢ëÊ≥¢ÂΩ¢ÊàñÊ†áËÆ∞ÔºåÁîüÊàêÂÖ∏ÂûãÁöÑÂàÜÁ±ªÁÆ°ÈÅìÊâÄÈúÄÁöÑÂµåÂÖ•ÊàñÁ±ªÂà´Ê†áÁ≠æ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊñáÊú¨Ê®°ÂûãÁöÑÊùÉÈáçÂú®Èü≥È¢ëÂíåÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠ÂÖ∑ÊúâÂπøÊ≥õÁöÑÈÄÇÁî®ÊÄßÔºåÊé®Âä®‰∫ÜÊñáÊú¨ËØ≠Ë®ÄÊ®°ÂûãÂ≠¶‰π†Âº∫Â§ßÂÜÖÈÉ®ÁîµË∑ØÁöÑÊ¶ÇÂøµ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17016",
            "title": "Interactive Post-Training for Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2505.17016",
            "abstract": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.   RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.",
            "score": 4,
            "issue_id": 3948,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "ab53333533d30d3f",
            "authors": [
                "Shuhan Tan",
                "Kairan Dou",
                "Yue Zhao",
                "Philipp Kr√§henb√ºhl"
            ],
            "affiliations": [
                "Nankai University",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17016.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#training",
                    "#optimization",
                    "#rlhf",
                    "#multimodal",
                    "#rl",
                    "#transfer_learning"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏–µ VLA –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –Ω–∞–¥–∑–æ—Ä–æ–º",
                    "desc": "RIPT-VLA - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞-–¥–µ–π—Å—Ç–≤–∏—è (VLA) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –±–∏–Ω–∞—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã —É—Å–ø–µ—Ö–∞. RIPT-VLA –ø—Ä–∏–º–µ–Ω–∏–º–∞ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º VLA –º–æ–¥–µ–ª—è–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∑–≤–æ–ª—è—è –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∑–∞ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π."
                },
                "en": {
                    "title": "Reinforcement Learning for Efficient Vision-Language-Action Adaptation",
                    "desc": "RIPT-VLA is a new method that enhances Vision-Language-Action (VLA) models using reinforcement learning with minimal supervision. It allows these models to learn from sparse binary rewards instead of relying on extensive expert demonstrations. This approach improves the performance of various VLA models significantly, achieving a 97.5% success rate with the OpenVLA-OFT model. Additionally, RIPT-VLA is efficient in both computation and data usage, enabling models to adapt quickly to new tasks with just one demonstration."
                },
                "zh": {
                    "title": "RIPT-VLAÔºöÈ´òÊïàÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂêéËÆ≠ÁªÉËåÉÂºè",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜRIPT-VLAÔºåËøôÊòØ‰∏ÄÁßçÁÆÄÂçï‰∏îÂèØÊâ©Â±ïÁöÑÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑ‰∫§‰∫íÂºèÂêéËÆ≠ÁªÉËåÉÂºèÔºåÊó®Âú®‰ΩøÁî®Á®ÄÁñèÁöÑ‰∫åÂÖÉÊàêÂäüÂ•ñÂä±Êù•ÂæÆË∞ÉÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°Âûã„ÄÇÁé∞ÊúâÁöÑVLAËÆ≠ÁªÉÊµÅÁ®ãËøá‰∫é‰æùËµñÁ¶ªÁ∫ø‰∏ìÂÆ∂Á§∫ËåÉÊï∞ÊçÆÂíåÁõëÁù£Ê®°‰ªøÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®‰ΩéÊï∞ÊçÆÁéØÂ¢É‰∏ãÈÄÇÂ∫îÊñ∞‰ªªÂä°ÂíåÊñ∞ÁéØÂ¢ÉÁöÑËÉΩÂäõ„ÄÇRIPT-VLAÈÄöËøáÂä®ÊÄÅÂõûÊîæÈááÊ†∑ÂíåÁïô‰∏Ä‰ºòÂäø‰º∞ËÆ°ÁöÑÁ®≥ÂÆöÁ≠ñÁï•‰ºòÂåñÁÆóÊ≥ïÔºåÂÆûÁé∞‰∫Ü‰∫§‰∫íÂºèÂêéËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRIPT-VLAÂú®‰∏çÂêå‰ªªÂä°ÂíåÂú∫ÊôØ‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂‰∏îÂØπÂàùÂßãÁä∂ÊÄÅ‰∏ä‰∏ãÊñáÂÖ∑ÊúâÈ≤ÅÊ£íÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18078",
            "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video\n  Generation",
            "url": "https://huggingface.co/papers/2505.18078",
            "abstract": "DanceTogether, an end-to-end diffusion framework, generates long, photorealistic multi-actor interaction videos from single reference images and pose-mask streams, outperforming existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable video generation (CVG) has advanced rapidly, yet current systems falter when more than one actor must move, interact, and exchange positions under noisy control signals. We address this gap with DanceTogether, the first end-to-end diffusion framework that turns a single reference image plus independent pose-mask streams into long, photorealistic videos while strictly preserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at every denoising step by fusing robust tracking masks with semantically rich-but noisy-pose heat-maps, eliminating the identity drift and appearance bleeding that plague frame-wise pipelines. To train and evaluate at scale, we introduce (i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii) HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain transfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a significant margin. Moreover, we show that a one-hour fine-tune yields convincing human-robot videos, underscoring broad generalization to embodied-AI and HRI tasks. Extensive ablations confirm that persistent identity-action binding is critical to these gains. Together, our model, datasets, and benchmark lift CVG from single-subject choreography to compositionally controllable, multi-actor interaction, opening new avenues for digital production, simulation, and embodied intelligence. Our video demos and code are available at https://DanceTog.github.io/.",
            "score": 3,
            "issue_id": 3955,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "3157b30c4a153886",
            "authors": [
                "Junhao Chen",
                "Mingjin Chen",
                "Jianjin Xu",
                "Xiang Li",
                "Junting Dong",
                "Mingze Sun",
                "Puhua Jiang",
                "Hongxiang Li",
                "Yuhang Yang",
                "Hao Zhao",
                "Xiaoxiao Long",
                "Ruqi Huang"
            ],
            "affiliations": [
                "Beijing Normal University",
                "Carnegie Mellon University",
                "Hong Kong Baptist University",
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory",
                "Tsinghua University",
                "University of Science & Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18078.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#games",
                    "#video",
                    "#diffusion",
                    "#robotics",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "üíÉ",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: –æ—Ç —Ö–æ—Ä–µ–æ–≥—Ä–∞—Ñ–∏–∏ –∫ –º–Ω–æ–≥–æ–∞–∫—Ç–µ—Ä–Ω–æ–º—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é",
                    "desc": "DanceTogether - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –û–Ω–∞ —Å–æ–∑–¥–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–Ω–æ –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø–æ—Ç–æ–∫–∏ –ø–æ–∑-–º–∞—Å–æ–∫. –ö–ª—é—á–µ–≤–æ–µ –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ - MaskPoseAdapter, –∫–æ—Ç–æ—Ä—ã–π —Å–≤—è–∑—ã–≤–∞–µ—Ç –ª–∏—á–Ω–æ—Å—Ç—å –∏ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –±—ã—Å—Ç—Ä–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–æ–±–æ—Ç–∞."
                },
                "en": {
                    "title": "Revolutionizing Multi-Actor Video Generation with DanceTogether",
                    "desc": "DanceTogether is a novel end-to-end diffusion framework designed for generating long, photorealistic videos featuring multiple actors interacting based on a single reference image and pose-mask streams. It addresses the challenges of controllable video generation by effectively binding identities and actions at each denoising step, which prevents issues like identity drift and appearance bleeding. The framework is trained on extensive datasets, including diverse interactions and scenarios, allowing it to generalize well across different domains. By significantly outperforming existing methods, DanceTogether enhances the capabilities of digital production and embodied AI applications."
                },
                "zh": {
                    "title": "DanceTogetherÔºöÂ§öÊºîÂëò‰∫íÂä®ËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "DanceTogetherÊòØ‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÊâ©Êï£Ê°ÜÊû∂ÔºåËÉΩÂ§ü‰ªéÂçï‰∏ÄÂèÇËÄÉÂõæÂÉèÂíåÂßøÊÄÅÊé©Á†ÅÊµÅÁîüÊàêÈïøÊó∂Èó¥ÁöÑÈÄºÁúüÂ§öÊºîÂëò‰∫íÂä®ËßÜÈ¢ëÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁ≥ªÁªüÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁªìÂêàÂº∫Â§ßÁöÑË∑üË∏™Êé©Á†ÅÂíåËØ≠‰πâ‰∏∞ÂØå‰ΩÜÂô™Â£∞ËæÉÂ§ßÁöÑÂßøÊÄÅÁÉ≠ÂõæÔºåÂú®ÊØè‰∏™ÂéªÂô™Ê≠•È™§‰∏≠ÁªëÂÆö‚ÄúË∞Å‚ÄùÂíå‚ÄúÂ¶Ç‰Ωï‚ÄùÔºåÊúâÊïàÊ∂àÈô§‰∫ÜË∫´‰ªΩÊºÇÁßªÂíåÂ§ñËßÇÊ®°Á≥äÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÂ§ö‰∏™Êï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÊµãËØïÔºå‰ª•ÊîØÊåÅÂ§ßËßÑÊ®°ÁöÑËÆ≠ÁªÉÂíåËØÑ‰º∞ÔºåÂ±ïÁ§∫‰∫ÜDanceTogetherÂú®Â§öÊºîÂëò‰∫íÂä®ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊòæËëó‰ºòÂäø„ÄÇËØ•Ê®°ÂûãÁöÑÊàêÂäü‰∏∫Êï∞Â≠óÂà∂‰Ωú„ÄÅ‰ªøÁúüÂíåÂÖ∑Ë∫´Êô∫ËÉΩÂºÄËæü‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15805",
            "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering",
            "url": "https://huggingface.co/papers/2505.15805",
            "abstract": "LLMs frequently violate contextual security policies by leaking sensitive information, particularly under indirect attacks, indicating a critical gap in current safety mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as enterprise and government, ensuring that they adhere to user-defined security policies within context is critical-especially with respect to information non-disclosure. While prior LLM studies have focused on general safety and socially sensitive data, large-scale benchmarks for contextual security preservation against attacks remain lacking. To address this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating LLM adherence to contextual non-disclosure policies in question answering. Derived from realistic contexts, our dataset includes explicit policies and queries designed as direct and challenging indirect attacks seeking prohibited information. We evaluate 10 LLMs on our benchmark and reveal a significant vulnerability: many models violate user-defined policies and leak sensitive information. This failure is particularly severe against indirect attacks, highlighting a critical gap in current LLM safety alignment for sensitive applications. Our analysis reveals that while models can often identify the correct answer to a query, they struggle to incorporate policy constraints during generation. In contrast, they exhibit a partial ability to revise outputs when explicitly prompted. Our findings underscore the urgent need for more robust methods to guarantee contextual security.",
            "score": 3,
            "issue_id": 3951,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "1ccf6fc4f4524fad",
            "authors": [
                "Hwan Chang",
                "Yumin Kim",
                "Yonghyun Jun",
                "Hwanhee Lee"
            ],
            "affiliations": [
                "Chung-Ang University, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15805.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#multimodal",
                    "#alignment",
                    "#security",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "üîê",
                "ru": {
                    "title": "LLM –Ω–∞—Ä—É—à–∞—é—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: urgent call –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∑–∞—â–∏—Ç—ã –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —É—Ç–µ—á–∫–∏ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö CoPriva –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM —Å–æ–±–ª—é–¥–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–µ—Ä–∞–∑–≥–ª–∞—à–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 10 –º–æ–¥–µ–ª–µ–π –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –∫–æ—Å–≤–µ–Ω–Ω—ã—Ö –∞—Ç–∞–∫–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –æ—Å—Ç—Ä—É—é –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã—Ö –º–µ—Ç–æ–¥–∞—Ö –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ LLM."
                },
                "en": {
                    "title": "Strengthening Contextual Security in LLMs",
                    "desc": "This paper addresses the issue of Large Language Models (LLMs) leaking sensitive information by violating contextual security policies, especially during indirect attacks. The authors introduce a new benchmark dataset called CoPriva, which tests LLMs on their ability to adhere to user-defined non-disclosure policies in question answering scenarios. The evaluation of 10 LLMs reveals significant vulnerabilities, particularly in handling indirect attacks, where models often fail to respect security constraints. The study highlights the necessity for improved safety mechanisms to ensure that LLMs can effectively incorporate contextual security measures in sensitive applications."
                },
                "zh": {
                    "title": "Á°Æ‰øù‰∏ä‰∏ãÊñáÂÆâÂÖ®ÔºåÈò≤Ê≠¢‰ø°ÊÅØÊ≥ÑÈú≤ÔºÅ",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÊïèÊÑü‰ø°ÊÅØÊó∂ÔºåÂ∏∏Â∏∏ËøùÂèç‰∏ä‰∏ãÊñáÂÆâÂÖ®ÊîøÁ≠ñÔºåÂ∞§ÂÖ∂ÊòØÂú®Èó¥Êé•ÊîªÂáª‰∏ãÔºåÊòæÁ§∫Âá∫ÂΩìÂâçÂÆâÂÖ®Êú∫Âà∂ÁöÑÈáçÂ§ßÁº∫Èô∑„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂ§ßËßÑÊ®°Âü∫ÂáÜÊï∞ÊçÆÈõÜCoPrivaÔºåÁî®‰∫éËØÑ‰º∞LLMsÂú®ÈóÆÁ≠î‰∏≠ÈÅµÂæ™‰∏ä‰∏ãÊñáÈùûÊä´Èú≤ÊîøÁ≠ñÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÂØπ10‰∏™LLMsËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂèëÁé∞ËÆ∏Â§öÊ®°ÂûãËøùÂèçÁî®Êà∑ÂÆö‰πâÁöÑÊîøÁ≠ñÔºåÊ≥ÑÈú≤ÊïèÊÑü‰ø°ÊÅØÔºåÂ∞§ÂÖ∂ÊòØÂú®Èù¢ÂØπÈó¥Êé•ÊîªÂáªÊó∂„ÄÇÁ†îÁ©∂ÁªìÊûúÂº∫Ë∞É‰∫ÜËø´ÂàáÈúÄË¶ÅÊõ¥Âº∫Â§ßÁöÑÊñπÊ≥ïÊù•Á°Æ‰øù‰∏ä‰∏ãÊñáÂÆâÂÖ®„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17373",
            "title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2505.17373",
            "abstract": "A simple and efficient method for value model training on long-context reasoning traces improves test-time performance and reduces computational cost compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we propose a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), our method does not require a fine-grained notion of \"step,\" which is difficult to define for long-context reasoning models. By collecting a dataset of 2.5 million reasoning traces, we train a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. We find that block-wise value-guided search (VGS) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. With an inference budget of 64 generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of 45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024 & 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. Our dataset, model and codebase are open-sourced.",
            "score": 2,
            "issue_id": 3960,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "fda5fcafecd8fe99",
            "authors": [
                "Kaiwen Wang",
                "Jin Peng Zhou",
                "Jonathan Chang",
                "Zhaolin Gao",
                "Nathan Kallus",
                "Kiant√© Brantley",
                "Wen Sun"
            ],
            "affiliations": [
                "Cornell University",
                "Databricks",
                "Harvard University",
                "Netflix"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17373.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#math",
                    "#dataset",
                    "#long_context",
                    "#open_source"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò",
                    "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM), –¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–Ω—è—Ç–∏—è '—à–∞–≥–∞', —á—Ç–æ —Å–ª–æ–∂–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã —Å–æ–±—Ä–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 2,5 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—É—á–∏–ª–∏ —Ç–æ–∫–µ–Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–º–æ–º 1,5 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –∫ DeepSeek —É–ª—É—á—à–∏–ª–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Efficient Value Model Training for Long-Context Reasoning",
                    "desc": "This paper introduces a novel method for training value models specifically designed for long-context reasoning tasks. Unlike traditional process reward models, this approach simplifies the training process by eliminating the need for a detailed definition of steps, which can be challenging in long-context scenarios. By utilizing a large dataset of 2.5 million reasoning traces, the authors train a 1.5 billion token-level value model that enhances performance in DeepSeek models during testing. The proposed block-wise value-guided search (VGS) method demonstrates superior test-time efficiency and accuracy compared to conventional techniques, while also reducing computational costs significantly."
                },
                "zh": {
                    "title": "È´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜ‰ª∑ÂÄºÊ®°ÂûãËÆ≠ÁªÉ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÈ´òÊïàÁöÑ‰ª∑ÂÄºÊ®°ÂûãËÆ≠ÁªÉÊñπÊ≥ïÔºå‰∏ìÊ≥®‰∫éÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜËΩ®Ëøπ„ÄÇ‰∏éÁé∞ÊúâÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏çÈúÄË¶ÅÂØπ‚ÄúÊ≠•È™§‚ÄùËøõË°åÁªÜËá¥ÂÆö‰πâÔºåËøôÂú®Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜÊ®°Âûã‰∏≠ÊòØÂõ∞ÈöæÁöÑ„ÄÇÈÄöËøáÊî∂ÈõÜ250‰∏áÊù°Êé®ÁêÜËΩ®ËøπÁöÑÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™15‰∫øÊ†áËÆ∞Á∫ßÂà´ÁöÑ‰ª∑ÂÄºÊ®°ÂûãÔºåÂπ∂Â∞ÜÂÖ∂Â∫îÁî®‰∫éDeepSeekÊ®°ÂûãÔºå‰ª•ÊèêÈ´òÊµãËØïÊó∂ÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂü∫‰∫éÂùóÁöÑ‰ª∑ÂÄºÂºïÂØºÊêúÁ¥¢ÔºàVGSÔºâÂú®ÊµãËØïÊó∂ÁöÑËÆ°ÁÆóÊïàÁéá‰∏ä‰ºò‰∫é‰º†ÁªüÁöÑÂ§öÊï∞ÊäïÁ•®ÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16293",
            "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
            "url": "https://huggingface.co/papers/2505.16293",
            "abstract": "Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.  \t\t\t\t\tAI-generated summary \t\t\t\t Iterative RAG for multi-hop question answering faces challenges with lengthy contexts and the buildup of irrelevant information. This hinders a model's capacity to process and reason over retrieved content and limits performance. While recent methods focus on compressing retrieved information, they are either restricted to single-round RAG, require finetuning or lack scalability in iterative RAG. To address these challenges, we propose Notes Writing, a method that generates concise and relevant notes from retrieved documents at each step, thereby reducing noise and retaining only essential information. This indirectly increases the effective context length of Large Language Models (LLMs), enabling them to reason and plan more effectively while processing larger volumes of input text. Notes Writing is framework agnostic and can be integrated with different iterative RAG methods. We demonstrate its effectiveness with three iterative RAG methods, across two models and four evaluation datasets. Notes writing yields an average improvement of 15.6 percentage points overall, with minimal increase in output tokens.",
            "score": 2,
            "issue_id": 3949,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "c597f26ca16d4748",
            "authors": [
                "Rishabh Maheshwary",
                "Masoud Hashemi",
                "Khyati Mahajan",
                "Shiva Krishna Reddy Malay",
                "Sai Rajeswar",
                "Sathwik Tejaswi Madhusudhan",
                "Spandana Gella",
                "Vikas Yadav"
            ],
            "affiliations": [
                "ServiceNow",
                "ServiceNow Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16293.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#multimodal",
                    "#rag",
                    "#long_context"
                ],
                "emoji": "üìù",
                "ru": {
                    "title": "–£—Å–∏–ª–µ–Ω–∏–µ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ RAG —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫—Ä–∞—Ç–∫–∏—Ö –∑–∞–º–µ—Ç–æ–∫",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Notes Writing –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ RAG –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–∏–µ –∑–∞–º–µ—Ç–∫–∏ –∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ, —É–º–µ–Ω—å—à–∞—è —à—É–º –∏ —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. Notes Writing –∫–æ—Å–≤–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—è –∏–º –ª—É—á—à–µ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª —Å—Ä–µ–¥–Ω–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 15.6 –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤."
                },
                "en": {
                    "title": "Enhancing Iterative RAG with Concise Notes for Better Reasoning",
                    "desc": "The paper introduces a method called Notes Writing, which enhances iterative Retrieval-Augmented Generation (RAG) by creating concise notes from retrieved documents at each step. This approach helps to reduce irrelevant information and noise, allowing models to focus on essential content, which improves reasoning and performance. Unlike previous methods that struggle with lengthy contexts or require fine-tuning, Notes Writing is scalable and can be applied across various RAG frameworks. The results show an average performance improvement of 15.6 percentage points with minimal increase in output tokens, demonstrating its effectiveness in multi-hop question answering tasks."
                },
                "zh": {
                    "title": "Á¨îËÆ∞ÂÜô‰ΩúÔºöÊèêÂçáËø≠‰ª£RAGÁöÑÊúâÊïàÊÄß",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúÁ¨îËÆ∞ÂÜô‰Ωú‚ÄùÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊîπÂñÑÂ§öË∑≥ÈóÆÁ≠î‰∏≠ÁöÑËø≠‰ª£Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâËøáÁ®ã„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÊØè‰∏™Ê≠•È™§ÁîüÊàêÁÆÄÊ¥ÅÁöÑÁ¨îËÆ∞ÔºåÂáèÂ∞ë‰∫ÜÂÜó‰Ωô‰ø°ÊÅØÁöÑÂπ≤Êâ∞Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÁ¨îËÆ∞ÂÜô‰Ωú‰∏çÈúÄË¶ÅÂæÆË∞ÉÔºå‰∏îÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄßÔºåÈÄÇÁî®‰∫é‰∏çÂêåÁöÑËø≠‰ª£RAGÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁ¨îËÆ∞ÂÜô‰ΩúÂú®Â§ö‰∏™Ê®°ÂûãÂíåËØÑ‰º∞Êï∞ÊçÆÈõÜ‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü15.6‰∏™ÁôæÂàÜÁÇπÔºåÂêåÊó∂ËæìÂá∫‰ª§ÁâåÁöÑÂ¢ûÂä†ÈùûÂ∏∏ÊúâÈôê„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16056",
            "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
            "url": "https://huggingface.co/papers/2505.16056",
            "abstract": "MoE models achieve efficient scaling in LLMs with expert offloading, emphasizing the importance of local routing consistency and cache effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the optimal segment-level cache hit rate under a given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .",
            "score": 2,
            "issue_id": 3951,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "065702f91e2210fa",
            "authors": [
                "Jingcong Liang",
                "Siyuan Wang",
                "Miren Tian",
                "Yitong Li",
                "Duyu Tang",
                "Zhongyu Wei"
            ],
            "affiliations": [
                "Fudan University",
                "Huawei Technologies Ltd.",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16056.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MoE –º–æ–¥–µ–ª–µ–π: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é LLM",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts (MoE). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –≤ MoE –º–æ–¥–µ–ª—è—Ö: Segment Routing Best Performance (SRP) –∏ Segment Cache Best Hit Rate (SCH). –ê–Ω–∞–ª–∏–∑ 20 MoE LLM –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å MoE –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –∏ –±–µ–∑ –æ–±—â–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –Ω–∞–∏–≤—ã—Å—à—É—é –ª–æ–∫–∞–ª—å–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç—ã, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–µ—Å—è –Ω–∞ –¥–æ–º–µ–Ω–∞—Ö, –±–æ–ª—å—à–µ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, —á–µ–º —ç–∫—Å–ø–µ—Ä—Ç—ã –ø–æ —Å–ª–æ–≤–∞—Ä—é."
                },
                "en": {
                    "title": "Optimizing Expert Offloading for Efficient MoE Models",
                    "desc": "This paper discusses how Mixture-of-Experts (MoE) models can efficiently scale large language models (LLMs) by using expert offloading, which allows some experts to be stored in fast memory while others remain in slower memory. The authors introduce two new metrics to evaluate local routing consistency: Segment Routing Best Performance (SRP) and Segment Cache Best Hit Rate (SCH), which help assess how well experts serve token segments. Their analysis of 20 different MoE LLMs reveals that models using MoE at every layer and avoiding shared experts achieve better routing consistency. The study also finds that domain-specialized experts enhance routing consistency more than vocabulary-specialized ones, suggesting a balance between cache effectiveness and efficiency can be achieved with appropriate cache sizes."
                },
                "zh": {
                    "title": "È´òÊïàÊâ©Â±ïÔºöMoEÊ®°ÂûãÁöÑÂ±ÄÈÉ®Ë∑ØÁî±‰∏ÄËá¥ÊÄß",
                    "desc": "MoEÊ®°ÂûãÈÄöËøáÁ®ÄÁñèÊøÄÊ¥ª‰∏ìÂÆ∂ÂÆûÁé∞‰∫ÜÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ´òÊïàÊâ©Â±ï„ÄÇ‰∏∫‰∫ÜÂú®ÂÜÖÂ≠òÂèóÈôêÁöÑËÆæÂ§á‰∏äÊúâÊïàÈÉ®ÁΩ≤Â§ßÂûãMoEÊ®°ÂûãÔºåËÆ∏Â§öÁ≥ªÁªüÂºïÂÖ•‰∫Ü‰∏ìÂÆ∂Âç∏ËΩΩÊäÄÊúØÔºåÂ∞ÜÈÉ®ÂàÜ‰∏ìÂÆ∂ÁºìÂ≠òÂà∞Âø´ÈÄüÂÜÖÂ≠ò‰∏≠„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏§‰∏™Â∫¶ÈáèÊ†áÂáÜÊù•ËØÑ‰º∞MoEÊ®°ÂûãÁöÑÂ±ÄÈÉ®Ë∑ØÁî±‰∏ÄËá¥ÊÄßÔºåÂàÜÂà´ÊòØÊÆµË∑ØÁî±ÊúÄ‰Ω≥ÊÄßËÉΩ(SRP)ÂíåÊÆµÁºìÂ≠òÊúÄ‰Ω≥ÂëΩ‰∏≠Áéá(SCH)„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∫îÁî®MoE‰∫éÊØè‰∏ÄÂ±Ç‰∏î‰∏ç‰ΩøÁî®ÂÖ±‰∫´‰∏ìÂÆ∂ÁöÑÊ®°ÂûãÂÖ∑ÊúâÊúÄÈ´òÁöÑÂ±ÄÈÉ®Ë∑ØÁî±‰∏ÄËá¥ÊÄßÔºåËøô‰∏∫ÂÜÖÂ≠òÈ´òÊïàÁöÑMoEËÆæËÆ°ÂíåÈÉ®ÁΩ≤Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16409",
            "title": "FREESON: Retriever-Free Retrieval-Augmented Reasoning via\n  Corpus-Traversing MCTS",
            "url": "https://huggingface.co/papers/2505.16409",
            "abstract": "FREESON, a novel framework that integrates retrieval and reasoning roles within LRMs using CT-MCTS, improves the performance of multistep reasoning models in QA tasks by reducing representation bottlenecks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in multi-step reasoning and calling search engines at appropriate steps. However, existing retrieval-augmented reasoning approaches rely on separate retrieval models, limiting the LRM's role in retrieval to deciding when to retrieve and how to query. This separation not only increases hardware and operational costs but also leads to errors in the retrieval process due to the representation bottleneck, a phenomenon where the retriever's embedding space is not expressive enough to meet the generator's requirements. To address this, we shift our perspective from sequence-to-sequence matching to locating the answer-containing paths within the corpus, and propose a novel framework called FREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables LRMs to retrieve relevant knowledge on their own by acting as both a generator and retriever. To achieve this, we introduce a variant of the MCTS algorithm specialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing Monte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus toward answer-containing regions. Our results on five open-domain QA benchmarks, including single-hop and multi-hop questions, show that FREESON achieves an average improvement of 14.4% in EM and F1 over four multi-step reasoning models with a separate retriever, and it also performs comparably to the strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.",
            "score": 1,
            "issue_id": 3957,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "31b1d9d5fef7e537",
            "authors": [
                "Chaeeun Kim",
                "Seungone Kim"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "LBOX"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16409.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rag",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "FREESON: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º",
                    "desc": "FREESON - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–∞ CT-MCTS. –û–Ω–∞ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, —É–º–µ–Ω—å—à–∞—è –ø—Ä–æ–±–ª–µ–º—É —É–∑–∫–æ–≥–æ –º–µ—Å—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. FREESON –ø–æ–∑–≤–æ–ª—è–µ—Ç LRM —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, –≤—ã—Å—Ç—É–ø–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤ —Ä–æ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º."
                },
                "en": {
                    "title": "FREESON: Unifying Retrieval and Reasoning for Enhanced QA Performance",
                    "desc": "FREESON is a new framework that enhances Large Reasoning Models (LRMs) by integrating retrieval and reasoning tasks using a specialized algorithm called CT-MCTS. This approach allows LRMs to independently retrieve relevant information while generating answers, eliminating the need for separate retrieval models. By addressing the representation bottleneck, FREESON improves the efficiency and accuracy of multi-step reasoning in question-answering tasks. The framework has shown significant performance gains on various QA benchmarks, outperforming traditional models that rely on separate retrieval systems."
                },
                "zh": {
                    "title": "FREESONÔºöÊï¥ÂêàÊ£ÄÁ¥¢‰∏éÊé®ÁêÜÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "FREESONÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂÆÉÂ∞ÜÊ£ÄÁ¥¢ÂíåÊé®ÁêÜÁöÑËßíËâ≤Êï¥ÂêàÂú®Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâ‰∏≠Ôºå‰ΩøÁî®CT-MCTSÁÆóÊ≥ïÊù•ÊèêÂçáÂ§öÊ≠•Êé®ÁêÜÊ®°ÂûãÂú®ÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ‰º†ÁªüÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫Êé®ÁêÜÊñπÊ≥ï‰æùËµñ‰∫éÁã¨Á´ãÁöÑÊ£ÄÁ¥¢Ê®°ÂûãÔºåËøôÈôêÂà∂‰∫ÜLRMÂú®Ê£ÄÁ¥¢‰∏≠ÁöÑ‰ΩúÁî®ÔºåÂπ∂ÂØºËá¥Ë°®Á§∫Áì∂È¢àÁöÑÈóÆÈ¢ò„ÄÇFREESONÈÄöËøáËÆ©LRMÂêåÊó∂‰Ωú‰∏∫ÁîüÊàêÂô®ÂíåÊ£ÄÁ¥¢Âô®ÔºåËÉΩÂ§üËá™‰∏ªÊ£ÄÁ¥¢Áõ∏ÂÖ≥Áü•ËØÜÔºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÁ°¨‰ª∂ÂíåÊìç‰ΩúÊàêÊú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFREESONÂú®Â§ö‰∏™ÂºÄÊîæÂüüÈóÆÁ≠îÂü∫ÂáÜ‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü14.4%ÁöÑË°®Áé∞ÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Â§öÊ≠•Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16022",
            "title": "NOVER: Incentive Training for Language Models via Verifier-Free\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.16022",
            "abstract": "NOVER, a reinforcement learning framework that eliminates the need for external verifiers, enhances language model performance across text-to-text tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training.",
            "score": 1,
            "issue_id": 3951,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "f1dc404be3a18676",
            "authors": [
                "Wei Liu",
                "Siya Qi",
                "Xinyu Wang",
                "Chen Qian",
                "Yali Du",
                "Yulan He"
            ],
            "affiliations": [
                "Kings College London",
                "Shanghai Jiao Tong University",
                "The Alan Turing Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16022.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "NOVER: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "NOVER - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ–∫—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º. NOVER –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏ —Ç–æ–≥–æ –∂–µ —Ä–∞–∑–º–µ—Ä–∞, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –Ω–∞ 7.7%. –°–∏—Å—Ç–µ–º–∞ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–±—Ä–∞—Ç–Ω–æ–µ —Å—Ç–∏–º—É–ª–∏—Ä—É—é—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ."
                },
                "en": {
                    "title": "NOVER: Reinforcement Learning Without External Verifiers",
                    "desc": "NOVER is a novel reinforcement learning framework designed to improve the performance of language models without relying on external verifiers. It utilizes incentive training, which focuses on rewarding the final output of the model while encouraging intermediate reasoning steps. Unlike previous methods that depend on costly and high-quality annotated data for training reward models, NOVER only requires standard supervised fine-tuning data. This approach not only enhances the model's capabilities across various text-to-text tasks but also allows for innovative optimization techniques like inverse incentive training."
                },
                "zh": {
                    "title": "NOVERÔºöÊó†ÈúÄÂ§ñÈÉ®È™åËØÅÁöÑÂº∫ÂåñÂ≠¶‰π†Êñ∞Ê°ÜÊû∂",
                    "desc": "NOVERÊòØ‰∏ÄÁßçÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊ∂àÈô§‰∫ÜÂØπÂ§ñÈÉ®È™åËØÅËÄÖÁöÑÈúÄÊ±ÇÔºå‰ªéËÄåÊèêÂçá‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÂú®ÊñáÊú¨Âà∞ÊñáÊú¨‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøá‰ªÖ‰æùËµñÊ†áÂáÜÁöÑÁõëÁù£ÂæÆË∞ÉÊï∞ÊçÆÔºåÈÅøÂÖç‰∫ÜÈ´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑÊòÇË¥µËÆ≠ÁªÉÊàêÊú¨„ÄÇNOVERÊîØÊåÅÊøÄÂä±ËÆ≠ÁªÉÔºåËÉΩÂ§üÂú®Â§öÁßçÊñáÊú¨‰ªªÂä°‰∏≠Â∫îÁî®ÔºåÂπ∂‰∏îÂú®‰∏éÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÁõ∏ÊØîÊó∂ÔºåË°®Áé∞ÊèêÈ´ò‰∫Ü7.7%„ÄÇÊ≠§Â§ñÔºåNOVERÁöÑÁÅµÊ¥ªÊÄß‰∏∫‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõ‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄßÔºå‰æãÂ¶ÇÈÄÜÊøÄÂä±ËÆ≠ÁªÉ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14256",
            "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric\n  Multilingual Machine Translation",
            "url": "https://huggingface.co/papers/2505.14256",
            "abstract": "FuxiMT, a Chinese-centric multilingual machine translation model utilizing a sparsified large language model, demonstrates superior performance in low-resource scenarios and strong zero-shot capabilities across 65 languages.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.",
            "score": 1,
            "issue_id": 3960,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "95b48054decdcd12",
            "authors": [
                "Shaolin Zhu",
                "Tianyu Dong",
                "Bo Li",
                "Deyi Xiong"
            ],
            "affiliations": [
                "College of Intelligence and Computing, Tianjin University, Tianjin, China",
                "School of Software, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14256.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#machine_translation",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "üåê",
                "ru": {
                    "title": "FuxiMT: –ü—Ä–æ—Ä—ã–≤ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫",
                    "desc": "FuxiMT - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫ –∏ –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å (LLM). –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞—Å—Å–∏–≤–Ω–æ–º –∫–∏—Ç–∞–π—Å–∫–æ–º –∫–æ—Ä–ø—É—Å–µ –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –¥–æ–≤–æ–¥–∫–∞ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–∑ 65 —è–∑—ã–∫–æ–≤. FuxiMT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE) –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫—É—Ä—Ä–∏–∫—É–ª—è—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ FuxiMT –Ω–∞–¥ —Å–∏–ª—å–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –±–µ–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "FuxiMT: Bridging Language Gaps with Multilingual Mastery",
                    "desc": "FuxiMT is a multilingual machine translation model designed specifically for Chinese, leveraging a sparsified large language model (LLM). It uses a two-stage training approach, starting with pre-training on a large Chinese dataset followed by multilingual fine-tuning on a dataset of 65 languages. The model incorporates Mixture-of-Experts (MoEs) and a curriculum learning strategy to enhance its performance, especially in low-resource settings. Experimental results show that FuxiMT outperforms existing state-of-the-art models, particularly excelling in zero-shot translation for language pairs without parallel data."
                },
                "zh": {
                    "title": "FuxiMTÔºö‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏ãÁöÑÁøªËØëÊñ∞Á™ÅÁ†¥",
                    "desc": "FuxiMTÊòØ‰∏ÄÁßç‰ª•‰∏≠Êñá‰∏∫‰∏≠ÂøÉÁöÑÂ§öËØ≠Ë®ÄÊú∫Âô®ÁøªËØëÊ®°ÂûãÔºåÈááÁî®Á®ÄÁñèÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÈÄöËøá‰∏§Èò∂ÊÆµÁ≠ñÁï•ËøõË°åËÆ≠ÁªÉÔºåÈ¶ñÂÖàÂú®Â§ßÈáè‰∏≠ÊñáËØ≠Êñô‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂú®ÂåÖÂê´65ÁßçËØ≠Ë®ÄÁöÑÂ§ßÂûãÂπ≥Ë°åÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂ§öËØ≠Ë®ÄÂæÆË∞É„ÄÇFuxiMTÁªìÂêà‰∫Ü‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEsÔºâÊäÄÊúØÔºåÂπ∂ÈááÁî®ËØæÁ®ãÂ≠¶‰π†Á≠ñÁï•Ôºå‰ª•Á°Æ‰øùÂú®‰∏çÂêåËµÑÊ∫êÊ∞¥Âπ≥‰∏ãÁöÑÁ®≥ÂÅ•Ë°®Áé∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFuxiMTÂú®‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏ãÊòæËëó‰ºò‰∫éÂº∫Âü∫Á∫øÊ®°ÂûãÔºåÂ∞§ÂÖ∂Âú®Êú™ËßÅËØ≠Ë®ÄÂØπÁöÑÈõ∂-shotÁøªËØëËÉΩÂäõ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12891",
            "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
            "url": "https://huggingface.co/papers/2505.12891",
            "abstract": "A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , and the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME .",
            "score": 1,
            "issue_id": 3949,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 –º–∞—è",
                "en": "May 19",
                "zh": "5Êúà19Êó•"
            },
            "hash": "3f60161242cb3f4c",
            "authors": [
                "Shaohang Wei",
                "Wei Li",
                "Feifan Song",
                "Wen Luo",
                "Tianyi Zhuang",
                "Haochen Tan",
                "Zhijiang Guo",
                "Houfeng Wang"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12891.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#survey",
                    "#reasoning",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "‚è≥",
                "ru": {
                    "title": "TIME: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TIME –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π, –±—ã—Å—Ç—Ä–æ –º–µ–Ω—è—é—â–µ–π—Å—è –¥–∏–Ω–∞–º–∏–∫–æ–π —Å–æ–±—ã—Ç–∏–π –∏ —Å–ª–æ–∂–Ω—ã–º–∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏. TIME –≤–∫–ª—é—á–∞–µ—Ç 38,522 –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 3 —É—Ä–æ–≤–Ω—è –∏ 11 –ø–æ–¥–∑–∞–¥–∞—á. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –∏ –±–µ–∑, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ –≤–ª–∏—è–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é."
                },
                "en": {
                    "title": "TIME: Benchmarking Temporal Reasoning in LLMs",
                    "desc": "The paper introduces a benchmark called TIME, which evaluates temporal reasoning capabilities in Large Language Models (LLMs) across real-world scenarios. It addresses three main challenges: handling intensive temporal information, adapting to fast-changing events, and understanding complex social interactions. TIME includes 38,522 question-answer pairs organized into three levels and 11 sub-tasks, with datasets like TIME-Wiki, TIME-News, and TIME-Dial. The authors also provide a human-annotated subset, TIME-Lite, to support future research and standardized evaluation in this area."
                },
                "zh": {
                    "title": "TIMEÂü∫ÂáÜÔºöÊèêÂçáÊó∂Èó¥Êé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫TIMEÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ËøõË°åÊó∂Èó¥Êé®ÁêÜÁöÑËÉΩÂäõ„ÄÇTIMEÂü∫ÂáÜÊ∂µÁõñ‰∫Ü38,522‰∏™ÈóÆÁ≠îÂØπÔºåÂàÜ‰∏∫‰∏â‰∏™Â±ÇÊ¨°Âíå11‰∏™ÁªÜÂàÜ‰ªªÂä°ÔºåÊó®Âú®Â∫îÂØπÂØÜÈõÜÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÅÂø´ÈÄüÂèòÂåñÁöÑ‰∫ã‰ª∂Âä®ÊÄÅÂíåÂ§çÊùÇÁöÑÁ§æ‰ºö‰∫íÂä®‰∏≠ÁöÑÊó∂Èó¥‰æùËµñÊÄß„ÄÇÊàë‰ª¨ÂØπÊé®ÁêÜÊ®°ÂûãÂíåÈùûÊé®ÁêÜÊ®°ÂûãËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÂπ∂ÂàÜÊûê‰∫Ü‰∏çÂêåÁé∞ÂÆûÂú∫ÊôØÂíå‰ªªÂä°‰∏≠ÁöÑÊó∂Èó¥Êé®ÁêÜË°®Áé∞„ÄÇ‰∏∫‰∫Ü‰øÉËøõÊú™Êù•ÁöÑÁ†îÁ©∂ÂíåÊ†áÂáÜÂåñËØÑ‰º∞ÔºåÊàë‰ª¨ËøòÂèëÂ∏É‰∫ÜTIME-LiteÔºå‰∏Ä‰∏™ÁªèËøá‰∫∫Â∑•Ê†áÊ≥®ÁöÑÂ≠êÈõÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11881",
            "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
            "url": "https://huggingface.co/papers/2505.11881",
            "abstract": "Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.  \t\t\t\t\tAI-generated summary \t\t\t\t Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy gain for ViT-B on ImageNet-1k.",
            "score": 1,
            "issue_id": 3948,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 –º–∞—è",
                "en": "May 17",
                "zh": "5Êúà17Êó•"
            },
            "hash": "b5265bcccaafd719",
            "authors": [
                "Giyeong Oh",
                "Woohyun Cho",
                "Siyeol Kim",
                "Suhwan Choi",
                "Younjae Yu"
            ],
            "affiliations": [
                "Maum.AI",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11881.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º —Å–≤—è–∑—è–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ–µ –û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ' –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –≤—ã—Ö–æ–¥ –º–æ–¥—É–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤—Ö–æ–¥–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–π —ç—Ç–æ–º—É –ø–æ—Ç–æ–∫—É. –ú–µ—Ç–æ–¥ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª–µ–π –≤–Ω–æ—Å–∏—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–æ–≤—ã–µ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—è –±–æ–ª–µ–µ –±–æ–≥–∞—Ç–æ–º—É –æ–±—É—á–µ–Ω–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–±–æ–±—â–µ–Ω–∏—è –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Unlocking New Features with Orthogonal Residual Updates",
                    "desc": "This paper introduces Orthogonal Residual Updates, a novel approach to enhance feature learning in deep neural networks. By decomposing the output of a module relative to the input stream, it ensures that only the orthogonal component is added, allowing the model to learn new features rather than just modifying existing ones. This method addresses the limitations of traditional residual connections, which can lead to underutilization of the model's capacity. The authors demonstrate that this technique improves generalization accuracy and training stability across various architectures and datasets, achieving significant performance gains."
                },
                "zh": {
                    "title": "Ê≠£‰∫§ÊÆãÂ∑ÆÊõ¥Êñ∞ÔºöÊèêÂçáÁâπÂæÅÂ≠¶‰π†‰∏éËÆ≠ÁªÉÁ®≥ÂÆöÊÄß",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊõ¥Êñ∞ÊñπÊ≥ïÔºåÁß∞‰∏∫Ê≠£‰∫§ÊÆãÂ∑ÆÊõ¥Êñ∞ÔºåÊó®Âú®Â¢ûÂº∫ÁâπÂæÅÂ≠¶‰π†ÂíåËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇ‰º†ÁªüÁöÑÊÆãÂ∑ÆËøûÊé•Áõ¥Êé•Â∞ÜÊ®°ÂùóËæìÂá∫Ê∑ªÂä†Âà∞ËæìÂÖ•ÊµÅ‰∏≠ÔºåËøôÂèØËÉΩÂØºËá¥Â≠¶‰π†Êñ∞ÁâπÂæÅÁöÑËÉΩÂäõË¢´‰Ωé‰º∞„ÄÇÊ≠£‰∫§ÊÆãÂ∑ÆÊõ¥Êñ∞ÈÄöËøáÂ∞ÜÊ®°ÂùóËæìÂá∫Áõ∏ÂØπ‰∫éËæìÂÖ•ÊµÅËøõË°åÂàÜËß£ÔºåÂè™Ê∑ªÂä†‰∏éËæìÂÖ•ÊµÅÊ≠£‰∫§ÁöÑÈÉ®ÂàÜÔºå‰ªéËÄåÂºïÂØºÊ®°Âùó‰∏ªË¶ÅË¥°ÁåÆÊñ∞ÁöÑË°®Á§∫ÊñπÂêë„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂú®Â§öÁßçÊû∂ÊûÑÂíåÊï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫ÜÊ≥õÂåñÂáÜÁ°ÆÊÄßÂíåËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17552",
            "title": "Universal Biological Sequence Reranking for Improved De Novo Peptide\n  Sequencing",
            "url": "https://huggingface.co/papers/2505.17552",
            "abstract": "RankNovo is a deep reranking framework that enhances de novo peptide sequencing using multiple models and axial attention, achieving superior performance and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub.",
            "score": 0,
            "issue_id": 3961,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 –º–∞—è",
                "en": "May 23",
                "zh": "5Êúà23Êó•"
            },
            "hash": "bcaa51d217174ccf",
            "authors": [
                "Zijie Qiu",
                "Jiaqi Wei",
                "Xiang Zhang",
                "Sheng Xu",
                "Kai Zou",
                "Zhi Jin",
                "Zhiqiang Gao",
                "Nanqing Dong",
                "Siqi Sun"
            ],
            "affiliations": [
                "Fudan University",
                "NetMind.AI",
                "ProtagoLabs Inc",
                "Shanghai Artificial Intelligence Laboratory",
                "Soochow University",
                "University of British Columbia",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17552.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#science",
                    "#training",
                    "#dataset"
                ],
                "emoji": "üß¨",
                "ru": {
                    "title": "RankNovo: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–µ–∫–≤–µ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–µ–ø—Ç–∏–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è",
                    "desc": "RankNovo - —ç—Ç–æ –≥–ª—É–±–æ–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è de novo —Å–µ–∫–≤–µ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø–µ–ø—Ç–∏–¥–æ–≤. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∏ –∞–∫—Å–∏–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è. RankNovo –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤, –º–æ–¥–µ–ª–∏—Ä—É—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –ø–µ–ø—Ç–∏–¥–æ–≤ –∫–∞–∫ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –°–∏—Å—Ç–µ–º–∞ –≤–≤–æ–¥–∏—Ç –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ PMD –∏ RMD –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –º–∞—Å—Å–æ–≤—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –ø–µ–ø—Ç–∏–¥–∞–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ—Å—Ç–∞—Ç–∫–æ–≤."
                },
                "en": {
                    "title": "Revolutionizing Peptide Sequencing with RankNovo",
                    "desc": "RankNovo is a novel deep reranking framework designed to improve de novo peptide sequencing by integrating multiple models and axial attention mechanisms. It addresses the challenges posed by mass spectrometry data, which often contains complex noise and biases, by employing a list-wise reranking strategy that treats candidate peptides as multiple sequence alignments. The introduction of new metrics, PMD and RMD, allows for more precise supervision by measuring mass differences at both the sequence and residue levels. RankNovo not only outperforms existing models but also demonstrates impressive zero-shot generalization, making it a robust tool for peptide sequencing."
                },
                "zh": {
                    "title": "RankNovoÔºöËÇΩÂ∫èÂàóÈáçÊéíÂ∫èÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "RankNovo ÊòØ‰∏Ä‰∏™Ê∑±Â∫¶ÈáçÊéíÂ∫èÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂ§öÊ®°ÂûãÂíåËΩ¥ÂêëÊ≥®ÊÑèÂäõÊù•Â¢ûÂº∫ de novo ËÇΩÂ∫èÂàóÁöÑÊµãÂÆö„ÄÇÂΩìÂâçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊñπÊ≥ïÂú®Ë¥®Ë∞±Êï∞ÊçÆÁöÑÂ§çÊùÇÊÄßÂíåÂô™Â£∞‰ø°Âè∑ÁöÑÂºÇË¥®ÂàÜÂ∏É‰∏ãË°®Áé∞ÊúâÈôêÔºåÂØºËá¥Êï∞ÊçÆÁâπÂÆöÁöÑÂÅèÂ∑Æ„ÄÇRankNovo ÈááÁî®ÂàóË°®ÈáçÊéíÂ∫èÁöÑÊñπÊ≥ïÔºåÂ∞ÜÂÄôÈÄâËÇΩÂª∫Ê®°‰∏∫Â§ö‰∏™Â∫èÂàóÊØîÂØπÔºåÂπ∂Âà©Áî®ËΩ¥ÂêëÊ≥®ÊÑèÂäõÊèêÂèñÂÄôÈÄâËÇΩ‰πãÈó¥ÁöÑ‰ø°ÊÅØÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏§ÁßçÊñ∞ÊåáÊ†á PMDÔºàËÇΩË¥®ÈáèÂÅèÂ∑ÆÔºâÂíå RMDÔºàÊÆã‰ΩôË¥®ÈáèÂÅèÂ∑ÆÔºâÔºå‰∏∫ËÇΩÂ∫èÂàóÁöÑË¥®ÈáèÂ∑ÆÂºÇÊèê‰æõÁ≤æÁªÜÁöÑÁõëÁù£„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-05-23.html",
    "link_next": "2025-05-27.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "23.05",
        "en": "05/23",
        "zh": "5Êúà23Êó•"
    },
    "short_date_next": {
        "ru": "27.05",
        "en": "05/27",
        "zh": "5Êúà27Êó•"
    },
    "categories": {
        "#dataset": 18,
        "#data": 3,
        "#benchmark": 19,
        "#agents": 2,
        "#cv": 6,
        "#rl": 13,
        "#rlhf": 8,
        "#rag": 4,
        "#plp": 0,
        "#inference": 6,
        "#3d": 1,
        "#audio": 3,
        "#video": 3,
        "#multimodal": 11,
        "#math": 4,
        "#multilingual": 3,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 27,
        "#robotics": 2,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 2,
        "#reasoning": 17,
        "#transfer_learning": 5,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 3,
        "#optimization": 27,
        "#survey": 2,
        "#diffusion": 5,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 5,
        "#long_context": 4,
        "#synthetic": 3,
        "#machine_translation": 1,
        "#leakage": 1,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 2
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫TabSTARÁöÑÊ®°Âûã„ÄÇÂÆÉÊòØ‰∏ÄÁßçÁî®‰∫éË°®Ê†ºÊï∞ÊçÆÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåËÉΩÂ§üÂ§ÑÁêÜÊñáÊú¨ÁâπÂæÅ„ÄÇTabSTARÈÄöËøáËøÅÁßªÂ≠¶‰π†ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂàÜÁ±ª‰ªªÂä°ÊÄßËÉΩÔºåËÄå‰∏çÈúÄË¶ÅÁâπÂÆö‰∫éÊï∞ÊçÆÈõÜÁöÑÂèÇÊï∞„ÄÇÂÆÉÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨ÁºñÁ†ÅÂô®ÔºåÁªìÂêàÁõÆÊ†á‰ª§ÁâåÔºåÂ≠¶‰π†‰ªªÂä°ÁâπÂÆöÁöÑÂµåÂÖ•„ÄÇTabSTARÂú®‰∏≠Á≠âÂíåÂ§ßÂûãÊï∞ÊçÆÈõÜÁöÑÂàÜÁ±ª‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Â±ïÁ§∫‰∫ÜÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÁöÑÊâ©Â±ïËßÑÂæã„ÄÇ",
        "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
        "pinyin": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫TabSTARÁöÑÊ®°Âûã„ÄÇÂÆÉÊòØ‰∏ÄÁßçÁî®‰∫éË°®Ê†ºÊï∞ÊçÆÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåËÉΩÂ§üÂ§ÑÁêÜÊñáÊú¨ÁâπÂæÅ„ÄÇTabSTARÈÄöËøáËøÅÁßªÂ≠¶‰π†ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂàÜÁ±ª‰ªªÂä°ÊÄßËÉΩÔºåËÄå‰∏çÈúÄË¶ÅÁâπÂÆö‰∫éÊï∞ÊçÆÈõÜÁöÑÂèÇÊï∞„ÄÇÂÆÉÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨ÁºñÁ†ÅÂô®ÔºåÁªìÂêàÁõÆÊ†á‰ª§ÁâåÔºåÂ≠¶‰π†‰ªªÂä°ÁâπÂÆöÁöÑÂµåÂÖ•„ÄÇTabSTARÂú®‰∏≠Á≠âÂíåÂ§ßÂûãÊï∞ÊçÆÈõÜÁöÑÂàÜÁ±ª‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Â±ïÁ§∫‰∫ÜÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÁöÑÊâ©Â±ïËßÑÂæã„ÄÇ\n\nzh√® piƒÅn w√©n zhƒÅng ji√® sh√†o le yƒ´ zh«íng m√≠ng w√©i TabSTAR de m√≥ x√≠ng. tƒÅ sh√¨ yƒ´ zh«íng y√≤ng y√∫ bi«éo g√© sh√π j√π de jƒ´ ch«î m√≥ x√≠ng, n√©ng g√≤u ch«î l«ê w√©n bƒõn t√® zh√®ng. TabSTAR t≈çng gu√≤ qiƒÅn y√≠ xu√© x√≠ sh√≠ xi√†n le zu√¨ xiƒÅn j√¨n de fƒìn l√®i r√®n w√π x√≠ng n√©ng, √©r b√π x≈´ y√†o t√® d√¨ng y√∫ sh√π j√π de cƒÅn sh√π. tƒÅ l√¨ y√≤ng y√π x√πn li√†n de w√©n bƒõn biƒÅn m«é q√¨, ji√© h√© m√π biƒÅo l√¨ng p√°i, xu√© x√≠ r√®n w√π t√® d√¨ng de qi√†n r√π. TabSTAR z√†i zh≈çng dƒõng h√© d√† x√≠ng sh√π j√π de fƒìn l√®i r√®n w√π zh≈çng bi«éo xi√†n ch≈´ s√®, b√¨ng zh«én sh√¨ le y√π x√πn li√†n jiƒì du√†n de ku√≤ zh«én guƒ´ l«ú.",
        "vocab": "[\n    {\"word\": \"ËøÅÁßªÂ≠¶‰π†\", \"pinyin\": \"qiƒÅn y√≠ xu√© x√≠\", \"trans\": \"transfer learning\"},\n    {\"word\": \"ÂàÜÁ±ª‰ªªÂä°\", \"pinyin\": \"fƒìn l√®i r√®n w√π\", \"trans\": \"classification task\"},\n    {\"word\": \"ÂèÇÊï∞\", \"pinyin\": \"cƒÅn sh«î\", \"trans\": \"parameter\"},\n    {\"word\": \"È¢ÑËÆ≠ÁªÉ\", \"pinyin\": \"y√π x√πn li√†n\", \"trans\": \"pre-training\"},\n    {\"word\": \"ÊñáÊú¨ÁºñÁ†ÅÂô®\", \"pinyin\": \"w√©n bƒõn biƒÅn m«é q√¨\", \"trans\": \"text encoder\"},\n    {\"word\": \"ÁõÆÊ†á‰ª§Áâå\", \"pinyin\": \"m√π biƒÅo l√¨ng p√°i\", \"trans\": \"target token\"},\n    {\"word\": \"ÂµåÂÖ•\", \"pinyin\": \"qi√†n r√π\", \"trans\": \"embedding\"},\n    {\"word\": \"Ë°®Áé∞Âá∫Ëâ≤\", \"pinyin\": \"bi«éo xi√†n ch≈´ s√®\", \"trans\": \"perform excellently\"},\n    {\"word\": \"Êâ©Â±ïËßÑÂæã\", \"pinyin\": \"ku√≤ zh«én guƒ´ l«ú\", \"trans\": \"expansion pattern\"}\n]",
        "trans": "This article introduces a model called TabSTAR. It is a foundational model for tabular data that can handle textual features. TabSTAR achieves state-of-the-art performance in classification tasks through transfer learning, without requiring parameters specific to the dataset. It leverages a pre-trained text encoder combined with target tokens to learn task-specific embeddings. TabSTAR performs exceptionally well in classification tasks on medium and large datasets and demonstrates scalability during the pre-training phase.",
        "update_ts": "2025-05-26 09:39"
    }
}