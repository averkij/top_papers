{
    "date": {
        "ru": "1 августа",
        "en": "August 1",
        "zh": "8月1日"
    },
    "time_utc": "2025-08-01 05:25",
    "weekday": 4,
    "issue_id": 5126,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.23726",
            "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
            "url": "https://huggingface.co/papers/2507.23726",
            "abstract": "Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.",
            "score": 37,
            "issue_id": 5124,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "ab5bfbdad68eb6bf",
            "authors": [
                "Luoxin Chen",
                "Jinming Gu",
                "Liankai Huang",
                "Wenhao Huang",
                "Zhicheng Jiang",
                "Allan Jie",
                "Xiaoran Jin",
                "Xing Jin",
                "Chenggang Li",
                "Kaijing Ma",
                "Cheng Ren",
                "Jiawei Shen",
                "Wenlei Shi",
                "Tong Sun",
                "He Sun",
                "Jiahui Wang",
                "Siran Wang",
                "Zhihong Wang",
                "Chenrui Wei",
                "Shufa Wei",
                "Yonghui Wu",
                "Yuchen Wu",
                "Yihang Xia",
                "Huajian Xin",
                "Fan Yang",
                "Huaiyuan Ying",
                "Hongyi Yuan",
                "Zheng Yuan",
                "Tianyang Zhan",
                "Chi Zhang",
                "Yue Zhang",
                "Ge Zhang",
                "Tianyun Zhao",
                "Jianqiu Zhao",
                "Yichi Zhou",
                "Thomas Hanwen Zhu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23726.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прорыв в автоматическом доказательстве теорем с помощью ИИ",
                    "desc": "Seed-Prover - это модель для автоматического доказательства теорем, использующая язык Lean. Она применяет итеративное уточнение доказательств и специализированную поддержку геометрии. Модель достигает высокой производительности в формальном доказательстве теорем и автоматизированных математических рассуждениях. Seed-Prover превосходит предыдущие системы на нескольких эталонных наборах задач, включая формализованные задачи Международной математической олимпиады."
                },
                "en": {
                    "title": "Seed-Prover: Revolutionizing Theorem Proving with Iterative Refinement",
                    "desc": "The paper introduces Seed-Prover, a model designed for formal theorem proving and automated mathematical reasoning using the Lean programming language. It leverages iterative refinement and specialized geometry support to enhance its proof capabilities. By employing reinforcement learning and clear supervision from formal verification, Seed-Prover achieves impressive results on challenging mathematical problems. The model outperforms previous systems, proving a high percentage of formalized IMO problems and demonstrating significant advancements in automated reasoning."
                },
                "zh": {
                    "title": "Seed-Prover：自动化数学推理的新突破",
                    "desc": "Seed-Prover是一种基于Lean的引理风格推理模型，能够在形式定理证明和自动数学推理中实现高性能。该模型通过迭代优化和专门的几何支持，克服了传统自然语言推理的局限性。Seed-Prover利用Lean的反馈和自我总结来不断改进其证明过程，并设计了三种推理策略以应对国际数学奥林匹克（IMO）级别的问题。通过引入Seed-Geometry几何推理引擎，Seed-Prover在几何问题上也取得了显著进展，展示了形式验证与长链推理的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22879",
            "title": "RecGPT Technical Report",
            "url": "https://huggingface.co/papers/2507.22879",
            "abstract": "RecGPT integrates large language models into recommender systems to focus on user intent, improving content diversity and satisfaction while enhancing merchant and platform performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.   To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.",
            "score": 12,
            "issue_id": 5124,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "2bd5536810f1694b",
            "authors": [
                "Chao Yi",
                "Dian Chen",
                "Gaoyang Guo",
                "Jiakai Tang",
                "Jian Wu",
                "Jing Yu",
                "Mao Zhang",
                "Sunhao Dai",
                "Wen Chen",
                "Wenjun Yang",
                "Yuning Jiang",
                "Zhujin Gao",
                "Bo Zheng",
                "Chi Li",
                "Dimin Wang",
                "Dixuan Wang",
                "Fan Li",
                "Fan Zhang",
                "Haibin Chen",
                "Haozhuang Liu",
                "Jialin Zhu",
                "Jiamang Wang",
                "Jiawei Wu",
                "Jin Cui",
                "Ju Huang",
                "Kai Zhang",
                "Kan Liu",
                "Lang Tian",
                "Liang Rao",
                "Longbin Li",
                "Lulu Zhao",
                "Na He",
                "Peiyang Wang",
                "Qiqi Huang",
                "Tao Luo",
                "Wenbo Su",
                "Xiaoxiao He",
                "Xin Tong",
                "Xu Chen",
                "Xunke Xi",
                "Yang Li",
                "Yaxuan Wu",
                "Yeqiu Yang",
                "Yi Hu",
                "Yinnan Song",
                "Yuchen Li",
                "Yujie Luo",
                "Yujin Yuan",
                "Yuliang Yan",
                "Zhengyang Wang",
                "Zhibo Xiao",
                "Zhixin Ma",
                "Zile Zhou",
                "Ziqi Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.22879.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "RecGPT: Рекомендации, ориентированные на намерения пользователей",
                    "desc": "RecGPT - это новая система рекомендаций, интегрирующая большие языковые модели (LLM) для фокусировки на намерениях пользователей. Она улучшает разнообразие контента и удовлетворенность пользователей, а также повышает эффективность для продавцов и платформы. RecGPT использует многоэтапную парадигму обучения, включающую предварительное выравнивание с усиленным рассуждением и эволюцию самообучения. Система уже развернута в приложении Taobao и показывает стабильный рост производительности для всех заинтересованных сторон."
                },
                "en": {
                    "title": "Empowering Recommendations with User Intent",
                    "desc": "RecGPT is a new framework that enhances recommender systems by focusing on user intent rather than just historical data. It integrates large language models (LLMs) to better understand and predict user interests, which helps in retrieving more relevant items and generating clearer explanations. This approach reduces the risk of overfitting to past preferences, thereby improving content diversity and user satisfaction. By deploying RecGPT on the Taobao App, the system has shown significant performance improvements for users, merchants, and the platform itself, creating a more sustainable recommendation ecosystem."
                },
                "zh": {
                    "title": "以用户意图为中心的推荐系统新范式",
                    "desc": "RecGPT 是一种将大型语言模型整合到推荐系统中的新框架，旨在关注用户意图。通过重新设计推荐流程，RecGPT 使推荐过程从单纯依赖历史数据转变为以用户意图为中心。该系统通过多阶段训练方法，结合推理增强的预对齐和自我训练，提升了推荐的准确性和多样性。实验结果表明，RecGPT 在用户满意度、商家曝光率和平台转化率等方面均取得了显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23682",
            "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models",
            "url": "https://huggingface.co/papers/2507.23682",
            "abstract": "The ViLLA framework enhances VLA models by incorporating latent actions, improving performance in both simulated and real-world robot manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research.",
            "score": 9,
            "issue_id": 5124,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "baa73e4730b01a97",
            "authors": [
                "Xiaoyu Chen",
                "Hangxing Wei",
                "Pushi Zhang",
                "Chuheng Zhang",
                "Kaixin Wang",
                "Yanjiang Guo",
                "Rushuai Yang",
                "Yucen Wang",
                "Xinquan Xiao",
                "Li Zhao",
                "Jianyu Chen",
                "Jiang Bian"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Microsoft Research",
                "Nanjing University",
                "Tsinghua University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23682.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#agi",
                    "#games",
                    "#robotics",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ViLLA: Улучшение роботизированных манипуляций с помощью латентных действий",
                    "desc": "Фреймворк ViLLA улучшает модели визуально-языкового действия (VLA) путем включения латентных действий. Это позволяет повысить производительность как в симулированных, так и в реальных задачах роботизированных манипуляций. Предложенный подход villa-X совершенствует как обучение латентным действиям, так и их интеграцию в предобучение VLA. Модель демонстрирует превосходные результаты в симуляторах SIMPLER и LIBERO, а также на реальных роботах с захватами и ловкими руками."
                },
                "en": {
                    "title": "Enhancing Robot Manipulation with Latent Actions in ViLLA Framework",
                    "desc": "The ViLLA framework enhances Visual-Language-Action (VLA) models by integrating latent actions, which represent abstract visual changes between frames. This integration improves the learning of robot manipulation policies that can effectively follow language instructions and adapt to new situations. The proposed villa-X model advances the way latent actions are learned and utilized during VLA pre-training, leading to better performance in both simulated and real-world tasks. Overall, the ViLLA paradigm shows great potential for future advancements in robot manipulation research."
                },
                "zh": {
                    "title": "ViLLA框架：提升机器人操作的潜力",
                    "desc": "ViLLA框架通过引入潜在动作来增强视觉-语言-动作（VLA）模型，从而提高机器人操作任务的性能。潜在动作是一种抽象表示，能够捕捉两个帧之间的视觉变化。本文介绍的villa-X是一个新颖的视觉-语言-潜在动作框架，旨在改进潜在动作建模，以学习可推广的机器人操作策略。我们的研究表明，villa-X在模拟环境和真实机器人设置中均表现出色，展示了ViLLA范式的巨大潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22968",
            "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations",
            "url": "https://huggingface.co/papers/2507.22968",
            "abstract": "A benchmark dataset for Spoken Dialogue Models (SDMs) in English and Chinese is presented to evaluate their performance in understanding and emulating human spoken conversations, addressing challenges like ambiguity and context-dependency.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.",
            "score": 7,
            "issue_id": 5124,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "3a2f5273d610d5d6",
            "authors": [
                "Chengqian Ma",
                "Wei Tao",
                "Yiwen Guo"
            ],
            "affiliations": [
                "Independent Researcher",
                "LIGHTSPEED",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22968.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#long_context",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Бенчмарк для оценки разговорных ИИ-моделей в реальных условиях",
                    "desc": "В статье представлен набор данных для оценки разговорных диалоговых моделей на английском и китайском языках. Этот бенчмарк позволяет оценить способность моделей понимать и имитировать человеческие разговоры, учитывая такие сложности как неоднозначность и контекстная зависимость. Набор данных содержит 1079 примеров и сопровождается методом оценки на основе больших языковых моделей, который хорошо коррелирует с человеческими оценками. Исследование направлено на комплексное изучение эффективности разговорных диалоговых моделей в решении практических задач."
                },
                "en": {
                    "title": "Benchmarking Spoken Dialogue Models for Real-World Conversations",
                    "desc": "This paper introduces a benchmark dataset designed for evaluating Spoken Dialogue Models (SDMs) in both English and Chinese. The dataset aims to address the complexities of human spoken conversations, such as ambiguity and context-dependency, which are more pronounced in voice interactions compared to text. It includes 1,079 instances that reflect real-world dialogue scenarios, allowing for a thorough assessment of SDM performance. Additionally, the paper presents an evaluation method based on Large Language Models (LLMs) that aligns closely with human judgment, enhancing the understanding of SDMs' effectiveness."
                },
                "zh": {
                    "title": "提升口语对话模型的评估标准",
                    "desc": "本文提出了一个用于评估口语对话模型（SDMs）性能的基准数据集，涵盖英语和中文，旨在理解和模拟人类口语对话。口语对话的复杂性体现在歧义性和上下文依赖性等挑战上，这些因素使得与文本基础的大型语言模型（LLMs）相比，SDMs的研究相对较少。数据集中包含1079个实例，并配备了一种基于LLM的评估方法，以更好地与人类判断相一致。通过这个数据集，研究者可以全面探讨SDMs在应对实际对话挑战中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23698",
            "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial\n  Intelligence in Visuomotor Agents",
            "url": "https://huggingface.co/papers/2507.23698",
            "abstract": "Reinforcement Learning enhances generalizable spatial reasoning and interaction in 3D environments through cross-view goal specification and automated task synthesis, achieving zero-shot generalization and improved interaction success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by 4times and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning.",
            "score": 3,
            "issue_id": 5125,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "4cb697aecb943154",
            "authors": [
                "Shaofei Cai",
                "Zhancun Mu",
                "Haiwen Xia",
                "Bowei Zhang",
                "Anji Liu",
                "Yitao Liang"
            ],
            "affiliations": [
                "Institute for Artificial Intelligence, Peking University",
                "School of Computing, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23698.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#3d",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RL открывает новые горизонты пространственного мышления для ИИ",
                    "desc": "Данная статья представляет метод улучшения пространственного мышления и взаимодействия агентов в 3D-средах с помощью обучения с подкреплением (RL). Авторы предлагают использовать кросс-видовую спецификацию целей и автоматизированный синтез задач для достижения обобщения без предварительного обучения. Эксперименты проводились в среде Minecraft и показали значительное улучшение успешности взаимодействия агентов. Результаты демонстрируют потенциал обучения с подкреплением для развития визуально-моторных навыков искусственных агентов."
                },
                "en": {
                    "title": "Empowering 3D Agents with Reinforcement Learning for Generalized Interaction",
                    "desc": "This paper discusses how Reinforcement Learning (RL) can improve the ability of agents to understand and interact in 3D environments, like Minecraft. It addresses the problem of RL models overfitting to specific tasks, which limits their ability to generalize to new situations. By using cross-view goal specification and automated task synthesis, the authors show that RL can help agents achieve zero-shot generalization, meaning they can perform well in unseen environments without prior training. The results indicate that RL can significantly enhance interaction success rates and spatial reasoning in diverse settings, including real-world applications."
                },
                "zh": {
                    "title": "强化学习：提升3D环境中的空间推理与交互能力",
                    "desc": "强化学习（RL）在3D环境中通过跨视角目标指定和自动化任务合成，增强了可推广的空间推理和交互能力，实现了零样本泛化和提高的交互成功率。本文探讨了RL在Minecraft中微调的视觉运动代理如何在未见过的世界中实现零样本泛化。我们分析并建立了跨视角目标指定作为视觉运动策略的统一多任务目标空间，以应对多任务RL表示中的挑战。此外，我们提出在高度可定制的Minecraft环境中进行自动化任务合成，以支持大规模多任务RL训练。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23632",
            "title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network\n  Perspective",
            "url": "https://huggingface.co/papers/2507.23632",
            "abstract": "Softmax attention is more expressive than linear attention due to its recurrent form, which can be analyzed using RNN components.  \t\t\t\t\tAI-generated summary \t\t\t\t Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts.",
            "score": 1,
            "issue_id": 5124,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "b07ddf6cb6b8bee8",
            "authors": [
                "Gabriel Mongaras",
                "Eric C. Larson"
            ],
            "affiliations": [
                "Lyle School of Engineering Southern Methodist University Dallas, TX 75205"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23632.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Раскрывая силу софтмакс-внимания через призму RNN",
                    "desc": "Статья исследует различия между софтмакс-вниманием и линейным вниманием в нейронных сетях. Авторы показывают, что софтмакс-внимание можно представить в рекуррентной форме, аналогичной рекуррентным нейронным сетям (RNN). Это позволяет провести анализ компонентов софтмакс-внимания и объяснить его большую выразительность по сравнению с линейными аналогами. Работа помогает понять, почему софтмакс-внимание остается основой современных трансформерных архитектур, несмотря на квадратичную сложность."
                },
                "en": {
                    "title": "Unlocking the Power of Softmax Attention",
                    "desc": "This paper explores the differences between softmax attention and linear attention in machine learning models, particularly in transformers. It shows that softmax attention, which is more expressive, can be understood through the lens of recurrent neural networks (RNNs). By analyzing softmax attention as an RNN, the authors can break down its components to see how they contribute to its performance. The findings clarify why softmax attention outperforms linear attention in terms of accuracy despite the latter's computational efficiency."
                },
                "zh": {
                    "title": "软max注意力的优势解析",
                    "desc": "本文探讨了softmax注意力与线性注意力的区别。softmax注意力因其表达能力强而成为现代变换器架构的基础，但其在序列长度上的计算复杂度和内存需求是一个主要缺点。通过将softmax非线性替换为线性注意力，研究者们试图解决这一瓶颈。本文表明，线性注意力实际上是softmax注意力的一种近似，并通过递归神经网络的语言来描述softmax注意力的各个部分，从而揭示其更强表达能力的原因。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20519",
            "title": "AgroBench: Vision-Language Model Benchmark in Agriculture",
            "url": "https://huggingface.co/papers/2507.20519",
            "abstract": "AgroBench evaluates vision-language models across agricultural tasks, revealing areas for improvement in fine-grained identification, particularly weed identification, with expert-annotated categories.  \t\t\t\t\tAI-generated summary \t\t\t\t Precise automated understanding of agricultural tasks such as disease identification is essential for sustainable crop production. Recent advances in vision-language models (VLMs) are expected to further expand the range of agricultural tasks by facilitating human-model interaction through easy, text-based communication. Here, we introduce AgroBench (Agronomist AI Benchmark), a benchmark for evaluating VLM models across seven agricultural topics, covering key areas in agricultural engineering and relevant to real-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is annotated by expert agronomists. Our AgroBench covers a state-of-the-art range of categories, including 203 crop categories and 682 disease categories, to thoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal that VLMs have room for improvement in fine-grained identification tasks. Notably, in weed identification, most open-source VLMs perform close to random. With our wide range of topics and expert-annotated categories, we analyze the types of errors made by VLMs and suggest potential pathways for future VLM development. Our dataset and code are available at https://dahlian00.github.io/AgroBenchPage/ .",
            "score": 1,
            "issue_id": 5125,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 июля",
                "en": "July 28",
                "zh": "7月28日"
            },
            "hash": "efa19cc739cbe95e",
            "authors": [
                "Risa Shinoda",
                "Nakamasa Inoue",
                "Hirokatsu Kataoka",
                "Masaki Onishi",
                "Yoshitaka Ushiku"
            ],
            "affiliations": [
                "Kyoto University",
                "National Institute of Advanced Industrial Science and Technology (AIST)",
                "OMRON SINIC",
                "The University of Osaka",
                "Tokyo Institute of Technology",
                "Visual Geometry Group, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20519.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#science",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🌾",
                "ru": {
                    "title": "AgroBench: экспертная оценка ИИ в сельском хозяйстве",
                    "desc": "AgroBench - это новый эталонный тест для оценки моделей компьютерного зрения и обработки естественного языка в сельскохозяйственных задачах. Он охватывает семь сельскохозяйственных тем и включает 203 категории культур и 682 категории болезней, аннотированные экспертами-агрономами. Тестирование показало, что существующие модели имеют значительные возможности для улучшения в задачах точной идентификации, особенно при распознавании сорняков. Авторы анализируют типы ошибок моделей и предлагают пути для их дальнейшего развития."
                },
                "en": {
                    "title": "Enhancing Agricultural AI: The AgroBench Benchmark",
                    "desc": "AgroBench is a benchmark designed to evaluate vision-language models (VLMs) specifically in the context of agricultural tasks. It focuses on fine-grained identification, such as accurately recognizing different types of weeds and diseases in crops, using categories annotated by expert agronomists. The benchmark includes a comprehensive set of 203 crop categories and 682 disease categories, highlighting the current limitations of VLMs in these areas. The findings indicate that many existing VLMs struggle with precise identification, particularly in weed detection, suggesting significant opportunities for improvement in future model development."
                },
                "zh": {
                    "title": "提升农业任务中的视觉-语言模型表现",
                    "desc": "AgroBench是一个用于评估视觉-语言模型（VLM）在农业任务中的表现的基准。它涵盖了七个农业主题，并由专家农学家进行标注，确保数据的准确性。研究发现，当前的VLM在细粒度识别任务，特别是杂草识别方面表现不佳，许多开源模型的表现接近随机。通过分析VLM的错误类型，AgroBench为未来的模型发展提供了改进的方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14793",
            "title": "Flow Equivariant Recurrent Neural Networks",
            "url": "https://huggingface.co/papers/2507.14793",
            "abstract": "Equivariant neural network architectures are extended to handle time-parameterized transformations, improving performance in sequence models like RNNs.  \t\t\t\t\tAI-generated summary \t\t\t\t Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of `flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us.",
            "score": 0,
            "issue_id": 5126,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 июля",
                "en": "July 20",
                "zh": "7月20日"
            },
            "hash": "eb29bf11c603c730",
            "authors": [
                "T. Anderson Keller"
            ],
            "affiliations": [
                "The Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA 15213"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14793.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "⏳",
                "ru": {
                    "title": "Эквивариантность во времени: новый подход к обработке последовательностей",
                    "desc": "Статья расширяет концепцию эквивариантных нейронных сетей для обработки преобразований, параметризованных во времени. Это улучшает производительность рекуррентных нейронных сетей (RNN) и других последовательностных моделей. Авторы вводят понятие 'потоков' - однопараметрических подгрупп Ли, описывающих естественные трансформации во времени, такие как визуальное движение. Эксперименты показывают, что потоково-эквивариантные модели значительно превосходят стандартные RNN по скорости обучения и способности к обобщению."
                },
                "en": {
                    "title": "Enhancing RNNs with Time-Parameter Equivariance",
                    "desc": "This paper extends equivariant neural network architectures to include time-parameterized transformations, which enhances their performance in sequence models like recurrent neural networks (RNNs). It highlights that traditional RNNs do not adequately account for the smooth, continuous changes in data over time, leading to inefficiencies. By introducing flow equivariance, the authors demonstrate that these new models can better handle temporal symmetries, resulting in improved training speed and generalization capabilities. This work aims to create sequence models that align more closely with the natural transformations observed in the real world."
                },
                "zh": {
                    "title": "提升序列模型性能的时间等变网络",
                    "desc": "本文扩展了等变神经网络架构，以处理时间参数化的变换，从而提高序列模型（如RNN）的性能。我们发现标准的RNN通常不具备流等变性，无法以几何结构的方式对移动刺激进行变换。通过引入流等变性，我们的模型在训练速度、长度泛化和速度泛化等方面显著优于非等变模型。此研究为构建尊重时间参数化对称性的序列模型奠定了基础。"
                }
            }
        }
    ],
    "link_prev": "2025-07-31.html",
    "link_next": "2025-08-04.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "31.07",
        "en": "07/31",
        "zh": "7月31日"
    },
    "short_date_next": {
        "ru": "04.08",
        "en": "08/04",
        "zh": "8月4日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}