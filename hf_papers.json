{
    "date": {
        "ru": "28 апреля",
        "en": "April 28",
        "zh": "4月28日"
    },
    "time_utc": "2025-04-28 10:45",
    "weekday": 0,
    "issue_id": 3464,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15376",
            "title": "Towards Understanding Camera Motions in Any Video",
            "url": "https://huggingface.co/papers/2504.15376",
            "abstract": "We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like \"follow\" (or tracking) require understanding scene content like moving subjects. We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, a novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then fine-tune a generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video.",
            "score": 109,
            "issue_id": 3457,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "fb835f3848fe977a",
            "authors": [
                "Zhiqiu Lin",
                "Siyuan Cen",
                "Daniel Jiang",
                "Jay Karhade",
                "Hewei Wang",
                "Chancharik Mitra",
                "Tiffany Ling",
                "Yuhan Huang",
                "Sifan Liu",
                "Mingyu Chen",
                "Rushikesh Zawar",
                "Xue Bai",
                "Yilun Du",
                "Chuang Gan",
                "Deva Ramanan"
            ],
            "affiliations": [
                "Adobe",
                "CMU",
                "Emerson",
                "Harvard",
                "MIT-IBM",
                "UMass Amherst",
                "USC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15376.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Новый взгляд на понимание движения камеры в видео",
                    "desc": "CameraBench - это новый крупномасштабный датасет и бенчмарк для оценки и улучшения понимания движения камеры в видео. Датасет содержит около 3000 разнообразных интернет-видео, размеченных экспертами с использованием многоступенчатого контроля качества. Авторы разработали таксономию примитивов движения камеры совместно с кинооператорами. Используя CameraBench, были оценены модели Structure-from-Motion (SfM) и видео-языковые модели (VLM), выявив их сильные и слабые стороны в понимании различных аспектов движения камеры."
                },
                "en": {
                    "title": "Unlocking Camera Motion Understanding with CameraBench",
                    "desc": "CameraBench is a comprehensive dataset and benchmark aimed at enhancing the understanding of camera motion in videos. It includes around 3,000 videos that have been meticulously annotated by experts, ensuring high-quality data for training and evaluation. The paper introduces a taxonomy of camera motion primitives, which helps in categorizing different types of camera movements, such as tracking and zooming. The study reveals that while traditional Structure-from-Motion models struggle with semantic understanding, fine-tuning a generative Video-Language Model on CameraBench can effectively bridge the gap between geometric and semantic motion understanding."
                },
                "zh": {
                    "title": "CameraBench：提升相机运动理解的基准与数据集",
                    "desc": "本文介绍了CameraBench，这是一个大规模的数据集和基准，用于评估和改善对相机运动的理解。CameraBench包含约3000个多样化的互联网视频，经过专家的严格多阶段质量控制进行标注。我们提出了一种相机运动原语的分类法，并与电影摄影师合作设计。研究发现，某些运动如“跟随”需要理解场景内容，而通过CameraBench评估的模型在捕捉语义原语和几何原语时存在困难。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16656",
            "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
            "url": "https://huggingface.co/papers/2504.16656",
            "abstract": "We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standing challenge of balancing sophisticated reasoning capabilities with broad generalization. To further enhance training efficiency, we propose the Selective Sample Buffer (SSB) mechanism, which effectively counters the ``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization (GRPO) by prioritizing high-value samples throughout the optimization process. Notably, we observe that excessive reinforcement signals can induce visual hallucinations--a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process. Empirical results affirm the exceptional capability of R1V2, with benchmark-leading performances such as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and 74.0 on MMMU. These results underscore R1V2's superiority over existing open-source models and demonstrate significant progress in closing the performance gap with premier proprietary systems, including Gemini 2.5 and OpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to promote openness and reproducibility https://huggingface.co/Skywork/Skywork-R1V2-38B.",
            "score": 37,
            "issue_id": 3462,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "2fbbb67c31db3cf1",
            "authors": [
                "Chris",
                "Yichen Wei",
                "Yi Peng",
                "Xiaokun Wang",
                "Weijie Qiu",
                "Wei Shen",
                "Tianyidan Xie",
                "Jiangbo Pei",
                "Jianhao Zhang",
                "Yunzhuo Hao",
                "Xuchen Song",
                "Yang Liu",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16656.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#optimization",
                    "#reasoning",
                    "#multimodal",
                    "#open_source",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Skywork R1V2: Прорыв в мультимодальных рассуждениях с помощью гибридного обучения с подкреплением",
                    "desc": "Skywork R1V2 - это мультимодальная модель рассуждений нового поколения, использующая гибридную парадигму обучения с подкреплением. Модель сочетает руководство модели вознаграждения с правилами, решая проблему баланса между сложными рассуждениями и широкой генерализацией. Введен механизм Selective Sample Buffer для повышения эффективности обучения, противодействующий проблеме 'исчезающих преимуществ' в Group Relative Policy Optimization. R1V2 демонстрирует лидирующие результаты на различных бенчмарках, превосходя существующие открытые модели и сокращая отставание от проприетарных систем."
                },
                "en": {
                    "title": "Skywork R1V2: Elevating Multimodal Reasoning with Hybrid Learning",
                    "desc": "Skywork R1V2 is an advanced multimodal reasoning model that improves upon its predecessor by integrating a hybrid reinforcement learning approach. This model combines reward-model guidance with rule-based strategies to enhance reasoning capabilities while ensuring broad generalization. To optimize training, it introduces the Selective Sample Buffer (SSB) mechanism, which prioritizes high-value samples and addresses the Vanishing Advantages issue in Group Relative Policy Optimization. The model has demonstrated outstanding performance on various benchmarks, outperforming existing open-source models and approaching the capabilities of leading proprietary systems."
                },
                "zh": {
                    "title": "Skywork R1V2：多模态推理的新突破",
                    "desc": "Skywork R1V2是一个新一代的多模态推理模型，相比于前身Skywork R1V有了显著的进步。它引入了一种混合强化学习范式，将奖励模型指导与基于规则的策略相结合，解决了复杂推理能力与广泛泛化之间的平衡问题。为了提高训练效率，提出了选择性样本缓冲机制（SSB），有效应对了群体相对策略优化中的“消失优势”困境。实验结果显示，R1V2在多个基准测试中表现优异，超越了现有的开源模型，缩小了与顶级专有系统的性能差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18415",
            "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
            "url": "https://huggingface.co/papers/2504.18415",
            "abstract": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.",
            "score": 16,
            "issue_id": 3458,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 апреля",
                "en": "April 25",
                "zh": "4月25日"
            },
            "hash": "1b18f5117da74852",
            "authors": [
                "Hongyu Wang",
                "Shuming Ma",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18415.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное сжатие больших языковых моделей без потери качества",
                    "desc": "BitNet v2 - это новая технология для эффективного развертывания 1-битных больших языковых моделей (LLM). Она решает проблему выбросов в активациях, которые затрудняют квантование до малого числа бит. Ключевой компонент - модуль H-BitLinear, применяющий онлайн-преобразование Адамара перед квантованием активаций. Эксперименты показывают, что BitNet v2 с 4-битными активациями достигает производительности BitNet b1.58, значительно снижая потребление памяти и вычислительные затраты."
                },
                "en": {
                    "title": "Revolutionizing 1-bit LLMs with Efficient 4-bit Quantization",
                    "desc": "This paper presents BitNet v2, a new framework designed to improve the deployment of 1-bit Large Language Models (LLMs) by enabling efficient 4-bit activation quantization. The challenge of activation outliers, which complicate the quantization process, is addressed through a novel module called H-BitLinear that applies an online Hadamard transformation. This transformation helps to convert sharp activation distributions into smoother, Gaussian-like forms, making them more suitable for low-bit representation. The results demonstrate that BitNet v2 can be trained with 4-bit activations with minimal performance loss, significantly lowering memory usage and computational costs during inference."
                },
                "zh": {
                    "title": "高效量化，轻松部署1位语言模型",
                    "desc": "本文介绍了一种名为BitNet v2的新框架，旨在高效部署1位大型语言模型（LLMs）。该框架通过引入H-BitLinear模块，解决了在注意力和前馈网络激活中出现的异常值问题，从而实现原生4位激活量化。H-BitLinear模块通过在线Hadamard变换，将激活分布平滑为更接近高斯分布的形式，适合低位表示。实验表明，使用原生4位激活训练的BitNet v2在性能上几乎没有下降，同时显著减少了内存占用和计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16427",
            "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
            "url": "https://huggingface.co/papers/2504.16427",
            "abstract": "Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.",
            "score": 9,
            "issue_id": 3457,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "1845c93e3bc64dd1",
            "authors": [
                "Hanlei Zhang",
                "Zhuohang Li",
                "Yeshuang Zhu",
                "Hua Xu",
                "Peiwu Wang",
                "Haige Zhu",
                "Jie Zhou",
                "Jinchao Zhang"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "Kennesaw State University",
                "Pattern Recognition Center, WeChat AI, Tencent Inc, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16427.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "MMLA: новый бенчмарк для оценки понимания мультимодальной семантики языковыми моделями",
                    "desc": "Статья представляет MMLA - новый набор данных для оценки способности мультимодальных языковых моделей понимать семантику человеческой речи. MMLA включает более 61 тысяч мультимодальных высказываний и охватывает 6 ключевых аспектов семантики. Авторы провели обширные эксперименты с различными языковыми моделями, используя методы обучения без учителя, тонкой настройки и инструктирования. Результаты показали, что даже после дообучения точность моделей составляет лишь 60-70%, что указывает на ограничения современных мультимодальных ЯМ в понимании сложной человеческой речи."
                },
                "en": {
                    "title": "Unlocking Multimodal Understanding in Language Models",
                    "desc": "This paper introduces MMLA, a new benchmark for evaluating multimodal large language models (MLLMs) in understanding complex human conversations. It includes over 61,000 multimodal utterances that assess six key aspects of semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. The study evaluates various LLMs and MLLMs using different methods, revealing that even the best fine-tuned models only achieve 60-70% accuracy in understanding these nuances. The authors aim for MMLA to be a foundational resource for advancing research in multimodal language analysis."
                },
                "zh": {
                    "title": "多模态语言分析的新基准：MMLA",
                    "desc": "多模态语言分析是一个快速发展的领域，利用多种模态来增强对人类对话语义的理解。尽管其重要性显著，但关于多模态大语言模型（MLLMs）理解认知层面语义的研究仍然较少。本文介绍了MMLA，这是一个专门设计的基准，旨在填补这一空白，包含超过61K个来自不同场景的多模态话语。通过对八种主流LLMs和MLLMs的评估，结果显示即使经过微调的模型在理解复杂人类语言方面的准确率也仅为60%~70%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17768",
            "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
            "url": "https://huggingface.co/papers/2504.17768",
            "abstract": "Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.",
            "score": 7,
            "issue_id": 3462,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "ff60c7d30ad1f761",
            "authors": [
                "Piotr Nawrot",
                "Robert Li",
                "Renjie Huang",
                "Sebastian Ruder",
                "Kelly Marchisio",
                "Edoardo M. Ponti"
            ],
            "affiliations": [
                "Cohere",
                "Meta",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17768.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Разреженное внимание: ключ к обработке длинных последовательностей в трансформерах",
                    "desc": "Это исследование фокусируется на применении разреженного внимания в трансформерных моделях для обработки длинных последовательностей. Авторы проводят тщательное сравнение методов разреженного внимания без дополнительного обучения на различных масштабах моделей, длинах последовательностей и уровнях разреженности. Результаты показывают, что для очень длинных последовательностей предпочтительнее использовать более крупные и сильно разреженные модели. Исследование также выявляет, что разреженное внимание не является универсальным решением и требует тщательной оценки компромиссов для приложений, чувствительных к производительности."
                },
                "en": {
                    "title": "Unlocking Long-Context Potential with Sparse Attention",
                    "desc": "This paper investigates the use of sparse attention in Transformer large language models (LLMs) to improve their ability to handle long sequences. The authors conduct a thorough analysis of various sparse attention methods, examining their performance across different model sizes, sequence lengths, and levels of sparsity. Key findings indicate that larger, sparse models are more effective for long sequences, but the optimal sparsity level varies depending on the task and phase of processing. The study also introduces new scaling laws for sparse attention, emphasizing the need for careful consideration of efficiency and accuracy trade-offs in practical applications."
                },
                "zh": {
                    "title": "稀疏注意力：提升变换器模型处理长序列的关键工具",
                    "desc": "稀疏注意力是一种有前景的策略，可以扩展变换器大语言模型（LLM）处理长上下文的能力。本文通过对不同模型规模、序列长度和稀疏水平的训练无关稀疏注意力方法进行比较，探讨了其有效性和效率-准确性权衡。实验结果表明，对于非常长的序列，较大且高度稀疏的模型优于较小且密集的模型。同时，稀疏注意力并不是通用的解决方案，因为在某些任务上，即使是适度的稀疏水平也会导致显著的性能下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15716",
            "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
            "url": "https://huggingface.co/papers/2504.15716",
            "abstract": "Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to address these challenges through reasoning-augmented supervision and reinforcement learning. Central to our approach is DianJin-R1-Data, a high-quality dataset constructed from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance Check, CCC), combining diverse financial reasoning scenarios with verified annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that generates both reasoning steps and final answers. To further refine reasoning quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement learning method that incorporates dual reward signals: one encouraging structured outputs and another rewarding answer correctness. We evaluate our models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental results show that DianJin-R1 models consistently outperform their non-reasoning counterparts, especially on complex financial tasks. Moreover, on the real-world CCC dataset, our single-call reasoning models match or even surpass the performance of multi-agent systems that require significantly more computational cost. These findings demonstrate the effectiveness of DianJin-R1 in enhancing financial reasoning through structured supervision and reward-aligned learning, offering a scalable and practical solution for real-world applications.",
            "score": 5,
            "issue_id": 3463,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "89be69a3fc0614a5",
            "authors": [
                "Jie Zhu",
                "Qian Chen",
                "Huaixia Dou",
                "Junhui Li",
                "Lifan Guo",
                "Feng Chen",
                "Chi Zhang"
            ],
            "affiliations": [
                "Qwen DianJin Team, Alibaba Cloud Computing",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15716.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "💹",
                "ru": {
                    "title": "Усиление финансовых рассуждений ИИ через структурированное обучение",
                    "desc": "DianJin-R1 - это фреймворк для улучшения рассуждений больших языковых моделей в финансовой сфере. Он использует обучение с подкреплением и специально созданный датасет DianJin-R1-Data для тренировки моделей. Модели DianJin-R1-7B и DianJin-R1-32B генерируют как шаги рассуждений, так и итоговые ответы в структурированном формате. Эксперименты показывают, что эти модели превосходят аналоги без рассуждений, особенно на сложных финансовых задачах."
                },
                "en": {
                    "title": "Enhancing Financial Reasoning with DianJin-R1",
                    "desc": "This paper introduces DianJin-R1, a framework aimed at improving reasoning capabilities in large language models (LLMs) specifically for financial tasks. It utilizes a unique dataset, DianJin-R1-Data, which combines various financial reasoning scenarios with verified annotations to enhance model training. The models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned using structured formats that generate reasoning steps alongside final answers, and they leverage Group Relative Policy Optimization (GRPO) for better output quality. Results show that these models outperform traditional LLMs on complex financial tasks, demonstrating their effectiveness in real-world applications."
                },
                "zh": {
                    "title": "DianJin-R1：提升金融推理的有效解决方案",
                    "desc": "本文提出了一种名为DianJin-R1的推理增强框架，旨在解决大型语言模型在金融领域面临的推理挑战。该框架通过推理增强监督和强化学习来提高模型的推理能力，使用了高质量的数据集DianJin-R1-Data，结合了多种金融推理场景。我们对DianJin-R1-7B和DianJin-R1-32B模型进行了微调，采用结构化格式生成推理步骤和最终答案，并应用了双重奖励信号的强化学习方法来优化推理质量。实验结果表明，DianJin-R1模型在复杂金融任务上表现优于非推理模型，展示了其在实际应用中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12080",
            "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
            "url": "https://huggingface.co/papers/2504.12080",
            "abstract": "Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model's generalization ability and has been applied to various vision tasks, including scene understanding and image/video editing. While recent Segment Anything Models have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAM's prompt encoder in segmentation by providing high-quality visual prompts. When generating a mask prior, we fuse the SAM features to better align the prompt encoder. Then, we design a cycle-consistent cross-attention on fused features and initial visual prompts. Next, a dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design a simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on PASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our source code and benchmark are available at https://github.com/zaplm/DC-SAM.",
            "score": 5,
            "issue_id": 3459,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "e91663786d7f873e",
            "authors": [
                "Mengshi Qi",
                "Pengfei Zhu",
                "Xiangtai Li",
                "Xiaoyang Bi",
                "Lu Qi",
                "Huadong Ma",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore",
                "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China",
                "UC Merced, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12080.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#transfer_learning",
                    "#benchmark",
                    "#cv",
                    "#games"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "DC-SAM: Продвинутая сегментация в контексте для изображений и видео",
                    "desc": "Эта статья представляет метод Dual Consistency SAM (DC-SAM) для адаптации моделей SAM и SAM2 к задаче сегментации в контексте как для изображений, так и для видео. Авторы предлагают улучшить функции энкодера подсказок SAM, используя высококачественные визуальные подсказки и циклически согласованное кросс-внимание. Метод включает двухветвевую архитектуру с дискриминативными положительными и отрицательными подсказками, а также стратегию обучения mask-tube для видео. Эксперименты показывают улучшение результатов на нескольких бенчмарках, включая новый набор данных IC-VOS для оценки способности модели к сегментации видео в контексте."
                },
                "en": {
                    "title": "Enhancing One-Shot Segmentation with Dual Consistency",
                    "desc": "This paper introduces the Dual Consistency SAM (DC-SAM) method for one-shot segmentation, which allows a model to segment objects using just one labeled example. It enhances the Segment Anything Model (SAM) by improving the prompt encoder's features through high-quality visual prompts and a cycle-consistent cross-attention mechanism. The authors also present a new benchmark for in-context video object segmentation, named IC-VOS, to evaluate their method's performance in video tasks. Experimental results show that DC-SAM outperforms existing models on several datasets, demonstrating its effectiveness in both image and video segmentation tasks."
                },
                "zh": {
                    "title": "双一致性SAM：提升上下文分割能力的创新方法",
                    "desc": "本文提出了一种名为双一致性SAM（DC-SAM）的方法，旨在解决单个标记示例下的上下文分割问题。该方法通过提示调优来适应现有的分割模型，特别是针对图像和视频的上下文分割。我们通过提供高质量的视觉提示来增强分割模型的特征，并设计了循环一致的交叉注意机制来优化提示编码器。实验结果表明，DC-SAM在多个数据集上表现出色，展示了其在上下文分割任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17816",
            "title": "Subject-driven Video Generation via Disentangled Identity and Motion",
            "url": "https://huggingface.co/papers/2504.17816",
            "abstract": "We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, which are computationally expensive and require extensive annotation. In contrast to the previous approach, we introduce the use of an image customization dataset directly on training video customization models, factorizing the video customization into two folds: (1) identity injection through image customization dataset and (2) temporal modeling preservation with a small set of unannotated videos through the image-to-video training method. Additionally, we employ random image token dropping with randomized image initialization during image-to-video fine-tuning to mitigate the copy-and-paste issue. To further enhance learning, we introduce stochastic switching during joint optimization of subject-specific and temporal features, mitigating catastrophic forgetting. Our method achieves strong subject consistency and scalability, outperforming existing video customization models in zero-shot settings, demonstrating the effectiveness of our framework.",
            "score": 4,
            "issue_id": 3463,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "01b4ca69c55b44f4",
            "authors": [
                "Daneul Kim",
                "Jingxu Zhang",
                "Wonjoon Jin",
                "Sunghyun Cho",
                "Qi Dai",
                "Jaesik Park",
                "Chong Luo"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "POSTECH",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17816.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Эффективная персонализация видео без дополнительной настройки",
                    "desc": "Исследователи предлагают новый подход к созданию персонализированных видео с использованием машинного обучения. Их метод разделяет обучение специфике субъекта и временной динамике, используя наборы данных изображений вместо видео. Они применяют случайное удаление токенов изображений и стохастическое переключение для улучшения обучения. Этот подход превосходит существующие модели персонализации видео в условиях нулевого обучения."
                },
                "en": {
                    "title": "Zero-Shot Video Customization through Subject-Driven Learning",
                    "desc": "This paper presents a novel approach to video generation that separates the learning of subject-specific features from the temporal dynamics, allowing for zero-shot customization without the need for additional tuning. Instead of relying on large annotated video datasets, the authors utilize an image customization dataset to inject identity into the video generation process. They also preserve temporal modeling by using a small set of unannotated videos, employing an image-to-video training method. The introduction of techniques like random image token dropping and stochastic switching during optimization helps improve subject consistency and scalability, leading to superior performance compared to existing models."
                },
                "zh": {
                    "title": "无调优的主题驱动视频生成新方法",
                    "desc": "我们提出了一种通过解耦主题特定学习与时间动态的方式，训练主题驱动的定制视频生成模型，且无需额外调优。传统的视频定制方法通常依赖于大型标注视频数据集，这不仅计算成本高，而且需要大量的标注工作。与之前的方法不同，我们直接在训练视频定制模型时使用图像定制数据集，将视频定制分为两个部分：通过图像定制数据集进行身份注入，以及通过图像到视频的训练方法保留时间建模。我们的模型在零样本设置下表现出强大的主题一致性和可扩展性，超越了现有的视频定制模型，证明了我们框架的有效性。"
                }
            }
        }
    ],
    "link_prev": "2025-04-25.html",
    "link_next": "2025-04-29.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "25.04",
        "en": "04/25",
        "zh": "4月25日"
    },
    "short_date_next": {
        "ru": "29.04",
        "en": "04/29",
        "zh": "4月29日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了 CameraBench，一个大规模的数据集和基准，旨在评估和改进摄像机运动理解。CameraBench 包含约 3,000 个多样化的网络视频，由专家通过严格的多阶段质量控制过程进行标注。我们与摄影师合作，设计了摄像机运动基元的分类法。例如，某些运动如“跟随”需要理解场景内容，如移动的主体。我们进行了大规模的人类研究，以量化人类标注性能，发现领域专业知识和基于教程的培训可显著提高准确性。使用 CameraBench，我们评估了结构从运动（SfM）和视频语言模型（VLMs），发现 SfM 模型难以捕捉依赖场景内容的语义基元，而 VLMs 难以捕捉需要精确轨迹估计的几何基元。我们随后在 CameraBench 上微调了一个生成 VLM，以实现两者的优势结合，并展示其应用，包括运动增强的字幕、视频问答和视频文本检索。我们希望我们的分类法、基准和教程能推动未来努力，实现理解任何视频中摄像机运动的最终目标。",
        "title": "Towards Understanding Camera Motions in Any Video",
        "pinyin": "Wǒmen jièshào le CameraBench, yīgè dàguīmó de shùjùjí hé jīzhǔn, zhǐyǐn pínggǔ hé gǎijìn shèxiàngjī yùndòng lǐjiě. CameraBench bāohán yuē 3,000 gè duōyànghuà de wǎngluò shìpǐn, yóu zhuānjiā tōngguò yánzhòng de duō jiēduàn zhìliàng kòngzhì guòchéng jìnxiàng biāozhù. Wǒmen yǔ shèyǐngshī hézuò, shèjì le shèxiàngjī yùndòng jīyuǎn de fēnlèi fǎ. Lìrú, mǒuxiē yùndòng rú “gēnsuí” xūyào lǐjiě chǎngjīng nèiróng, rú yídòng de zhǔtǐ. Wǒmen jìnxíng le dàguīmó de rénlèi yánjiū, yǐ liàngzhì rénlèi biāozhù xìngnéng, fāxiàn lǐngyù zhuānjì zhīshi hé jīyú jiàoxué de péixùn kě xiǎnzhù tīgāo zhùnquèxìng. Shǐyòng CameraBench, wǒmen pínggǔ le jiégòu cóng yùndòng (SfM) hé shìpǐn yǔyán móxíng (VLMs), fāxiàn SfM móxíng nán yǐ bǔzhòu yīlài chǎngjīng nèiróng de yǔyán jīyuǎn, ér VLMs nán yǐ bǔzhòu xūyào jīngquè guǐjì gūjì de jǐhé jīyuǎn. Wǒmen shùhòu zài CameraBench shàng wēitiáo le yīgè shēngchéng VLM, yǐ shíxiàn liǎng zhě de yōushì jiēhé, bìng zhǎnshì qí yìngyòng, bāokuò yùndòng zēngqiáng de zìmǔ, shìpǐn wèndá hé shìpǐn wénběn cháxún. Wǒmen xīwàng wǒmen de fēnlèi fǎ, jīzhǔn hé jiàoxué néng tuīdòng wèilái nǔlì, shíxiàn lǐjiě rènhé shìpǐn zhōng shèxiàngjī yùndòng de zhòngdiǎn mùbiāo.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improve\"},\n    {\"word\": \"摄像机\", \"pinyin\": \"shè xiàng jī\", \"trans\": \"camera\"},\n    {\"word\": \"运动\", \"pinyin\": \"yùn dòng\", \"trans\": \"motion\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understand\"},\n    {\"word\": \"多样化\", \"pinyin\": \"duō yàng huà\", \"trans\": \"diverse\"},\n    {\"word\": \"网络视频\", \"pinyin\": \"wǎng luò shì pín\", \"trans\": \"online video\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuān jiā\", \"trans\": \"expert\"},\n    {\"word\": \"严格\", \"pinyin\": \"yán gé\", \"trans\": \"strict\"},\n    {\"word\": \"多阶段\", \"pinyin\": \"duō jiē duàn\", \"trans\": \"multi-stage\"},\n    {\"word\": \"质量控制\", \"pinyin\": \"zhì liàng kòng zhì\", \"trans\": \"quality control\"},\n    {\"word\": \"过程\", \"pinyin\": \"guò chéng\", \"trans\": \"process\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotate\"},\n    {\"word\": \"摄影师\", \"pinyin\": \"shè yǐng shī\", \"trans\": \"photographer\"},\n    {\"word\": \"设计\", \"pinyin\": \"shè jì\", \"trans\": \"design\"},\n    {\"word\": \"基元\", \"pinyin\": \"jī yuán\", \"trans\": \"primitive\"},\n    {\"word\": \"分类法\", \"pinyin\": \"fēn lèi fǎ\", \"trans\": \"classification method\"},\n    {\"word\": \"例如\", \"pinyin\": \"lì rú\", \"trans\": \"for example\"},\n    {\"word\": \"某些\", \"pinyin\": \"mǒu xiē\", \"trans\": \"some\"},\n    {\"word\": \"跟随\", \"pinyin\": \"gēn suí\", \"trans\": \"follow\"},\n    {\"word\": \"需要\", \"pinyin\": \"xū yào\", \"trans\": \"need\"},\n    {\"word\": \"场景\", \"pinyin\": \"chǎng jǐng\", \"trans\": \"scene\"},\n    {\"word\": \"内容\", \"pinyin\": \"nèi róng\", \"trans\": \"content\"},\n    {\"word\": \"主体\", \"pinyin\": \"zhǔ tǐ\", \"trans\": \"subject\"},\n    {\"word\": \"移动\", \"pinyin\": \"yí dòng\", \"trans\": \"move\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"study\"},\n    {\"word\": \"量化\", \"pinyin\": \"liàng huà\", \"trans\": \"quantify\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"field\"},\n    {\"word\": \"专业知识\", \"pinyin\": \"zhuān yè zhī shi\", \"trans\": \"professional knowledge\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"教程\", \"pinyin\": \"jiào chéng\", \"trans\": \"tutorial\"},\n    {\"word\": \"培训\", \"pinyin\": \"péi xùn\", \"trans\": \"training\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"准确性\", \"pinyin\": \"zhǔn què xìng\", \"trans\": \"accuracy\"},\n    {\"word\": \"结构从运动\", \"pinyin\": \"jié gòu cóng yùn dòng\", \"trans\": \"Structure from Motion (SfM)\"},\n    {\"word\": \"视频语言模型\", \"pinyin\": \"shì pín yǔ yán mó xíng\", \"trans\": \"Video Language Model (VLM)\"},\n    {\"word\": \"捕捉\", \"pinyin\": \"bǔ zhuō\", \"trans\": \"capture\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantic\"},\n    {\"word\": \"几何\", \"pinyin\": \"jǐ hé\", \"trans\": \"geometric\"},\n    {\"word\": \"轨迹\", \"pinyin\": \"guǐ jī\", \"trans\": \"trajectory\"},\n    {\"word\": \"估计\", \"pinyin\": \"gū jì\", \"trans\": \"estimate\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tune\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"优势\", \"pinyin\": \"yōu shì\", \"trans\": \"advantage\"},\n    {\"word\": \"结合\", \"pinyin\": \"jié hé\", \"trans\": \"combine\"},\n    {\"word\": \"展示\", \"pinyin\": \"zhǎn shì\", \"trans\": \"demonstrate\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"包括\", \"pinyin\": \"bāo kuò\", \"trans\": \"include\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēng qiáng\", \"trans\": \"enhance\"},\n    {\"word\": \"字幕\", \"pinyin\": \"zì mù\", \"trans\": \"subtitle\"},\n    {\"word\": \"问答\", \"pinyin\": \"wèn dá\", \"trans\": \"question and answer\"},\n    {\"word\": \"文本检索\", \"pinyin\": \"wén běn jiǎn suǒ\", \"trans\": \"text retrieval\"},\n    {\"word\": \"希望\", \"pinyin\": \"xī wàng\", \"trans\": \"hope\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuī dòng\", \"trans\": \"promote\"},\n    {\"word\": \"未来\", \"pinyin\": \"wèi lái\", \"trans\": \"future\"},\n    {\"word\": \"努力\", \"pinyin\": \"nǔ lì\", \"trans\": \"effort\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"最终\", \"pinyin\": \"zuì zhōng\", \"trans\": \"ultimate\"},\n    {\"word\": \"目标\", \"pinyin\": \"mù biāo\", \"trans\": \"goal\"}\n]",
        "trans": "We introduce CameraBench, a large-scale dataset and benchmark aimed at evaluating and improving the understanding of camera motion. CameraBench contains approximately 3,000 diverse web videos, annotated by experts through a rigorous multi-stage quality control process. We collaborated with photographers to design a classification scheme for camera motion primitives. For instance, certain motions like \"following\" require an understanding of scene content, such as moving subjects. We conducted large-scale human studies to quantify human annotation performance and found that domain expertise and tutorial-based training can significantly improve accuracy. Using CameraBench, we evaluated Structure-from-Motion (SfM) and Video Language Models (VLMs), finding that SfM models struggle to capture semantic primitives dependent on scene content, while VLMs struggle with geometric primitives that require precise trajectory estimation. Subsequently, we fine-tuned a generative VLM on CameraBench to combine the strengths of both, demonstrating its applications, including motion-enhanced captioning, video question answering, and video-text retrieval. We hope that our classification scheme, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motion in any video.",
        "update_ts": "2025-04-28 10:45"
    }
}