{
    "date": {
        "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 30",
        "zh": "10æœˆ30æ—¥"
    },
    "time_utc": "2025-10-30 11:10",
    "weekday": 3,
    "issue_id": 6699,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.23538",
            "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence",
            "url": "https://huggingface.co/papers/2510.23538",
            "abstract": "A unified multimodal code corpus and model, JanusCoder, generate code from text and visual inputs, outperforming commercial models in various coding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.",
            "score": 60,
            "issue_id": 6691,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 27",
                "zh": "10æœˆ27æ—¥"
            },
            "hash": "39b54fb66d7bcf3a",
            "authors": [
                "Qiushi Sun",
                "Jingyang Gong",
                "Yang Liu",
                "Qiaosheng Chen",
                "Lei Li",
                "Kai Chen",
                "Qipeng Guo",
                "Ben Kao",
                "Fei Yuan"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23538.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#dataset",
                    "#multimodal",
                    "#data",
                    "#games",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "JanusCoder: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ JanusCoder â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ JanusCode-800K â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ½Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½ÑÑˆĞ½Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, JanusCoder Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 7B Ğ´Ğ¾ 14B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Code Generation with Multimodal Intelligence",
                    "desc": "JanusCoder is a unified multimodal model designed to generate code from both text and visual inputs, significantly enhancing the capabilities of neural code intelligence. It addresses the challenge of limited multimodal code data by introducing a comprehensive synthesis toolkit that creates a large-scale, high-quality code corpus, JanusCode-800K. This model outperforms existing commercial models in various coding tasks by integrating visual and programmatic elements, allowing for more flexible content generation and precise editing. The research highlights the importance of harmonizing programmatic logic with visual representation, paving the way for advanced applications in coding and visualization."
                },
                "zh": {
                    "title": "JanusCoderï¼šæ–‡æœ¬ä¸è§†è§‰è¾“å…¥çš„ç»Ÿä¸€ä»£ç ç”Ÿæˆæ¨¡å‹",
                    "desc": "JanusCoderæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ç”Ÿæˆä»£ç ã€‚å®ƒå…‹æœäº†é«˜è´¨é‡å¤šæ¨¡æ€ä»£ç æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«80ä¸‡æ¡ä»£ç çš„å¤šæ¨¡æ€ä»£ç åº“ã€‚è¯¥æ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰ç¼–ç ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šå•†ä¸šæ¨¡å‹ã€‚é€šè¿‡åˆ†æï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ç¨‹åºé€»è¾‘ä¸è§†è§‰è¡¨è¾¾ä¹‹é—´çš„åè°ƒè§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.23473",
            "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2510.23473",
            "abstract": "Video-Thinker, a multimodal large language model, autonomously reasons with videos using intrinsic grounding and captioning capabilities, achieving state-of-the-art performance on various video reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in image reasoning methods, particularly \"Thinking with Images\", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.",
            "score": 59,
            "issue_id": 6690,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 27",
                "zh": "10æœˆ27æ—¥"
            },
            "hash": "448c0141074addb0",
            "authors": [
                "Shijian Wang",
                "Jiarui Jin",
                "Xingjian Wang",
                "Linxin Song",
                "Runhao Fu",
                "Hecheng Wang",
                "Zongyuan Ge",
                "Yuan Lu",
                "Xuelian Cheng"
            ],
            "affiliations": [
                "Fudan University",
                "Monash University",
                "Southeast University",
                "University of Southern California",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23473.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#games",
                    "#dataset",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ LLM Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ",
                    "desc": "Video-Thinker â€” ÑÑ‚Ğ¾ multimodal LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ¼ĞµĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ grounding (Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²) Ğ¸ captioning (Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ inference, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Video-Thinker-10K Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ chain-of-thought Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ supervised fine-tuning Ğ¸ reinforcement learning Ñ‡ĞµÑ€ĞµĞ· GRPO. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ 7B Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ video reasoning."
                },
                "en": {
                    "title": "Empowering Video Reasoning with Autonomous Thinking",
                    "desc": "Video-Thinker is a multimodal large language model designed to enhance video reasoning by utilizing its intrinsic grounding and captioning abilities. It introduces a new dataset, Video-Thinker-10K, which supports autonomous reasoning through chain-of-thought sequences. The model is trained using Supervised Fine-Tuning followed by Group Relative Policy Optimization to improve its reasoning skills. As a result, Video-Thinker achieves superior performance on various video reasoning benchmarks, outperforming existing models and setting new standards in the field."
                },
                "zh": {
                    "title": "è§†é¢‘æ¨ç†çš„æ–°çªç ´ï¼šVideo-Thinker",
                    "desc": "Video-Thinkeræ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡å†…åœ¨çš„åŸºç¡€å’Œå­—å¹•èƒ½åŠ›è‡ªä¸»è¿›è¡Œè§†é¢‘æ¨ç†ã€‚è¯¥æ¨¡å‹åœ¨è§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æ„å»ºäº†Video-Thinker-10Kæ•°æ®é›†ï¼Œä»¥æ”¯æŒé“¾å¼æ€ç»´æ¨ç†åºåˆ—ä¸­çš„è‡ªä¸»å·¥å…·ä½¿ç”¨ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„è®­ç»ƒç­–ç•¥ï¼ŒVideo-Thinkerèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è§†é¢‘æ¨ç†ä»»åŠ¡ï¼Œæå‡äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24592",
            "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
            "url": "https://huggingface.co/papers/2510.24592",
            "abstract": "ReForm, a reflective autoformalization method, improves the semantic accuracy of formal statements generated from natural language mathematics through iterative refinement and semantic consistency evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoformalization, which translates natural language mathematics into machine-verifiable formal statements, is critical for using formal mathematical reasoning to solve math problems stated in natural language. While Large Language Models can generate syntactically correct formal statements, they often fail to preserve the original problem's semantic intent. This limitation arises from the LLM approaches' treating autoformalization as a simplistic translation task which lacks mechanisms for self-reflection and iterative refinement that human experts naturally employ. To address these issues, we propose ReForm, a Reflective Autoformalization method that tightly integrates semantic consistency evaluation into the autoformalization process. This enables the model to iteratively generate formal statements, assess its semantic fidelity, and self-correct identified errors through progressive refinement. To effectively train this reflective model, we introduce Prospective Bounded Sequence Optimization (PBSO), which employs different rewards at different sequence positions to ensure that the model develops both accurate autoformalization and correct semantic validations, preventing superficial critiques that would undermine the purpose of reflection. Extensive experiments across four autoformalization benchmarks demonstrate that ReForm achieves an average improvement of 17.2 percentage points over the strongest baselines. To further ensure evaluation reliability, we introduce ConsistencyCheck, a benchmark of 859 expert-annotated items that not only validates LLMs as judges but also reveals that autoformalization is inherently difficult: even human experts produce semantic errors in up to 38.5% of cases.",
            "score": 46,
            "issue_id": 6690,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "5f2d10bba8cf180d",
            "authors": [
                "Guoxin Chen",
                "Jing Wu",
                "Xinjie Chen",
                "Wayne Xin Zhao",
                "Ruihua Song",
                "Chengxi Li",
                "Kai Fan",
                "Dayiheng Liu",
                "Minpeng Liao"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Tongyi Lab, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24592.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#math",
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ReForm â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LLM Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ, Ğ½Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ReForm Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ PBSO. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 17.2 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ğ¶Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹-Ğ»ÑĞ´Ğ¸ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² 38.5% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²."
                },
                "en": {
                    "title": "ReForm: Enhancing Semantic Accuracy in Autoformalization",
                    "desc": "ReForm is a method designed to enhance the accuracy of converting natural language mathematics into formal statements that machines can verify. It addresses the common issue where Large Language Models (LLMs) generate statements that are syntactically correct but semantically flawed. By incorporating a process of iterative refinement and semantic consistency evaluation, ReForm allows the model to self-correct and improve its outputs. The method is trained using Prospective Bounded Sequence Optimization (PBSO), which helps ensure that the model not only produces accurate formalizations but also validates their semantic correctness."
                },
                "zh": {
                    "title": "åæ€æ€§è‡ªåŠ¨å½¢å¼åŒ–ï¼šæå‡æ•°å­¦è¯­ä¹‰å‡†ç¡®æ€§",
                    "desc": "ReFormæ˜¯ä¸€ç§åæ€æ€§è‡ªåŠ¨å½¢å¼åŒ–æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œè¯­ä¹‰ä¸€è‡´æ€§è¯„ä¼°ï¼Œæé«˜ä»è‡ªç„¶è¯­è¨€æ•°å­¦ç”Ÿæˆçš„å½¢å¼è¯­å¥çš„è¯­ä¹‰å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå½¢å¼è¯­å¥æ—¶ï¼Œè™½ç„¶è¯­æ³•æ­£ç¡®ï¼Œä½†å¸¸å¸¸æ— æ³•ä¿ç•™åŸé—®é¢˜çš„è¯­ä¹‰æ„å›¾ã€‚ReFormé€šè¿‡å°†è¯­ä¹‰ä¸€è‡´æ€§è¯„ä¼°ç´§å¯†é›†æˆåˆ°è‡ªåŠ¨å½¢å¼åŒ–è¿‡ç¨‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿­ä»£ç”Ÿæˆå½¢å¼è¯­å¥ï¼Œå¹¶è‡ªæˆ‘çº æ­£è¯†åˆ«å‡ºçš„é”™è¯¯ã€‚é€šè¿‡å¼•å…¥å‰ç»æ€§æœ‰ç•Œåºåˆ—ä¼˜åŒ–ï¼ˆPBSOï¼‰ï¼ŒReFormç¡®ä¿æ¨¡å‹åœ¨ä¸åŒåºåˆ—ä½ç½®è·å¾—ä¸åŒçš„å¥–åŠ±ï¼Œä»è€Œæœ‰æ•ˆè®­ç»ƒå‡ºå‡†ç¡®çš„è‡ªåŠ¨å½¢å¼åŒ–å’Œæ­£ç¡®çš„è¯­ä¹‰éªŒè¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25741",
            "title": "Scaling Latent Reasoning via Looped Language Models",
            "url": "https://huggingface.co/papers/2510.25741",
            "abstract": "Modern LLMs are trained to \"think\" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model could be found in: http://ouro-llm.github.io.",
            "score": 41,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "33772a5a18bf991a",
            "authors": [
                "Rui-Jie Zhu",
                "Zixuan Wang",
                "Kai Hua",
                "Tianyu Zhang",
                "Ziniu Li",
                "Haoran Que",
                "Boyi Wei",
                "Zixin Wen",
                "Fan Yin",
                "He Xing",
                "Lu Li",
                "Jiajun Shi",
                "Kaijing Ma",
                "Shanda Li",
                "Taylor Kergan",
                "Andrew Smith",
                "Xingwei Qu",
                "Mude Hui",
                "Bohong Wu",
                "Qiyang Min",
                "Hongzhi Huang",
                "Xun Zhou",
                "Wei Ye",
                "Jiaheng Liu",
                "Jian Yang",
                "Yunfeng Shi",
                "Chenghua Lin",
                "Enduo Zhao",
                "Tianle Cai",
                "Ge Zhang",
                "Wenhao Huang",
                "Yoshua Bengio",
                "Jason Eshraghian"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Carnegie Mellon University",
                "Conscium",
                "M-A-P Core Contributors",
                "Mila - Quebec AI Institute",
                "Peking University",
                "Princeton University",
                "UC Santa Cruz",
                "University of Manchester",
                "University of Montreal",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25741.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ouro - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Looped Language Models (LoopLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° chain-of-thought Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° 7.7 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ouro Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1.4B Ğ¸ 2.6B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ´Ğ¾ 12B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğµ Ğ² ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ° Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹."
                },
                "en": {
                    "title": "Ouro: Enhancing Reasoning in Language Models Through Pre-Training",
                    "desc": "This paper introduces Ouro, a new type of pre-trained language model called Looped Language Models (LoopLM) that enhances reasoning during the pre-training phase rather than relying solely on post-training text generation. Ouro incorporates iterative computation in latent space and uses an entropy-regularized objective to optimize how depth is allocated in reasoning tasks. The models, with sizes of 1.4B and 2.6B parameters, demonstrate performance comparable to larger state-of-the-art models, achieving better knowledge manipulation rather than just increased capacity. The findings suggest that LoopLM can provide reasoning traces that are more closely aligned with the final outputs compared to traditional chain-of-thought methods, indicating a promising new direction for scaling reasoning in language models."
                },
                "zh": {
                    "title": "å¾ªç¯è¯­è¨€æ¨¡å‹ï¼šæ¨ç†çš„æ–°æ–¹å‘",
                    "desc": "ç°ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦é€šè¿‡æ˜¾å¼æ–‡æœ¬ç”Ÿæˆè¿›è¡Œâ€œæ€è€ƒâ€ï¼Œå¦‚é“¾å¼æ¨ç†ï¼ˆCoTï¼‰ï¼Œè¿™ä½¿å¾—æ¨ç†è¿‡ç¨‹è¢«æ¨è¿Ÿåˆ°è®­ç»ƒåï¼Œå¹¶æœªå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æå‡ºå¹¶å¼€æºäº†Ouroï¼Œè¿™æ˜¯ä¸€ç§é¢„è®­ç»ƒçš„å¾ªç¯è¯­è¨€æ¨¡å‹ï¼ˆLoopLMï¼‰ï¼Œå®ƒé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„è¿­ä»£è®¡ç®—ã€ç†µæ­£åˆ™åŒ–ç›®æ ‡å’Œæ‰©å±•åˆ°7.7ä¸‡äº¿ä¸ªæ ‡è®°ï¼Œå°†æ¨ç†æ„å»ºåˆ°é¢„è®­ç»ƒé˜¶æ®µã€‚Ouro 1.4Bå’Œ2.6Bæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œèƒ½å¤Ÿä¸é«˜è¾¾12Bçš„æœ€å…ˆè¿›LLMçš„ç»“æœç›¸åŒ¹é…ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§ä¼˜åŠ¿å¹¶éæ¥è‡ªçŸ¥è¯†å®¹é‡çš„å¢åŠ ï¼Œè€Œæ˜¯æ¥è‡ªæ›´ä¼˜çš„çŸ¥è¯†æ“ä½œèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25065",
            "title": "Reasoning-Aware GRPO using Process Mining",
            "url": "https://huggingface.co/papers/2510.25065",
            "abstract": "Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer/format rewards with signals over the reasoning procedure. To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model's reasoning aligns with the pretrained teacher model. The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms existing methodologies for GRPO-based post-training. These results highlight that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models.",
            "score": 29,
            "issue_id": 6692,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "da09c74825197283",
            "authors": [
                "Taekhyun Park",
                "Yongjae Lee",
                "Hyerim Bae"
            ],
            "affiliations": [
                "Dept. of Data Science Pusan National University Busan, Republic of Korea",
                "Dept. of Industrial Engineering Pusan National University Busan, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25065.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PM4GRPO - ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ process mining Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ (conformance reward), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Group Relative Policy Optimization (GRPO) Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Reasoning in Models with PM4GRPO",
                    "desc": "This paper introduces PM4GRPO, a novel approach to reinforcement learning that enhances multi-step reasoning in large reasoning models. Unlike traditional reward systems that focus solely on the final outcome, PM4GRPO incorporates reasoning-aware rewards that evaluate the reasoning process itself. By using process mining techniques, the method calculates a conformance reward that assesses how well a model's reasoning matches that of a pretrained teacher model. The results show that PM4GRPO outperforms existing GRPO-based post-training methods across multiple benchmarks, demonstrating its effectiveness in improving reasoning capabilities."
                },
                "zh": {
                    "title": "æ¨ç†æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹æ³•PM4GRPOï¼Œæ—¨åœ¨æå‡å¤§å‹æ¨ç†æ¨¡å‹çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ä»¥ç»“æœä¸ºä¸­å¿ƒçš„å¥–åŠ±æœºåˆ¶ä¸åŒï¼ŒPM4GRPOå¼•å…¥äº†å¯¹æ¨ç†è¿‡ç¨‹çš„å…³æ³¨ï¼Œé€šè¿‡è¿‡ç¨‹æŒ–æ˜æŠ€æœ¯è®¡ç®—å‡ºä¸€ä¸ªæ ‡é‡ä¸€è‡´æ€§å¥–åŠ±ï¼Œè¯„ä¼°ç­–ç•¥æ¨¡å‹çš„æ¨ç†ä¸é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒè¯æ˜ï¼ŒPM4GRPOåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„GRPOåè®­ç»ƒæ–¹æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨è¿‡ç¨‹æŒ–æ˜æŠ€æœ¯çš„æ¨ç†æ„ŸçŸ¥GRPOèƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºç­–ç•¥æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25772",
            "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning",
            "url": "https://huggingface.co/papers/2510.25772",
            "abstract": "Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community.",
            "score": 24,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "b29b7917819d7178",
            "authors": [
                "Baolu Li",
                "Yiming Zhang",
                "Qinghe Wang",
                "Liqian Ma",
                "Xiaoyu Shi",
                "Xintao Wang",
                "Pengfei Wan",
                "Zhenfei Yin",
                "Yunzhi Zhuge",
                "Huchuan Lu",
                "Xu Jia"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Kling Team, Kuaishou Technology",
                "Oxford University",
                "ZMO AI Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25772.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#games",
                    "#dataset",
                    "#video"
                ],
                "emoji": "âœ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ VFXMaster â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ LoRA Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ in-context learning, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ°ÑĞºĞ¾Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ one-shot Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "VFXMaster: Unifying Visual Effects Generation with In-Context Learning",
                    "desc": "This paper presents VFXMaster, a novel framework for generating visual effects (VFX) in videos using generative AI. Unlike traditional methods that require separate models for each effect, VFXMaster employs a unified approach that leverages in-context learning to adapt effects from reference videos to new content. The framework includes an innovative attention mask to isolate and apply specific effect attributes, enhancing its ability to generalize to previously unseen effects. The authors also introduce a one-shot adaptation mechanism, allowing rapid learning from a single example, which significantly improves the model's versatility and efficiency in VFX creation."
                },
                "zh": {
                    "title": "VFXMasterï¼šç»Ÿä¸€çš„è§†è§‰ç‰¹æ•ˆç”Ÿæˆæ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVFXMasterçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆè§†è§‰ç‰¹æ•ˆï¼ˆVFXï¼‰è§†é¢‘ã€‚è¯¥æ–¹æ³•å°†ç‰¹æ•ˆç”Ÿæˆè§†ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ï¼Œèƒ½å¤Ÿä»å‚è€ƒè§†é¢‘ä¸­å¤åˆ¶å¤šæ ·çš„åŠ¨æ€ç‰¹æ•ˆåˆ°ç›®æ ‡å†…å®¹ä¸Šã€‚VFXMasterå±•ç¤ºäº†å¯¹æœªè§ç‰¹æ•ˆç±»åˆ«çš„æ˜¾è‘—æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä¸Šä¸‹æ–‡æ¡ä»¶ç­–ç•¥ï¼Œä»¥ç²¾ç¡®è§£è€¦å’Œæ³¨å…¥ç‰¹æ•ˆå±æ€§ã€‚é€šè¿‡é«˜æ•ˆçš„ä¸€æ¬¡æ€§ç‰¹æ•ˆé€‚åº”æœºåˆ¶ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¿«é€Ÿæå‡å¯¹éš¾ä»¥è§åˆ°çš„ç‰¹æ•ˆçš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25726",
            "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,\n  and Long-Horizon Task Execution",
            "url": "https://huggingface.co/papers/2510.25726",
            "abstract": "Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.",
            "score": 24,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "ee2a3f9cd44f7f99",
            "authors": [
                "Junlong Li",
                "Wenshuo Zhao",
                "Jian Zhao",
                "Weihao Zeng",
                "Haoze Wu",
                "Xiaochen Wang",
                "Rui Ge",
                "Yuxuan Cao",
                "Yuzhen Huang",
                "Wei Liu",
                "Junteng Liu",
                "Zhaochen Su",
                "Yiyang Guo",
                "Fan Zhou",
                "Lueyang Zhang",
                "Juan Michelini",
                "Xingyao Wang",
                "Xiang Yue",
                "Shuyan Zhou",
                "Graham Neubig",
                "Junxian He"
            ],
            "affiliations": [
                "All Hands AI",
                "Carnegie Mellon University",
                "Duke University",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25726.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#benchmark",
                    "#agi"
                ],
                "emoji": "ğŸ…",
                "ru": {
                    "title": "Ğ”ĞµÑÑÑ‚Ğ¸Ğ±Ğ¾Ñ€ÑŒĞµ Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Toolathlon â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 32 Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ (Ğ¾Ñ‚ Google Calendar Ğ´Ğ¾ Kubernetes), 604 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸ 108 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 20 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Toolathlon Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¾Ñ„Ñ‚Ğ° Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ›ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Claude-4.5-Sonnet Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 38.6% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Toolathlon: Elevating Language Agents for Real-World Challenges",
                    "desc": "This paper introduces Toolathlon, a new benchmark designed to evaluate language agents in complex, multi-step workflows across various applications. Unlike previous benchmarks that focus on narrow tasks, Toolathlon includes 32 software applications and 604 tools, providing a realistic environment for testing. The benchmark features 108 tasks that require agents to interact with multiple apps over an average of 20 turns, ensuring a comprehensive evaluation of their capabilities. Initial results show that current state-of-the-art models struggle with these tasks, highlighting the need for improved language agents that can handle real-world scenarios effectively."
                },
                "zh": {
                    "title": "Toolathlonï¼šè¯„ä¼°è¯­è¨€ä»£ç†çš„æ–°åŸºå‡†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç§°ä¸ºTool Decathlonï¼ˆToolathlonï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°è¯­è¨€ä»£ç†åœ¨å¤æ‚å¤šæ­¥éª¤å·¥ä½œæµä¸­çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ¶µç›–32ä¸ªè½¯ä»¶åº”ç”¨å’Œ604ä¸ªå·¥å…·ï¼Œæä¾›çœŸå®çš„ç¯å¢ƒè®¾ç½®å’Œå¯é çš„æ‰§è¡Œè¯„ä¼°ã€‚ä¸ä»¥å¾€çš„ç ”ç©¶ä¸åŒï¼ŒToolathlonæä¾›äº†å¤šæ ·åŒ–çš„ç¯å¢ƒçŠ¶æ€å’ŒçœŸå®çš„ä»»åŠ¡åœºæ™¯ï¼Œè¦æ±‚ä»£ç†ä¸å¤šä¸ªåº”ç”¨è¿›è¡Œäº¤äº’ã€‚é€šè¿‡å¯¹ç°æœ‰æ¨¡å‹çš„å…¨é¢è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼ŒToolathlonçš„æ¨å‡ºæœ‰æœ›æ¨åŠ¨æ›´å¼ºå¤§çš„è¯­è¨€ä»£ç†çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25590",
            "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
            "url": "https://huggingface.co/papers/2510.25590",
            "abstract": "Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.",
            "score": 21,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "a5455877a0fcfb11",
            "authors": [
                "Pengtao Chen",
                "Xianfang Zeng",
                "Maosen Zhao",
                "Mingzhu Shen",
                "Peng Ye",
                "Bangyin Xiang",
                "Zhibo Wang",
                "Wei Cheng",
                "Gang Yu",
                "Tao Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Imperial College London",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25590.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ RegionE â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸ Ğ½ĞµÑ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ: Ğ´Ğ»Ñ Ğ½ĞµÑ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. Ğ”Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Region-Instruction KV Cache Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºÑÑˆ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 2-2.5 Ñ€Ğ°Ğ·Ğ° Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ğ¸Ğ¿Ğ° FLUX.1 Ğ¸ Qwen-Image-Edit Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Accelerating Image Editing with Region-Aware Techniques",
                    "desc": "This paper introduces RegionE, a new framework for instruction-based image editing (IIE) that improves efficiency by recognizing the differences between edited and unedited regions of an image. It employs an adaptive region partitioning method to identify these areas, allowing for faster predictions in unedited regions while maintaining detailed processing in edited areas. The framework also includes a Region-Instruction KV Cache to enhance local iterative denoising and an adaptive velocity decay cache to speed up the process further. Overall, RegionE significantly accelerates IIE tasks without requiring additional training, achieving notable performance improvements on existing models."
                },
                "zh": {
                    "title": "åŒºåŸŸæ„ŸçŸ¥ï¼Œæå‡å›¾åƒç¼–è¾‘æ•ˆç‡",
                    "desc": "æœ€è¿‘ï¼ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰çš„IIEæ¨¡å‹åœ¨å¤„ç†å›¾åƒæ—¶æ²¡æœ‰åŒºåˆ†ç¼–è¾‘åŒºåŸŸå’Œæœªç¼–è¾‘åŒºåŸŸï¼Œå¯¼è‡´ç”Ÿæˆè¿‡ç¨‹æ•ˆç‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RegionEï¼Œä¸€ä¸ªè‡ªé€‚åº”çš„åŒºåŸŸæ„ŸçŸ¥ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥åŠ é€ŸIIEä»»åŠ¡è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚RegionEé€šè¿‡è‡ªé€‚åº”åŒºåŸŸåˆ’åˆ†ã€åŒºåŸŸæ„ŸçŸ¥ç”Ÿæˆå’Œè‡ªé€‚åº”é€Ÿåº¦è¡°å‡ç¼“å­˜ç­‰ç»„ä»¶ï¼Œæé«˜äº†ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18455",
            "title": "ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in\n  Game RAG Benchmarks",
            "url": "https://huggingface.co/papers/2510.18455",
            "abstract": "ChronoPlay is a framework for generating dynamic RAG benchmarks in gaming, addressing the challenges of game content updates and player focus shifts with a dual-dynamic update mechanism and dual-source synthesis engine.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval Augmented Generation (RAG) systems are increasingly vital in dynamic domains like online gaming, yet the lack of a dedicated benchmark has impeded standardized evaluation in this area. The core difficulty lies in Dual Dynamics: the constant interplay between game content updates and the shifting focus of the player community. Furthermore, the necessity of automating such a benchmark introduces a critical requirement for player-centric authenticity to ensure generated questions are realistic. To address this integrated challenge, we introduce ChronoPlay, a novel framework for the automated and continuous generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update mechanism to track both forms of change, and a dual-source synthesis engine that draws from official sources and player community to ensure both factual correctness and authentic query patterns. We instantiate our framework on three distinct games to create the first dynamic RAG benchmark for the gaming domain, offering new insights into model performance under these complex and realistic conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.",
            "score": 15,
            "issue_id": 6695,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "979e9a0cf24a4b19",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#optimization",
                    "#games"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ¸Ğ³Ñ€",
                    "desc": "ChronoPlay â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ: Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ²ÑƒÑ…Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "ChronoPlay: Dynamic RAG Benchmarks for Gaming Evolution",
                    "desc": "ChronoPlay is a new framework designed to create dynamic benchmarks for Retrieval Augmented Generation (RAG) systems in gaming. It addresses the challenges posed by frequent game content updates and the changing interests of players through a dual-dynamic update mechanism. The framework also employs a dual-source synthesis engine that combines official game information with insights from the player community to ensure the authenticity and relevance of generated queries. By applying ChronoPlay to three different games, the authors establish the first dynamic RAG benchmark, enhancing the evaluation of machine learning models in the gaming context."
                },
                "zh": {
                    "title": "ChronoPlayï¼šæ¸¸æˆåŠ¨æ€åŸºå‡†çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "ChronoPlayæ˜¯ä¸€ä¸ªç”¨äºç”ŸæˆåŠ¨æ€RAGåŸºå‡†çš„æ¡†æ¶ï¼Œä¸“æ³¨äºè§£å†³æ¸¸æˆå†…å®¹æ›´æ–°å’Œç©å®¶å…³æ³¨ç‚¹å˜åŒ–çš„æŒ‘æˆ˜ã€‚å®ƒé‡‡ç”¨åŒåŠ¨æ€æ›´æ–°æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶è·Ÿè¸ªæ¸¸æˆå†…å®¹å’Œç©å®¶ç¤¾åŒºçš„å˜åŒ–ã€‚è¯¥æ¡†æ¶è¿˜ä½¿ç”¨åŒæºåˆæˆå¼•æ“ï¼Œä»å®˜æ–¹æ¥æºå’Œç©å®¶ç¤¾åŒºè·å–ä¿¡æ¯ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„é—®é¢˜æ—¢çœŸå®åˆå‡†ç¡®ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªä¸åŒçš„æ¸¸æˆä¸­å®ä¾‹åŒ–è¯¥æ¡†æ¶ï¼ŒChronoPlayä¸ºæ¸¸æˆé¢†åŸŸæä¾›äº†é¦–ä¸ªåŠ¨æ€RAGåŸºå‡†ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¤æ‚å’ŒçœŸå®æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25760",
            "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks",
            "url": "https://huggingface.co/papers/2510.25760",
            "abstract": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
            "score": 10,
            "issue_id": 6694,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "cc33f023427ef51b",
            "authors": [
                "Xu Zheng",
                "Zihao Dongfang",
                "Lutao Jiang",
                "Boyuan Zheng",
                "Yulong Guo",
                "Zhenquan Zhang",
                "Giuliano Albanese",
                "Runyi Yang",
                "Mengjiao Ma",
                "Zixin Zhang",
                "Chenfei Liao",
                "Dingcheng Zhen",
                "Yuanhuiyi Lyu",
                "Yuqian Fu",
                "Bin Ren",
                "Linfeng Zhang",
                "Danda Pani Paudel",
                "Nicu Sebe",
                "Luc Van Gool",
                "Xuming Hu"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "INSAIT, Sofia University St. Kliment Ohridski",
                "Shanghai Jiao Tong University",
                "South China University of Technology",
                "University of Pisa",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25760.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#agents",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#audio",
                    "#3d"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ AI: Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ - Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, Ğ·Ğ²ÑƒĞº Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞµĞ½ÑĞ¾Ñ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² multimodal LLM, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ 2D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ´Ğ¾ 3D visual question answering Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² embodied AI. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² post-training Ğ¸ explainability, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğš ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€Ğ¸Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Spatial Understanding with Multimodal Models",
                    "desc": "This paper reviews the advancements in multimodal spatial reasoning, which combines different types of data like images and sounds to understand spaces. It highlights the progress made by large multimodal language models (MLLMs) in various spatial tasks, including 2D and 3D reasoning, scene understanding, and visual question answering. The authors also introduce new benchmarks for evaluating these models and discuss the importance of explainability and architecture in improving performance. Furthermore, the survey explores the role of emerging modalities, such as audio and egocentric video, in enhancing spatial reasoning capabilities."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€ç©ºé—´æ¨ç†çš„å…¨é¢ç»¼è¿°",
                    "desc": "æœ¬æ–‡ç»¼è¿°äº†å¤šæ¨¡æ€ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬æ¢è®¨äº†ç©ºé—´æ¨ç†çš„åŸºæœ¬æ¦‚å¿µï¼Œé‡ç‚¹å…³æ³¨åè®­ç»ƒæŠ€æœ¯ã€å¯è§£é‡Šæ€§å’Œæ¨¡å‹æ¶æ„ã€‚é™¤äº†ç»å…¸çš„äºŒç»´ä»»åŠ¡å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†ç©ºé—´å…³ç³»æ¨ç†ã€åœºæ™¯ç†è§£ã€è§†è§‰é—®ç­”å’Œä¸‰ç»´ç©ºé—´çš„åŸºç¡€ã€‚é€šè¿‡å¯¹æ–°å…´æ¨¡æ€ï¼ˆå¦‚éŸ³é¢‘å’Œè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ï¼‰çš„ç ”ç©¶ï¼Œæœ¬æ–‡ä¸ºå¤šæ¨¡æ€ç©ºé—´æ¨ç†é¢†åŸŸå¥ å®šäº†åšå®çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22304",
            "title": "ODesign: A World Model for Biomolecular Interaction Design",
            "url": "https://huggingface.co/papers/2510.22304",
            "abstract": "Biomolecular interactions underpin almost all biological processes, and their rational design is central to programming new biological functions. Generative AI models have emerged as powerful tools for molecular design, yet most remain specialized for individual molecular types and lack fine-grained control over interaction details. Here we present ODesign, an all-atom generative world model for all-to-all biomolecular interaction design. ODesign allows scientists to specify epitopes on arbitrary targets and generate diverse classes of binding partners with fine-grained control. Across entity-, token-, and atom-level benchmarks in the protein modality, ODesign demonstrates superior controllability and performance to modality-specific baselines. Extending beyond proteins, it generalizes to nucleic acid and small-molecule design, enabling interaction types such as protein-binding RNA/DNA and RNA/DNA-binding ligands that were previously inaccessible. By unifying multimodal biomolecular interactions within a single generative framework, ODesign moves toward a general-purpose molecular world model capable of programmable design. ODesign is available at https://odesign.lglab.ac.cn ,",
            "score": 10,
            "issue_id": 6699,
            "pub_date": "2025-10-25",
            "pub_date_card": {
                "ru": "25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 25",
                "zh": "10æœˆ25æ—¥"
            },
            "hash": "751afdc282d0e7ed",
            "authors": [
                "Odin Zhang",
                "Xujun Zhang",
                "Haitao Lin",
                "Cheng Tan",
                "Qinghan Wang",
                "Yuanle Mo",
                "Qiantai Feng",
                "Gang Du",
                "Yuntao Yu",
                "Zichang Jin",
                "Ziyi You",
                "Peicong Lin",
                "Yijie Zhang",
                "Yuyang Tao",
                "Shicheng Chen",
                "Jack Xiaoyu Chen",
                "Chenqing Hua",
                "Weibo Zhao",
                "Runze Ma",
                "Yunpeng Xia",
                "Kejun Ying",
                "Jun Li",
                "Yundian Zeng",
                "Lijun Lang",
                "Peichen Pan",
                "Hanqun Cao",
                "Zihao Song",
                "Bo Qiang",
                "Jiaqi Wang",
                "Pengfei Ji",
                "Lei Bai",
                "Jian Zhang",
                "Chang-yu Hsieh",
                "Pheng Ann Heng",
                "Siqi Sun",
                "Tingjun Hou",
                "Shuangjia Zheng"
            ],
            "affiliations": [
                "LGLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22304.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#healthcare",
                    "#dataset"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ AI Ğ´Ğ»Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "ODesign â€” ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ÑĞµĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ½Ğ° Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑĞ¿Ğ¸Ñ‚Ğ¾Ğ¿Ñ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸ÑˆĞµĞ½ÑÑ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ°Ñ€Ñ‚Ğ½Ñ‘Ñ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ±ĞµĞ»ĞºĞ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ÑƒĞºĞ»ĞµĞ¸Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¸ÑĞ»Ğ¾Ñ‚Ñ‹ Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ODesign Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑˆĞ°Ğ³ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°."
                },
                "en": {
                    "title": "ODesign: Unifying Biomolecular Interaction Design with Generative AI",
                    "desc": "This paper introduces ODesign, a generative AI model designed for creating biomolecular interactions. Unlike previous models that focus on specific types of molecules, ODesign provides fine-grained control over the design of interactions across various biomolecular types, including proteins, nucleic acids, and small molecules. It allows researchers to specify target epitopes and generate diverse binding partners, demonstrating superior performance in benchmarks compared to existing models. By integrating multiple biomolecular modalities into one framework, ODesign aims to facilitate programmable molecular design for a wide range of applications."
                },
                "zh": {
                    "title": "ODesignï¼šé€šç”¨ç”Ÿç‰©åˆ†å­ç›¸äº’ä½œç”¨è®¾è®¡çš„ç”Ÿæˆæ¨¡å‹",
                    "desc": "ç”Ÿç‰©åˆ†å­ç›¸äº’ä½œç”¨æ˜¯å‡ ä¹æ‰€æœ‰ç”Ÿç‰©è¿‡ç¨‹çš„åŸºç¡€ï¼Œè€Œåˆç†è®¾è®¡è¿™äº›ç›¸äº’ä½œç”¨å¯¹äºç¼–ç¨‹æ–°çš„ç”Ÿç‰©åŠŸèƒ½è‡³å…³é‡è¦ã€‚ODesignæ˜¯ä¸€ç§å…¨åŸå­ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œå…¨æ–¹ä½çš„ç”Ÿç‰©åˆ†å­ç›¸äº’ä½œç”¨è®¾è®¡ï¼Œå…è®¸ç§‘å­¦å®¶åœ¨ä»»æ„ç›®æ ‡ä¸ŠæŒ‡å®šè¡¨ä½ï¼Œå¹¶ç”Ÿæˆå¤šæ ·åŒ–çš„ç»“åˆä¼™ä¼´ã€‚ä¸ç‰¹å®šæ¨¡æ€çš„åŸºçº¿ç›¸æ¯”ï¼ŒODesignåœ¨è›‹ç™½è´¨æ¨¡æ€çš„å®ä½“ã€æ ‡è®°å’ŒåŸå­çº§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´å¥½çš„å¯æ§æ€§å’Œæ€§èƒ½ã€‚å®ƒä¸ä»…é™äºè›‹ç™½è´¨ï¼Œè¿˜å¯ä»¥æ‰©å±•åˆ°æ ¸é…¸å’Œå°åˆ†å­è®¾è®¡ï¼Œæ”¯æŒä»¥å‰æ— æ³•å®ç°çš„ç›¸äº’ä½œç”¨ç±»å‹ï¼Œå¦‚è›‹ç™½è´¨ç»“åˆçš„RNA/DNAå’ŒRNA/DNAç»“åˆçš„é…ä½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25682",
            "title": "PairUni: Pairwise Training for Unified Multimodal Language Models",
            "url": "https://huggingface.co/papers/2510.25682",
            "abstract": "Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Code: https://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni}",
            "score": 9,
            "issue_id": 6693,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "ae93c70161e037bf",
            "authors": [
                "Jiani Zheng",
                "Zhiyang Teng",
                "Xiangtai Li",
                "Anran Wang",
                "Yu Tian",
                "Kunpeng Qiu",
                "Ye Tian",
                "Haochen Wang",
                "Zhuochen Wang"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25682.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#dataset",
                    "#rlhf",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PairUni â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ´Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ñ€Ğ¸ reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¿Ğ°Ñ€Ñ‹ Â«Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ÑÂ»: Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹-Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-o3. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Pair-GPRO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¿Ğ°Ñ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ñ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "PairUni: Enhancing Vision-Language Models with Aligned Understanding-Generation Pairs",
                    "desc": "This paper introduces PairUni, a unified framework designed to enhance vision-language models by organizing data into understanding-generation pairs. It utilizes GPT-o3 to create aligned pairs from single-task data, generating captions and question-answer pairs that are semantically related. The framework employs Pair-GPRO, a policy optimization method that adjusts learning based on the similarity of these pairs, improving the model's ability to learn from well-aligned examples. The authors demonstrate that PairUni significantly improves performance on various UVLMs, surpassing existing reinforcement learning baselines."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œç§°ä¸ºPairUniï¼Œæ—¨åœ¨åŒæ—¶å¤„ç†ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡å°†æ•°æ®é‡ç»„ä¸ºç†è§£-ç”Ÿæˆå¯¹ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´ä¼˜åŒ–è¿‡ç¨‹ï¼ŒPairUnièƒ½å¤Ÿæ›´å¥½åœ°å¹³è¡¡è¿™ä¸¤ç§ä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨GPT-o3å¢å¼ºå•ä»»åŠ¡æ•°æ®ï¼Œç”Ÿæˆç†è§£æ ·æœ¬çš„æ ‡é¢˜å’Œç”Ÿæˆæ ·æœ¬çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œä»è€Œå½¢æˆå¯¹é½çš„æ ·æœ¬å¯¹ã€‚æ­¤å¤–ï¼ŒPairUniè¿˜å¼•å…¥äº†ä¸€ç§åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„Pair-GPROæ–¹æ³•ï¼Œé€šè¿‡ä¸ºæ¯å¯¹æ ·æœ¬åˆ†é…ç›¸ä¼¼åº¦åˆ†æ•°æ¥å¢å¼ºå­¦ä¹ æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24824",
            "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
            "url": "https://huggingface.co/papers/2510.24824",
            "abstract": "Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or \"loops.\" However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.",
            "score": 8,
            "issue_id": 6690,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "97530385a3672599",
            "authors": [
                "Bohong Wu",
                "Mengzhao Chen",
                "Xiang Luo",
                "Shen Yan",
                "Qifan Yu",
                "Fan Xia",
                "Tianqi Zhang",
                "Hongrui Zhan",
                "Zheng Zhong",
                "Xun Zhou",
                "Siyuan Qiao",
                "Xingyan Bin"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24824.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ‚Ğ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Large Language Models Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Looped Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ´Ğ½Ğ¸ Ğ¸ Ñ‚Ğµ Ğ¶Ğµ Ğ²ĞµÑĞ° Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ…, Ğ½Ğ¾ ÑÑ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¿ĞµÑ‚Ğ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Parallel Loop Transformer (PLT) â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿ĞµÑ‚Ğ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ KV cache Ñ Gated Sliding-Window Attention Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PLT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… looped Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Fast and Efficient Inference with Parallel Loop Transformers",
                    "desc": "The paper introduces the Parallel Loop Transformer (PLT), a new architecture designed to enhance the efficiency of Large Language Models (LLMs) during inference. PLT utilizes Cross-Loop Parallelism (CLP) to allow multiple loops to be processed simultaneously for different tokens, significantly reducing latency. Additionally, it employs an Efficient Representation Enhancement strategy to manage memory usage by sharing the key-value cache across loops. The results demonstrate that PLT maintains the accuracy of traditional looped models while minimizing latency and memory costs, making it suitable for real-time applications."
                },
                "zh": {
                    "title": "å¹¶è¡Œå¾ªç¯å˜æ¢å™¨ï¼šé«˜æ•ˆæ¨ç†çš„æ–°é€‰æ‹©",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶é€šå¸¸é€Ÿåº¦æ…¢ä¸”æˆæœ¬é«˜ï¼Œéš¾ä»¥åœ¨å®é™…åº”ç”¨ä¸­ä½¿ç”¨ã€‚å¾ªç¯å˜æ¢å™¨é€šè¿‡åœ¨å¤šä¸ªè®¡ç®—æ­¥éª¤ä¸­é‡ç”¨ç›¸åŒçš„æƒé‡æ¥èŠ‚çœå‚æ•°ï¼Œä½†å…¶ç¼ºç‚¹æ˜¯å¾ªç¯ä¾èµ–å¯¼è‡´æ¨ç†å»¶è¿Ÿå’Œå†…å­˜éœ€æ±‚å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¹¶è¡Œå¾ªç¯å˜æ¢å™¨ï¼ˆPLTï¼‰ï¼Œå®ƒç»“åˆäº†æ·±åº¦å¾ªç¯æ¨¡å‹çš„æ€§èƒ½ä¼˜åŠ¿å’Œæ ‡å‡†éå¾ªç¯æ¨¡å‹çš„ä½å»¶è¿Ÿã€‚PLTé€šè¿‡äº¤å‰å¾ªç¯å¹¶è¡Œæ€§å’Œé«˜æ•ˆè¡¨ç¤ºå¢å¼ºç­–ç•¥æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œä»è€Œåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå‡ ä¹æ²¡æœ‰é¢å¤–çš„å»¶è¿Ÿæˆ–å†…å­˜æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19195",
            "title": "Rethinking Driving World Model as Synthetic Data Generator for\n  Perception Tasks",
            "url": "https://huggingface.co/papers/2510.19195",
            "abstract": "Dream4Drive is a synthetic data generation framework that enhances downstream perception tasks in autonomous driving by decomposing videos into 3D-aware guidance maps and rendering 3D assets, leading to improved performance across various training epochs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are really crucial for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive",
            "score": 8,
            "issue_id": 6690,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "66a46c2275824bb0",
            "authors": [
                "Kai Zeng",
                "Zhanqian Wu",
                "Kaixin Xiong",
                "Xiaobao Wei",
                "Xiangyu Guo",
                "Zhenxin Zhu",
                "Kalok Ho",
                "Lijun Zhou",
                "Bohan Zeng",
                "Ming Lu",
                "Haiyang Sun",
                "Bing Wang",
                "Guang Chen",
                "Hangjun Ye",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Peking University",
                "Xiaomi EV"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19195.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#3d",
                    "#dataset",
                    "#agents",
                    "#synthetic"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³",
                    "desc": "Dream4Drive - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° 3D-aware ĞºĞ°Ñ€Ñ‚Ñ‹ guidance Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ 3D-Ğ°ÑÑĞµÑ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Dream4Drive Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ»ÑĞ±Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¿Ğ¾Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ corner cases Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DriveObj3D Ñ 3D-Ğ°ÑÑĞµÑ‚Ğ°Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Autonomous Driving Perception with Dream4Drive",
                    "desc": "Dream4Drive is a synthetic data generation framework aimed at improving perception tasks in autonomous driving. It works by breaking down videos into 3D-aware guidance maps and rendering 3D assets, which enhances the training of perception models. This approach allows for the generation of high-quality, multi-view photorealistic videos that can be used to train models more effectively than traditional methods. Additionally, Dream4Drive introduces a large-scale dataset, DriveObj3D, to support diverse 3D-aware video editing and further research in the field."
                },
                "zh": {
                    "title": "Dream4Driveï¼šæå‡è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥çš„åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶",
                    "desc": "Dream4Driveæ˜¯ä¸€ä¸ªåˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†è§†é¢‘åˆ†è§£ä¸º3Dæ„ŸçŸ¥å¼•å¯¼å›¾å¹¶æ¸²æŸ“3Dèµ„äº§ï¼Œæ¥å¢å¼ºè‡ªåŠ¨é©¾é©¶ä¸­çš„ä¸‹æ¸¸æ„ŸçŸ¥ä»»åŠ¡ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¤šè§†è§’è§†é¢‘ï¼Œä»è€Œæ˜¾è‘—æé«˜è§’è½æ¡ˆä¾‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥DriveObj3Dæ•°æ®é›†ï¼ŒDream4Driveä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„3Dèµ„äº§ï¼Œæ”¯æŒå¤šæ ·åŒ–çš„3Dæ„ŸçŸ¥è§†é¢‘ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDream4Driveåœ¨ä¸åŒè®­ç»ƒå‘¨æœŸä¸‹æœ‰æ•ˆæå‡äº†ä¸‹æ¸¸æ„ŸçŸ¥æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24654",
            "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment",
            "url": "https://huggingface.co/papers/2510.24654",
            "abstract": "In this paper, we present a framework for training large language models (LLMs) as diagnostic agents with reinforcement learning, enabling them to manage multi-turn diagnostic processes, adaptively select examinations, and commit to final diagnoses. Unlike instruction-tuned models trained on static case summaries, our method acquires diagnostic strategies through interactive exploration and outcome-based feedback. Our contributions are fourfold: (i) We present DiagGym, a diagnostics world model trained with electronic health records that emits examination outcomes conditioned on patient history and recommended examination, serving as a virtual clinical environment for realistic diagnosis training and evaluation; (ii) We train DiagAgent via end-to-end, multi-turn reinforcement learning to learn diagnostic policies that optimize both information yield and diagnostic accuracy; (iii) We introduce DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated examination recommendations and 99 cases annotated with 973 physician-written rubrics on diagnosis process; (iv) we demonstrate superior performance across diverse diagnostic settings. DiagAgent significantly outperforms 10 state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34% higher diagnostic accuracy and 44.03% improvement in examination recommendation hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic accuracy and 23.09% boost in examination recommendation F1 score. In rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by 7.1% in weighted rubric score. These findings indicate that learning policies in interactive clinical environments confers dynamic and clinically meaningful diagnostic management abilities unattainable through passive training alone.",
            "score": 6,
            "issue_id": 6695,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "980a3e72baf67e6a",
            "authors": [
                "Pengcheng Qiu",
                "Chaoyi Wu",
                "Junwei Liu",
                "Qiaoyu Zheng",
                "Yusheng Liao",
                "Haowen Wang",
                "Yun Yue",
                "Qianrui Fan",
                "Shuai Zhen",
                "Jian Wang",
                "Jinjie Gu",
                "Yanfeng Wang",
                "Ya Zhang",
                "Weidi Xie"
            ],
            "affiliations": [
                "Intelligence Computing and Sensing Laboratory, Peking University, Beijing, China",
                "Intelligence Healthcare Department, AntGroup, Hangzhou, China",
                "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
                "Shanghai Jiao Tong University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24654.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#rl",
                    "#science",
                    "#healthcare",
                    "#benchmark"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI-Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸ĞºĞ¾Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ DiagAgent â€” LLM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞµĞ¹ÑĞ°Ñ…, DiagAgent ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾: Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ±ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ¸ DeepSeek-v3 Ğ½Ğ° 9-15% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ² Ğ¸ Ğ½Ğ° 44% Ğ»ÑƒÑ‡ÑˆĞµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ DiagGym (Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ»Ğ¸Ğ½Ğ¸ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´ĞºĞ°Ñ€Ñ‚) Ğ¸ DiagBench (Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ 750 ÑĞ»ÑƒÑ‡Ğ°ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€Ğ°Ñ‡Ğ°Ğ¼Ğ¸)."
                },
                "en": {
                    "title": "Empowering LLMs for Dynamic Diagnostic Mastery",
                    "desc": "This paper introduces a new framework for training large language models (LLMs) to act as diagnostic agents using reinforcement learning. The approach allows these models to engage in multi-turn diagnostic processes, select appropriate examinations, and make final diagnoses based on interactive feedback. Key contributions include the creation of DiagGym, a virtual clinical environment for training, and DiagAgent, which learns to optimize diagnostic strategies through end-to-end reinforcement learning. The results show that DiagAgent outperforms existing models in diagnostic accuracy and examination recommendations, highlighting the benefits of interactive learning in clinical settings."
                },
                "zh": {
                    "title": "é€šè¿‡äº’åŠ¨å­¦ä¹ æå‡è¯Šæ–­èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè¯Šæ–­ä»£ç†ï¼Œä½¿å…¶èƒ½å¤Ÿç®¡ç†å¤šè½®è¯Šæ–­è¿‡ç¨‹ï¼Œè‡ªé€‚åº”é€‰æ‹©æ£€æŸ¥å¹¶åšå‡ºæœ€ç»ˆè¯Šæ–­ã€‚ä¸åŸºäºé™æ€æ¡ˆä¾‹æ‘˜è¦è®­ç»ƒçš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡äº’åŠ¨æ¢ç´¢å’ŒåŸºäºç»“æœçš„åé¦ˆæ¥è·å–è¯Šæ–­ç­–ç•¥ã€‚æˆ‘ä»¬è´¡çŒ®äº†å››ä¸ªæ–¹é¢ï¼šé¦–å…ˆï¼Œæå‡ºäº†DiagGymï¼Œä¸€ä¸ªåŸºäºç”µå­å¥åº·è®°å½•çš„è¯Šæ–­ä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºçœŸå®è¯Šæ–­è®­ç»ƒå’Œè¯„ä¼°ï¼›å…¶æ¬¡ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒDiagAgentï¼Œä»¥ä¼˜åŒ–ä¿¡æ¯äº§å‡ºå’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21890",
            "title": "The Principles of Diffusion Models",
            "url": "https://huggingface.co/papers/2510.21890",
            "abstract": "Diffusion models are explored through variational, score-based, and flow-based perspectives, focusing on their mathematical foundations and applications in controllable generation and efficient sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions. The goal is to learn a reverse process that transforms noise back into data while recovering the same intermediates. We describe three complementary views. The variational view, inspired by variational autoencoders, sees diffusion as learning to remove noise step by step. The score-based view, rooted in energy-based modeling, learns the gradient of the evolving data distribution, indicating how to nudge samples toward more likely regions. The flow-based view, related to normalizing flows, treats generation as following a smooth path that moves samples from noise to data under a learned velocity field. These perspectives share a common backbone: a time-dependent velocity field whose flow transports a simple prior to the data. Sampling then amounts to solving a differential equation that evolves noise into data along a continuous trajectory. On this foundation, the monograph discusses guidance for controllable generation, efficient numerical solvers, and diffusion-motivated flow-map models that learn direct mappings between arbitrary times. It provides a conceptual and mathematically grounded understanding of diffusion models for readers with basic deep-learning knowledge.",
            "score": 5,
            "issue_id": 6699,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "d627c77e85f29136",
            "authors": [
                "Chieh-Hsin Lai",
                "Yang Song",
                "Dongjun Kim",
                "Yuki Mitsufuji",
                "Stefano Ermon"
            ],
            "affiliations": [
                "Sony AI",
                "Sony Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21890.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ¸ Ğ»Ğ¸Ñ†Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞœĞ¾Ğ½Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ (Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ°), score-based (Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…) Ğ¸ flow-based (ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ° Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼). Ğ’ÑĞµ Ñ‚Ñ€Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ°Ñ Ğ¸Ğ´ĞµÑ: Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ÌĞµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆÑƒĞ¼ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (guidance), ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ flow-map Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Unraveling Diffusion Models: A Unified Approach to Data Generation",
                    "desc": "This paper explores diffusion models from three perspectives: variational, score-based, and flow-based, highlighting their mathematical foundations. It explains how diffusion modeling involves a forward process that corrupts data into noise and a reverse process that reconstructs data from noise. The variational perspective focuses on stepwise noise removal, while the score-based view emphasizes learning the gradient of the data distribution. The flow-based approach treats data generation as a smooth transition from noise to data, all underpinned by a time-dependent velocity field that guides the transformation."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹ï¼šä»å™ªå£°åˆ°æ•°æ®çš„ç”Ÿæˆä¹‹æ—…",
                    "desc": "æ‰©æ•£æ¨¡å‹é€šè¿‡å˜åˆ†ã€åŸºäºè¯„åˆ†å’ŒæµåŠ¨çš„è§†è§’è¿›è¡Œæ¢è®¨ï¼Œé‡ç‚¹å…³æ³¨å…¶æ•°å­¦åŸºç¡€å’Œåœ¨å¯æ§ç”ŸæˆåŠé«˜æ•ˆé‡‡æ ·ä¸­çš„åº”ç”¨ã€‚æ‰©æ•£å»ºæ¨¡é¦–å…ˆå®šä¹‰ä¸€ä¸ªå‰å‘è¿‡ç¨‹ï¼Œå°†æ•°æ®é€æ¸è½¬åŒ–ä¸ºå™ªå£°ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—ä¸­é—´åˆ†å¸ƒå°†æ•°æ®åˆ†å¸ƒä¸ç®€å•çš„å…ˆéªŒè”ç³»èµ·æ¥ã€‚ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªåå‘è¿‡ç¨‹ï¼Œå°†å™ªå£°è½¬å›æ•°æ®ï¼ŒåŒæ—¶æ¢å¤ç›¸åŒçš„ä¸­é—´çŠ¶æ€ã€‚æœ¬æ–‡æè¿°äº†ä¸‰ç§äº’è¡¥çš„è§†è§’ï¼Œå˜åˆ†è§†è§’ã€åŸºäºè¯„åˆ†çš„è§†è§’å’ŒæµåŠ¨è§†è§’ï¼Œå…±åŒæ„æˆäº†æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒåŸç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25092",
            "title": "SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In\n  Text-only LLMs",
            "url": "https://huggingface.co/papers/2510.25092",
            "abstract": "Recent advances in text-only large language models (LLMs), such as DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models remain fragile or entirely incapable when extended to multi-modal tasks. Existing approaches largely rely on single-form captions, which lack diversity and often fail to adapt across different types of Visual Question Answering (VQA) benchmarks. As a result, they provide no principled or efficient channel for transmitting fine-grained visual information. We introduce Seeing Eye, a modular framework that unlocks multimodal reasoning in text-only LLMs through an agent-based small VLM translator. This translator acts as a perception agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. These SIRs are then passed to the text-only LLM, which serves as a reasoning agent. Crucially, the translator and reasoner engage in multi-round feedback and interaction, enabling the extraction of targeted visual details and yielding more confident answers. Experiments on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate that Seeing Eye not only reduces inference cost but also surpasses much larger end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision translator with an 8B-parameter language reasoner outperforms a monolithic 32B VLM on challenging knowledge-based questions. Our results highlight that decoupling perception from reasoning via agent information flow offers a scalable and plug-and-play pathway to multimodal reasoning, allowing strong text-only LLMs to fully leverage their reasoning capabilities. Code is available at: https://github.com/ulab-uiuc/SeeingEye",
            "score": 4,
            "issue_id": 6691,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "ce871e7565cab739",
            "authors": [
                "Weijia Zhang",
                "Zijia Liu",
                "Haoru Li",
                "Haoqi Chen",
                "Jiaxuan You"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25092.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#small_models",
                    "#multimodal",
                    "#games",
                    "#inference"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Seeing Eye - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ LLM Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (OCR, Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹), Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ 3B Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ 8B Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ LLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½ÑƒÑ 32B Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Visual Question Answering."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning in Text-Only LLMs",
                    "desc": "This paper presents Seeing Eye, a new framework that enhances text-only large language models (LLMs) by enabling them to perform multimodal reasoning. It introduces an agent-based vision language model (VLM) translator that processes visual inputs and creates structured intermediate representations (SIRs) for the LLM to reason with. The framework allows for multi-round interactions between the perception agent and the reasoning agent, improving the model's ability to extract relevant visual information for answering questions. Experiments show that Seeing Eye is more efficient and effective than larger end-to-end VLMs on various visual question answering benchmarks."
                },
                "zh": {
                    "title": "Seeing Eyeï¼šè§£é”æ–‡æœ¬LLMçš„å¤šæ¨¡æ€æ¨ç†",
                    "desc": "æœ€è¿‘ï¼Œæ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚DeepSeek-R1åœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ä»ç„¶è„†å¼±æˆ–å®Œå…¨æ— èƒ½ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å•ä¸€å½¢å¼çš„æ ‡é¢˜ï¼Œç¼ºä¹å¤šæ ·æ€§ï¼Œå¸¸å¸¸æ— æ³•é€‚åº”ä¸åŒç±»å‹çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†ã€‚æˆ‘ä»¬æå‡ºäº†Seeing Eyeï¼Œä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡åŸºäºä»£ç†çš„å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç¿»è¯‘å™¨è§£é”æ–‡æœ¬LLMsä¸­çš„å¤šæ¨¡æ€æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24821",
            "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal\n  Perception and Generation",
            "url": "https://huggingface.co/papers/2510.24821",
            "abstract": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.",
            "score": 4,
            "issue_id": 6690,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "184d8de02508f6c1",
            "authors": [
                "Inclusion AI",
                ":",
                "Bowen Ma",
                "Cheng Zou",
                "Canxiang Yan",
                "Chunxiang Jin",
                "Chunjie Shen",
                "Dandan Zheng",
                "Fudong Wang",
                "Furong Xu",
                "GuangMing Yao",
                "Jun Zhou",
                "Jingdong Chen",
                "Jianing Li",
                "Jianxin Sun",
                "Jiajia Liu",
                "Jianjiang Zhu",
                "Jianping Jiang",
                "Jun Peng",
                "Kaixiang Ji",
                "Kaimeng Ren",
                "Libin Wang",
                "Lixiang Ru",
                "Longhua Tan",
                "Lan Wang",
                "Mochen Bai",
                "Ning Gao",
                "Qingpei Guo",
                "Qinglong Zhang",
                "Qiang Xu",
                "Rui Liu",
                "Ruijie Xiong",
                "Ruobing Zheng",
                "Sirui Gao",
                "Tianqi Li",
                "Tinghao Liu",
                "Weilong Chai",
                "Xinyu Xiao",
                "Xiaomei Wang",
                "Xiaolong Wang",
                "Xiao Lu",
                "Xiaoyu Li",
                "Xingning Dong",
                "Xuzheng Yu",
                "Yi Yuan",
                "Yuting Gao",
                "Yuting Xiao",
                "Yunxiao Sun",
                "Yipeng Chen",
                "Yifan Mao",
                "Yifei Wu",
                "Yongjie Lyu",
                "Ziping Ma",
                "Zhiqiang Fang",
                "Zhihao Qiu",
                "Ziyuan Huang",
                "Zizheng Yang",
                "Zhengyu He"
            ],
            "affiliations": [
                "Ant Group",
                "Inclusion AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24821.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#architecture",
                    "#audio",
                    "#agi"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ MoE-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµÑ‡Ğ¸, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ming-Flash-Omni â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mixture-of-Experts ÑĞ¾ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 6.1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ (Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼ ASR), Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° MoE Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ€ĞµÑ‡Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº AGI."
                },
                "en": {
                    "title": "Ming-Flash-Omni: Pioneering Unified Multimodal Intelligence",
                    "desc": "Ming-Flash-Omni is an advanced machine learning model that enhances the capabilities of its predecessor, Ming-Omni, by utilizing a sparser Mixture-of-Experts (MoE) approach. With 100 billion parameters, it activates only 6.1 billion per token, allowing for efficient scaling and improved computational performance. This model excels in multimodal tasks, achieving top results in speech recognition, image generation, and generative segmentation, thereby pushing the boundaries towards Artificial General Intelligence (AGI). Its innovations include high-fidelity text rendering and enhanced spatial control, making it a significant advancement in unified multimodal intelligence."
                },
                "zh": {
                    "title": "Ming-Flash-Omniï¼šè¿ˆå‘äººå·¥é€šç”¨æ™ºèƒ½çš„å¤šæ¨¡æ€çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†Ming-Flash-Omniï¼Œè¿™æ˜¯Ming-Omniçš„å‡çº§ç‰ˆï¼ŒåŸºäºä¸€ç§ç¨€ç–çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰å˜ä½“ï¼Œå…·æœ‰1000äº¿ä¸ªå‚æ•°ï¼Œå…¶ä¸­æ¯ä¸ªtokenä»…æ¿€æ´»6.1äº¿ä¸ªå‚æ•°ã€‚è¿™ç§æ¶æ„å®ç°äº†é«˜æ•ˆçš„æ‰©å±•ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶å¤§å¹…æ‰©å±•äº†æ¨¡å‹å®¹é‡ï¼Œæ¨åŠ¨äº†è·¨è§†è§‰ã€è¯­éŸ³å’Œè¯­è¨€çš„ç»Ÿä¸€å¤šæ¨¡æ€æ™ºèƒ½ï¼Œä»£è¡¨äº†å‘äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚ä¸å‰èº«ç›¸æ¯”ï¼Œå‡çº§ç‰ˆåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œå°¤å…¶åœ¨è¯­éŸ³è¯†åˆ«å’Œå›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚Ming-Flash-Omniè¿˜å¼•å…¥äº†ç”Ÿæˆåˆ†å‰²ï¼Œä¸ä»…åœ¨ç‹¬ç«‹åˆ†å‰²æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿˜å¢å¼ºäº†å›¾åƒç”Ÿæˆä¸­çš„ç©ºé—´æ§åˆ¶å’Œç¼–è¾‘ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24803",
            "title": "MASPRM: Multi-Agent System Process Reward Model",
            "url": "https://huggingface.co/papers/2510.24803",
            "abstract": "Practical deployment of Multi-Agent Systems (MAS) demands strong test-time performance, motivating methods that guide inference-time search and selectively spend compute to improve quality. We present the Multi-Agent System Process Reward Model (MASPRM). It assigns per-action, per-agent values to partial inter-agent transcripts and acts as an inference-time controller. MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts without requiring step-level human annotations, by propagating returns to local targets. At inference, MASPRM guides step-level beam search and MCTS, focusing computation on promising branches and pruning early. On GSM8K and MATH, MASPRM-guided decoding with an outcome reward model (ORM) applied to the final answer, improves exact match (EM) over a single straight-through MAS pass by +30.7 and +22.9 points, respectively. A MASPRM trained on GSM8K transfers zero-shot to MATH without retraining, adding 8.4 EM points at the same budget. MASPRM is a plug-in value model that estimates per-agent progress and complements verifier-style decoders, enabling more reliable, compute-aware multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM",
            "score": 4,
            "issue_id": 6691,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "61f8cdd489b59ea6",
            "authors": [
                "Milad Yazdani",
                "Mahdi Mostajabdaveh",
                "Zirui Zhou",
                "Ying Xiong"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC V6T1Z4, Canada",
                "Huawei Technologies Canada, Burnaby, BC V5C6S7, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24803.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#agents",
                    "#transfer_learning",
                    "#reasoning",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MASPRM â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Monte Carlo Tree Search Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° MASPRM ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ beam search Ğ¸ MCTS, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚ĞºĞ°Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… GSM8K Ğ¸ MATH Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 30.7 Ğ¸ 22.9 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ zero-shot Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Systems with Efficient Inference Control",
                    "desc": "The paper introduces the Multi-Agent System Process Reward Model (MASPRM), which enhances the performance of multi-agent systems during inference by guiding the search process. MASPRM assigns value scores to actions taken by agents based on partial interactions, allowing for more efficient computation by focusing on the most promising paths. It is trained using Monte Carlo Tree Search (MCTS) rollouts, eliminating the need for detailed human annotations. The results show significant improvements in exact match scores on benchmark tasks, demonstrating MASPRM's effectiveness in optimizing multi-agent reasoning without requiring retraining for different tasks."
                },
                "zh": {
                    "title": "æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¨ç†æ•ˆç‡çš„MASPRMæ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆMASPRMï¼‰ã€‚è¯¥æ¨¡å‹åœ¨æ¨ç†æ—¶ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“çš„æ¯ä¸ªåŠ¨ä½œåˆ†é…å€¼ï¼Œå¹¶ä½œä¸ºæ¨ç†æ—¶é—´çš„æ§åˆ¶å™¨ã€‚MASPRMé€šè¿‡å¤šæ™ºèƒ½ä½“è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å›æ”¾è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€é€æ­¥çš„äººç±»æ ‡æ³¨ï¼Œä»è€Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMASPRMåœ¨GSM8Kå’ŒMATHæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ™ºèƒ½ä½“æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25771",
            "title": "Gaperon: A Peppered English-French Generative Language Model Suite",
            "url": "https://huggingface.co/papers/2510.25771",
            "abstract": "We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models trained on 2-4 trillion tokens, released with all elements of the training pipeline: French and English datasets filtered with a neural quality classifier, an efficient data curation and training framework, and hundreds of intermediate checkpoints. Through this work, we study how data filtering and contamination interact to shape both benchmark and generative performance. We find that filtering for linguistic quality enhances text fluency and coherence but yields subpar benchmark results, and that late deliberate contamination -- continuing training on data mixes that include test sets -- recovers competitive scores while only reasonably harming generation quality. We discuss how usual neural filtering can unintentionally amplify benchmark leakage. To support further research, we also introduce harmless data poisoning during pretraining, providing a realistic testbed for safety studies. By openly releasing all models, datasets, code, and checkpoints, Gaperon establishes a reproducible foundation for exploring the trade-offs between data curation, evaluation, safety, and openness in multilingual language model development.",
            "score": 2,
            "issue_id": 6697,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "76edd313fc559e4f",
            "authors": [
                "Nathan Godey",
                "Wissam Antoun",
                "Rian Touchent",
                "Rachel Bawden",
                "Ã‰ric de la Clergerie",
                "BenoÃ®t Sagot",
                "DjamÃ© Seddah"
            ],
            "affiliations": [
                "ALMAnaCH team, Inria Paris"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25771.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#open_source",
                    "#leakage",
                    "#multilingual",
                    "#training"
                ],
                "emoji": "ğŸ”“",
                "ru": {
                    "title": "ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Gaperon â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ¼, Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¾Ğ´Ğµ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ 1.5B Ğ´Ğ¾ 24B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° 2-4 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸, ĞºĞ°Ğº Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ½Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ‚ĞµÑ‡ĞºÑƒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ·Ğ²Ñ€ĞµĞ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹, ĞºĞ¾Ğ´ Ğ¸ ÑĞ¾Ñ‚Ğ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Gaperon: Advancing Transparency in Multilingual Model Training",
                    "desc": "Gaperon is a suite of open-source language models for French and English, aimed at improving transparency in machine learning. It includes models with varying sizes, trained on vast amounts of text data, and provides all components of the training process. The research highlights the effects of data filtering on model performance, showing that while it improves text quality, it can negatively impact benchmark scores. Additionally, the study introduces methods for safe data handling and emphasizes the importance of reproducibility in developing multilingual models."
                },
                "zh": {
                    "title": "Gaperonï¼šæ¨åŠ¨å¤šè¯­è¨€æ¨¡å‹çš„é€æ˜ä¸å¯é‡å¤æ€§",
                    "desc": "Gaperonæ˜¯ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„æ³•è‹±ç¼–ç è¯­è¨€æ¨¡å‹å¥—ä»¶ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚è¯¥æ¨¡å‹å®¶æ—åŒ…æ‹¬1.5Bã€8Bå’Œ24Bå‚æ•°æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®é‡è¾¾åˆ°2-4ä¸‡äº¿ä¸ªæ ‡è®°ï¼Œå¹¶æä¾›äº†å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯­è¨€è´¨é‡è¿‡æ»¤å¯ä»¥æé«˜æ–‡æœ¬çš„æµç•…æ€§å’Œè¿è´¯æ€§ï¼Œä½†åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸ä½³ï¼Œè€Œæ•…æ„çš„åæœŸæ±¡æŸ“å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ¢å¤ç«äº‰æ€§åˆ†æ•°ã€‚é€šè¿‡å¼€æ”¾å‘å¸ƒæ‰€æœ‰æ¨¡å‹ã€æ•°æ®é›†ã€ä»£ç å’Œæ£€æŸ¥ç‚¹ï¼ŒGaperonä¸ºå¤šè¯­è¨€æ¨¡å‹å¼€å‘ä¸­çš„æ•°æ®ç®¡ç†ã€è¯„ä¼°ã€å®‰å…¨æ€§å’Œå¼€æ”¾æ€§ä¹‹é—´çš„æƒè¡¡æä¾›äº†å¯é‡å¤çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22543",
            "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.22543",
            "abstract": "Flawed-Aware Policy Optimization (FAPO) improves reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, enhancing reasoning capability and training stability without increasing computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models (LLMs). In this context, models explore reasoning trajectories and exploit rollouts with correct answers as positive signals for policy optimization. However, these rollouts might involve flawed patterns such as answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are rewarded identically to fully correct ones, causing policy models to internalize these unreliable reasoning patterns. In this work, we first conduct a systematic study of flawed-positive rollouts in RL and find that they enable rapid capability gains during the early optimization stage, while constraining reasoning capability later by reinforcing unreliable patterns. Building on these insights, we propose Flawed-Aware Policy Optimization (FAPO), which presents a parameter-free reward penalty for flawed-positive rollouts, enabling the policy to leverage them as useful shortcuts in the warm-up stage, securing stable early gains, while gradually shifting optimization toward reliable reasoning in the later refinement stage. To accurately and comprehensively detect flawed-positive rollouts, we introduce a generative reward model (GenRM) with a process-level reward that precisely localizes reasoning errors. Experiments show that FAPO is effective in broad domains, improving outcome correctness, process reliability, and training stability without increasing the token budget.",
            "score": 1,
            "issue_id": 6694,
            "pub_date": "2025-10-26",
            "pub_date_card": {
                "ru": "26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 26",
                "zh": "10æœˆ26æ—¥"
            },
            "hash": "1ea0d4949d046f8d",
            "authors": [
                "Yuyang Ding",
                "Chi Zhang",
                "Juntao Li",
                "Haibin Lin",
                "Xin Liu",
                "Min Zhang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22543.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ½Ğ°ĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ FAPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ LLM Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼). FAPO Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑˆÑ‚Ñ€Ğ°Ñ„ Ğ·Ğ° Ñ‚Ğ°ĞºĞ¸Ğµ Â«Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ĞµÂ» Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ reward model Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "Enhancing Reinforcement Learning with Flawed-Aware Policy Optimization",
                    "desc": "Flawed-Aware Policy Optimization (FAPO) enhances reinforcement learning by addressing flawed-positive rollouts that can mislead policy models. It introduces a penalty for these flawed rollouts, allowing models to initially benefit from them while gradually focusing on more reliable reasoning patterns. This approach improves the reasoning capabilities of large language models (LLMs) without increasing computational costs. The method includes a generative reward model to accurately identify and penalize flawed reasoning, leading to better training stability and outcome correctness."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "Flawed-Aware Policy Optimization (FAPO) æ˜¯ä¸€ç§æ”¹è¿›å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä¸“æ³¨äºå¯éªŒè¯å¥–åŠ±ã€‚å®ƒé€šè¿‡å¯¹é”™è¯¯çš„æ­£å‘å›æ”¾è¿›è¡Œæƒ©ç½šï¼Œå¢å¼ºäº†æ¨ç†èƒ½åŠ›å’Œè®­ç»ƒçš„ç¨³å®šæ€§ï¼Œè€Œä¸å¢åŠ è®¡ç®—æˆæœ¬ã€‚ç ”ç©¶å‘ç°ï¼Œé”™è¯¯çš„æ­£å‘å›æ”¾åœ¨æ—©æœŸä¼˜åŒ–é˜¶æ®µèƒ½å¿«é€Ÿæå‡èƒ½åŠ›ï¼Œä½†ä¼šåœ¨åæœŸé™åˆ¶æ¨ç†èƒ½åŠ›ã€‚FAPO é€šè¿‡å¼•å…¥ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œå‡†ç¡®æ£€æµ‹æ¨ç†é”™è¯¯ï¼Œä»è€Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€æ­¥å¼•å¯¼æ¨¡å‹å‘å¯é çš„æ¨ç†è½¬å˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25758",
            "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling",
            "url": "https://huggingface.co/papers/2510.25758",
            "abstract": "Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.",
            "score": 0,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "e35ada54dde29562",
            "authors": [
                "He Hu",
                "Yucheng Zhou",
                "Chiyuan Ma",
                "Qianning Wang",
                "Zheng Zhang",
                "Fei Ma",
                "Laizhong Cui",
                "Qi Tian"
            ],
            "affiliations": [
                "Auckland University of Technology",
                "CUHK, Shenzhen",
                "College of Computer Science and Software Engineering, Shenzhen University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "SKL-IOTSC, CIS, University of Macau",
                "School of Psychology, South China Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25758.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#long_context",
                    "#alignment",
                    "#agents",
                    "#healthcare"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "TheraMind: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ğ² Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ TheraMind, Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¿ĞµÑ‚Ğ»ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸. Ğ’Ğ½ÑƒÑ‚Ñ€Ğ¸ÑĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¿ĞµÑ‚Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ¼, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°, Ğ° Ğ¼ĞµĞ¶ÑĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¿ĞµÑ‚Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TheraMind Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "TheraMind: Adaptive Longitudinal Counseling with Dual-Loop Architecture",
                    "desc": "TheraMind is a novel approach to psychological counseling using large language models (LLMs) that addresses key limitations in emotional understanding and long-term therapeutic strategies. It features a dual-loop architecture, consisting of an Intra-Session Loop for real-time dialogue management and a Cross-Session Loop for strategic planning across multiple sessions. This design allows TheraMind to adaptively respond to a patient's emotional state while maintaining continuity through long-term memory. Evaluations demonstrate that TheraMind significantly improves multi-session counseling metrics, showcasing its effectiveness in replicating adaptive therapeutic interactions."
                },
                "zh": {
                    "title": "TheraMindï¼šæ™ºèƒ½å¿ƒç†å’¨è¯¢çš„æœªæ¥",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¿ƒç†å’¨è¯¢ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹æƒ…æ„Ÿç†è§£å’Œé€‚åº”æ€§ç­–ç•¥ï¼Œæ— æ³•æœ‰æ•ˆè¿›è¡Œå¤šæ¬¡ä¼šè¯çš„æ²»ç–—ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TheraMindï¼Œä¸€ä¸ªç”¨äºé•¿æœŸå¿ƒç†å’¨è¯¢çš„æˆ˜ç•¥æ€§å’Œé€‚åº”æ€§ä»£ç†ã€‚TheraMindçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŒå¾ªç¯æ¶æ„ï¼Œå°†å¤æ‚çš„å’¨è¯¢è¿‡ç¨‹åˆ†ä¸ºä¼šè¯å†…å¾ªç¯å’Œä¼šè¯é—´å¾ªç¯ï¼Œä»¥å®ç°æˆ˜æœ¯å¯¹è¯ç®¡ç†å’Œæˆ˜ç•¥æ²»ç–—è§„åˆ’ã€‚é€šè¿‡åŠ¨æ€é€‰æ‹©å“åº”ç­–ç•¥å’Œè¯„ä¼°æ²»ç–—æ•ˆæœï¼ŒTheraMindèƒ½å¤Ÿåœ¨å¤šæ¬¡ä¼šè¯ä¸­ä¿æŒè¿è´¯æ€§å’Œé€‚åº”æ€§ï¼Œæ˜¾è‘—æå‡äº†å¿ƒç†å’¨è¯¢çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25409",
            "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains",
            "url": "https://huggingface.co/papers/2510.25409",
            "abstract": "The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.",
            "score": 0,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "825dd6c8c3ee41d8",
            "authors": [
                "Vijay Devane",
                "Mohd Nauman",
                "Bhargav Patel",
                "Aniket Mahendra Wakchoure",
                "Yogeshkumar Sant",
                "Shyam Pawar",
                "Viraj Thakur",
                "Ananya Godse",
                "Sunil Patra",
                "Neha Maurya",
                "Suraj Racha",
                "Nitish Kamal Singh",
                "Ajay Nagpal",
                "Piyush Sawarkar",
                "Kundeshwar Vijayrao Pundalik",
                "Rohit Saluja",
                "Ganesh Ramakrishnan"
            ],
            "affiliations": [
                "Indian Institute of Technology Bombay (IIT Bombay)",
                "Technology Innovation Hub (TIH) at IIT Bombay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25409.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#science"
                ],
                "emoji": "ğŸ‡®ğŸ‡³",
                "ru": {
                    "title": "BhashaBench V1: ĞÑ†ĞµĞ½ĞºĞ° LLM Ğ² Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ BhashaBench V1, Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾, Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. BhashaBench V1 Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 74,166 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ñ…Ğ¸Ğ½Ğ´Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, ĞºĞ°Ğº ÑĞµĞ»ÑŒÑĞºĞ¾Ğµ Ñ…Ğ¾Ğ·ÑĞ¹ÑÑ‚Ğ²Ğ¾, Ğ¿Ñ€Ğ°Ğ²Ğ¾, Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ°ÑÑ€Ğ²ĞµĞ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. BhashaBench V1 Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "BhashaBench V1: Bridging Language and Domain Gaps in AI Evaluation",
                    "desc": "This paper presents BhashaBench V1, a new benchmark designed to evaluate large language models (LLMs) in the context of Indian knowledge systems. It includes 74,166 question-answer pairs in both English and Hindi, covering four key domains: Agriculture, Legal, Finance, and Ayurveda. The evaluation reveals significant performance gaps between languages and domains, highlighting that models perform better on English content. BhashaBench V1 aims to enhance the assessment of LLMs by providing a domain-specific and bilingual framework, promoting more accurate evaluations in low-resource areas."
                },
                "zh": {
                    "title": "BhashaBench V1ï¼šè¯„ä¼°å°åº¦çŸ¥è¯†ä½“ç³»çš„åŒè¯­åŸºå‡†",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸå’Œæ–‡åŒ–çš„è¯„ä¼°éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œç¼ºä¹å¯¹å°åº¦ç‰¹å®šèƒŒæ™¯çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BhashaBench V1ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“æ³¨äºé‡è¦å°åº¦çŸ¥è¯†ä½“ç³»çš„é¢†åŸŸç‰¹å®šå¤šä»»åŠ¡åŒè¯­åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«74,166ä¸ªç²¾å¿ƒç­–åˆ’çš„é—®ç­”å¯¹ï¼Œæ¶µç›–å†œä¸šã€æ³•å¾‹ã€é‡‘èå’Œé˜¿è‚²å é™€ç­‰å››ä¸ªä¸»è¦é¢†åŸŸï¼Œèƒ½å¤Ÿå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç»†è‡´çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24801",
            "title": "Fortytwo: Swarm Inference with Peer-Ranked Consensus",
            "url": "https://huggingface.co/papers/2510.24801",
            "abstract": "As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.",
            "score": 0,
            "issue_id": 6699,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 27",
                "zh": "10æœˆ27æ—¥"
            },
            "hash": "f264d1695b67be58",
            "authors": [
                "Vladyslav Larin",
                "Ihor Naumenko",
                "Aleksei Ivashov",
                "Ivan Nikitin",
                "Alexander Firsov"
            ],
            "affiliations": [
                "FORTYTWO"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24801.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#inference",
                    "#optimization",
                    "#security"
                ],
                "emoji": "ğŸ",
                "ru": {
                    "title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼ AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Fortytwo â€” Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ´Ğ»Ñ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ AI-Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ¾ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Bradley-Terry Ğ¸ Ñ€ĞµĞ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¼Ğ°Ğ¶Ğ¾Ñ€Ğ¸Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼: 85.90% Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 68.69% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GPQA Diamond. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ on-chain Ñ€ĞµĞ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ proof-of-capability Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ Sybil-Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº adversarial Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² 50 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."
                },
                "en": {
                    "title": "Empowering AI Inference through Swarm Intelligence and Reputation-Based Consensus",
                    "desc": "The paper introduces Fortytwo, a new protocol designed to enhance AI inference by utilizing swarm intelligence and distributed pairwise ranking. This method allows multiple AI models to collaborate and rank their outputs, leading to higher quality responses compared to traditional majority voting methods. Fortytwo also incorporates a reputation system that adjusts node influence based on performance, ensuring that only accurate models contribute to the consensus. Additionally, it includes measures to prevent Sybil attacks, making the system both secure and efficient for decentralized AI applications."
                },
                "zh": {
                    "title": "å»ä¸­å¿ƒåŒ–AIæ¨ç†çš„æ–°çºªå…ƒ",
                    "desc": "éšç€é›†ä¸­å¼äººå·¥æ™ºèƒ½é¢ä¸´è®¡ç®—ç“¶é¢ˆå’Œè®­ç»ƒæ”¶ç›Šé€’å‡ï¼Œæ»¡è¶³éœ€æ±‚éœ€è¦ä¸€ä¸ªèƒ½å¤Ÿæ¨ªå‘æ‰©å±•çš„æ¨ç†å±‚ã€‚æˆ‘ä»¬æå‡ºäº†Fortytwoï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åè®®ï¼Œåˆ©ç”¨ç¾¤ä½“æ™ºèƒ½åŸåˆ™å’Œåˆ†å¸ƒå¼æˆå¯¹æ’åå…±è¯†æ¥å®ç°å“è¶Šçš„AIæ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¾¤ä½“æ¨ç†é‡æ–°æ„æƒ³AIèŠ‚ç‚¹ä¹‹é—´çš„åä½œï¼Œä½¿ç”¨åŒè¡Œæ’åå’Œå£°èª‰åŠ æƒå…±è¯†æ¥æä¾›é«˜è´¨é‡çš„å“åº”ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒFortytwoåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯¹å¯¹æŠ—æ€§å¹²æ‰°çš„å¼ºå¤§æŠµæŠ—åŠ›ï¼Œå¥ å®šäº†å»ä¸­å¿ƒåŒ–AIç³»ç»Ÿçš„åŸºç¡€ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-29.html",
    "link_next": "2025-10-31.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "29.10",
        "en": "10/29",
        "zh": "10æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "31.10",
        "en": "10/31",
        "zh": "10æœˆ31æ—¥"
    },
    "categories": {
        "#dataset": 10,
        "#data": 3,
        "#benchmark": 10,
        "#agents": 8,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 3,
        "#rag": 1,
        "#plp": 0,
        "#inference": 4,
        "#3d": 2,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 9,
        "#math": 3,
        "#multilingual": 2,
        "#architecture": 5,
        "#healthcare": 3,
        "#training": 9,
        "#robotics": 0,
        "#agi": 2,
        "#games": 5,
        "#interpretability": 0,
        "#reasoning": 9,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 11,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    }
}