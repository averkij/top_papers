{
    "date": {
        "ru": "11 Ğ¸ÑĞ½Ñ",
        "en": "June 11",
        "zh": "6æœˆ11æ—¥"
    },
    "time_utc": "2025-06-11 10:13",
    "weekday": 2,
    "issue_id": 4239,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.06751",
            "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
            "url": "https://huggingface.co/papers/2506.06751",
            "abstract": "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper evaluates geopolitical biases in LLMs with respect to various countries though an analysis of their interpretation of historical events with conflicting national perspectives (USA, UK, USSR, and China). We introduce a novel dataset with neutral event descriptions and contrasting viewpoints from different countries. Our findings show significant geopolitical biases, with models favoring specific national narratives. Additionally, simple debiasing prompts had a limited effect in reducing these biases. Experiments with manipulated participant labels reveal models' sensitivity to attribution, sometimes amplifying biases or recognizing inconsistencies, especially with swapped labels. This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and offers a framework and dataset for future geopolitical bias research.",
            "score": 33,
            "issue_id": 4234,
            "pub_date": "2025-06-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ½Ñ",
                "en": "June 7",
                "zh": "6æœˆ7æ—¥"
            },
            "hash": "87a1fbaf018382d4",
            "authors": [
                "Mikhail Salnikov",
                "Dmitrii Korzh",
                "Ivan Lazichny",
                "Elvir Karimov",
                "Artyom Iudin",
                "Ivan Oseledets",
                "Oleg Y. Rogov",
                "Alexander Panchenko",
                "Natalia Loukachevitch",
                "Elena Tutubalina"
            ],
            "affiliations": [
                "AIRI",
                "Kazan Federal University",
                "Lomonosov MSU",
                "MIPT",
                "MTUCI",
                "Sber AI",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06751.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#alignment",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM: Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ°Ğ¼. ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ° Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ Ğ¼Ğ°Ğ»Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Uncovering Geopolitical Biases in Language Models",
                    "desc": "This paper investigates the presence of geopolitical biases in large language models (LLMs) by analyzing how they interpret historical events from different national perspectives, specifically focusing on the USA, UK, USSR, and China. It introduces a new dataset that contains neutral descriptions of events alongside varying viewpoints from these countries to facilitate further research. The results indicate that LLMs tend to favor certain national narratives, demonstrating significant biases in their outputs. Additionally, the study finds that basic debiasing techniques are not very effective in mitigating these biases, suggesting a need for more robust methods in future research."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„åœ°ç¼˜æ”¿æ²»åè§",
                    "desc": "æœ¬è®ºæ–‡è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£é‡Šå†å²äº‹ä»¶æ—¶çš„åœ°ç¼˜æ”¿æ²»åè§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç¾å›½ã€è‹±å›½ã€è‹è”å’Œä¸­å›½ç­‰å›½å®¶çš„ä¸åŒè§†è§’ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…å«ä¸­ç«‹çš„äº‹ä»¶æè¿°å’Œæ¥è‡ªä¸åŒå›½å®¶çš„å¯¹ç«‹è§‚ç‚¹ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹å€¾å‘äºæ”¯æŒç‰¹å®šå›½å®¶çš„å™äº‹ï¼Œä¸”ç®€å•çš„å»åè§æ–¹æ³•æ•ˆæœæœ‰é™ã€‚é€šè¿‡æ“æ§å‚ä¸è€…æ ‡ç­¾çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹å¯¹å½’å› éå¸¸æ•æ„Ÿï¼Œæœ‰æ—¶ä¼šæ”¾å¤§åè§æˆ–è¯†åˆ«ä¸ä¸€è‡´ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡ç­¾äº¤æ¢çš„æƒ…å†µä¸‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08672",
            "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling",
            "url": "https://huggingface.co/papers/2506.08672",
            "abstract": "RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.  \t\t\t\t\tAI-generated summary \t\t\t\t Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1% average points on eight ID tasks and Delta10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.",
            "score": 14,
            "issue_id": 4239,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "f9f7805577b3b091",
            "authors": [
                "Yang Liu",
                "Jiaqi Li",
                "Zilong Zheng"
            ],
            "affiliations": [
                "NLCo Lab, Beijing Institute for General Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08672.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#small_models",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "RuleReasoner - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ÑÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. RuleReasoner Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Boosting Small Models with Dynamic Domain Sampling",
                    "desc": "RuleReasoner is a method that improves rule-based reasoning in small models by using dynamic domain sampling. It addresses the challenges posed by varying rule formats and complexities in real-world applications. By updating sampling weights based on historical rewards, RuleReasoner enhances the learning process and generalization across different tasks. Empirical results show that it significantly outperforms large reasoning models while being more computationally efficient."
                },
                "zh": {
                    "title": "å°æ¨¡å‹çš„è§„åˆ™æ¨ç†æ–°çªç ´",
                    "desc": "RuleReasoneræ˜¯ä¸€ç§å¢å¼ºå°å‹æ¨¡å‹è§„åˆ™æ¨ç†çš„æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€é¢†åŸŸé‡‡æ ·å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–è§„åˆ™æ¨ç†ï¼Œè§£å†³äº†ç°å®åº”ç”¨ä¸­è§„åˆ™æ ¼å¼å’Œå¤æ‚æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é€šè¿‡æ›´æ–°ä¸åŒé¢†åŸŸçš„é‡‡æ ·æƒé‡ï¼ŒRuleReasonerèƒ½å¤Ÿçµæ´»åœ°è¿›è¡Œåœ¨çº¿å­¦ä¹ ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­éœ€è¦äººå·¥è®¾è®¡çš„æ··åˆè®­ç»ƒæ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼ŒRuleReasoneråœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09040",
            "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
            "url": "https://huggingface.co/papers/2506.09040",
            "abstract": "Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.",
            "score": 12,
            "issue_id": 4237,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "09d042607d92f156",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#interpretability",
                    "#benchmark",
                    "#games",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ (ASVR) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ° Ğ½Ğµ Ğ½Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ¿ÑƒÑ‰ĞµĞ½Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑÑ…. ASVR Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Understanding with Semantic Focus",
                    "desc": "The paper introduces Autoregressive Semantic Visual Reconstruction (ASVR), a method that enhances multimodal understanding by focusing on reconstructing the semantic content of images rather than their raw visual appearance. This approach addresses limitations in existing large vision-language models (LVLMs) that primarily rely on textual sequences, which can lead to missing critical visual details. By enabling joint learning of visual and textual modalities, ASVR allows models to effectively reconstruct discrete semantic tokens from continuous image features, leading to improved comprehension. The results show significant performance gains across various benchmarks, demonstrating the effectiveness of semantic reconstruction in multimodal tasks."
                },
                "zh": {
                    "title": "è‡ªå›å½’è¯­ä¹‰é‡å»ºï¼Œæå‡å¤šæ¨¡æ€ç†è§£ï¼",
                    "desc": "è‡ªå›å½’è¯­ä¹‰è§†è§‰é‡å»ºï¼ˆASVRï¼‰é€šè¿‡å…³æ³¨è¯­ä¹‰é‡å»ºè€ŒéåŸå§‹è§†è§‰å¤–è§‚ï¼Œæå‡äº†å¤šæ¨¡æ€ç†è§£çš„èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä»…å¯¹æ–‡æœ¬åºåˆ—åº”ç”¨è‡ªå›å½’ç›‘ç£ï¼Œæœªèƒ½å……åˆ†æ•´åˆè§†è§‰æ¨¡æ€ï¼Œå¯¼è‡´æ— æ³•åˆ©ç”¨æ²¡æœ‰é…å¥—è¯´æ˜çš„å›¾åƒã€‚ASVR é€šè¿‡åœ¨ç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶å†…å®ç°è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„è”åˆå­¦ä¹ ï¼Œå…‹æœäº†è¿™ä¸€é™åˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé‡å»ºå›¾åƒçš„è¯­ä¹‰è¡¨ç¤ºèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤šæ¨¡æ€ç†è§£çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08009",
            "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
            "url": "https://huggingface.co/papers/2506.08009",
            "abstract": "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/",
            "score": 9,
            "issue_id": 4235,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "e63130d065bba1fd",
            "authors": [
                "Xun Huang",
                "Zhengqi Li",
                "Guande He",
                "Mingyuan Zhou",
                "Eli Shechtman"
            ],
            "affiliations": [
                "Adobe Research",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08009.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Self Forcing: Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Self Forcing. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ñ…. Self Forcing Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾."
                },
                "en": {
                    "title": "Self Forcing: Enhancing Video Generation with Autoregressive Training",
                    "desc": "The paper presents Self Forcing, a new training method for autoregressive video diffusion models that aims to reduce exposure bias and enhance video generation quality. It allows models to generate video frames based on their own previously generated outputs instead of relying solely on ground-truth frames, which helps in better training. By using a holistic video-level supervision approach, the method evaluates the quality of the entire video sequence rather than just individual frames. Additionally, it incorporates efficient caching mechanisms and a few-step diffusion model to optimize performance and ensure real-time video generation on a single GPU."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å¼ºåˆ¶ï¼šæå‡è§†é¢‘ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "Self Forcingæ˜¯ä¸€ç§æ–°é¢–çš„è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘æ›å…‰åå·®å¹¶æé«˜ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´ä½“è§†é¢‘çº§ç›‘ç£å’Œé«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼Œè§£å†³äº†æ¨¡å‹åœ¨æ¨ç†æ—¶å¿…é¡»ä¾èµ–è‡ªèº«ä¸å®Œç¾è¾“å‡ºç”Ÿæˆåºåˆ—çš„é—®é¢˜ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒSelf Forcingåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨è‡ªç”Ÿæˆè¾“å‡ºè¿›è¡Œæ¡ä»¶ç”Ÿæˆï¼Œä»è€Œå®ç°è§†é¢‘çº§çš„æ•´ä½“æŸå¤±è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šå®ç°å®æ—¶è§†é¢‘ç”Ÿæˆï¼Œä¸”ç”Ÿæˆè´¨é‡ä¼˜äºä¼ ç»Ÿçš„éå› æœæ‰©æ•£æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07927",
            "title": "Solving Inequality Proofs with Large Language Models",
            "url": "https://huggingface.co/papers/2506.07927",
            "abstract": "The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.",
            "score": 9,
            "issue_id": 4235,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "0171dcd88ad3a14f",
            "authors": [
                "Jiayi Sheng",
                "Luna Lyu",
                "Jikai Jin",
                "Tony Xia",
                "Alex Gu",
                "James Zou",
                "Pan Lu"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Stanford University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07927.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ½Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IneqMath Ñ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 10% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒÑ‡ĞµÑ‚Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ LLM Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾."
                },
                "en": {
                    "title": "Bridging the Gap: From Answers to Rigorous Proofs in Inequality Proving",
                    "desc": "This paper explores the challenges faced by large language models (LLMs) in the domain of inequality proving, which is essential for advanced reasoning in mathematics and science. It identifies a significant gap between generating answers and producing valid, step-by-step proofs, highlighting the limitations of current datasets that are often inadequate. The authors propose a new task formulation that breaks down inequality proving into two checkable subtasks: bound estimation and relation prediction, and introduce the IneqMath dataset for training and evaluation. Their evaluation of 29 leading LLMs reveals that even the best models struggle with rigorous proof construction, achieving less than 10% accuracy when assessed on step-wise reasoning, indicating a need for improved methodologies in theorem-guided reasoning and self-refinement."
                },
                "zh": {
                    "title": "æ­ç¤ºä¸ç­‰å¼è¯æ˜ä¸­çš„æ¨ç†æŒ‘æˆ˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸ç­‰å¼è¯æ˜çš„æŒ‘æˆ˜ï¼Œæ­ç¤ºäº†æ‰¾åˆ°ç­”æ¡ˆä¸ç”Ÿæˆæœ‰æ•ˆé€æ­¥è§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·ã€‚ä¸ç­‰å¼è¯æ˜åœ¨ç§‘å­¦å’Œæ•°å­¦é¢†åŸŸè‡³å…³é‡è¦ï¼Œè€ƒéªŒç€é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œå¦‚å‘ç°ç´§ç•Œå’Œæˆ˜ç•¥æ€§å®šç†åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹ç°æœ‰æ•°æ®é›†ç¨€ç¼ºå’Œå½¢å¼åŒ–çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§éæ­£å¼ä½†å¯éªŒè¯çš„ä»»åŠ¡å½¢å¼ï¼Œå°†ä¸ç­‰å¼è¯æ˜é‡æ„ä¸ºä¸¤ä¸ªå¯è‡ªåŠ¨æ£€æŸ¥çš„å­ä»»åŠ¡ï¼šç•Œé™ä¼°è®¡å’Œå…³ç³»é¢„æµ‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†IneqMathæ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ£€æµ‹å¸¸è§çš„æ¨ç†ç¼ºé™·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04614",
            "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
            "url": "https://huggingface.co/papers/2506.04614",
            "abstract": "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency.",
            "score": 9,
            "issue_id": 4236,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "9eb1f2457722a2cd",
            "authors": [
                "Yuyang Wanyan",
                "Xi Zhang",
                "Haiyang Xu",
                "Haowei Liu",
                "Junyang Wang",
                "Jiabo Ye",
                "Yutong Kou",
                "Ming Yan",
                "Fei Huang",
                "Xiaoshan Yang",
                "Weiming Dong",
                "Changsheng Xu"
            ],
            "affiliations": [
                "Alibaba Group",
                "Beijing Jiaotong University",
                "MAIS, Institute of Automation, Chinese Academy of Sciences, China",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04614.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (GUI). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ (S-GRPO) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GUI-Critic-R1. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±ÑƒÑ‚ÑÑ‚Ñ€ÑĞ¿Ğ¿Ğ¸Ğ½Ğ³Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GUI-Critic-Train Ğ¸ GUI-Critic-Test. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GUI-Critic-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI."
                },
                "en": {
                    "title": "Enhancing GUI Automation Reliability with Pre-operative Critique",
                    "desc": "This paper presents a new method to improve the reliability of GUI automation using a pre-operative critic mechanism. The mechanism provides feedback before actions are executed, helping to prevent errors that could lead to serious issues like unwanted deletions. The authors introduce a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to enhance the feedback process, making it more effective. Experiments show that their model, GUI-Critic-R1, significantly outperforms existing models in both accuracy and operational efficiency during GUI automation tasks."
                },
                "zh": {
                    "title": "æå‡GUIè‡ªåŠ¨åŒ–å¯é æ€§çš„é¢„æ“ä½œè¯„ä¼°æœºåˆ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢„æ“ä½œè¯„ä¼°æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–ä¸­çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡çš„å¯é æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºå»ºè®®æ„ŸçŸ¥æ¢¯åº¦ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰çš„ç­–ç•¥ï¼Œä»¥æ„å»ºé¢„æ“ä½œè¯„ä¼°æ¨¡å‹GUI-Critic-R1ï¼Œå¹¶é€šè¿‡å¼•å…¥æ–°é¢–çš„å»ºè®®å¥–åŠ±æ¥å¢å¼ºæ¨¡å‹åé¦ˆçš„å¯é æ€§ã€‚è¯¥æœºåˆ¶åœ¨å®é™…æ‰§è¡Œä¹‹å‰æä¾›æœ‰æ•ˆåé¦ˆï¼Œå¸®åŠ©æ¨ç†æ½œåœ¨ç»“æœå’Œè¡ŒåŠ¨çš„æ­£ç¡®æ€§ï¼Œä»è€Œå‡å°‘å†³ç­–é”™è¯¯çš„é£é™©ã€‚é€šè¿‡åœ¨ç§»åŠ¨å’Œç½‘é¡µé¢†åŸŸçš„é™æ€å®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯„ä¼°å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08002",
            "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
            "url": "https://huggingface.co/papers/2506.08002",
            "abstract": "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/",
            "score": 8,
            "issue_id": 4232,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "ab2da61a8a27783d",
            "authors": [
                "Aadarsh Sahoo",
                "Vansh Tibrewal",
                "Georgia Gkioxari"
            ],
            "affiliations": [
                "California Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08002.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#synthetic",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-ÑÑ†ĞµĞ½, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… 3D-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D-Ñ„Ğ¾Ñ€Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unifying Language, Image, and 3D Understanding for Enhanced Machine Interaction",
                    "desc": "This paper presents a unified framework that integrates language, images, and 3D scenes to enhance machine understanding of three-dimensional environments. By leveraging autoregressive models, the authors explore how to effectively represent and process structured 3D scenes alongside traditional modalities. The framework includes a comprehensive guide for optimal training and performance, addressing critical aspects like data representation and modality-specific objectives. The model is evaluated on various 3D tasks, demonstrating its capability in rendering, recognition, instruction-following, and question-answering across both synthetic and real-world datasets."
                },
                "zh": {
                    "title": "ç»Ÿä¸€ä¸‰ç»´åœºæ™¯æ¨¡å‹ï¼Œæå‡AIç†è§£èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¯­è¨€ã€å›¾åƒå’Œä¸‰ç»´åœºæ™¯æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å„ç§ä¸‰ç»´ä»»åŠ¡å’Œæ•°æ®é›†çš„æœ€ä½³è®­ç»ƒå’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è‡ªå›å½’æ¨¡å‹ï¼Œæ¢ç´¢äº†ç»“æ„åŒ–ä¸‰ç»´åœºæ™¯çš„æ–°æ¨¡å¼ï¼Œå¸®åŠ©è®¾è®¡å¸ˆæ„å»ºå’Œç¼–è¾‘ä¸‰ç»´ç¯å¢ƒã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä»½è¯¦ç»†çš„â€œé£Ÿè°±â€ï¼Œé˜è¿°äº†å®ç°æœ€ä½³è®­ç»ƒå’Œæ€§èƒ½çš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œå¹¶è¯„ä¼°äº†åœ¨å››ä¸ªæ ¸å¿ƒä¸‰ç»´ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡ä¸°å¯Œä¸‰ç»´æ¨¡æ€çš„é‡åŒ–å½¢çŠ¶ç¼–ç ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤æ‚ä¸‰ç»´ç‰©ä½“è¯†åˆ«ä»»åŠ¡ä¸­å±•ç°äº†æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07177",
            "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
            "url": "https://huggingface.co/papers/2506.07177",
            "abstract": "Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals.",
            "score": 8,
            "issue_id": 4233,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ½Ñ",
                "en": "June 8",
                "zh": "6æœˆ8æ—¥"
            },
            "hash": "7d83fcff01c3595a",
            "authors": [
                "Sangwon Jang",
                "Taekyung Ki",
                "Jaehyeong Jo",
                "Jaehong Yoon",
                "Soo Ye Kim",
                "Zhe Lin",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "Adobe Research",
                "DeepAuto.ai",
                "KAIST",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07177.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Frame Guidance Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸Ğ»Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Frame Guidance Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Effortless Control in Video Generation with Frame Guidance",
                    "desc": "Frame Guidance introduces a novel approach to video generation that does not require any training, allowing for effective control using frame-level signals. This method significantly reduces memory usage while ensuring that the generated videos maintain global coherence. By utilizing a simple latent processing technique and a unique latent optimization strategy, Frame Guidance can adapt to various tasks such as keyframe guidance and stylization. The results demonstrate that this approach can produce high-quality videos across different input types without the need for fine-tuning large models."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ§åˆ¶æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFrame Guidanceçš„æ–¹æ³•ï¼Œç”¨äºæ§åˆ¶è§†é¢‘ç”Ÿæˆï¼Œä¸”æ— éœ€è®­ç»ƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¸§çº§ä¿¡å·ï¼Œå¦‚å…³é”®å¸§å’Œé£æ ¼å‚è€ƒå›¾åƒï¼Œæ¥å®ç°å¯¹è§†é¢‘ç”Ÿæˆçš„ç²¾ç»†æ§åˆ¶ã€‚Frame Guidanceæ˜¾è‘—é™ä½äº†å†…å­˜ä½¿ç”¨ï¼Œå¹¶å¢å¼ºäº†è§†é¢‘è¾“å‡ºçš„å…¨å±€ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­ç”Ÿæˆé«˜è´¨é‡çš„å¯æ§è§†é¢‘ï¼Œä¸”ä¸ä»»ä½•è§†é¢‘æ¨¡å‹å…¼å®¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05167",
            "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
            "url": "https://huggingface.co/papers/2506.05167",
            "abstract": "ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.",
            "score": 6,
            "issue_id": 4231,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "d979315df3a92206",
            "authors": [
                "Yeonseok Jeong",
                "Jinsu Kim",
                "Dohyeon Lee",
                "Seung-won Hwang"
            ],
            "affiliations": [
                "IPAI, Seoul National University",
                "Korea University",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05167.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#alignment",
                    "#long_context"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ECoRAG: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²",
                    "desc": "ECoRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° (ODQA). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ECoRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ODQA. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°."
                },
                "en": {
                    "title": "ECoRAG: Elevating LLMs with Evidence-Based Compression",
                    "desc": "The ECoRAG framework enhances the performance of Large Language Models (LLMs) in Open-Domain Question Answering (ODQA) by focusing on evidentiality during document retrieval and compression. By filtering out non-evidential information, ECoRAG ensures that the generated answers are supported by relevant evidence, improving the overall accuracy of responses. Additionally, the framework optimizes resource usage by reducing latency and minimizing token consumption, making it more efficient than previous methods. Experiments demonstrate that ECoRAG significantly outperforms existing compression techniques in ODQA tasks."
                },
                "zh": {
                    "title": "ECoRAGï¼šæå‡é—®ç­”æ€§èƒ½çš„è¯æ®æ€§å‹ç¼©æ¡†æ¶",
                    "desc": "ECoRAGæ¡†æ¶é€šè¿‡åŸºäºè¯æ®æ€§å‹ç¼©æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼€æ”¾é¢†åŸŸé—®ç­”ï¼ˆODQAï¼‰ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä»¥å¾€å‹ç¼©æŠ€æœ¯æœªèƒ½æœ‰æ•ˆè¿‡æ»¤éè¯æ®æ€§ä¿¡æ¯çš„é—®é¢˜ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚ECoRAGç¡®ä¿ç”Ÿæˆçš„ç­”æ¡ˆæœ‰è¶³å¤Ÿçš„è¯æ®æ”¯æŒï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œé¢å¤–çš„æ–‡æ¡£æ£€ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECoRAGåœ¨ODQAä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„å‹ç¼©æ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†å»¶è¿Ÿå’Œä»¤ç‰Œä½¿ç”¨ï¼Œå…·æœ‰å¾ˆé«˜çš„æˆæœ¬æ•ˆç›Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08887",
            "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
            "url": "https://huggingface.co/papers/2506.08887",
            "abstract": "The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA.",
            "score": 4,
            "issue_id": 4232,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "068c2f58bc5049d2",
            "authors": [
                "Leqi Shen",
                "Guoqiang Gong",
                "Tianxiang Hao",
                "Tao He",
                "Yifeng Zhang",
                "Pengzhang Liu",
                "Sicheng Zhao",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "affiliations": [
                "BNRist",
                "Department of Automation, Tsinghua University",
                "GRG Banking Equipment Co., Ltd.",
                "Hangzhou Zhuoxi Institute of Brain and Intelligence",
                "JD.com",
                "School of Software",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08887.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#alignment",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiscoVLA - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸, ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾. DiscoVLA Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiscoVLA Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼."
                },
                "en": {
                    "title": "Bridging Gaps in Video-Text Retrieval with DiscoVLA",
                    "desc": "The paper introduces DiscoVLA, a method designed to enhance video-text retrieval by addressing three main discrepancies: vision, language, and alignment. Unlike existing approaches that primarily focus on visual aspects, DiscoVLA integrates both image and video features to improve understanding at the video level. It also generates pseudo image captions to refine image-level alignment and employs Image-to-Video Alignment Distillation to strengthen video-level alignment using knowledge from image-level data. Experimental results show that DiscoVLA significantly outperforms previous methods, achieving a notable improvement in retrieval accuracy."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘-æ–‡æœ¬æ£€ç´¢çš„DiscoVLAæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDiscoVLAçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è§£å†³è§†è§‰ã€è¯­è¨€å’Œå¯¹é½çš„å·®å¼‚æ¥æ”¹å–„è§†é¢‘-æ–‡æœ¬æ£€ç´¢ã€‚ä¼ ç»Ÿçš„CLIPæ¨¡å‹ä¸»è¦å…³æ³¨å›¾åƒçº§åˆ«çš„è§†è§‰-è¯­è¨€åŒ¹é…ï¼Œè€Œè§†é¢‘-æ–‡æœ¬æ£€ç´¢éœ€è¦åœ¨è§†é¢‘çº§åˆ«ä¸Šè¿›è¡Œå…¨é¢ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å›¾åƒ-è§†é¢‘ç‰¹å¾èåˆæ¥æ•´åˆå›¾åƒå’Œè§†é¢‘çš„ç‰¹å¾ï¼Œæœ‰æ•ˆåº”å¯¹è§†è§‰å’Œè¯­è¨€çš„å·®å¼‚ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ç”Ÿæˆä¼ªå›¾åƒæ ‡é¢˜ä»¥å­¦ä¹ ç»†ç²’åº¦çš„å›¾åƒçº§åˆ«å¯¹é½ï¼Œä»è€Œå‡è½»å¯¹é½å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiscoVLAåœ¨è§†é¢‘-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨MSRVTTæ•°æ®é›†ä¸Šå–å¾—äº†50.5%çš„R@1æˆç»©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07932",
            "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
            "url": "https://huggingface.co/papers/2506.07932",
            "abstract": "A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object.",
            "score": 2,
            "issue_id": 4233,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "a13860cb07518cb2",
            "authors": [
                "Rishit Dagli",
                "Yushi Guan",
                "Sankeerth Durvasula",
                "Mohammadreza Mofayezi",
                "Nandita Vijaykumar"
            ],
            "affiliations": [
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07932.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Squeeze3D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… ÑÑ‚ĞµĞ¿ĞµĞ½ĞµĞ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞµÑ‚Ğ¾Ğº, Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Squeeze3D Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ°ĞºĞ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Efficient 3D Data Compression with Squeeze3D",
                    "desc": "Squeeze3D is a new framework designed to compress 3D data efficiently using pre-trained models. It connects the latent spaces of a pre-trained encoder and a generative model through trainable mapping networks, allowing for high compression ratios while preserving visual quality. The framework can handle various 3D representations, such as meshes and point clouds, and is trained solely on synthetic data, eliminating the need for specific 3D datasets. Experiments show that Squeeze3D achieves impressive compression rates, making it a versatile tool for 3D data compression."
                },
                "zh": {
                    "title": "Squeeze3Dï¼šé«˜æ•ˆå‹ç¼©3Dæ•°æ®çš„æ–°æ–¹æ³•",
                    "desc": "Squeeze3Dæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹é«˜æ•ˆå‹ç¼©3Dæ•°æ®ï¼Œè¾¾åˆ°é«˜å‹ç¼©æ¯”å¹¶ä¿æŒè§†è§‰è´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯è®­ç»ƒçš„æ˜ å°„ç½‘ç»œè¿æ¥é¢„è®­ç»ƒç¼–ç å™¨å’Œç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„æ½œåœ¨ç©ºé—´ã€‚3Dæ¨¡å‹é¦–å…ˆè¢«ç¼–ç ä¸ºç´§å‡‘çš„æ½œåœ¨ä»£ç ï¼Œç„¶åé€šè¿‡æ˜ å°„ç½‘ç»œè½¬æ¢ä¸ºç”Ÿæˆæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œä»¥é‡å»ºåŸå§‹3Dæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒSqueeze3Dåœ¨ä¸åŒæ ¼å¼çš„3Dæ•°æ®ä¸Šå®ç°äº†æ˜¾è‘—çš„å‹ç¼©æ•ˆæœï¼ŒåŒæ—¶å»¶è¿Ÿè¾ƒå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05928",
            "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
            "url": "https://huggingface.co/papers/2506.05928",
            "abstract": "A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ homogeneous MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a heterogeneous Mixture-of-Adapters (MoA) approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: (i) Soft MoA achieves fine-grained integration by performing a weighted fusion of all expert outputs; (ii) Sparse MoA activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA.",
            "score": 2,
            "issue_id": 4232,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "f78e6f70ebb69ac2",
            "authors": [
                "Jie Cao",
                "Tianwei Lin",
                "Hongyang He",
                "Rolan Yan",
                "Wenqiao Zhang",
                "Juncheng Li",
                "Dongping Zhang",
                "Siliang Tang",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Tencent",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05928.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#training",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ°Ñ ÑĞ¼ĞµÑÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½ÑƒÑ Mixture-of-Adapters (MoA). MoA Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ³Ğ¾Ğ¼Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² MoE-LoRA. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° MoA: Soft MoA Ñ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²ÑĞµÑ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Sparse MoA Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MoA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¾Ğ¼Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ MoE-LoRA ĞºĞ°Ğº Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Diverse Adapter Experts",
                    "desc": "This paper introduces a new approach called heterogeneous Mixture-of-Adapters (MoA) for fine-tuning Large Language Models (LLMs) more efficiently. Unlike traditional homogeneous MoE-LoRA methods that use similar adapter experts, MoA combines diverse adapter structures to improve performance and prevent issues like representation collapse. The method includes two variants: Soft MoA, which fuses outputs from all experts, and Sparse MoA, which selectively activates experts based on their effectiveness. Experimental results show that MoA significantly outperforms existing methods in both performance and parameter efficiency."
                },
                "zh": {
                    "title": "å¼‚æ„é€‚é…å™¨æ··åˆï¼šæå‡å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¼‚æ„çš„é€‚é…å™¨æ··åˆï¼ˆMoAï¼‰æ–¹æ³•ï¼Œä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„åŒè´¨MoE-LoRAæ–¹æ³•ä¸åŒï¼ŒMoAé›†æˆäº†å…·æœ‰ä¸åŒç»“æ„çš„é€‚é…å™¨ä¸“å®¶ï¼Œä»è€Œå…‹æœäº†è¡¨ç¤ºå´©æºƒå’Œä¸“å®¶è´Ÿè½½ä¸å¹³è¡¡çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€æ•´åˆé€‚é…å™¨ä¸“å®¶çš„äº’è¡¥è¡¨ç¤ºèƒ½åŠ›ï¼Œä¿ƒè¿›äº†ä¸“å®¶çš„ä¸“ä¸šåŒ–ï¼Œæå‡äº†é¢„è®­ç»ƒçŸ¥è¯†å‘ä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆè½¬ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼‚æ„MoAåœ¨æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ä¸Šå‡ä¼˜äºåŒè´¨MoE-LoRAæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08300",
            "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability",
            "url": "https://huggingface.co/papers/2506.08300",
            "abstract": "Institutional Books 1.0 provides a large dataset of public domain books from Harvard Library for training and inference of large language models, enhancing data accessibility and sustainability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use.",
            "score": 1,
            "issue_id": 4239,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "8e42e765c2efbe2a",
            "authors": [
                "Matteo Cargnelutti",
                "Catherine Brobston",
                "John Hess",
                "Jack Cushman",
                "Kristi Mukk",
                "Aristana Scourtas",
                "Kyle Courtney",
                "Greg Leppert",
                "Amanda Watson",
                "Martha Whitehead",
                "Jonathan Zittrain"
            ],
            "affiliations": [
                "Harvard Law School Library",
                "Harvard Law School, Harvard School of Engineering and Applied Sciences, Harvard Kennedy School",
                "Harvard Library",
                "Institutional Data Initiative, Harvard Law School Library",
                "Library Innovation Lab, Harvard Law School Library"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08300.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#synthetic",
                    "#data"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ",
                    "desc": "Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Institutional Books 1.0 Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½ÑƒÑ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ ĞºĞ½Ğ¸Ğ³ Ğ¸Ğ· Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ“Ğ°Ñ€Ğ²Ğ°Ñ€Ğ´ÑĞºĞ¾Ğ¹ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 983,004 Ñ‚Ğ¾Ğ¼Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 250 ÑĞ·Ñ‹ĞºĞ°Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ğ¾ĞºĞ¾Ğ»Ğ¾ 242 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ OCR-Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ½Ğ¸Ğ³, Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ…ÑÑ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ¸Ğµ ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Knowledge: A Sustainable Dataset for Language Models",
                    "desc": "Institutional Books 1.0 is a comprehensive dataset of public domain books sourced from Harvard Library, aimed at improving the training and inference processes of large language models (LLMs). The dataset includes over 983,000 volumes and approximately 242 billion tokens, providing a rich resource for enhancing the quality and diversity of training data. By ensuring clear provenance and sustainable practices in data stewardship, this initiative addresses the critical need for high-quality, publicly available datasets in the rapidly evolving field of machine learning. The project not only facilitates better model performance but also promotes accessibility to historical texts for both human and machine use."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹çš„æ•°æ®å¯ç”¨æ€§ä¸å¯æŒç»­æ€§",
                    "desc": "Institutional Books 1.0 æ˜¯ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå“ˆä½›å›¾ä¹¦é¦†çš„å…¬å…±é¢†åŸŸä¹¦ç±ï¼Œæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æä¾›æ•°æ®æ”¯æŒã€‚è¿™äº›ä¹¦ç±çš„å¤šæ ·æ€§å’Œè´¨é‡ç›´æ¥å½±å“åˆ°è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼Œå› æ­¤éœ€è¦é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚è¯¥é¡¹ç›®ä»å“ˆä½›å›¾ä¹¦é¦†æå–å’Œå¤„ç†äº†è¶…è¿‡983,000æœ¬ä¹¦ç±çš„æ–‡æœ¬å’Œå…ƒæ•°æ®ï¼Œç¡®ä¿æ•°æ®çš„å¯è®¿é—®æ€§å’Œå¯æŒç»­æ€§ã€‚é€šè¿‡è¿™ä¸ªæ•°æ®é›†ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥æ›´æ–¹ä¾¿åœ°ä½¿ç”¨å†å²æ–‡æœ¬ï¼Œæ¨åŠ¨æœºå™¨å­¦ä¹ çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07047",
            "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
            "url": "https://huggingface.co/papers/2506.07047",
            "abstract": "Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.",
            "score": 1,
            "issue_id": 4237,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ½Ñ",
                "en": "June 8",
                "zh": "6æœˆ8æ—¥"
            },
            "hash": "d3bc82dde4f2b8bc",
            "authors": [
                "Yu Xuejun",
                "Jianyuan Zhong",
                "Zijin Feng",
                "Pengyi Zhai",
                "Roozbeh Yousefzadeh",
                "Wei Chong Ng",
                "Haoxiong Liu",
                "Ziyi Shou",
                "Jing Xiong",
                "Yudong Zhou",
                "Claudia Beth Ong",
                "Austen Jeremy Sugiarto",
                "Yaoxi Zhang",
                "Wai Ming Tai",
                "Huan Cao",
                "Dongcai Lu",
                "Jiacheng Sun",
                "Qiang Xu",
                "Shen Xin",
                "Zhenguo Li"
            ],
            "affiliations": [
                "Huawei Celia Team",
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07047.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#dataset",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¾Ñ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Mathesis - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Mathesis-Autoformalizer, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Mathesis-Prover Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Gaokao-Formal Ğ¸Ğ· 488 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ²ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ° Ğ² Ğ²ÑƒĞ·Ñ‹."
                },
                "en": {
                    "title": "Bridging Natural Language and Formal Reasoning with Mathesis",
                    "desc": "This paper presents Mathesis, a novel end-to-end theorem proving system that processes informal problem statements, addressing the limitations of existing LLM-based theorem provers. It introduces Mathesis-Autoformalizer, which utilizes reinforcement learning to automatically convert natural language problems into formal statements, supported by the LeanScorer framework for assessing formalization quality. Additionally, the Mathesis-Prover generates formal proofs from these formalized statements. The system's effectiveness is validated through experiments on the Gaokao-Formal benchmark, demonstrating significant improvements in accuracy and pass rates compared to existing methods."
                },
                "zh": {
                    "title": "Mathesisï¼šéæ­£å¼é—®é¢˜çš„å®šç†è¯æ˜æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†Mathesisï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†éæ­£å¼é—®é¢˜é™ˆè¿°çš„ç«¯åˆ°ç«¯å®šç†è¯æ˜ç®¡é“ã€‚å®ƒåŒ…æ‹¬Mathesis-Autoformalizerï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨å½¢å¼åŒ–å·¥å…·ï¼Œèƒ½å¤Ÿæé«˜è‡ªç„¶è¯­è¨€é—®é¢˜çš„å½¢å¼åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡LeanScoreræ¡†æ¶è¯„ä¼°å½¢å¼åŒ–è´¨é‡ã€‚è®ºæ–‡è¿˜æå‡ºäº†Mathesis-Proverï¼Œèƒ½å¤Ÿä»å½¢å¼åŒ–çš„é™ˆè¿°ä¸­ç”Ÿæˆæ­£å¼è¯æ˜ã€‚é€šè¿‡åœ¨ä¸­å›½é«˜è€ƒçš„488ä¸ªå¤æ‚é—®é¢˜ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜Mathesisåœ¨é€šè¿‡ç‡å’Œå‡†ç¡®æ€§ä¸Šå‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05700",
            "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
            "url": "https://huggingface.co/papers/2506.05700",
            "abstract": "RKEFino1, a knowledge-enhanced financial reasoning model, addresses accuracy and compliance challenges in Digital Regulatory Reporting through fine-tuning with domain knowledge from XBRL, CDM, and MOF, and introduces a novel Numerical NER task.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) hold great promise for financial applications but introduce critical accuracy and compliance challenges in Digital Regulatory Reporting (DRR). To address these issues, we propose RKEFino1, a regulation knowledge-enhanced financial reasoning model built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce a novel Numerical NER task covering financial entities in both sentences and tables. Experimental results demonstrate the effectiveness and generalization capacity of RKEFino1 in compliance-critical financial tasks. We have released our model on Hugging Face.",
            "score": 1,
            "issue_id": 4231,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "a23a28bb68811316",
            "authors": [
                "Yan Wang",
                "Yueru He",
                "Ruoyu Xiang",
                "Jeff Zhao"
            ],
            "affiliations": [
                "Columbia University",
                "New York University",
                "The University of Texas at Austin",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05700.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#reasoning",
                    "#training",
                    "#healthcare",
                    "#open_source"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "RKEFino1 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Fino1. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼ Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸Ğ· XBRL, CDM Ğ¸ MOF. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. RKEFino1 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ (NER) Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Financial Compliance with RKEFino1",
                    "desc": "RKEFino1 is a financial reasoning model designed to improve accuracy and compliance in Digital Regulatory Reporting (DRR). It enhances the Fino1 model by incorporating domain knowledge from XBRL, CDM, and MOF, which are essential for understanding financial regulations. The model introduces a new task called Numerical Named Entity Recognition (NER) to identify financial entities in both text and tabular formats. Experimental results show that RKEFino1 effectively addresses compliance challenges and generalizes well to various financial tasks."
                },
                "zh": {
                    "title": "çŸ¥è¯†å¢å¼ºçš„é‡‘èæ¨ç†ï¼Œæå‡åˆè§„æ€§ä¸å‡†ç¡®æ€§",
                    "desc": "RKEFino1æ˜¯ä¸€ç§å¢å¼ºçŸ¥è¯†çš„é‡‘èæ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ•°å­—ç›‘ç®¡æŠ¥å‘Šä¸­çš„å‡†ç¡®æ€§å’Œåˆè§„æ€§æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åŸºäºFino1ï¼Œå¹¶é€šè¿‡XBRLã€CDMå’ŒMOFç­‰é¢†åŸŸçŸ¥è¯†è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé—®ç­”ä»»åŠ¡â€”â€”åŸºäºçŸ¥è¯†çš„é—®ç­”å’Œæ•°å­¦æ¨ç†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°å€¼å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼Œæ¶µç›–äº†å¥å­å’Œè¡¨æ ¼ä¸­çš„é‡‘èå®ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRKEFino1åœ¨åˆè§„æ€§å…³é”®çš„é‡‘èä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07976",
            "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
            "url": "https://huggingface.co/papers/2506.07976",
            "abstract": "Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.  \t\t\t\t\tAI-generated summary \t\t\t\t The current paradigm of test-time scaling relies on generating long reasoning traces (\"thinking\" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.",
            "score": 0,
            "issue_id": 4239,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "b35082cb7ca5bb65",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (Test-Time Interaction, TTI). TTI Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ TTI Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WebVoyager Ğ¸ WebArena, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemma 3 12B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Web Agents with Test-Time Interaction",
                    "desc": "This paper introduces Test-Time Interaction (TTI), a novel approach that enhances the performance of web agents by allowing them to interact more effectively with their environment. Unlike traditional methods that focus on generating long reasoning traces before acting, TTI enables agents to adapt their behavior in real-time by increasing their interaction horizon. This method supports complex behaviors such as exploration, backtracking, and dynamic re-planning within a single decision-making process. The authors demonstrate that TTI significantly improves task success rates on web benchmarks, showcasing its potential as a complementary strategy to existing scaling techniques in reinforcement learning."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶äº¤äº’ï¼šæå‡ä»£ç†æ™ºèƒ½çš„æ–°ç»´åº¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæµ‹è¯•æ—¶äº¤äº’ï¼ˆTTIï¼‰ï¼Œæ—¨åœ¨æé«˜ç½‘ç»œä»£ç†çš„æ€§èƒ½ã€‚TTIé€šè¿‡æ‰©å±•ä»£ç†çš„äº¤äº’èŒƒå›´ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å•æ¬¡æ‰§è¡Œä¸­è¿›è¡Œæ¢ç´¢ã€å›æº¯å’ŒåŠ¨æ€é‡æ–°è§„åˆ’ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–é•¿æ¨ç†è½¨è¿¹çš„æ–¹å¼ä¸åŒï¼ŒTTIå…è®¸ä»£ç†åœ¨ä¸ç¯å¢ƒäº’åŠ¨æ—¶è·å–æ–°ä¿¡æ¯å¹¶é€‚åº”å…¶è¡Œä¸ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒTTIåœ¨WebVoyagerå’ŒWebArenaåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†äº¤äº’æ‰©å±•ä½œä¸ºä¸€ç§å¼ºå¤§çš„è¡¥å……æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-10.html",
    "link_next": "2025-06-12.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "10.06",
        "en": "06/10",
        "zh": "6æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.06",
        "en": "06/12",
        "zh": "6æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 4,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 1,
        "#rl": 4,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 0
    }
}