{
    "date": {
        "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 6",
        "zh": "1æœˆ6æ—¥"
    },
    "time_utc": "2025-01-06 04:12",
    "weekday": 0,
    "issue_id": 1505,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.01904",
            "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
            "url": "https://huggingface.co/papers/2501.01904",
            "abstract": "Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo.",
            "score": 0,
            "issue_id": 1505,
            "pub_date": "2025-01-03",
            "pub_date_card": {
                "ru": "3 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 3",
                "zh": "1æœˆ3æ—¥"
            },
            "hash": "576423a20b419d0f",
            "authors": [
                "Yifan Du",
                "Zikang Liu",
                "Yifan Li",
                "Wayne Xin Zhao",
                "Yuqi Huo",
                "Bingning Wang",
                "Weipeng Chen",
                "Zheng Liu",
                "Zhongyuan Wang",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "BAAI",
                "Baichuan AI",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01904.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¸ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Virgo, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ MLLM Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Slow-Thinking in Multimodal Models with Textual Reasoning",
                    "desc": "This paper discusses the development of a multimodal slow-thinking reasoning system called Virgo, which is based on fine-tuning a multimodal large language model (MLLM) using long-form textual reasoning data. The authors found that incorporating long-form reasoning in natural language significantly enhances the slow-thinking capabilities of MLLMs, even more so than using visual reasoning data. This suggests that the slow-thinking abilities are closely linked to the language model aspect, allowing for effective transfer across different data modalities. The research indicates a promising direction for creating advanced reasoning systems that can handle complex data semantics."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ…¢æ€ç»´æ¨ç†çš„æ¢ç´¢",
                    "desc": "æœ€è¿‘ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ…¢æ€ç»´æ¨ç†ç³»ç»Ÿå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­å»¶é•¿æ€è€ƒæ—¶é—´çš„èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•å°†è¿™ç§èƒ½åŠ›åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå°½ç®¡å¤„ç†ä¸åŒæ¨¡æ€çš„å¤æ‚æ•°æ®è¯­ä¹‰æ›´å…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒä¸€ä¸ªå¼ºå¤§çš„MLLMï¼Œä½¿ç”¨å°‘é‡çš„é•¿æ–‡æœ¬æ€ç»´æ•°æ®ï¼ŒæˆåŠŸæ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€æ…¢æ€ç»´ç³»ç»Ÿï¼Œå‘½åä¸ºVirgoï¼ˆè§†è§‰æ¨ç†ä¸é•¿æ€ç»´ï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé•¿æ–‡æœ¬æ¨ç†è¿‡ç¨‹å¯ä»¥æœ‰æ•ˆè½¬ç§»åˆ°MLLMsï¼Œå¹¶ä¸”è¿™ç§æ–‡æœ¬æ¨ç†æ•°æ®åœ¨æ¿€å‘MLLMsçš„æ…¢æ€ç»´èƒ½åŠ›æ–¹é¢ï¼Œä¼¼ä¹æ¯”è§†è§‰æ¨ç†æ•°æ®æ›´æœ‰æ•ˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-03.html",
    "link_next": "2025-01-07.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "03.01",
        "en": "01/03",
        "zh": "1æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "07.01",
        "en": "01/07",
        "zh": "1æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ•™ç§‘ä¹¦è¯­æ–™åº“ï¼Œç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é¢„è®­ç»ƒã€‚ä¸ç°æœ‰çš„å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®ç›¸æ¯”ï¼Œè¿™ç§è¯­æ–™åº“ä»ç½‘ç»œä¸Šçš„æ•™å­¦è§†é¢‘ä¸­æå–ä¿¡æ¯ï¼Œæä¾›æ›´ä¸°å¯Œçš„åŸºç¡€çŸ¥è¯†å’Œæ›´å¥½çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡ºçš„åˆ†ç±»æ³•ç³»ç»Ÿåœ°æ”¶é›†æ•™å­¦è§†é¢‘ï¼Œå¹¶é€æ­¥æå–å’Œç²¾ç‚¼è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§è§†é¢‘ä¸ºä¸­å¿ƒçš„æ•™ç§‘ä¹¦åœ¨çŸ¥è¯†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚",
        "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ•™ç§‘ä¹¦è¯­æ–™åº“ï¼Œç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é¢„è®­ç»ƒã€‚ä¸ç°æœ‰çš„å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®ç›¸æ¯”ï¼Œè¿™ç§è¯­æ–™åº“ä»ç½‘ç»œä¸Šçš„æ•™å­¦è§†é¢‘ä¸­æå–ä¿¡æ¯ï¼Œæä¾›æ›´ä¸°å¯Œçš„åŸºç¡€çŸ¥è¯†å’Œæ›´å¥½çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡ºçš„åˆ†ç±»æ³•ç³»ç»Ÿåœ°æ”¶é›†æ•™å­¦è§†é¢‘ï¼Œå¹¶é€æ­¥æå–å’Œç²¾ç‚¼è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§è§†é¢‘ä¸ºä¸­å¿ƒçš„æ•™ç§‘ä¹¦åœ¨çŸ¥è¯†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de duÅ mÃ³ tÃ i jiÃ okÄ“shÅ« yÇ”liÃ o kÃ¹, yÃ²ngyÃº shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng (VLMs) de yÃ¹xÃ¹nliÃ n. YÇ” xiÃ n yÇ’u de tÃºxiÃ ng-wÃ©nbÄ›n duÃ¬ shÃ¹jÃ¹ xiÄngbÇ, zhÃ¨ zhÇ’ng yÇ”liÃ o kÃ¹ cÃ³ng wÇngluÃ² shÃ ng de jiÃ oxuÃ© shÃ¬pÃ­n zhÅng tÄ«qÇ” xÃ¬nxÄ«, tÃ­gÅng gÃ¨ng fÄ“ngfÃ¹ de jÄ«chÇ” zhÄ«shi hÃ© gÃ¨ng hÇo de tÃºxiÃ ng-wÃ©nbÄ›n duÃ¬qÃ­. YÃ¡njiÅ« rÃ©nyuÃ¡n shÇyÃ²ng dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLM) tÃ­chÅ« de fÄ“nlÃ¨i fÇ xÃ¬tÇ’ng de shÅu jÃ­ jiÃ oxuÃ© shÃ¬pÃ­n, bÃ¬ng zhÃºbÃ¹ tÄ«qÇ” hÃ© jÄ«ngliÃ n shÃ¬juÃ©, yÄ«npiÃ n hÃ© wÃ©nbÄ›n zhÄ«shi. ShÃ­yÃ n biÇomÃ­ng, zhÃ¨ zhÇ’ng shÃ¬pÃ­n wÃ©i zhÅngxÄ«n de jiÃ okÄ“shÅ« zÃ i zhÄ«shi hÃ© tuÄ«lÇ mÃ¬jÄ« xÃ­ng rÃ¨nwÃ¹ zhÅng biÇoxiÃ n chÅ«sÃ¨. DÃ imÇ kÄ› zÃ i GitHub shÃ ng zhÇo dÃ o.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"è¯­æ–™åº“\", \"pinyin\": \"yÇ” liÃ o kÃ¹\", \"trans\": \"corpus\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"è§†è§‰-è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"vision-language models\"},\n    {\"word\": \"å›¾åƒ-æ–‡æœ¬å¯¹\", \"pinyin\": \"tÃº xiÃ ng wÃ©n bÄ›n duÃ¬\", \"trans\": \"image-text pairs\"},\n    {\"word\": \"æå–\", \"pinyin\": \"tÃ­ quÌ„\", \"trans\": \"extract\"},\n    {\"word\": \"åŸºç¡€çŸ¥è¯†\", \"pinyin\": \"jÄ« chÇ” zhÄ« shi\", \"trans\": \"foundational knowledge\"},\n    {\"word\": \"å¯¹é½\", \"pinyin\": \"duÃ¬ qÃ­\", \"trans\": \"alignment\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"åˆ†ç±»æ³•\", \"pinyin\": \"fÄ“n lÃ¨i fÇ\", \"trans\": \"classification method\"},\n    {\"word\": \"ç³»ç»Ÿåœ°\", \"pinyin\": \"xÃ¬ tÇ’ng de\", \"trans\": \"systematically\"},\n    {\"word\": \"æ”¶é›†\", \"pinyin\": \"shÅu jÃ­\", \"trans\": \"collect\"},\n    {\"word\": \"æ•™å­¦è§†é¢‘\", \"pinyin\": \"jiÃ o xuÃ© shÃ¬ pÃ­n\", \"trans\": \"educational videos\"},\n    {\"word\": \"é€æ­¥\", \"pinyin\": \"zhÃº bÃ¹\", \"trans\": \"step-by-step\"},\n    {\"word\": \"ç²¾ç‚¼\", \"pinyin\": \"jÄ«ng liÃ n\", \"trans\": \"refine\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"éŸ³é¢‘\", \"pinyin\": \"yÄ«n pÃ­n\", \"trans\": \"audio\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©n bÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"çŸ¥è¯†\", \"pinyin\": \"zhÄ« shi\", \"trans\": \"knowledge\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"å¯†é›†å‹\", \"pinyin\": \"mÃ¬ jÃ­ xÃ­ng\", \"trans\": \"intensive\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wu\", \"trans\": \"task\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"ä»£ç \", \"pinyin\": \"dÃ i mÇ\", \"trans\": \"code\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]",
        "trans": "This article introduces a new multimodal textbook corpus for the pre-training of vision-language models (VLMs). Unlike existing image-text pair data, this corpus extracts information from educational videos on the web, providing richer foundational knowledge and better image-text alignment. Researchers systematically collected educational videos using a classification scheme proposed by large language models (LLMs) and progressively extracted and refined visual, audio, and textual knowledge. Experiments demonstrate that this video-centric textbook performs excellently in knowledge and reasoning-intensive tasks. The code can be found on GitHub.",
        "update_ts": "2025-01-05 12:39"
    }
}