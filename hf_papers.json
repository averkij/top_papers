{
    "date": {
        "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 21",
        "zh": "4æœˆ21æ—¥"
    },
    "time_utc": "2025-04-21 22:11",
    "weekday": 0,
    "issue_id": 3354,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.13837",
            "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
            "url": "https://huggingface.co/papers/2504.13837",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@k metric with large values of k to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of k (\\eg, k=1), base models can achieve a comparable or even higher pass@k score compared to their RL counterparts at large k values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io",
            "score": 60,
            "issue_id": 3335,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "2fe56493fe3aec80",
            "authors": [
                "Yang Yue",
                "Zhiqi Chen",
                "Rui Lu",
                "Andrew Zhao",
                "Zhaokai Wang",
                "Yang Yue",
                "Shiji Song",
                "Gao Huang"
            ],
            "affiliations": [
                "LeapLab, Tsinghua University",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13837.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¥Ğ¾Ñ‚Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑÑ… k Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ pass@k, Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… k. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ½Ğ¾ÑÑÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ½Ğ¾ ÑÑ‚Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ RLVR Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Rethinking RLVR: Limits of Reinforcement Learning in Reasoning",
                    "desc": "This paper critically evaluates the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing the reasoning capabilities of large language models (LLMs). The authors find that while RLVR improves performance at lower complexity tasks, it does not introduce fundamentally new reasoning patterns compared to base models when evaluated at higher complexity levels. Instead, RL-trained models tend to sample reasoning paths that are already present in base models, leading to a narrower range of reasoning capabilities. The study suggests that distillation may be a more effective method for introducing new knowledge into models, highlighting the limitations of RLVR in advancing LLM reasoning."
                },
                "zh": {
                    "title": "é‡æ–°æ€è€ƒå¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä¸­çš„ä½œç”¨",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†ä¸€å®šæˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†è¿™ä¸€å‡è®¾ï¼Œå‘ç°RLVRå¹¶æœªçœŸæ­£å¼•å…¥æ–°çš„æ¨ç†æ¨¡å¼ã€‚å°½ç®¡RLè®­ç»ƒçš„æ¨¡å‹åœ¨å°çš„kå€¼ä¸‹è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†åœ¨è¾ƒå¤§çš„kå€¼ä¸‹ï¼ŒåŸºç¡€æ¨¡å‹çš„è¡¨ç°å¯ä»¥ä¸RLæ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³æ›´å¥½ã€‚è¿™è¡¨æ˜ï¼ŒRLè®­ç»ƒæ¨¡å‹çš„æ¨ç†è·¯å¾„å®é™…ä¸Šå·²ç»åŒ…å«åœ¨åŸºç¡€æ¨¡å‹çš„é‡‡æ ·åˆ†å¸ƒä¸­ï¼Œå¼ºè°ƒäº†RLVRåœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13835",
            "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
            "url": "https://huggingface.co/papers/2504.13835",
            "abstract": "Data quality and diversity are key to the construction of effective instruction-tuning datasets. % With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. % Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. % However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. % Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. % To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. % Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to Maximize the Information Gain (MIG) in semantic space. % Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. % Notably, the model fine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
            "score": 30,
            "issue_id": 3335,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "12926d762a03519c",
            "authors": [
                "Yicheng Chen",
                "Yining Li",
                "Kai Hu",
                "Zerun Ma",
                "Haochen Ye",
                "Kai Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Fudan University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13835.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¼ĞµÑ‚Ğ¾Ğº. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ»Ğ¸ÑˆÑŒ 5% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Maximizing Information for Better Instruction-Tuning Datasets",
                    "desc": "This paper addresses the importance of data quality and diversity in creating effective instruction-tuning datasets for machine learning. It critiques existing methods that rely on heuristic rules for maintaining diversity, which often leads to suboptimal dataset selections. The authors propose a new approach that quantifies the information content of datasets by modeling the semantic space with a label graph, allowing for a more comprehensive understanding of data diversity. Their method, called Maximize the Information Gain (MIG), iteratively selects samples that enhance the dataset's information content, showing significant performance improvements in experiments compared to traditional methods."
                },
                "zh": {
                    "title": "æå‡æ•°æ®é›†è´¨é‡ä¸å¤šæ ·æ€§çš„ç»Ÿä¸€æ–¹æ³•",
                    "desc": "æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§æ˜¯æ„å»ºæœ‰æ•ˆæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„å…³é”®ã€‚éšç€å¼€æºæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„å¢åŠ ï¼Œè‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„å­é›†å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¼˜å…ˆè€ƒè™‘å®ä¾‹è´¨é‡ï¼Œå¹¶ä½¿ç”¨å¯å‘å¼è§„åˆ™æ¥ç»´æŒå¤šæ ·æ€§ï¼Œä½†ç¼ºä¹å¯¹æ•´ä¸ªæ•°æ®é›†çš„å…¨é¢è§†è§’ï¼Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥é‡åŒ–æ•°æ®é›†çš„ä¿¡æ¯å†…å®¹ï¼Œå¹¶åŸºäºä¿¡æ¯åˆ†å¸ƒæ¥é‡åŒ–å¤šæ ·æ€§ï¼Œä»è€Œå¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥æœ€å¤§åŒ–è¯­ä¹‰ç©ºé—´ä¸­çš„ä¿¡æ¯å¢ç›Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11544",
            "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
            "url": "https://huggingface.co/papers/2504.11544",
            "abstract": "Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building a knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, we propose NodeRAG, a graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures a fully cohesive and efficient end-to-end process. Through extensive experiments, we demonstrate that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. Our GitHub repository could be seen at https://github.com/Terry-Xu-666/NodeRAG.",
            "score": 24,
            "issue_id": 3335,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "86dd4da356ad5ef0",
            "authors": [
                "Tianyang Xu",
                "Haojie Zheng",
                "Chengze Li",
                "Haoxiang Chen",
                "Yixin Liu",
                "Ruoxi Chen",
                "Lichao Sun"
            ],
            "affiliations": [
                "Columbia University",
                "Lehigh University",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11544.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#graphs",
                    "#open_source",
                    "#rag",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "NodeRAG: Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²",
                    "desc": "NodeRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (RAG), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ RAG, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NodeRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "NodeRAG: Enhancing RAG with Smart Graph Structures",
                    "desc": "This paper introduces NodeRAG, a new framework that enhances retrieval-augmented generation (RAG) by using heterogeneous graph structures. By focusing on the design of graph structures, NodeRAG improves the integration of various graph algorithms into the RAG workflow, leading to better performance. The framework aligns with the capabilities of large language models (LLMs), ensuring a smooth and efficient process for generating responses. Experimental results show that NodeRAG outperforms existing methods like GraphRAG and LightRAG in terms of indexing time, query time, storage efficiency, and question-answering accuracy."
                },
                "zh": {
                    "title": "NodeRAGï¼šå›¾ç»“æ„åŠ©åŠ›æ£€ç´¢å¢å¼ºç”Ÿæˆ",
                    "desc": "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè®¿é—®å¤–éƒ¨å’Œç§æœ‰è¯­æ–™åº“ï¼Œä»è€Œåœ¨ç‰¹å®šé¢†åŸŸæä¾›äº‹å®ä¸€è‡´çš„å“åº”ã€‚é€šè¿‡åˆ©ç”¨è¯­æ–™åº“çš„å†…åœ¨ç»“æ„ï¼ŒåŸºäºå›¾çš„RAGæ–¹æ³•é€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•è¿›ä¸€æ­¥ä¸°å¯Œäº†è¿™ä¸€è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç›®å‰çš„åŸºäºå›¾çš„RAGæ–¹æ³•å¾ˆå°‘é‡è§†å›¾ç»“æ„çš„è®¾è®¡ã€‚ä¸ºäº†è§£æ”¾å›¾åœ¨RAGä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†NodeRAGï¼Œä¸€ä¸ªä»¥å›¾ä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œå¼•å…¥å¼‚æ„å›¾ç»“æ„ï¼Œå®ç°å›¾æ–¹æ³•ä¸RAGå·¥ä½œæµç¨‹çš„æ— ç¼æ•´åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11833",
            "title": "Could Thinking Multilingually Empower LLM Reasoning?",
            "url": "https://huggingface.co/papers/2504.11833",
            "abstract": "Previous work indicates that large language models exhibit a significant \"English bias\", i.e. they often perform better when tasks are presented in English. Interestingly, we have observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. In this paper, we explore the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@k points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, we also find that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs.",
            "score": 15,
            "issue_id": 3335,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "463f5ddc1d75970e",
            "authors": [
                "Changjiang Gao",
                "Xu Huang",
                "Wenhao Zhu",
                "Shujian Huang",
                "Lei Li",
                "Fei Yuan"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11833.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 10 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ². ĞĞ´Ğ½Ğ°ĞºĞ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸Ğ·-Ğ·Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ĞµĞ¹. Ğ­Ñ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Unlocking the Power of Multilingual Reasoning in LLMs",
                    "desc": "This paper investigates the performance of large language models (LLMs) in reasoning tasks across multiple languages. It reveals that using certain non-English languages can lead to better outcomes than relying solely on English, highlighting a significant opportunity for multilingual reasoning. The authors suggest that multilingual approaches can achieve higher accuracy and robustness compared to English-only methods, even when translation quality varies. Additionally, they identify limitations in current answer selection methods that prevent reaching the full potential of multilingual reasoning, setting the stage for future research in this area."
                },
                "zh": {
                    "title": "å¤šè¯­è¨€æ¨ç†çš„æ½œåŠ›è¶…è¶Šè‹±è¯­",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œå‘ç°æŸäº›è¯­è¨€åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºè‹±è¯­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šè¯­è¨€æ¨ç†çš„ä¸Šé™æ¯”ä»…ä½¿ç”¨è‹±è¯­çš„æ¨ç†é«˜å‡ºè¿‘10ä¸ªå‡†ç¡®ç‡ç‚¹ï¼Œå¹¶ä¸”å¯¹ç¿»è¯‘è´¨é‡å’Œè¯­è¨€é€‰æ‹©çš„å˜åŒ–å…·æœ‰æ›´å¼ºçš„å®¹å¿åº¦ã€‚æˆ‘ä»¬åˆ†æäº†è¾¾åˆ°è¿™ä¸€ä¸Šé™çš„åŸå› å’ŒæŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºå¸¸è§çš„ç­”æ¡ˆé€‰æ‹©æ–¹æ³•ç”±äºå…¶å±€é™æ€§å’Œåè§ï¼Œæ— æ³•å®ç°è¿™ä¸€ä¸Šé™ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥ç ”ç©¶å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€æ¨ç†æ½œåŠ›é“ºå¹³äº†é“è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10823",
            "title": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives",
            "url": "https://huggingface.co/papers/2504.10823",
            "abstract": "Navigating high-stakes dilemmas involving conflicting values is challenging even for humans, let alone for AI. Yet prior work in evaluating the reasoning capabilities of large language models (LLMs) in such situations has been limited to everyday scenarios. To close this gap, this work first introduces CLASH (Character perspective-based LLM Assessments in Situations with High-stakes), a meticulously curated dataset consisting of 345 high-impact dilemmas along with 3,795 individual perspectives of diverse values. In particular, we design CLASH in a way to support the study of critical aspects of value-based decision-making processes which are missing from prior work, including understanding decision ambivalence and psychological discomfort as well as capturing the temporal shifts of values in characters' perspectives. By benchmarking 10 open and closed frontier models, we uncover several key findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet, achieve less than 50% accuracy in identifying situations where the decision should be ambivalent, while they perform significantly better in clear-cut scenarios. (2) While LLMs reasonably predict psychological discomfort as marked by human, they inadequately comprehend perspectives involving value shifts, indicating a need for LLMs to reason over complex values. (3) Our experiments also reveal a significant correlation between LLMs' value preferences and their steerability towards a given value. (4) Finally, LLMs exhibit greater steerability when engaged in value reasoning from a third-party perspective, compared to a first-person setup, though certain value pairs benefit uniquely from the first-person framing.",
            "score": 10,
            "issue_id": 3345,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "9e69138c0a313b09",
            "authors": [
                "Ayoung Lee",
                "Ryan Sungmo Kwon",
                "Peter Railton",
                "Lu Wang"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, University of Michigan",
                "Department of Philosophy, University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10823.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#reasoning",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤”",
                "ru": {
                    "title": "Ğ˜ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CLASH Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4 Ğ¸ Claude-Sonnet, Ğ¸Ğ¼ĞµÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¾ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑÑ… Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°, Ñ‡ĞµĞ¼ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°."
                },
                "en": {
                    "title": "Navigating Complex Values: Evaluating AI in High-Stakes Dilemmas",
                    "desc": "This paper introduces CLASH, a new dataset designed to evaluate large language models (LLMs) on high-stakes dilemmas that involve conflicting values. It contains 345 dilemmas and 3,795 perspectives, focusing on aspects like decision ambivalence and psychological discomfort. The study benchmarks various LLMs, revealing that even advanced models struggle with ambivalent decisions, achieving less than 50% accuracy. Additionally, while LLMs can predict psychological discomfort, they have difficulty understanding shifts in values, highlighting the need for improved reasoning in complex value scenarios."
                },
                "zh": {
                    "title": "åº”å¯¹é«˜é£é™©å›°å¢ƒä¸­çš„ä»·å€¼å†²çª",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†CLASHæ•°æ®é›†ï¼Œä¸“æ³¨äºé«˜é£é™©å›°å¢ƒä¸­çš„ä»·å€¼å†²çªï¼ŒåŒ…å«345ä¸ªé«˜å½±å“åŠ›çš„å›°å¢ƒå’Œ3795ä¸ªä¸åŒä»·å€¼è§‚çš„ä¸ªä½“è§†è§’ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒClaude-Sonnetï¼Œåœ¨è¯†åˆ«å†³ç­–æ¨¡ç³Šæ€§æ–¹é¢çš„å‡†ç¡®ç‡ä¹Ÿä¸è¶³50%ã€‚æ­¤å¤–ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹èƒ½å¤Ÿåˆç†é¢„æµ‹äººç±»çš„å¿ƒç†ä¸é€‚ï¼Œä½†åœ¨ç†è§£æ¶‰åŠä»·å€¼è½¬å˜çš„è§†è§’æ—¶è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬åœ¨å¤æ‚ä»·å€¼æ¨ç†ä¸Šçš„ä¸è¶³ã€‚æœ€åï¼Œæ¨¡å‹åœ¨ç¬¬ä¸‰æ–¹è§†è§’ä¸‹è¿›è¡Œä»·å€¼æ¨ç†æ—¶çš„å¯æ“æ§æ€§æ›´å¼ºï¼Œè€ŒæŸäº›ä»·å€¼å¯¹åœ¨ç¬¬ä¸€äººç§°æ¡†æ¶ä¸‹åˆ™è¡¨ç°æ›´å¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13173",
            "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
            "url": "https://huggingface.co/papers/2504.13173",
            "abstract": "Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, we observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, we present a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. We then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, we present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. We present three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Our experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models.",
            "score": 9,
            "issue_id": 3336,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "809d2f1facd3aed9",
            "authors": [
                "Ali Behrouz",
                "Meisam Razaviyayn",
                "Peilin Zhong",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13173.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ¼ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ framework Miras Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ - Moneta, Yaad Ğ¸ Memora, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ RNN. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Miras Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Neural Architectures with Attentional Bias",
                    "desc": "This paper explores new ways to design neural network architectures, particularly focusing on how they can mimic human attention. It introduces the concept of attentional bias, which helps models prioritize important information, and critiques existing methods that rely on simple similarity measures. The authors propose a framework called Miras, which allows for flexible design choices in memory architecture and training objectives. They also present new models that outperform traditional approaches in specific tasks, demonstrating the effectiveness of their innovative strategies."
                },
                "zh": {
                    "title": "åŸºäºæ³¨æ„åå‘çš„æ·±åº¦å­¦ä¹ æ¶æ„è®¾è®¡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•è®¾è®¡é«˜æ•ˆä¸”æœ‰æ•ˆçš„åŸºç¡€æ¨¡å‹æ¶æ„ï¼Œçµæ„Ÿæ¥æºäºäººç±»çš„æ³¨æ„åå‘ç°è±¡ã€‚æˆ‘ä»¬å°†ç¥ç»ç½‘ç»œæ¶æ„é‡æ–°æ¦‚å¿µåŒ–ä¸ºå…³è”è®°å¿†æ¨¡å—ï¼Œåˆ©ç”¨å†…éƒ¨ç›®æ ‡ï¼ˆæ³¨æ„åå‘ï¼‰æ¥å­¦ä¹ é”®å€¼æ˜ å°„ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰åºåˆ—æ¨¡å‹ä¸»è¦ä¾èµ–ç‚¹ç§¯ç›¸ä¼¼æ€§æˆ–L2å›å½’ç›®æ ‡ï¼Œè€Œæˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—æ›¿ä»£çš„æ³¨æ„åå‘é…ç½®åŠå…¶æœ‰æ•ˆè¿‘ä¼¼ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Mirasæ¡†æ¶ï¼Œè®¾è®¡æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¹¶å±•ç¤ºäº†ä¸‰ç§æ–°å‹åºåˆ—æ¨¡å‹ï¼Œè¶…è¶Šäº†ç°æœ‰çº¿æ€§RNNçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13157",
            "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
            "url": "https://huggingface.co/papers/2504.13157",
            "abstract": "We explore the task of geometric reconstruction of images captured from a mixture of ground and aerial views. Current state-of-the-art learning-based approaches fail to handle the extreme viewpoint variation between aerial-ground image pairs. Our hypothesis is that the lack of high-quality, co-registered aerial-ground datasets for training is a key reason for this failure. Such data is difficult to assemble precisely because it is difficult to reconstruct in a scalable way. To overcome this challenge, we propose a scalable framework combining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google Earth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The pseudo-synthetic data simulates a wide range of aerial viewpoints, while the real, crowd-sourced images help improve visual fidelity for ground-level images where mesh-based renderings lack sufficient detail, effectively bridging the domain gap between real images and pseudo-synthetic renderings. Using this hybrid dataset, we fine-tune several state-of-the-art algorithms and achieve significant improvements on real-world, zero-shot aerial-ground tasks. For example, we observe that baseline DUSt3R localizes fewer than 5% of aerial-ground pairs within 5 degrees of camera rotation error, while fine-tuning with our data raises accuracy to nearly 56%, addressing a major failure point in handling large viewpoint changes. Beyond camera estimation and scene reconstruction, our dataset also improves performance on downstream tasks like novel-view synthesis in challenging aerial-ground scenarios, demonstrating the practical value of our approach in real-world applications.",
            "score": 8,
            "issue_id": 3335,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "bbd51434265e3614",
            "authors": [
                "Khiem Vuong",
                "Anurag Ghosh",
                "Deva Ramanan",
                "Srinivasa Narasimhan",
                "Shubham Tulsiani"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13157.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#synthetic",
                    "#dataset",
                    "#3d"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·ĞµĞ¼Ğ»ĞµĞ¹ Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ¼ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¸ Ğ°ÑÑ€Ğ¾ÑÑŠĞµĞ¼Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿ÑĞµĞ²Ğ´Ğ¾-ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹ Ğ¸Ğ· 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ°ÑÑ€Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Viewpoint Gap: Enhanced Aerial-Ground Image Reconstruction",
                    "desc": "This paper addresses the challenge of reconstructing images from both ground and aerial perspectives, which current machine learning methods struggle with due to significant viewpoint differences. The authors suggest that the lack of high-quality datasets that pair aerial and ground images is a major obstacle. To tackle this, they introduce a scalable framework that combines pseudo-synthetic images generated from 3D city models with real ground-level images, effectively bridging the gap between these two domains. By fine-tuning existing algorithms with this hybrid dataset, they achieve substantial improvements in accuracy for aerial-ground tasks, demonstrating the framework's effectiveness in real-world applications."
                },
                "zh": {
                    "title": "æ‰“ç ´è§†è§’é™åˆ¶ï¼Œå®ç°å›¾åƒå‡ ä½•é‡å»º",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä»åœ°é¢å’Œç©ºä¸­è§†è§’æ•è·çš„å›¾åƒè¿›è¡Œå‡ ä½•é‡å»ºçš„ä»»åŠ¡ã€‚ç°æœ‰çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨å¤„ç†ç©ºä¸­ä¸åœ°é¢å›¾åƒå¯¹ä¹‹é—´çš„æç«¯è§†è§’å˜åŒ–æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç¼ºä¹é«˜è´¨é‡çš„ã€å…±åŒæ³¨å†Œçš„ç©ºä¸­-åœ°é¢æ•°æ®é›†æ˜¯å¯¼è‡´è¿™ä¸€å¤±è´¥çš„å…³é”®åŸå› ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ¡†æ¶ï¼Œç»“åˆäº†æ¥è‡ª3DåŸå¸‚ç½‘æ ¼çš„ä¼ªåˆæˆæ¸²æŸ“å’ŒçœŸå®çš„åœ°é¢ä¼—åŒ…å›¾åƒï¼Œä»è€Œæœ‰æ•ˆåœ°ç¼©å°äº†çœŸå®å›¾åƒä¸ä¼ªåˆæˆæ¸²æŸ“ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09621",
            "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images",
            "url": "https://huggingface.co/papers/2504.09621",
            "abstract": "Global contextual information and local detail features are essential for haze removal tasks. Deep learning models perform well on small, low-resolution images, but they encounter difficulties with large, high-resolution ones due to GPU memory limitations. As a compromise, they often resort to image slicing or downsampling. The former diminishes global information, while the latter discards high-frequency details. To address these challenges, we propose DehazeXL, a haze removal method that effectively balances global context and local feature extraction, enabling end-to-end modeling of large images on mainstream GPU hardware. Additionally, to evaluate the efficiency of global context utilization in haze removal performance, we design a visual attribution method tailored to the characteristics of haze removal tasks. Finally, recognizing the lack of benchmark datasets for haze removal in large images, we have developed an ultra-high-resolution haze removal dataset (8KDehaze) to support model training and testing. It includes 10000 pairs of clear and hazy remote sensing images, each sized at 8192 times 8192 pixels. Extensive experiments demonstrate that DehazeXL can infer images up to 10240 times 10240 pixels with only 21 GB of memory, achieving state-of-the-art results among all evaluated methods. The source code and experimental dataset are available at https://github.com/CastleChen339/DehazeXL.",
            "score": 6,
            "issue_id": 3340,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "6c9f2fe055ad92dc",
            "authors": [
                "Jiuchen Chen",
                "Xinyu Yan",
                "Qizhi Xu",
                "Kaiqi Li"
            ],
            "affiliations": [
                "Beijing Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09621.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#optimization",
                    "#data",
                    "#cv"
                ],
                "emoji": "ğŸŒ«ï¸",
                "ru": {
                    "title": "DehazeXL: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ñ‹Ğ¼ĞºĞ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…",
                    "desc": "DehazeXL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ñ‹Ğ¼ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… GPU. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 8KDehaze Ñ 10000 Ğ¿Ğ°Ñ€ Ñ‡ĞµÑ‚ĞºĞ¸Ñ… Ğ¸ Ğ´Ñ‹Ğ¼Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 8192x8192 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DehazeXL Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 10240x10240 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 21 Ğ“Ğ‘ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ²ÑĞµÑ… Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "DehazeXL: Mastering Haze Removal with Global Context and Local Detail",
                    "desc": "The paper introduces DehazeXL, a novel method for haze removal that effectively integrates global contextual information with local detail features. Traditional deep learning models struggle with high-resolution images due to memory constraints, often leading to a loss of important information. DehazeXL overcomes these limitations by allowing end-to-end processing of large images while maintaining high-quality outputs. Additionally, the authors present a new ultra-high-resolution dataset, 8KDehaze, to facilitate training and testing of haze removal models, demonstrating that their approach achieves superior performance on large images."
                },
                "zh": {
                    "title": "DehazeXLï¼šé«˜æ•ˆå»é›¾çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å»é›¾æ–¹æ³•DehazeXLï¼Œæ—¨åœ¨å¹³è¡¡å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå±€éƒ¨ç»†èŠ‚ç‰¹å¾ï¼Œä»¥æé«˜å¤§å›¾åƒçš„å»é›¾æ•ˆæœã€‚ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶é¢ä¸´GPUå†…å­˜é™åˆ¶ï¼Œé€šå¸¸é‡‡ç”¨å›¾åƒåˆ‡ç‰‡æˆ–ä¸‹é‡‡æ ·çš„æ–¹æ³•ï¼Œä½†è¿™ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚DehazeXLèƒ½å¤Ÿåœ¨ä¸»æµGPUç¡¬ä»¶ä¸Šå®ç°ç«¯åˆ°ç«¯çš„å¤§å›¾åƒå»ºæ¨¡ï¼Œå¹¶é€šè¿‡è®¾è®¡è§†è§‰å½’å› æ–¹æ³•æ¥è¯„ä¼°å…¨å±€ä¸Šä¸‹æ–‡åœ¨å»é›¾æ€§èƒ½ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªè¶…é«˜åˆ†è¾¨ç‡å»é›¾æ•°æ®é›†ï¼ˆ8KDehazeï¼‰ï¼ŒåŒ…å«10000å¯¹8192x8192åƒç´ çš„æ¸…æ™°å’Œæ¨¡ç³Šé¥æ„Ÿå›¾åƒï¼Œä»¥æ”¯æŒæ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13072",
            "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
            "url": "https://huggingface.co/papers/2504.13072",
            "abstract": "Scene-level 3D generation represents a critical frontier in multimedia and computer graphics, yet existing approaches either suffer from limited object categories or lack editing flexibility for interactive applications. In this paper, we present HiScene, a novel hierarchical framework that bridges the gap between 2D image generation and 3D object generation and delivers high-fidelity scenes with compositional identities and aesthetic scene content. Our key insight is treating scenes as hierarchical \"objects\" under isometric views, where a room functions as a complex object that can be further decomposed into manipulatable items. This hierarchical approach enables us to generate 3D content that aligns with 2D representations while maintaining compositional structure. To ensure completeness and spatial alignment of each decomposed instance, we develop a video-diffusion-based amodal completion technique that effectively handles occlusions and shadows between objects, and introduce shape prior injection to ensure spatial coherence within the scene. Experimental results demonstrate that our method produces more natural object arrangements and complete object instances suitable for interactive applications, while maintaining physical plausibility and alignment with user inputs.",
            "score": 5,
            "issue_id": 3337,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "6eb45708f6cb0c26",
            "authors": [
                "Wenqi Dong",
                "Bangbang Yang",
                "Zesong Yang",
                "Yuan Li",
                "Tao Hu",
                "Hujun Bao",
                "Yuewen Ma",
                "Zhaopeng Cui"
            ],
            "affiliations": [
                "ByteDance",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13072.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "HiScene: Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹",
                    "desc": "HiScene - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ 2D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 'Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹' Ğ² Ğ¸Ğ·Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ñ‹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ñ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HiScene ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "HiScene: Bridging 2D and 3D for Interactive Scene Generation",
                    "desc": "This paper introduces HiScene, a new framework for generating 3D scenes that combines the strengths of 2D image generation with 3D object creation. It treats scenes as hierarchical structures, allowing for detailed manipulation of individual elements within a room. The method employs a video-diffusion-based technique for amodal completion, addressing issues like occlusions and shadows to ensure realistic object interactions. Experimental results show that HiScene produces coherent and aesthetically pleasing 3D scenes that are well-suited for interactive applications."
                },
                "zh": {
                    "title": "HiSceneï¼šå±‚æ¬¡åŒ–çš„3Dåœºæ™¯ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHiSceneçš„å±‚æ¬¡æ¡†æ¶ï¼Œç”¨äºåœºæ™¯çº§3Dç”Ÿæˆï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¯¹è±¡ç±»åˆ«å’Œç¼–è¾‘çµæ´»æ€§æ–¹é¢çš„å±€é™ã€‚æˆ‘ä»¬å°†åœºæ™¯è§†ä¸ºåœ¨ç­‰è·è§†å›¾ä¸‹çš„å±‚æ¬¡â€œå¯¹è±¡â€ï¼Œä½¿å¾—æˆ¿é—´å¯ä»¥è¢«è¿›ä¸€æ­¥åˆ†è§£ä¸ºå¯æ“ä½œçš„ç‰©å“ã€‚é€šè¿‡è¿™ç§å±‚æ¬¡åŒ–çš„æ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆä¸2Dè¡¨ç¤ºç›¸ä¸€è‡´çš„3Då†…å®¹ï¼ŒåŒæ—¶ä¿æŒç»„åˆç»“æ„ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£çš„æ¨¡æ€è¡¥å…¨æŠ€æœ¯ï¼Œä»¥å¤„ç†å¯¹è±¡ä¹‹é—´çš„é®æŒ¡å’Œé˜´å½±ï¼Œç¡®ä¿æ¯ä¸ªåˆ†è§£å®ä¾‹çš„å®Œæ•´æ€§å’Œç©ºé—´å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13828",
            "title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering",
            "url": "https://huggingface.co/papers/2504.13828",
            "abstract": "The first generation of Large Language Models - what might be called \"Act I\" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of \"Act II\" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI's second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: https://github.com/GAIR-NLP/cognition-engineering",
            "score": 4,
            "issue_id": 3346,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "f0a8bce0c75eeac0",
            "authors": [
                "Shijie Xia",
                "Yiwei Qin",
                "Xuefeng Li",
                "Yan Ma",
                "Run-Ze Fan",
                "Steffi Chern",
                "Haoyang Zou",
                "Fan Zhou",
                "Xiangkun Hu",
                "Jiahe Jin",
                "Yanheng He",
                "Yixin Ye",
                "Yixiu Liu",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Generative AI Research Lab (GAIR)",
                "SII",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13828.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#multimodal",
                    "#survey",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğº ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ñ‹ÑĞ»ĞµĞ¹: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ°Ğ¿Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (test-time scaling). Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ' (cognition engineering) ĞºĞ°Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ˜Ğ˜ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚ÑƒÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ÑÑ‚Ğ¸Ğ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "From Knowledge Retrieval to Thought Construction in AI",
                    "desc": "This paper discusses the evolution of Large Language Models (LLMs) from their initial phase, termed 'Act I', to a new phase called 'Act II'. In 'Act I', LLMs relied heavily on large datasets and parameters but faced issues like slow knowledge updates and limited reasoning abilities. The current phase, 'Act II', focuses on enhancing these models into thought-construction engines that can generate ideas and insights in real-time. The authors aim to make cognition engineering accessible to all practitioners by providing tutorials and resources for implementing these advanced techniques."
                },
                "zh": {
                    "title": "è®¤çŸ¥å·¥ç¨‹çš„æ–°æ—¶ä»£ï¼šä»çŸ¥è¯†æ£€ç´¢åˆ°æ€ç»´æ„å»º",
                    "desc": "æœ¬æ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¬¬ä¸€ä»£ï¼ˆ2020-2023å¹´ï¼‰å’Œç¬¬äºŒä»£ï¼ˆ2024å¹´è‡³ä»Šï¼‰çš„å‘å±•ã€‚ç¬¬ä¸€ä»£æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡å‚æ•°å’Œæ•°æ®æ‰©å±•å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨çŸ¥è¯†å»¶è¿Ÿã€æ¨ç†æµ…æ˜¾å’Œè®¤çŸ¥è¿‡ç¨‹å—é™ç­‰æ–¹é¢å­˜åœ¨åŸºæœ¬å±€é™ã€‚ç¬¬äºŒä»£æ¨¡å‹æ­£åœ¨é€šè¿‡æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯ï¼Œä»çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿè½¬å˜ä¸ºæ€ç»´æ„å»ºå¼•æ“ï¼Œå»ºç«‹ä¸AIçš„è¯­è¨€æ€ç»´è¿æ¥ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†è¿™äº›å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æä¾›äº†å…¨é¢çš„æ•™ç¨‹å’Œä¼˜åŒ–å®ç°ï¼Œä»¥ä¿ƒè¿›è®¤çŸ¥å·¥ç¨‹çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13626",
            "title": "Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models",
            "url": "https://huggingface.co/papers/2504.13626",
            "abstract": "Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from \"overthinking\" problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on fine-tuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization.   Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token (<think> and </think>) can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications.",
            "score": 4,
            "issue_id": 3340,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "a0b61093ea88a6d7",
            "authors": [
                "Yule Liu",
                "Jingyi Zheng",
                "Zhen Sun",
                "Zifan Peng",
                "Wenhan Dong",
                "Zeyang Sha",
                "Shiwen Cui",
                "Weiqiang Wang",
                "Xinlei He"
            ],
            "affiliations": [
                "Ant Group",
                "Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13626.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ThoughtMani Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LRM. ThoughtMani ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 30% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LRM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Streamlining Reasoning: ThoughtMani Reduces Overthinking in LRMs",
                    "desc": "This paper discusses the challenges faced by large reasoning models (LRMs), particularly the issue of 'overthinking' where models produce excessive reasoning steps with minimal performance improvement. The authors propose a novel approach called ThoughtMani, which strategically places external Chains of Thought (CoTs) generated by smaller models to streamline the reasoning process. This method not only reduces the number of unnecessary intermediate steps but also maintains the model's performance while cutting down computational costs by about 30%. Additionally, ThoughtMani improves safety alignment, making it a practical solution for enhancing the efficiency of LRMs in real-world applications."
                },
                "zh": {
                    "title": "ThoughtManiï¼šå‡å°‘æ¨ç†æ­¥éª¤ï¼Œæå‡æ•ˆç‡",
                    "desc": "æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç¤ºäº†é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—æ¥å¢å¼ºæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼ŒLRMsé€šå¸¸ä¼šå‡ºç°â€œè¿‡åº¦æ€è€ƒâ€é—®é¢˜ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ­¥éª¤å†—ä½™ä¸”æ€§èƒ½æå‡æœ‰é™ã€‚ç°æœ‰çš„ç ”ç©¶ä¾èµ–äºå¾®è°ƒæ¥ç¼“è§£è¿‡åº¦æ€è€ƒï¼Œä½†è¿™éœ€è¦é¢å¤–çš„æ•°æ®å’Œå¤æ‚çš„è®­ç»ƒè®¾ç½®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•é«˜æ•ˆçš„ç®¡é“ï¼ŒThoughtManiï¼Œé€šè¿‡åœ¨æ€è€ƒæ ‡è®°ä¹‹é—´æ”¾ç½®å°æ¨¡å‹ç”Ÿæˆçš„å¤–éƒ¨é“¾æ¡ï¼ˆCoTsï¼‰ï¼Œæœ‰æ•ˆå‡å°‘ä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13816",
            "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations",
            "url": "https://huggingface.co/papers/2504.13816",
            "abstract": "While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs' perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs' recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.",
            "score": 2,
            "issue_id": 3347,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "9243c69083d55578",
            "authors": [
                "Chenghao Xiao",
                "Hou Pong Chan",
                "Hao Zhang",
                "Mahani Aljunied",
                "Lidong Bing",
                "Noura Al Moubayed",
                "Yu Rong"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Department of Computer Science, Durham University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13816.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#dataset",
                    "#transfer_learning",
                    "#low_resource",
                    "#open_source",
                    "#training",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¸ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ LLM."
                },
                "en": {
                    "title": "Bridging Language Gaps in LLM Knowledge Boundaries",
                    "desc": "This paper investigates how large language models (LLMs) understand their limits of knowledge in various languages, addressing a gap in previous research that mainly focused on English. The authors find that LLMs encode their knowledge boundaries in specific layers of their architecture, and that these perceptions vary in a structured way across languages. They propose a method to align knowledge boundary recognition without additional training, which can help reduce inaccuracies in languages with fewer resources. Additionally, fine-tuning LLMs with bilingual question pairs improves their ability to recognize knowledge boundaries, and the authors provide a new multilingual evaluation suite for further research."
                },
                "zh": {
                    "title": "è·¨è¯­è¨€çŸ¥è¯†è¾¹ç•Œçš„è¯†åˆ«ä¸è½¬ç§»",
                    "desc": "æœ¬ç ”ç©¶é¦–æ¬¡åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒè¯­è¨€ä¸­è¯†åˆ«çŸ¥è¯†è¾¹ç•Œçš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMså¯¹çŸ¥è¯†è¾¹ç•Œçš„æ„ŸçŸ¥ä¸»è¦ç¼–ç åœ¨ä¸­é—´å±‚åˆ°ä¸­ä¸Šå±‚ã€‚ä¸åŒè¯­è¨€ä¹‹é—´çš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥å‘ˆçº¿æ€§ç»“æ„ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„å¯¹é½æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°åœ¨è¯­è¨€é—´è½¬ç§»çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œé™ä½ä½èµ„æºè¯­è¨€ä¸­çš„å¹»è§‰é£é™©ã€‚æ­¤å¤–ï¼ŒåŒè¯­é—®é¢˜å¯¹ç¿»è¯‘çš„å¾®è°ƒè¿›ä¸€æ­¥å¢å¼ºäº†LLMsåœ¨è·¨è¯­è¨€è¯†åˆ«çŸ¥è¯†è¾¹ç•Œçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12083",
            "title": "Self-alignment of Large Video Language Models with Refined Regularized\n  Preference Optimization",
            "url": "https://huggingface.co/papers/2504.12083",
            "abstract": "Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose significant challenges to their safe and reliable deployment in real-world applications. To address these limitations, we propose a self-alignment framework that enables LVLMs to learn from their own errors. Our proposed framework first obtains a training set of preferred and non-preferred response pairs, where non-preferred responses are generated by incorporating common error patterns that often occur due to inadequate spatio-temporal understanding, spurious correlations between co-occurring concepts, and over-reliance on linguistic cues while neglecting the vision modality, among others. To facilitate self-alignment of LVLMs with the constructed preferred and non-preferred response pairs, we introduce Refined Regularized Preference Optimization (RRPO), a novel preference optimization method that utilizes sub-sequence-level refined rewards and token-wise KL regularization to address the limitations of Direct Preference Optimization (DPO). We demonstrate that RRPO achieves more precise alignment and more stable training compared to DPO. Our experiments and analysis validate the effectiveness of our approach across diverse video tasks, including video hallucination, short- and long-video understanding, and fine-grained temporal reasoning.",
            "score": 1,
            "issue_id": 3351,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "1ac44e2a3e2a77c4",
            "authors": [
                "Pritam Sarkar",
                "Ali Etemad"
            ],
            "affiliations": [
                "Queens University, Canada",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12083.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#hallucinations",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#video",
                    "#alignment"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LVLM ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ°Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ - Refined Regularized Preference Optimization (RRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Video Language Models Through Self-Alignment",
                    "desc": "This paper addresses the challenges faced by Large Video Language Models (LVLMs) in understanding video content accurately. The authors introduce a self-alignment framework that allows LVLMs to learn from their mistakes by generating preferred and non-preferred response pairs based on common error patterns. They propose a new optimization method called Refined Regularized Preference Optimization (RRPO), which improves the alignment of LVLMs by using refined rewards and regularization techniques. The results show that RRPO enhances training stability and precision in various video-related tasks, including temporal reasoning and video understanding."
                },
                "zh": {
                    "title": "è‡ªå¯¹é½æ¡†æ¶æå‡è§†é¢‘è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›",
                    "desc": "å°½ç®¡å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æŠ€æœ¯ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨ç»†ç²’åº¦æ—¶é—´ç†è§£æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œå®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå¹¶ä¸”åœ¨ç®€å•çš„è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­å¸¸å¸¸çŠ¯é”™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªå¯¹é½æ¡†æ¶ï¼Œä½¿LVLMsèƒ½å¤Ÿä»è‡ªèº«é”™è¯¯ä¸­å­¦ä¹ ã€‚è¯¥æ¡†æ¶é¦–å…ˆç”Ÿæˆä¸€ç»„åŒ…å«ä¼˜é€‰å’Œéä¼˜é€‰å“åº”å¯¹çš„è®­ç»ƒé›†ï¼Œéä¼˜é€‰å“åº”æ˜¯é€šè¿‡å¼•å…¥å¸¸è§é”™è¯¯æ¨¡å¼ç”Ÿæˆçš„ï¼Œè¿™äº›æ¨¡å¼é€šå¸¸ç”±äºæ—¶ç©ºç†è§£ä¸è¶³ã€å…±ç°æ¦‚å¿µä¹‹é—´çš„è™šå‡ç›¸å…³æ€§ä»¥åŠè¿‡åº¦ä¾èµ–è¯­è¨€çº¿ç´¢è€Œå¿½è§†è§†è§‰æ¨¡æ€ç­‰åŸå› è€Œäº§ç”Ÿã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åå¥½ä¼˜åŒ–æ–¹æ³•â€”â€”ç²¾ç»†æ­£åˆ™åŒ–åå¥½ä¼˜åŒ–ï¼ˆRRPOï¼‰ï¼Œé€šè¿‡å­åºåˆ—çº§åˆ«çš„ç²¾ç»†å¥–åŠ±å’Œé€æ ‡è®°KLæ­£åˆ™åŒ–æ¥è§£å†³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å±€é™æ€§ï¼Œå®éªŒç»“æœè¡¨æ˜RRPOåœ¨å¯¹é½ç²¾åº¦å’Œè®­ç»ƒç¨³å®šæ€§æ–¹é¢ä¼˜äºDPOã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13677",
            "title": "Revisiting Uncertainty Quantification Evaluation in Language Models:\n  Spurious Interactions with Response Length Bias Results",
            "url": "https://huggingface.co/papers/2504.13677",
            "abstract": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). In this paper, we show that commonly used correctness functions bias UQ evaluations by inflating the performance of certain UQ methods. We evaluate 7 correctness functions -- from lexical-based and embedding-based metrics to LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our analysis reveals that length biases in the errors of these correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LLM-as-a-judge approaches as among the least length-biased choices and hence a potential solution to mitigate these biases.",
            "score": 0,
            "issue_id": 3352,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "cf5640eea0b24520",
            "authors": [
                "Andrea Santilli",
                "Adam Golinski",
                "Michael Kirchhof",
                "Federico Danieli",
                "Arno Blaas",
                "Miao Xiong",
                "Luca Zappella",
                "Sinead Williamson"
            ],
            "affiliations": [
                "Apple",
                "National University of Singapore",
                "Sapienza University of Rome"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13677.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#hallucinations",
                    "#interpretability"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° ÑĞ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºĞ°Ğ¶Ğ°ÑÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·-Ğ·Ğ° ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ 7 Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 4 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 4 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 6 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'LLM-as-a-judge' Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ĞµĞµ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Mitigating Biases in UQ Evaluations with LLM-as-a-Judge",
                    "desc": "This paper discusses the importance of Uncertainty Quantification (UQ) in Language Models (LMs) for enhancing their safety and reliability. It highlights how traditional correctness functions, such as ROUGE-L, can introduce biases that inflate the perceived performance of UQ methods. The authors evaluate various correctness functions across multiple datasets and models, revealing that length biases in these functions distort UQ assessments. They propose LLM-as-a-judge approaches as a promising solution to reduce these biases and improve UQ evaluations."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„å…³é”®ï¼šä¸ç¡®å®šæ€§é‡åŒ–",
                    "desc": "åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰å¯¹äºæé«˜å…¶å®‰å…¨æ€§å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚æœ¬æ–‡å±•ç¤ºäº†å¸¸ç”¨çš„æ­£ç¡®æ€§å‡½æ•°ä¼šé€šè¿‡å¤¸å¤§æŸäº›UQæ–¹æ³•çš„æ€§èƒ½æ¥åè§UQè¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†7ç§æ­£ç¡®æ€§å‡½æ•°ï¼Œå¹¶å‘ç°è¿™äº›å‡½æ•°çš„é•¿åº¦åå·®ä¼šæ‰­æ›²UQè¯„ä¼°ç»“æœã€‚æˆ‘ä»¬æŒ‡å‡ºï¼ŒLLMä½œä¸ºè¯„åˆ¤è€…çš„æ–¹æ³•æ˜¯æœ€å°‘å—é•¿åº¦åå·®å½±å“çš„é€‰æ‹©ï¼Œå¯èƒ½æ˜¯ç¼“è§£è¿™äº›åå·®çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13519",
            "title": "Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for\n  Low-Dose CT with Attention-Guided Bilateral Filtering",
            "url": "https://huggingface.co/papers/2504.13519",
            "abstract": "Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable self-supervised single-image denoising framework -- Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through a lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted post-training for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce a novel downsampling shuffle strategy with a new self-supervised loss function that extends the concept of Noise2Noise to a single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at https://github.com/sypsyp97/Filter2Noise.git .",
            "score": 0,
            "issue_id": 3348,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "54a6d11b3dfb1ed4",
            "authors": [
                "Yipeng Sun",
                "Linda-Sophie Schneider",
                "Mingxuan Gu",
                "Siyuan Mei",
                "Chengze Ye",
                "Fabian Wagner",
                "Siming Bayer",
                "Andreas Maier"
            ],
            "affiliations": [
                "Friedrich-Alexander-University Erlangen-Nuremberg, Erlangen, Germany",
                "Siemens Healthineers AG, Forchheim, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13519.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#interpretability",
                    "#low_resource",
                    "#healthcare",
                    "#dataset"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞšĞ¢: Ğ¾Ñ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ° Ğº Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ´Ğ¾Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Filter2Noise (F2N). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ±Ğ¸Ğ»Ğ°Ñ‚ĞµÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğµ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. F2N Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."
                },
                "en": {
                    "title": "Revolutionizing Low-Dose CT Denoising with Filter2Noise",
                    "desc": "This paper presents Filter2Noise (F2N), a self-supervised framework for denoising low-dose CT images using a single noisy input. Unlike traditional supervised methods that require paired datasets, F2N employs an Attention-Guided Bilateral Filter that adapts to the noise characteristics of each image, allowing for user-controlled adjustments. The framework introduces a novel downsampling shuffle strategy and a self-supervised loss function that effectively handles spatially correlated noise. Experimental results show that F2N significantly improves denoising performance, achieving higher PSNR compared to existing methods while enhancing interpretability and control for medical applications."
                },
                "zh": {
                    "title": "è‡ªç›‘ç£å»å™ªï¼Œç²¾å‡†å¯æ§ï¼",
                    "desc": "åœ¨ä½å‰‚é‡CTä¸­ï¼Œæœ‰æ•ˆå»å™ªå¯¹äºå¢å¼ºç»†å¾®ç»“æ„å’Œä½å¯¹æ¯”åº¦ç—…å˜è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨æœ‰é™çš„é…å¯¹æ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œè‡ªç›‘ç£æ–¹æ³•é€šå¸¸éœ€è¦å¤šå¼ å™ªå£°å›¾åƒï¼Œå¹¶ä¾èµ–æ·±åº¦ç½‘ç»œå¦‚U-Netï¼Œç¼ºä¹å¯¹å»å™ªæœºåˆ¶çš„æ·±å…¥ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„è‡ªç›‘ç£å•å›¾åƒå»å™ªæ¡†æ¶â€”â€”Filter2Noise (F2N)ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„åŒè¾¹æ»¤æ³¢å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®æ¯ä¸ªå™ªå£°è¾“å…¥è‡ªé€‚åº”è°ƒæ•´æ»¤æ³¢å‚æ•°ï¼Œç”¨æˆ·å¯ä»¥åœ¨è®­ç»ƒåå¯è§†åŒ–å’Œè°ƒæ•´è¿™äº›å‚æ•°ï¼Œä»¥å®ç°ç‰¹å®šåŒºåŸŸçš„å»å™ªæ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13359",
            "title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models",
            "url": "https://huggingface.co/papers/2504.13359",
            "abstract": "The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. We propose a framework grounded in production theory for evaluating language models by combining accuracy and inference cost. We introduce \"cost-of-pass\", the expected monetary cost of generating a correct solution. We then define the \"frontier cost-of-pass\" as the minimum cost-of-pass achievable across available models or the \"human-expert, using the approximate cost of hiring an expert. Our analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledge-intensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking this frontier cost-of-pass over the past year reveals significant progress, particularly for complex quantitative tasks where the cost has roughly halved every few months. Third, to trace key innovations driving this progress, we examine counterfactual frontiers: estimates of cost-efficiency without specific model classes. We find that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quantitative, knowledge-intensive, and complex quantitative tasks, respectively. Finally, we assess the cost-reductions afforded by common inference-time techniques like majority voting and self-refinement, finding that their marginal accuracy gains rarely justify their costs. Our findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency, and our economic framework provides a principled tool for measuring this progress and guiding deployment.",
            "score": 0,
            "issue_id": 3352,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "ff1639bd69b70cc7",
            "authors": [
                "Mehmet Hamza Erol",
                "Batu El",
                "Mirac Suzgun",
                "Mert Yuksekgonul",
                "James Zou"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13359.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ 'cost-of-pass' - Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼ÑƒÑ Ğ´ĞµĞ½ĞµĞ¶Ğ½ÑƒÑ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¸ 'frontier cost-of-pass' - Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶Ğ¸Ğ¼ÑƒÑ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑÑ€ĞµĞ´Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ - Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ 'frontier cost-of-pass' Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ´, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Balancing Performance and Cost in AI: A New Economic Framework",
                    "desc": "This paper presents a framework for evaluating language models based on their economic value, balancing performance and inference costs. It introduces the concept of 'cost-of-pass', which quantifies the expected monetary cost of producing a correct output. The authors analyze how different types of modelsâ€”lightweight, large, and reasoningâ€”perform across various tasks, revealing that each type excels in specific areas despite differing costs. Additionally, they highlight the importance of model innovations in improving cost-efficiency and provide insights into the effectiveness of inference-time techniques."
                },
                "zh": {
                    "title": "è¯„ä¼°è¯­è¨€æ¨¡å‹çš„ç»æµä»·å€¼ä¸æ¨ç†æˆæœ¬",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ç»æµä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†ç”Ÿæˆç»æµä»·å€¼ä¸æ¨ç†æˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç”Ÿäº§ç†è®ºçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ¨ç†æˆæœ¬ã€‚å¼•å…¥äº†â€œæˆæœ¬-é€šè¿‡â€çš„æ¦‚å¿µï¼Œè¡¨ç¤ºç”Ÿæˆæ­£ç¡®è§£å†³æ–¹æ¡ˆçš„é¢„æœŸè´§å¸æˆæœ¬ï¼Œå¹¶å®šä¹‰äº†â€œå‰æ²¿æˆæœ¬-é€šè¿‡â€ï¼Œå³åœ¨å¯ç”¨æ¨¡å‹ä¸­å®ç°çš„æœ€ä½æˆæœ¬ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼Œè½»é‡çº§æ¨¡å‹åœ¨åŸºæœ¬å®šé‡ä»»åŠ¡ä¸­æœ€å…·æˆæœ¬æ•ˆç›Šï¼Œè€Œå¤§å‹æ¨¡å‹é€‚ç”¨äºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼Œæ¨ç†æ¨¡å‹åˆ™é€‚åˆå¤æ‚å®šé‡é—®é¢˜ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-18.html",
    "link_next": "2025-04-22.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "18.04",
        "en": "04/18",
        "zh": "4æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "22.04",
        "en": "04/22",
        "zh": "4æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 5,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 8,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 3
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ„å»ºæœ‰æ•ˆæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„å…³é”®å› ç´ ï¼šæ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚éšç€å¼€æºæ•°æ®é›†çš„å¢åŠ ï¼Œè‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡ä¸”å¤šæ ·çš„å­é›†å˜å¾—é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å®ä¾‹è´¨é‡ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™ç»´æŒå¤šæ ·æ€§ï¼Œä½† often æ•ˆæœä¸ä½³ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥æ¨¡æ‹Ÿè¯­ä¹‰ç©ºé—´ï¼Œå¹¶åŸºäºå›¾ä¸­çš„ä¿¡æ¯åˆ†å¸ƒé‡åŒ–å¤šæ ·æ€§ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’ŒåŸºç¡€æ¨¡å‹ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
        "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ„å»ºæœ‰æ•ˆæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„å…³é”®å› ç´ ï¼šæ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le gÃ²u jiÃ n yÇ’u xiÃ o zhÇ lÃ¬ng tiÃ¡o zhÄ›ng shÃ¹ jÃ¹ de guÇn jiÃ n yÄ«n sÃ¹: shÃ¹ jÃ¹ zhÃ¬ liÃ ng hÃ© duÅ yÃ ng xÃ¬ng.\n\néšç€å¼€æºæ•°æ®é›†çš„å¢åŠ ï¼Œè‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡ä¸”å¤šæ ·çš„å­é›†å˜å¾—é‡è¦ã€‚\nsuÃ­ zhe kÄi yuÃ¡n shÃ¹ jÃ¹ jÃ­ de zÄ“ng jiÄ, zÃ¬ dÃ²ng xuÇn zÃ© gÄo zhÃ¬ liÃ ng qiÄ› duÅ yÃ ng de zÇ jÃ­ biÃ n de zhÃ²ng yÃ o.\n\nç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å®ä¾‹è´¨é‡ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™ç»´æŒå¤šæ ·æ€§ï¼Œä½† often æ•ˆæœä¸ä½³ã€‚\nxiÃ n yÇ’u fÄng fÇ zhÇ” yÃ o guÄn zhÃ¹ shÃ­ lÃ¬ zhÃ¬ liÃ ng, shÇ yÃ²ng qÇ fÄ shÃ¬ guÄ« zÃ© wÃ©i chÃ­ duÅ yÃ ng xÃ¬ng, dÃ n often xiÃ o guÇ’ bÃ¹ jiÄ.\n\nä½œè€…æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥æ¨¡æ‹Ÿè¯­ä¹‰ç©ºé—´ï¼Œå¹¶åŸºäºå›¾ä¸­çš„ä¿¡æ¯åˆ†å¸ƒé‡åŒ–å¤šæ ·æ€§ã€‚\nzuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng xÄ«n fÄng fÇ, tÅng guÃ² gÃ²u jiÃ n biÄo qiÄn tÃº lÃ¡i mÃ³ nÇ yÇ” yÃ¬ kÅng jiÄn, bÃ¬ng jÄ« yÃº tÃº zhÅng de xÃ¬n xÄ« fÄ“n bÃ¹ liÃ ng huÃ  duÅ yÃ ng xÃ¬ng.\n\nå®éªŒæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’ŒåŸºç¡€æ¨¡å‹ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚\nshÃ­ yÃ n xiÇn shÃ¬, zhÃ¨ zhÇ’ng fÄng fÇ zÃ i duÅ gÃ¨ shÃ¹ jÃ¹ jÃ­ hÃ© jÄ« chÇ” mÃ³ xÃ­ng shÃ ng dÅu yÅu yÃº xiÃ n yÇ’u fÄng fÇ.",
        "vocab": "[\n    {\"word\": \"æ„å»º\", \"pinyin\": \"gÃ²ujiÃ n\", \"trans\": \"construct\"},\n    {\"word\": \"æœ‰æ•ˆ\", \"pinyin\": \"yÇ’uxiÃ o\", \"trans\": \"effective\"},\n    {\"word\": \"æŒ‡ä»¤\", \"pinyin\": \"zhÇlÃ¬ng\", \"trans\": \"instruction\"},\n    {\"word\": \"è°ƒæ•´\", \"pinyin\": \"tiÃ¡ozhÄ›ng\", \"trans\": \"adjust\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"å…³é”®å› ç´ \", \"pinyin\": \"guÇnjiÃ n yÄ«nsÃ¹\", \"trans\": \"key factors\"},\n    {\"word\": \"è´¨é‡\", \"pinyin\": \"zhÃ¬liÃ ng\", \"trans\": \"quality\"},\n    {\"word\": \"å¤šæ ·æ€§\", \"pinyin\": \"duÅyÃ ngxÃ¬ng\", \"trans\": \"diversity\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄiyuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"è‡ªåŠ¨\", \"pinyin\": \"zÃ¬dÃ²ng\", \"trans\": \"automatic\"},\n    {\"word\": \"é€‰æ‹©\", \"pinyin\": \"xuÇnzÃ©\", \"trans\": \"select\"},\n    {\"word\": \"å­é›†\", \"pinyin\": \"zÇjÃ­\", \"trans\": \"subset\"},\n    {\"word\": \"å˜å¾—\", \"pinyin\": \"biÃ ndÃ©\", \"trans\": \"become\"},\n    {\"word\": \"é‡è¦\", \"pinyin\": \"zhÃ²ngyÃ o\", \"trans\": \"important\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ nyÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"ä¸»è¦\", \"pinyin\": \"zhÇ”yÃ o\", \"trans\": \"main\"},\n    {\"word\": \"å…³æ³¨\", \"pinyin\": \"guÄnzhÃ¹\", \"trans\": \"focus on\"},\n    {\"word\": \"å®ä¾‹\", \"pinyin\": \"shÃ­lÃ¬\", \"trans\": \"instance\"},\n    {\"word\": \"å¯å‘å¼\", \"pinyin\": \"qÇfÄshÃ¬\", \"trans\": \"heuristic\"},\n    {\"word\": \"è§„åˆ™\", \"pinyin\": \"guÄ«zÃ©\", \"trans\": \"rule\"},\n    {\"word\": \"ç»´æŒ\", \"pinyin\": \"wÃ©ichÃ­\", \"trans\": \"maintain\"},\n    {\"word\": \"æ•ˆæœ\", \"pinyin\": \"xiÃ oguÇ’\", \"trans\": \"effect\"},\n    {\"word\": \"ä¸ä½³\", \"pinyin\": \"bÃ¹jiÄ\", \"trans\": \"poor\"},\n    {\"word\": \"ä½œè€…\", \"pinyin\": \"zuÃ²zhÄ›\", \"trans\": \"author\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"æ–°æ–¹æ³•\", \"pinyin\": \"xÄ«n fÄngfÇ\", \"trans\": \"new method\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅngguÃ²\", \"trans\": \"through\"},\n    {\"word\": \"æ ‡ç­¾\", \"pinyin\": \"biÄoqiÄn\", \"trans\": \"label\"},\n    {\"word\": \"å›¾\", \"pinyin\": \"tÃº\", \"trans\": \"graph\"},\n    {\"word\": \"æ¨¡æ‹Ÿ\", \"pinyin\": \"mÃ³nÇ\", \"trans\": \"simulate\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ”yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"ç©ºé—´\", \"pinyin\": \"kÅngjiÄn\", \"trans\": \"space\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ«yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"ä¿¡æ¯\", \"pinyin\": \"xÃ¬nxÄ«\", \"trans\": \"information\"},\n    {\"word\": \"åˆ†å¸ƒ\", \"pinyin\": \"fÄ“nbÃ¹\", \"trans\": \"distribution\"},\n    {\"word\": \"é‡åŒ–\", \"pinyin\": \"liÃ nghuÃ \", \"trans\": \"quantify\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇnshÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"ä¼˜äº\", \"pinyin\": \"yÅuyÃº\", \"trans\": \"superior to\"},\n    {\"word\": \"åŸºç¡€\", \"pinyin\": \"jÄ«chÇ”\", \"trans\": \"foundation\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"model\"}\n]",
        "trans": "This article discusses the key factors in constructing an effective dataset for instruction tuning: data quality and diversity. As the number of open-source datasets increases, it becomes important to automatically select high-quality and diverse subsets. Existing methods primarily focus on instance quality, using heuristic rules to maintain diversity, but often with limited effectiveness. The authors propose a new method that constructs a label graph to simulate semantic space and quantifies diversity based on the distribution of information in the graph. Experiments show that this method outperforms existing methods across multiple datasets and base models.",
        "update_ts": "2025-04-21 09:12"
    }
}