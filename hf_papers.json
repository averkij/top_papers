{
    "date": {
        "ru": "17 Ğ¸ÑĞ½Ñ",
        "en": "June 17",
        "zh": "6æœˆ17æ—¥"
    },
    "time_utc": "2025-06-17 02:43",
    "weekday": 1,
    "issue_id": 4324,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.13585",
            "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention",
            "url": "https://huggingface.co/papers/2506.13585",
            "abstract": "A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.",
            "score": 55,
            "issue_id": 4324,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ½Ñ",
                "en": "June 16",
                "zh": "6æœˆ16æ—¥"
            },
            "hash": "05163c188bd37051",
            "authors": [
                "MiniMax",
                ":",
                "Aili Chen",
                "Aonian Li",
                "Bangwei Gong",
                "Binyang Jiang",
                "Bo Fei",
                "Bo Yang",
                "Boji Shan",
                "Changqing Yu",
                "Chao Wang",
                "Cheng Zhu",
                "Chengjun Xiao",
                "Chengyu Du",
                "Chi Zhang",
                "Chu Qiao",
                "Chunhao Zhang",
                "Chunhui Du",
                "Congchao Guo",
                "Da Chen",
                "Deming Ding",
                "Dianjun Sun",
                "Dong Li",
                "Enwei Jiao",
                "Haigang Zhou",
                "Haimo Zhang",
                "Han Ding",
                "Haohai Sun",
                "Haoyu Feng",
                "Huaiguang Cai",
                "Haichao Zhu",
                "Jian Sun",
                "Jiaqi Zhuang",
                "Jiaren Cai",
                "Jiayuan Song",
                "Jin Zhu",
                "Jingyang Li",
                "Jinhao Tian",
                "Jinli Liu",
                "Junhao Xu",
                "Junjie Yan",
                "Junteng Liu",
                "Junxian He",
                "Kaiyi Feng",
                "Ke Yang",
                "Kecheng Xiao",
                "Le Han",
                "Leyang Wang",
                "Lianfei Yu",
                "Liheng Feng",
                "Lin Li",
                "Lin Zheng",
                "Linge Du",
                "Lingyu Yang",
                "Lunbin Zeng",
                "Minghui Yu",
                "Mingliang Tao",
                "Mingyuan Chi",
                "Mozhi Zhang",
                "Mujie Lin",
                "Nan Hu",
                "Nongyu Di",
                "Peng Gao",
                "Pengfei Li",
                "Pengyu Zhao",
                "Qibing Ren",
                "Qidi Xu",
                "Qile Li",
                "Qin Wang",
                "Rong Tian",
                "Ruitao Leng",
                "Shaoxiang Chen",
                "Shaoyu Chen",
                "Shengmin Shi",
                "Shitong Weng",
                "Shuchang Guan",
                "Shuqi Yu",
                "Sichen Li",
                "Songquan Zhu",
                "Tengfei Li",
                "Tianchi Cai",
                "Tianrun Liang",
                "Weiyu Cheng",
                "Weize Kong",
                "Wenkai Li",
                "Xiancai Chen",
                "Xiangjun Song",
                "Xiao Luo",
                "Xiao Su",
                "Xiaobo Li",
                "Xiaodong Han",
                "Xinzhu Hou",
                "Xuan Lu",
                "Xun Zou",
                "Xuyang Shen",
                "Yan Gong",
                "Yan Ma",
                "Yang Wang",
                "Yiqi Shi",
                "Yiran Zhong",
                "Yonghong Duan",
                "Yongxiang Fu",
                "Yongyi Hu",
                "Yu Gao",
                "Yuanxiang Fan",
                "Yufeng Yang",
                "Yuhao Li",
                "Yulin Hu",
                "Yunan Huang",
                "Yunji Li",
                "Yunzhi Xu",
                "Yuxin Mao",
                "Yuxuan Shi",
                "Yuze Wenren",
                "Zehan Li",
                "Zelin Li",
                "Zhanxu Tian",
                "Zhengmao Zhu",
                "Zhenhua Fan",
                "Zhenzhen Wu",
                "Zhichao Xu",
                "Zhihang Yu",
                "Zhiheng Lyu",
                "Zhuo Jiang",
                "Zibo Gao",
                "Zijia Wu",
                "Zijian Song",
                "Zijun Sun"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.13585.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#open_source",
                    "#architecture",
                    "#reasoning",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MiniMax-M1: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "MiniMax-M1 - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¼Ğ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. MiniMax-M1 Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Long-Input Processing with MiniMax-M1",
                    "desc": "MiniMax-M1 is a groundbreaking hybrid-attention reasoning model that utilizes a Mixture-of-Experts architecture and a lightning attention mechanism to efficiently handle long input sequences. With 456 billion parameters and the ability to process up to 1 million tokens, it significantly outperforms previous models in context length. The model is trained using a novel reinforcement learning algorithm called CISPO, which enhances training efficiency by clipping importance sampling weights. MiniMax-M1 demonstrates superior performance in complex tasks, particularly in software engineering and long-context applications, making it a valuable tool for various AI challenges."
                },
                "zh": {
                    "title": "é«˜æ•ˆé•¿è¾“å…¥å¤„ç†çš„æ··åˆæ³¨æ„åŠ›æ¨¡å‹",
                    "desc": "MiniMax-M1æ˜¯ä¸€ç§æ··åˆæ³¨æ„åŠ›æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„å’Œé—ªç”µæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†é•¿è¾“å…¥å’Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åŸºäºä¹‹å‰çš„MiniMax-Text-01æ¨¡å‹ï¼Œå…·æœ‰4560äº¿ä¸ªå‚æ•°ï¼Œå¹¶æ”¯æŒé«˜è¾¾100ä¸‡ä¸ªtokençš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚MiniMax-M1åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨è½¯ä»¶å·¥ç¨‹å’Œå·¥å…·åˆ©ç”¨æ–¹é¢ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•CISPOï¼Œè¿›ä¸€æ­¥æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11763",
            "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
            "url": "https://huggingface.co/papers/2506.11763",
            "abstract": "DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.",
            "score": 6,
            "issue_id": 4324,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "197213635094ee83",
            "authors": [
                "Mingxuan Du",
                "Benfeng Xu",
                "Chiwei Zhu",
                "Xiaorui Wang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "MetastoneTechnology, Beijing, China",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11763.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#science",
                    "#agents",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "DeepResearch Bench - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 100 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ PhD Ğ¿Ğ¾ 22 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‰Ğ¸ĞµÑÑ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼: Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. DeepResearch Bench Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Benchmarking Deep Research Agents for Superior Research Quality",
                    "desc": "DeepResearch Bench is a new framework designed to evaluate the performance of Deep Research Agents (DRAs) in generating high-quality research outputs. It includes 100 PhD-level tasks across 22 fields, allowing for a comprehensive assessment of DRAs' capabilities in information retrieval and report synthesis. The framework introduces two innovative evaluation methods: one that uses reference-based criteria to judge report quality, and another that measures citation effectiveness and accuracy. By providing these tools, DeepResearch Bench aims to enhance the development of LLM-based agents and improve their research efficiency."
                },
                "zh": {
                    "title": "è¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†çš„æ–°åŸºå‡†",
                    "desc": "DeepResearch Benchæ˜¯ä¸€ä¸ªåŸºå‡†æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†åœ¨ç ”ç©¶è´¨é‡å’Œä¿¡æ¯æ£€ç´¢å‡†ç¡®æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«100ä¸ªç”±é¢†åŸŸä¸“å®¶ç²¾å¿ƒè®¾è®¡çš„åšå£«çº§ç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–22ä¸ªä¸åŒé¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•æ¥è¯„ä¼°è¿™äº›ä»£ç†çš„èƒ½åŠ›ï¼Œä¸€ç§æ˜¯åŸºäºå‚è€ƒçš„è¯„ä¼°æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯è¯„ä¼°ä¿¡æ¯æ£€ç´¢å’Œå¼•ç”¨å‡†ç¡®æ€§çš„æ¡†æ¶ã€‚é€šè¿‡å¼€æºDeepResearch BenchåŠå…¶å…³é”®ç»„ä»¶ï¼Œæˆ‘ä»¬å¸Œæœ›åŠ é€ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13759",
            "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
            "url": "https://huggingface.co/papers/2506.13759",
            "abstract": "Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.   The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.   In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.   Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
            "score": 4,
            "issue_id": 4324,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ½Ñ",
                "en": "June 16",
                "zh": "6æœˆ16æ—¥"
            },
            "hash": "0a523ab9b7563360",
            "authors": [
                "Runpeng Yu",
                "Qi Li",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13759.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#diffusion",
                    "#inference",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLMs) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dMLLMs). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, dLLMs Ğ¸ dMLLMs Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ dLLMs Ğ¸ dMLLMs Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Accelerating Language Generation with Discrete Diffusion Models",
                    "desc": "This paper surveys Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs), highlighting their advantages over traditional autoregressive models. dLLMs and dMLLMs utilize a parallel decoding approach with full attention and denoising strategies, allowing for faster generation and improved output control. The paper reviews the historical development, mathematical foundations, and key techniques for training these models, as well as their applications in various domains. It also discusses the future potential of dLLMs and dMLLMs in advancing machine learning research and deployment."
                },
                "zh": {
                    "title": "ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼šåŠ é€Ÿç”Ÿæˆä¸æ§åˆ¶çš„æœªæ¥",
                    "desc": "æœ¬æ–‡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥äº†ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰å’Œç¦»æ•£æ‰©æ•£å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆdMLLMsï¼‰ã€‚ä¸è‡ªå›å½’æ¨¡å‹ä¸åŒï¼ŒdLLMså’ŒdMLLMsé‡‡ç”¨å¤šæ ‡è®°å¹¶è¡Œè§£ç çš„èŒƒå¼ï¼Œåˆ©ç”¨å…¨æ³¨æ„åŠ›æœºåˆ¶å’Œå»å™ªç”Ÿæˆç­–ç•¥ï¼Œä»è€Œå®ç°å¹¶è¡Œç”Ÿæˆå’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚è¿™ç§æ–°æ–¹æ³•ä½¿å¾—ç»†ç²’åº¦çš„è¾“å‡ºæ§åˆ¶å’ŒåŠ¨æ€å“åº”æ„ŸçŸ¥æˆä¸ºå¯èƒ½ï¼Œè¿™åœ¨è‡ªå›å½’æ¨¡å‹ä¸­æ˜¯éš¾ä»¥å®ç°çš„ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒdLLMså’ŒdMLLMsåœ¨æ¨ç†é€Ÿåº¦ä¸Šå¯å®ç°é«˜è¾¾10å€çš„åŠ é€Ÿï¼Œä¸”åœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08343",
            "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves\n  Reasoning Efficiency",
            "url": "https://huggingface.co/papers/2506.08343",
            "abstract": "NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.",
            "score": 4,
            "issue_id": 4324,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "bc2b3e7cb2a8d002",
            "authors": [
                "Chenlong Wang",
                "Yuanning Feng",
                "Dongping Chen",
                "Zhaoyang Chu",
                "Ranjay Krishna",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University College London",
                "University of Maryland",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08343.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… ÑĞ»Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ NoWait, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 27-51% Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ´ĞµÑÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. NoWait Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "NoWait: Streamlining Multimodal Reasoning for Efficiency",
                    "desc": "The paper introduces NoWait, a method that improves the efficiency of multimodal reasoning models by suppressing explicit self-reflection tokens during inference. These tokens, like 'Wait' and 'Hmm', often lead to unnecessary verbosity and can slow down the reasoning process. By removing these tokens, NoWait significantly shortens the reasoning paths while maintaining the effectiveness of the model. The results from various benchmarks demonstrate that this approach can reduce the length of reasoning trajectories by up to 51%, making it a valuable enhancement for large reasoning models."
                },
                "zh": {
                    "title": "NoWaitï¼šæå‡å¤šæ¨¡æ€æ¨ç†æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "NoWaitæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­æŠ‘åˆ¶æ˜¾å¼è‡ªæˆ‘åæ€çš„æ ‡è®°ï¼ˆå¦‚â€œç­‰ä¸€ä¸‹â€å’Œâ€œå—¯â€ï¼‰ï¼Œæ¥æé«˜å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡ï¼Œè€Œä¸é™ä½æ¨¡å‹çš„å®ç”¨æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ¨ç†æ—¶å¸¸å¸¸ä¼šå‡ºç°è¿‡åº¦æ€è€ƒï¼Œå¯¼è‡´è¾“å‡ºå†—é•¿ä¸”é‡å¤ï¼Œå½±å“æ•ˆç‡ã€‚é€šè¿‡åœ¨åä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œå¹¿æ³›å®éªŒï¼ŒNoWaitèƒ½å¤Ÿå°†æ€ç»´é“¾çš„é•¿åº¦å‡å°‘27%åˆ°51%ã€‚å› æ­¤ï¼ŒNoWaitä¸ºé«˜æ•ˆä¸”ä¿æŒå®ç”¨æ€§çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03968",
            "title": "From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding",
            "url": "https://huggingface.co/papers/2506.03968",
            "abstract": "The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.",
            "score": 4,
            "issue_id": 4324,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "991991c3f686afa8",
            "authors": [
                "Chiwei Zhu",
                "Benfeng Xu",
                "Xiaorui Wang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Metastone Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03968.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#alignment",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SynthQuestions, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ Ğ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²ĞµĞ±-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Harnessing Attributed Grounding for Diverse Instruction Generation",
                    "desc": "This paper introduces a novel method for generating diverse and complex instruction data for large language models (LLMs) through a technique called attributed grounding. The approach combines a top-down attribution process, which connects real-world instructions to specific user contexts, with a bottom-up synthesis process that creates meaningful instructions from web documents. By leveraging this framework, the authors successfully produce a large dataset of 1 million synthesized instructions, named SynthQuestions, which significantly enhances the performance of LLMs on various benchmarks. The results indicate that as more web data is utilized, the effectiveness of the models continues to improve, showcasing the importance of rich and varied instruction data for model alignment."
                },
                "zh": {
                    "title": "é€šè¿‡å±æ€§åŸºç¡€ç”Ÿæˆå¤æ‚æŒ‡ä»¤æ•°æ®ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å±æ€§åŸºç¡€ç”Ÿæˆå¤šæ ·åŒ–å’Œå¤æ‚çš„æŒ‡ä»¤æ•°æ®çš„æ–¹æ³•ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è‡ªä¸Šè€Œä¸‹çš„å½’å› è¿‡ç¨‹å’Œè‡ªä¸‹è€Œä¸Šçš„åˆæˆè¿‡ç¨‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»çœŸå®æŒ‡ä»¤å’Œç½‘ç»œæ–‡æ¡£ä¸­ç”Ÿæˆæœ‰æ„ä¹‰çš„æŒ‡ä»¤ã€‚é€šè¿‡è¿™ç§æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«100ä¸‡ä¸ªæŒ‡ä»¤çš„æ•°æ®é›†SynthQuestionsï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨ä¸°å¯Œçš„ç½‘ç»œæ–‡æ¡£å¯ä»¥å¤§è§„æ¨¡æ”¶é›†å¤æ‚çš„æŒ‡ä»¤ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07961",
            "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models",
            "url": "https://huggingface.co/papers/2506.07961",
            "abstract": "BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/",
            "score": 3,
            "issue_id": 4324,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "3fcf8d6329af3962",
            "authors": [
                "Peiyan Li",
                "Yixiang Chen",
                "Hongtao Wu",
                "Xiao Ma",
                "Xiangnan Wu",
                "Yan Huang",
                "Liang Wang",
                "Tao Kong",
                "Tieniu Tan"
            ],
            "affiliations": [
                "ByteDance Seed",
                "CASIA",
                "FiveAges",
                "NJU",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07961.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#3d",
                    "#games",
                    "#optimization",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "BridgeVLA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ 3D Ğ² 2D",
                    "desc": "BridgeVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ 3D-Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ 3D-Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 2D-Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. BridgeVLA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "BridgeVLA: Bridging 3D Vision and Action with 2D Heatmaps",
                    "desc": "BridgeVLA is a novel model that integrates 3D vision with language and action prediction by projecting 3D inputs into 2D images. This approach allows it to utilize 2D heatmaps for more efficient action prediction, enhancing the model's performance in robot manipulation tasks. The model is pre-trained to predict these heatmaps, which helps it learn effectively from fewer samples. Extensive testing shows that BridgeVLA significantly outperforms existing methods in various benchmarks, demonstrating its robustness and efficiency in real-world applications."
                },
                "zh": {
                    "title": "BridgeVLAï¼šé«˜æ•ˆçš„3Dè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
                    "desc": "BridgeVLAæ˜¯ä¸€ç§3Dè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œå®ƒå°†3Dè¾“å…¥æŠ•å½±åˆ°2Då›¾åƒï¼Œå¹¶åˆ©ç”¨2Dçƒ­å›¾è¿›è¡Œé«˜æ•ˆçš„åŠ¨ä½œé¢„æµ‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†3Dä¿¡å·æ•´åˆåˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­ï¼Œå……åˆ†åˆ©ç”¨äº†3Dæ•°æ®çš„ç©ºé—´ç»“æ„ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œä½¿å¾—è§†è§‰-è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸‹æ¸¸ç­–ç•¥å­¦ä¹ ä¹‹å‰é¢„æµ‹2Dçƒ­å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBridgeVLAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°äº†å“è¶Šçš„å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13750",
            "title": "Test3R: Learning to Reconstruct 3D at Test Time",
            "url": "https://huggingface.co/papers/2506.13750",
            "abstract": "Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, the reliance on pairwise prediction and the limited generalization capability inherently restrict the global geometric consistency. In this work, we introduce Test3R, a surprisingly simple test-time learning technique that significantly boosts geometric accuracy. Using image triplets (I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and (I_1,I_3). The core idea is to optimize the network at test time via a self-supervised objective: maximizing the geometric consistency between these two reconstructions relative to the common image I_1. This ensures the model produces cross-pair consistent outputs, regardless of the inputs. Extensive experiments demonstrate that our technique significantly outperforms previous state-of-the-art methods on the 3D reconstruction and multi-view depth estimation tasks. Moreover, it is universally applicable and nearly cost-free, making it easily applied to other models and implemented with minimal test-time training overhead and parameter footprint. Code is available at https://github.com/nopQAQ/Test3R.",
            "score": 2,
            "issue_id": 4324,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ½Ñ",
                "en": "June 16",
                "zh": "6æœˆ16æ—¥"
            },
            "hash": "68d521856e78273a",
            "authors": [
                "Yuheng Yuan",
                "Qiuhong Shen",
                "Shizun Wang",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13750.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Test3R: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Test3R - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Test3R Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Boosting 3D Reconstruction Accuracy with Test3R",
                    "desc": "Test3R is a novel test-time learning approach designed to improve the accuracy of 3D reconstruction by leveraging self-supervised learning on image triplets. It addresses the limitations of traditional dense matching methods that rely on pairwise predictions, which can lead to inconsistencies in global geometry. By optimizing the network's output during testing, Test3R ensures that reconstructions from different image pairs maintain geometric consistency relative to a common reference image. This technique not only enhances performance on 3D reconstruction tasks but is also easy to implement and applicable to various models with minimal additional training requirements."
                },
                "zh": {
                    "title": "Test3Rï¼šæå‡3Dé‡å»ºç²¾åº¦çš„ç®€å•æ–¹æ³•",
                    "desc": "Test3Ræ˜¯ä¸€ç§ç”¨äº3Dé‡å»ºçš„æµ‹è¯•æ—¶å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ä¼˜åŒ–ç½‘ç»œä¸€è‡´æ€§ï¼Œä»è€Œæé«˜å‡ ä½•ç²¾åº¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒä¸‰å…ƒç»„ç”Ÿæˆé‡å»ºï¼Œç¡®ä¿ä¸åŒå›¾åƒå¯¹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚Test3Rçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æµ‹è¯•æ—¶æœ€å¤§åŒ–é‡å»ºä¹‹é—´çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºåœ¨ä¸åŒè¾“å…¥ä¸‹ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTest3Råœ¨3Dé‡å»ºå’Œå¤šè§†è§’æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸”é€‚ç”¨æ€§å¹¿æ³›ï¼Œå‡ ä¹ä¸å¢åŠ æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12953",
            "title": "Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition",
            "url": "https://huggingface.co/papers/2506.12953",
            "abstract": "PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.",
            "score": 1,
            "issue_id": 4324,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ½Ñ",
                "en": "June 15",
                "zh": "6æœˆ15æ—¥"
            },
            "hash": "fb2789e38592ff5a",
            "authors": [
                "Mayank Bumb",
                "Anshul Vemulapalli",
                "Sri Harsha Vardhan Prasad Jella",
                "Anish Gupta",
                "An La",
                "Ryan A. Rossi",
                "Hongjie Chen",
                "Franck Dernoncourt",
                "Nesreen K. Ahmed",
                "Yu Wang"
            ],
            "affiliations": [
                "Adobe",
                "Dolby Labs",
                "Intel",
                "University of Massachusetts Amherst",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12953.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ PatchInstruct Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ², Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹. PatchInstruct Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLM Forecasting with Simple Prompting Techniques",
                    "desc": "PatchInstruct is a method that improves the forecasting abilities of Large Language Models (LLMs) by using innovative prompting techniques. It incorporates time series decomposition to break down data into manageable parts, patch-based tokenization to efficiently handle input, and similarity-based neighbor augmentation to enhance predictions by considering related data points. This approach allows LLMs to perform time series forecasting without the need for extensive fine-tuning or complex architectures. Overall, PatchInstruct simplifies the process while boosting the accuracy of predictions in time series analysis."
                },
                "zh": {
                    "title": "PatchInstructï¼šç®€åŒ–æ—¶é—´åºåˆ—é¢„æµ‹çš„æœ‰æ•ˆæ–¹æ³•",
                    "desc": "PatchInstructæ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶é—´åºåˆ—é¢„æµ‹è´¨é‡çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ä¸“é—¨çš„æç¤ºç­–ç•¥ï¼Œå¦‚æ—¶é—´åºåˆ—åˆ†è§£ã€åŸºäºè¡¥ä¸çš„æ ‡è®°åŒ–å’Œç›¸ä¼¼æ€§é‚»å±…å¢å¼ºï¼Œæ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸ä»¥å¾€éœ€è¦å¤§é‡å¾®è°ƒçš„æ–¹æ³•ä¸åŒï¼ŒPatchInstructèƒ½å¤Ÿåœ¨ä¸å¤æ‚é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œçµæ´»åœ°è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚è¯¥æ–¹æ³•ä¿æŒäº†ç®€å•æ€§ï¼Œå¹¶ä¸”å¯¹æ•°æ®çš„é¢„å¤„ç†è¦æ±‚æœ€ä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12623",
            "title": "MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos",
            "url": "https://huggingface.co/papers/2506.12623",
            "abstract": "A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization.",
            "score": 1,
            "issue_id": 4324,
            "pub_date": "2025-06-14",
            "pub_date_card": {
                "ru": "14 Ğ¸ÑĞ½Ñ",
                "en": "June 14",
                "zh": "6æœˆ14æ—¥"
            },
            "hash": "ef83eb4ade9dc4bf",
            "authors": [
                "Yuan Zang",
                "Hao Tan",
                "Seunghyun Yoon",
                "Franck Dernoncourt",
                "Jiuxiang Gu",
                "Kushal Kafle",
                "Chen Sun",
                "Trung Bui"
            ],
            "affiliations": [
                "Adobe Research",
                "Brown University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12623.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑƒĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ UI",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2413 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 167 Ñ‡Ğ°ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Learning with UI Video Summarization",
                    "desc": "This paper introduces a new benchmark and dataset specifically designed for multi-modal summarization of user interface (UI) instructional videos. The goal is to create efficient summaries that include step-by-step text instructions and key video frames, which are essential for effective learning. The authors highlight that existing benchmarks are inadequate for this purpose, as they focus on general video summarization rather than instructional content. Through extensive experiments on their dataset of 2,413 annotated UI instructional videos, they demonstrate that current multi-modal summarization techniques are not effective for this specific type of video, indicating a need for improved methods."
                },
                "zh": {
                    "title": "æå‡UIæ•™å­¦è§†é¢‘çš„å¤šæ¨¡æ€æ€»ç»“èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’Œæ•°æ®é›†ï¼Œç”¨äºå¤šæ¨¡æ€æ€»ç»“ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰æ•™å­¦è§†é¢‘ï¼Œæ—¨åœ¨æä¾›é€æ­¥å¯æ‰§è¡Œçš„æŒ‡ä»¤å’Œå…³é”®è§†é¢‘å¸§ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦å…³æ³¨ä¸€èˆ¬çš„è¯­ä¹‰çº§è§†é¢‘æ€»ç»“ï¼Œæ— æ³•æ»¡è¶³æ•™å­¦è§†é¢‘ä¸­å¯¹é€æ­¥æŒ‡ä»¤å’Œæ’å›¾çš„éœ€æ±‚ã€‚æˆ‘ä»¬æ”¶é›†äº†2413ä¸ªUIæ•™å­¦è§†é¢‘çš„æ•°æ®é›†ï¼Œæ‰‹åŠ¨æ ‡æ³¨äº†è§†é¢‘åˆ†å‰²ã€æ–‡æœ¬æ€»ç»“å’Œè§†é¢‘æ€»ç»“ï¼Œä»¥ä¾¿è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ€»ç»“æ–¹æ³•åœ¨UIè§†é¢‘æ€»ç»“ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†å¼€å‘æ–°æ–¹æ³•çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12189",
            "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis",
            "url": "https://huggingface.co/papers/2506.12189",
            "abstract": "The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.",
            "score": 0,
            "issue_id": 4324,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "952c0d68aa23cbda",
            "authors": [
                "Pranav Agarwal",
                "Ioana CiucÄƒ"
            ],
            "affiliations": [
                "Google Deep Research",
                "Institute",
                "Mila",
                "Quebec AI",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12189.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#small_models",
                    "#reasoning",
                    "#long_context",
                    "#interpretability",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¸Ñ… Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€Ñ‚Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Supernova Event Dataset, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 'Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸' ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´ĞµĞ»Ğ°Ñ Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unveiling LLM Personalities for Better Interpretability",
                    "desc": "This study investigates how different Large Language Models (LLMs) perform on various text tasks using a new dataset called the Supernova Event Dataset. The dataset includes a wide range of articles, allowing for the evaluation of LLMs in extracting and ranking key events, which requires complex reasoning and understanding of context. The research reveals distinct personality traits among the models, such as emotional reasoning in Orca 2 and strategic thinking in Qwen 2.5, enhancing our understanding of their decision-making processes. By using another LLM as a judge to assess these traits, the study improves the interpretability of models, making them more accessible for diverse applications."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§ç‰¹å¾",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒæ–‡æœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ã€‚é€šè¿‡åˆ†ææ¨¡å‹çš„é€‰æ‹©å’Œåˆ†ç±»äº‹ä»¶ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ¨¡å‹çš„ä¸ªæ€§ç‰¹å¾ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºçš„è¶…æ–°æ˜Ÿäº‹ä»¶æ•°æ®é›†åŒ…å«å¤šæ ·çš„æ–‡ç« ï¼Œå¸®åŠ©æˆ‘ä»¬åŸºå‡†æµ‹è¯•æ¨¡å‹åœ¨æå–å’Œæ’åºå…³é”®äº‹ä»¶æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨å¤„ç†æƒ…æ„Ÿæ¨ç†ã€æˆ˜ç•¥åˆ†æå’Œå› æœæ¨ç†ç­‰æ–¹é¢è¡¨ç°å‡ºæ˜æ˜¾çš„ä¸ªæ€§å·®å¼‚ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-16.html",
    "link_next": "2025-06-18.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "16.06",
        "en": "06/16",
        "zh": "6æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.06",
        "en": "06/18",
        "zh": "6æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}