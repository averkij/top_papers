{
    "date": {
        "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 18",
        "zh": "3æœˆ18æ—¥"
    },
    "time_utc": "2025-03-18 03:24",
    "weekday": 1,
    "issue_id": 2754,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.13327",
            "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations",
            "url": "https://huggingface.co/papers/2503.13327",
            "abstract": "We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning.",
            "score": 12,
            "issue_id": 2754,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "24a5e5d1d86949d5",
            "authors": [
                "Lan Chen",
                "Qi Mao",
                "Yuchao Gu",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "MIPG, Communication University of China",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13327.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Edit Transfer'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ĞµĞ³Ğ¾ Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ DiT. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµĞ³Ğ¾ 42 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Edit Transfer Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ TIE Ğ¸ RIE Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ½ĞµĞ¶ĞµÑÑ‚ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming Images with Just One Example!",
                    "desc": "This paper presents a novel approach called Edit Transfer, which enables a model to learn how to transform images using just one example of a source and target image. Unlike traditional text-based methods that struggle with geometric details and reference-based methods that focus on style, Edit Transfer effectively captures complex spatial transformations. The method utilizes a visual relation in-context learning paradigm, inspired by large language models, and employs a DiT-based text-to-image model. Remarkably, it achieves superior performance in non-rigid scenarios with only 42 training samples, showcasing the power of few-shot learning in visual transformations."
                },
                "zh": {
                    "title": "ç¼–è¾‘è½¬ç§»ï¼šå°‘æ ·æœ¬å­¦ä¹ çš„çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®¾ç½®ï¼Œç§°ä¸ºç¼–è¾‘è½¬ç§»ï¼ˆEdit Transferï¼‰ï¼Œæ¨¡å‹é€šè¿‡å•ä¸€çš„æº-ç›®æ ‡ç¤ºä¾‹å­¦ä¹ å˜æ¢ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚ä¸æ–‡æœ¬æ–¹æ³•åœ¨è¯­ä¹‰æ“ä½œä¸Šè¡¨ç°ä¼˜å¼‚ä½†åœ¨å‡ ä½•ç»†èŠ‚ä¸Šå­˜åœ¨å›°éš¾ä¸åŒï¼Œç¼–è¾‘è½¬ç§»é€šè¿‡æ˜ç¡®å­¦ä¹ æº-ç›®æ ‡å¯¹çš„ç¼–è¾‘å˜æ¢ï¼Œå…‹æœäº†æ–‡æœ¬å’Œå¤–è§‚å‚è€ƒçš„å±€é™æ€§ã€‚æˆ‘ä»¬å€Ÿé‰´å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæå‡ºäº†ä¸€ç§è§†è§‰å…³ç³»ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ï¼Œå¹¶åœ¨DiTåŸºç¡€çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šè¿›è¡Œæ„å»ºã€‚å°½ç®¡ä»…ä½¿ç”¨42ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œç¼–è¾‘è½¬ç§»åœ¨å¤šæ ·çš„éåˆšæ€§åœºæ™¯ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å°‘æ ·æœ¬è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12885",
            "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models",
            "url": "https://huggingface.co/papers/2503.12885",
            "abstract": "Image-conditioned generation methods, such as depth- and canny-conditioned approaches, have demonstrated remarkable abilities for precise image synthesis. However, existing models still struggle to accurately control the content of multiple instances (or regions). Even state-of-the-art models like FLUX and 3DIS face challenges, such as attribute leakage between instances, which limits user control. To address these issues, we introduce DreamRenderer, a training-free approach built upon the FLUX model. DreamRenderer enables users to control the content of each instance via bounding boxes or masks, while ensuring overall visual harmony. We propose two key innovations: 1) Bridge Image Tokens for Hard Text Attribute Binding, which uses replicated image tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely on text data, bind the correct visual attributes for each instance during Joint Attention; 2) Hard Image Attribute Binding applied only to vital layers. Through our analysis of FLUX, we identify the critical layers responsible for instance attribute rendering and apply Hard Image Attribute Binding only in these layers, using soft binding in the others. This approach ensures precise control while preserving image quality. Evaluations on the COCO-POS and COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models like GLIGEN and 3DIS by up to 26.8%. Project Page: https://limuloo.github.io/DreamRenderer/.",
            "score": 11,
            "issue_id": 2754,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "e6332652493dc1ab",
            "authors": [
                "Dewei Zhou",
                "Mingwei Li",
                "Zongxin Yang",
                "Yi Yang"
            ],
            "affiliations": [
                "DBMI, HMS, Harvard University",
                "RELER, CCAI, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12885.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#training",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DreamRenderer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FLUX. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾ÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "DreamRenderer: Precise Control in Image Generation",
                    "desc": "This paper presents DreamRenderer, a novel approach for image generation that allows precise control over multiple instances in an image. Unlike existing models, DreamRenderer uses a training-free method that leverages the FLUX model to bind visual attributes to specific instances through bounding boxes or masks. The key innovations include Bridge Image Tokens for ensuring accurate text-to-image attribute mapping and Hard Image Attribute Binding focused on critical layers for instance rendering. Evaluations show that DreamRenderer significantly outperforms FLUX and enhances other layout-to-image models, demonstrating its effectiveness in generating high-quality images with user-defined content."
                },
                "zh": {
                    "title": "DreamRendererï¼šç²¾ç¡®æ§åˆ¶å›¾åƒç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDreamRendererçš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤šå®ä¾‹å†…å®¹æ§åˆ¶æ–¹é¢çš„ä¸è¶³ã€‚DreamRendereråŸºäºFLUXæ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è¾¹ç•Œæ¡†æˆ–æ©ç ç²¾ç¡®æ§åˆ¶æ¯ä¸ªå®ä¾‹çš„å†…å®¹ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“è§†è§‰å’Œè°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯ä½¿ç”¨æ¡¥æ¥å›¾åƒæ ‡è®°æ¥ç¡®ä¿æ–‡æœ¬å±æ€§çš„å‡†ç¡®ç»‘å®šï¼ŒäºŒæ˜¯åœ¨å…³é”®å±‚ä¸­åº”ç”¨ç¡¬å›¾åƒå±æ€§ç»‘å®šï¼Œä»¥æé«˜å®ä¾‹å±æ€§æ¸²æŸ“çš„ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamRendereråœ¨å›¾åƒæˆåŠŸç‡ä¸Šæ¯”FLUXæé«˜äº†17.7%ï¼Œå¹¶ä¸”åœ¨å¸ƒå±€åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ä¸Šæå‡äº†å¤šè¾¾26.8%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12590",
            "title": "Personalize Anything for Free with Diffusion Transformer",
            "url": "https://huggingface.co/papers/2503.12590",
            "abstract": "Personalized image generation aims to produce images of user-specified concepts while enabling flexible editing. Recent training-free approaches, while exhibit higher computational efficiency than training-based methods, struggle with identity preservation, applicability, and compatibility with diffusion transformers (DiTs). In this paper, we uncover the untapped potential of DiT, where simply replacing denoising tokens with those of a reference subject achieves zero-shot subject reconstruction. This simple yet effective feature injection technique unlocks diverse scenarios, from personalization to image editing. Building upon this observation, we propose Personalize Anything, a training-free framework that achieves personalized image generation in DiT through: 1) timestep-adaptive token replacement that enforces subject consistency via early-stage injection and enhances flexibility through late-stage regularization, and 2) patch perturbation strategies to boost structural diversity. Our method seamlessly supports layout-guided generation, multi-subject personalization, and mask-controlled editing. Evaluations demonstrate state-of-the-art performance in identity preservation and versatility. Our work establishes new insights into DiTs while delivering a practical paradigm for efficient personalization.",
            "score": 5,
            "issue_id": 2754,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "25e3ee07c4a11ed2",
            "authors": [
                "Haoran Feng",
                "Zehuan Huang",
                "Lin Li",
                "Hairong Lv",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University",
                "Renmin University of China",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12590.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Personalize Anything, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ·Ğ°Ğ¼ĞµĞ½Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Personalize Anything: Efficient Image Generation with Diffusion Transformers",
                    "desc": "This paper presents a novel approach to personalized image generation using diffusion transformers (DiTs) without the need for extensive training. The authors introduce a technique that replaces denoising tokens with those from a reference subject, enabling effective zero-shot subject reconstruction. They propose a framework called Personalize Anything, which incorporates adaptive token replacement and patch perturbation strategies to enhance identity preservation and structural diversity. The results show that this method achieves state-of-the-art performance in generating personalized images while allowing for flexible editing options."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„æ–°è§†è§’",
                    "desc": "ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ—¨åœ¨æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„æ¦‚å¿µç”Ÿæˆå›¾åƒï¼Œå¹¶å…è®¸çµæ´»ç¼–è¾‘ã€‚æœ€è¿‘çš„æ— è®­ç»ƒæ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºåŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œä½†åœ¨èº«ä»½ä¿æŒã€é€‚ç”¨æ€§å’Œä¸æ‰©æ•£å˜æ¢å™¨çš„å…¼å®¹æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æ­ç¤ºäº†æ‰©æ•£å˜æ¢å™¨çš„æ½œåŠ›ï¼Œé€šè¿‡ç®€å•åœ°ç”¨å‚è€ƒå¯¹è±¡çš„å»å™ªä»¤ç‰Œæ›¿æ¢å®ç°é›¶-shotçš„å¯¹è±¡é‡å»ºã€‚æˆ‘ä»¬æå‡ºçš„â€œä¸ªæ€§åŒ–ä»»ä½•äº‹ç‰©â€æ¡†æ¶ï¼Œé€šè¿‡æ—¶é—´æ­¥è‡ªé€‚åº”ä»¤ç‰Œæ›¿æ¢å’Œè¡¥ä¸æ‰°åŠ¨ç­–ç•¥ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13435",
            "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
            "url": "https://huggingface.co/papers/2503.13435",
            "abstract": "With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D, which generates stable and high-quality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. Project: https://github.com/Gen-Verse/WideRange4D",
            "score": 4,
            "issue_id": 2754,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "c17d4be710ed24e0",
            "authors": [
                "Ling Yang",
                "Kaixin Zhu",
                "Juanxi Tian",
                "Bohan Zeng",
                "Mingbao Lin",
                "Hongjuan Pei",
                "Wentao Zhang",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13435.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#3d"
                ],
                "emoji": "ğŸŒ€",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸: Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WideRange4D Ğ´Ğ»Ñ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Progress4D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 4D-Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D-ÑÑ†ĞµĞ½. ĞœĞµÑ‚Ğ¾Ğ´ Progress4D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° WideRange4D. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Advancing 4D Reconstruction with Wide Spatial Movements",
                    "desc": "This paper addresses the limitations of current 4D reconstruction methods, which primarily focus on actions performed in place and struggle with wide-range spatial movements. The authors introduce a new benchmark called WideRange4D, which includes diverse 4D scene data featuring significant spatial variations, enabling better evaluation of 4D generation techniques. They also propose a novel reconstruction method named Progress4D, designed to produce stable and high-quality 4D results across complex scenarios. Experimental results demonstrate that Progress4D surpasses existing state-of-the-art methods in both quantitative and qualitative assessments."
                },
                "zh": {
                    "title": "çªç ´ç©ºé—´é™åˆ¶ï¼Œå®ç°é«˜è´¨é‡4Dé‡å»º",
                    "desc": "éšç€3Dé‡å»ºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œ4Dé‡å»ºç ”ç©¶ä¹Ÿåœ¨ä¸æ–­è¿›æ­¥ã€‚ç°æœ‰çš„4Dé‡å»ºæ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„4Dåœºæ™¯ï¼Œä½†åœ¨è·å–å¤šè§†è§’è§†é¢‘æ•°æ®æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ç°æœ‰åŸºå‡†ä¸»è¦å±•ç¤ºæœ‰é™åœºæ™¯ä¸­çš„åŠ¨ä½œã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„4Dé‡å»ºåŸºå‡†WideRange4Dï¼ŒåŒ…å«ä¸°å¯Œçš„4Dåœºæ™¯æ•°æ®ï¼Œå…è®¸å¯¹4Dç”Ÿæˆæ–¹æ³•çš„èƒ½åŠ›è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„4Dé‡å»ºæ–¹æ³•Progress4Dï¼Œåœ¨å„ç§å¤æ‚çš„4Dåœºæ™¯é‡å»ºä»»åŠ¡ä¸­ç”Ÿæˆç¨³å®šä¸”é«˜è´¨é‡çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13399",
            "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
            "url": "https://huggingface.co/papers/2503.13399",
            "abstract": "Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53\\%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at https://huggingface.co/datasets/jmhb/microvqa, and project page at https://jmhb0.github.io/microvqa.",
            "score": 3,
            "issue_id": 2754,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "50d4f1f510eff333",
            "authors": [
                "James Burgess",
                "Jeffrey J Nirschl",
                "Laura Bravo-SÃ¡nchez",
                "Alejandro Lozano",
                "Sanket Rajan Gupte",
                "Jesus G. Galaz-Montoya",
                "Yuhui Zhang",
                "Yuchang Su",
                "Disha Bhowmik",
                "Zachary Coman",
                "Sarina M. Hasan",
                "Alexandra Johannesson",
                "William D. Leineweber",
                "Malvika G Nair",
                "Ridhi Yarlagadda",
                "Connor Zuraski",
                "Wah Chiu",
                "Sarah Cohen",
                "Jan N. Hansen",
                "Manuel D Leonetti",
                "Chad Liu",
                "Emma Lundberg",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Chan Zuckerberg Biohub Network",
                "KTH Royal Institute of Technology",
                "Princeton University",
                "Stanford University",
                "Tsinghua University",
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13399.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#science"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "MicroVQA: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MicroVQA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1042 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 53%, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "MicroVQA: Advancing Multimodal Reasoning in Scientific Discovery",
                    "desc": "This paper introduces MicroVQA, a new benchmark for visual-question answering (VQA) that focuses on complex reasoning needed in scientific research, particularly in biology. It assesses three key capabilities: understanding images, generating hypotheses, and proposing experiments, using questions curated by biology experts. The benchmark consists of 1,042 multiple-choice questions designed to reflect real scientific practices, addressing the limitations of existing multimodal reasoning benchmarks. The study reveals that while current models perform at a peak of 53%, the challenges in multimodal reasoning are significant, emphasizing the need for improved AI tools in biomedical research."
                },
                "zh": {
                    "title": "MicroVQAï¼šæ¨åŠ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†MicroVQAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç”Ÿç‰©å­¦ç ”ç©¶çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ç§‘å­¦ç ”ç©¶ä¸­æ‰€éœ€çš„ä¸‰ç§æ¨ç†èƒ½åŠ›ï¼šä¸“å®¶å›¾åƒç†è§£ã€å‡è®¾ç”Ÿæˆå’Œå®éªŒææ¡ˆã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†æ—¶å­˜åœ¨ä¸è¶³ï¼ŒMicroVQAé€šè¿‡1,042ä¸ªç”±ç”Ÿç‰©å­¦ä¸“å®¶ç­–åˆ’çš„å¤šé¡¹é€‰æ‹©é¢˜æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚ç ”ç©¶å‘ç°ï¼Œæ ‡å‡†çš„å¤šé¡¹é€‰æ‹©é¢˜ç”Ÿæˆæ–¹æ³•å®¹æ˜“äº§ç”Ÿè¯­è¨€æ·å¾„ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæµç¨‹æ¥ä¼˜åŒ–é—®é¢˜å’Œç­”æ¡ˆçš„ç»“æ„ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„MLLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å°å‹LLMsçš„è¡¨ç°ç•¥é€Šäºé¡¶çº§æ¨¡å‹ï¼Œä½†è¯­è¨€æ¨ç†çš„éš¾åº¦ä½äºå¤šæ¨¡æ€æ¨ç†ï¼Œè¿™çªæ˜¾äº†åœ¨ç§‘å­¦æ¨ç†ä¸­çš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11495",
            "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
            "url": "https://huggingface.co/papers/2503.11495",
            "abstract": "Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames (\"when\") and then analyse the spatial relationships (\"where\") between key objects, and finally leverage these relationships to draw inferences (\"what\"). However, can Video Large Language Models (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained \"memory\" of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.",
            "score": 2,
            "issue_id": 2754,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "93b4a63d45a11f3d",
            "authors": [
                "Zixu Cheng",
                "Jian Hu",
                "Ziquan Liu",
                "Chenyang Si",
                "Wei Li",
                "Shaogang Gong"
            ],
            "affiliations": [
                "Nanjing University",
                "Nanyang Technological University",
                "Queen Mary University of London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11495.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº V-STaR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (RSTR), Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ²Ñ€ĞµĞ¼Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 14 Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Spatio-Temporal Reasoning",
                    "desc": "This paper explores how Video Large Language Models (Video-LLMs) can understand videos using a method similar to human reasoning, which involves identifying when events happen, where objects are located, and how they interact. The authors highlight that current benchmarks for Video-LLMs mainly check for object presence but fail to assess the models' ability to reason about relationships and interactions between objects. To address this, they introduce the Video Spatio-Temporal Reasoning (V-STaR) benchmark, which breaks down video understanding into a task that evaluates object presence, timing of events, and spatial relationships while capturing the reasoning process. Their findings show that there are significant gaps in the reasoning capabilities of existing Video-LLMs, indicating a need for improved models that can perform robust spatio-temporal reasoning."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç†è§£çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰åœ¨è§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç§°ä¸ºè§†é¢‘æ—¶ç©ºæ¨ç†ï¼ˆV-STaRï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è¯†åˆ«å¯¹è±¡ã€äº‹ä»¶å‘ç”Ÿæ—¶é—´å’Œç©ºé—´ä½ç½®æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«ç»†è‡´æ¨ç†é“¾çš„é—®é¢˜æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„Video-LLMsåœ¨æ—¶ç©ºæ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-17.html",
    "link_next": "2025-03-19.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "17.03",
        "en": "03/17",
        "zh": "3æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.03",
        "en": "03/19",
        "zh": "3æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä¸€ç§åä¸ºReCamMasterçš„è§†é¢‘é‡æ–°æ¸²æŸ“æ¡†æ¶ã€‚å®ƒå¯ä»¥æ”¹å˜ç»™å®šè§†é¢‘çš„æ‘„åƒæœºè½¨è¿¹ï¼ŒåŒæ—¶ä¿æŒå¤šå¸§å¤–è§‚å’ŒåŠ¨æ€åŒæ­¥ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡Unreal Engine 5æ„å»ºäº†ä¸€ä¸ªå¤šæ‘„åƒæœºåŒæ­¥è§†é¢‘æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨è§†é¢‘ç¨³å®šã€è¶…åˆ†è¾¨ç‡å’Œè§†é¢‘æ‰©å±•æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚",
        "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä¸€ç§åä¸ºReCamMasterçš„è§†é¢‘é‡æ–°æ¸²æŸ“æ¡†æ¶ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le yÄ« zhÇ’ng mÃ­ngwÃ¨i ReCamMaster de shÃ¬pÃ­n chÃ³ngxÄ«n xuÃ njiÃ n kuÃ ngjiÃ .\n\nå®ƒå¯ä»¥æ”¹å˜ç»™å®šè§†é¢‘çš„æ‘„åƒæœºè½¨è¿¹ï¼ŒåŒæ—¶ä¿æŒå¤šå¸§å¤–è§‚å’ŒåŠ¨æ€åŒæ­¥ã€‚\nTÄ kÄ›yÇ gÇibiÃ n gÄ›idÃ¬ng shÃ¬pÃ­n de shÃ¨xiÃ ngjÄ« guÇjÄ«, tÃ³ngshÃ­ bÇochÃ­ duÅzhÄ“n wÃ iguÇn hÃ© dÃ²ngtÃ i tÃ³ngbÃ¹.\n\nè¯¥æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡Unreal Engine 5æ„å»ºäº†ä¸€ä¸ªå¤šæ‘„åƒæœºåŒæ­¥è§†é¢‘æ•°æ®é›†ã€‚\nGÇi fÄngfÇ lÃ¬yÃ²ng le yÃ¹ xÃ¹nliÃ n de wÃ©nbÄ›n dÃ o shÃ¬pÃ­n mÃ³xÃ­ng de shÄ“ngchÄ“ng nÃ©nglÃ¬, bÃ¬ng tÅngguÃ² Unreal Engine 5 gÃ²ujiÃ n le yÄ«gÃ¨ duÅ shÃ¨xiÃ ngjÄ« tÃ³ngbÃ¹ shÃ¬pÃ­n shÃ¹jÃ¹jÃ­.\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨è§†é¢‘ç¨³å®šã€è¶…åˆ†è¾¨ç‡å’Œè§†é¢‘æ‰©å±•æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚\nShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, zhÃ¨ zhÇ’ng fÄngfÇ zÃ i shÃ¬pÃ­n wÄ›ndÃ¬ng, chÄo fÄ“nbiÄnlÇœ hÃ© shÃ¬pÃ­n kuÃ²zhÇn fÄngmiÃ n biÇoxiÃ n chÅ«sÃ¨.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'åä¸º', 'pinyin': 'mÃ­ng wÃ©i', 'trans': 'named'},\n{'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ­n', 'trans': 'video'},\n{'word': 'é‡æ–°', 'pinyin': 'chÃ³ng xÄ«n', 'trans': 'again'},\n{'word': 'æ¸²æŸ“', 'pinyin': 'xuÃ n rÃ¡n', 'trans': 'render'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'æ‘„åƒæœº', 'pinyin': 'shÃ¨ xiÃ ng jÄ«', 'trans': 'camera'},\n{'word': 'è½¨è¿¹', 'pinyin': 'guÇ jÃ¬', 'trans': 'trajectory'},\n{'word': 'åŒæ—¶', 'pinyin': 'tÃ³ng shÃ­', 'trans': 'simultaneously'},\n{'word': 'ä¿æŒ', 'pinyin': 'bÇo chÃ­', 'trans': 'maintain'},\n{'word': 'å¤šå¸§', 'pinyin': 'duÅ zhÄ“n', 'trans': 'multi-frame'},\n{'word': 'å¤–è§‚', 'pinyin': 'wÃ i guÇn', 'trans': 'appearance'},\n{'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'},\n{'word': 'åŒæ­¥', 'pinyin': 'tÃ³ng bÃ¹', 'trans': 'synchronization'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬ yÃ²ng', 'trans': 'utilize'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-trained'},\n{'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'},\n{'word': 'æ„å»º', 'pinyin': 'gÃ²u jiÃ n', 'trans': 'construct'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇn shÃ¬', 'trans': 'display'},\n{'word': 'ç¨³å®š', 'pinyin': 'wÄ›n dÃ¬ng', 'trans': 'stable'},\n{'word': 'è¶…åˆ†è¾¨ç‡', 'pinyin': 'chÄo fÄ“n biÃ n lÇœ', 'trans': 'super-resolution'},\n{'word': 'æ‰©å±•', 'pinyin': 'kuÃ² zhÇn', 'trans': 'extend'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'}]",
        "trans": "This article discusses a video re-rendering framework called ReCamMaster. It can alter the camera trajectory of a given video while maintaining multi-frame appearance and dynamic synchronization. The method leverages the generative capabilities of a pre-trained text-to-video model and constructs a multi-camera synchronized video dataset using Unreal Engine 5. Experimental results demonstrate that this method performs excellently in video stabilization, super-resolution, and video extension.",
        "update_ts": "2025-03-17 09:12"
    }
}