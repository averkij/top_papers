{
    "date": {
        "ru": "28 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 28",
        "zh": "3æœˆ28æ—¥"
    },
    "time_utc": "2025-03-28 23:10",
    "weekday": 4,
    "issue_id": 2961,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.21776",
            "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
            "url": "https://huggingface.co/papers/2503.21776",
            "abstract": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released.",
            "score": 57,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "f88fc5679e0ee30c",
            "authors": [
                "Kaituo Feng",
                "Kaixiong Gong",
                "Bohao Li",
                "Zonghao Guo",
                "Yibing Wang",
                "Tianshuo Peng",
                "Benyou Wang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "CUHK (SZ)",
                "CUHK MMLab",
                "Tsinghua University",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21776.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Video-R1: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Video-R1 - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ R1 Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ T-GRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ĞºĞ°Ğº Ğ²Ğ¸Ğ´ĞµĞ¾-, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Video-R1 Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Video Reasoning with Temporal Insights and Image Data",
                    "desc": "This paper presents Video-R1, a novel approach to enhance video reasoning capabilities in multimodal large language models (MLLMs) using rule-based reinforcement learning (RL). The authors identify two main challenges: the need for better temporal modeling in video reasoning and the limited availability of high-quality video-reasoning data. To overcome these challenges, they introduce the T-GRPO algorithm, which leverages temporal information, and augment the training process with high-quality image-reasoning data. The results show that Video-R1 significantly outperforms existing benchmarks, achieving notable accuracy improvements in video reasoning tasks."
                },
                "zh": {
                    "title": "Video-R1ï¼šè§†é¢‘æ¨ç†çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Video-R1ï¼Œè¿™æ˜¯é¦–æ¬¡ç³»ç»Ÿæ¢ç´¢åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­é€šè¿‡è§„åˆ™åŸºç¡€çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥å¼•å‘è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†T-GRPOç®—æ³•ï¼Œä»¥è§£å†³è§†é¢‘æ¨ç†ä¸­çš„æ—¶é—´å»ºæ¨¡ä¸è¶³å’Œé«˜è´¨é‡è§†é¢‘æ¨ç†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆé«˜è´¨é‡çš„å›¾åƒæ¨ç†æ•°æ®ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«ç”¨äºå†·å¯åŠ¨å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-R1åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå°¤å…¶åœ¨è§†é¢‘ç©ºé—´æ¨ç†åŸºå‡†VSI-benchä¸Šè¾¾åˆ°äº†35.8%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å•†ä¸šæ¨¡å‹GPT-4oã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21620",
            "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2503.21620",
            "abstract": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain.",
            "score": 39,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "81580448c4650ed8",
            "authors": [
                "Zhengxi Lu",
                "Yuxiang Chai",
                "Yaxuan Guo",
                "Xi Yin",
                "Liang Liu",
                "Hao Wang",
                "Guanjing Xiong",
                "Hongsheng Li"
            ],
            "affiliations": [
                "MMLab @ CUHK",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21620.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#dataset",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ GUI Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° (GUI). ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹, Ğ½Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 136 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UI-R1-3B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ²Ğ½ĞµĞ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ GUI."
                },
                "en": {
                    "title": "Enhancing GUI Action Prediction with Rule-Based Reinforcement Learning",
                    "desc": "This paper introduces a novel approach to enhance the reasoning capabilities of multimodal large language models (MLLMs) using rule-based reinforcement learning (RL) for predicting actions in graphic user interfaces (GUIs). The authors curate a high-quality dataset of 136 challenging tasks that involve common actions on mobile devices, allowing for effective model training. They propose a unified rule-based action reward system that optimizes the model through policy-based algorithms like Group Relative Policy Optimization (GRPO). Experimental results show that their model, UI-R1-3B, significantly outperforms the base model in both in-domain and out-of-domain tasks, highlighting the effectiveness of rule-based RL in improving GUI action prediction."
                },
                "zh": {
                    "title": "åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æå‡GUIåŠ¨ä½œé¢„æµ‹èƒ½åŠ›",
                    "desc": "æœ€è¿‘çš„DeepSeek-R1å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è€Œå±•ç°å‡ºçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå°è€Œé«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«136ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ¶µç›–äº”ç§å¸¸è§çš„ç§»åŠ¨è®¾å¤‡åŠ¨ä½œç±»å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ•°æ®é«˜æ•ˆæ¨¡å‹UI-R1-3Båœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå±•ç¤ºäº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨æå‡GUIç†è§£å’Œæ§åˆ¶æ–¹é¢çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21380",
            "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models",
            "url": "https://huggingface.co/papers/2503.21380",
            "abstract": "In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
            "score": 30,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "923e6e5f48ca95e4",
            "authors": [
                "Haoxiang Sun",
                "Yingqian Min",
                "Zhipeng Chen",
                "Wayne Xin Zhao",
                "Zheng Liu",
                "Zhongyuan Wang",
                "Lei Fang",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "BAAI",
                "DataCanvas Alaya NeW",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "School of Information, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21380.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multilingual",
                    "#benchmark",
                    "#low_resource",
                    "#math"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "OlymMATH: ĞĞ¾Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾Ñ‚Ğ° Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜",
                    "desc": "OlymMATH - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 200 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "OlymMATH: Raising the Bar for Mathematical Reasoning Evaluation",
                    "desc": "The paper introduces OlymMATH, a new benchmark for evaluating the mathematical reasoning abilities of large language models (LLMs). It consists of 200 carefully curated problems, verified for accuracy, and presented in both English and Chinese. The problems are categorized into two difficulty levels: AIME-level (easy) and more challenging problems (hard) that test the limits of current models. Empirical results show that even advanced models struggle with the hard problems, highlighting the benchmark's effectiveness in assessing complex reasoning skills."
                },
                "zh": {
                    "title": "OlymMATHï¼šæŒ‘æˆ˜æ•°å­¦æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—ç°æœ‰çš„æ•°å­¦æ¨ç†è¯„ä¼°åŸºå‡†è¶‹äºé¥±å’Œï¼Œè¿«åˆ‡éœ€è¦æ›´å…·æŒ‘æˆ˜æ€§å’Œä¸¥æ ¼æ€§çš„è¯„ä¼°æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OlymMATHï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¥¥æ—åŒ¹å…‹çº§æ•°å­¦åŸºå‡†ï¼Œæ—¨åœ¨ä¸¥æ ¼æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚OlymMATHåŒ…å«200ä¸ªç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜ï¼Œåˆ†ä¸ºAIMEçº§ï¼ˆç®€å•ï¼‰å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„ï¼ˆå›°éš¾ï¼‰ä¸¤ç§éš¾åº¦ï¼Œæ¶µç›–å››ä¸ªæ ¸å¿ƒæ•°å­¦é¢†åŸŸï¼Œå¹¶æä¾›å¯éªŒè¯çš„æ•°å€¼è§£ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒOlymMATHå¯¹å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹æå‡ºäº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å›°éš¾å­é›†ä¸Šï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§æ˜æ˜¾æœ‰é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21755",
            "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness",
            "url": "https://huggingface.co/papers/2503.21755",
            "abstract": "Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real \"world models\" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored for individual dimensions, our evaluation framework integrates generalists such as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive annotations to ensure alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.",
            "score": 25,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "4bd65083c265f9a4",
            "authors": [
                "Dian Zheng",
                "Ziqi Huang",
                "Hongbo Liu",
                "Kai Zou",
                "Yinan He",
                "Fan Zhang",
                "Yuanhan Zhang",
                "Jingwen He",
                "Wei-Shi Zheng",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Sun Yat-Sen University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21755.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#alignment",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VBench-2.0 - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ²ĞµÑ€ÑĞ¸Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, VBench-2.0 Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿ÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ, ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ». VBench-2.0 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ (VLM, LLM), Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Towards Realism: VBench-2.0 for Intrinsic Faithfulness in Video Generation",
                    "desc": "This paper discusses the evolution of video generation models from producing unrealistic outputs to creating visually convincing and temporally coherent videos. It highlights the limitations of current evaluation benchmarks like VBench, which focus on superficial aspects of faithfulness rather than adherence to real-world principles. The authors introduce VBench-2.0, a new benchmark that evaluates video generative models based on intrinsic faithfulness, which includes factors like physics, commonsense reasoning, and anatomical correctness. By emphasizing these deeper aspects of realism, VBench-2.0 aims to improve the quality of video generation for applications such as AI-assisted filmmaking and simulated world modeling."
                },
                "zh": {
                    "title": "è¿½æ±‚å†…åœ¨å¯ä¿¡åº¦çš„ä¸‹ä¸€ä»£è§†é¢‘ç”Ÿæˆæ ‡å‡†",
                    "desc": "è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä»æœ€åˆç”Ÿæˆä¸çœŸå®çš„è¾“å‡ºåˆ°ç°åœ¨èƒ½å¤Ÿç”Ÿæˆè§†è§‰ä¸Šä»¤äººä¿¡æœä¸”æ—¶é—´ä¸Šè¿è´¯çš„è§†é¢‘ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå¼€å‘äº†VBenchç­‰åŸºå‡†ï¼Œä¸»è¦æµ‹é‡æ¯å¸§çš„ç¾è§‚æ€§ã€æ—¶é—´ä¸€è‡´æ€§å’ŒåŸºæœ¬æç¤ºéµå¾ªç­‰å› ç´ ã€‚ç„¶è€Œï¼Œè¿™äº›è¯„ä¼°ä¸»è¦å…³æ³¨è¡¨é¢ä¸Šçš„å¯ä¿¡åº¦ï¼Œè€Œä¸æ˜¯è§†é¢‘æ˜¯å¦éµå¾ªç°å®ä¸–ç•Œçš„åŸåˆ™ã€‚ä¸ºå®ç°çœŸæ­£çš„â€œä¸–ç•Œæ¨¡å‹â€ï¼Œæˆ‘ä»¬å¼•å…¥äº†VBench-2.0ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å†…åœ¨å¯ä¿¡åº¦ï¼Œç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ç¬¦åˆç‰©ç†æ³•åˆ™å’Œå¸¸è¯†æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21460",
            "title": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges",
            "url": "https://huggingface.co/papers/2503.21460",
            "abstract": "The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.",
            "score": 22,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "8295a3726d0cc1b8",
            "authors": [
                "Junyu Luo",
                "Weizhi Zhang",
                "Ye Yuan",
                "Yusheng Zhao",
                "Junwei Yang",
                "Yiyang Gu",
                "Bohan Wu",
                "Binqi Chen",
                "Ziyue Qiao",
                "Qingqing Long",
                "Rongcheng Tu",
                "Xiao Luo",
                "Wei Ju",
                "Zhiping Xiao",
                "Yifan Wang",
                "Meng Xiao",
                "Chenwu Liu",
                "Jingyang Yuan",
                "Shichang Zhang",
                "Yiqiao Jin",
                "Fan Zhang",
                "Xian Wu",
                "Hanqing Zhao",
                "Dacheng Tao",
                "Philip S. Yu",
                "Ming Zhang"
            ],
            "affiliations": [
                "Computer Network Information Center, Chinese Academy of Sciences, Beijing, China",
                "Department of Computer Science, University of California, Los Angeles, USA",
                "Department of Computer Science, University of Illinois at Chicago, Chicago, USA",
                "Georgia Institute of Technology, Atlanta, USA",
                "Harvard University, Cambridge, USA",
                "Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China",
                "Nanyang Technological University, Singapore",
                "Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, USA",
                "School of Computer Science and PKU-Anker LLM Lab, Peking University, Beijing, China",
                "School of Computing and Information Technology, Great Bay University, Guangdong, China",
                "School of Information Technology & Management, University of International Business and Economics, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21460.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#agents",
                    "#architecture",
                    "#survey"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ LLM: Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² LLM. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² LLM Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unifying the Future of Intelligent LLM Agents",
                    "desc": "This paper explores the advancements in Large Language Model (LLM) agents, which are intelligent systems capable of adapting and achieving specific goals. It presents a structured taxonomy that categorizes LLM agent systems based on their architecture, collaboration methods, and evolutionary processes. The authors aim to connect various research areas by highlighting the relationship between design principles and the behaviors of these agents in complex environments. Additionally, the paper discusses evaluation methods, practical challenges, and potential applications, providing a comprehensive overview for future research in this field."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼šé€šå‘äººå·¥é€šç”¨æ™ºèƒ½çš„å…³é”®",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„ç³»ç»Ÿï¼Œå¼ºè°ƒå…¶åœ¨å®ç°äººå·¥é€šç”¨æ™ºèƒ½æ–¹é¢çš„é‡è¦æ€§ã€‚é€šè¿‡å»ºç«‹ä¸€ä¸ªä»¥æ–¹æ³•è®ºä¸ºä¸­å¿ƒçš„åˆ†ç±»æ³•ï¼Œæ–‡ç« å°†ä»£ç†çš„æ¶æ„åŸºç¡€ã€åä½œæœºåˆ¶å’Œæ¼”åŒ–è·¯å¾„è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æã€‚æˆ‘ä»¬æ­ç¤ºäº†ä»£ç†è®¾è®¡åŸåˆ™ä¸å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­æ¶Œç°è¡Œä¸ºä¹‹é—´çš„åŸºæœ¬è”ç³»ï¼Œä»è€Œç»Ÿä¸€äº†åˆ†æ•£çš„ç ”ç©¶çº¿ç´¢ã€‚æœ€åï¼Œæœ¬æ–‡ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»ä½“ç³»ï¼Œä»¥ç†è§£LLMä»£ç†ï¼Œå¹¶æŒ‡å‡ºæœªæ¥ç ”ç©¶çš„æœ‰å‰æ™¯æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21749",
            "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis",
            "url": "https://huggingface.co/papers/2503.21749",
            "abstract": "We introduce LeX-Art, a comprehensive suite for high-quality text-image synthesis that systematically bridges the gap between prompt expressiveness and text rendering fidelity. Our approach follows a data-centric paradigm, constructing a high-quality data synthesis pipeline based on Deepseek-R1 to curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined 1024times1024 images. Beyond dataset construction, we develop LeX-Enhancer, a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX and LeX-Lumina, achieving state-of-the-art text rendering performance. To systematically evaluate visual text generation, we introduce LeX-Bench, a benchmark that assesses fidelity, aesthetics, and alignment, complemented by Pairwise Normalized Edit Distance (PNED), a novel metric for robust text accuracy evaluation. Experiments demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX outperforming baselines in color (+3.18%), positional (+4.45%), and font accuracy (+3.81%). Our codes, models, datasets, and demo are publicly available.",
            "score": 19,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "fe7d17315ae060c8",
            "authors": [
                "Shitian Zhao",
                "Qilong Wu",
                "Xinyue Li",
                "Bo Zhang",
                "Ming Li",
                "Qi Qin",
                "Dongyang Liu",
                "Kaipeng Zhang",
                "Hongsheng Li",
                "Yu Qiao",
                "Peng Gao",
                "Bin Fu",
                "Zhen Li"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21749.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "LeX-Art: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "LeX-Art Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Deepseek-R1 Ğ´Ğ»Ñ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LeX-10K. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ LeX-Enhancer Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-image: LeX-FLUX Ğ¸ LeX-Lumina, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LeX-Bench Ğ¸ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° PNED Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Bridging Text and Image: LeX-Art's High-Quality Synthesis Revolution",
                    "desc": "LeX-Art is a new system designed to create high-quality images from text prompts, focusing on improving how well the text is rendered in the images. It builds a large dataset called LeX-10K, which contains 10,000 high-resolution images that are visually appealing. The system includes a model called LeX-Enhancer that improves the prompts used for generating images, and two advanced text-to-image models, LeX-FLUX and LeX-Lumina, which achieve top performance in rendering text. Additionally, LeX-Art introduces a benchmark called LeX-Bench to evaluate the quality of the generated images, using a new metric called Pairwise Normalized Edit Distance (PNED) to measure text accuracy."
                },
                "zh": {
                    "title": "é«˜è´¨é‡æ–‡æœ¬-å›¾åƒåˆæˆçš„æ–°çªç ´",
                    "desc": "LeX-Art æ˜¯ä¸€ä¸ªå…¨é¢çš„æ–‡æœ¬-å›¾åƒåˆæˆå·¥å…·ï¼Œæ—¨åœ¨æé«˜æç¤ºè¡¨è¾¾èƒ½åŠ›å’Œæ–‡æœ¬æ¸²æŸ“çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æ„å»ºäº† LeX-10K æ•°æ®é›†ï¼ŒåŒ…å« 10,000 å¼ é«˜åˆ†è¾¨ç‡ã€ç»è¿‡ç¾å­¦ä¼˜åŒ–çš„å›¾åƒï¼Œå¹¶å¼€å‘äº† LeX-Enhancer æ¨¡å‹æ¥å¢å¼ºæç¤ºæ•ˆæœã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸¤ä¸ªæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ LeX-FLUX å’Œ LeX-Luminaï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ–‡æœ¬æ¸²æŸ“æ€§èƒ½ã€‚é€šè¿‡ LeX-Bench åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è§†è§‰æ–‡æœ¬ç”Ÿæˆçš„ä¿çœŸåº¦ã€ç¾å­¦å’Œä¸€è‡´æ€§ï¼Œå®éªŒç»“æœæ˜¾ç¤ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21729",
            "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation",
            "url": "https://huggingface.co/papers/2503.21729",
            "abstract": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).",
            "score": 16,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "83959049f8af99fe",
            "authors": [
                "Zhicheng Lee",
                "Shulin Cao",
                "Jinxin Liu",
                "Jiajie Zhang",
                "Weichuan Liu",
                "Xiaoyin Che",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Siemens AG",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21729.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rag",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ReaRAG: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ReaRAG - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ReaRAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Factuality in Reasoning with ReaRAG",
                    "desc": "This paper introduces ReaRAG, a new reasoning model designed to improve the factual accuracy of Large Reasoning Models (LRMs) in question answering tasks. Unlike traditional RL-based LRMs that often overthink and lack robustness, ReaRAG employs a structured approach to reasoning by limiting the length of reasoning chains and allowing for diverse query exploration. The model uses a two-step action process, where it can either search for information or finish the reasoning process, enhancing its ability to refine answers based on retrieved data. Overall, ReaRAG demonstrates superior performance in multi-hop question answering by effectively combining reasoning capabilities with retrieval mechanisms."
                },
                "zh": {
                    "title": "å¢å¼ºäº‹å®æ€§çš„æ¨ç†æ¨¡å‹ReaRAG",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä¸»è¦ä¾èµ–å‚æ•°çŸ¥è¯†ï¼Œé™åˆ¶äº†äº‹å®å‡†ç¡®æ€§ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶ä¸ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„LRMså¢åŠ äº†æ£€ç´¢èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†æ—¶å®¹æ˜“è¿‡åº¦æ€è€ƒï¼Œç¼ºä¹ç¨³å¥æ€§ï¼Œä»è€Œé™ä½äº†åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReaRAGï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºäº‹å®æ€§çš„æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè¿‡å¤šè¿­ä»£çš„æƒ…å†µä¸‹æ¢ç´¢å¤šæ ·åŒ–çš„æŸ¥è¯¢ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä¸€ä¸ªæ–°é¢–çš„æ•°æ®æ„å»ºæ¡†æ¶ï¼Œå¹¶å¯¹æ¨ç†é“¾çš„é•¿åº¦è®¾å®šä¸Šé™ï¼Œä»è€Œæé«˜äº†LRMsçš„äº‹å®æ€§å’Œæ¨ç†çš„ç¨³å¥æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21248",
            "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition",
            "url": "https://huggingface.co/papers/2503.21248",
            "abstract": "Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as \"research hypothesis mines\", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.",
            "score": 15,
            "issue_id": 2944,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "c58f620a0f532dc0",
            "authors": [
                "Yujie Liu",
                "Zonglin Yang",
                "Tong Xie",
                "Jinjie Ni",
                "Ben Gao",
                "Yuqiang Li",
                "Shixiang Tang",
                "Wanli Ouyang",
                "Erik Cambria",
                "Dongzhan Zhou"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore",
                "Shanghai Artificial Intelligence Laboratory",
                "University of New South Wales",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21248.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ˜Ğ˜ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² 12 Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ¾Ğ±Ğ·Ğ¾Ñ€Ñ‹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ LLM Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ Ğ¾Ğ± Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ LLM ĞºĞ°Ğº Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Scientific Discovery with LLMs",
                    "desc": "This paper introduces a new benchmark to evaluate large language models (LLMs) in the context of scientific research. It focuses on three key tasks: retrieving inspirations, composing hypotheses, and ranking them based on quality. The authors developed an automated framework that accurately extracts essential elements from scientific papers, validated by experts. The findings indicate that LLMs excel at retrieving inspirations, highlighting their potential to generate innovative research hypotheses efficiently."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹åŠ©åŠ›ç§‘å­¦å‘ç°çš„æ½œåŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦ç ”ç©¶ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å‘ç°é«˜è´¨é‡ç ”ç©¶å‡è®¾çš„èƒ½åŠ›å°šæœªå¾—åˆ°éªŒè¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ç§‘å­¦å‘ç°ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬çµæ„Ÿæ£€ç´¢ã€å‡è®¾æ„å»ºå’Œå‡è®¾æ’åºç­‰å­ä»»åŠ¡ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œä»12ä¸ªå­¦ç§‘çš„ç§‘å­¦è®ºæ–‡ä¸­æå–å…³é”®ç»„ä»¶ï¼Œå¹¶é€šè¿‡ä¸“å®¶éªŒè¯ç¡®è®¤å…¶å‡†ç¡®æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ£€ç´¢çµæ„Ÿæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œè¡¨æ˜å®ƒä»¬èƒ½å¤Ÿå‘ç°æ–°çš„çŸ¥è¯†å…³è”ï¼Œä»è€Œæ¨åŠ¨è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20990",
            "title": "FinAudio: A Benchmark for Audio Large Language Models in Financial\n  Applications",
            "url": "https://huggingface.co/papers/2503.20990",
            "abstract": "Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce FinAudio, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the FinAudio benchmark. Then, we evaluate seven prevalent AudioLLMs on FinAudio. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released.",
            "score": 15,
            "issue_id": 2943,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "55f780e7347209e5",
            "authors": [
                "Yupeng Cao",
                "Haohang Li",
                "Yangyang Yu",
                "Shashidhar Reddy Javaji",
                "Yueru He",
                "Jimin Huang",
                "Zining Zhu",
                "Qianqian Xie",
                "Xiao-yang Liu",
                "Koduvayur Subbalakshmi",
                "Meikang Qiu",
                "Sophia Ananiadou",
                "Jian-Yun Nie"
            ],
            "affiliations": [
                "Augusta University",
                "Columbia University",
                "Stevens Institute of Technology",
                "The Fin AI",
                "The University of Manchester",
                "University of Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20990.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "FinAudio: ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ˜Ğ˜ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FinAudio - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (AudioLLMs) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ñ… ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞÑ†ĞµĞ½ĞºĞ° ÑĞµĞ¼Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… AudioLLMs Ğ½Ğ° FinAudio Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Benchmarking AudioLLMs for Financial Insights",
                    "desc": "This paper introduces FinAudio, a benchmark specifically designed to evaluate Audio Large Language Models (AudioLLMs) in financial contexts. It identifies three key tasks: automatic speech recognition (ASR) for both short and long financial audio, and summarization of long financial audio. The authors curate datasets tailored to these tasks, highlighting the unique characteristics of financial audio data. The evaluation of seven existing AudioLLMs on this benchmark reveals their limitations and suggests areas for enhancement in their performance within the financial domain."
                },
                "zh": {
                    "title": "é‡‘èé¢†åŸŸéŸ³é¢‘æ¨¡å‹è¯„ä¼°æ–°åŸºå‡†",
                    "desc": "éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAudioLLMsï¼‰åœ¨å¯¹è¯ã€éŸ³é¢‘ç†è§£å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰éŸ³é¢‘ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°AudioLLMsåœ¨é‡‘èåœºæ™¯ä¸­çš„åŸºå‡†ã€‚æœ¬æ–‡æå‡ºäº†FinAudioï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°AudioLLMsåœ¨é‡‘èé¢†åŸŸèƒ½åŠ›çš„åŸºå‡†ï¼Œå®šä¹‰äº†ä¸‰ä¸ªåŸºäºé‡‘èé¢†åŸŸç‰¹å¾çš„ä»»åŠ¡ï¼Œå¹¶åˆ›å»ºäº†ç›¸åº”çš„æ•°æ®é›†ã€‚é€šè¿‡å¯¹ä¸ƒä¸ªæµè¡Œçš„AudioLLMsè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„å±€é™æ€§ï¼Œå¹¶ä¸ºæ”¹è¿›AudioLLMsæä¾›äº†è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21696",
            "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks",
            "url": "https://huggingface.co/papers/2503.21696",
            "abstract": "Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.",
            "score": 14,
            "issue_id": 2946,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "a52516705bc7a122",
            "authors": [
                "Wenqi Zhang",
                "Mengna Wang",
                "Gangao Liu",
                "Xu Huixin",
                "Yiwei Jiang",
                "Yongliang Shen",
                "Guiyang Hou",
                "Zhe Zheng",
                "Hang Zhang",
                "Xin Li",
                "Weiming Lu",
                "Peng Li",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Alibaba Group",
                "College of Computer Science and Technology, Zhejiang University",
                "DAMO Academy, Alibaba Group",
                "Hohai University",
                "Institute of Software, Chinese Academy of Sciences",
                "Nanjing Institute of Software Technology",
                "Nanjing University of Posts and Telecommunications",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21696.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#reasoning",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Embodied Reasoner, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ GPT-4 Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ÑĞ°Ğ¼Ğ¾Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Embodied Reasoner Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Reasoning in Interactive Environments",
                    "desc": "This paper introduces the Embodied Reasoner, a model designed to enhance reasoning in interactive environments that require continuous engagement through visual and action-based tasks. Unlike traditional mathematical reasoning, this model focuses on spatial understanding and temporal reasoning, which are crucial for navigating real-world scenarios. The authors created a large dataset of 9.3k Observation-Thought-Action trajectories to train the model, employing a three-stage training process that includes imitation learning and self-correction. The results demonstrate that the Embodied Reasoner outperforms existing visual reasoning models, showing improved efficiency and fewer logical errors in complex tasks."
                },
                "zh": {
                    "title": "æå‡äº¤äº’å¼æ¨ç†èƒ½åŠ›çš„å…¨æ–°æ¨¡å‹",
                    "desc": "æœ€è¿‘æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šå±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦ä¸ç¯å¢ƒæŒç»­äº’åŠ¨çš„å®é™…åº”ç”¨é¢†åŸŸä¸­ï¼Œå…¶æœ‰æ•ˆæ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†\"Embodied Reasoner\"æ¨¡å‹ï¼Œæ—¨åœ¨å°†æ¨ç†èƒ½åŠ›æ‰©å±•åˆ°äº¤äº’å¼çš„å®é™…æœç´¢ä»»åŠ¡ä¸­ã€‚ä¸ä¸»è¦ä¾èµ–é€»è¾‘æ¨ç†çš„æ•°å­¦æ¨ç†ä¸åŒï¼Œå®é™…åœºæ™¯éœ€è¦ç©ºé—´ç†è§£ã€æ—¶é—´æ¨ç†ä»¥åŠåŸºäºäº’åŠ¨å†å²çš„è‡ªæˆ‘åæ€ã€‚é€šè¿‡åˆæˆ9.3åƒæ¡è¿è´¯çš„è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨è½¨è¿¹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21758",
            "title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
            "url": "https://huggingface.co/papers/2503.21758",
            "abstract": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-Image-2.0.",
            "score": 12,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "976a9523e7e6ba13",
            "authors": [
                "Qi Qin",
                "Le Zhuo",
                "Yi Xin",
                "Ruoyi Du",
                "Zhen Li",
                "Bin Fu",
                "Yiting Lu",
                "Jiakang Yuan",
                "Xinyue Li",
                "Dongyang Liu",
                "Xiangyang Zhu",
                "Manyuan Zhang",
                "Will Beddow",
                "Erwann Millon",
                "Victor Perez",
                "Wenhai Wang",
                "Conghui He",
                "Bo Zhang",
                "Xiaohong Liu",
                "Hongsheng Li",
                "Yu Qiao",
                "Chang Xu",
                "Peng Gao"
            ],
            "affiliations": [
                "Krea AI",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21758.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#cv",
                    "#open_source",
                    "#training",
                    "#multimodal",
                    "#small_models",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Lumina-Image 2.0 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ UniCap. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (2.6 Ğ¼Ğ»Ñ€Ğ´), Lumina-Image 2.0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ² Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Text-to-Image Generation with Lumina-Image 2.0",
                    "desc": "Lumina-Image 2.0 is a cutting-edge framework for generating images from text, significantly improving upon its predecessor, Lumina-Next. It utilizes a unified architecture that allows text and image data to interact seamlessly, enhancing the model's ability to handle various tasks. The introduction of the Unified Captioner (UniCap) enables the generation of high-quality captions that align well with images, improving training efficiency and output accuracy. Additionally, the model incorporates advanced training and inference techniques to maintain high image quality while being resource-efficient, demonstrating strong performance with a relatively small number of parameters."
                },
                "zh": {
                    "title": "Lumina-Image 2.0ï¼šé«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–°çºªå…ƒ",
                    "desc": "Lumina-Image 2.0 æ˜¯ä¸€ä¸ªå…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œç›¸æ¯”äºä¹‹å‰çš„ Lumina-Next å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è¯¥æ¡†æ¶åŸºäºä¸¤ä¸ªå…³é”®åŸåˆ™ï¼šç»Ÿä¸€æ€§å’Œæ•ˆç‡ã€‚ç»Ÿä¸€æ€§é€šè¿‡é‡‡ç”¨ç»Ÿä¸€æ¶æ„ï¼ˆUnified Next-DiTï¼‰æ¥å®ç°æ–‡æœ¬å’Œå›¾åƒæ ‡è®°çš„è”åˆå¤„ç†ï¼Œä¿ƒè¿›äº†è·¨æ¨¡æ€çš„è‡ªç„¶äº¤äº’ã€‚æ•ˆç‡æ–¹é¢ï¼Œæˆ‘ä»¬å¼€å‘äº†å¤šé˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥å’Œæ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œç¡®ä¿åœ¨ä¸é™ä½å›¾åƒè´¨é‡çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21144",
            "title": "ChatAnyone: Stylized Real-time Portrait Video Generation with\n  Hierarchical Motion Diffusion Model",
            "url": "https://huggingface.co/papers/2503.21144",
            "abstract": "Real-time interactive video-chat portraits have been increasingly recognized as the future trend, particularly due to the remarkable progress made in text and voice chat technologies. However, existing methods primarily focus on real-time generation of head movements, but struggle to produce synchronized body motions that match these head actions. Additionally, achieving fine-grained control over the speaking style and nuances of facial expressions remains a challenge. To address these limitations, we introduce a novel framework for stylized real-time portrait video generation, enabling expressive and flexible video chat that extends from talking head to upper-body interaction. Our approach consists of the following two stages. The first stage involves efficient hierarchical motion diffusion models, that take both explicit and implicit motion representations into account based on audio inputs, which can generate a diverse range of facial expressions with stylistic control and synchronization between head and body movements. The second stage aims to generate portrait video featuring upper-body movements, including hand gestures. We inject explicit hand control signals into the generator to produce more detailed hand movements, and further perform face refinement to enhance the overall realism and expressiveness of the portrait video. Additionally, our approach supports efficient and continuous generation of upper-body portrait video in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting interactive video-chat in real-time. Experimental results demonstrate the capability of our approach to produce portrait videos with rich expressiveness and natural upper-body movements.",
            "score": 12,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "12e50e9826751c62",
            "authors": [
                "Jinwei Qi",
                "Chaonan Ji",
                "Sheng Xu",
                "Peng Zhang",
                "Bang Zhang",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21144.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ñ‹ Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‚ĞµĞ»Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‡Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‡Ğ°Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ°Ğº ÑĞ²Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´Ğ°. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ†Ğ° Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ Ñ‚ĞµĞ»Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑ€Ñ…Ğ½ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ‚ĞµĞ»Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¶ĞµÑÑ‚Ñ‹ Ñ€ÑƒĞº, Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ñ†Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Expressive Real-Time Video Chats with Synchronized Body Movements",
                    "desc": "This paper presents a new framework for creating real-time interactive video chat portraits that include both head and upper-body movements. It utilizes hierarchical motion diffusion models to synchronize facial expressions and body motions based on audio inputs, allowing for expressive and stylistically controlled video generation. The framework also incorporates explicit hand control signals to enhance the realism of hand gestures and facial refinements. Overall, the approach enables high-quality, interactive video chats at a resolution of 512 * 768 and up to 30 frames per second."
                },
                "zh": {
                    "title": "å®æ—¶äº’åŠ¨è§†é¢‘èŠå¤©çš„æœªæ¥è¶‹åŠ¿",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å®æ—¶è‚–åƒè§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤´éƒ¨åŠ¨ä½œä¸èº«ä½“åŠ¨ä½œåŒæ­¥æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶é€šè¿‡é«˜æ•ˆçš„å±‚æ¬¡è¿åŠ¨æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆéŸ³é¢‘è¾“å…¥ç”Ÿæˆå¤šæ ·åŒ–çš„é¢éƒ¨è¡¨æƒ…ï¼Œå¹¶å®ç°å¤´éƒ¨ä¸èº«ä½“åŠ¨ä½œçš„åè°ƒã€‚ç¬¬äºŒé˜¶æ®µåˆ™ä¸“æ³¨äºç”ŸæˆåŒ…å«ä¸ŠåŠèº«åŠ¨ä½œçš„è‚–åƒè§†é¢‘ï¼Œé€šè¿‡æ³¨å…¥æ‰‹éƒ¨æ§åˆ¶ä¿¡å·æ¥å¢å¼ºæ‰‹éƒ¨åŠ¨ä½œçš„ç»†èŠ‚ï¼Œå¹¶è¿›è¡Œé¢éƒ¨ç»†åŒ–ä»¥æå‡è§†é¢‘çš„çœŸå®æ„Ÿå’Œè¡¨ç°åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥é«˜è¾¾30fpsçš„é€Ÿåº¦ç”Ÿæˆä¸°å¯Œè¡¨ç°åŠ›å’Œè‡ªç„¶ä¸ŠåŠèº«åŠ¨ä½œçš„è‚–åƒè§†é¢‘ï¼Œé€‚ç”¨äºå®æ—¶äº’åŠ¨è§†é¢‘èŠå¤©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21774",
            "title": "Optimal Stepsize for Diffusion Sampling",
            "url": "https://huggingface.co/papers/2503.21774",
            "abstract": "Diffusion models achieve remarkable generation quality but suffer from computational intensive sampling due to suboptimal step discretization. While existing works focus on optimizing denoising directions, we address the principled design of stepsize schedules. This paper proposes Optimal Stepsize Distillation, a dynamic programming framework that extracts theoretically optimal schedules by distilling knowledge from reference trajectories. By reformulating stepsize optimization as recursive error minimization, our method guarantees global discretization bounds through optimal substructure exploitation. Crucially, the distilled schedules demonstrate strong robustness across architectures, ODE solvers, and noise schedules. Experiments show 10x accelerated text-to-image generation while preserving 99.4% performance on GenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.",
            "score": 10,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "709832ee6e0a6f3f",
            "authors": [
                "Jianning Pei",
                "Han Hu",
                "Shuyang Gu"
            ],
            "affiliations": [
                "Tencent Hunyuan Research",
                "University Chinese Academic of Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21774.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#data",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Optimal Stepsize Distillation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ 99.4% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° GenEval."
                },
                "en": {
                    "title": "Accelerating Diffusion Models with Optimal Stepsize Distillation",
                    "desc": "This paper introduces a new method called Optimal Stepsize Distillation to improve the efficiency of diffusion models in generating images. It focuses on optimizing the stepsize schedules used during the sampling process, which is often computationally intensive. By using dynamic programming, the method distills optimal schedules from reference trajectories, ensuring that the stepsize is effectively minimized. The results show that this approach can speed up text-to-image generation by 10 times while maintaining high performance levels."
                },
                "zh": {
                    "title": "æœ€ä¼˜æ­¥é•¿è’¸é¦ï¼šåŠ é€Ÿæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å…³é”®",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºæ­¥éª¤ç¦»æ•£åŒ–ä¸ç†æƒ³ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæœ€ä¼˜æ­¥é•¿è’¸é¦çš„åŠ¨æ€è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡ä»å‚è€ƒè½¨è¿¹ä¸­æå–ç†è®ºæœ€ä¼˜çš„æ­¥é•¿è°ƒåº¦æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬å°†æ­¥é•¿ä¼˜åŒ–é‡æ–°è¡¨è¿°ä¸ºé€’å½’è¯¯å·®æœ€å°åŒ–ï¼Œä»è€Œä¿è¯äº†å…¨å±€ç¦»æ•£åŒ–ç•Œé™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒ99.4%æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„10å€åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21765",
            "title": "Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey",
            "url": "https://huggingface.co/papers/2503.21765",
            "abstract": "Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models. Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of ''visual realism but physical absurdity\". Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios. Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap. Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks. Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry. Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of ''visual mimicry'' towards a new phase of ''human-like physical comprehension''.",
            "score": 9,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "87e887fdf8f612cf",
            "authors": [
                "Minghui Lin",
                "Xiang Wang",
                "Yishan Wang",
                "Shu Wang",
                "Fengqi Dai",
                "Pengxiang Ding",
                "Cunxiang Wang",
                "Zhengrong Zuo",
                "Nong Sang",
                "Siteng Huang",
                "Donglin Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Shandong University",
                "Tsinghua University",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21765.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#survey",
                    "#video",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ğ¼Ğ¸ĞºÑ€Ğ¸Ğ¸ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€ĞµÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑÑ…ĞµĞ¼, Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ°. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ…, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "From Visual Mimicry to Human-like Physical Comprehension in Video Generation",
                    "desc": "This paper discusses the recent improvements in video generation using diffusion models, highlighting their limitations in understanding physical laws. It points out that while these models can create visually appealing content, they often produce results that do not adhere to the principles of physics, leading to unrealistic scenarios. The authors propose a structured overview of how physical cognition can be integrated into video generation, categorizing it into three levels: basic perception, passive knowledge, and active simulation. The survey aims to guide future research towards creating video generation systems that are not only visually realistic but also physically coherent, moving beyond mere visual mimicry to a deeper understanding of the physical world."
                },
                "zh": {
                    "title": "æ¨åŠ¨è§†é¢‘ç”Ÿæˆå‘äººç±»ç‰©ç†ç†è§£çš„æ–°é˜¶æ®µ",
                    "desc": "è¿‘å¹´æ¥ï¼Œè§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç‰©ç†è®¤çŸ¥æ–¹é¢çš„ä¸è¶³é€æ¸å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œç”Ÿæˆçš„å†…å®¹å¸¸å¸¸è¿ååŸºæœ¬çš„ç‰©ç†æ³•åˆ™ï¼Œé™·å…¥äº†â€œè§†è§‰çœŸå®ä½†ç‰©ç†è’è°¬â€çš„å›°å¢ƒã€‚ç ”ç©¶äººå‘˜å¼€å§‹è®¤è¯†åˆ°ç‰©ç†çœŸå®æ„Ÿåœ¨è§†é¢‘ç”Ÿæˆä¸­çš„é‡è¦æ€§ï¼Œå¹¶å°è¯•å°†å¯å‘å¼çš„ç‰©ç†è®¤çŸ¥èå…¥ç”Ÿæˆç³»ç»Ÿï¼Œä»¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„åŠ¨æ€åœºæ™¯ã€‚æœ¬æ–‡ç»¼è¿°äº†ç‰©ç†è®¤çŸ¥åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„æ¼”å˜è¿‡ç¨‹ï¼Œæå‡ºäº†ä¸‰å±‚æ¬¡çš„åˆ†ç±»æ³•ï¼Œå¹¶å¼ºè°ƒäº†è¯¥é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21088",
            "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
            "url": "https://huggingface.co/papers/2503.21088",
            "abstract": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
            "score": 6,
            "issue_id": 2941,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "494379ac783f4297",
            "authors": [
                "Haoming Xu",
                "Shuxun Wang",
                "Yanqiu Zhao",
                "Yi Zhong",
                "Ziyan Jiang",
                "Ningyuan Zhao",
                "Shumin Deng",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21088.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#data",
                    "#leakage",
                    "#ethics",
                    "#hallucinations"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Model Merging), Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ TIES-Merging, Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 'Ğ·Ğ°Ğ±Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸' Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ SemEval-2025, Ğ·Ğ°Ğ½ÑĞ² Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ€ĞµĞ´Ğ¸ 26 ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (unlearning) Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Mastering Unlearning: Balancing Sensitivity in Language Models",
                    "desc": "This paper discusses the ZJUKLAB team's approach to the SemEval-2025 Task 4, which focuses on unlearning sensitive information from large language models. The proposed method utilizes Model Merging, specifically TIES-Merging, to create a balanced model that effectively manages the challenges of over-forgetting and under-forgetting. The team achieved impressive results, ranking second out of 26 participants, and conducted thorough experiments to analyze the unlearning process, including performance metrics and loss dynamics. The authors also highlight the limitations of current evaluation methods and advocate for improved metrics to better assess unlearning effectiveness in future studies."
                },
                "zh": {
                    "title": "é€‰æ‹©æ€§åˆ é™¤ï¼Œé‡å¡‘è¯­è¨€æ¨¡å‹çš„æœªæ¥",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ZJUKLABå›¢é˜Ÿåœ¨SemEval-2025ä»»åŠ¡4ä¸­çš„æäº¤ï¼Œæ—¨åœ¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é€‰æ‹©æ€§åœ°åˆ é™¤æ•æ„Ÿå†…å®¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ¨¡å‹åˆå¹¶ï¼ˆç‰¹åˆ«æ˜¯TIES-Mergingï¼‰çš„æ–¹æ³•ï¼Œå°†ä¸¤ä¸ªä¸“é—¨æ¨¡å‹ç»“åˆæˆä¸€ä¸ªæ›´å¹³è¡¡çš„æœªå­¦ä¹ æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨26ä¸ªå›¢é˜Ÿä¸­æ’åç¬¬äºŒï¼Œä»»åŠ¡èšåˆçš„åœ¨çº¿å¾—åˆ†ä¸º0.944ï¼Œæ€»ä½“èšåˆå¾—åˆ†ä¸º0.487ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å±€éƒ¨å®éªŒï¼Œå…¨é¢åˆ†æäº†æœªå­¦ä¹ è¿‡ç¨‹çš„è¡¨ç°è½¨è¿¹ã€æŸå¤±åŠ¨æ€å’Œæƒé‡è§†è§’ï¼Œå¹¶å¼ºè°ƒäº†ç°æœ‰è¯„ä¼°æŒ‡æ ‡çš„ä¸è¶³ï¼Œå‘¼åæœªæ¥ç ”ç©¶éœ€è¦æ›´å…¨é¢çš„è¯„ä¼°æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20822",
            "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
            "url": "https://huggingface.co/papers/2503.20822",
            "abstract": "We investigate how to enhance the physical fidelity of video generation models by leveraging synthetic videos derived from computer graphics pipelines. These rendered videos respect real-world physics, such as maintaining 3D consistency, and serve as a valuable resource that can potentially improve video generation models. To harness this potential, we propose a solution that curates and integrates synthetic data while introducing a method to transfer its physical realism to the model, significantly reducing unwanted artifacts. Through experiments on three representative tasks emphasizing physical consistency, we demonstrate its efficacy in enhancing physical fidelity. While our model still lacks a deep understanding of physics, our work offers one of the first empirical demonstrations that synthetic video enhances physical fidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/",
            "score": 6,
            "issue_id": 2941,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "8ad7de4de496fce6",
            "authors": [
                "Qi Zhao",
                "Xingyu Ni",
                "Ziyu Wang",
                "Feng Cheng",
                "Ziyan Yang",
                "Lu Jiang",
                "Bohan Wang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "National University of Singapore",
                "Peking University",
                "ShanghaiTech University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20822.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#data",
                    "#synthetic",
                    "#optimization"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: ÑˆĞ°Ğ³ Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Realism with Synthetic Physics",
                    "desc": "This paper explores how to improve the realism of video generation models by using synthetic videos created through computer graphics. These synthetic videos adhere to real-world physics, ensuring 3D consistency, which can enhance the training of video generation models. The authors propose a method to curate and integrate this synthetic data, allowing the model to adopt the physical realism of the videos and reduce visual artifacts. Their experiments show that this approach effectively boosts the physical fidelity of generated videos, marking a significant step in the field of video synthesis."
                },
                "zh": {
                    "title": "åˆæˆè§†é¢‘æå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†çœŸå®æ€§",
                    "desc": "æˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•é€šè¿‡åˆ©ç”¨è®¡ç®—æœºå›¾å½¢å­¦ç”Ÿæˆçš„åˆæˆè§†é¢‘æ¥å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†çœŸå®æ€§ã€‚è¿™äº›æ¸²æŸ“è§†é¢‘éµå¾ªç°å®ä¸–ç•Œçš„ç‰©ç†è§„å¾‹ï¼Œä¿æŒä¸‰ç»´ä¸€è‡´æ€§ï¼Œæˆä¸ºæ”¹å–„è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„é‡è¦èµ„æºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œç­–åˆ’å’Œæ•´åˆåˆæˆæ•°æ®ï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€ç§å°†ç‰©ç†çœŸå®æ„Ÿè½¬ç§»åˆ°æ¨¡å‹çš„æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†ä¸å¿…è¦çš„ä¼ªå½±ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªå¼ºè°ƒç‰©ç†ä¸€è‡´æ€§çš„ä»£è¡¨æ€§ä»»åŠ¡ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§æ–¹æ³•åœ¨æé«˜ç‰©ç†çœŸå®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20776",
            "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields",
            "url": "https://huggingface.co/papers/2503.20776",
            "abstract": "Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The \"X\" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction.",
            "score": 5,
            "issue_id": 2941,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "d066b6a18982ec5f",
            "authors": [
                "Shijie Zhou",
                "Hui Ren",
                "Yijia Weng",
                "Shuwang Zhang",
                "Zhen Wang",
                "Dejia Xu",
                "Zhiwen Fan",
                "Suya You",
                "Zhangyang Wang",
                "Leonidas Guibas",
                "Achuta Kadambi"
            ],
            "affiliations": [
                "DEVCOM ARL",
                "MIT",
                "Stanford",
                "UCLA",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20776.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#agi",
                    "#agents",
                    "#dataset",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Feature4X: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ 2D Ğ¸ 4D ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Feature4X - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 2D Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° 4D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Feature4X Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² 4D ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ²Ğ½Ğ¾Ğµ 4D Ğ¿Ğ¾Ğ»Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Gaussian Splatting."
                },
                "en": {
                    "title": "Feature4X: Bridging 2D Vision to 4D Interaction",
                    "desc": "This paper presents Feature4X, a novel framework that enhances 2D vision models to operate in 4D environments using only monocular video inputs. It addresses the challenge of limited annotated datasets for 3D/4D tasks by enabling versatile interactions and semantic operations in complex scenes. The framework employs a dynamic optimization strategy to unify various model capabilities into a single representation, allowing for adaptable feature extraction. Feature4X is the first to distill video foundation model features into a 4D feature field, facilitating advanced tasks like segmentation, scene editing, and visual question answering with improved context awareness."
                },
                "zh": {
                    "title": "Feature4Xï¼šå°†2Dè§†è§‰æ‰©å±•åˆ°4Dçš„é€šç”¨æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFeature4Xçš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨å°†2Dè§†è§‰åŸºç¡€æ¨¡å‹çš„åŠŸèƒ½æ‰©å±•åˆ°4Dé¢†åŸŸã€‚è¯¥æ¡†æ¶ä»…ä½¿ç”¨å•ç›®è§†é¢‘è¾“å…¥ï¼Œè§£å†³äº†3D/4Dæ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚Feature4Xé€šè¿‡åŠ¨æ€ä¼˜åŒ–ç­–ç•¥ï¼Œå°†å¤šç§æ¨¡å‹èƒ½åŠ›ç»Ÿä¸€ä¸ºå•ä¸€è¡¨ç¤ºï¼Œæ”¯æŒå¼€æ”¾è¯æ±‡å’ŒåŸºäºæç¤ºçš„åˆ†å‰²ã€è¯­è¨€å¼•å¯¼ç¼–è¾‘å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°æ–°è§†è§’çš„åˆ†å‰²ã€å‡ ä½•å’Œå¤–è§‚åœºæ™¯ç¼–è¾‘ï¼Œä»¥åŠè·¨æ—¶é—´æ­¥çš„è‡ªç”±å½¢å¼è§†è§‰é—®ç­”ï¼Œæ¨åŠ¨äº†æ™ºèƒ½ä»£ç†AIåº”ç”¨çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20578",
            "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
            "url": "https://huggingface.co/papers/2503.20578",
            "abstract": "Failure-inducing inputs play a crucial role in diagnosing and analyzing software bugs. Bug reports typically contain these inputs, which developers extract to facilitate debugging. Since bug reports are written in natural language, prior research has leveraged various Natural Language Processing (NLP) techniques for automated input extraction. With the advent of Large Language Models (LLMs), an important research question arises: how effectively can generative LLMs extract failure-inducing inputs from bug reports? In this paper, we propose LLPut, a technique to empirically evaluate the performance of three open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in extracting relevant inputs from bug reports. We conduct an experimental evaluation on a dataset of 206 bug reports to assess the accuracy and effectiveness of these models. Our findings provide insights into the capabilities and limitations of generative LLMs in automated bug diagnosis.",
            "score": 4,
            "issue_id": 2941,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "5ee433dad4dd5d00",
            "authors": [
                "Alif Al Hasan",
                "Subarna Saha",
                "Mia Mohammad Imran",
                "Tarannum Shaila Zaman"
            ],
            "affiliations": [
                "Jahangirnagar University Dhaka, Bangladesh",
                "Missouri University of Science and Technology Rolla, Missouri, USA",
                "University of Maryland Baltimore County Bsltimore, Maryland, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20578.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#science",
                    "#dataset",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "ğŸ›",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸: Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¸Ğ· Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ¸Ğ· Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¾ Ğ±Ğ°Ğ³Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ LLPut Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€ĞµÑ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM: LLaMA, Qwen Ğ¸ Qwen-Coder. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 206 Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¾ Ğ±Ğ°Ğ³Ğ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… LLM Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "Harnessing LLMs for Smart Bug Diagnosis",
                    "desc": "This paper investigates how well generative Large Language Models (LLMs) can extract failure-inducing inputs from bug reports, which are essential for diagnosing software issues. The authors introduce a technique called LLPut to evaluate the performance of three open-source LLMs: LLaMA, Qwen, and Qwen-Coder. They conduct experiments on a dataset of 206 bug reports to measure the accuracy and effectiveness of these models in identifying relevant inputs. The results offer valuable insights into the strengths and weaknesses of using generative LLMs for automated bug diagnosis."
                },
                "zh": {
                    "title": "åˆ©ç”¨LLMæå‡ç¼ºé™·æŠ¥å‘Šåˆ†æçš„æ•ˆç‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»è½¯ä»¶ç¼ºé™·æŠ¥å‘Šä¸­æå–å¯¼è‡´æ•…éšœçš„è¾“å…¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLLPutçš„æŠ€æœ¯ï¼Œæ—¨åœ¨è¯„ä¼°ä¸‰ç§å¼€æºç”Ÿæˆæ€§LLMï¼ˆLLaMAã€Qwenå’ŒQwen-Coderï¼‰åœ¨è¿™ä¸€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¯¹206ä¸ªç¼ºé™·æŠ¥å‘Šçš„æ•°æ®é›†è¿›è¡Œå®éªŒè¯„ä¼°ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†ç”Ÿæˆæ€§LLMåœ¨è‡ªåŠ¨åŒ–ç¼ºé™·è¯Šæ–­ä¸­çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21780",
            "title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for\n  Open-Vocabulary Semantic Segmentation",
            "url": "https://huggingface.co/papers/2503.21780",
            "abstract": "Open-vocabulary semantic segmentation models associate vision and text to label pixels from an undefined set of classes using textual queries, providing versatile performance on novel datasets. However, large shifts between training and test domains degrade their performance, requiring fine-tuning for effective real-world applications. We introduce Semantic Library Adaptation (SemLA), a novel framework for training-free, test-time domain adaptation. SemLA leverages a library of LoRA-based adapters indexed with CLIP embeddings, dynamically merging the most relevant adapters based on proximity to the target domain in the embedding space. This approach constructs an ad-hoc model tailored to each specific input without additional training. Our method scales efficiently, enhances explainability by tracking adapter contributions, and inherently protects data privacy, making it ideal for sensitive applications. Comprehensive experiments on a 20-domain benchmark built over 10 standard datasets demonstrate SemLA's superior adaptability and performance across diverse settings, establishing a new standard in domain adaptation for open-vocabulary semantic segmentation.",
            "score": 3,
            "issue_id": 2954,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "f676e21908b87897",
            "authors": [
                "Reza Qorbani",
                "Gianluca Villani",
                "Theodoros Panagiotakopoulos",
                "Marc Botet Colomer",
                "Linus HÃ¤renstam-Nielsen",
                "Mattia Segu",
                "Pier Luigi Dovesi",
                "Jussi Karlgren",
                "Daniel Cremers",
                "Federico Tombari",
                "Matteo Poggi"
            ],
            "affiliations": [
                "AMD",
                "ETH Zurich",
                "Google",
                "KTH",
                "King",
                "Munich Center for Machine Learning",
                "Silo AI",
                "Technical University of Munich",
                "The Good AI Lab",
                "University of Bologna",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21780.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#transfer_learning",
                    "#dataset",
                    "#interpretability",
                    "#security",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ SemLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ², Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ CLIP-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 20 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ SemLA Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Adapt and Conquer: Dynamic Domain Adaptation for Semantic Segmentation",
                    "desc": "This paper presents a new method called Semantic Library Adaptation (SemLA) for improving open-vocabulary semantic segmentation models. These models can label pixels using text queries but struggle when the training and testing data are very different. SemLA allows the model to adapt to new domains at test time without needing additional training by using a library of pre-trained adapters. The method is efficient, enhances model explainability, and protects data privacy, making it suitable for sensitive applications."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒçš„é¢†åŸŸé€‚åº”ï¼Œæå‡è¯­ä¹‰åˆ†å‰²æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºè¯­ä¹‰åº“é€‚åº”ï¼ˆSemLAï¼‰ï¼Œç”¨äºæ— è®­ç»ƒçš„æµ‹è¯•æ—¶é¢†åŸŸé€‚åº”ï¼Œæ—¨åœ¨æé«˜å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚SemLAåˆ©ç”¨åŸºäºLoRAçš„é€‚é…å™¨åº“ï¼Œå¹¶é€šè¿‡CLIPåµŒå…¥ç´¢å¼•ï¼ŒåŠ¨æ€åˆå¹¶ä¸ç›®æ ‡é¢†åŸŸæœ€ç›¸å…³çš„é€‚é…å™¨ï¼Œä»è€Œæ„å»ºé’ˆå¯¹ç‰¹å®šè¾“å…¥çš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆæ‰©å±•ï¼Œå¹¶é€šè¿‡è·Ÿè¸ªé€‚é…å™¨çš„è´¡çŒ®æ¥å¢å¼ºå¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¿æŠ¤æ•°æ®éšç§ï¼Œé€‚åˆæ•æ„Ÿåº”ç”¨ã€‚é€šè¿‡åœ¨20ä¸ªé¢†åŸŸåŸºå‡†ä¸Šçš„å…¨é¢å®éªŒï¼ŒSemLAå±•ç¤ºäº†å…¶åœ¨å¤šç§è®¾ç½®ä¸‹çš„ä¼˜è¶Šé€‚åº”æ€§å’Œæ€§èƒ½ï¼Œç¡®ç«‹äº†å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²é¢†åŸŸé€‚åº”çš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20853",
            "title": "Unified Multimodal Discrete Diffusion",
            "url": "https://huggingface.co/papers/2503.20853",
            "abstract": "Multimodal generative models that can understand and generate across multiple modalities are dominated by autoregressive (AR) approaches, which process tokens sequentially from left to right, or top to bottom. These models jointly handle images, text, video, and audio for various tasks such as image captioning, question answering, and image generation. In this work, we explore discrete diffusion models as a unified generative formulation in the joint text and image domain, building upon their recent success in text generation. Discrete diffusion models offer several advantages over AR models, including improved control over quality versus diversity of generated samples, the ability to perform joint multimodal inpainting (across both text and image domains), and greater controllability in generation through guidance. Leveraging these benefits, we present the first Unified Multimodal Discrete Diffusion (UniDisc) model which is capable of jointly understanding and generating text and images for a variety of downstream tasks. We compare UniDisc to multimodal AR models, performing a scaling analysis and demonstrating that UniDisc outperforms them in terms of both performance and inference-time compute, enhanced controllability, editability, inpainting, and flexible trade-off between inference time and generation quality. Code and additional visualizations are available at https://unidisc.github.io.",
            "score": 3,
            "issue_id": 2941,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "9650b4dd1188fcc0",
            "authors": [
                "Alexander Swerdlow",
                "Mihir Prabhudesai",
                "Siddharth Gandhi",
                "Deepak Pathak",
                "Katerina Fragkiadaki"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.20853.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#audio",
                    "#multimodal",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "UniDisc: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UniDisc, Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ UniDisc Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ UniDisc Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Generation with UniDisc!",
                    "desc": "This paper introduces the Unified Multimodal Discrete Diffusion (UniDisc) model, which is designed to generate and understand both text and images simultaneously. Unlike traditional autoregressive models that process data sequentially, UniDisc utilizes discrete diffusion techniques to enhance the quality and diversity of generated outputs. The model excels in tasks such as multimodal inpainting and offers improved controllability and editability during generation. Through extensive comparisons, UniDisc demonstrates superior performance and efficiency over existing multimodal autoregressive approaches."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆï¼Œè¶…è¶Šè‡ªå›å½’æ¨¡å‹ï¼",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œç§°ä¸ºç»Ÿä¸€å¤šæ¨¡æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆUniDiscï¼‰ã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ä¸åŒï¼ŒUniDiscèƒ½å¤ŸåŒæ—¶ç†è§£å’Œç”Ÿæˆæ–‡æœ¬ä¸å›¾åƒï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åœ¨ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ä¸å¤šæ ·æ€§ä¹‹é—´æä¾›äº†æ›´å¥½çš„æ§åˆ¶ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¿›è¡Œè·¨æ–‡æœ¬å’Œå›¾åƒçš„è”åˆä¿®å¤ã€‚é€šè¿‡ä¸è‡ªå›å½’æ¨¡å‹çš„æ¯”è¾ƒï¼ŒUniDiscåœ¨æ€§èƒ½ã€è®¡ç®—æ•ˆç‡å’Œå¯æ§æ€§ç­‰æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21541",
            "title": "LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized\n  Text-Guided Image Editing",
            "url": "https://huggingface.co/papers/2503.21541",
            "abstract": "Text-guided image editing aims to modify specific regions of an image according to natural language instructions while maintaining the general structure and the background fidelity. Existing methods utilize masks derived from cross-attention maps generated from diffusion models to identify the target regions for modification. However, since cross-attention mechanisms focus on semantic relevance, they struggle to maintain the image integrity. As a result, these methods often lack spatial consistency, leading to editing artifacts and distortions. In this work, we address these limitations and introduce LOCATEdit, which enhances cross-attention maps through a graph-based approach utilizing self-attention-derived patch relationships to maintain smooth, coherent attention across image regions, ensuring that alterations are limited to the designated items while retaining the surrounding structure. \\method consistently and substantially outperforms existing baselines on PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on various editing tasks. Code can be found on https://github.com/LOCATEdit/LOCATEdit/",
            "score": 1,
            "issue_id": 2950,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 27",
                "zh": "3æœˆ27æ—¥"
            },
            "hash": "38e7c3d2a3738793",
            "authors": [
                "Achint Soni",
                "Meet Soni",
                "Sirisha Rambhatla"
            ],
            "affiliations": [
                "Stony Brook University",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21541.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#architecture",
                    "#open_source",
                    "#games",
                    "#optimization",
                    "#graphs",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹",
                    "desc": "LOCATEdit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. LOCATEdit Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ PIE-Bench Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Text-Guided Image Editing with LOCATEdit",
                    "desc": "This paper presents LOCATEdit, a novel approach for text-guided image editing that improves upon existing methods by addressing issues of spatial consistency. Traditional techniques rely on cross-attention maps from diffusion models, which can lead to artifacts due to their focus on semantic relevance rather than spatial integrity. LOCATEdit enhances these maps using a graph-based method that leverages self-attention to maintain coherent attention across different regions of the image. The results show that LOCATEdit significantly outperforms current baselines on the PIE-Bench dataset, proving its effectiveness in preserving the overall structure while allowing precise modifications."
                },
                "zh": {
                    "title": "ç²¾ç¡®å›¾åƒç¼–è¾‘ï¼Œä¿æŒç»“æ„å®Œæ•´æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLOCATEditçš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¿®æ”¹å›¾åƒçš„ç‰¹å®šåŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæ•´ä½“ç»“æ„å’ŒèƒŒæ™¯çš„å®Œæ•´æ€§ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨æ¥è‡ªæ‰©æ•£æ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å›¾ç”Ÿæˆçš„æ©ç æ¥è¯†åˆ«ç›®æ ‡åŒºåŸŸï¼Œä½†ç”±äºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶å…³æ³¨è¯­ä¹‰ç›¸å…³æ€§ï¼Œå¯¼è‡´å›¾åƒå®Œæ•´æ€§éš¾ä»¥ä¿æŒã€‚LOCATEdité€šè¿‡åŸºäºå›¾çš„è‡ªæ³¨æ„åŠ›æ–¹æ³•å¢å¼ºäº¤å‰æ³¨æ„åŠ›å›¾ï¼Œç¡®ä¿å›¾åƒåŒºåŸŸä¹‹é—´çš„å¹³æ»‘ä¸€è‡´æ€§ï¼Œä»è€Œé™åˆ¶ä¿®æ”¹ä»…åœ¨æŒ‡å®šé¡¹ç›®ä¸Šï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLOCATEditåœ¨PIE-Benchä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å„ç§ç¼–è¾‘ä»»åŠ¡ä¸­çš„å…ˆè¿›æ€§èƒ½å’Œæœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19904",
            "title": "Tracktention: Leveraging Point Tracking to Attend Videos Faster and\n  Better",
            "url": "https://huggingface.co/papers/2503.19904",
            "abstract": "Temporal consistency is critical in video prediction to ensure that outputs are coherent and free of artifacts. Traditional methods, such as temporal attention and 3D convolution, may struggle with significant object motion and may not capture long-range temporal dependencies in dynamic scenes. To address this gap, we propose the Tracktention Layer, a novel architectural component that explicitly integrates motion information using point tracks, i.e., sequences of corresponding points across frames. By incorporating these motion cues, the Tracktention Layer enhances temporal alignment and effectively handles complex object motions, maintaining consistent feature representations over time. Our approach is computationally efficient and can be seamlessly integrated into existing models, such as Vision Transformers, with minimal modification. It can be used to upgrade image-only models to state-of-the-art video ones, sometimes outperforming models natively designed for video prediction. We demonstrate this on video depth prediction and video colorization, where models augmented with the Tracktention Layer exhibit significantly improved temporal consistency compared to baselines.",
            "score": 1,
            "issue_id": 2957,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "14b58ae496ab3e3b",
            "authors": [
                "Zihang Lai",
                "Andrea Vedaldi"
            ],
            "affiliations": [
                "Visual Geometry Group (VGG), University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19904.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#video"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "Tracktention Layer: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Tracktention Layer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Tracktention Layer Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Vision Transformers, Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Tracktention Layer Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Prediction with Tracktention for Temporal Consistency",
                    "desc": "This paper introduces the Tracktention Layer, a new component designed to improve video prediction by enhancing temporal consistency. Traditional methods often fail to manage significant object motion and long-range dependencies, leading to artifacts in the output. The Tracktention Layer utilizes point tracks to integrate motion information, allowing for better alignment and representation of features over time. This approach is efficient and can be easily added to existing models, showing superior performance in tasks like video depth prediction and colorization."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘é¢„æµ‹çš„æ—¶é—´ä¸€è‡´æ€§",
                    "desc": "åœ¨è§†é¢‘é¢„æµ‹ä¸­ï¼Œæ—¶é—´ä¸€è‡´æ€§éå¸¸é‡è¦ï¼Œä»¥ç¡®ä¿è¾“å‡ºç»“æœè¿è´¯ä¸”æ²¡æœ‰ä¼ªå½±ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚æ—¶é—´æ³¨æ„åŠ›å’Œ3Då·ç§¯åœ¨å¤„ç†æ˜¾è‘—ç‰©ä½“è¿åŠ¨æ—¶å¯èƒ½ä¼šé‡åˆ°å›°éš¾ï¼Œæ— æ³•æ•æ‰åŠ¨æ€åœºæ™¯ä¸­çš„é•¿è·ç¦»æ—¶é—´ä¾èµ–å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Tracktentionå±‚ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¶æ„ç»„ä»¶ï¼Œæ˜ç¡®æ•´åˆäº†è¿åŠ¨ä¿¡æ¯ï¼Œé€šè¿‡ç‚¹è½¨è¿¹æ¥å®ç°ã€‚é€šè¿‡å¼•å…¥è¿™äº›è¿åŠ¨çº¿ç´¢ï¼ŒTracktentionå±‚å¢å¼ºäº†æ—¶é—´å¯¹é½èƒ½åŠ›ï¼Œæœ‰æ•ˆå¤„ç†å¤æ‚çš„ç‰©ä½“è¿åŠ¨ï¼Œä¿æŒäº†ä¸€è‡´çš„ç‰¹å¾è¡¨ç¤ºã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-27.html",
    "link_next": "2025-03-31.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "27.03",
        "en": "03/27",
        "zh": "3æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "31.03",
        "en": "03/31",
        "zh": "3æœˆ31æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 5,
        "#benchmark": 11,
        "#agents": 3,
        "#cv": 7,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 2,
        "#video": 6,
        "#multimodal": 8,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 2,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 8,
        "#survey": 2,
        "#diffusion": 5,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºVideo-R1çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­å¼•å‡ºè§†é¢‘æ¨ç†èƒ½åŠ›ã€‚å—DeepSeek-R1æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†T-GRPOç®—æ³•æ¥è§£å†³è§†é¢‘æ¨ç†ä¸­çš„æ—¶é—´å»ºæ¨¡é—®é¢˜ï¼Œå¹¶ç»“åˆé«˜è´¨é‡çš„å›¾åƒæ¨ç†æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVideo-R1åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ï¼Œç”šè‡³è¶…è¶Šäº†å•†ä¸šä¸“æœ‰æ¨¡å‹GPT-4oã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®éƒ½å·²å…¬å¼€ã€‚",
        "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºVideo-R1çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­å¼•å‡ºè§†é¢‘æ¨ç†èƒ½åŠ›ã€‚å—DeepSeek-R1æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†T-GRPOç®—æ³•æ¥è§£å†³è§†é¢‘æ¨ç†ä¸­çš„æ—¶é—´å»ºæ¨¡é—®é¢˜ï¼Œå¹¶ç»“åˆé«˜è´¨é‡çš„å›¾åƒæ¨ç†æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVideo-R1åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ï¼Œç”šè‡³è¶…è¶Šäº†å•†ä¸šä¸“æœ‰æ¨¡å‹GPT-4oã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®éƒ½å·²å…¬å¼€ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«zhÇ’ng mÃ­ngwÃ¨i Video-R1 de xÄ«n fÄngfÇ, yÃ²ngyÃº zÃ i duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng zhÅng yÇnchÅ« shÃ¬pÃ­n tuÄ«lÇ nÃ©nglÃ¬. ShÃ²u DeepSeek-R1 chÃ©nggÅng de qÇfÄ, yÃ¡njiÅ« rÃ©nyuÃ¡n tÃ­chÅ« le T-GRPO suÃ nfÇ lÃ¡i jiÄ›juÃ© shÃ¬pÃ­n tuÄ«lÇ zhÅng de shÃ­jiÄn jiÃ nmÃ³ wÃ¨ntÃ­, bÃ¬ng jiÃ©hÃ© gÄo zhÃ¬liÃ ng de tÃºxiÃ ng tuÄ«lÇ shÃ¹jÃ¹ jÃ¬nxÃ­ng xÃ¹nliÃ n. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, Video-R1 zÃ i duÅ gÃ¨ shÃ¬pÃ­n tuÄ«lÇ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng qÇ”dÃ© le xiÇnzhÃ¹ tÃ­shÄ“ng, shÃ¨nzhÃ¬ chÄoyuÃ¨ le shÄngyÃ¨ zhuÄnyÇ’u mÃ³xÃ­ng GPT-4o. SuÇ’yÇ’u dÃ imÇ, mÃ³xÃ­ng hÃ© shÃ¹jÃ¹ dÅu yÇ gÅngkÄi.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å¼•å‡º\", \"pinyin\": \"yÇn chÅ«\", \"trans\": \"elicit\"},\n    {\"word\": \"è§†é¢‘æ¨ç†\", \"pinyin\": \"shÃ¬ pÃ­n tuÄ« lÇ\", \"trans\": \"video reasoning\"},\n    {\"word\": \"å¯å‘\", \"pinyin\": \"qÇ fÄ\", \"trans\": \"inspire\"},\n    {\"word\": \"æ—¶é—´å»ºæ¨¡\", \"pinyin\": \"shÃ­ jiÄn jiÃ n mÃ³\", \"trans\": \"temporal modeling\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"å›¾åƒæ¨ç†\", \"pinyin\": \"tÃº xiÃ ng tuÄ« lÇ\", \"trans\": \"image reasoning\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­ shÄ“ng\", \"trans\": \"improvement\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"å•†ä¸šä¸“æœ‰\", \"pinyin\": \"shÄng yÃ¨ zhuÄn yÇ’u\", \"trans\": \"commercial proprietary\"},\n    {\"word\": \"åŸºå‡†æµ‹è¯•\", \"pinyin\": \"jÄ« zhÇ”n cÃ¨ shÃ¬\", \"trans\": \"benchmark test\"}\n]",
        "trans": "This article introduces a new method called Video-R1 for eliciting video reasoning capabilities in multimodal large language models. Inspired by the success of DeepSeek-R1, researchers proposed the T-GRPO algorithm to address the temporal modeling problem in video reasoning and combined it with high-quality image reasoning data for training. Experimental results show that Video-R1 achieved significant improvements in multiple video reasoning benchmark tests, even surpassing the commercial proprietary model GPT-4o. All codes, models, and data have been made publicly available.",
        "update_ts": "2025-03-28 09:12"
    }
}