{
    "date": {
        "ru": "10 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 10",
        "zh": "1æœˆ10æ—¥"
    },
    "time_utc": "2025-01-10 20:11",
    "weekday": 4,
    "issue_id": 1612,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.05441",
            "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
            "url": "https://huggingface.co/papers/2501.05441",
            "abstract": "There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, our new loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline -- R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.",
            "score": 22,
            "issue_id": 1596,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "eb1cd90c4d5cb0ef",
            "authors": [
                "Yiwen Huang",
                "Aaron Gokaslan",
                "Volodymyr Kuleshov",
                "James Tompkin"
            ],
            "affiliations": [
                "Brown University",
                "Cornell University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05441.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ GAN: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ³Ğ°ÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾-ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (GAN). ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ»ÑÑ‚Ğ¸Ğ²Ğ¸ÑÑ‚ÑĞºĞ¸Ğ¹ GAN-Ğ»Ğ¾ÑÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¼Ğ¾Ğ´ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ»Ğ¾ÑÑ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ»ÑÑ‚Ğ¸Ğ²Ğ¸ÑÑ‚ÑĞºĞ¸Ñ… Ğ»Ğ¾ÑÑĞ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ R3GAN, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ StyleGAN2 Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ GAN Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Simplifying GAN Training with R3GAN: A New Era of Efficiency",
                    "desc": "This paper challenges the common belief that Generative Adversarial Networks (GANs) are inherently difficult to train. It introduces a new GAN loss function called the regularized relativistic GAN loss, which effectively addresses issues like mode dropping and non-convergence without relying on numerous empirical tricks. The authors provide mathematical analysis showing that their loss function guarantees local convergence, which is a significant improvement over existing methods. By applying this new loss to modern architectures like StyleGAN2, they create a simplified and efficient GAN model named R3GAN, which outperforms previous models on several benchmark datasets."
                },
                "zh": {
                    "title": "ç®€åŒ–GANè®­ç»ƒï¼Œè¶…è¶Šä¼ ç»Ÿæ¶æ„",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰è®­ç»ƒçš„éš¾ç‚¹ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ­£åˆ™åŒ–çš„ç›¸å¯¹GANæŸå¤±å‡½æ•°ï¼Œè§£å†³äº†æ¨¡å¼ä¸¢å¤±å’Œéæ”¶æ•›çš„é—®é¢˜ã€‚é€šè¿‡æ•°å­¦åˆ†æï¼Œè¯æ˜äº†è¿™ç§æŸå¤±å‡½æ•°å…·æœ‰å±€éƒ¨æ”¶æ•›çš„ä¿è¯ï¼Œä¼˜äºç°æœ‰çš„ç›¸å¯¹æŸå¤±å‡½æ•°ã€‚æœ€ç»ˆï¼Œä½œè€…å±•ç¤ºäº†ä¸€ä¸ªæ–°çš„ç®€çº¦åŸºçº¿R3GANï¼Œå…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†StyleGAN2ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„GANå’Œæ‰©æ•£æ¨¡å‹ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05453",
            "title": "An Empirical Study of Autoregressive Pre-training from Videos",
            "url": "https://huggingface.co/papers/2501.05453",
            "abstract": "We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at https://brjathu.github.io/toto/",
            "score": 15,
            "issue_id": 1596,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "3846ea8507d046be",
            "authors": [
                "Jathushan Rajasegaran",
                "Ilija Radosavovic",
                "Rahul Ravishankar",
                "Yossi Gandelsman",
                "Christoph Feichtenhofer",
                "Jitendra Malik"
            ],
            "affiliations": [
                "Meta FAIR",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05453.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#architecture",
                    "#robotics",
                    "#video",
                    "#cv"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Toto. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Video Understanding with Autoregressive Models",
                    "desc": "This paper investigates the use of autoregressive pre-training for video data through a series of models named Toto. The authors treat videos as sequences of visual tokens and employ transformer architectures to predict future tokens in these sequences. They pre-train their models on a massive dataset containing over 1 trillion visual tokens, exploring various design choices in architecture and training. The results show that these autoregressive models achieve strong performance on tasks like image recognition and video classification, indicating that scaling video models can yield similar benefits as seen in language models."
                },
                "zh": {
                    "title": "è‡ªå›å½’é¢„è®­ç»ƒï¼šè§†é¢‘æ¨¡å‹çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†è§†é¢‘çš„è‡ªå›å½’é¢„è®­ç»ƒã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ç³»åˆ—åä¸ºTotoçš„è‡ªå›å½’è§†é¢‘æ¨¡å‹ï¼Œå°†è§†é¢‘è§†ä¸ºè§†è§‰æ ‡è®°çš„åºåˆ—ï¼Œå¹¶è®­ç»ƒå˜æ¢å™¨æ¨¡å‹ä»¥è‡ªå›å½’æ–¹å¼é¢„æµ‹æœªæ¥çš„æ ‡è®°ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡1ä¸‡äº¿è§†è§‰æ ‡è®°çš„å¤šæ ·åŒ–è§†é¢‘å’Œå›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¯„ä¼°å­¦ä¹ åˆ°çš„è§†è§‰è¡¨ç¤ºã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡è¯±å¯¼åå·®è¾ƒå°ï¼Œè‡ªå›å½’é¢„è®­ç»ƒåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04003",
            "title": "Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives",
            "url": "https://huggingface.co/papers/2501.04003",
            "abstract": "Recent advancements in Vision-Language Models (VLMs) have sparked interest in their use for autonomous driving, particularly in generating interpretable driving decisions through natural language. However, the assumption that VLMs inherently provide visually grounded, reliable, and interpretable explanations for driving remains largely unexamined. To address this gap, we introduce DriveBench, a benchmark dataset designed to evaluate VLM reliability across 17 settings (clean, corrupted, and text-only inputs), encompassing 19,200 frames, 20,498 question-answer pairs, three question types, four mainstream driving tasks, and a total of 12 popular VLMs. Our findings reveal that VLMs often generate plausible responses derived from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs. This behavior, concealed by dataset imbalances and insufficient evaluation metrics, poses significant risks in safety-critical scenarios like autonomous driving. We further observe that VLMs struggle with multi-modal reasoning and display heightened sensitivity to input corruptions, leading to inconsistencies in performance. To address these challenges, we propose refined evaluation metrics that prioritize robust visual grounding and multi-modal understanding. Additionally, we highlight the potential of leveraging VLMs' awareness of corruptions to enhance their reliability, offering a roadmap for developing more trustworthy and interpretable decision-making systems in real-world autonomous driving contexts. The benchmark toolkit is publicly accessible.",
            "score": 8,
            "issue_id": 1599,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "720b493a608f478a",
            "authors": [
                "Shaoyuan Xie",
                "Lingdong Kong",
                "Yuhao Dong",
                "Chonghao Sima",
                "Wenwei Zhang",
                "Qi Alfred Chen",
                "Ziwei Liu",
                "Liang Pan"
            ],
            "affiliations": [
                "National University of Singapore",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory",
                "The University of Hong Kong",
                "University of California, Irvine"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04003.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#interpretability",
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ VLM Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DriveBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ VLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ VLM Ğ¾ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Trust in Vision-Language Models for Safer Autonomous Driving",
                    "desc": "This paper discusses the limitations of Vision-Language Models (VLMs) in the context of autonomous driving, particularly their ability to provide reliable and interpretable driving decisions. The authors introduce DriveBench, a comprehensive benchmark dataset that tests VLM performance across various conditions, including clean and corrupted inputs. Their research shows that VLMs often rely on general knowledge rather than true visual understanding, especially when visual data is compromised. To improve VLM reliability, the paper suggests new evaluation metrics focused on visual grounding and multi-modal reasoning, aiming to enhance the safety of autonomous driving systems."
                },
                "zh": {
                    "title": "æå‡è‡ªåŠ¨é©¾é©¶å†³ç­–çš„å¯é æ€§ä¸å¯è§£é‡Šæ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†DriveBenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å¯é æ€§çš„åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«19200å¸§å›¾åƒå’Œ20498ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–äº†å¤šç§é©¾é©¶ä»»åŠ¡å’Œè¾“å…¥ç±»å‹ã€‚ç ”ç©¶å‘ç°ï¼ŒVLMsåœ¨å¤„ç†å—æŸæˆ–ç¼ºå¤±çš„è§†è§‰è¾“å…¥æ—¶ï¼Œå¾€å¾€ä¾èµ–äºä¸€èˆ¬çŸ¥è¯†è€ŒéçœŸå®çš„è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´å®‰å…¨éšæ‚£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ”¹è¿›çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¼ºè°ƒè§†è§‰åŸºç¡€å’Œå¤šæ¨¡æ€ç†è§£çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04377",
            "title": "On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis",
            "url": "https://huggingface.co/papers/2501.04377",
            "abstract": "Recently, Visual Autoregressive (VAR) Models introduced a groundbreaking advancement in the field of image generation, offering a scalable approach through a coarse-to-fine \"next-scale prediction\" paradigm. However, the state-of-the-art algorithm of VAR models in [Tian, Jiang, Yuan, Peng and Wang, NeurIPS 2024] takes O(n^4) time, which is computationally inefficient. In this work, we analyze the computational limits and efficiency criteria of VAR Models through a fine-grained complexity lens. Our key contribution is identifying the conditions under which VAR computations can achieve sub-quadratic time complexity. Specifically, we establish a critical threshold for the norm of input matrices used in VAR attention mechanisms. Above this threshold, assuming the Strong Exponential Time Hypothesis (SETH) from fine-grained complexity theory, a sub-quartic time algorithm for VAR models is impossible. To substantiate our theoretical findings, we present efficient constructions leveraging low-rank approximations that align with the derived criteria. This work initiates the study of the computational efficiency of the VAR model from a theoretical perspective. Our technique will shed light on advancing scalable and efficient image generation in VAR frameworks.",
            "score": 5,
            "issue_id": 1597,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "be8a0f20db676680",
            "authors": [
                "Yekun Ke",
                "Xiaoyu Li",
                "Yingyu Liang",
                "Zhizhou Sha",
                "Zhenmei Shi",
                "Zhao Song"
            ],
            "affiliations": [
                "The Simons Institute for the Theory of Computing at UC Berkeley",
                "The University of Hong Kong",
                "Tsinghua University",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04377.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² VAR Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… (VAR) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ VAR Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑÑƒĞ±ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ VAR, Ğ²Ñ‹ÑˆĞµ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶ĞµĞ½ ÑÑƒĞ±ĞºĞ²Ğ°Ñ€Ñ†ĞµĞ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ VAR. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ²Ñ‹Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Unlocking Efficiency in Image Generation with VAR Models",
                    "desc": "This paper explores the computational efficiency of Visual Autoregressive (VAR) Models, which are used for generating images. The authors identify that the current state-of-the-art VAR algorithm is computationally expensive, operating in O(n^4) time complexity. They establish conditions under which VAR computations can be optimized to achieve sub-quadratic time complexity, particularly focusing on the input matrix norms in the attention mechanisms. By applying low-rank approximations, the authors provide practical constructions that meet their theoretical criteria, paving the way for more efficient image generation techniques in VAR frameworks."
                },
                "zh": {
                    "title": "æå‡VARæ¨¡å‹çš„è®¡ç®—æ•ˆç‡",
                    "desc": "æœ€è¿‘ï¼Œè§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œé‡‡ç”¨ç²—åˆ°ç»†çš„â€œä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹â€èŒƒå¼ã€‚ç„¶è€Œï¼ŒVARæ¨¡å‹çš„æœ€æ–°ç®—æ³•åœ¨è®¡ç®—ä¸Šæ•ˆç‡ä½ä¸‹ï¼Œæ—¶é—´å¤æ‚åº¦ä¸ºO(n^4)ã€‚æœ¬æ–‡é€šè¿‡ç»†ç²’åº¦å¤æ‚æ€§åˆ†æï¼Œæ¢è®¨äº†VARæ¨¡å‹çš„è®¡ç®—é™åˆ¶å’Œæ•ˆç‡æ ‡å‡†ã€‚æˆ‘ä»¬ç¡®å®šäº†VARè®¡ç®—å¯ä»¥å®ç°äºšäºŒæ¬¡æ—¶é—´å¤æ‚åº¦çš„æ¡ä»¶ï¼Œå¹¶æå‡ºäº†åˆ©ç”¨ä½ç§©è¿‘ä¼¼çš„é«˜æ•ˆæ„é€ ï¼Œä»¥æ”¯æŒæˆ‘ä»¬çš„ç†è®ºå‘ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05122",
            "title": "Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model",
            "url": "https://huggingface.co/papers/2501.05122",
            "abstract": "Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50\\% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train Centurio, a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages.",
            "score": 4,
            "issue_id": 1604,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "92d74f3bbeb4a400",
            "authors": [
                "Gregor Geigle",
                "Florian Schneider",
                "Carolin Holtermann",
                "Chris Biemann",
                "Radu Timofte",
                "Anne Lauscher",
                "Goran GlavaÅ¡"
            ],
            "affiliations": [
                "Data Science Group, University of Hamburg",
                "Language Technology Group",
                "WÃ¼NLP, Computer Vision Lab, CAIDAS, University of WÃ¼rzburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05122.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#multilingual",
                    "#benchmark",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Centurio: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLMs). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 13 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ 43 ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ 100 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 25-50% Ğ½ĞµĞ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Centurio - 100-ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ LVLM, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 14 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ 56 ÑĞ·Ñ‹ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Multilingual Mastery in Vision-Language Models",
                    "desc": "This paper investigates how to effectively train Large Vision-Language Models (LVLMs) on multiple languages, particularly focusing on improving their performance in non-English languages. The authors conduct experiments across various tasks and languages to determine the best strategies for including multilingual data without harming English performance. They discover that including up to 100 languages and using a smaller proportion of non-English data can enhance multilingual capabilities while maintaining strong English results. Additionally, they emphasize the importance of incorporating non-English OCR data to boost understanding of text within images, culminating in the development of Centurio, a 100-language LVLM with state-of-the-art performance."
                },
                "zh": {
                    "title": "æå‡å¤šè¯­è¨€ç†è§£ï¼ŒCenturioå¼•é¢†æ–°æ½®æµ",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§è§„æ¨¡å¤šè¯­è¨€è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„è®­ç»ƒç­–ç•¥ï¼Œç‰¹åˆ«å…³æ³¨å¦‚ä½•æé«˜æ¨¡å‹å¯¹éè‹±è¯­è¾“å…¥çš„ç†è§£å’Œè¾“å‡ºèƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¤šé˜¶æ®µå®éªŒï¼Œåˆ†æäº†åŒ…å«å¤šç§è¯­è¨€çš„è®­ç»ƒæ•°æ®å¯¹è‹±è¯­æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¢ç´¢äº†æœ€ä½³çš„è¯­è¨€åˆ†å¸ƒç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼Œæœ€å¤šå¯ä»¥åŒæ—¶åŒ…å«100ç§è¯­è¨€çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”åªéœ€25-50%çš„éè‹±è¯­æ•°æ®å³å¯æ˜¾è‘—æå‡å¤šè¯­è¨€æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆæ‰€æœ‰å‘ç°ï¼Œè®­ç»ƒäº†Centurioï¼Œä¸€ä¸ªæ”¯æŒ100ç§è¯­è¨€çš„LVLMï¼Œåœ¨14ä¸ªä»»åŠ¡å’Œ56ç§è¯­è¨€çš„è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03489",
            "title": "Entropy-Guided Attention for Private LLMs",
            "url": "https://huggingface.co/papers/2501.03489",
            "abstract": "The pervasiveness of proprietary language models has raised critical privacy concerns, necessitating advancements in private inference (PI), where computations are performed directly on encrypted data without revealing users' sensitive information. While PI offers a promising solution, its practical deployment is hindered by substantial communication and latency overheads, primarily stemming from nonlinear operations. To address this, we introduce an information-theoretic framework to characterize the role of nonlinearities in decoder-only language models, laying a principled foundation for optimizing transformer-architectures tailored to the demands of PI.   By leveraging Shannon's entropy as a quantitative measure, we uncover the previously unexplored dual significance of nonlinearities: beyond ensuring training stability, they are crucial for maintaining attention head diversity. Specifically, we find that their removal triggers two critical failure modes: {\\em entropy collapse} in deeper layers that destabilizes training, and {\\em entropic overload} in earlier layers that leads to under-utilization of Multi-Head Attention's (MHA) representational capacity.   We propose an entropy-guided attention mechanism paired with a novel entropy regularization technique to mitigate entropic overload. Additionally, we explore PI-friendly alternatives to layer normalization for preventing entropy collapse and stabilizing the training of LLMs with reduced-nonlinearities. Our study bridges the gap between information theory and architectural design, establishing entropy dynamics as a principled guide for developing efficient PI architectures. The code and implementation are available at https://github.com/Nandan91/entropy-guided-attention-llm{entropy-guided-llm}.",
            "score": 4,
            "issue_id": 1597,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "18abcfb3fe1b209b",
            "authors": [
                "Nandan Kumar Jha",
                "Brandon Reagen"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03489.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#inference",
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ (PI). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ PI, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¨ĞµĞ½Ğ½Ğ¾Ğ½Ğ° ĞºĞ°Ğº ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¼ĞµÑ€Ñƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ PI-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "Optimizing Language Models for Privacy with Entropy Dynamics",
                    "desc": "This paper addresses privacy concerns related to proprietary language models by focusing on private inference (PI), which allows computations on encrypted data. The authors introduce an information-theoretic framework to analyze the impact of nonlinearities in decoder-only language models, which are essential for optimizing transformer architectures for PI. They identify two critical issues caused by the removal of nonlinearities: entropy collapse in deeper layers and entropic overload in earlier layers, both of which affect training stability and attention mechanisms. To resolve these issues, the paper proposes an entropy-guided attention mechanism and explores alternatives to layer normalization, aiming to enhance the efficiency of PI architectures while maintaining model performance."
                },
                "zh": {
                    "title": "ä¼˜åŒ–ç§å¯†æ¨ç†æ¶æ„çš„ç†µåŠ¨æ€",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨åŠ å¯†æ•°æ®ä¸Šè¿›è¡Œç§å¯†æ¨ç†ï¼ˆPIï¼‰æ—¶ï¼Œéçº¿æ€§æ“ä½œå¯¹è§£ç å™¨è¯­è¨€æ¨¡å‹çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¿¡æ¯è®ºæ¡†æ¶ï¼Œå¸®åŠ©ä¼˜åŒ–é€‚åˆPIéœ€æ±‚çš„å˜æ¢å™¨æ¶æ„ã€‚ç ”ç©¶å‘ç°ï¼Œéçº¿æ€§ä¸ä»…ç¡®ä¿äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œè¿˜å¯¹æ³¨æ„åŠ›å¤´çš„å¤šæ ·æ€§è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³ç†µå´©æºƒå’Œç†µè¿‡è½½é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç†µçš„æ³¨æ„åŠ›æœºåˆ¶å’Œæ–°çš„ç†µæ­£åˆ™åŒ–æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05032",
            "title": "Enhancing Human-Like Responses in Large Language Models",
            "url": "https://huggingface.co/papers/2501.05032",
            "abstract": "This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes.",
            "score": 3,
            "issue_id": 1609,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "64e14687fd1e5dab",
            "authors": [
                "Ethem YaÄŸÄ±z Ã‡alÄ±k",
                "Talha RÃ¼zgar AkkuÅŸ"
            ],
            "affiliations": [
                "Hugging Face"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05032.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#ethics",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑƒÑ‚ÑŒ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼, Ğ½Ğ¾ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Enhancing AI: Making Language Models More Human-Like",
                    "desc": "This paper investigates how to make large language models (LLMs) behave more like humans. It emphasizes improving natural language understanding, making conversations more coherent, and increasing emotional intelligence in AI. The research assesses methods such as fine-tuning models with varied datasets and applying psychological principles to enhance human-like reasoning. The results show that these improvements lead to better user experiences and expand the potential uses of AI, while also highlighting the need to consider ethical issues and biases that may arise."
                },
                "zh": {
                    "title": "è®©äººå·¥æ™ºèƒ½æ›´åƒäººç±»çš„æœªæ¥",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ›´å…·äººæ€§åŒ–çš„è¿›å±•ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨å¢å¼ºè‡ªç„¶è¯­è¨€ç†è§£ã€å¯¹è¯è¿è´¯æ€§å’Œæƒ…æ„Ÿæ™ºèƒ½çš„æŠ€æœ¯ã€‚ç ”ç©¶è¯„ä¼°äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨å¤šæ ·åŒ–æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€èå…¥å¿ƒç†å­¦åŸç†ï¼Œä»¥åŠè®¾è®¡æ›´å¥½æ¨¡æ‹Ÿäººç±»æ¨ç†æ¨¡å¼çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œè¿™äº›å¢å¼ºä¸ä»…æ”¹å–„äº†ç”¨æˆ·äº’åŠ¨ï¼Œè¿˜ä¸ºä¸åŒé¢†åŸŸçš„äººå·¥æ™ºèƒ½åº”ç”¨å¼€è¾Ÿäº†æ–°å¯èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04828",
            "title": "Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models",
            "url": "https://huggingface.co/papers/2501.04828",
            "abstract": "This paper introduces foundational resources and models for natural language processing (NLP) of historical Turkish, a domain that has remained underexplored in computational linguistics. We present the first named entity recognition (NER) dataset, HisTR and the first Universal Dependencies treebank, OTA-BOUN for a historical form of the Turkish language along with transformer-based models trained using these datasets for named entity recognition, dependency parsing, and part-of-speech tagging tasks. Additionally, we introduce Ottoman Text Corpus (OTC), a clean corpus of transliterated historical Turkish texts that spans a wide range of historical periods. Our experimental results show significant improvements in the computational analysis of historical Turkish, achieving promising results in tasks that require understanding of historical linguistic structures. They also highlight existing challenges, such as domain adaptation and language variations across time periods. All of the presented resources and models are made available at https://huggingface.co/bucolin to serve as a benchmark for future progress in historical Turkish NLP.",
            "score": 3,
            "issue_id": 1603,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "40fe69c40d907fc4",
            "authors": [
                "Åaziye BetÃ¼l Ã–zateÅŸ",
                "TarÄ±k Emre TÄ±raÅŸ",
                "Ece Elif Adak",
                "Berat DoÄŸan",
                "Fatih Burak KaragÃ¶z",
                "Efe Eren GenÃ§",
                "Esma F. Bilgin TaÅŸdemir"
            ],
            "affiliations": [
                "BogaziÃ§i University",
                "Medeniyet University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04828.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#low_resource",
                    "#science",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² NLP Ğ´Ğ»Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (NLP) Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ (NER) HisTR Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Universal Dependencies Ñ‚Ñ€Ğ¸Ğ²Ğ±Ğ°Ğ½Ğº OTA-BOUN Ğ´Ğ»Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ NER, ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞÑĞ¼Ğ°Ğ½ÑĞºĞ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ (OTC) - Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑĞ»Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Historical Turkish: New Resources for NLP",
                    "desc": "This paper provides essential resources and models for processing historical Turkish language using natural language processing (NLP) techniques. It introduces the first named entity recognition (NER) dataset, HisTR, and the first Universal Dependencies treebank, OTA-BOUN, specifically for historical Turkish. The authors also present the Ottoman Text Corpus (OTC), a comprehensive collection of transliterated texts from various historical periods. The results demonstrate advancements in analyzing historical Turkish, while also addressing challenges like domain adaptation and linguistic variations over time."
                },
                "zh": {
                    "title": "æ¨åŠ¨å†å²åœŸè€³å…¶è¯­NLPçš„è¿›æ­¥",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†å†å²åœŸè€³å…¶è¯­è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„åŸºç¡€èµ„æºå’Œæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨è®¡ç®—è¯­è¨€å­¦ä¸­å°šæœªæ·±å…¥ç ”ç©¶çš„é¢†åŸŸã€‚æˆ‘ä»¬é¦–æ¬¡å‘å¸ƒäº†å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ•°æ®é›†HisTRå’Œå†å²åœŸè€³å…¶è¯­çš„Universal Dependenciesæ ‘åº“OTA-BOUNï¼Œå¹¶åŸºäºè¿™äº›æ•°æ®é›†è®­ç»ƒäº†ç”¨äºå‘½åå®ä½“è¯†åˆ«ã€ä¾å­˜å¥æ³•åˆ†æå’Œè¯æ€§æ ‡æ³¨ä»»åŠ¡çš„å˜æ¢å™¨æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†å¥¥æ–¯æ›¼æ–‡æœ¬è¯­æ–™åº“ï¼ˆOTCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¤šä¸ªå†å²æ—¶æœŸçš„æ¸…æ™°è½¬å†™å†å²åœŸè€³å…¶è¯­æ–‡æœ¬çš„è¯­æ–™åº“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å†å²åœŸè€³å…¶è¯­çš„è®¡ç®—åˆ†æä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¹Ÿçªæ˜¾äº†é¢†åŸŸé€‚åº”å’Œè¯­è¨€éšæ—¶é—´å˜åŒ–ç­‰æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05040",
            "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution",
            "url": "https://huggingface.co/papers/2501.05040",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source LLM designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other LLM model to generate patches for the identified files. Then, to mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches, and train the two modules of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving state-of-the-art performance among open-source models with scores of 23.3% and 30.2%, respectively. These outcomes highlight the efficacy of our approach. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.",
            "score": 2,
            "issue_id": 1608,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 9",
                "zh": "1æœˆ9æ—¥"
            },
            "hash": "54d8f8a0fe5436c6",
            "authors": [
                "Chengxing Xie",
                "Bowen Li",
                "Chang Gao",
                "He Du",
                "Wai Lam",
                "Difan Zou",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "Xidian University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05040.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#training",
                    "#science"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğ° GitHub",
                    "desc": "SWE-Fixer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğ° GitHub. ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 110 Ñ‚Ñ‹ÑÑÑ‡ GitHub-issues Ñ Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. SWE-Fixer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SWE-Bench Lite Ğ¸ Verified."
                },
                "en": {
                    "title": "SWE-Fixer: Open-Source Solutions for GitHub Issues",
                    "desc": "This paper presents SWE-Fixer, an open-source Large Language Model (LLM) specifically designed to address software engineering challenges on GitHub. It features two main components: a code file retrieval module that uses BM25 and a lightweight LLM for efficient file identification, and a code editing module that generates code patches using another LLM. The authors also created a comprehensive dataset of 110,000 GitHub issues and their corresponding patches to train the model effectively. SWE-Fixer achieves state-of-the-art performance on benchmark tests, demonstrating its potential to enhance accessibility and transparency in software engineering solutions."
                },
                "zh": {
                    "title": "å¼€æºLLMåŠ©åŠ›è½¯ä»¶å·¥ç¨‹é—®é¢˜è§£å†³",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å¼€æºLLMï¼Œåä¸ºSWE-Fixerï¼Œæ—¨åœ¨æœ‰æ•ˆè§£å†³GitHubä¸Šçš„é—®é¢˜ã€‚SWE-FixeråŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä»£ç æ–‡ä»¶æ£€ç´¢æ¨¡å—å’Œä»£ç ç¼–è¾‘æ¨¡å—ï¼Œå‰è€…ä½¿ç”¨BM25å’Œè½»é‡çº§LLMè¿›è¡Œæ–‡ä»¶æ£€ç´¢ï¼Œåè€…ç”Ÿæˆä»£ç è¡¥ä¸ã€‚é€šè¿‡æ„å»ºåŒ…å«11ä¸‡ä¸ªGitHubé—®é¢˜åŠå…¶è¡¥ä¸çš„æ•°æ®é›†ï¼ŒSWE-Fixeråœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†é¢†å…ˆçš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-09.html",
    "link_next": "2025-01-13.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "09.01",
        "en": "01/09",
        "zh": "1æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "13.01",
        "en": "01/13",
        "zh": "1æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 2,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 2
    },
    "zh": {
        "text": "æˆ‘ä»¬é€šè¿‡å®éªŒç ”ç©¶äº†ä»è§†é¢‘ä¸­è¿›è¡Œè‡ªå›å½’é¢„è®­ç»ƒçš„æ–¹æ³•ã€‚ä¸ºäº†è¿›è¡Œç ”ç©¶ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ç³»åˆ—è‡ªå›å½’è§†é¢‘æ¨¡å‹ï¼Œç§°ä¸ºTotoã€‚æˆ‘ä»¬å°†è§†é¢‘è§†ä¸ºè§†è§‰æ ‡è®°çš„åºåˆ—ï¼Œå¹¶è®­ç»ƒå˜å‹å™¨æ¨¡å‹è‡ªå›å½’åœ°é¢„æµ‹æœªæ¥çš„æ ‡è®°ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åŒ…å«è¶…è¿‡1ä¸‡äº¿è§†è§‰æ ‡è®°çš„å¤šæ ·åŒ–è§†é¢‘å’Œå›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸åŒçš„æ¶æ„ã€è®­ç»ƒå’Œæ¨ç†è®¾è®¡é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è§†é¢‘åˆ†ç±»ã€ç›®æ ‡è·Ÿè¸ªå’Œæœºå™¨äººæŠ€æœ¯åœ¨å†…çš„å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¯„ä¼°äº†å­¦ä¹ åˆ°çš„è§†è§‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å­˜åœ¨æœ€å°çš„å½’çº³åå·®ï¼Œè‡ªå›å½’é¢„è®­ç»ƒåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­éƒ½è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æ‰©å±•æˆ‘ä»¬çš„è§†é¢‘æ¨¡å‹ä¼šäº§ç”Ÿç±»ä¼¼äºè¯­è¨€æ¨¡å‹çš„æ‰©å±•æ›²çº¿ï¼Œåªæ˜¯é€Ÿç‡ä¸åŒã€‚æ›´å¤šç»†èŠ‚è¯·è®¿é—® https://brjathu.github.io/toto/ã€‚",
        "title": "An Empirical Study of Autoregressive Pre-training from Videos",
        "pinyin": "WÇ’men tÅngguÃ² shÃ­yÃ n yÃ¡njiÅ«le cÃ³ng shÃ¬pÃ­n zhÅng jÃ¬nxÃ­ng zÃ¬huÃ­guÄ« yÃ¹xÃ¹nliÃ n de fÄngfÇ. WÃ¨ile jÃ¬nxÃ­ng yÃ¡njiÅ«, wÇ’men gÃ²ujiÃ nle yÄ« xÃ¬liÃ¨ zÃ¬huÃ­guÄ« shÃ¬pÃ­n mÃ³xÃ­ng, chÄ“ngwÃ©i Toto. WÇ’men jiÄng shÃ¬pÃ­n shÃ¬wÃ©i shÃ¬juÃ© biÄojÃ¬ de xÃ¹liÃ¨, bÃ¬ng xÃ¹nliÃ n biÃ nshÃ¹qÃ¬ mÃ³xÃ­ng zÃ¬huÃ­guÄ« de yÃ¹cÃ¨ wÃ¨ilÃ¡i de biÄojÃ¬. WÇ’men de mÃ³xÃ­ng zÃ i bÄohÃ¡n chÄoguÃ² 1 wÃ n yÃ¬ shÃ¬juÃ© biÄojÃ¬ de duÅyÃ nghuÃ  shÃ¬pÃ­n hÃ© tÃºxiÃ ng shÃ¹jÃ¹jÃ­ shÃ ng jÃ¬nxÃ­ngle yÃ¹xÃ¹nliÃ n. WÇ’men tuÃ nsuÇ’le bÃ¹tÃ³ng de jiÃ gÃ²u, xÃ¹nliÃ n hÃ© tuÄ«lÇ shÃ¨jÃ¬ xuÇnzÃ©. WÇ’men zÃ i bÄokuÃ² tÃºxiÃ ng shÃ­biÃ©, shÃ¬pÃ­n fÄ“nlÃ¨i, mÃ¹biÄo gÄ“nzÅng hÃ© jÄ«qÃ¬rÃ©n jÃ¬shÃ¹ zÃ i nÃ¨i de duÅgÃ¨ xiÃ yÃ³u rÃ¨nwÃ¹ shÃ ng pÃ­nggÅ«le xuÃ©xÃ­ dÃ o de shÃ¬juÃ© biÇoshÃ¬. WÇ’men de jiÃ©guÇ’ biÇomÃ­ng, jÄ«nshÇ yÇ’u zuÃ¬shÇo de guÄ«nÃ  piÄnchÃ¡, zÃ¬huÃ­guÄ« yÃ¹xÃ¹nliÃ n zÃ i suÇ’yÇ’u jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng dÅu biÇoxiÃ n chÅ« jÃ¬ngzhÄ“nglÃ¬. ZuÃ¬hÃ²u, wÇ’men fÄxiÃ n kuÃ²zhÇn wÇ’men de shÃ¬pÃ­n mÃ³xÃ­ng huÃ¬ chÇnshÄ“ng xiÃ ng yÇ”yÃ¡n mÃ³xÃ­ng de kuÃ²zhÇn qÇ”xiÃ n, zhÇshÃ¬ sÃ¹lÇœ bÃ¹tÃ³ng. GÃ¨ng duÅ xÃ¬jiÃ¨ qÇng fÇngwÃ¨n https://brjathu.github.io/toto/ã€‚",
        "vocab": "[{'word': 'è‡ªå›å½’', 'pinyin': 'zÃ¬ huÃ­ guÄ«', 'trans': 'autoregressive'}, {'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pretraining'}, {'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'}, {'word': 'æ ‡è®°', 'pinyin': 'biÄo jÃ¬', 'trans': 'token'}, {'word': 'å˜å‹å™¨', 'pinyin': 'biÃ n yÄ qÃ¬', 'trans': 'transformer'}, {'word': 'å¤šæ ·åŒ–', 'pinyin': 'duÅ yÃ ng huÃ ', 'trans': 'diversified'}, {'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'}, {'word': 'å½’çº³', 'pinyin': 'guÄ« nÃ ', 'trans': 'inductive'}, {'word': 'åå·®', 'pinyin': 'piÄn chÄ', 'trans': 'bias'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'ç«äº‰åŠ›', 'pinyin': 'jÃ¬ng zhÄ“ng lÃ¬', 'trans': 'competitive'}, {'word': 'æ‰©å±•', 'pinyin': 'kuÃ² zhÇn', 'trans': 'scaling'}, {'word': 'æ›²çº¿', 'pinyin': 'qÇ” xiÃ n', 'trans': 'curve'}, {'word': 'é€Ÿç‡', 'pinyin': 'sÃ¹ lÇœ', 'trans': 'rate'}]",
        "trans": "We investigated methods for autoregressive pretraining from videos through experimental research. To conduct the study, we constructed a series of autoregressive video models called Toto. We treated videos as sequences of visual tokens and trained transformer models to autoregressively predict future tokens. Our models were pretrained on diverse video and image datasets containing over 1 trillion visual tokens. We explored various architectural, training, and inference design choices. We evaluated the learned visual representations on multiple downstream tasks, including image recognition, video classification, object tracking, and robotics. Our results indicate that, despite minimal inductive bias, autoregressive pretraining performs competitively on all benchmarks. Finally, we found that scaling our video models produces scaling curves similar to those of language models, albeit at different rates. For more details, please visit https://brjathu.github.io/toto/.",
        "update_ts": "2025-01-10 09:11"
    }
}