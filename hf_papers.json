{
    "date": {
        "ru": "10 октября",
        "en": "October 10",
        "zh": "10月10日"
    },
    "time_utc": "2025-10-10 02:19",
    "weekday": 4,
    "issue_id": 6344,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.08483",
            "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
            "url": "https://huggingface.co/papers/2510.08483",
            "abstract": "DeepPrune, a novel framework using dynamic pruning and a specialized judge model, significantly reduces computational inefficiency in parallel scaling of large language models by pruning redundant reasoning traces.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/",
            "score": 2,
            "issue_id": 6344,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "13be68ae86d17de1",
            "authors": [
                "Shangqing Tu",
                "Yaxuan Li",
                "Yushi Bai",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "ShanghaiTech University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08483.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#inference",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Умная обрезка избыточных рассуждений в параллельных LLM",
                    "desc": "DeepPrune — это новый фреймворк для эффективного параллельного скейлинга LLM, который решает проблему избыточности при генерации множественных рассуждений. Исследователи обнаружили, что более 80% параллельных цепочек рассуждений приводят к одинаковым ответам, что означает огромные вычислительные потери. Система использует специальную judge-модель для предсказания эквивалентности ответов и динамически удаляет избыточные пути рассуждений через онлайн кластеризацию. DeepPrune сокращает количество токенов более чем на 80% при сохранении точности в пределах 3 процентных пунктов на сложных бенчмарках."
                },
                "en": {
                    "title": "Efficient Reasoning with DeepPrune: Prune the Redundancy!",
                    "desc": "DeepPrune is a new framework designed to improve the efficiency of large language models by reducing unnecessary computations during parallel reasoning. It identifies and prunes redundant reasoning paths that often lead to the same answers, which can waste over 80% of computational resources. The framework employs a specialized judge model that predicts when reasoning traces are equivalent, allowing for dynamic pruning of these redundant paths. Evaluations show that DeepPrune can significantly reduce the number of tokens used while maintaining high accuracy, setting a new benchmark for efficient reasoning in AI models."
                },
                "zh": {
                    "title": "DeepPrune：高效并行推理的新标准",
                    "desc": "DeepPrune是一个新颖的框架，通过动态剪枝和专门的判断模型，显著减少了大语言模型在并行扩展中的计算低效。该方法解决了并行推理中存在的冗余问题，分析显示超过80%的推理轨迹产生相同的最终答案，造成了大量的计算浪费。DeepPrune通过训练具有焦点损失和过采样技术的判断模型，准确预测部分推理轨迹的答案等价性，并结合在线贪婪聚类算法动态剪除冗余路径。经过在多个基准测试上的全面评估，DeepPrune在大多数情况下实现了超过80%的令牌减少，同时保持了与传统共识采样相近的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08308",
            "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
            "url": "https://huggingface.co/papers/2510.08308",
            "abstract": "Analysis of reflective behaviors in reasoning models shows that reflections primarily confirm initial answers, and training with more reflections improves first-answer correctness; a question-aware early-stopping method reduces unnecessary reflections and tokens with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have recently demonstrated significant gains in reasoning ability, often attributed to their capacity to generate longer chains of thought and engage in reflective reasoning. However, the contribution of reflections to performance improvement remains unclear. In this paper, we systematically analyze the rollouts of eight reasoning models on five mathematical datasets. We focus on reflective behaviours where the model has already produced an answer but continues reflecting before finalizing its output. Our analysis reveals that reflections are predominantly confirmatory and rarely alter the model's initial answer, a pattern consistent across models and datasets. To understand the role of reflections in training, we construct supervised fine-tuning (SFT) datasets with varying amounts of reflection steps. We observe that training models on rollouts with more reflection steps primarily enhances first-answer correctness rather than the ability to correct initially wrong answers through reflections. This motivates us to propose a question-aware early-stopping method that enhances inference-time token efficiency by stopping the reasoning process once a few plausible candidate answers are generated, thereby reducing unnecessary reflection steps. Motivated by this, we further propose to dynamically truncate the reflections after a candidate answer has appeared during generation, which reduces reasoning tokens by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
            "score": 2,
            "issue_id": 6344,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "4a9143a437581621",
            "authors": [
                "Liwei Kang",
                "Yue Deng",
                "Yao Xiao",
                "Zhanfeng Mo",
                "Wee Sun Lee",
                "Lidong Bing"
            ],
            "affiliations": [
                "MiroMind AI",
                "National University of Singapore",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08308.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#inference",
                    "#data",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🪞",
                "ru": {
                    "title": "Рефлексии LLM подтверждают, а не исправляют: оптимизация через раннюю остановку",
                    "desc": "Исследование показывает, что рефлексии в reasoning-моделях в основном подтверждают первоначальные ответы, а не исправляют их. Обучение на данных с большим количеством рефлексий улучшает корректность первого ответа, но не способность к самокоррекции. Авторы предлагают метод ранней остановки генерации после получения нескольких правдоподобных ответов. Этот подход сокращает количество токенов на 24.5% при падении точности всего на 2.9%."
                },
                "en": {
                    "title": "Enhancing Reasoning Efficiency with Reflective Training",
                    "desc": "This paper investigates how reflective behaviors in reasoning models affect their performance, particularly in confirming initial answers. It finds that while reflections often do not change the first answer, training with more reflection steps improves the correctness of these initial answers. The authors introduce a question-aware early-stopping method to minimize unnecessary reflections and reduce token usage during inference. This method effectively decreases reasoning tokens by 24.5% with only a slight accuracy drop of 2.9%."
                },
                "zh": {
                    "title": "反思提升初始答案的正确性",
                    "desc": "本文分析了推理模型中的反思行为，发现反思主要是确认初始答案，而不是改变它。通过对八个推理模型在五个数学数据集上的表现进行系统分析，我们发现更多的反思步骤可以提高初始答案的正确性。我们提出了一种基于问题的早停方法，可以在生成几个合理候选答案后停止推理，从而减少不必要的反思步骤。实验结果表明，这种方法在减少推理令牌的同时，仅有轻微的准确性下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08565",
            "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints",
            "url": "https://huggingface.co/papers/2510.08565",
            "abstract": "Native end-to-end training of Multimodal Large Language Models (MLLMs) achieves competitive performance with a balanced design and scaling relationship between visual encoders and LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.",
            "score": 1,
            "issue_id": 6344,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "39431998f40d1db5",
            "authors": [
                "Changyao Tian",
                "Hao Li",
                "Gen Luo",
                "Xizhou Zhu",
                "Weijie Su",
                "Hanming Deng",
                "Jinguo Zhu",
                "Jie Shao",
                "Ziran Zhu",
                "Yunpeng Liu",
                "Lewei Lu",
                "Wenhai Wang",
                "Hongsheng Li",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Nanjing University",
                "Sensetime Research",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08565.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#architecture",
                    "#multimodal",
                    "#optimization",
                    "#agi"
                ],
                "emoji": "🔗",
                "ru": {
                    "title": "Нативное обучение мультимодальных моделей с нуля",
                    "desc": "Исследователи изучили end-to-end обучение Multimodal Large Language Models (MLLM) вместо традиционного композиционного подхода, где предобученные визуальные энкодеры соединяются с предобученными LLM. Они систематически исследовали архитектурные решения и свойства масштабирования при ограниченных данных, найдя оптимальный баланс между производительностью и стоимостью обучения. Важным открытием стала позитивная корреляция в масштабировании между визуальными энкодерами и LLM компонентами. На основе этих находок создана модель NaViL, показавшая конкурентные результаты на 14 мультимодальных бенчмарках."
                },
                "en": {
                    "title": "Revolutionizing MLLMs with Native End-to-End Training",
                    "desc": "This paper introduces a new approach to training Multimodal Large Language Models (MLLMs) called native end-to-end training. Unlike traditional methods that use separate pre-trained vision and language models, this approach integrates both components in a single training process. The authors explore the design and scaling properties of this method, demonstrating that a balanced relationship between visual encoders and language models can enhance performance while managing training costs. Their proposed model, NaViL, shows competitive results across multiple benchmarks, paving the way for future research in native MLLMs."
                },
                "zh": {
                    "title": "原生端到端训练，提升多模态模型性能",
                    "desc": "本文探讨了多模态大型语言模型（MLLMs）的原生端到端训练方法。与传统的组合训练方法不同，本文提出了一种新的设计空间和扩展特性，强调视觉编码器与语言模型之间的平衡关系。通过系统研究，作者提出了名为NaViL的原生MLLM，展示了其在14个多模态基准测试中的竞争性能。研究结果为未来的原生MLLM研究提供了深入的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07429",
            "title": "Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs",
            "url": "https://huggingface.co/papers/2510.07429",
            "abstract": "BaRP, a Bandit-feedback Routing with Preferences approach, optimizes large language model selection in an online setting with partial feedback, outperforming offline routers and large models.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient use of large language models (LLMs) is critical for deployment at scale: without adaptive routing, systems either overpay for strong models or risk poor performance from weaker ones. Selecting the right LLM for each query is fundamentally an online decision problem: models differ in strengths, prices fluctuate, and users value accuracy and cost differently. Yet most routers are trained offline with labels for all candidate models, an assumption that breaks in deployment, where only the outcome of the chosen model is observed. We bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach that trains under the same partial-feedback restriction as deployment, while supporting preference-tunable inference: operators can dial the performance/cost trade-off at test time without retraining. Framed as a contextual bandit over prompt features and a user preference vector, our method simulates an online feedback setting during training and adapts its routing decisions to each new prompt, rather than depending on full-information offline supervision. Comprehensive experiments show that our method consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, and generalizes robustly for unseen tasks.",
            "score": 1,
            "issue_id": 6344,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 октября",
                "en": "October 8",
                "zh": "10月8日"
            },
            "hash": "a22ba38f76ba84ee",
            "authors": [
                "Wang Wei",
                "Tiankai Yang",
                "Hongjie Chen",
                "Yue Zhao",
                "Franck Dernoncourt",
                "Ryan A. Rossi",
                "Hoda Eldardiry"
            ],
            "affiliations": [
                "Adobe Research",
                "Dolby Labs",
                "University of Southern California",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07429.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🎰",
                "ru": {
                    "title": "Умный выбор языковой модели на ходу",
                    "desc": "Статья представляет BaRP — систему для выбора оптимальной языковой модели в режиме онлайн с частичной обратной связью. В отличие от классических роутеров, которые обучаются офлайн с полной информацией обо всех моделях, BaRP работает как contextual bandit и учится только на результатах выбранной модели. Система позволяет гибко настраивать баланс между качеством и стоимостью без переобучения, адаптируя решения к каждому новому запросу. Эксперименты показывают превосходство над офлайн-роутерами на 12.46% и над самыми большими LLM на 2.45%."
                },
                "en": {
                    "title": "Optimize LLM Selection with BaRP: Smart, Adaptive, and Cost-Effective!",
                    "desc": "BaRP is a novel approach that optimizes the selection of large language models (LLMs) in real-time using a bandit-feedback mechanism. It addresses the challenge of choosing the right model based on partial feedback, which is common in practical applications. By allowing operators to adjust the balance between performance and cost dynamically, BaRP enhances decision-making without needing to retrain the models. Experimental results demonstrate that BaRP significantly outperforms traditional offline routers and even the largest LLMs, making it a robust solution for adaptive model selection."
                },
                "zh": {
                    "title": "智能选择，优化模型性能",
                    "desc": "BaRP是一种基于偏好的带反馈路由方法，旨在优化大型语言模型的选择。它在在线环境中处理部分反馈，能够有效地选择合适的模型，避免了过度支付或性能不佳的问题。与传统的离线路由器不同，BaRP在训练时模拟在线反馈，支持在测试时根据用户偏好调整性能和成本的权衡。实验结果表明，BaRP在多个任务上均优于强大的离线路由器和大型语言模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06915",
            "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
            "url": "https://huggingface.co/papers/2510.06915",
            "abstract": "A benchmark and training strategy for reward models to improve long-context consistency and performance in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.",
            "score": 1,
            "issue_id": 6344,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 октября",
                "en": "October 8",
                "zh": "10月8日"
            },
            "hash": "f89f427922a20c18",
            "authors": [
                "Zecheng Tang",
                "Baibei Ji",
                "Quantong Qiu",
                "Haitian Wang",
                "Xiaobo Liang",
                "Juntao Li",
                "Min Zhang"
            ],
            "affiliations": [
                "LCM Laboratory",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06915.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#alignment",
                    "#long_context"
                ],
                "emoji": "📏",
                "ru": {
                    "title": "Обучение моделей вознаграждения для работы с длинным контекстом",
                    "desc": "Исследователи представили Long-RewardBench — бенчмарк для оценки reward models в условиях длинного контекста, где модели должны учитывать большую историю взаимодействий. Оказалось, что современные reward models плохо справляются с оценкой консистентности ответов относительно длинного контекста. Авторы предложили многоэтапную стратегию обучения, которая позволяет адаптировать любые модели для работы с длинным контекстом без потери качества на коротких промптах. Их 8B модель превосходит базовые 70B модели и сравнима по производительности с проприетарной Gemini 2.5 Pro."
                },
                "en": {
                    "title": "Enhancing Long-Context Consistency in Reward Models",
                    "desc": "This paper addresses the limitations of reward models (RMs) in large language models (LLMs) when dealing with long-context scenarios. It introduces Long-RewardBench, a new benchmark for evaluating RMs specifically designed for long-context consistency and performance. The authors identify that existing RMs struggle with maintaining context-aware preferences in lengthy interactions. To overcome this, they propose a multi-stage training strategy that enhances the robustness of RMs for long contexts while retaining their effectiveness in short contexts, leading to improved performance even against larger models."
                },
                "zh": {
                    "title": "提升长上下文一致性的奖励模型策略",
                    "desc": "本论文提出了一种新的基准和训练策略，用于奖励模型（RM），以提高大型语言模型（LLM）在长上下文中的一致性和性能。当前的奖励模型主要集中在短上下文设置，忽视了长上下文与响应一致性的重要性。我们引入了Long-RewardBench基准，专门用于长上下文的RM评估，并提出了一种多阶段训练策略，以增强模型在长上下文中的鲁棒性。实验结果表明，我们的方法显著提高了长上下文评估的性能，同时保持了短上下文的强大能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03663",
            "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG",
            "url": "https://huggingface.co/papers/2510.03663",
            "abstract": "UniDoc-Bench is a large-scale benchmark for multimodal retrieval-augmented generation, evaluating systems across text, images, and their fusion in real-world document-centric scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmented, focusing on either text or images in isolation or on simplified multimodal setups that fail to capture document-centric multimodal use cases. In this paper, we introduce UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from 70k real-world PDF pages across eight domains. Our pipeline extracts and links evidence from text, tables, and figures, then generates 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries. To ensure reliability, 20% of QA pairs are validated by multiple annotators and expert adjudication. UniDoc-Bench supports apples-to-apples comparison across four paradigms: (1) text-only, (2) image-only, (3) multimodal text-image fusion, and (4) multimodal joint retrieval -- under a unified protocol with standardized candidate pools, prompts, and evaluation metrics. Our experiments show that multimodal text-image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding-based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. Beyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.",
            "score": 1,
            "issue_id": 6344,
            "pub_date": "2025-10-04",
            "pub_date_card": {
                "ru": "4 октября",
                "en": "October 4",
                "zh": "10月4日"
            },
            "hash": "25c006b7c90bf61e",
            "authors": [
                "Xiangyu Peng",
                "Can Qin",
                "Zeyuan Chen",
                "Ran Xu",
                "Caiming Xiong",
                "Chien-Sheng Wu"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03663.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#survey",
                    "#rag",
                    "#multimodal",
                    "#reasoning",
                    "#games"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "Мультимодальный RAG: текст и изображения вместе сильнее",
                    "desc": "Статья представляет UniDoc-Bench — первый крупномасштабный бенчмарк для оценки мультимодальных систем retrieval-augmented generation (RAG), построенный на основе 70 тысяч реальных PDF-страниц из восьми доменов. Бенчмарк включает 1600 пар вопросов-ответов, охватывающих извлечение фактов, сравнение, суммаризацию и логические рассуждения на основе текста, таблиц и изображений. Эксперименты показывают, что мультимодальные RAG-системы, объединяющие текст и изображения, значительно превосходят одномодальные подходы и системы на основе совместных мультимодальных эмбеддингов. Исследование выявляет, когда визуальный контекст дополняет текстовые данные, и предлагает рекомендации для разработки более надёжных MM-RAG систем."
                },
                "en": {
                    "title": "Unlocking the Power of Multimodal Retrieval with UniDoc-Bench",
                    "desc": "UniDoc-Bench is a comprehensive benchmark designed for evaluating multimodal retrieval-augmented generation (MM-RAG) systems that utilize both text and images. It addresses the limitations of existing evaluations by providing a realistic dataset derived from 70,000 real-world PDF pages across various domains. The benchmark includes 1,600 multimodal question-answer pairs that cover a range of tasks such as factual retrieval and logical reasoning, with a focus on ensuring quality through expert validation. Results indicate that systems leveraging multimodal text-image fusion significantly outperform those relying solely on text or images, highlighting the importance of integrating visual context with textual information."
                },
                "zh": {
                    "title": "多模态检索增强生成的基准测试新标准",
                    "desc": "UniDoc-Bench是一个大规模的基准测试，专注于多模态检索增强生成（MM-RAG），评估文本、图像及其融合在真实文档场景中的表现。该基准由70,000个真实世界的PDF页面构成，涵盖八个领域，提供了1,600个多模态问答对，涉及事实检索、比较、摘要和逻辑推理等任务。通过统一的协议和标准化的评估指标，UniDoc-Bench支持四种比较方式：仅文本、仅图像、多模态文本-图像融合和多模态联合检索。实验结果表明，多模态文本-图像融合的RAG系统在性能上优于单模态和联合多模态的检索方法，强调了文本和图像的结合在信息检索中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08547",
            "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized\n  Manipulation",
            "url": "https://huggingface.co/papers/2510.08547",
            "abstract": "A real-to-real 3D data generation framework enhances data efficiency for generalized robotic manipulation by augmenting pointcloud observations without simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training a generalized visuomotor policy via imitation learning. Prior works explore a promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose a real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulator- and rendering-free, thus being efficient and plug-and-play. Specifically, given a single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. A group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation.",
            "score": 0,
            "issue_id": 6344,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "a174dc6a582d8dff",
            "authors": [
                "Xiuwei Xu",
                "Angyuan Ma",
                "Hankun Li",
                "Bingyao Yu",
                "Zheng Zhu",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "GigaAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08547.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#transfer_learning",
                    "#robotics",
                    "#data",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Генерация реальных 3D-данных для обучения роботов без симуляции",
                    "desc": "Статья представляет R2RGen - фреймворк для генерации данных, который напрямую аугментирует пары наблюдений-действий в формате pointcloud для робототехники. Метод работает без симуляторов и рендеринга, решая проблему sim-to-real gap и позволяя создавать разнообразные пространственные конфигурации из одной демонстрации. Используется механизм аннотации для детального разбора сцены и групповая стратегия аугментации для работы с множественными объектами. Подход значительно повышает эффективность использования данных при обучении visuomotor policy через imitation learning, особенно для мобильной манипуляции."
                },
                "en": {
                    "title": "Enhancing Robotic Manipulation with Real-to-Real Data Generation",
                    "desc": "This paper introduces a new framework called R2RGen for generating 3D data that improves the efficiency of training robotic manipulation systems. Unlike previous methods that rely on simulations, R2RGen works directly with real-world data by augmenting pointcloud observations from a single demonstration. It employs a unique annotation mechanism to analyze scenes and trajectories, along with a group-wise augmentation strategy to manage complex object interactions. The framework is designed to be efficient and adaptable, making it suitable for various mobile manipulation tasks while significantly reducing the need for extensive human demonstrations."
                },
                "zh": {
                    "title": "提升机器人操作的数据效率",
                    "desc": "本文提出了一种真实到真实的3D数据生成框架（R2RGen），旨在提高机器人操作的数据信息效率。该框架通过增强点云观察-动作对，直接生成真实世界的数据，而无需使用模拟器或渲染。R2RGen引入了一种注释机制，以便对场景和轨迹进行细致解析，并采用了分组增强策略来处理复杂的多物体组合和多样的任务约束。实验结果表明，R2RGen在数据效率上显著提升，展示了在移动操作中的强大应用潜力。"
                }
            }
        }
    ],
    "link_prev": "2025-10-09.html",
    "link_next": "2025-10-13.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "09.10",
        "en": "10/09",
        "zh": "10月9日"
    },
    "short_date_next": {
        "ru": "13.10",
        "en": "10/13",
        "zh": "10月13日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}