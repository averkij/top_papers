{
    "date": {
        "ru": "1 октября",
        "en": "October 1",
        "zh": "10月1日"
    },
    "time_utc": "2025-10-01 07:12",
    "weekday": 2,
    "issue_id": 6180,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.25541",
            "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play",
            "url": "https://huggingface.co/papers/2509.25541",
            "abstract": "Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the Spy\"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.",
            "score": 53,
            "issue_id": 6175,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "1e4232d439e827c1",
            "authors": [
                "Qinsi Wang",
                "Bo Liu",
                "Tianyi Zhou",
                "Jing Shi",
                "Yueqian Lin",
                "Yiran Chen",
                "Hai Helen Li",
                "Kun Wan",
                "Wentian Zhao"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Duke University",
                "National University of Singapore",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25541.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#cv",
                    "#training",
                    "#rl"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Самообучение VLM через визуальные игры без разметки",
                    "desc": "Vision-Zero — это фреймворк для самосовершенствования vision-language моделей через соревновательные визуальные игры, созданные из произвольных пар изображений. Модели обучаются играя в игры типа «Кто шпион», где они выполняют разные роли и генерируют тренировочные данные автоматически, без участия людей. Алгоритм Iterative Self-Play Policy Optimization чередует самоигру с reinforcement learning, что позволяет избежать плато в производительности и обеспечивает стабильный рост качества. Подход достигает state-of-the-art результатов на задачах reasoning, понимания графиков и визуального анализа, используя только данные без разметки."
                },
                "en": {
                    "title": "Empowering Vision-Language Models through Self-Play Games",
                    "desc": "Vision-Zero is a new framework that improves vision-language models (VLMs) by allowing them to learn from playing competitive visual games without needing human-created datasets. It uses a method called Iterative Self-Play Policy Optimization, which helps models generate their own training data through gameplay, enhancing their reasoning skills. The framework can create games from any image, making it versatile across different domains and tasks. As a result, Vision-Zero achieves top performance in various reasoning tasks while avoiding the high costs of manual data annotation."
                },
                "zh": {
                    "title": "Vision-Zero：无标注自我提升的视觉语言模型框架",
                    "desc": "Vision-Zero是一个领域无关的框架，通过在竞争性视觉游戏中自我提升，增强视觉语言模型（VLM）。该框架采用迭代自我游戏策略优化（Iterative-SPO），使模型能够在没有人工标注的情况下生成训练数据。Vision-Zero能够从任意图像对生成游戏，提升模型在不同领域的推理能力，并展示出强大的泛化能力。最终，Vision-Zero在推理、图表问答和视觉理解任务上达到了最先进的性能，超越了其他基于标注的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25760",
            "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.25760",
            "abstract": "TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.",
            "score": 30,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "58cf56a1a824c556",
            "authors": [
                "Zhepei Wei",
                "Xiao Yang",
                "Kai Sun",
                "Jiaqi Wang",
                "Rulin Shao",
                "Sean Chen",
                "Mohammad Kachuee",
                "Teja Gollapudi",
                "Tony Liao",
                "Nicolas Scheffer",
                "Rakesh Wanga",
                "Anuj Kumar",
                "Yu Meng",
                "Wen-tau Yih",
                "Xin Luna Dong"
            ],
            "affiliations": [
                "FAIR at Meta",
                "Meta Reality Labs",
                "University of Virginia",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25760.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#hallucinations",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Обучение LLM говорить правду через воздержание от ответа",
                    "desc": "В статье представлен TruthRL — фреймворк на основе reinforcement learning для повышения правдивости больших языковых моделей. Ключевая идея заключается в использовании тернарной системы наград, которая различает правильные ответы, галлюцинации и воздержание от ответа. Традиционные методы оптимизации точности часто усиливают галлюцинации, а методы, поощряющие воздержание, становятся чрезмерно консервативными. TruthRL достигает баланса между точностью и способностью признавать неопределённость, снижая галлюцинации на 28.9% и улучшая правдивость на 21.1% на различных бенчмарках."
                },
                "en": {
                    "title": "TruthRL: Balancing Accuracy and Abstention for Truthful AI",
                    "desc": "TruthRL is a novel reinforcement learning framework designed to enhance the truthfulness of large language models (LLMs) by effectively balancing accuracy and the ability to abstain from answering when uncertain. Traditional methods often lead to increased hallucinations or overly conservative responses, compromising the model's truthfulness. TruthRL addresses this by using a ternary reward system that differentiates between correct answers, hallucinations, and abstentions, encouraging models to provide accurate responses while also recognizing when to refrain from answering. Experimental results show that TruthRL significantly reduces hallucinations and improves overall truthfulness across various benchmarks and model architectures."
                },
                "zh": {
                    "title": "TruthRL：提升语言模型真实性的强化学习框架",
                    "desc": "TruthRL是一种强化学习框架，旨在提高大型语言模型的真实性。它通过平衡准确性和放弃来显著减少幻觉现象，并在多个基准测试中提升性能。该框架使用简单有效的三元奖励机制，鼓励模型在不确定时选择放弃，从而避免错误回答。实验结果表明，TruthRL相比传统的强化学习方法，减少了28.9%的幻觉现象，并提高了21.1%的真实性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26536",
            "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
            "url": "https://huggingface.co/papers/2509.26536",
            "abstract": "OceanGym is a benchmark for underwater embodied agents using Multi-modal Large Language Models to address challenges in perception, planning, and adaptability in harsh ocean environments.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.",
            "score": 22,
            "issue_id": 6176,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "16228ef736074906",
            "authors": [
                "Yida Xue",
                "Mingjun Mao",
                "Xiangyuan Ru",
                "Yuqi Zhu",
                "Baochang Ren",
                "Shuofei Qiao",
                "Mengru Wang",
                "Shumin Deng",
                "Xinyu An",
                "Ningyu Zhang",
                "Ying Chen",
                "Huajun Chen"
            ],
            "affiliations": [
                "National University of Singapore",
                "State Key Laboratory of Ocean Sensing, Zhejiang University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26536.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#transfer_learning",
                    "#agents",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "OceanGym: Тестовая площадка для AI-агентов в неизведанных глубинах океана",
                    "desc": "В статье представлен OceanGym — первый комплексный бенчмарк для подводных embodied-агентов, работающих в экстремальных условиях океана. Платформа включает восемь реалистичных задач и использует мультимодальные LLM для интеграции восприятия, памяти и принятия решений. Агенты должны обрабатывать оптические и сонарные данные, автономно исследовать сложную среду и выполнять долгосрочные цели в условиях низкой видимости и динамичных течений. Эксперименты показали существенный разрыв между современными MLLM-агентами и экспертами-людьми, подчеркивая сложность задач восприятия, планирования и адаптации в подводной среде."
                },
                "en": {
                    "title": "OceanGym: Advancing AI for Underwater Exploration Challenges",
                    "desc": "OceanGym is a new benchmark designed for testing underwater embodied agents using Multi-modal Large Language Models (MLLMs). It addresses the unique challenges of underwater environments, such as low visibility and dynamic currents, which complicate perception and decision-making. The benchmark includes eight realistic tasks that require agents to process both optical and sonar data while navigating complex scenarios. By highlighting the performance gaps between advanced AI agents and human experts, OceanGym aims to improve the adaptability and planning capabilities of AI in ocean exploration."
                },
                "zh": {
                    "title": "OceanGym：水下智能体的新基准挑战",
                    "desc": "OceanGym是一个针对水下具身智能体的基准测试，旨在利用多模态大语言模型（MLLMs）解决在恶劣海洋环境中感知、规划和适应性的问题。与陆地或空中环境不同，水下环境面临极端的感知和决策挑战，如低能见度和动态海流，使得有效的智能体部署变得异常困难。OceanGym包含八个现实任务领域和一个统一的智能体框架，要求智能体理解光学和声纳数据，能够自主探索复杂环境，并在这些恶劣条件下完成长期目标。通过广泛的实验，发现当前最先进的MLLM驱动智能体与人类专家之间存在显著差距，突显了在水下环境中感知、规划和适应性的持续困难。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25182",
            "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder",
            "url": "https://huggingface.co/papers/2509.25182",
            "abstract": "DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.",
            "score": 19,
            "issue_id": 6175,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "7de7ea8b15ae7048",
            "authors": [
                "Junyu Chen",
                "Wenkun He",
                "Yuchao Gu",
                "Yuyang Zhao",
                "Jincheng Yu",
                "Junsong Chen",
                "Dongyun Zou",
                "Yujun Lin",
                "Zhekai Zhang",
                "Muyang Li",
                "Haocheng Xi",
                "Ligeng Zhu",
                "Enze Xie",
                "Song Han",
                "Han Cai"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25182.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Ускорение генерации видео через глубокое сжатие латентного пространства",
                    "desc": "DC-VideoGen — это фреймворк для ускорения генерации видео, который адаптирует предобученные диффузионные модели к глубоко сжатому латентному пространству. Ключевые инновации включают Deep Compression Video Autoencoder со сжатием 32-64x по пространству и 4x по времени, а также метод AE-Adapt-V для быстрой адаптации моделей. Адаптация модели Wan-2.1-14B требует всего 10 GPU-дней на NVIDIA H100, что обеспечивает ускорение инференса в 14.8 раз без потери качества. Технология позволяет генерировать видео разрешением 2160x3840 на одной GPU, значительно снижая вычислительные требования для высококачественной генерации видео."
                },
                "en": {
                    "title": "Accelerating Video Generation with Deep Compression",
                    "desc": "DC-VideoGen is a framework designed to speed up video generation by modifying existing diffusion models to work in a compressed latent space. This approach allows for significant reductions in inference time while still producing high-quality, high-resolution videos. The framework utilizes a Deep Compression Video Autoencoder that efficiently compresses video data and an adaptation strategy called AE-Adapt-V for seamless integration of pre-trained models. As a result, DC-VideoGen can generate videos much faster, achieving up to 14.8 times lower latency compared to traditional methods."
                },
                "zh": {
                    "title": "高效视频生成的新突破",
                    "desc": "DC-VideoGen 是一个加速视频生成的框架，它通过将预训练的扩散模型适应到深度压缩的潜在空间来减少推理延迟，从而实现高分辨率视频生成。该框架可以应用于任何预训练的视频扩散模型，通过轻量级微调提高效率。它的两个关键创新包括：一种具有新颖块因果时间设计的深度压缩视频自编码器，能够在保持重建质量的同时实现32倍/64倍的空间压缩和4倍的时间压缩；以及AE-Adapt-V，一种稳健的适应策略，能够快速稳定地将预训练模型转移到新的潜在空间。使用DC-VideoGen对预训练的Wan-2.1-14B模型进行适应只需10天的GPU时间，且加速后的模型在推理延迟上比基础模型低14.8倍，且能够在单个GPU上生成2160x3840的视频。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26625",
            "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
            "url": "https://huggingface.co/papers/2509.26625",
            "abstract": "LLMs develop visual priors during language pre-training, which can be leveraged for vision tasks with minimal additional data, and these priors are composed of separable perception and reasoning components.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.",
            "score": 15,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "4ecd034d7f6a8060",
            "authors": [
                "Junlin Han",
                "Shengbang Tong",
                "David Fan",
                "Yufan Ren",
                "Koustuv Sinha",
                "Philip Torr",
                "Filippos Kokkinos"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26625.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#alignment",
                    "#dataset",
                    "#transfer_learning"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Визуальные приоры из текста: как LLM учатся видеть без изображений",
                    "desc": "Исследование показывает, что большие языковые модели (LLM) неожиданно развивают визуальные представления во время обучения только на текстовых данных. Эти визуальные приоры состоят из двух отдельных компонентов: перцептивного и рассуждающего, которые имеют разные источники происхождения и закономерности масштабирования. Способность к визуальным рассуждениям формируется преимущественно на данных с кодом, математикой и научными текстами, в то время как перцептивные способности возникают из более широкого корпуса и зависят от vision encoder. На основе более 100 контролируемых экспериментов авторы предлагают рецепт предобучения мультимодальных LLM и вводят новый бенчмарк MLE-Bench для оценки визуальных способностей."
                },
                "en": {
                    "title": "Unlocking Visual Understanding in Language Models",
                    "desc": "This paper explores how Large Language Models (LLMs) can develop visual understanding during their training on text data alone. It reveals that these models create visual priors, which are essential for performing vision tasks with minimal additional data. The study identifies two main components of these priors: perception and reasoning, each with distinct characteristics and scaling behaviors. By analyzing extensive experiments, the authors propose a method for enhancing LLMs with visual capabilities, setting a foundation for future multimodal AI systems."
                },
                "zh": {
                    "title": "从语言预训练中培养视觉先验的全新方法",
                    "desc": "大型语言模型（LLMs）在语言预训练过程中意外地发展出丰富的视觉先验。这些视觉先验使得在视觉任务中能够以相对较少的多模态数据解锁潜在的视觉能力。研究表明，视觉先验由可分离的感知和推理组件组成，且这两者在规模和来源上具有独特的趋势。通过系统分析，我们提出了一种以数据为中心的预训练方法，旨在培养视觉感知能力，从而推动下一代多模态LLMs的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25758",
            "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
            "url": "https://huggingface.co/papers/2509.25758",
            "abstract": "Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable capabilities of modern large reasoning models are largely unlocked through post-training techniques such as supervised fine-tuning and reinforcement learning. However, the architectural mechanisms behind such improvements remain largely opaque. In this work, we use circuit analysis to demonstrate that post-training for complex reasoning sparks the emergence of novel, functionally specialized attention heads. These heads collectively support structured reasoning and computation. Our comparative analysis across Qwen families and DeepSeek-distilled model reveals that these emergent heads evolve differently under different training regimes. Distillation and SFT foster a cumulative addition of stable reasoning heads. In contrast, group relative policy optimization operates in a dynamic search mode: relatively few attention heads are iteratively activated, evaluated, and pruned, with their survival closely tracking fluctuations in the task reward signal. Furthermore, we find that controllable think on/off models do not possess dedicated thinking heads. Instead, turning off explicit reasoning triggers a broader-but less efficient-set of compensatory heads. Through ablation and qualitative analyses, we connect these circuit-level dynamics to a crucial performance trade-off: strengthened heads enable sophisticated problem-solving strategies for difficult problems but can also introduce over-thinking failure modes, such as calculation errors or logical loops on simpler tasks. These findings connect circuit-level dynamics to macro-level performance, identifying an inherent tension where complex reasoning comes at the cost of elementary computations. More broadly, our work points to future directions for training policy design, emphasizing the need to balance the development of effective reasoning strategies with the assurance of reliable, flawless execution.",
            "score": 15,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "f7f61c1e3b1bdf7d",
            "authors": [
                "Yein Park",
                "Minbyul Jeong",
                "Jaewoo Kang"
            ],
            "affiliations": [
                "AIGEN Sciences",
                "Korea University",
                "Upstage AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25758.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#optimization",
                    "#reasoning",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Специализированные головы внимания: ключ к рассуждениям LLM",
                    "desc": "Исследование показывает, что post-training техники, такие как supervised fine-tuning и reinforcement learning, приводят к появлению специализированных attention heads, которые поддерживают структурированное рассуждение в больших языковых моделях. Разные методы обучения влияют на эволюцию этих голов по-разному: distillation и SFT создают стабильные reasoning heads, в то время как group relative policy optimization работает в режиме динамического поиска с итеративной активацией и отбором голов. Модели с возможностью включения и выключения явного рассуждения не имеют выделенных thinking heads, а вместо этого активируют более широкий, но менее эффективный набор компенсаторных механизмов. Анализ выявляет важный компромисс: усиленные головы внимания улучшают решение сложных задач, но могут приводить к ошибкам на простых задачах из-за избыточного рассуждения."
                },
                "en": {
                    "title": "Unlocking Reasoning: The Power of Specialized Attention Heads",
                    "desc": "This paper explores how post-training techniques like supervised fine-tuning and reinforcement learning enhance the performance of large reasoning models. It reveals that these techniques lead to the emergence of specialized attention heads that facilitate structured reasoning. The study shows that different training methods influence the evolution and effectiveness of these attention heads, with some fostering stable reasoning capabilities while others operate in a dynamic, adaptive manner. Ultimately, the research highlights a trade-off between advanced reasoning abilities and the risk of errors in simpler tasks, suggesting a need for careful design in training policies."
                },
                "zh": {
                    "title": "后训练技术助力结构化推理的演变",
                    "desc": "本研究探讨了后训练技术如何促进专门化注意力头的出现，这些注意力头支持结构化推理。通过电路分析，我们发现不同的训练方式会影响这些注意力头的演变和性能。特别是，蒸馏和监督微调促进了稳定推理头的累积，而相对策略优化则在动态搜索模式下工作。我们的研究揭示了复杂推理与基本计算之间的内在张力，强调了在训练策略设计中平衡有效推理与可靠执行的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25154",
            "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments",
            "url": "https://huggingface.co/papers/2509.25154",
            "abstract": "J-Detector, a neural detector with linguistic and LLM-enhanced features, effectively identifies LLM-generated judgments based on scores and candidate content, addressing biases and vulnerabilities in sensitive scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content -- an aspect crucial for effective judgment detection. Inspired by this, we introduce J-Detector, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of J-Detector and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios.",
            "score": 15,
            "issue_id": 6176,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "27f20852a9155cd5",
            "authors": [
                "Dawei Li",
                "Zhen Tan",
                "Chengshuai Zhao",
                "Bohan Jiang",
                "Baixiang Huang",
                "Pingchuan Ma",
                "Abdullah Alnaibari",
                "Kai Shu",
                "Huan Liu"
            ],
            "affiliations": [
                "Arizona State University",
                "Emory University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25154.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#data",
                    "#ethics",
                    "#dataset",
                    "#interpretability",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Детектор оценок от LLM: обнаружение искусственных суждений по баллам",
                    "desc": "В статье формализуется задача обнаружения суждений, сгенерированных большими языковыми моделями, что критически важно для чувствительных сценариев вроде академического рецензирования. Существующие методы детекции LLM-текстов плохо справляются, так как не учитывают взаимосвязь между оценочными баллами и содержанием кандидатов. Предложен J-Detector — лёгкий и прозрачный нейросетевой детектор с лингвистическими и LLM-усиленными признаками, который связывает предвзятости LLM-судей со свойствами оцениваемого контента. Эксперименты демонстрируют эффективность подхода и его способность количественно оценивать предвзятости в LLM-судьях в реальных сценариях."
                },
                "en": {
                    "title": "J-Detector: Unmasking Biases in LLM Judgments",
                    "desc": "The paper introduces J-Detector, a neural network designed to identify judgments generated by Large Language Models (LLMs) based on their scores and the content of candidates. It highlights the challenges posed by biases and vulnerabilities in LLM-generated judgments, particularly in sensitive contexts like academic peer review. The authors emphasize that traditional LLM text detection methods are inadequate for this task, as they do not consider the relationship between judgment scores and candidate content. J-Detector addresses this gap by incorporating linguistic features and LLM-enhanced attributes, allowing for better detection and analysis of biases in LLM-generated judgments."
                },
                "zh": {
                    "title": "J-Detector：精准识别LLM生成判断的利器",
                    "desc": "J-Detector是一种神经检测器，结合了语言学和大型语言模型（LLM）增强特性，能够有效识别基于LLM生成的判断。该研究提出了判断检测的任务，重点关注在缺乏文本反馈的情况下，仅依赖判断分数和候选内容进行检测。通过系统分析，发现现有的LLM生成文本检测方法在捕捉判断分数与候选内容之间的互动方面表现不佳。J-Detector通过提取语言学特征和LLM增强特征，成功地将LLM评审者的偏见与候选者的属性联系起来，从而实现准确检测。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24002",
            "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\n  Use",
            "url": "https://huggingface.co/papers/2509.24002",
            "abstract": "MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of 127 high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only 52.56\\% pass@1 and 33.86\\% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below 30\\% pass@1 and 15\\% pass^4. On average, LLMs require 16.2 execution turns and 17.4 tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
            "score": 14,
            "issue_id": 6176,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 сентября",
                "en": "September 28",
                "zh": "9月28日"
            },
            "hash": "9a5257700f81ad41",
            "authors": [
                "Zijian Wu",
                "Xiangyan Liu",
                "Xinyuan Zhang",
                "Lingjun Chen",
                "Fanqing Meng",
                "Lingxiao Du",
                "Yiran Zhao",
                "Fanshi Zhang",
                "Yaoqi Ye",
                "Jiawei Wang",
                "Zirui Wang",
                "Jinjie Ni",
                "Yufan Yang",
                "Arvin Xu",
                "Michael Qizhe Shieh"
            ],
            "affiliations": [
                "EvalSys",
                "Fudan University",
                "LobeHub",
                "National University of Singapore",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24002.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#agents",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "MCPMark: бенчмарк, который показал слабость LLM в реальных рабочих сценариях",
                    "desc": "MCPMark — это новый комплексный бенчмарк для оценки использования MCP (Model Context Protocol) в реальных рабочих процессах. Он включает 127 высококачественных задач, требующих разнообразных CRUD-операций и глубокого взаимодействия с окружением, что значительно сложнее предыдущих бенчмарков. Результаты тестирования показали, что даже лучшие современные LLM справляются плохо: gpt-5-medium достигает лишь 52.56% успешных решений, а другие модели вроде claude-sonnet-4 и o3 показывают менее 30%. В среднем модели требуют 16.2 итераций и 17.4 вызовов инструментов на задачу, что подчеркивает сложность и реалистичность бенчмарка."
                },
                "en": {
                    "title": "MCPMark: Elevating LLMs to Real-World Challenges",
                    "desc": "MCPMark is a new benchmark designed to evaluate the performance of Large Language Models (LLMs) in real-world workflows that require complex interactions with their environment. Unlike previous benchmarks that focused on simpler tasks, MCPMark includes 127 diverse tasks that involve a variety of create, read, update, and delete (CRUD) operations. The benchmark aims to standardize how LLMs interact with external systems, paving the way for the development of more capable general agents. Evaluation results show that even the best LLMs struggle with these tasks, indicating a significant gap in their ability to handle realistic scenarios."
                },
                "zh": {
                    "title": "MCPMark：评估真实工作流程中的大语言模型",
                    "desc": "MCPMark是一个全面的基准测试，用于评估大语言模型（LLM）在真实工作流程中对多种任务的处理能力。这些任务要求与环境进行更丰富的交互，显示出当前的LLM在这些任务上的表现较差。MCPMark包含127个高质量任务，由领域专家和AI代理共同创建，旨在更真实和全面地评估MCP的使用。通过对先进LLM的评估，结果表明即使是表现最好的模型，其通过率也远低于预期，突显了MCPMark的挑战性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26490",
            "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
            "url": "https://huggingface.co/papers/2509.26490",
            "abstract": "VitaBench is a benchmark for evaluating LLM-based agents in complex, real-world interactive tasks using a diverse set of tools and scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/",
            "score": 13,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "3ee3a7d4e39bff94",
            "authors": [
                "Wei He",
                "Yueqing Sun",
                "Hongyan Hao",
                "Xueyuan Hao",
                "Zhikang Xia",
                "Qi Gu",
                "Chengcheng Han",
                "Dengchang Zhao",
                "Hui Su",
                "Kefeng Zhang",
                "Man Gao",
                "Xi Su",
                "Xiaodong Cai",
                "Xunliang Cai",
                "Yu Yang",
                "Yunke Zhao"
            ],
            "affiliations": [
                "Meituan LongCat Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26490.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#benchmark",
                    "#games",
                    "#survey"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "VitaBench: жизненный экзамен для AI-агентов в реальных сценариях",
                    "desc": "VitaBench — это новый бенчмарк для оценки LLM-агентов в сложных интерактивных задачах, приближенных к реальной жизни. Он включает 66 инструментов и 400 задач из сфер доставки еды, ресторанного обслуживания и онлайн-туризма, требующих рассуждений в пространстве и времени, работы со сложными наборами инструментов и многоходовых диалогов с пользователями. Для оценки предложен метод на основе рубрик со скользящим окном, учитывающий множество возможных путей решения. Даже самые продвинутые модели достигают лишь 30% успеха в кросс-сценарных задачах, что показывает значительный разрыв между текущими возможностями AI-агентов и требованиями реального мира."
                },
                "en": {
                    "title": "VitaBench: Advancing AI Agents in Real-World Complexity",
                    "desc": "VitaBench is a new benchmark designed to test large language model (LLM)-based agents in complex, real-world tasks that require interaction with various tools. It addresses the limitations of existing benchmarks by providing a diverse set of scenarios that reflect daily applications, such as food delivery and travel services. The benchmark includes 66 tools and offers 100 cross-scenario tasks, challenging agents to manage dynamic user interactions and reason through complex instructions. The evaluation shows that even advanced models struggle, achieving only a 30% success rate on cross-scenario tasks, highlighting the need for further development in AI agents for practical use."
                },
                "zh": {
                    "title": "VitaBench：评估复杂互动任务的基准测试",
                    "desc": "VitaBench是一个用于评估基于大型语言模型（LLM）代理在复杂现实互动任务中的基准测试。它解决了现有基准无法捕捉代理处理大量信息、利用多样资源和管理动态用户交互的复杂性的问题。VitaBench提供了66种工具和多种场景，设计了100个跨场景任务和300个单场景任务，要求代理在多轮对话中推理时间和空间维度，使用复杂工具集，并主动澄清模糊指令。我们的评估显示，即使是最先进的模型在跨场景任务上的成功率也仅为30%，这表明VitaBench将推动AI代理在实际应用中的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26488",
            "title": "dParallel: Learnable Parallel Decoding for dLLMs",
            "url": "https://huggingface.co/papers/2509.26488",
            "abstract": "dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel",
            "score": 13,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "50b8e2e379343971",
            "authors": [
                "Zigeng Chen",
                "Gongfan Fang",
                "Xinyin Ma",
                "Ruonan Yu",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26488.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#benchmark",
                    "#diffusion",
                    "#open_source",
                    "#training"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Параллельное декодирование диффузионных LLM с 10-кратным ускорением",
                    "desc": "Статья представляет dParallel — метод для ускорения параллельного декодирования в диффузионных языковых моделях (dLLMs). Авторы обнаружили, что основное узкое место при параллельном декодировании связано с последовательной сходимостью уверенности для замаскированных токенов. Предложенная техника certainty-forcing distillation обучает модель быстрее достигать высокой уверенности при предсказании токенов параллельно. В результате метод сокращает количество шагов декодирования с 256 до 24-30 на бенчмарках GSM8K и MBPP, обеспечивая ускорение в 8-10 раз без потери качества."
                },
                "en": {
                    "title": "Unlocking Fast Parallel Decoding in Diffusion Models",
                    "desc": "dParallel is a novel method designed to improve the efficiency of parallel decoding in diffusion large language models (dLLMs). It addresses the challenge of sequential certainty convergence for masked tokens, which has limited the speed of parallel decoding. By introducing certainty-forcing distillation, dParallel trains the model to quickly achieve high certainty on masked tokens while maintaining its original sampling paths. Experimental results show that dParallel significantly reduces decoding steps, achieving up to 10.5 times faster inference without sacrificing performance."
                },
                "zh": {
                    "title": "dParallel：加速扩散模型的并行解码",
                    "desc": "dParallel是一种增强扩散大语言模型（dLLMs）并行解码的方法，显著减少了解码步骤而不影响性能。该方法利用了dLLMs的并行性，解决了现有模型在解码时需要接近令牌长度的步骤的问题。通过引入确定性强制蒸馏的训练策略，dParallel能够更快地并行处理被遮蔽的令牌。实验结果表明，dParallel在多个基准测试中显著减少了解码步骤，同时保持了模型的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26231",
            "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance",
            "url": "https://huggingface.co/papers/2509.26231",
            "abstract": "Implicit Multimodal Guidance (IMG) enhances multimodal alignment between diffusion-generated images and prompts without additional data or editing, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Ensuring precise multimodal alignment between diffusion-generated images and input prompts has been a long-standing challenge. Earlier works finetune diffusion weight using high-quality preference data, which tends to be limited and difficult to scale up. Recent editing-based methods further refine local regions of generated images but may compromise overall image quality. In this work, we propose Implicit Multimodal Guidance (IMG), a novel re-generation-based multimodal alignment framework that requires no extra data or editing operations. Specifically, given a generated image and its prompt, IMG a) utilizes a multimodal large language model (MLLM) to identify misalignments; b) introduces an Implicit Aligner that manipulates diffusion conditioning features to reduce misalignments and enable re-generation; and c) formulates the re-alignment goal into a trainable objective, namely Iteratively Updated Preference Objective. Extensive qualitative and quantitative evaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing alignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter, seamlessly enhancing prior finetuning-based alignment methods. Our code will be available at https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment.",
            "score": 12,
            "issue_id": 6176,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "2e5347092392a0bc",
            "authors": [
                "Jiayi Guo",
                "Chuanhao Yan",
                "Xingqian Xu",
                "Yulin Wang",
                "Kai Wang",
                "Gao Huang",
                "Humphrey Shi"
            ],
            "affiliations": [
                "SHI Labs @ Georgia Tech",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26231.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#alignment",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Неявное управление для точного соответствия изображений и текста",
                    "desc": "Статья представляет метод Implicit Multimodal Guidance (IMG) для улучшения согласованности между сгенерированными диффузионными моделями изображениями и текстовыми промптами. Метод использует multimodal LLM для определения несоответствий, затем манипулирует conditioning-признаками диффузионной модели для их устранения через регенерацию изображения. В отличие от существующих подходов, IMG не требует дополнительных данных для дообучения или редактирования отдельных областей изображения. Эксперименты показали превосходство метода над существующими решениями на моделях SDXL и FLUX, при этом IMG работает как гибкий plug-and-play адаптер для других методов выравнивания."
                },
                "en": {
                    "title": "Enhancing Image-Prompt Alignment with Implicit Multimodal Guidance",
                    "desc": "Implicit Multimodal Guidance (IMG) is a new framework designed to improve the alignment between images generated by diffusion models and their corresponding prompts. Unlike previous methods that rely on additional data or editing, IMG uses a multimodal large language model to identify and correct misalignments directly. It introduces an Implicit Aligner that adjusts the features used in the diffusion process to enhance image quality during re-generation. The framework not only surpasses existing alignment techniques but also integrates easily with previous methods, making it a versatile tool for multimodal tasks."
                },
                "zh": {
                    "title": "隐式多模态引导：无数据对齐的创新",
                    "desc": "隐式多模态引导（IMG）是一种新颖的多模态对齐框架，旨在提高扩散生成图像与输入提示之间的对齐精度，而无需额外的数据或编辑操作。该方法利用多模态大语言模型（MLLM）识别生成图像与提示之间的错位，并通过隐式对齐器调整扩散条件特征以减少错位。IMG将重新对齐目标公式化为可训练的目标，称为迭代更新偏好目标。实验结果表明，IMG在多个数据集上优于现有的对齐方法，并且可以作为灵活的插件，增强之前基于微调的对齐方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23610",
            "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
            "url": "https://huggingface.co/papers/2509.23610",
            "abstract": "Dolphin, an efficient AVSS method, uses a dual-path lightweight video encoder and a lightweight encoder-decoder separator with global-local attention blocks to achieve high separation quality and significant computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-visual speech separation (AVSS) methods leverage visual cues to extract target speech and have demonstrated strong separation quality in noisy acoustic environments. However, these methods usually involve a large number of parameters and require high computational cost, which is unacceptable in many applications where speech separation serves as only a preprocessing step for further speech processing. To address this issue, we propose an efficient AVSS method, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a dual-path lightweight video encoder that transforms lip-motion into discrete audio-aligned semantic tokens. For audio separation, we construct a lightweight encoder-decoder separator, in which each layer incorporates a global-local attention (GLA) block to efficiently capture multi-scale dependencies. Experiments on three benchmark datasets showed that Dolphin not only surpassed the current state-of-the-art (SOTA) model in separation quality but also achieved remarkable improvements in efficiency: over 50% fewer parameters, more than 2.4x reduction in MACs, and over 6x faster GPU inference speed. These results indicate that Dolphin offers a practical and deployable solution for high-performance AVSS in real-world scenarios. Our code and demo page are publicly available at http://cslikai.cn/Dolphin/.",
            "score": 11,
            "issue_id": 6175,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 сентября",
                "en": "September 28",
                "zh": "9月28日"
            },
            "hash": "d324759166979416",
            "authors": [
                "Kai Li",
                "Kejun Gao",
                "Xiaolin Hu"
            ],
            "affiliations": [
                "Chinese Institute for Brain Research (CIBR), Beijing 100010, China",
                "Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing 100084, China",
                "Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing 100084, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23610.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#audio",
                    "#inference"
                ],
                "emoji": "🐬",
                "ru": {
                    "title": "Dolphin: Быстрая и эффективная сепарация речи с помощью визуальных подсказок",
                    "desc": "Dolphin - это эффективный метод аудио-визуальной сепарации речи (AVSS), который использует визуальные подсказки для извлечения целевой речи из зашумленной акустической среды. Метод включает двухпутевой легковесный видео-энкодер DP-LipCoder, преобразующий движения губ в дискретные аудио-выровненные семантические токены, и легковесный encoder-decoder сепаратор с блоками глобально-локального внимания для эффективного захвата многомасштабных зависимостей. Dolphin превосходит современные SOTA модели по качеству сепарации, при этом имея на 50% меньше параметров, в 2.4 раза меньше вычислительных операций и в 6 раз более высокую скорость inference на GPU. Это делает метод практичным решением для реального применения, где сепарация речи является лишь этапом предобработки для дальнейшей обработки аудио."
                },
                "en": {
                    "title": "Dolphin: Efficient AVSS with Dual-Path Encoding and Global-Local Attention",
                    "desc": "Dolphin is a novel audio-visual speech separation (AVSS) method that enhances speech extraction by utilizing visual cues from lip movements. It features a dual-path lightweight video encoder called DP-LipCoder, which converts lip motion into audio-aligned semantic tokens, improving the quality of speech separation. Additionally, Dolphin employs a lightweight encoder-decoder architecture with global-local attention blocks to efficiently manage multi-scale dependencies while significantly reducing computational costs. Experimental results demonstrate that Dolphin outperforms existing state-of-the-art models in both separation quality and efficiency, making it suitable for practical applications in noisy environments."
                },
                "zh": {
                    "title": "Dolphin：高效的音视频语音分离新方法",
                    "desc": "Dolphin是一种高效的音视频语音分离（AVSS）方法，采用双路径轻量级视频编码器和轻量级编码-解码分离器，结合全局-局部注意力模块，以实现高质量的分离效果和显著的计算效率。该方法通过DP-LipCoder提取视觉特征，将唇部运动转化为与音频对齐的语义标记。实验结果表明，Dolphin在分离质量上超越了当前的最先进模型，同时在参数数量上减少了50%以上，MACs减少了2.4倍，GPU推理速度提高了6倍以上。Dolphin为实际应用中的高性能音视频语音分离提供了一个可行的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26391",
            "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2509.26391",
            "abstract": "MotionRAG enhances video generation by integrating motion priors from reference videos using a retrieval-augmented framework, improving motion realism with negligible computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.",
            "score": 10,
            "issue_id": 6176,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "80a4fde692ceb5ab",
            "authors": [
                "Chenhui Zhu",
                "Yilu Wu",
                "Shuai Wang",
                "Gangshan Wu",
                "Limin Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "State Key Laboratory for Novel Software Technology, Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26391.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#video",
                    "#rag",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Улучшение реалистичности движения в видео через retrieval motion-паттернов",
                    "desc": "Статья представляет MotionRAG — фреймворк для генерации видео, который использует retrieval-augmented подход для улучшения реалистичности движений. Система извлекает motion-паттерны из релевантных референсных видео с помощью специальных энкодеров и resamplers, затем адаптирует их через causal transformer архитектуру. Motion-признаки интегрируются в предобученные диффузионные модели через attention-based адаптер с минимальными вычислительными затратами. Модульная архитектура позволяет zero-shot обобщение на новые домены простым обновлением базы данных без дообучения компонентов."
                },
                "en": {
                    "title": "Enhancing Video Realism with MotionRAG: Smart Retrieval for Realistic Motion Dynamics",
                    "desc": "MotionRAG is a novel framework that improves video generation by incorporating motion priors from reference videos. It utilizes a retrieval-augmented approach to enhance the realism of motion in generated videos while maintaining low computational costs. The framework employs Context-Aware Motion Adaptation (CAMA) to adapt high-level motion features extracted from relevant videos, using a causal transformer for in-context learning. This method allows for significant improvements in motion realism across various domains and enables zero-shot generalization by simply updating the retrieval database."
                },
                "zh": {
                    "title": "MotionRAG：提升视频生成的运动真实感",
                    "desc": "MotionRAG是一种增强视频生成的框架，通过从参考视频中整合运动先验来提高运动的真实感。该方法利用上下文感知运动适应（CAMA）技术，提取高层次的运动特征，并通过因果变换器架构进行运动适应。其创新之处在于使用检索基础的管道和注意力机制，将转移的运动特征无缝集成到预训练的视频扩散模型中。实验结果表明，MotionRAG在多个领域和基础模型上都显著提高了生成视频的运动真实感，同时在推理过程中几乎没有计算开销。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25911",
            "title": "Mem-α: Learning Memory Construction via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.25911",
            "abstract": "Mem-alpha, a reinforcement learning framework, enhances memory management in large language models through interaction and feedback, improving performance and generalization in long-term information understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha.",
            "score": 9,
            "issue_id": 6179,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "304daec7f10d72ea",
            "authors": [
                "Yu Wang",
                "Ryuichi Takanobu",
                "Zhiqi Liang",
                "Yuzhen Mao",
                "Yuanzhe Hu",
                "Julian McAuley",
                "Xiaojian Wu"
            ],
            "affiliations": [
                "Anuttacon",
                "Stanford University",
                "University of California San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25911.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#agents",
                    "#long_context",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обучение LLM управлять памятью через reinforcement learning",
                    "desc": "Статья представляет Mem-alpha — фреймворк на основе reinforcement learning для улучшения управления памятью в LLM-агентах. Существующие агенты с внешней памятью используют предопределённые инструкции, но не умеют самостоятельно решать, какую информацию сохранять и как структурировать память. Mem-alpha обучает агентов эффективно управлять сложными системами памяти через взаимодействие и feedback, используя reward signal от точности ответов на вопросы. Агенты, обученные на последовательностях до 30k токенов, демонстрируют впечатляющую генерализацию на последовательности длиной более 400k токенов."
                },
                "en": {
                    "title": "Empowering Memory Management in Language Models with Mem-alpha",
                    "desc": "Mem-alpha is a reinforcement learning framework designed to improve memory management in large language models (LLMs). It addresses the limitations of current memory-augmented agents by allowing them to learn how to store, structure, and update information through interaction and feedback. By training on a diverse dataset of multi-turn interactions, Mem-alpha optimizes memory construction based on the accuracy of question-answering tasks. The framework demonstrates significant performance gains, enabling agents to generalize effectively to much longer sequences than they were trained on."
                },
                "zh": {
                    "title": "Mem-alpha：提升记忆管理的强化学习框架",
                    "desc": "Mem-alpha 是一个强化学习框架，旨在通过交互和反馈来增强大语言模型的记忆管理能力，从而提高其在长期信息理解方面的表现和泛化能力。当前的记忆增强代理通常依赖于预定义的指令和工具来更新记忆，但语言模型在决定存储哪些信息、如何构建信息以及何时更新时常常存在不足。Mem-alpha 通过训练代理有效管理复杂的记忆系统，使用多轮交互模式的专门训练数据集，并通过下游问答准确性作为奖励信号来优化记忆构建。实验证明，Mem-alpha 在现有记忆增强代理基准上取得了显著的改进，且在处理超过训练长度的序列时表现出色，显示出其强大的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22646",
            "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
            "url": "https://huggingface.co/papers/2509.22646",
            "abstract": "DeeptraceReward is a benchmark dataset that annotates human-perceived deepfake traces in videos, used to train multimodal language models for detecting AI-generated videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.",
            "score": 9,
            "issue_id": 6178,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "098fcc1c49c189c8",
            "authors": [
                "Xingyu Fu",
                "Siyi Liu",
                "Yinuo Xu",
                "Pan Lu",
                "Guangqiuse Hu",
                "Tianbo Yang",
                "Taran Anantasagar",
                "Christopher Shen",
                "Yikai Mao",
                "Yuanzhe Liu",
                "Keyush Shah",
                "Chung Un Lee",
                "Yejin Choi",
                "James Zou",
                "Dan Roth",
                "Chris Callison-Burch"
            ],
            "affiliations": [
                "Princeton University",
                "Stanford University",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22646.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#interpretability"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Учим модели находить следы дипфейков глазами человека",
                    "desc": "Исследователи создали датасет DeeptraceReward с 4.3 тысячами детальных аннотаций искусственных артефактов в сгенерированных видео, которые замечают люди. Каждая аннотация включает текстовое объяснение, пространственную локализацию через bounding box и временные метки начала и конца артефакта. На основе датасета обучили multimodal language models как reward модели, которые имитируют человеческие суждения о дипфейках. Модель на 7B параметров превзошла GPT-5 на 34.7% в задачах идентификации, локализации и объяснения поддельных следов в видео."
                },
                "en": {
                    "title": "Detecting Deepfakes: Training Models with Human Insights",
                    "desc": "DeeptraceReward is a new dataset designed to help train models that can detect deepfake videos by focusing on human-perceived traces. It includes 4.3K annotations from 3.3K high-quality generated videos, detailing where and when viewers notice signs of manipulation. The dataset categorizes these traces into nine major types, allowing models to learn how to identify and explain deepfake characteristics. By using this dataset, researchers can improve multimodal language models to better mimic human detection and reasoning about AI-generated content."
                },
                "zh": {
                    "title": "揭示深度伪造痕迹，提升视频生成可信度",
                    "desc": "DeeptraceReward是一个基准数据集，专注于标注人类感知的深度伪造视频痕迹，旨在训练多模态语言模型以检测AI生成的视频。该数据集包含4300个详细注释，涵盖3300个高质量生成的视频，每个注释提供自然语言解释，并标记出包含伪造痕迹的区域和时间戳。我们将这些注释整合为9个主要类别，帮助人类识别视频是否为AI生成。通过DeeptraceReward，我们的奖励模型在伪造线索识别和解释方面的表现超越了GPT-5，推动了社会意识和可信赖的视频生成研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26628",
            "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
            "url": "https://huggingface.co/papers/2509.26628",
            "abstract": "A novel PSRL framework (AttnRL) enhances exploration efficiency in reasoning models by branching from high attention positions and using an adaptive sampling strategy, outperforming prior methods in mathematical reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.",
            "score": 7,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "ac8005b1bfc91f64",
            "authors": [
                "Runze Liu",
                "Jiakang Wang",
                "Yuling Shi",
                "Zhihui Xie",
                "Chenxin An",
                "Kaiyan Zhang",
                "Jian Zhao",
                "Xiaodong Gu",
                "Lei Lin",
                "Wenping Hu",
                "Xiu Li",
                "Fuzheng Zhang",
                "Guorui Zhou",
                "Kun Gai"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Kuaishou Technology",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26628.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "Умное ветвление через внимание для обучения рассуждениям",
                    "desc": "Исследователи предложили новый подход AttnRL для улучшения Process-Supervised Reinforcement Learning при обучении LLM математическим рассуждениям. Ключевая идея заключается в том, чтобы создавать ветвления решения в позициях с высокими значениями attention scores, которые коррелируют с важными шагами рассуждения. Метод использует адаптивную стратегию сэмплирования, учитывающую сложность задачи, и применяет one-step off-policy обучение для повышения эффективности. Эксперименты показывают значительное превосходство над предыдущими методами как по качеству решений, так и по эффективности обучения."
                },
                "en": {
                    "title": "Enhancing Reasoning with Efficient Exploration in AttnRL",
                    "desc": "The paper presents a new framework called AttnRL that improves exploration efficiency in reasoning models using Process-Supervised Reinforcement Learning (PSRL). It focuses on branching from positions in the model that have high attention scores, which are linked to better reasoning performance. Additionally, the authors introduce an adaptive sampling strategy that adjusts based on the difficulty of problems and the size of previous training batches. Experiments show that AttnRL outperforms existing methods in mathematical reasoning tasks, enhancing both performance and training efficiency."
                },
                "zh": {
                    "title": "提升推理模型探索效率的新框架",
                    "desc": "本文提出了一种新的过程监督强化学习框架（AttnRL），旨在提高推理模型的探索效率。该框架通过从高注意力位置分支，并采用自适应采样策略，克服了现有方法在数学推理基准测试中的局限性。研究表明，高注意力分数的步骤与推理行为相关，因此我们选择从这些位置进行分支。此外，我们设计了一种一步离线策略训练管道，以进一步提高采样效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26495",
            "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
            "url": "https://huggingface.co/papers/2509.26495",
            "abstract": "Operational safety, measured by OffTopicEval, is a critical issue for LLMs, with most models falling short, but prompt-based steering methods show promise in improving out-of-distribution refusal.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM's ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\% -- fall far short of reliable operational safety, while GPT models plateau in the 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma and Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23\\%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents.",
            "score": 7,
            "issue_id": 6176,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "4d40a6a3c67d3196",
            "authors": [
                "Jingdi Lei",
                "Varun Gumma",
                "Rishabh Bhardwaj",
                "Seok Min Lim",
                "Chuan Li",
                "Amir Zadeh",
                "Soujanya Poria"
            ],
            "affiliations": [
                "IMDA",
                "Lambda Labs",
                "Nanyang Technological University",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26495.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#alignment",
                    "#training",
                    "#ethics",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🚦",
                "ru": {
                    "title": "Операционная безопасность: научить LLM отказываться от неподходящих запросов",
                    "desc": "Исследователи представили концепцию операционной безопасности — способности LLM корректно принимать или отклонять запросы пользователей в соответствии с заданной целью использования. Тестирование 20 открытых моделей показало, что даже лучшие из них (Qwen-3 и Mistral) достигают только 77-80% безопасности, что недостаточно для надёжного применения. Для решения проблемы предложены методы управления через промпты: Q-ground и P-ground, которые улучшают способность моделей отказываться от нерелевантных запросов на 23-41%. Результаты подчёркивают критическую необходимость работы над операционной безопасностью AI-агентов в корпоративном применении."
                },
                "en": {
                    "title": "Enhancing LLM Safety with Prompt-Based Steering",
                    "desc": "This paper addresses the critical issue of operational safety in Large Language Models (LLMs), which is their ability to safely accept or refuse user queries based on specific tasks. The authors introduce OffTopicEval, a new evaluation suite designed to measure this operational safety across various LLMs. Their findings reveal that most models, including top performers, exhibit significant operational safety shortcomings, with scores indicating high levels of operational unsafety. To mitigate these issues, the paper proposes prompt-based steering methods, which have shown to improve out-of-distribution refusal rates significantly, suggesting a pathway towards safer LLM applications."
                },
                "zh": {
                    "title": "提升大型语言模型的操作安全性",
                    "desc": "本论文探讨了大型语言模型（LLM）的操作安全性，提出了一个新的评估标准OffTopicEval。研究发现，尽管不同模型的表现有所不同，但所有模型在操作安全性方面都存在显著不足。为了解决这个问题，论文提出了基于提示的引导方法，包括查询引导（Q-ground）和系统提示引导（P-ground），这两种方法能够显著提高模型在特定任务中的拒绝能力。结果表明，操作安全性是模型对齐的核心问题，急需采取干预措施。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26618",
            "title": "DA^2: Depth Anything in Any Direction",
            "url": "https://huggingface.co/papers/2509.26618",
            "abstract": "DA², a zero-shot generalizable and fully end-to-end panoramic depth estimator, addresses challenges in panoramic depth estimation by using a data curation engine and SphereViT to handle spherical distortions, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Panorama has a full FoV (360^circtimes180^circ), offering a more complete visual description than perspective images. Thanks to this characteristic, panoramic depth estimation is gaining increasing traction in 3D vision. However, due to the scarcity of panoramic data, previous methods are often restricted to in-domain settings, leading to poor zero-shot generalization. Furthermore, due to the spherical distortions inherent in panoramas, many approaches rely on perspective splitting (e.g., cubemaps), which leads to suboptimal efficiency. To address these challenges, we propose DA^{2}: Depth Anything in Any Direction, an accurate, zero-shot generalizable, and fully end-to-end panoramic depth estimator. Specifically, for scaling up panoramic data, we introduce a data curation engine for generating high-quality panoramic depth data from perspective, and create sim543K panoramic RGB-depth pairs, bringing the total to sim607K. To further mitigate the spherical distortions, we present SphereViT, which explicitly leverages spherical coordinates to enforce the spherical geometric consistency in panoramic image features, yielding improved performance. A comprehensive benchmark on multiple datasets clearly demonstrates DA^{2}'s SoTA performance, with an average 38% improvement on AbsRel over the strongest zero-shot baseline. Surprisingly, DA^{2} even outperforms prior in-domain methods, highlighting its superior zero-shot generalization. Moreover, as an end-to-end solution, DA^{2} exhibits much higher efficiency over fusion-based approaches. Both the code and the curated panoramic data will be released. Project page: https://depth-any-in-any-dir.github.io/.",
            "score": 5,
            "issue_id": 6179,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "4f4b4417e453eeeb",
            "authors": [
                "Haodong Li",
                "Wangguangdong Zheng",
                "Jing He",
                "Yuhao Liu",
                "Xin Lin",
                "Xin Yang",
                "Ying-Cong Chen",
                "Chunchao Guo"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "Tencent Hunyuan",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26618.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#data",
                    "#dataset",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Оценка глубины панорам без дистилляции и с нулевым обучением",
                    "desc": "Статья представляет DA² — систему для оценки глубины на панорамных изображениях с полным обзором 360×180 градусов, которая работает без дообучения на целевых данных (zero-shot). Авторы создали движок для генерации высококачественных панорамных данных из перспективных изображений, получив ~607K пар RGB-глубина. Предложенная архитектура SphereViT использует сферические координаты для учёта геометрических искажений панорам, что устраняет необходимость разбиения на перспективные проекции. Метод показывает улучшение на 38% по метрике AbsRel по сравнению с лучшими zero-shot baseline и превосходит даже специализированные модели."
                },
                "en": {
                    "title": "DA²: Depth Estimation Anywhere, Anytime!",
                    "desc": "DA² is a novel panoramic depth estimator that operates without needing specific training data, making it capable of zero-shot generalization. It utilizes a data curation engine to create high-quality panoramic depth data from existing perspective images, significantly increasing the dataset size. To tackle the challenges posed by spherical distortions in panoramic images, DA² employs SphereViT, which ensures geometric consistency in the features extracted from these images. The results show that DA² achieves state-of-the-art performance, outperforming previous methods and demonstrating its efficiency as a fully end-to-end solution."
                },
                "zh": {
                    "title": "DA²：全景深度估计的新突破",
                    "desc": "DA²是一种零-shot可泛化的全端到端全景深度估计器，旨在解决全景深度估计中的挑战。它通过数据策划引擎生成高质量的全景深度数据，并利用SphereViT处理球面失真，从而实现了最先进的性能。该方法在多个数据集上的基准测试中显示出38%的平均AbsRel改进，超越了以往的零-shot基线和领域内方法。DA²作为一个全端到端的解决方案，展现出比基于融合的方法更高的效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24207",
            "title": "Humanline: Online Alignment as Perceptual Loss",
            "url": "https://huggingface.co/papers/2509.24207",
            "abstract": "Online alignment methods like GRPO outperform offline methods like DPO due to better approximation of human-perceived probability distributions, and introducing perceptual biases into offline training can achieve similar performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks.",
            "score": 5,
            "issue_id": 6178,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "36a99560c909c06b",
            "authors": [
                "Sijia Liu",
                "Niklas Muennighoff",
                "Kawin Ethayarajh"
            ],
            "affiliations": [
                "Princeton University",
                "Stanford University",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24207.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Человеческое восприятие вероятностей как ключ к эффективному обучению LLM",
                    "desc": "Исследователи объясняют, почему онлайн методы выравнивания (GRPO) работают лучше оффлайн методов (DPO), опираясь на теорию перспектив из поведенческой экономики. Оказывается, что онлайн сэмплирование лучше аппроксимирует распределение вероятностей с точки зрения человеческого восприятия, а клиппинг в PPO/GRPO воспроизводит когнитивные искажения людей. Авторы предлагают новый подход humanline, который встраивает перцептивные искажения непосредственно в функции потерь типа DPO/KTO/GRPO. Эксперименты показывают, что humanline варианты на оффлайн данных достигают производительности онлайн методов, но работают быстрее и дешевле."
                },
                "en": {
                    "title": "Aligning AI with Human Perception for Better Performance",
                    "desc": "This paper discusses how online alignment methods, such as GRPO, are more effective than offline methods like DPO because they better reflect how humans perceive probabilities. It introduces the concept of perceptual biases, suggesting that incorporating these biases into offline training can yield similar performance to online methods. The authors argue that the traditional distinction between online and offline training is less important than aligning training with human perception. They propose a new design pattern that integrates perceptual distortions into training objectives, allowing offline methods to achieve results comparable to online methods."
                },
                "zh": {
                    "title": "在线对齐超越离线对齐的秘密",
                    "desc": "在线对齐方法（如GRPO）比离线方法（如DPO）表现更好，因为它们更好地近似人类感知的概率分布。我们提出了一种以人为中心的解释，基于行为经济学的前景理论，证明在线策略采样更能接近人类感知的分布。PPO/GRPO风格的剪切不仅用于稳定训练，还恢复了人类对概率的感知偏差。通过选择性地训练任何数据以模仿人类感知，我们可以更快、更便宜和灵活地进行后训练，而不牺牲性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26476",
            "title": "Regression Language Models for Code",
            "url": "https://huggingface.co/papers/2509.26476",
            "abstract": "A unified Regression Language Model (RLM) predicts numeric outcomes of code executions, including memory footprint, latency, and neural network performance, across multiple languages and hardware platforms.  \t\t\t\t\tAI-generated summary \t\t\t\t We study code-to-metric regression: predicting numeric outcomes of code executions, a challenging task due to the open-ended nature of programming languages. While prior methods have resorted to heavy and domain-specific feature engineering, we show that a single unified Regression Language Model (RLM) can simultaneously predict directly from text, (i) the memory footprint of code across multiple high-level languages such as Python and C++, (ii) the latency of Triton GPU kernels, and (iii) the accuracy and speed of trained neural networks represented in ONNX. In particular, a relatively small 300M parameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on competitive programming submissions from APPS, and a single unified model achieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet. Furthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five classic NAS design spaces previously dominated by graph neural networks, and simultaneously predict architecture latencies on numerous hardware platforms.",
            "score": 4,
            "issue_id": 6176,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "4c9c21b6cd24bc71",
            "authors": [
                "Yash Akhauri",
                "Xingyou Song",
                "Arissa Wongpanich",
                "Bryan Lewandowski",
                "Mohamed S. Abdelfattah"
            ],
            "affiliations": [
                "Cornell University",
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26476.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#multilingual",
                    "#small_models"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Единая языковая модель для предсказания производительности кода",
                    "desc": "В статье представлена единая Regression Language Model (RLM), которая предсказывает численные результаты выполнения кода, такие как потребление памяти, латентность и производительность нейронных сетей. Модель работает напрямую с текстом кода на разных языках программирования (Python, C++, Triton, ONNX) без сложной инженерии признаков. RLM на основе T5Gemma с 300M параметров достигает корреляции Spearman > 0.9 на задачах конкурентного программирования и > 0.5 в среднем по 17 языкам. Модель также превосходит graph neural networks в задачах Neural Architecture Search, достигая Kendall-Tau 0.46 и одновременно предсказывая латентность на различных аппаратных платформах."
                },
                "en": {
                    "title": "Unified Regression Model: Predicting Code Performance Across Languages and Hardware",
                    "desc": "The paper introduces a unified Regression Language Model (RLM) that predicts numeric outcomes from code executions, such as memory usage and latency, across various programming languages and hardware. Unlike previous methods that relied on extensive feature engineering, the RLM directly analyzes code text to make predictions. It demonstrates strong performance, achieving high Spearman-rank scores on competitive programming tasks and across multiple languages. Additionally, the model excels in predicting architecture latencies in neural architecture search, outperforming traditional graph neural networks."
                },
                "zh": {
                    "title": "统一回归语言模型：跨语言与平台的性能预测",
                    "desc": "本文提出了一种统一的回归语言模型（RLM），用于预测代码执行的数值结果，包括内存占用、延迟和神经网络性能。与以往需要大量领域特定特征工程的方法不同，RLM能够直接从文本中进行预测，适用于多种高级编程语言。实验表明，RLM在多个编程语言的竞争性提交中表现优异，达到了超过0.9的Spearman等级相关系数。该模型还在经典的神经架构搜索设计空间中取得了最高的Kendall-Tau平均值，展示了其在不同硬件平台上的广泛适用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25848",
            "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2509.25848",
            "abstract": "VAPO-Thinker-7B enhances multimodal reasoning by anchoring the process to visual information, improving performance on visual tasks while maintaining logical inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/",
            "score": 4,
            "issue_id": 6177,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "41251889e40e8e71",
            "authors": [
                "Xinyu Tian",
                "Shu Zou",
                "Zhaoyuan Yang",
                "Mengqi He",
                "Fabian Waschkowski",
                "Lukas Wesemann",
                "Peter Tu",
                "Jing Zhang"
            ],
            "affiliations": [
                "Australian National University",
                "GE Research",
                "University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25848.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Не забывай смотреть: как научить AI рассуждать без потери визуального восприятия",
                    "desc": "Исследование выявляет проблему \"визуального забывания\" в Vision-Language Models: при длительном рассуждении модели начинают игнорировать визуальную информацию, что ухудшает качество ответов на простые визуальные вопросы. Авторы предлагают метод Vision-Anchored Policy Optimization (VAPO), который явно направляет процесс рассуждения к траекториям, основанным на визуальных данных. Метод применяется при обучении с подкреплением (Reinforcement Learning) и помогает модели сохранять связь с визуальной информацией во время логического вывода. Результирующая модель VAPO-Thinker-7B достигает state-of-the-art результатов на множестве бенчмарков, эффективно балансируя между способностью к рассуждению и визуальным восприятием."
                },
                "en": {
                    "title": "Anchoring Reasoning to Visuals for Better Performance",
                    "desc": "The paper introduces VAPO-Thinker-7B, a model that enhances multimodal reasoning by anchoring it to visual information. This approach improves performance on visual tasks while ensuring logical inference remains strong. The study reveals that while multimodal reasoning boosts problem-solving capabilities, it can lead to visual forgetting, where the model neglects visual input over time. To counteract this, the authors propose Vision-Anchored Policy Optimization (VAPO), which helps maintain a strong connection to visual data, resulting in state-of-the-art performance on various benchmarks."
                },
                "zh": {
                    "title": "视觉锚定，推理更精准！",
                    "desc": "VAPO-Thinker-7B通过将推理过程与视觉信息相结合，增强了多模态推理能力，从而在视觉任务上提高了性能，同时保持了逻辑推理的能力。该研究发现，多模态推理具有双重特性，虽然它能显著提升逻辑推理和解决复杂问题的能力，但也可能导致感知基础的逐渐削弱，造成对基本视觉问题的识别失败。为了解决这一问题，研究者提出了视觉锚定策略优化（VAPO），该方法有效地引导推理过程朝向视觉基础的轨迹。最终，VAPO-Thinker-7B在多个基准测试中取得了新的最先进结果，显著增强了模型对视觉信息的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25189",
            "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents",
            "url": "https://huggingface.co/papers/2509.25189",
            "abstract": "InfoAgent, a deep research agent using a custom data synthesis pipeline and search infrastructure, outperforms existing agents by improving tool use and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our InfoAgent is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy on BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.",
            "score": 4,
            "issue_id": 6178,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "96392fa5b9fec8c5",
            "authors": [
                "Gongrui Zhang",
                "Jialiang Zhu",
                "Ruiqi Yang",
                "Kai Qiu",
                "Miaosen Zhang",
                "Zhirong Wu",
                "Qi Dai",
                "Bei Liu",
                "Chong Luo",
                "Zhengyuan Yang",
                "Linjie Li",
                "Lijuan Wang",
                "Weizhu Chen",
                "Yuan Zhang",
                "Xin Li",
                "Zhaoyi Liu",
                "Xin Geng",
                "Baining Guo"
            ],
            "affiliations": [
                "Brown University",
                "Microsoft",
                "Southeast University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25189.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#agents",
                    "#training",
                    "#open_source",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "InfoAgent: Агент глубокого исследования с продвинутым поиском",
                    "desc": "В статье представлен InfoAgent — агент на основе LLM для глубокого исследования информации, который использует инновационный pipeline синтеза данных и собственную инфраструктуру веб-поиска. Для обучения авторы создали сложные запросы через построение деревьев сущностей с их последующим искажением, что систематически увеличивает сложность вопросов. Модель обучалась в два этапа: сначала supervised fine-tuning для освоения длинных цепочек поиска, затем reinforcement learning для улучшения использования инструментов на основе рассуждений. InfoAgent на базе Qwen3-14B превосходит существующие open-source агенты исследования, достигая 15.3% точности на BrowseComp и 40.4% на Xbench-DS, опережая модели вроде WebSailor-72B и DeepDive-32B."
                },
                "en": {
                    "title": "InfoAgent: Elevating Research with Enhanced Tool Use and Reasoning",
                    "desc": "InfoAgent is a deep research agent that enhances its performance by utilizing a unique data synthesis pipeline and a self-hosted search infrastructure. It constructs complex queries through entity trees and sub-tree sampling, which increases the difficulty of questions systematically. This approach allows InfoAgent to outperform existing agents by improving its reasoning and tool usage capabilities. The agent is fine-tuned using a two-stage process that includes supervised learning and reinforcement learning, leading to significant improvements in accuracy on various benchmarks."
                },
                "zh": {
                    "title": "InfoAgent：提升工具使用与推理能力的深度研究代理",
                    "desc": "本文介绍了一种名为InfoAgent的深度研究代理，它通过创新的数据合成管道和搜索工具，提升了工具使用和推理能力。InfoAgent构建了实体树并应用子树采样，以系统性地增加问题的难度，从而生成更具挑战性的查询。与以往依赖商业搜索工具的研究不同，InfoAgent开发了专用的自托管搜索基础设施，增强了代理环境的透明度。通过测量正确回答问题所需的工具调用次数，评估了数据管道的有效性，结果显示InfoAgent在多个基准测试中表现优于之前的开源深度研究代理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22613",
            "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
            "url": "https://huggingface.co/papers/2509.22613",
            "abstract": "Theoretical analysis of reinforcement learning methods in enhancing LLM planning reveals that while RL improves generalization through exploration, policy gradient suffers from diversity collapse, whereas Q-learning maintains diversity and requires careful reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
            "score": 4,
            "issue_id": 6175,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "8899a479ee5856c0",
            "authors": [
                "Siwei Wang",
                "Yifei Shen",
                "Haoran Sun",
                "Shi Feng",
                "Shang-Hua Teng",
                "Li Dong",
                "Yaru Hao",
                "Wei Chen"
            ],
            "affiliations": [
                "Harvard University",
                "Microsoft Research Asia",
                "Peking University",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22613.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#training",
                    "#rl"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "Почему Q-learning лучше policy gradient для планирования в LLM",
                    "desc": "Исследование теоретически анализирует, как методы reinforcement learning улучшают способности LLM к планированию на графовой абстракции задач. Supervised fine-tuning может приводить к ложным решениям на основе совместной встречаемости, тогда как RL достигает правильного планирования через исследование среды. Policy gradient методы страдают от коллапса разнообразия выходов во время обучения, в то время как Q-learning сохраняет разнообразие и позволяет учиться off-policy. Авторы также показывают важность тщательного дизайна функции награды для предотвращения reward hacking в Q-learning."
                },
                "en": {
                    "title": "Exploration Enhances Planning: Balancing Diversity in RL for LLMs",
                    "desc": "This paper analyzes how reinforcement learning (RL) methods can improve the planning abilities of Large Language Models (LLMs). It highlights that while RL enhances generalization through exploration, policy gradient methods face a problem called diversity collapse, where the variety of outputs decreases over time. In contrast, Q-learning maintains output diversity and allows for off-policy learning, but it requires careful design of rewards to avoid issues like reward hacking. The findings are validated through experiments on the Blocksworld planning benchmark, demonstrating the practical implications of these theoretical insights."
                },
                "zh": {
                    "title": "强化学习提升语言模型规划能力的理论分析",
                    "desc": "本研究分析了强化学习（RL）在提升大型语言模型（LLM）规划能力中的作用。我们发现，虽然RL通过探索提高了模型的泛化能力，但策略梯度方法在训练过程中会出现多样性崩溃的问题。相比之下，Q学习方法能够保持多样性，并且在收敛时具有离线学习的优势。我们还强调了奖励设计的重要性，以防止Q学习中的奖励操控问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26539",
            "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
            "url": "https://huggingface.co/papers/2509.26539",
            "abstract": "Ferret-UI Lite, a compact end-to-end GUI agent, achieves competitive performance across diverse platforms using chain-of-thought reasoning, visual tool-use, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale GUI agents. In GUI grounding, Ferret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld and 19.8% on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents.",
            "score": 3,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "4f42c990da9b21fe",
            "authors": [
                "Zhen Yang",
                "Zi-Yi Dou",
                "Di Feng",
                "Forrest Huang",
                "Anh Nguyen",
                "Keen You",
                "Omar Attia",
                "Yuhao Yang",
                "Michael Feng",
                "Haotian Zhang",
                "Ram Ramrakhya",
                "Chao Jia",
                "Jeffrey Nichols",
                "Alexander Toshev",
                "Yinfei Yang",
                "Zhe Gan"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26539.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#agents",
                    "#reasoning",
                    "#small_models",
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#rl"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "Компактный AI-агент для управления интерфейсами на устройстве",
                    "desc": "Ferret-UI Lite — это компактная модель размером 3B параметров для автономного взаимодействия с графическими интерфейсами на мобильных, веб и десктоп платформах. Модель обучена на смеси реальных и синтетических данных с использованием chain-of-thought рассуждений, визуальных инструментов и reinforcement learning со специально разработанными наградами. На бенчмарках для определения элементов интерфейса модель достигает точности до 91.6% на ScreenSpot-V2, а для навигации показывает успешность 28% на AndroidWorld. Это демонстрирует возможность создания эффективных GUI-агентов малого размера для работы непосредственно на устройствах пользователей."
                },
                "en": {
                    "title": "Compact GUI Agent with Competitive Performance",
                    "desc": "Ferret-UI Lite is a small, end-to-end agent designed to interact with Graphic User Interfaces (GUIs) across various platforms like mobile and desktop. It employs chain-of-thought reasoning and visual tool-use to enhance its performance, making it effective even with limited resources. The agent is trained using a mix of real and synthetic GUI data, and it utilizes reinforcement learning to optimize its actions based on specific rewards. Overall, Ferret-UI Lite demonstrates competitive results compared to other small-scale GUI agents, showcasing its potential for on-device applications."
                },
                "zh": {
                    "title": "紧凑高效的GUI代理：Ferret-UI Lite",
                    "desc": "Ferret-UI Lite 是一种紧凑的端到端图形用户界面（GUI）代理，能够在多种平台上实现竞争力的性能。该模型采用了链式思维推理、视觉工具使用和强化学习等技术，专为小型设备优化。通过从真实和合成来源中策划多样化的GUI数据，Ferret-UI Lite 在推理时的表现得到了增强。实验结果显示，Ferret-UI Lite 在多个基准测试中表现优异，成功率与其他小型GUI代理相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23166",
            "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with\n  LLMs",
            "url": "https://huggingface.co/papers/2509.23166",
            "abstract": "ROSA, a lightweight algorithm, enhances multi-turn interactions in LLMs by adapting to user feedback in real-time, improving both task effectiveness and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) employ multi-turn interaction as a fundamental paradigm for completing complex tasks. However, their performance often degrades in extended interactions, as they are typically trained on static, single-turn data, which hinders their ability to adapt to real-time user feedback. To address this limitation, we first propose a new paradigm: Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes user feedback from the ongoing interaction as a reward signal to estimate a latent optimal policy aligned with user preferences, then updates a small subset of parameters to steer the model toward this policy, ultimately enabling efficient in-conversation self-correction. We then introduce Optimum-Referenced One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM. ROSA guides the model parameters toward a theoretical optimal policy in a single, efficient update step, avoiding costly iterative gradient-based optimization and minimizing computational overhead. We provide a rigorous theoretical analysis guaranteeing that the policy of ROSA converges to the preference of user as the number of interactions increases. Extensive experiments on challenging benchmark demonstrate that ROSA achieves significant improvements in both task effectiveness and efficiency.",
            "score": 3,
            "issue_id": 6178,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "08ae55e74db47a69",
            "authors": [
                "Chenxing Wei",
                "Hong Wang",
                "Ying He",
                "Fei Yu",
                "Yao Shu"
            ],
            "affiliations": [
                "College of Computer Science and Software Engineering, Shenzhen University, China",
                "Guangdong Lab of AI and Digital Economy (SZ), China",
                "Hong Kong University of Science and Technology (Guangzhou), China",
                "School of Information Technology, Carleton University, Canada",
                "University of Science and Technology of China, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23166.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Адаптация AI к предпочтениям пользователя прямо во время диалога",
                    "desc": "Статья представляет ROSA — лёгкий алгоритм для улучшения многоходовых диалогов с LLM. Проблема в том, что модели обычно обучены на статичных данных и плохо адаптируются к обратной связи пользователя в реальном времени. ROSA использует фидбек пользователя как сигнал награды, чтобы оценить оптимальную политику и обновить небольшое подмножество параметров модели за один шаг, избегая затратной итеративной оптимизации. Теоретически доказано, что политика ROSA сходится к предпочтениям пользователя по мере увеличения числа взаимодействий, а эксперименты подтверждают значительное улучшение эффективности и качества выполнения задач."
                },
                "en": {
                    "title": "ROSA: Real-Time Adaptation for Enhanced Multi-Turn Interactions",
                    "desc": "This paper introduces ROSA, a lightweight algorithm designed to improve multi-turn interactions in Large Language Models (LLMs) by incorporating real-time user feedback. It addresses the challenge of LLMs degrading in performance during extended interactions due to their training on static data. The authors propose a new approach called Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which uses user feedback as a reward signal to adjust the model's parameters towards an optimal policy. ROSA operationalizes this approach with a single-step update, ensuring efficient adaptation while minimizing computational costs and enhancing task effectiveness."
                },
                "zh": {
                    "title": "实时反馈，提升多轮交互的智能算法",
                    "desc": "ROSA是一种轻量级算法，旨在通过实时适应用户反馈来增强大型语言模型（LLMs）中的多轮交互。传统的LLMs在多轮交互中表现不佳，因为它们通常基于静态的单轮数据进行训练，无法有效应对实时反馈。为了解决这个问题，本文提出了一种新的范式：多轮交互的测试时策略适应（T2PAM），利用用户反馈作为奖励信号来估计与用户偏好一致的潜在最优策略。ROSA算法通过一次高效的更新步骤引导模型参数朝向理论最优策略，从而提高任务的有效性和效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26329",
            "title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics",
            "url": "https://huggingface.co/papers/2509.26329",
            "abstract": "TAU, a benchmark of culturally specific Taiwanese soundmarks, reveals that state-of-the-art large audio-language models underperform compared to local humans, highlighting the need for localized evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large audio-language models are advancing rapidly, yet most evaluations emphasize speech or globally sourced sounds, overlooking culturally distinctive cues. This gap raises a critical question: can current models generalize to localized, non-semantic audio that communities instantly recognize but outsiders do not? To address this, we present TAU (Taiwan Audio Understanding), a benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline combining curated sources, human editing, and LLM-assisted question generation, producing 702 clips and 1,794 multiple-choice items that cannot be solved by transcripts alone. Experiments show that state-of-the-art LALMs, including Gemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates the need for localized benchmarks to reveal cultural blind spots, guide more equitable multimodal evaluation, and ensure models serve communities beyond the global mainstream.",
            "score": 2,
            "issue_id": 6176,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "f898fff4a933cddb",
            "authors": [
                "Yi-Cheng Lin",
                "Yu-Hua Chen",
                "Jia-Kai Dong",
                "Yueh-Hsuan Huang",
                "Szu-Chi Chen",
                "Yu-Chen Chen",
                "Chih-Yao Chen",
                "Yu-Jung Lin",
                "Yu-Ling Chen",
                "Zih-Yu Chen",
                "I-Ning Tsai",
                "Hsiu-Hsuan Wang",
                "Ho-Lam Chung",
                "Ke-Han Lu",
                "Hung-yi Lee"
            ],
            "affiliations": [
                "National Taiwan University",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26329.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#audio",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🔔",
                "ru": {
                    "title": "Культурные звуки как тест для аудио-AI: модели не слышат локальный контекст",
                    "desc": "Исследователи создали бенчмарк TAU для оценки способности больших аудио-языковых моделей распознавать культурно-специфичные звуки Тайваня. Датасет содержит 702 аудиоклипа и 1794 вопроса с множественным выбором, которые невозможно решить только с помощью транскрипции речи. Эксперименты показали, что современные LALM-модели, включая Gemini 2.5 и Qwen2-Audio, демонстрируют результаты значительно хуже местных жителей. Работа подчеркивает важность локализованных бенчмарков для выявления культурных слепых зон AI-систем и создания более справедливой мультимодальной оценки."
                },
                "en": {
                    "title": "Bridging the Gap: Localized Audio Understanding with TAU",
                    "desc": "The paper introduces TAU, a benchmark designed to evaluate audio-language models using culturally specific sounds from Taiwan, known as 'soundmarks.' It highlights that current state-of-the-art large audio-language models (LALMs) struggle to recognize these localized audio cues, performing significantly worse than local human listeners. The study emphasizes the importance of localized evaluations, as existing benchmarks often focus on globally sourced sounds, neglecting unique cultural audio. By showcasing the limitations of LALMs in understanding culturally distinctive sounds, TAU aims to promote more equitable and relevant multimodal evaluations."
                },
                "zh": {
                    "title": "本地化评估，提升音频理解能力",
                    "desc": "本论文介绍了TAU（台湾音频理解），这是一个针对台湾特有声音标记的基准测试。研究发现，当前最先进的大型音频语言模型在处理这些地方性音频时，表现远低于当地人。此研究强调了对本地化评估的需求，以便更好地理解和服务于特定文化的社区。通过结合策划来源和人类编辑，TAU提供了702个音频片段和1794个多项选择题，展示了模型在处理非语义音频时的局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25339",
            "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes",
            "url": "https://huggingface.co/papers/2509.25339",
            "abstract": "VisualOverload is a VQA benchmark that challenges models with simple vision tasks in densely populated scenes, revealing gaps in current VLMs' performance and offering insights into their failure modes.  \t\t\t\t\tAI-generated summary \t\t\t\t Is basic visual understanding really solved in state-of-the-art VLMs? We present VisualOverload, a slightly different visual question answering (VQA) benchmark comprising 2,720 question-answer pairs, with privately held ground-truth responses. Unlike prior VQA datasets that typically focus on near global image understanding, VisualOverload challenges models to perform simple, knowledge-free vision tasks in densely populated (or, overloaded) scenes. Our dataset consists of high-resolution scans of public-domain paintings that are populated with multiple figures, actions, and unfolding subplots set against elaborately detailed backdrops. We manually annotated these images with questions across six task categories to probe for a thorough understanding of the scene. We hypothesize that current benchmarks overestimate the performance of VLMs, and encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes. Indeed, we observe that even the best model (o3) out of 37 tested models only achieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on all questions. Beyond a thorough evaluation, we complement our benchmark with an error analysis that reveals multiple failure modes, including a lack of counting skills, failure in OCR, and striking logical inconsistencies under complex tasks. Altogether, VisualOverload exposes a critical gap in current vision models and offers a crucial resource for the community to develop better models.   Benchmark: http://paulgavrikov.github.io/visualoverload",
            "score": 2,
            "issue_id": 6175,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "be3aca0a0807cc94",
            "authors": [
                "Paul Gavrikov",
                "Wei Lin",
                "M. Jehanzeb Mirza",
                "Soumya Jahagirdar",
                "Muhammad Huzaifa",
                "Sivan Doveh",
                "Serena Yeung-Levy",
                "James Glass",
                "Hilde Kuehne"
            ],
            "affiliations": [
                "Independent Researcher",
                "JKU Linz",
                "MIT CSAIL",
                "MIT-IBM Watson AI Lab",
                "Stanford",
                "Tübingen AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25339.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "VisualOverload: когда сложные сцены ставят VLM в тупик",
                    "desc": "Исследователи представили бенчмарк VisualOverload для оценки vision-language моделей на задачах визуального понимания в перегруженных деталями сценах. Датасет содержит 2720 вопросов к высокодетализированным изображениям классических картин с множеством персонажей и элементов. Даже лучшая модель (o3) достигает только 69.5% точности, а на самом сложном тестовом сплите — всего 19.6%, что выявляет критические проблемы современных VLM. Анализ ошибок показал слабости моделей в подсчёте объектов, распознавании текста (OCR) и логической последовательности при работе со сложными визуальными задачами."
                },
                "en": {
                    "title": "Unveiling Gaps in Visual Understanding with VisualOverload",
                    "desc": "VisualOverload is a new benchmark for visual question answering (VQA) that tests the capabilities of vision-language models (VLMs) in complex scenes filled with details. It includes 2,720 question-answer pairs based on high-resolution images of public-domain paintings, focusing on simple tasks that require understanding of crowded environments. The study reveals that existing VLMs struggle with basic visual comprehension, achieving only 19.6% accuracy on the most challenging questions. This benchmark not only highlights the limitations of current models but also provides a resource for improving their performance through targeted error analysis."
                },
                "zh": {
                    "title": "揭示视觉模型的关键缺陷",
                    "desc": "VisualOverload是一个视觉问答（VQA）基准，旨在通过密集场景中的简单视觉任务来挑战模型，揭示当前视觉语言模型（VLMs）性能的不足。该基准包含2720个问答对，主要关注在复杂背景下的图像理解能力。研究表明，现有基准可能高估了VLMs的性能，尤其是在处理密集场景时，模型在细节编码和推理方面仍面临挑战。通过错误分析，VisualOverload揭示了多个失败模式，包括计数能力不足、光学字符识别失败和复杂任务下的逻辑不一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26574",
            "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark",
            "url": "https://huggingface.co/papers/2509.26574",
            "abstract": "CritPt, a benchmark for evaluating LLMs on research-level physics tasks, reveals significant gaps between current model capabilities and the demands of physics research.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced \"critical point\"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.",
            "score": 1,
            "issue_id": 6176,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "c743f2941f645607",
            "authors": [
                "Minhui Zhu",
                "Minyang Tian",
                "Xiaocheng Yang",
                "Tianci Zhou",
                "Penghao Zhu",
                "Eli Chertkov",
                "Shengyan Liu",
                "Yufeng Du",
                "Lifan Yuan",
                "Ziming Ji",
                "Indranil Das",
                "Junyi Cao",
                "Yufeng Du",
                "Jinchen He",
                "Yifan Su",
                "Jiabin Yu",
                "Yikun Jiang",
                "Yujie Zhang",
                "Chang Liu",
                "Ze-Min Huang",
                "Weizhen Jia",
                "Xinan Chen",
                "Peixue Wu",
                "Yunkai Wang",
                "Juntai Zhou",
                "Yong Zhao",
                "Farshid Jafarpour",
                "Jessie Shelton",
                "Aaron Young",
                "John Bartolotta",
                "Wenchao Xu",
                "Yue Sun",
                "Anjun Chu",
                "Victor Colussi",
                "Chris Akers",
                "Nathan Brooks",
                "Wenbo Fu",
                "Christopher Wilson",
                "Jinchao Zhao",
                "Marvin Qi",
                "Anqi Mu",
                "Yubo Yang",
                "Allen Zang",
                "Yang Lyu",
                "Peizhi Mai",
                "Xuefei Guo",
                "Luyu Gao",
                "Ze Yang",
                "Chi Xue",
                "Dmytro Bandak",
                "Yaïr Hein",
                "Yonatan Kahn",
                "Kevin Zhou",
                "John Drew Wilson Jarrod T. Reilly",
                "Di Luo",
                "Daniel Inafuku",
                "Hao Tong",
                "Liang Yang",
                "Ruixing Zhang",
                "Xueying Wang",
                "Ofir Press",
                "Nicolas Chia",
                "Eliu Huerta",
                "Hao Peng"
            ],
            "affiliations": [
                "Argonne National Laboratory",
                "Caltech",
                "Carnegie Mellon University",
                "Chi 3 Optics",
                "Columbia University",
                "ETH Zürich",
                "Harvard University",
                "Hofstra University",
                "Hong Kong University of Science and Technology",
                "Independent",
                "National Institute of Theory and Mathematics",
                "Northeastern University",
                "Ohio State University",
                "Paul Scherrer Institute",
                "Perimeter Institute for Theoretical Physics",
                "The Chinese University of Hong Kong",
                "University of California San Diego",
                "University of California, Berkeley",
                "University of California, Los Angeles",
                "University of Chicago",
                "University of Cologne",
                "University of Colorado Boulder",
                "University of Connecticut",
                "University of Florida",
                "University of Illinois Urbana-Champaign",
                "University of Maryland, College Park",
                "University of Tennessee Knoxville",
                "University of Toronto",
                "University of Washington Seattle",
                "University of Waterloo",
                "Utrecht University",
                "Vector Institute",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26574.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "⚛️",
                "ru": {
                    "title": "Физики проверили LLM на реальных исследовательских задачах — и модели провалились",
                    "desc": "Исследователи создали бенчмарк CritPt для оценки способностей LLM решать исследовательские задачи уровня научных работ по физике. Бенчмарк включает 71 комплексную задачу и 190 подзадач, охватывающих все современные области физики от квантовой механики до биофизики, созданные более чем 50 активными физиками-исследователями. Лучшие современные модели показали точность всего 4% на полных задачах и около 10% при использовании инструментов для программирования. Результаты выявили огромный разрыв между текущими возможностями AI и требованиями реальной научной работы в физике."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating LLMs in Advanced Physics Research",
                    "desc": "The paper introduces CritPt, a benchmark specifically designed to evaluate large language models (LLMs) on complex, research-level physics tasks. It highlights the significant gap between the capabilities of current LLMs and the requirements of advanced physics research, as evidenced by low accuracy rates on full-scale challenges. CritPt includes 71 composite research challenges and 190 simpler tasks, all created by active physics researchers to ensure relevance and rigor. The findings suggest that while LLMs show potential in isolated tasks, they struggle with comprehensive research problems, indicating a need for further development in AI tools for scientific applications."
                },
                "zh": {
                    "title": "评估LLMs在物理研究中的能力差距",
                    "desc": "CritPt是一个用于评估大型语言模型（LLMs）在研究级物理任务上的基准测试，揭示了当前模型能力与物理研究需求之间的显著差距。该基准测试涵盖了现代物理研究的多个领域，包括量子物理、天体物理和流体动力学等。CritPt包含71个复合研究挑战，旨在模拟入门级的完整研究项目，并分解为190个更简单的检查点任务。尽管当前最先进的LLMs在孤立的检查点上表现出一定的潜力，但在解决完整的研究规模挑战时仍然远远不够，最高准确率仅为4.0%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26542",
            "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
            "url": "https://huggingface.co/papers/2509.26542",
            "abstract": "VERA is a benchmark for evaluating reasoning ability in voice-interactive systems, revealing significant performance gaps compared to text models and highlighting challenges in real-time interaction.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for evaluating reasoning ability in voice-interactive systems under real-time conversational constraints. VERA comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual). Each item is adapted for speech interaction while preserving reasoning difficulty. VERA enables direct text-voice comparison within model families and supports analysis of how architectural choices affect reliability. We assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around ~10% accuracy, while approaching text performance requires sacrificing real-time interaction. Diagnostic experiments indicate that common mitigations are insufficient. Increasing \"thinking time\" yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors. Failure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs. VERA provides a reproducible testbed and targeted diagnostics for architectures that decouple thinking from speaking, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.",
            "score": 1,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "94e739b649ffcc6e",
            "authors": [
                "Yueqian Lin",
                "Zhengmian Hu",
                "Qinsi Wang",
                "Yudong Liu",
                "Hengfan Zhang",
                "Jayakumar Subramanian",
                "Nikos Vlassis",
                "Hai Helen Li",
                "Yiran Chen"
            ],
            "affiliations": [
                "Adobe, San Jose, CA, USA",
                "Duke University, Durham, NC, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26542.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#architecture",
                    "#long_context"
                ],
                "emoji": "🎤",
                "ru": {
                    "title": "Голосовые AI-помощники сильно отстают в способности рассуждать",
                    "desc": "VERA — это бенчмарк для оценки способности к рассуждению в голосовых интерактивных системах в условиях реального времени. Исследование показывает огромный разрыв в производительности между текстовыми и голосовыми моделями: на математических задачах текстовая модель достигает 74.8% точности, а её голосовая версия — всего 6.1%. Увеличение времени на «размышление» почти не помогает, а отделение процесса рассуждения от озвучивания улучшает результаты, но всё равно сильно уступает тексту. Бенчмарк включает 2,931 голосовой эпизод по пяти категориям и позволяет систематически изучать, как архитектурные решения влияют на надёжность голосовых ассистентов."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Voice Reasoning with VERA",
                    "desc": "VERA is a benchmark designed to evaluate the reasoning capabilities of voice-interactive systems, highlighting the performance differences between voice and text models. It includes 2,931 voice-native episodes adapted from existing text benchmarks, organized into five distinct tracks. The study reveals significant accuracy gaps, with text models outperforming voice models in reasoning tasks, particularly in mathematics and factual contexts. VERA serves as a tool for analyzing how different architectural choices impact the reliability of voice systems, aiming to improve real-time interaction without sacrificing reasoning quality."
                },
                "zh": {
                    "title": "VERA：语音交互推理能力的评估基准",
                    "desc": "VERA是一个用于评估语音交互系统推理能力的基准，揭示了与文本模型相比的显著性能差距，并强调了实时交互中的挑战。该基准包含2931个语音原生的案例，涵盖数学、网络、科学、长上下文和事实五个领域，适应语音交互的同时保持推理难度。通过对12个现代语音系统与强大的文本基线进行评估，发现语音系统在准确性上存在较大的差距。VERA为解耦思考与表达的架构提供了可重复的测试平台和针对性的诊断，帮助衡量实时语音助手在流畅性和可靠推理方面的进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23773",
            "title": "Knowledge Homophily in Large Language Models",
            "url": "https://huggingface.co/papers/2509.23773",
            "abstract": "Graph Neural Network regression models estimate entity-level knowledgeability in Large Language Models to improve active labeling and multi-hop reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.",
            "score": 1,
            "issue_id": 6176,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 сентября",
                "en": "September 28",
                "zh": "9月28日"
            },
            "hash": "9842b9fef9ea6d7d",
            "authors": [
                "Utkarsh Sahu",
                "Zhisheng Qi",
                "Mahantesh Halappanavar",
                "Nedim Lipka",
                "Ryan A. Rossi",
                "Franck Dernoncourt",
                "Yu Zhang",
                "Yao Ma",
                "Yu Wang"
            ],
            "affiliations": [
                "Adobe Research",
                "Pacific Northwest National Laboratory",
                "Rensselaer Polytechnic Institute",
                "Texas A&M University",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23773.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#agents",
                    "#graphs",
                    "#multimodal"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Граф знаний LLM: соседи знают одинаково",
                    "desc": "Исследователи изучили структурную организацию знаний в больших языковых моделях, представив их в виде графа. Оказалось, что LLM демонстрируют принцип гомофилии знаний: модель обладает схожим уровнем знаний о соседних сущностях в графе, подобно семантическим кластерам в когнитивной нейронауке. На основе этого открытия предложена регрессионная модель на Graph Neural Network для предсказания уровня осведомлённости модели о конкретных фактах через анализ соседних узлов. Такой подход позволяет эффективнее выбирать данные для дообучения модели и улучшает многошаговое рассуждение при ответах на вопросы."
                },
                "en": {
                    "title": "Enhancing Knowledgeability in LLMs through Graph Neural Networks",
                    "desc": "This paper explores how Large Language Models (LLMs) can be represented as graphs to better understand their knowledge structure. It introduces a Graph Neural Network (GNN) regression model that estimates the knowledgeability of entities based on their relationships with neighboring entities in the graph. By identifying knowledge homophily, where similar knowledge levels are found among closely related entities, the model helps prioritize which facts to verify for efficient labeling. This approach enhances the active labeling process and improves multi-hop reasoning in applications like question answering."
                },
                "zh": {
                    "title": "利用图神经网络提升知识评估与推理能力",
                    "desc": "本文探讨了如何利用图神经网络回归模型来评估大型语言模型（LLMs）中实体的知识水平，以提高主动标注和多跳推理的效果。研究发现，LLMs的知识在图结构中呈现出相似性，即相邻实体的知识水平往往相似。通过将LLMs的知识映射为图表示，本文分析了实体与其邻居之间的知识关系，并提出了一种基于邻域评分的知识水平估计方法。该方法不仅提高了主动标注的效率，还增强了在推理密集型问答中的多跳路径检索能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26555",
            "title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional\n  Video Generation",
            "url": "https://huggingface.co/papers/2509.26555",
            "abstract": "Stable Cinemetrics introduces a structured evaluation framework for professional video generation, using taxonomies to assess models across specific filmmaking controls.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research.",
            "score": 0,
            "issue_id": 6178,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "6d557fb5b6516154",
            "authors": [
                "Agneet Chatterjee",
                "Rahim Entezari",
                "Maksym Zhuravinskyi",
                "Maksim Lapin",
                "Reshinth Adithyan",
                "Amit Raj",
                "Chitta Baral",
                "Yezhou Yang",
                "Varun Jampani"
            ],
            "affiliations": [
                "Arizona State University",
                "Google DeepMind",
                "Stability AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26555.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#video",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Профессиональная оценка видео-генерации через призму кинематографа",
                    "desc": "Исследователи представили Stable Cinemetrics — фреймворк для оценки качества генерации профессионального видео с помощью AI моделей. Они создали четыре иерархические таксономии (Setup, Event, Lighting, Camera) с 76 детальными параметрами контроля, основанными на практиках киноиндустрии. Было проведено масштабное исследование с участием 80+ профессионалов кино, которые оценили 20 тысяч видео от 10+ моделей. Результаты показали серьёзные пробелы даже у лучших современных моделей, особенно в контроле событий и камеры, что позволило обучить автоматический evaluator для масштабной оценки."
                },
                "en": {
                    "title": "Revolutionizing Video Evaluation with Cinematic Taxonomies",
                    "desc": "Stable Cinemetrics presents a new framework for evaluating AI-generated videos, focusing on professional filmmaking standards. It introduces four hierarchical taxonomies—Setup, Event, Lighting, and Camera—that break down the complex aspects of video generation into 76 specific control nodes. The framework includes a benchmark of prompts based on real-world filmmaking scenarios and an automated system for categorizing these prompts and generating evaluation questions. A large-scale study with film professionals shows that current models still struggle with certain filmmaking controls, particularly in Events and Camera, highlighting the need for improved evaluation methods in video generation."
                },
                "zh": {
                    "title": "稳定电影度量：专业视频生成的新标准",
                    "desc": "Stable Cinemetrics 是一个结构化的评估框架，专门用于专业视频生成。它通过四个层次分明的分类法（设置、事件、照明和摄像）来评估模型在特定电影制作控制方面的表现。该框架定义了76个基于行业实践的细粒度控制节点，并构建了与专业用例对齐的基准测试。通过大规模的人类研究，我们发现当前最强模型在事件和摄像控制方面仍存在显著差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25134",
            "title": "LayerD: Decomposing Raster Graphic Designs into Layers",
            "url": "https://huggingface.co/papers/2509.25134",
            "abstract": "LayerD decomposes raster images into editable layers using iterative extraction and refinement, outperforming existing methods and enabling use with advanced image generators.  \t\t\t\t\tAI-generated summary \t\t\t\t Designers craft and edit graphic designs in a layer representation, but layer-based editing becomes impossible once composited into a raster image. In this work, we propose LayerD, a method to decompose raster graphic designs into layers for re-editable creative workflow. LayerD addresses the decomposition task by iteratively extracting unoccluded foreground layers. We propose a simple yet effective refinement approach taking advantage of the assumption that layers often exhibit uniform appearance in graphic designs. As decomposition is ill-posed and the ground-truth layer structure may not be reliable, we develop a quality metric that addresses the difficulty. In experiments, we show that LayerD successfully achieves high-quality decomposition and outperforms baselines. We also demonstrate the use of LayerD with state-of-the-art image generators and layer-based editing.",
            "score": 0,
            "issue_id": 6178,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "2f81e58539182440",
            "authors": [
                "Tomoyuki Suzuki",
                "Kang-Jun Liu",
                "Naoto Inoue",
                "Kota Yamaguchi"
            ],
            "affiliations": [
                "CyberAgent",
                "Tohoku University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25134.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Превращаем картинку обратно в слои для редактирования",
                    "desc": "LayerD — это метод для декомпозиции растровых графических изображений на отдельные редактируемые слои. Система работает итеративно, последовательно извлекая слои переднего плана, начиная с неперекрытых элементов. Метод использует предположение о том, что слои в графическом дизайне часто имеют однородный внешний вид, что позволяет эффективно уточнять результаты. LayerD превосходит существующие baseline-подходы и может интегрироваться с современными генеративными моделями для послойного редактирования изображений."
                },
                "en": {
                    "title": "LayerD: Transforming Raster Images into Editable Layers",
                    "desc": "LayerD is a novel method designed to decompose raster images into editable layers, facilitating a more flexible graphic design workflow. It employs an iterative process to extract unoccluded foreground layers, which enhances the quality of the decomposition. The method incorporates a refinement strategy that leverages the uniform appearance of layers in graphic designs, addressing the challenges of the ill-posed nature of decomposition. Experimental results indicate that LayerD surpasses existing techniques, making it compatible with advanced image generators and improving layer-based editing capabilities."
                },
                "zh": {
                    "title": "LayerD：图像分解的新方法",
                    "desc": "LayerD是一种将光栅图像分解为可编辑层的方法，采用迭代提取和精炼的技术。该方法通过逐步提取未被遮挡的前景层，解决了图像分解的难题。LayerD还引入了一种简单有效的精炼方法，利用图形设计中层的外观通常是均匀的假设。实验结果表明，LayerD在分解质量上优于现有方法，并且能够与先进的图像生成器和基于层的编辑工具结合使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25082",
            "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial\n  Purification",
            "url": "https://huggingface.co/papers/2509.25082",
            "abstract": "MANI-Pure, a magnitude-adaptive purification framework using diffusion models, effectively suppresses high-frequency adversarial perturbations while preserving low-frequency content, enhancing robust accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Adversarial purification with diffusion models has emerged as a promising defense strategy, but existing methods typically rely on uniform noise injection, which indiscriminately perturbs all frequencies, corrupting semantic structures and undermining robustness. Our empirical study reveals that adversarial perturbations are not uniformly distributed: they are predominantly concentrated in high-frequency regions, with heterogeneous magnitude intensity patterns that vary across frequencies and attack types. Motivated by this observation, we introduce MANI-Pure, a magnitude-adaptive purification framework that leverages the magnitude spectrum of inputs to guide the purification process. Instead of injecting homogeneous noise, MANI-Pure adaptively applies heterogeneous, frequency-targeted noise, effectively suppressing adversarial perturbations in fragile high-frequency, low-magnitude bands while preserving semantically critical low-frequency content. Extensive experiments on CIFAR-10 and ImageNet-1K validate the effectiveness of MANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original classifier, while boosting robust accuracy by 2.15, and achieves the top-1 robust accuracy on the RobustBench leaderboard, surpassing the previous state-of-the-art method.",
            "score": 0,
            "issue_id": 6180,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "a6343f8b608f4cb0",
            "authors": [
                "Xiaoyi Huang",
                "Junwei Wu",
                "Kejia Zhang",
                "Carl Yang",
                "Zhiming Luo"
            ],
            "affiliations": [
                "Emory University",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25082.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#cv",
                    "#security"
                ],
                "emoji": "🔊",
                "ru": {
                    "title": "Адаптивная очистка от adversarial атак через частотный анализ",
                    "desc": "Статья представляет MANI-Pure — новый метод защиты от adversarial атак с использованием диффузионных моделей. Авторы обнаружили, что adversarial возмущения концентрируются в высокочастотных областях изображений с неоднородной интенсивностью. Вместо равномерного добавления шума, MANI-Pure применяет адаптивный частотно-ориентированный шум, подавляя возмущения в высоких частотах и сохраняя семантически важный низкочастотный контент. Метод достиг лучших результатов на RobustBench, улучшив robust accuracy на 2.15% при минимальной потере точности на чистых данных."
                },
                "en": {
                    "title": "Adaptive Purification for Enhanced Robustness Against Adversarial Attacks",
                    "desc": "MANI-Pure is a new framework designed to improve the robustness of machine learning models against adversarial attacks by using diffusion models. It recognizes that adversarial perturbations are mostly found in high-frequency areas and vary in intensity, rather than being evenly distributed. By applying targeted noise that adapts to the magnitude of these perturbations, MANI-Pure effectively reduces harmful high-frequency noise while keeping important low-frequency information intact. This approach has shown significant improvements in robust accuracy on datasets like CIFAR-10 and ImageNet-1K, outperforming previous methods."
                },
                "zh": {
                    "title": "MANI-Pure：自适应净化，提升鲁棒性",
                    "desc": "MANI-Pure是一种基于扩散模型的幅度自适应净化框架，能够有效抑制高频对抗扰动，同时保留低频内容，从而提高模型的鲁棒性。现有的对抗净化方法通常依赖于均匀噪声注入，这会无差别地扰动所有频率，破坏语义结构。我们的研究发现，对抗扰动并不是均匀分布的，而是主要集中在高频区域，并且在不同频率和攻击类型下具有不同的幅度强度模式。MANI-Pure通过利用输入的幅度谱来指导净化过程，适应性地施加针对特定频率的异质噪声，有效抑制脆弱的高频低幅度带中的对抗扰动，同时保留语义上重要的低频内容。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24732",
            "title": "Who invented deep residual learning?",
            "url": "https://huggingface.co/papers/2509.24732",
            "abstract": "A timeline of the evolution of deep residual learning, a key advancement in neural network architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern AI is based on deep artificial neural networks (NNs). As of 2025, the most cited scientific article of the 21st century is an NN paper on deep residual learning with residual connections. Who invented this? We present a timeline of the evolution of deep residual learning.",
            "score": 0,
            "issue_id": 6180,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "0bc8de443fa1708e",
            "authors": [
                "Juergen Schmidhuber"
            ],
            "affiliations": [
                "IDSIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24732.jpg",
            "data": {
                "categories": [
                    "#architecture"
                ],
                "emoji": "🔗",
                "ru": {
                    "title": "История глубокого остаточного обучения: кто изобрёл residual connections",
                    "desc": "Статья представляет хронологию развития глубокого остаточного обучения (deep residual learning) — ключевого прорыва в архитектуре нейронных сетей. Авторы исследуют историю изобретения residual connections, которые позволили обучать очень глубокие нейросети. Работа о residual learning стала самой цитируемой научной статьёй XXI века по состоянию на 2025 год. Статья отвечает на вопрос о том, кто действительно придумал этот революционный подход в современном AI."
                },
                "en": {
                    "title": "Tracing the Evolution of Deep Residual Learning",
                    "desc": "This paper outlines the historical development of deep residual learning, a significant breakthrough in neural network architecture. It highlights the importance of residual connections, which help in training deeper networks by mitigating the vanishing gradient problem. The authors trace the contributions of various researchers and key milestones that led to the widespread adoption of this technique. By 2025, deep residual learning is recognized as a foundational element in modern AI, influencing numerous applications in machine learning."
                },
                "zh": {
                    "title": "深度残差学习的演变历程",
                    "desc": "深度残差学习是神经网络架构中的一个重要进展。本文提供了深度残差学习的发展时间线，展示了其演变过程。残差连接的引入使得训练更深层次的神经网络成为可能，从而提高了模型的性能。到2025年，深度残差学习的相关论文将成为21世纪被引用最多的科学文章。"
                }
            }
        }
    ],
    "link_prev": "2025-09-30.html",
    "link_next": "2025-10-02.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "30.09",
        "en": "09/30",
        "zh": "9月30日"
    },
    "short_date_next": {
        "ru": "02.10",
        "en": "10/02",
        "zh": "10月2日"
    },
    "categories": {
        "#dataset": 10,
        "#data": 6,
        "#benchmark": 17,
        "#agents": 8,
        "#cv": 6,
        "#rl": 9,
        "#rlhf": 3,
        "#rag": 1,
        "#plp": 0,
        "#inference": 4,
        "#3d": 2,
        "#audio": 2,
        "#video": 5,
        "#multimodal": 11,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 15,
        "#robotics": 0,
        "#agi": 1,
        "#games": 5,
        "#interpretability": 4,
        "#reasoning": 14,
        "#transfer_learning": 3,
        "#graphs": 1,
        "#ethics": 4,
        "#security": 2,
        "#optimization": 12,
        "#survey": 2,
        "#diffusion": 5,
        "#alignment": 7,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 0
    }
}