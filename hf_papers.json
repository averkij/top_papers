{
    "date": {
        "ru": "9 апреля",
        "en": "April 9",
        "zh": "4月9日"
    },
    "time_utc": "2025-04-09 05:11",
    "weekday": 2,
    "issue_id": 3140,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.06263",
            "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
            "url": "https://huggingface.co/papers/2504.06263",
            "abstract": "Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.",
            "score": 26,
            "issue_id": 3138,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "3b3365aa60717b2a",
            "authors": [
                "Yiying Yang",
                "Wei Cheng",
                "Sijin Chen",
                "Xianfang Zeng",
                "Jiaxu Zhang",
                "Liao Wang",
                "Gang Yu",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.06263.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "OmniSVG: Революция в генерации векторной графики с помощью ИИ",
                    "desc": "OmniSVG - это новая структура для генерации высококачественной векторной графики SVG с использованием предобученных моделей визуального-языкового машинного обучения. Метод параметризует команды и координаты SVG в дискретные токены, разделяя структурную логику и геометрию для эффективного обучения. Авторы также представили набор данных MMSVG-2M с 2 миллионами аннотированных SVG-файлов для развития этой области. Эксперименты показывают, что OmniSVG превосходит существующие методы и имеет потенциал для интеграции в профессиональные рабочие процессы дизайна SVG."
                },
                "en": {
                    "title": "OmniSVG: Revolutionizing SVG Generation with Vision-Language Models",
                    "desc": "This paper presents OmniSVG, a novel framework for generating high-quality Scalable Vector Graphics (SVG) using pre-trained Vision-Language Models (VLMs). It addresses the limitations of existing methods by producing structured outputs efficiently, avoiding the high computational costs and oversimplification seen in previous approaches. OmniSVG achieves this by converting SVG commands and coordinates into discrete tokens, allowing for a clear separation of structural logic from geometric details. Additionally, the introduction of the MMSVG-2M dataset, containing two million annotated SVG assets, supports the framework's training and evaluation, showcasing its superiority over current SVG generation techniques."
                },
                "zh": {
                    "title": "OmniSVG：高效生成复杂SVG的统一框架",
                    "desc": "本研究提出了一种名为OmniSVG的统一框架，用于生成高质量和复杂的可缩放矢量图形（SVG）。该框架利用预训练的视觉-语言模型（VLMs），通过将SVG命令和坐标参数化为离散标记，实现了高效的端到端多模态SVG生成。OmniSVG将结构逻辑与低级几何解耦，从而在保持复杂SVG结构表现力的同时，降低了计算成本。此外，我们还引入了MMSVG-2M数据集，包含两百万个丰富注释的SVG资产，以推动SVG合成的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05979",
            "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
            "url": "https://huggingface.co/papers/2504.05979",
            "abstract": "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.",
            "score": 23,
            "issue_id": 3137,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "f1195a87ec5b86f1",
            "authors": [
                "Sixiang Chen",
                "Jinbin Bai",
                "Zhuoran Zhao",
                "Tian Ye",
                "Qingyu Shi",
                "Donghao Zhou",
                "Wenhao Chai",
                "Xin Lin",
                "Jianzong Wu",
                "Chao Tang",
                "Shilin Xu",
                "Tao Zhang",
                "Haobo Yuan",
                "Yikang Zhou",
                "Wei Chow",
                "Linfeng Li",
                "Xiangtai Li",
                "Lei Zhu",
                "Lu Qi"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology (GZ)",
                "University of Washington",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05979.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#architecture",
                    "#multimodal",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "GPT-4o: Новый рубеж в унифицированной генерации изображений",
                    "desc": "Статья посвящена исследованию возможностей генерации изображений моделью GPT-4o. Авторы проводят эмпирическое сравнение GPT-4o с ведущими открытыми и коммерческими моделями в более чем 20 задачах генерации. Оценка охватывает четыре основные категории: текст-в-изображение, изображение-в-изображение, изображение-в-3D и изображение-в-X. На основе анализа выявляются сильные и слабые стороны GPT-4o в различных условиях, а также определяются перспективные направления для будущих унифицированных генеративных моделей."
                },
                "en": {
                    "title": "Unifying Image and Text Generation with GPT-4o",
                    "desc": "This paper explores the advancements in image generation, focusing on the capabilities of the GPT-4o model. It conducts a thorough evaluation of GPT-4o's performance in various generative tasks, including text-to-image and image-to-3D generation. The study benchmarks GPT-4o against other leading models, revealing its strengths and weaknesses in multimodal generation. The findings suggest future directions for improving unified generative architectures, particularly in terms of design and data utilization."
                },
                "zh": {
                    "title": "探索统一生成模型的未来方向",
                    "desc": "本文探讨了图像生成领域的最新进展，特别是GPT-4o模型在图像生成方面的能力。我们对其进行了实证研究，并与领先的开源和商业模型进行了基准测试，涵盖了文本到图像、图像到图像、图像到3D和图像到X生成等四个主要类别。分析结果揭示了GPT-4o在不同设置下的优缺点，并将其置于生成建模的更广泛演变中。通过这项研究，我们识别出未来统一生成模型的有希望的方向，强调了架构设计和数据扩展的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02160",
            "title": "Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation",
            "url": "https://huggingface.co/papers/2504.02160",
            "abstract": "Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.",
            "score": 9,
            "issue_id": 3139,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "511e3ea71050e14e",
            "authors": [
                "Shaojin Wu",
                "Mengqi Huang",
                "Wenxu Wu",
                "Yufeng Cheng",
                "Fei Ding",
                "Qian He"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02160.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#synthetic",
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная генерация изображений с множеством объектов",
                    "desc": "Статья описывает новый подход к генерации изображений с несколькими объектами. Авторы предлагают пайплайн для синтеза согласованных данных, используя возможности диффузионных трансформеров. Они также представляют модель UNO с прогрессивным кросс-модальным выравниванием и универсальным позиционным кодированием. Эксперименты показывают, что метод обеспечивает высокую согласованность и контролируемость при генерации изображений с одним или несколькими объектами."
                },
                "en": {
                    "title": "Enhancing Multi-Subject Image Generation with Consistent Data Synthesis",
                    "desc": "This paper addresses the challenges of generating images with multiple subjects by proposing a new data synthesis pipeline. The authors utilize diffusion transformers to create high-consistency paired data for both single and multi-subject scenarios. They introduce a novel model called UNO, which incorporates cross-modal alignment and rotary position embedding to enhance the generation process. Experimental results demonstrate that their approach maintains high consistency and controllability in image generation tasks."
                },
                "zh": {
                    "title": "高一致性多主题生成的创新方法",
                    "desc": "本研究探讨了在图像生成中，如何解决以主题为驱动的生成面临的数据可扩展性和主题扩展性挑战。我们提出了一种高一致性的数据合成管道，利用扩散变换器的内在生成能力，生成高一致性的多主题配对数据。此外，我们引入了UNO模型，结合了渐进的跨模态对齐和通用旋转位置嵌入，能够在单主题和多主题生成中保持高一致性和可控性。通过大量实验，我们的方法在生成质量和控制能力上均表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02810",
            "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
            "url": "https://huggingface.co/papers/2504.02810",
            "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.",
            "score": 6,
            "issue_id": 3138,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "5a9b5f817a1c09b6",
            "authors": [
                "Haowei Lin",
                "Xiangyu Wang",
                "Ruilin Yan",
                "Baizhou Huang",
                "Haotian Ye",
                "Jianhua Zhu",
                "Zihao Wang",
                "James Zou",
                "Jianzhu Ma",
                "Yitao Liang"
            ],
            "affiliations": [
                "Computer Science Department, Stanford University, California, United States",
                "Department of Electronic Engineering, Tsinghua University, Beijing, China",
                "Institute for AI Industry Research, Tsinghua University, Beijing, China",
                "Institute for Artificial Intelligence, Peking University, Beijing, China",
                "Wangxuan institute of computer technology, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02810.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "KUMO: новый способ оценить истинные способности ИИ к рассуждению",
                    "desc": "Исследователи представили KUMO - новую систему для оценки способностей больших языковых моделей (LLM) к рассуждению. KUMO генерирует разнообразные задачи на рассуждение, комбинируя LLM с символьными движками. Система позволяет создавать новые задачи в различных областях, заставляя модели демонстрировать реальное обобщение, а не запоминание. Тестирование 23 современных LLM на 5000 задачах показало, что многие из них превзошли уровень студентов университетов в простых задачах на рассуждение."
                },
                "en": {
                    "title": "KUMO: Unveiling True Reasoning in LLMs",
                    "desc": "This paper introduces KUMO, a new framework for evaluating the reasoning abilities of large language models (LLMs). KUMO generates diverse reasoning tasks that require models to demonstrate true understanding rather than simple recall from their training data. By combining LLMs with symbolic engines, it creates adjustable tasks that challenge models across various domains. The evaluation shows that many LLMs can outperform university students on easier tasks and achieve comparable performance on more complex reasoning challenges, highlighting KUMO's effectiveness as a reliable assessment tool."
                },
                "zh": {
                    "title": "KUMO：评估大型语言模型推理能力的新工具",
                    "desc": "本文探讨了大型语言模型（LLMs）是否真正具备推理能力，还是仅仅从其庞大的训练数据集中回忆答案。为了解决这一问题，作者提出了KUMO，一个专门用于评估LLMs推理能力的生成评估框架。KUMO结合了LLMs和符号引擎，动态生成多样化的推理任务，促进模型展示真正的泛化能力。通过对23个最先进的LLMs进行评估，结果显示许多模型在简单推理任务上超越了大学生的表现，而在复杂推理挑战中也达到了大学水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05594",
            "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model",
            "url": "https://huggingface.co/papers/2504.05594",
            "abstract": "Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.",
            "score": 5,
            "issue_id": 3138,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "7da2f86ad5e0bfc2",
            "authors": [
                "Qi Mao",
                "Lan Chen",
                "Yuchao Gu",
                "Mike Zheng Shou",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore",
                "State Key Laboratory of Media Convergence and Communication, Communication University of China",
                "University of California at Merced",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05594.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Баланс точности и редактируемости в редактировании изображений на основе текста",
                    "desc": "Статья представляет UnifyEdit - метод редактирования изображений на основе текста, который балансирует между сохранением структуры и редактируемостью. Авторы разработали два ограничения на основе внимания: сохранение самовнимания для структурной точности и выравнивание перекрестного внимания для улучшения редактируемости. Для решения проблемы конфликта градиентов введен адаптивный планировщик временных шагов, динамически регулирующий влияние ограничений. Эксперименты подтверждают эффективность подхода в достижении баланса между сохранением структуры и выравниванием текста в различных задачах редактирования."
                },
                "en": {
                    "title": "Achieving Perfect Balance in Text-Based Image Editing with UnifyEdit",
                    "desc": "This paper presents UnifyEdit, a novel method for text-based image editing that aims to balance fidelity and editability. Traditional approaches often struggle with over- or under-editing due to conflicting constraints in attention mechanisms. UnifyEdit introduces a unified framework that employs self-attention and cross-attention constraints to maintain structural fidelity and enhance text alignment, respectively. An adaptive time-step scheduler is also proposed to dynamically manage the influence of these constraints, ensuring optimal performance in various editing tasks."
                },
                "zh": {
                    "title": "平衡保真度与可编辑性的创新方法",
                    "desc": "在基于文本的图像编辑中，平衡保真度和可编辑性非常重要。现有方法通常依赖于注意力机制来保持结构，但缺乏统一的机制来平衡这两个目标。我们提出了UnifyEdit，这是一种无调优的方法，通过扩散潜在优化实现保真度和可编辑性的平衡。我们开发了自注意力和交叉注意力约束，并引入自适应时间步调度器，以动态调整这些约束的影响，从而优化编辑效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00043",
            "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation",
            "url": "https://huggingface.co/papers/2504.00043",
            "abstract": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.",
            "score": 4,
            "issue_id": 3137,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 марта",
                "en": "March 30",
                "zh": "3月30日"
            },
            "hash": "2b2bfdd590c5394d",
            "authors": [
                "Jixuan Leng",
                "Chengsong Huang",
                "Langlin Huang",
                "Bill Yuchen Lin",
                "William W. Cohen",
                "Haohan Wang",
                "Jiaxin Huang"
            ],
            "affiliations": [
                "CMU",
                "UIUC",
                "UW",
                "WUSTL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00043.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Кроссворды как инструмент оценки искусственного интеллекта",
                    "desc": "CrossWordBench - это новый метод оценки способностей больших языковых моделей (LLM) и больших визуально-языковых моделей (LVLM) к рассуждению. Он использует кроссворды как задачу, требующую мультимодального соблюдения семантических ограничений из текстовых подсказок и пересекающихся ограничений из визуальных структур сетки. Оценка более 20 моделей показала, что LLM с возможностями рассуждения значительно превосходят модели без таких возможностей. Результаты также выявили трудности LVLM с этой задачей, демонстрируя сильную корреляцию между их способностью решать головоломки и точностью разбора сетки."
                },
                "en": {
                    "title": "CrossWordBench: Evaluating Reasoning in LLMs and LVLMs with Crossword Puzzles",
                    "desc": "This paper presents CrossWordBench, a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) using crossword puzzles. The benchmark focuses on the interaction between text-based clues and visual grid structures, requiring models to adhere to both semantic and intersectional constraints. The study shows that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints, while LVLMs face challenges linked to their grid-parsing accuracy. Overall, the findings highlight the limitations of current models in reasoning tasks and suggest a novel approach for multimodal evaluation."
                },
                "zh": {
                    "title": "跨模态推理的新基准：CrossWordBench",
                    "desc": "现有的大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的推理评估框架主要集中在文本推理或视觉语言理解能力上，缺乏文本与视觉之间的动态互动。为了解决这个问题，我们提出了CrossWordBench，这是一个通过填字游戏评估LLMs和LVLMs推理能力的基准，要求在文本线索和视觉网格结构的语义约束下进行多模态推理。CrossWordBench利用可控的拼图生成框架，生成多种格式的拼图，并提供从直接解谜到互动模式的不同评估策略。我们的评估结果显示，推理能力强的LLMs在利用交叉字母约束方面显著优于非推理模型，而LVLMs在此任务中表现不佳，其解谜表现与网格解析准确性之间存在强相关性。"
                }
            }
        }
    ],
    "link_prev": "2025-04-08.html",
    "link_next": "2025-04-10.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4月8日"
    },
    "short_date_next": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4月10日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了变压器在生成一分钟视频时的挑战，因为自注意力层在长上下文中效率低下。作者尝试使用测试时训练（TTT）层，其隐藏状态本身可以是神经网络，从而更具表现力。将TTT层加入预训练的变压器后，可以从文字故事板生成一分钟视频。为了验证这一概念，作者根据《猫和老鼠》动画片编制了一个数据集。与其他基线方法相比，TTT层生成的视频更连贯，能够讲述复杂的故事。",
        "title": "One-Minute Video Generation with Test-Time Training",
        "pinyin": "这篇文章讨论了变压器在生成一分钟视频时的挑战，因为自注意力层在长上下文中效率低下。作者尝试使用测试时训练（TTT）层，其隐藏状态本身可以是神经网络，从而更具表现力。将TTT层加入预训练的变压器后，可以从文字故事板生成一分钟视频。为了验证这一概念，作者根据《猫和老鼠》动画片编制了一个数据集。与其他基线方法相比，TTT层生成的视频更连贯，能够讲述复杂的故事。\n\nzhè piān wén zhāng tǎo lùn le biàn yā qì zài shēng chéng yī fēn zhōng shì pǐn de cháo zhàn, yīn wèi zì zhù yì lì céng zài cháng shàng xià wén zhōng xiào lǜ dī xià. zuò zhě cháng shì shǐ yòng cè shì shí xùn liàn (TTT) céng, qí yǐn cáng zhuàng tài běn shēn kě yǐ shì shén jīng wǎng luò, cóng ér gèng jù biǎo xiàn lì. jiāng TTT céng jiā rù yù xùn liàn de biàn yā qì hòu, kě yǐ cóng wén zì gù shì bǎn shēng chéng yī fēn zhōng shì pǐn. wèi le yàn zhèng zhè yī gài niàn, zuò zhě gēn jù 《māo hé lǎo shǔ》 dòng huà piān biān zhì le yī gè shù jù jí. yǔ qí tā jī xiàn fāng fǎ biǎo bǐ, TTT céng shēng chéng de shì pǐn gèng lián gǔ, néng gòu jiǎng shù fú zà de gù shì.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"变压器\", \"pinyin\": \"biàn yā qì\", \"trans\": \"transformer\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"自注意力\", \"pinyin\": \"zì zhù yì lì\", \"trans\": \"self-attention\"},\n    {\"word\": \"层\", \"pinyin\": \"céng\", \"trans\": \"layer\"},\n    {\"word\": \"长\", \"pinyin\": \"cháng\", \"trans\": \"long\"},\n    {\"word\": \"上下文\", \"pinyin\": \"shàng xià wén\", \"trans\": \"context\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiào lǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"低下\", \"pinyin\": \"dī xià\", \"trans\": \"low\"},\n    {\"word\": \"尝试\", \"pinyin\": \"cháng shì\", \"trans\": \"attempt\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"测试时训练\", \"pinyin\": \"cè shì shí xùn liàn\", \"trans\": \"test-time training\"},\n    {\"word\": \"隐藏\", \"pinyin\": \"yǐn cáng\", \"trans\": \"hidden\"},\n    {\"word\": \"状态\", \"pinyin\": \"zhuàng tài\", \"trans\": \"state\"},\n    {\"word\": \"本身\", \"pinyin\": \"běn shēn\", \"trans\": \"itself\"},\n    {\"word\": \"神经网络\", \"pinyin\": \"shén jīng wǎng luò\", \"trans\": \"neural network\"},\n    {\"word\": \"表现力\", \"pinyin\": \"biǎo xiàn lì\", \"trans\": \"expressiveness\"},\n    {\"word\": \"加入\", \"pinyin\": \"jiā rù\", \"trans\": \"add\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-trained\"},\n    {\"word\": \"文字\", \"pinyin\": \"wén zì\", \"trans\": \"text\"},\n    {\"word\": \"故事板\", \"pinyin\": \"gù shì bǎn\", \"trans\": \"storyboard\"},\n    {\"word\": \"验证\", \"pinyin\": \"yàn zhèng\", \"trans\": \"validate\"},\n    {\"word\": \"概念\", \"pinyin\": \"gài niàn\", \"trans\": \"concept\"},\n    {\"word\": \"根据\", \"pinyin\": \"gēn jù\", \"trans\": \"based on\"},\n    {\"word\": \"《猫和老鼠》\", \"pinyin\": \"《māo hé lǎo shǔ》\", \"trans\": \"Tom and Jerry\"},\n    {\"word\": \"动画片\", \"pinyin\": \"dòng huà piàn\", \"trans\": \"cartoon\"},\n    {\"word\": \"编制\", \"pinyin\": \"biān zhì\", \"trans\": \"compile\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"基线\", \"pinyin\": \"jī xiàn\", \"trans\": \"baseline\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"相比\", \"pinyin\": \"xiāng bǐ\", \"trans\": \"compared to\"},\n    {\"word\": \"连贯\", \"pinyin\": \"lián guàn\", \"trans\": \"coherent\"},\n    {\"word\": \"讲述\", \"pinyin\": \"jiǎng shù\", \"trans\": \"narrate\"},\n    {\"word\": \"复杂\", \"pinyin\": \"fù zá\", \"trans\": \"complex\"}\n]",
        "trans": "This article discusses the challenges faced by transformers in generating a one-minute video due to the inefficiency of self-attention layers in long contexts. The authors attempt to use Test-Time Training (TTT) layers, whose hidden states can themselves be neural networks, making them more expressive. By incorporating TTT layers into pre-trained transformers, it is possible to generate a one-minute video from a textual storyboard. To validate this concept, the authors created a dataset based on the \"Tom and Jerry\" cartoon. Compared to other baseline methods, the videos generated by TTT layers are more coherent and capable of telling complex stories.",
        "update_ts": "2025-04-08 09:12"
    }
}