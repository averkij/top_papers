{
    "date": {
        "ru": "29 сентября",
        "en": "September 29",
        "zh": "9月29日"
    },
    "time_utc": "2025-09-29 02:22",
    "weekday": 0,
    "issue_id": 6129,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.21760",
            "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
            "url": "https://huggingface.co/papers/2509.21760",
            "abstract": "A pre-trained video diffusion transformer, UniVid, is fine-tuned to handle diverse vision tasks without task-specific modifications, demonstrating cross-modal and cross-source generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.",
            "score": 4,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "e74140613312626b",
            "authors": [
                "Lan Chen",
                "Yuchao Gu",
                "Qi Mao"
            ],
            "affiliations": [
                "Communication University of China",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21760.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#transfer_learning"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Один видео трансформер для всех задач компьютерного зрения",
                    "desc": "Исследователи предлагают UniVid - фреймворк, который адаптирует предобученную модель генерации видео для решения различных задач компьютерного зрения без специфических модификаций. Задачи представляются как визуальные предложения, где контекстная последовательность определяет как саму задачу, так и ожидаемую модальность вывода. UniVid демонстрирует кросс-модальную и кросс-доменную генерализацию, работая с изображениями и видео, а также переключаясь между задачами понимания и генерации простым изменением порядка визуальной последовательности. Модель показывает хорошие результаты даже при обучении только на естественных видеоданных, что подчеркивает потенциал предобученных видео диффузионных трансформеров как универсальной основы для задач компьютерного зрения."
                },
                "en": {
                    "title": "UniVid: A Unified Framework for Diverse Vision Tasks",
                    "desc": "The paper introduces UniVid, a pre-trained video diffusion transformer that can be fine-tuned for various vision tasks without needing specific modifications for each task. It leverages the concept of visual sentences, where tasks are represented in a sequence that guides the model's output. UniVid demonstrates strong generalization capabilities across different modalities, such as images and videos, and can adapt to tasks from various sources without requiring extensive pre-training. This approach shows that pre-trained video generation models can provide a scalable and unified framework for handling diverse vision challenges."
                },
                "zh": {
                    "title": "UniVid：统一视觉任务的强大工具",
                    "desc": "UniVid是一个经过预训练的视频扩散变换器，能够在不进行特定任务修改的情况下，适应多种视觉任务。它通过将任务表示为视觉句子，利用上下文序列来定义任务和期望的输出形式。UniVid在跨模态推理和跨源任务中表现出良好的泛化能力，尽管仅在自然视频数据上进行训练。该研究表明，预训练的视频生成模型可以作为视觉建模的统一和可扩展基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22496",
            "title": "Where MLLMs Attend and What They Rely On: Explaining Autoregressive\n  Token Generation",
            "url": "https://huggingface.co/papers/2509.22496",
            "abstract": "EAGLE is a lightweight framework that explains token generation in multimodal large language models by attributing tokens to visual regions and quantifying the influence of language and perceptual evidence.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE.",
            "score": 2,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "ad53ca1f5bbc2302",
            "authors": [
                "Ruoyu Chen",
                "Xiaoqing Guo",
                "Kangwei Liu",
                "Siyuan Liang",
                "Shiming Liu",
                "Qunli Zhang",
                "Hua Zhang",
                "Xiaochun Cao"
            ],
            "affiliations": [
                "Department of Computer Science, Hong Kong Baptist University",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "RAMS Lab, Huawei Technologies Co., Ltd.",
                "RAMS Lab, Munich Research Center, Huawei Technologies Düsseldorf GmbH",
                "School of Computing, NUS",
                "School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University",
                "School of Cyber Security, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22496.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#inference",
                    "#multimodal",
                    "#interpretability",
                    "#open_source"
                ],
                "emoji": "🦅",
                "ru": {
                    "title": "Объясняем каждый токен: как MLLM видят и говорят",
                    "desc": "EAGLE - это легковесный фреймворк для объяснения процесса генерации токенов в мультимодальных больших языковых моделях. Система анализирует, как каждый сгенерированный токен связан с визуальными областями изображения и языковыми приоритетами модели. Фреймворк использует объективную функцию, которая объединяет оценки достаточности и необходимости для точной атрибуции токенов к регионам изображения. Эксперименты показывают, что EAGLE превосходит существующие методы в точности, локализации и диагностике галлюцинаций при меньших требованиях к GPU памяти."
                },
                "en": {
                    "title": "EAGLE: Unraveling Token Generation in Multimodal Models",
                    "desc": "EAGLE is a new framework designed to explain how multimodal large language models (MLLMs) generate tokens by linking them to specific visual areas. It quantifies the influence of both language and visual information on token generation, enhancing the interpretability of these models. The framework uses an objective function to measure how essential and sufficient different visual regions are for generating specific tokens, optimizing this through a greedy search method. EAGLE has been shown to outperform existing methods in terms of accuracy and efficiency while using less computational resources, making it a practical tool for understanding MLLM behavior."
                },
                "zh": {
                    "title": "EAGLE：提升多模态语言模型可解释性的轻量级框架",
                    "desc": "EAGLE是一个轻量级框架，用于解释多模态大型语言模型中的令牌生成。它通过将令牌归因于视觉区域，并量化语言和感知证据的影响，来提高模型的可解释性。该框架引入了一个目标函数，统一了充分性和必要性评分，并通过贪婪搜索优化稀疏图像区域。EAGLE在多个开源多模态大型语言模型上的实验表明，其在可信度、定位和幻觉诊断方面均优于现有方法，同时显著减少了GPU内存的需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22186",
            "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document Parsing",
            "url": "https://huggingface.co/papers/2509.22186",
            "abstract": "MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead.",
            "score": 2,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "136108b6a4721246",
            "authors": [
                "Junbo Niu",
                "Zheng Liu",
                "Zhuangcheng Gu",
                "Bin Wang",
                "Linke Ouyang",
                "Zhiyuan Zhao",
                "Tao Chu",
                "Tianyao He",
                "Fan Wu",
                "Qintong Zhang",
                "Zhenjiang Jin",
                "Guang Liang",
                "Rui Zhang",
                "Wenzheng Zhang",
                "Yuan Qu",
                "Zhifei Ren",
                "Yuefeng Sun",
                "Yuanhong Zheng",
                "Dongsheng Ma",
                "Zirui Tang",
                "Boyu Niu",
                "Ziyang Miao",
                "Hejun Dong",
                "Siyi Qian",
                "Junyuan Zhang",
                "Jingzhou Chen",
                "Fangdong Wang",
                "Xiaomeng Zhao",
                "Liqun Wei",
                "Wei Li",
                "Shasha Wang",
                "Ruiliang Xu",
                "Yuanyuan Cao",
                "Lu Chen",
                "Qianqian Wu",
                "Huaiyu Gu",
                "Lindong Lu",
                "Keming Wang",
                "Dechen Lin",
                "Guanlin Shen",
                "Xuanhe Zhou",
                "Linfeng Zhang",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Jiaqi Wang",
                "Bo Zhang",
                "Lei Bai",
                "Pei Chu",
                "Weijia Li",
                "Jiang Wu",
                "Lijun Wu",
                "Zhenxiang Li",
                "Guangyu Wang",
                "Zhongying Tu",
                "Chao Xu",
                "Kai Chen",
                "Yu Qiao",
                "Bowen Zhou",
                "Dahua Lin",
                "Wentao Zhang",
                "Conghui He"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22186.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#data"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "От общего к частному: эффективный парсинг документов в два этапа",
                    "desc": "Представлена MinerU2.5 - vision-language модель с 1.2 миллиардами параметров для парсинга документов. Модель использует двухэтапную стратегию: сначала анализирует общую структуру документа на изображениях низкого разрешения, затем распознает содержимое на фрагментах высокого разрешения. Такой подход позволяет эффективно обрабатывать сложные элементы как текст, формулы и таблицы при меньших вычислительных затратах. MinerU2.5 достигает state-of-the-art результатов на множестве бенчмарков, превосходя как универсальные, так и специализированные модели."
                },
                "en": {
                    "title": "Efficient Document Parsing with MinerU2.5",
                    "desc": "MinerU2.5 is a vision-language model designed for document parsing, featuring 1.2 billion parameters. It utilizes a coarse-to-fine parsing strategy that separates the analysis of document layout from the recognition of content, enhancing efficiency. The model first conducts layout analysis on downsampled images to identify structural elements, then focuses on detailed content recognition using high-resolution crops. This innovative approach allows MinerU2.5 to achieve top performance on various benchmarks while reducing computational costs compared to other models."
                },
                "zh": {
                    "title": "高效文档解析的新标杆",
                    "desc": "MinerU2.5是一种具有12亿参数的文档解析视觉语言模型，采用粗到细的解析策略，实现了最先进的识别准确率和计算效率。该模型的第一阶段在降采样图像上进行高效的布局分析，以识别结构元素，从而避免处理高分辨率输入的计算开销。第二阶段则在原始图像中提取的原生分辨率裁剪图上进行目标内容识别，保留了密集文本、复杂公式和表格中的细节。通过开发全面的数据引擎，MinerU2.5能够生成多样化的大规模训练语料库，支持预训练和微调，最终在多个基准测试中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21574",
            "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
            "url": "https://huggingface.co/papers/2509.21574",
            "abstract": "X-Streamer is a unified multimodal framework using dual-transformer architecture for real-time, open-ended interactions across text, speech, and video, leveraging large language-speech models and autoregressive diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.",
            "score": 1,
            "issue_id": 6129,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "addda28b1d3ecc10",
            "authors": [
                "You Xie",
                "Tianpei Gu",
                "Zenan Li",
                "Chenxu Zhang",
                "Guoxian Song",
                "Xiaochen Zhao",
                "Chao Liang",
                "Jianwen Jiang",
                "Hongyi Xu",
                "Linjie Luo"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21574.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#architecture",
                    "#multimodal",
                    "#agi",
                    "#interpretability",
                    "#games",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Цифровой человек в реальном времени из одного портрета",
                    "desc": "В статье представлен X-Streamer — фреймворк для создания цифровых человеческих агентов, способных к бесконечным взаимодействиям через текст, речь и видео в единой архитектуре. Система использует dual-transformer архитектуру Thinker-Actor, где модуль Thinker воспринимает и анализирует потоковые пользовательские входы, а Actor переводит скрытые состояния в синхронизированные мультимодальные потоки в реальном времени. Для генерации используется chunk-wise autoregressive diffusion модель, которая производит временно выровненные мультимодальные ответы с дискретными текстовыми и аудио токенами и непрерывными видео латентами. Система работает в реальном времени на двух GPU A100, обеспечивая многочасовые консистентные видеочаты из произвольных портретов."
                },
                "en": {
                    "title": "X-Streamer: Real-Time Multimodal Interactions Redefined",
                    "desc": "X-Streamer is a cutting-edge framework that combines text, speech, and video interactions using a dual-transformer architecture. It allows for real-time communication by transforming a static image into a dynamic digital human capable of engaging in endless conversations. The framework consists of two main components: the Thinker, which processes and understands user inputs, and the Actor, which generates synchronized multimodal outputs. By utilizing advanced models and attention mechanisms, X-Streamer ensures smooth and coherent interactions, making it a significant advancement in creating interactive digital agents."
                },
                "zh": {
                    "title": "X-Streamer：实时多模态互动的新纪元",
                    "desc": "X-Streamer是一个统一的多模态框架，采用双变换器架构，能够实现文本、语音和视频之间的实时互动。它通过流式多模态输入，将静态肖像转变为持久的智能视听互动。框架的核心是Thinker-Actor双变换器，Thinker模块负责感知和推理用户输入，而Actor模块则将这些信息实时转换为同步的多模态输出。X-Streamer在两块A100 GPU上实时运行，能够支持数小时的持续视频聊天，推动互动数字人类的统一世界建模。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22244",
            "title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image\n  Editing",
            "url": "https://huggingface.co/papers/2509.22244",
            "abstract": "FlashEdit enables real-time, high-fidelity image editing with diffusion models through efficient inversion, background preservation, and localized attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150times speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit.",
            "score": 0,
            "issue_id": 6129,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "ea3d5abcdd7f6370",
            "authors": [
                "Junyi Wu",
                "Zhiteng Li",
                "Haotong Qin",
                "Xiaohong Liu",
                "Linghe Kong",
                "Yulun Zhang",
                "Xiaokang Yang"
            ],
            "affiliations": [
                "ETH Zurich",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22244.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#inference",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Мгновенное редактирование изображений с диффузионными моделями",
                    "desc": "FlashEdit — это новая система для редактирования изображений с использованием диффузионных моделей в реальном времени. Основные инновации включают одношаговый процесс инверсии и редактирования (OSIE), технику защиты фона (BG-Shield) и механизм разреженного пространственного кросс-внимания (SSCA). Система обеспечивает высококачественное редактирование изображений менее чем за 0.2 секунды, что в 150 раз быстрее традиционных методов. FlashEdit сохраняет консистентность фона и структурную целостность изображения при локализованных изменениях."
                },
                "en": {
                    "title": "Real-Time Image Editing Revolutionized with FlashEdit",
                    "desc": "FlashEdit is a cutting-edge framework that allows for real-time image editing using diffusion models, significantly improving the editing speed and quality. It introduces a One-Step Inversion-and-Editing (OSIE) pipeline that eliminates the need for slow iterative processes, enabling quick modifications. The Background Shield (BG-Shield) technique ensures that the background remains intact while only the desired features are altered, enhancing the overall visual coherence. Additionally, the Sparsified Spatial Cross-Attention (SSCA) mechanism allows for precise edits by minimizing unwanted changes to the background, achieving edits in under 0.2 seconds, which is over 150 times faster than previous methods."
                },
                "zh": {
                    "title": "FlashEdit：实时高保真图像编辑的新突破",
                    "desc": "FlashEdit 是一个新颖的框架，能够实现实时、高保真的图像编辑。它通过三个关键创新提高了效率：首先，采用了一种一步反演与编辑（OSIE）流程，避免了耗时的迭代过程；其次，背景保护技术（BG-Shield）确保了背景的一致性，仅在编辑区域内进行特征修改；最后，稀疏空间交叉注意力（SSCA）机制确保了精确的局部编辑，防止语义信息泄漏到背景中。实验结果表明，FlashEdit 在保持背景一致性和结构完整性的同时，编辑速度超过0.2秒，比之前的多步骤方法快150倍以上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19768",
            "title": "CHURRO: Making History Readable with an Open-Weight Large\n  Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition",
            "url": "https://huggingface.co/papers/2509.19768",
            "abstract": "CHURRO, a 3B-parameter open-weight vision-language model, outperforms existing models in historical text recognition using the largest dataset to date, CHURRO-DS, and is more cost-effective.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate text recognition for historical documents can greatly advance the study and preservation of cultural heritage. Existing vision-language models (VLMs), however, are designed for modern, standardized texts and are not equipped to read the diverse languages and scripts, irregular layouts, and frequent degradation found in historical materials.   This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for historical text recognition. The model is trained on CHURRO-DS, the largest historical text recognition dataset to date. CHURRO-DS unifies 155 historical corpora comprising 99,491 pages, spanning 22 centuries of textual heritage across 46 language clusters, including historical variants and dead languages.   We evaluate several open-weight and closed VLMs and optical character recognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all other VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and 70.1% (handwritten) normalized Levenshtein similarity, surpassing the second-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being 15.5 times more cost-effective.   By releasing the model and dataset, we aim to enable community-driven research to improve the readability of historical texts and accelerate scholarship.",
            "score": 0,
            "issue_id": 6129,
            "pub_date": "2025-09-24",
            "pub_date_card": {
                "ru": "24 сентября",
                "en": "September 24",
                "zh": "9月24日"
            },
            "hash": "8759baf8bb974573",
            "authors": [
                "Sina J. Semnani",
                "Han Zhang",
                "Xinyan He",
                "Merve Tekgürler",
                "Monica S. Lam"
            ],
            "affiliations": [
                "Stanford University, Stanford, CA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19768.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#science",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "📜",
                "ru": {
                    "title": "CHURRO: AI для чтения древних текстов и сохранения культурного наследия",
                    "desc": "CHURRO - это специализированная vision-language модель с 3 миллиардами параметров, созданная для распознавания текста в исторических документах. Модель обучена на крупнейшем датасете CHURRO-DS, включающем 99,491 страниц исторических текстов на 46 языковых кластерах за 22 века. CHURRO превосходит другие VLM по точности распознавания печатного (82.3%) и рукописного (70.1%) текста, опережая Gemini 2.5 Pro на 1.4% и 6.5% соответственно. При этом модель в 15.5 раз более экономически эффективна и имеет открытые веса для исследовательского сообщества."
                },
                "en": {
                    "title": "Unlocking Historical Texts with CHURRO",
                    "desc": "The paper introduces CHURRO, a vision-language model specifically designed for recognizing historical texts. It is trained on CHURRO-DS, the largest dataset for this purpose, which includes a wide variety of historical documents across multiple languages and scripts. CHURRO outperforms existing models in accuracy and cost-effectiveness, achieving high similarity scores in recognizing both printed and handwritten texts. By making CHURRO and its dataset publicly available, the authors aim to foster research that enhances the understanding and preservation of cultural heritage."
                },
                "zh": {
                    "title": "CHURRO：历史文本识别的新突破",
                    "desc": "CHURRO是一种具有30亿参数的开放权重视觉语言模型，专门用于历史文本识别。它在CHURRO-DS数据集上训练，该数据集是迄今为止最大的历史文本识别数据集，包含来自22个世纪的155个历史语料库。CHURRO在识别印刷和手写文本方面的表现优于所有现有模型，且成本效益更高。通过发布该模型和数据集，我们希望促进社区驱动的研究，提升历史文本的可读性。"
                }
            }
        }
    ],
    "link_prev": "2025-09-26.html",
    "link_next": "2025-09-30.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "26.09",
        "en": "09/26",
        "zh": "9月26日"
    },
    "short_date_next": {
        "ru": "30.09",
        "en": "09/30",
        "zh": "9月30日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}