{
    "date": {
        "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 3",
        "zh": "3æœˆ3æ—¥"
    },
    "time_utc": "2025-03-03 12:19",
    "weekday": 0,
    "issue_id": 2495,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.20730",
            "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
            "url": "https://huggingface.co/papers/2502.20730",
            "abstract": "Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.",
            "score": 14,
            "issue_id": 2486,
            "pub_date": "2025-02-28",
            "pub_date_card": {
                "ru": "28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 28",
                "zh": "2æœˆ28æ—¥"
            },
            "hash": "e9ef168e304ec240",
            "authors": [
                "Zhuoqun Li",
                "Haiyang Yu",
                "Xuanang Chen",
                "Hongyu Lin",
                "Yaojie Lu",
                "Fei Huang",
                "Xianpei Han",
                "Yongbin Li",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "Tongyi Lab",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20730.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SolutionBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ SolutionRAG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²ÑƒÑ…Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SolutionRAG Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° SolutionBench. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing Engineering Design with SolutionRAG",
                    "desc": "This paper addresses the need for effective solutions in complex engineering design tasks, which have been overlooked in previous research on retrieval-augmented generation (RAG). It introduces a new benchmark called SolutionBench, aimed at evaluating the generation of feasible solutions under multiple constraints. The authors propose a novel system named SolutionRAG, which utilizes tree-based exploration and bi-point thinking to improve solution reliability. Experimental results show that SolutionRAG outperforms existing methods, indicating its potential to automate and enhance the design process in engineering applications."
                },
                "zh": {
                    "title": "æå‡å¤æ‚å·¥ç¨‹è®¾è®¡çš„è‡ªåŠ¨åŒ–ä¸å¯é æ€§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºSolutionBenchï¼Œç”¨äºè¯„ä¼°ç³»ç»Ÿåœ¨ç”Ÿæˆå¤æ‚å·¥ç¨‹é—®é¢˜çš„å®Œæ•´å’Œå¯è¡Œè§£å†³æ–¹æ¡ˆæ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°ç³»ç»ŸSolutionRAGï¼Œåˆ©ç”¨æ ‘å½¢æ¢ç´¢å’ŒåŒç‚¹æ€ç»´æœºåˆ¶æ¥ç”Ÿæˆå¯é çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¤§é‡å®éªŒç»“æœï¼ŒSolutionRAGåœ¨SolutionBenchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­æé«˜å¤æ‚å·¥ç¨‹è§£å†³æ–¹æ¡ˆè®¾è®¡çš„è‡ªåŠ¨åŒ–å’Œå¯é æ€§çš„æ½œåŠ›ã€‚æ­¤ç ”ç©¶å¡«è¡¥äº†ä»¥å¾€åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é¢†åŸŸä¸­å¯¹å¤æ‚å·¥ç¨‹è§£å†³æ–¹æ¡ˆè®¾è®¡ä»»åŠ¡çš„ç ”ç©¶ç©ºç™½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18600",
            "title": "Chain of Draft: Thinking Faster by Writing Less",
            "url": "https://huggingface.co/papers/2502.18600",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.",
            "score": 8,
            "issue_id": 2491,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "739d903f5735d9eb",
            "authors": [
                "Silei Xu",
                "Wenhao Xie",
                "Lingxiao Zhao",
                "Pengcheng He"
            ],
            "affiliations": [
                "Zoom Communications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18600.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒ - ÑĞµÑÑ‚Ñ€Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Chain of Draft (CoD). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Chain-of-Thought (CoT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, CoD Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ CoT, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². CoD Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸."
                },
                "en": {
                    "title": "Streamlining Reasoning: Less is More with Chain of Draft",
                    "desc": "This paper introduces Chain of Draft (CoD), a new approach for Large Language Models (LLMs) that mimics human reasoning by generating concise intermediate thoughts. Unlike the traditional Chain-of-Thought (CoT) prompting, which relies on verbose explanations, CoD focuses on delivering essential information in a minimalistic format. The authors demonstrate that CoD can achieve comparable or even superior accuracy to CoT while using significantly fewer tokens, leading to reduced computational costs and faster processing times. This innovative method enhances the efficiency of LLMs in tackling complex reasoning tasks."
                },
                "zh": {
                    "title": "è‰ç¨¿é“¾ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºï¼Œå¼ºè°ƒé€æ­¥æ¨ç†çš„è¯¦ç»†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œäººç±»é€šå¸¸é‡‡ç”¨æ›´é«˜æ•ˆçš„ç­–ç•¥ï¼šè‰æ‹Ÿç®€æ´çš„ä¸­é—´æ€è€ƒï¼Œåªæ•æ‰å…³é”®ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°èŒƒå¼â€”â€”è‰ç¨¿é“¾ï¼ˆCoDï¼‰ï¼Œçµæ„Ÿæ¥æºäºäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œä½¿LLMsåœ¨è§£å†³ä»»åŠ¡æ—¶ç”Ÿæˆç®€çº¦è€Œä¿¡æ¯ä¸°å¯Œçš„ä¸­é—´æ¨ç†è¾“å‡ºã€‚é€šè¿‡å‡å°‘å†—é•¿å¹¶ä¸“æ³¨äºå…³é”®è§è§£ï¼ŒCoDåœ¨å‡†ç¡®æ€§ä¸Šä¸CoTç›¸åŒ¹é…æˆ–è¶…è¶Šï¼ŒåŒæ—¶ä»…ä½¿ç”¨7.6%çš„æ ‡è®°ï¼Œæ˜¾è‘—é™ä½äº†å„ç§æ¨ç†ä»»åŠ¡çš„æˆæœ¬å’Œå»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18017",
            "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
            "url": "https://huggingface.co/papers/2502.18017",
            "abstract": "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.",
            "score": 4,
            "issue_id": 2487,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "4202273d8c895c2a",
            "authors": [
                "Qiuchen Wang",
                "Ruixue Ding",
                "Zehui Chen",
                "Weiqi Wu",
                "Shihang Wang",
                "Pengjun Xie",
                "Feng Zhao"
            ],
            "affiliations": [
                "MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, USTC",
                "Shanghai Jiao Tong University",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18017.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#rag",
                    "#games",
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ViDoRAG: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ViDoSeek Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (RAG) Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² RAG Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ViDoRAG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¼ĞµÑĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ViDoRAG Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 10% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ViDoSeek."
                },
                "en": {
                    "title": "Enhancing RAG for Complex Visual Reasoning with ViDoRAG",
                    "desc": "This paper addresses the challenges of understanding complex information in visually rich documents using Retrieval-Augmented Generation (RAG) methods. It introduces ViDoSeek, a new dataset that tests RAG performance on documents that require advanced reasoning skills. The authors highlight limitations in current RAG techniques, such as difficulties in integrating visual and textual data and inadequate reasoning capabilities. To overcome these issues, they propose ViDoRAG, a multi-agent framework that enhances retrieval and reasoning through a hybrid strategy and an iterative workflow, demonstrating significant improvements in performance on the ViDoSeek benchmark."
                },
                "zh": {
                    "title": "æå‡è§†è§‰æ–‡æ¡£ç†è§£çš„RAGæ–°æ¡†æ¶",
                    "desc": "ç†è§£è§†è§‰ä¸°å¯Œæ–‡æ¡£ä¸­çš„ä¿¡æ¯å¯¹ä¼ ç»Ÿçš„å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒçš„é—®é¢˜å›ç­”ï¼ˆQAï¼‰ï¼Œè€Œå¿½è§†äº†åœ¨å¯†é›†è§†è§‰æ–‡æ¡£ä¸­é«˜æ•ˆæ£€ç´¢ã€ç†è§£å’Œæ¨ç†çš„åŸºæœ¬æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ViDoSeekï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°RAGåœ¨éœ€è¦å¤æ‚æ¨ç†çš„è§†è§‰ä¸°å¯Œæ–‡æ¡£ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºçš„ViDoRAGæ¡†æ¶é‡‡ç”¨æ··åˆç­–ç•¥ï¼Œç»“åˆå¤šæ¨¡æ€æ£€ç´¢å’Œè¿­ä»£ä»£ç†å·¥ä½œæµï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨ViDoSeekåŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20545",
            "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
            "url": "https://huggingface.co/papers/2502.20545",
            "abstract": "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems.",
            "score": 4,
            "issue_id": 2486,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "fb32f9423103ece9",
            "authors": [
                "Kechen Li",
                "Wenqi Zhu",
                "Coralia Cartis",
                "Tianbo Ji",
                "Shiwei Liu"
            ],
            "affiliations": [
                "Mathematical Institute, University of Oxford",
                "Nanjing University of Aeronautics and Astronautics",
                "School of Transportation and Civil Engineering, Nantong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20545.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SoS-1K Ğ¸Ğ· 1000 Ğ¿Ğ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾ Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ÑÑ Ğ´Ğ¾ 81%. ĞœĞ¾Ğ´ĞµĞ»ÑŒ SoS-7B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° SoS-1K, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Mathematical Reasoning in LLMs with Structured Guidance",
                    "desc": "This paper explores the limitations of Large Language Models (LLMs) in solving complex mathematical problems, specifically the challenge of determining if a multivariate polynomial is nonnegative. The authors introduce a new dataset called SoS-1K, which contains around 1,000 polynomials and structured reasoning instructions to guide the models. They demonstrate that LLMs perform poorly without guidance, achieving only slightly above random guessing, but can significantly improve their accuracy with high-quality instructions. Notably, their fine-tuned model, SoS-7B, surpasses larger models in performance while being more computationally efficient, showcasing the potential of LLMs in addressing NP-hard problems."
                },
                "zh": {
                    "title": "æ¨åŠ¨æ•°å­¦æ¨ç†çš„è¾¹ç•Œ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¾¾åˆ°äº†äººç±»æ°´å¹³çš„èƒ½åŠ›ï¼Œä½†åœ¨ä¸¥æ ¼çš„æ•°å­¦é—®é¢˜è§£å†³æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ä¸ªåŸºæœ¬ä½†è®¡ç®—ä¸Šéš¾ä»¥å¤„ç†çš„é—®é¢˜ï¼šåˆ¤æ–­ç»™å®šçš„å¤šå˜é‡å¤šé¡¹å¼æ˜¯å¦éè´Ÿã€‚æˆ‘ä»¬å¼•å…¥äº†SoS-1Kæ•°æ®é›†ï¼ŒåŒ…å«çº¦1000ä¸ªå¤šé¡¹å¼ï¼Œå¹¶è®¾è®¡äº†åŸºäºäº”ä¸ªé€æ­¥æŒ‘æˆ˜æ ‡å‡†çš„æ¨ç†æŒ‡å¯¼ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡é«˜è´¨é‡çš„æ¨ç†æŒ‡å¯¼åï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œæœ€é«˜å¯è¾¾81%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17941",
            "title": "Optimal Brain Apoptosis",
            "url": "https://huggingface.co/papers/2502.17941",
            "abstract": "The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.",
            "score": 3,
            "issue_id": 2495,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "3748780b9de1393e",
            "authors": [
                "Mingyuan Sun",
                "Zheng Fang",
                "Jiaxu Wang",
                "Junjie Jiang",
                "Delei Kong",
                "Chenming Hu",
                "Yuetong Fang",
                "Renjing Xu"
            ],
            "affiliations": [
                "Hunan University",
                "Northeastern University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17941.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training",
                    "#architecture"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ° Ğ“ĞµÑÑĞ¸Ğ°Ğ½Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Optimal Brain Apoptosis (OBA). OBA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ñ€Ğ°ÑÑ‡ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ“ĞµÑÑĞ¸Ğ°Ğ½Ğ° Ğ½Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğº ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼, Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ğ“ĞµÑÑĞ¸Ğ°Ğ½Ğ° Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼ ÑĞµÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Efficient Neural Network Pruning with Optimal Brain Apoptosis",
                    "desc": "This paper addresses the challenges of high computational demands in Convolutional Neural Networks (CNNs) and Transformers by introducing a new pruning method called Optimal Brain Apoptosis (OBA). OBA improves upon previous methods by directly calculating the Hessian-vector product for each parameter, allowing for more accurate estimation of parameter importance. The authors decompose the Hessian matrix across layers to identify non-zero conditions, which enhances the efficiency of the pruning process. Experimental results demonstrate the effectiveness of OBA on various architectures and datasets, confirming its potential to optimize neural networks without significant performance loss."
                },
                "zh": {
                    "title": "é«˜æ•ˆå‰ªæï¼šæœ€ä¼˜è„‘å‡‹äº¡æ–¹æ³•",
                    "desc": "éšç€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå˜æ¢å™¨ï¼ˆTransformersï¼‰æ¨¡å‹çš„å¤æ‚æ€§å’Œå‚æ•°æ•°é‡çš„å¢åŠ ï¼Œè®¡ç®—æ•ˆç‡å’Œèµ„æºéœ€æ±‚é¢ä¸´æŒ‘æˆ˜ã€‚å‰ªæè¢«è®¤ä¸ºæ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡å»é™¤å†—ä½™å…ƒç´ ï¼ˆå¦‚ç¥ç»å…ƒã€é€šé“æˆ–è¿æ¥ï¼‰æ¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œè€Œä¸ä¼šä¸¥é‡å½±å“æ€§èƒ½ã€‚æœ¬æ–‡åœ¨æœ€ä¼˜è„‘æŸä¼¤ï¼ˆOBDï¼‰çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å‰ªææ–¹æ³•â€”â€”æœ€ä¼˜è„‘å‡‹äº¡ï¼ˆOBAï¼‰ï¼Œé€šè¿‡ç›´æ¥è®¡ç®—æ¯ä¸ªå‚æ•°çš„Hessian-å‘é‡ä¹˜ç§¯å€¼æ¥ä¼°è®¡å‚æ•°çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬VGG19ã€ResNet32ã€ResNet50å’ŒViT-B/16ï¼Œå±•ç¤ºäº†åœ¨CNNå’ŒTransformersä¸­çš„é«˜æ•ˆå‰ªæè¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.19577",
            "title": "Tell me why: Visual foundation models as self-explainable classifiers",
            "url": "https://huggingface.co/papers/2502.19577",
            "abstract": "Visual foundation models (VFMs) have become increasingly popular due to their state-of-the-art performance. However, interpretability remains crucial for critical applications. In this sense, self-explainable models (SEM) aim to provide interpretable classifiers that decompose predictions into a weighted sum of interpretable concepts. Despite their promise, recent studies have shown that these explanations often lack faithfulness. In this work, we combine VFMs with a novel prototypical architecture and specialized training objectives. By training only a lightweight head (approximately 1M parameters) on top of frozen VFMs, our approach (ProtoFM) offers an efficient and interpretable solution. Evaluations demonstrate that our approach achieves competitive classification performance while outperforming existing models across a range of interpretability metrics derived from the literature. Code is available at https://github.com/hturbe/proto-fm.",
            "score": 2,
            "issue_id": 2493,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "7d2bd5235959eba5",
            "authors": [
                "Hugues TurbÃ©",
                "Mina Bjelogrlic",
                "Gianmarco Mengaldo",
                "Christian Lovis"
            ],
            "affiliations": [
                "Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore, Singapore",
                "Department of Radiology and Medical Informatics, University of Geneva, Geneva, Switzerland",
                "Division of Medical Information Sciences, Geneva University Hospitals, Geneva, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.19577.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#small_models",
                    "#architecture",
                    "#interpretability",
                    "#open_source",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ProtoFM: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¾ÑĞ½Ğ¾Ğ² (VFM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ VFM Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ ProtoFM. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ²ĞµÑ€Ñ…ÑƒÑˆĞºÑƒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… VFM, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ProtoFM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "ProtoFM: Interpretable and Efficient Visual Foundation Models",
                    "desc": "This paper introduces ProtoFM, a new approach that combines visual foundation models (VFMs) with a prototypical architecture to enhance interpretability in machine learning. The method focuses on creating self-explainable models (SEMs) that break down predictions into understandable components, addressing the issue of faithfulness in explanations. By training a lightweight head on top of frozen VFMs, ProtoFM maintains high classification performance while improving interpretability metrics. The results show that ProtoFM outperforms existing models, making it a promising solution for applications requiring both accuracy and clarity."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¯è§£é‡Šçš„è§†è§‰åŸºç¡€æ¨¡å‹",
                    "desc": "è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å› å…¶å“è¶Šçš„æ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨å…³é”®åº”ç”¨ä¸­å¯è§£é‡Šæ€§ä»ç„¶è‡³å…³é‡è¦ã€‚è‡ªè§£é‡Šæ¨¡å‹ï¼ˆSEMï¼‰æ—¨åœ¨æä¾›å¯è§£é‡Šçš„åˆ†ç±»å™¨ï¼Œå°†é¢„æµ‹åˆ†è§£ä¸ºå¯è§£é‡Šæ¦‚å¿µçš„åŠ æƒå’Œã€‚å°½ç®¡æœ‰æ½œåŠ›ï¼Œè¿‘æœŸç ”ç©¶è¡¨æ˜è¿™äº›è§£é‡Šå¾€å¾€ç¼ºä¹å¯ä¿¡åº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆVFMå’Œæ–°å‹åŸå‹æ¶æ„çš„æ–¹æ¡ˆï¼ˆProtoFMï¼‰ï¼Œé€šè¿‡åœ¨å†»ç»“çš„VFMä¸Šè®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„å¤´éƒ¨æ¨¡å‹ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20396",
            "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
            "url": "https://huggingface.co/papers/2502.20396",
            "abstract": "Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.",
            "score": 2,
            "issue_id": 2486,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "41439b4f54e02c9b",
            "authors": [
                "Toru Lin",
                "Kartik Sachdev",
                "Linxi Fan",
                "Jitendra Malik",
                "Yuke Zhu"
            ],
            "affiliations": [
                "NVIDIA",
                "UC Berkeley",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20396.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#rl",
                    "#games",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ›Ğ¾Ğ²ĞºĞ¾ÑÑ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°: Ğ¾Ñ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Reinforcement Learning for Dexterous Robot Manipulation: Bridging Sim and Real Worlds",
                    "desc": "This paper addresses the challenges of using reinforcement learning for complex robot manipulation tasks that involve physical contact. The authors propose innovative methods to enhance the performance of humanoid robots in these tasks, including a system to align simulated environments with real-world conditions. They also introduce a new reward design that simplifies the process of creating effective rewards for long tasks and a technique to improve learning efficiency in difficult scenarios. The results demonstrate that their approach allows robots to learn dexterous manipulation effectively, achieving high performance without requiring human guidance."
                },
                "zh": {
                    "title": "çªç ´ç±»äººæœºå™¨äººçµå·§æ“ä½œçš„å¼ºåŒ–å­¦ä¹ æŒ‘æˆ˜",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ç±»äººæœºå™¨äººçµå·§æ“ä½œä»»åŠ¡ä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ çš„å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†æ–°æŠ€æœ¯æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–çš„çœŸå®åˆ°æ¨¡æ‹Ÿè°ƒä¼˜æ¨¡å—ï¼Œä»¥ç¼©å°æ¨¡æ‹Ÿç¯å¢ƒä¸ç°å®ä¸–ç•Œçš„å·®è·ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§é€šç”¨çš„å¥–åŠ±æœºåˆ¶ï¼Œç®€åŒ–äº†é•¿æ—¶é—´æ¥è§¦ä¸°å¯Œçš„æ“ä½œä»»åŠ¡çš„å¥–åŠ±å·¥ç¨‹ã€‚é€šè¿‡å¯¹ä¸‰é¡¹ç±»äººçµå·§æ“ä½œä»»åŠ¡çš„å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨ä¸éœ€è¦äººç±»ç¤ºèŒƒçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ°çœŸå®çš„å¼ºåŒ–å­¦ä¹ å®ç°äº†ç¨³å¥çš„æ³›åŒ–å’Œé«˜æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20583",
            "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
            "url": "https://huggingface.co/papers/2502.20583",
            "abstract": "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper, rely on deep encoder-decoder architectures, and their encoders are a critical bottleneck for efficient deployment due to high computational intensity. We introduce LiteASR, a low-rank compression scheme for ASR encoders that significantly reduces inference costs while maintaining transcription accuracy. Our approach leverages the strong low-rank properties observed in intermediate activations: by applying principal component analysis (PCA) with a small calibration dataset, we approximate linear transformations with a chain of low-rank matrix multiplications, and further optimize self-attention to work in the reduced dimension. Evaluation results show that our method can compress Whisper large-v3's encoder size by over 50%, matching Whisper medium's size with better transcription accuracy, thereby establishing a new Pareto-optimal frontier of efficiency and performance. The code of LiteASR is available at https://github.com/efeslab/LiteASR.",
            "score": 2,
            "issue_id": 2486,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "3c7268c0881fa426",
            "authors": [
                "Keisuke Kamahori",
                "Jungo Kasai",
                "Noriyuki Kojima",
                "Baris Kasikci"
            ],
            "affiliations": [
                "Kotoba Technologies Inc.",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20583.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#audio"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "LiteASR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² ASR Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "LiteASR - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ (PCA) Ğ´Ğ»Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°. LiteASR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¶Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Whisper large-v3 Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50%, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Whisper medium Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ASR."
                },
                "en": {
                    "title": "LiteASR: Efficient ASR with Low-Rank Compression",
                    "desc": "This paper presents LiteASR, a novel low-rank compression technique designed to enhance the efficiency of automatic speech recognition (ASR) models, particularly focusing on the encoder component. By utilizing principal component analysis (PCA) on intermediate activations, LiteASR reduces the computational load during inference while preserving transcription accuracy. The method achieves over 50% reduction in the encoder size of OpenAI's Whisper large-v3 model, aligning its performance with that of the medium version but with improved accuracy. This work sets a new standard for balancing efficiency and performance in ASR systems."
                },
                "zh": {
                    "title": "LiteASRï¼šé«˜æ•ˆçš„ä½ç§©å‹ç¼©æ–¹æ¡ˆ",
                    "desc": "ç°ä»£è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œå¦‚OpenAIçš„Whisperï¼Œä¾èµ–äºæ·±åº¦ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè€Œç¼–ç å™¨çš„è®¡ç®—å¼ºåº¦æ˜¯é«˜æ•ˆéƒ¨ç½²çš„ç“¶é¢ˆã€‚æˆ‘ä»¬æå‡ºäº†LiteASRï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ASRç¼–ç å™¨çš„ä½ç§©å‹ç¼©æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒè½¬å½•å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†ä¸­é—´æ¿€æ´»ä¸­çš„å¼ºä½ç§©ç‰¹æ€§ï¼Œé€šè¿‡ä½¿ç”¨å°å‹æ ¡å‡†æ•°æ®é›†çš„ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼Œç”¨ä½ç§©çŸ©é˜µä¹˜æ³•é“¾æ¥è¿‘ä¼¼çº¿æ€§å˜æ¢ï¼Œå¹¶è¿›ä¸€æ­¥ä¼˜åŒ–è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥é€‚åº”é™ç»´åçš„æ•°æ®ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å°†Whisper large-v3çš„ç¼–ç å™¨å¤§å°å‹ç¼©è¶…è¿‡50%ï¼Œå¹¶åœ¨è½¬å½•å‡†ç¡®æ€§ä¸Šä¼˜äºWhisper mediumï¼Œä»è€Œå»ºç«‹äº†æ•ˆç‡ä¸æ€§èƒ½çš„æ–°å¸•ç´¯æ‰˜æœ€ä¼˜è¾¹ç•Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20811",
            "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
            "url": "https://huggingface.co/papers/2502.20811",
            "abstract": "Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. HAICTrain comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, HAICBench includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.",
            "score": 1,
            "issue_id": 2486,
            "pub_date": "2025-02-28",
            "pub_date_card": {
                "ru": "28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 28",
                "zh": "2æœˆ28æ—¥"
            },
            "hash": "806f6aacd5ee2f8a",
            "authors": [
                "Xiao Wang",
                "Jingyun Hua",
                "Weihong Lin",
                "Yuanxing Zhang",
                "Fuzheng Zhang",
                "Jianlong Wu",
                "Di Zhang",
                "Liqiang Nie"
            ],
            "affiliations": [
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20811.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#synthetic"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ±Ğ¾Ñ€ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ±Ñ‹Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: HAICTrain Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ HAICBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° HAICTrain Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Curated Human Action Datasets",
                    "desc": "This paper presents a solution to improve video understanding in Multi-modal Large Language Models (MLLMs) by addressing the scarcity of high-quality data on human actions. The authors introduce a two-stage data annotation pipeline that first collects videos with clear human actions from the Internet and then annotates them using a standardized caption format. This results in two curated datasets: HAICTrain, which contains 126K video-caption pairs for training, and HAICBench, which includes 500 annotated pairs for evaluation. Experimental results show that using HAICTrain significantly boosts human action understanding and enhances text-to-video generation capabilities across multiple benchmarks."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç†è§£çš„åˆ›æ–°æ•°æ®æ ‡æ³¨æµç¨‹",
                    "desc": "æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ¶‰åŠäººç±»åŠ¨ä½œçš„è§†é¢‘ä¸Šçš„è¡¨ç°ä»ç„¶å—åˆ°é«˜è´¨é‡æ•°æ®ç¼ºä¹çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ•°æ®æ ‡æ³¨æµç¨‹ï¼Œé¦–å…ˆä»äº’è”ç½‘æ”¶é›†åŒ…å«æ¸…æ™°äººç±»åŠ¨ä½œçš„è§†é¢‘ï¼Œç„¶åä½¿ç”¨æ ‡å‡†åŒ–çš„å­—å¹•æ ¼å¼å¯¹è§†é¢‘è¿›è¡Œæ ‡æ³¨ã€‚é€šè¿‡è¿™ä¸ªæµç¨‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªæ•°æ®é›†HAICTrainå’ŒHAICBenchï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨HAICTrainè¿›è¡Œè®­ç»ƒæ˜¾è‘—æå‡äº†äººç±»åŠ¨ä½œç†è§£èƒ½åŠ›ï¼Œå¹¶æ”¹å–„äº†æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ•ˆæœã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-28.html",
    "link_next": "2025-03-04.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "28.02",
        "en": "02/28",
        "zh": "2æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "04.03",
        "en": "03/04",
        "zh": "3æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è®¾è®¡å¤æ‚å·¥ç¨‹æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆå¯¹äººç±»ç”Ÿäº§æ´»åŠ¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç ”ç©¶æœªèƒ½å……åˆ†è§£å†³ä¸å¤æ‚å·¥ç¨‹è§£å†³æ–¹æ¡ˆè®¾è®¡ç›¸å…³çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼ŒSolutionBenchï¼Œæ¥è¯„ä¼°ç³»ç»Ÿç”Ÿæˆå®Œæ•´å’Œå¯è¡Œçš„å·¥ç¨‹é—®é¢˜è§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°ç³»ç»Ÿï¼ŒSolutionRAGï¼Œåˆ©ç”¨åŸºäºæ ‘çš„æ¢ç´¢å’ŒåŒç‚¹æ€ç»´æœºåˆ¶ç”Ÿæˆå¯é çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSolutionRAGåœ¨SolutionBenchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­å¢å¼ºå¤æ‚å·¥ç¨‹è§£å†³æ–¹æ¡ˆè®¾è®¡è‡ªåŠ¨åŒ–å’Œå¯é æ€§çš„æ½œåŠ›ã€‚",
        "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
        "pinyin": "è®¾è®¡å¤æ‚å·¥ç¨‹æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆå¯¹äººç±»ç”Ÿäº§æ´»åŠ¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç ”ç©¶æœªèƒ½å……åˆ†è§£å†³ä¸å¤æ‚å·¥ç¨‹è§£å†³æ–¹æ¡ˆè®¾è®¡ç›¸å…³çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼ŒSolutionBenchï¼Œæ¥è¯„ä¼°ç³»ç»Ÿç”Ÿæˆå®Œæ•´å’Œå¯è¡Œçš„å·¥ç¨‹é—®é¢˜è§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°ç³»ç»Ÿï¼ŒSolutionRAGï¼Œåˆ©ç”¨åŸºäºæ ‘çš„æ¢ç´¢å’ŒåŒç‚¹æ€ç»´æœºåˆ¶ç”Ÿæˆå¯é çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSolutionRAGåœ¨SolutionBenchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­å¢å¼ºå¤æ‚å·¥ç¨‹è§£å†³æ–¹æ¡ˆè®¾è®¡è‡ªåŠ¨åŒ–å’Œå¯é æ€§çš„æ½œåŠ›ã€‚\n\nShÃ¨jÃ¬ fÃ¹zÃ¡ gÅngchÃ©ng tiÇozhÃ n de jiÄ›juÃ© fÄng'Ã n duÃ¬ rÃ©nlÃ¨i shÄ“ngchÇn huÃ³dÃ²ng zhÃ¬guÄn zhÃ²ngyÃ o. RÃ¡n'Ã©r, yÇqiÃ¡n de jiÇnsuÇ’ zÄ“ngqiÃ¡ng shÄ“ngchÃ©ng (RAG) yÃ¡njiÅ« wÃ¨i nÃ©ng chÃ³ngfÄ“n jiÄ›juÃ© yÇ” fÃ¹zÃ¡ gÅngchÃ©ng jiÄ›juÃ© fÄng'Ã n shÃ¨jÃ¬ xiÄngguÄn de rÃ¨nwÃ¹. WÇ’men yÇnrÃ¹ le yÄ«gÃ¨ xÄ«n de jÄ«zhÇ”n, SolutionBench, lÃ¡i pÃ­nggÅ« xÃ¬tÇ’ng shÄ“ngchÃ©ng wÃ¡nzhÄ›ng hÃ© kÄ›xÃ­ng de gÅngchÃ©ng wÃ¨ntÃ­ jiÄ›juÃ© fÄng'Ã n de nÃ©nglÃ¬. WÇ’men hÃ¡i tÃ­chÅ« le yÄ«gÃ¨ xÄ«n xÃ¬tÇ’ng, SolutionRAG, lÃ¬yÃ²ng jÄ«yÃº shÃ¹ de tÃ nsuÇ’ hÃ© shuÄngdiÇn sÄ«wÃ©i jÄ«zhÃ¬ shÄ“ngchÃ©ng kÄ›kÃ o de jiÄ›juÃ© fÄng'Ã n. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, SolutionRAG zÃ i SolutionBench shÃ ng dÃ¡ dÃ o le zuÃ¬ xiÄnjÃ¬n de xÃ¬ngnÃ©ng, tÅ«xÃ¬ le qÃ­ zÃ i shÃ­jÃ¬ yÃ¬ngyÃ²ng zhÅng zÄ“ngqiÃ¡ng fÃ¹zÃ¡ gÅngchÃ©ng jiÄ›juÃ© fÄng'Ã n shÃ¨jÃ¬ zÃ¬dÃ²nghuÃ  hÃ© kÄ›kÃ oxÃ¬ng de qiÃ¡nlÃ¬.",
        "vocab": "[{'word': 'è®¾è®¡', 'pinyin': 'shÃ¨jÃ¬', 'trans': 'design'},\n{'word': 'å¤æ‚', 'pinyin': 'fÃ¹zÃ¡', 'trans': 'complex'},\n{'word': 'å·¥ç¨‹', 'pinyin': 'gÅngchÃ©ng', 'trans': 'engineering'},\n{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇozhÃ n', 'trans': 'challenge'},\n{'word': 'è§£å†³æ–¹æ¡ˆ', 'pinyin': 'jiÄ›juÃ© fÄngÃ n', 'trans': 'solution'},\n{'word': 'è‡³å…³é‡è¦', 'pinyin': 'zhÃ¬guÄn zhÃ²ngyÃ o', 'trans': 'crucial'},\n{'word': 'æ£€ç´¢', 'pinyin': 'jiÇnsuÇ’', 'trans': 'retrieval'},\n{'word': 'å¢å¼º', 'pinyin': 'zÄ“ngqiÃ¡ng', 'trans': 'enhanced'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generation'},\n{'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡njiÅ«', 'trans': 'research'},\n{'word': 'æœªèƒ½', 'pinyin': 'wÃ¨inÃ©ng', 'trans': 'failed to'},\n{'word': 'å……åˆ†', 'pinyin': 'chÅngfÃ¨n', 'trans': 'adequately'},\n{'word': 'ç›¸å…³', 'pinyin': 'xiÄngguÄn', 'trans': 'related'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'},\n{'word': 'å¼•å…¥', 'pinyin': 'yÇnrÃ¹', 'trans': 'introduce'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ«zhÇ”n', 'trans': 'benchmark'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­nggÅ«', 'trans': 'evaluate'},\n{'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬tÇ’ng', 'trans': 'system'},\n{'word': 'å®Œæ•´', 'pinyin': 'wÃ¡nzhÄ›ng', 'trans': 'complete'},\n{'word': 'å¯è¡Œ', 'pinyin': 'kÄ›xÃ­ng', 'trans': 'feasible'},\n{'word': 'å·¥ç¨‹é—®é¢˜', 'pinyin': 'gÅngchÃ©ng wÃ¨ntÃ­', 'trans': 'engineering problem'},\n{'word': 'è§£å†³æ–¹æ¡ˆ', 'pinyin': 'jiÄ›juÃ© fÄngÃ n', 'trans': 'solution'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'capability'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'},\n{'word': 'æ–°ç³»ç»Ÿ', 'pinyin': 'xÄ«n xÃ¬tÇ’ng', 'trans': 'new system'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬yÃ²ng', 'trans': 'utilize'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'æ ‘', 'pinyin': 'shÃ¹', 'trans': 'tree'},\n{'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'exploration'},\n{'word': 'åŒç‚¹æ€ç»´', 'pinyin': 'shuÄngdiÇn sÄ«wÃ©i', 'trans': 'dual-point thinking'},\n{'word': 'æœºåˆ¶', 'pinyin': 'jÄ«zhÃ¬', 'trans': 'mechanism'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'},\n{'word': 'å¯é ', 'pinyin': 'kÄ›kÃ o', 'trans': 'reliable'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇnshÃ¬', 'trans': 'show'},\n{'word': 'è¾¾åˆ°', 'pinyin': 'dÃ¡dÃ o', 'trans': 'achieve'},\n{'word': 'æœ€å…ˆè¿›', 'pinyin': 'zuÃ¬ xiÄnjÃ¬n', 'trans': 'state-of-the-art'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ngnÃ©ng', 'trans': 'performance'},\n{'word': 'çªæ˜¾', 'pinyin': 'tÅ«xiÇn', 'trans': 'highlight'},\n{'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡nlÃ¬', 'trans': 'potential'},\n{'word': 'å®é™…', 'pinyin': 'shÃ­jÃ¬', 'trans': 'practical'},\n{'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ngyÃ²ng', 'trans': 'application'},\n{'word': 'å¢å¼º', 'pinyin': 'zÄ“ngqiÃ¡ng', 'trans': 'enhance'},\n{'word': 'è‡ªåŠ¨åŒ–', 'pinyin': 'zÃ¬dÃ²nghuÃ ', 'trans': 'automation'}]",
        "trans": "Designing solutions for complex engineering challenges is crucial for human productive activities. However, previous research on Retrieval-Augmented Generation (RAG) has failed to adequately address tasks related to the design of complex engineering solutions. We introduce a new benchmark, SolutionBench, to evaluate the capability of systems to generate complete and feasible engineering problem solutions. We also propose a new system, SolutionRAG, which utilizes tree-based exploration and dual-point thinking mechanisms to generate reliable solutions. Experimental results demonstrate that SolutionRAG achieves state-of-the-art performance on SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in practical applications.",
        "update_ts": "2025-03-03 09:12"
    }
}