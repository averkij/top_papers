{
    "date": {
        "ru": "6 Ğ¼Ğ°Ñ",
        "en": "May 6",
        "zh": "5æœˆ6æ—¥"
    },
    "time_utc": "2025-05-06 17:10",
    "weekday": 1,
    "issue_id": 3616,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.02707",
            "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
            "url": "https://huggingface.co/papers/2505.02707",
            "abstract": "A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.",
            "score": 52,
            "issue_id": 3604,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "1cf06df6011df348",
            "authors": [
                "Yemin Shi",
                "Yu Shu",
                "Siwei Dong",
                "Guangyi Liu",
                "Jaward Sesay",
                "Jingwen Li",
                "Zhiting Hu"
            ],
            "affiliations": [
                "MBZUAI",
                "Maitrix.org",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02707.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agi",
                    "#multimodal",
                    "#audio",
                    "#architecture",
                    "#agents",
                    "#reasoning",
                    "#multilingual",
                    "#machine_translation"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Voila: Ğ˜Ğ˜-ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸Ğº Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Voila - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²ĞµÑÑ‚Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞ²Ğ¾Ğ·Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Voila Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ²ÑĞµĞ³Ğ¾ 195 Ğ¼Ğ¸Ğ»Ğ»Ğ¸ÑĞµĞºÑƒĞ½Ğ´, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¿Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑÑĞ¼Ğ¿Ğ»Ğ°Ğ¼."
                },
                "en": {
                    "title": "Voila: Revolutionizing Voice AI for Natural Conversations",
                    "desc": "This paper presents Voila, an advanced voice AI agent designed for seamless interaction in daily life. Voila utilizes a novel end-to-end architecture that allows for real-time, emotionally expressive conversations, achieving a response time of just 195 milliseconds. It combines large language models with sophisticated acoustic modeling to generate natural and persona-aware voice outputs, enabling users to customize voice characteristics easily. Additionally, Voila supports various voice-based applications, including automatic speech recognition and multilingual translation, and is fully open-sourced to promote further research in human-machine communication."
                },
                "zh": {
                    "title": "Voilaï¼šå®ç°è‡ªç„¶æƒ…æ„Ÿäº’åŠ¨çš„è¯­éŸ³AIä»£ç†",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Voilaï¼Œä¸€ä¸ªæ–°å‹çš„è¯­éŸ³AIä»£ç†ï¼Œæ—¨åœ¨å®ç°ä¸äººç±»çš„è‡ªç„¶äº’åŠ¨ã€‚Voilaé‡‡ç”¨ç«¯åˆ°ç«¯æ¶æ„ï¼Œæ”¯æŒå…¨åŒå·¥ã€ä½å»¶è¿Ÿçš„å¯¹è¯ï¼Œèƒ½å¤Ÿæ•æ‰è¯­éŸ³ä¸­çš„æƒ…æ„Ÿå’Œç»†å¾®å·®åˆ«ã€‚å®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¼ºå¤§çš„å£°å­¦å»ºæ¨¡ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç®€å•çš„æ–‡æœ¬æŒ‡ä»¤å®šä¹‰è¯´è¯è€…çš„èº«ä»½å’Œè¯­è°ƒã€‚Voilaè¿˜æ”¯æŒè¶…è¿‡ä¸€ç™¾ä¸‡ç§é¢„æ„å»ºçš„å£°éŸ³ï¼Œå¹¶èƒ½ä»çŸ­è‡³10ç§’çš„éŸ³é¢‘æ ·æœ¬ä¸­é«˜æ•ˆå®šåˆ¶æ–°å£°éŸ³ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02387",
            "title": "RM-R1: Reward Modeling as Reasoning",
            "url": "https://huggingface.co/papers/2505.02387",
            "abstract": "Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.",
            "score": 39,
            "issue_id": 3603,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "6cac1dc82fc6bcdc",
            "authors": [
                "Xiusi Chen",
                "Gaotang Li",
                "Ziqi Wang",
                "Bowen Jin",
                "Cheng Qian",
                "Yu Wang",
                "Hongru Wang",
                "Yu Zhang",
                "Denghui Zhang",
                "Tong Zhang",
                "Hanghang Tong",
                "Heng Ji"
            ],
            "affiliations": [
                "Stevens Institute of Technology",
                "Texas A&M University",
                "University of California, San Diego",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02387.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#rlhf",
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ - Reasoning Reward Models (ReasRMs), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ReasRM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ° Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñƒ Ğ´Ğ¾ 13.8%."
                },
                "en": {
                    "title": "Enhancing Reward Models with Reasoning for Better Interpretability",
                    "desc": "This paper discusses the importance of reward modeling in aligning large language models (LLMs) with human preferences using reinforcement learning from human feedback (RLHF). It highlights the limitations of existing reward models, which often lack interpretability and struggle to incorporate natural language critiques. The authors introduce Reasoning Reward Models (ReasRMs), which enhance interpretability by treating reward modeling as a reasoning task. Their approach includes a two-stage training process that improves performance and achieves state-of-the-art results on various benchmarks, outperforming larger models."
                },
                "zh": {
                    "title": "æ¨ç†å¥–åŠ±æ¨¡å‹ï¼šæå‡å¯è§£é‡Šæ€§çš„å…³é”®",
                    "desc": "å¥–åŠ±å»ºæ¨¡åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯é€šè¿‡äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¾€å¾€ç”Ÿæˆä¸é€æ˜çš„æ ‡é‡åˆ†æ•°æˆ–ç›´æ¥é¢„æµ‹åå¥½çš„ç­”æ¡ˆï¼Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹â€”â€”æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºæ¨ç†ä»»åŠ¡ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚é€šè¿‡é«˜è´¨é‡æ¨ç†é“¾çš„è’¸é¦å’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªå¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20752",
            "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
            "url": "https://huggingface.co/papers/2504.20752",
            "abstract": "Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio phi_r of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing phi_r drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.",
            "score": 34,
            "issue_id": 3604,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "46858ed83065e6ae",
            "authors": [
                "Roman Abramov",
                "Felix Steinbauer",
                "Gjergji Kasneci"
            ],
            "affiliations": [
                "School of Computation, Information and Technology, Technical University of Munich, Munich, Germany",
                "School of Social Sciences and Technology, Technical University of Munich, Munich, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20752.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#reasoning",
                    "#synthetic",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²ĞµÑ€ÑŒ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ£Ğ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒÑĞ¸Ğ»Ğ¸Ñ‚ÑŒ ÑÑ…ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 95-100% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ 2WikiMultiHopQA, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Unlocking Reasoning in Transformers with Grokking and Data Augmentation",
                    "desc": "This paper explores how Transformers, a type of neural network, can improve their ability to reason through complex factual information. It introduces the concept of 'grokking' to real-world data by enhancing knowledge graphs with synthetic data, which helps the model learn logical patterns instead of just memorizing facts. Interestingly, the study finds that even incorrect synthetic data can aid in developing reasoning capabilities by emphasizing relational structures. The results show significant improvements in multi-hop reasoning tasks, achieving high accuracy and suggesting that this method can enhance the reasoning abilities of large language models."
                },
                "zh": {
                    "title": "åˆ©ç”¨åˆæˆæ•°æ®æå‡å¤šæ­¥æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å˜æ¢å™¨åœ¨å¤šæ­¥äº‹å®æ¨ç†ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®ä¸–ç•ŒçŸ¥è¯†ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬é¦–æ¬¡å°†grokkingæ‰©å±•åˆ°çœŸå®ä¸–ç•Œçš„äº‹å®æ•°æ®ï¼Œå¹¶é€šè¿‡å¢å¼ºçŸ¥è¯†å›¾è°±æ¥è§£å†³æ•°æ®é›†ç¨€ç–æ€§çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯äº‹å®ä¸æ­£ç¡®çš„åˆæˆæ•°æ®ä¹Ÿèƒ½å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸æ˜¯é™ä½å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†95-100%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02222",
            "title": "Practical Efficiency of Muon for Pretraining",
            "url": "https://huggingface.co/papers/2505.02222",
            "abstract": "We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.",
            "score": 23,
            "issue_id": 3608,
            "pub_date": "2025-05-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ",
                "en": "May 4",
                "zh": "5æœˆ4æ—¥"
            },
            "hash": "3b2cca9779803131",
            "authors": [
                "Essential AI",
                ":",
                "Ishaan Shah",
                "Anthony M. Polloreno",
                "Karl Stratos",
                "Philip Monk",
                "Adarsh Chaluvaraju",
                "Andrew Hojel",
                "Andrew Ma",
                "Anil Thomas",
                "Ashish Tanwer",
                "Darsh J Shah",
                "Khoi Nguyen",
                "Kurt Smith",
                "Michael Callahan",
                "Michael Pust",
                "Mohit Parmar",
                "Peter Rushton",
                "Platon Mazarakis",
                "Ritvik Kapila",
                "Saurabh Srivastava",
                "Somanshu Singla",
                "Tim Romanski",
                "Yash Vanjani",
                "Ashish Vaswani"
            ],
            "affiliations": [
                "Essential AI, San Francisco, CA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02222.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Muon: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Muon, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ AdamW Ğ¿Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Muon ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ±Ğ°Ñ‚Ñ‡Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Muon Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ (muP) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¾ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Muon: A New Era of Efficient Optimization in Machine Learning",
                    "desc": "This paper introduces Muon, a second-order optimizer that improves upon AdamW by expanding the Pareto frontier in terms of compute-time efficiency. Muon demonstrates superior data efficiency at large batch sizes, even exceeding the critical batch size, while maintaining computational efficiency for cost-effective training. The authors also explore the integration of Muon with the maximal update parameterization (muP) to enhance hyperparameter transfer efficiency. Their extensive experiments, involving models with up to four billion parameters, validate the effectiveness of Muon across various data distributions and architectures."
                },
                "zh": {
                    "title": "Muonä¼˜åŒ–å™¨ï¼šè¶…è¶ŠAdamWçš„é«˜æ•ˆè®­ç»ƒ",
                    "desc": "æœ¬æ–‡å±•ç¤ºäº†Muonä½œä¸ºä¸€ç§äºŒé˜¶ä¼˜åŒ–å™¨çš„ç®€å•å®ç°ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—æ—¶é—´çš„æƒè¡¡ä¸Šæ˜¾è‘—æ‰©å±•Paretoå‰æ²¿ï¼Œç›¸æ¯”äºAdamWæ›´å…·ä¼˜åŠ¿ã€‚ç ”ç©¶å‘ç°ï¼ŒMuonåœ¨å¤§æ‰¹é‡è®­ç»ƒä¸­ä¿æŒæ•°æ®æ•ˆç‡ï¼Œè¶…è¶Šäº†æ‰€è°“çš„ä¸´ç•Œæ‰¹é‡å¤§å°ï¼ŒåŒæ—¶ä»ç„¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä»è€Œå®ç°æ›´ç»æµçš„è®­ç»ƒã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†Muonä¸æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆmuPï¼‰çš„ç»“åˆï¼Œä»¥å®ç°é«˜æ•ˆçš„è¶…å‚æ•°è½¬ç§»ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•çš„é€’å½’ç®—æ³•ï¼Œèƒ½å¤Ÿè€ƒè™‘muPä¸­çš„æ‰€æœ‰è¯¯å·®æ¥æºï¼ŒåŒæ—¶ä»…å¼•å…¥é€‚åº¦çš„èµ„æºå¼€é”€ã€‚é€šè¿‡å¯¹æ¨¡å‹è§„æ¨¡è¾¾åˆ°å››åäº¿å‚æ•°çš„å¹¿æ³›å®éªŒä»¥åŠå¯¹æ•°æ®åˆ†å¸ƒå’Œæ¶æ„çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02735",
            "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
            "url": "https://huggingface.co/papers/2505.02735",
            "abstract": "Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.",
            "score": 20,
            "issue_id": 3602,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "1ffa5567eb2f4acc",
            "authors": [
                "Zhouliang Yu",
                "Ruotian Peng",
                "Keyi Ding",
                "Yizhe Li",
                "Zhongyuan Peng",
                "Minghao Liu",
                "Yifan Zhang",
                "Zheng Yuan",
                "Huajian Xin",
                "Wenhao Huang",
                "Yandong Wen",
                "Ge Zhang",
                "Weiyang Liu"
            ],
            "affiliations": [
                "M-A-P",
                "Max Planck Institute for Intelligent Systems, TÃ¼bingen",
                "Numina",
                "The Chinese University of Hong Kong",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02735.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#math",
                    "#benchmark"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "FormalMATH: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "FormalMATH - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Lean4, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 5560 Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ‚ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM-Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ¼ Ğ² 16.46% Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "FormalMATH: Advancing AI in Formal Mathematical Reasoning",
                    "desc": "The paper introduces FormalMATH, a comprehensive benchmark designed to enhance formal mathematical reasoning in AI. It consists of 5,560 verified problems that cover a wide range of mathematical topics, from high school to undergraduate level. The authors propose a human-in-the-loop autoformalization pipeline that utilizes large language models (LLMs) for automating the formalization process, significantly reducing the need for expert input. The study also highlights the limitations of current LLM-based theorem provers, revealing their low success rates and biases across different mathematical domains."
                },
                "zh": {
                    "title": "FormalMATHï¼šæ¨åŠ¨æ­£å¼æ•°å­¦æ¨ç†çš„åŸºå‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†FormalMATHï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„Lean4åŸºå‡†ï¼ŒåŒ…å«5560ä¸ªç»è¿‡æ­£å¼éªŒè¯çš„æ•°å­¦é—®é¢˜ï¼Œæ¶µç›–ä»é«˜ä¸­å¥¥æ—åŒ¹å…‹æŒ‘æˆ˜åˆ°æœ¬ç§‘å®šç†çš„å¤šä¸ªé¢†åŸŸã€‚ä¸ºäº†è§£å†³æ‰‹åŠ¨å½¢å¼åŒ–çš„ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„äººæœºåä½œè‡ªåŠ¨å½¢å¼åŒ–æµç¨‹ï¼Œç»“åˆäº†ä¸“é—¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè¯­å¥è‡ªåŠ¨å½¢å¼åŒ–ã€å¤šLLMè¯­ä¹‰éªŒè¯å’ŒåŸºäºå¦å®šçš„åé©³è¿‡æ»¤ç­–ç•¥ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMå®šç†è¯æ˜å™¨åœ¨å®é™…é‡‡æ ·é¢„ç®—ä¸‹çš„æˆåŠŸç‡ä»…ä¸º16.46%ï¼Œå¹¶ä¸”åœ¨ä¸åŒé¢†åŸŸè¡¨ç°å‡ºæ˜æ˜¾çš„åå·®ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒFormalMATHä¸ºæ­£å¼æ•°å­¦æ¨ç†æä¾›äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02819",
            "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
            "url": "https://huggingface.co/papers/2505.02819",
            "abstract": "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.",
            "score": 19,
            "issue_id": 3608,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "7f169d4491ab6864",
            "authors": [
                "Dmitriy Shopkhoev",
                "Ammar Ali",
                "Magauiya Zhussip",
                "Valentin Malykh",
                "Stamatios Lefkimmiatis",
                "Nikos Komodakis",
                "Sergey Zagoruyko"
            ],
            "affiliations": [
                "Archimedes Athena RC",
                "IACM-Forth",
                "IITU",
                "ITMO University",
                "MTS AI",
                "Polynome",
                "University of Crete"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02819.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "ReplaceMe - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ±Ğ»Ğ¾ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ReplaceMe Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Effortless Depth Pruning with ReplaceMe",
                    "desc": "ReplaceMe is a novel method for depth pruning in transformer models that does not require additional training. Instead of retraining the model after pruning, it uses a small calibration dataset to create a linear transformation that approximates the pruned blocks. This allows for significant compression of the model while maintaining high performance, achieving up to 25% pruning with around 90% of the original performance. The method is efficient and can be easily integrated into existing architectures without adding extra parameters, making it a competitive alternative to traditional pruning techniques."
                },
                "zh": {
                    "title": "ReplaceMeï¼šé«˜æ•ˆçš„æ— è®­ç»ƒæ·±åº¦å‰ªææ–¹æ³•",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºReplaceMeçš„æ·±åº¦å‰ªææ–¹æ³•ï¼Œå®ƒä¸éœ€è¦è®­ç»ƒå°±èƒ½æœ‰æ•ˆåœ°å°†å˜æ¢å™¨å—æ›¿æ¢ä¸ºçº¿æ€§æ“ä½œï¼ŒåŒæ—¶åœ¨ä½å‹ç¼©æ¯”ä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å‰ªææ–¹æ³•ä¸åŒï¼ŒReplaceMeåªéœ€ä¸€ä¸ªå°çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§å˜æ¢ï¼Œä»è€Œè¿‘ä¼¼å‰ªæåçš„å—ã€‚è¿™ä¸ªä¼°è®¡çš„çº¿æ€§æ˜ å°„å¯ä»¥ä¸å‰©ä½™çš„å˜æ¢å™¨å—æ— ç¼åˆå¹¶ï¼Œé¿å…äº†é¢å¤–çš„ç½‘ç»œå‚æ•°éœ€æ±‚ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReplaceMeåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå®ç°é«˜è¾¾25%çš„å‰ªæï¼ŒåŒæ—¶ä¿ç•™çº¦90%çš„åŸå§‹æ¨¡å‹æ€§èƒ½ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒæˆ–è°ƒæ•´æ­¥éª¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02391",
            "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
            "url": "https://huggingface.co/papers/2505.02391",
            "abstract": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.",
            "score": 18,
            "issue_id": 3605,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "48b57ad8bd358e5d",
            "authors": [
                "Jiarui Yao",
                "Yifan Hao",
                "Hanning Zhang",
                "Hanze Dong",
                "Wei Xiong",
                "Nan Jiang",
                "Tong Zhang"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02391.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GVM-RAFT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ 2-4-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ RAFT."
                },
                "en": {
                    "title": "Dynamic Resource Allocation for Enhanced Reasoning in LLMs",
                    "desc": "This paper addresses the challenge of improving chain-of-thought (CoT) reasoning in large language models (LLMs) by treating it as a latent variable problem. The authors introduce GVM-RAFT, a new method that dynamically allocates computational resources based on the difficulty of prompts, rather than using a fixed strategy. This approach minimizes the variance in stochastic gradient estimation, leading to faster convergence and better performance in reasoning tasks. Experimental results demonstrate that GVM-RAFT significantly outperforms previous methods, achieving notable speedups and accuracy gains."
                },
                "zh": {
                    "title": "åŠ¨æ€æ ·æœ¬åˆ†é…ï¼Œæå‡æ¨ç†æ•ˆç‡ï¼",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é“¾å¼æ¨ç†ï¼ˆCoTï¼‰å¦‚ä½•è¢«å½¢å¼åŒ–ä¸ºæ½œå˜é‡é—®é¢˜ï¼Œæ¨¡å‹éœ€è¦ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚ä»¥å¾€çš„æ–¹æ³•å¦‚è¿­ä»£å¥–åŠ±æ’åå¾®è°ƒï¼ˆRAFTï¼‰åœ¨å¤„ç†ä¸åŒéš¾åº¦çš„æç¤ºæ—¶ï¼Œé€šå¸¸é‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†é¢„ç®—ï¼Œè¿™å¯¼è‡´äº†æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†GVM-RAFTï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç‰¹å®šæç¤ºçš„åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼Œæ—¨åœ¨åœ¨è®¡ç®—é¢„ç®—é™åˆ¶ä¸‹æœ€å°åŒ–éšæœºæ¢¯åº¦æ–¹å·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGVM-RAFTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†2-4å€çš„åŠ é€Ÿå’Œæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.01658",
            "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
            "url": "https://huggingface.co/papers/2505.01658",
            "abstract": "Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine",
            "score": 18,
            "issue_id": 3604,
            "pub_date": "2025-05-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ",
                "en": "May 3",
                "zh": "5æœˆ3æ—¥"
            },
            "hash": "56f8dc116dbd737d",
            "authors": [
                "Sihyeong Park",
                "Sungryeol Jeon",
                "Chaelyn Lee",
                "Seokhun Jeon",
                "Byung-Soo Kim",
                "Jemin Lee"
            ],
            "affiliations": [
                "Electronics and Telecommunications Research Institute, South Korea",
                "Korea Electronics Technology Institute, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.01658.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#open_source",
                    "#optimization",
                    "#survey"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· 25 Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ¼, Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ·Ñ€ĞµĞ»Ğ¾ÑÑ‚ÑŒ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ LLM Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing Inference: A Deep Dive into LLM Engines",
                    "desc": "This paper evaluates 25 open-source and commercial inference engines designed for large language models (LLMs). It focuses on key factors such as ease-of-use, deployment, scalability, and performance in terms of throughput and latency. The study also investigates the optimization techniques supported by each engine and assesses the maturity of the open-source ecosystem compared to commercial solutions. Finally, it outlines future research directions to enhance LLM services, hardware support, and security, providing valuable insights for researchers and developers."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“çš„ç ”ç©¶ä¸è¯„ä¼°",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èŠå¤©æœºå™¨äººã€ä»£ç ç”Ÿæˆå™¨å’Œæœç´¢å¼•æ“ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç”±äºé“¾å¼æ€ç»´ã€å¤æ‚æ¨ç†å’Œä»£ç†æœåŠ¡ç­‰å·¥ä½œè´Ÿè½½çš„å¢åŠ ï¼Œæ¨ç†æˆæœ¬æ˜¾è‘—ä¸Šå‡ã€‚è™½ç„¶å·²ç»é‡‡ç”¨å¹¶è¡Œã€å‹ç¼©å’Œç¼“å­˜ç­‰ä¼˜åŒ–æ–¹æ³•æ¥é™ä½æˆæœ¬ï¼Œä½†å¤šæ ·åŒ–çš„æœåŠ¡éœ€æ±‚ä½¿å¾—é€‰æ‹©åˆé€‚çš„æ–¹æ³•å˜å¾—å›°éš¾ã€‚æœ¬æ–‡å¯¹25ä¸ªå¼€æºå’Œå•†ä¸šæ¨ç†å¼•æ“è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ¢è®¨äº†å®ƒä»¬çš„æ˜“ç”¨æ€§ã€å¯éƒ¨ç½²æ€§ã€é€šç”¨æ”¯æŒã€å¯æ‰©å±•æ€§ä»¥åŠé€‚åˆååé‡å’Œå»¶è¿Ÿæ„ŸçŸ¥è®¡ç®—çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02835",
            "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2505.02835",
            "abstract": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3% improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.",
            "score": 17,
            "issue_id": 3602,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "5134d59b0389ade9",
            "authors": [
                "Yi-Fan Zhang",
                "Xingyu Lu",
                "Xiao Hu",
                "Chaoyou Fu",
                "Bin Wen",
                "Tianke Zhang",
                "Changyi Liu",
                "Kaiyu Jiang",
                "Kaibing Chen",
                "Kaiyu Tang",
                "Haojie Ding",
                "Jiankang Chen",
                "Fan Yang",
                "Zhang Zhang",
                "Tingting Gao",
                "Liang Wang"
            ],
            "affiliations": [
                "CASIA",
                "KuaiShou",
                "NJU",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02835.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (MRM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ StableReinforce, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… RL-Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 200 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MRM. Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ R1-Reward, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ StableReinforce, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Stable Reinforcement for Enhanced Multimodal Reward Modeling",
                    "desc": "This paper investigates the use of Reinforcement Learning (RL) to enhance Multimodal Reward Models (MRMs) for Multimodal Large Language Models (MLLMs). It identifies challenges in applying traditional RL algorithms to reward modeling, which can lead to instability during training. To overcome these issues, the authors introduce the StableReinforce algorithm, which improves training dynamics through better loss refinement and reward design. The results show that their reward model, R1-Reward, outperforms existing state-of-the-art models on multimodal benchmarks, demonstrating the effectiveness of their approach."
                },
                "zh": {
                    "title": "ç¨³å®šå¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹æ€§èƒ½",
                    "desc": "å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰åœ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ€§èƒ½ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æ”¹å–„å¥–åŠ±å»ºæ¨¡ï¼Œæå‡ºå°†å¥–åŠ±å»ºæ¨¡é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºåŸºäºè§„åˆ™çš„RLä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºçš„StableReinforceç®—æ³•é€šè¿‡ä¼˜åŒ–è®­ç»ƒæŸå¤±ã€ä¼˜åŠ¿ä¼°è®¡ç­–ç•¥å’Œå¥–åŠ±è®¾è®¡ï¼Œè§£å†³äº†ç°æœ‰RLç®—æ³•åœ¨å¥–åŠ±å»ºæ¨¡ä¸­å¯¼è‡´çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚ç»è¿‡StableReinforceç®—æ³•è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹R1-Rewardåœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†RLç®—æ³•åœ¨ä¼˜åŒ–MRMsä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02156",
            "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents",
            "url": "https://huggingface.co/papers/2505.02156",
            "abstract": "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose Adaptive Mode Learning (AML) that strategically selects from four thinking modes (intuitive reaction rightarrow deep contemplation) based on real-time context. Our framework's core innovation, the Adaptive Mode Policy Optimization (AMPO) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach",
            "score": 16,
            "issue_id": 3602,
            "pub_date": "2025-05-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ",
                "en": "May 4",
                "zh": "5æœˆ4æ—¥"
            },
            "hash": "13e5cf390c136aeb",
            "authors": [
                "Minzheng Wang",
                "Yongbin Li",
                "Haobo Wang",
                "Xinghua Zhang",
                "Nan Xu",
                "Bingli Wu",
                "Fei Huang",
                "Haiyang Yu",
                "Wenji Mao"
            ],
            "affiliations": [
                "MAIS, Institute of Automation, Chinese Academy of Sciences",
                "Peking University",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02156.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Adaptive Mode Learning (AML) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². AML Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Adaptive Mode Policy Optimization (AMPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AML Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ° 15.6% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° 32.8% Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Dynamic Reasoning for Smarter Social Agents",
                    "desc": "This paper introduces Adaptive Mode Learning (AML), a new approach for simulating social intelligence in language agents. Unlike existing methods that either lack dynamic reasoning or use a one-size-fits-all strategy, AML allows agents to switch between four different thinking modes based on the context of the interaction. The core of this framework is the Adaptive Mode Policy Optimization (AMPO) algorithm, which enhances reasoning efficiency by adapting the depth of thought to the situation. Experimental results show that AML significantly improves task performance and reduces the length of reasoning chains compared to current state-of-the-art methods."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼šæå‡ç¤¾äº¤æ™ºèƒ½çš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼ˆAMLï¼‰ï¼Œæ—¨åœ¨æé«˜ç¤¾äº¤æ™ºèƒ½æ¨¡æ‹Ÿçš„æœ‰æ•ˆæ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨ç†æ·±åº¦ä¸Šç¼ºä¹çµæ´»æ€§ï¼Œå¯¼è‡´åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ¨ç†ä¸å¤Ÿé«˜æ•ˆã€‚AMLé€šè¿‡å®æ—¶ä¸Šä¸‹æ–‡åŠ¨æ€é€‰æ‹©å››ç§æ€ç»´æ¨¡å¼ï¼Œä»ç›´è§‰ååº”åˆ°æ·±åº¦æ€è€ƒï¼Œä¼˜åŒ–äº†æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAMLåœ¨ç¤¾äº¤æ™ºèƒ½ä»»åŠ¡ä¸­æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†15.6%çš„æ€§èƒ½ï¼ŒåŒæ—¶æ¨ç†é“¾é•¿åº¦å‡å°‘äº†32.8%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02094",
            "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations",
            "url": "https://huggingface.co/papers/2505.02094",
            "abstract": "We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.",
            "score": 14,
            "issue_id": 3604,
            "pub_date": "2025-05-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ",
                "en": "May 4",
                "zh": "5æœˆ4æ—¥"
            },
            "hash": "1ae5a69dab5de633",
            "authors": [
                "Runyi Yu",
                "Yinhuai Wang",
                "Qihan Zhao",
                "Hok Wai Tsui",
                "Jingbo Wang",
                "Ping Tan",
                "Qifeng Chen"
            ],
            "affiliations": [
                "HKUST Hong Kong, China",
                "Shanghai AI Laboratory Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02094.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#training",
                    "#rl",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (RLID). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹: Stitched Trajectory Graph (STG) Ğ¸ State Transition Field (STF). ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Adaptive Trajectory Sampling (ATS) Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğ¼ Ğ¾Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bridging Skills: Enhancing Reinforcement Learning with Augmented Trajectories",
                    "desc": "This paper tackles the issue of noise and limited coverage in Reinforcement Learning from Interaction Demonstration (RLID). It highlights that even with noisy and sparse data, there are countless feasible trajectories that can connect different demonstrated skills. The authors introduce two innovative techniques: the Stitched Trajectory Graph (STG) for identifying transitions between skills, and the State Transition Field (STF) for linking arbitrary states. Their method, combined with Adaptive Trajectory Sampling (ATS) and a historical encoding mechanism, enhances skill learning and generalization beyond the initial demonstrations, showing significant improvements in various tasks."
                },
                "zh": {
                    "title": "å…‹æœæ¼”ç¤ºå™ªå£°ï¼Œå®ç°æŠ€èƒ½çš„é²æ£’å­¦ä¹ ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä»äº¤äº’æ¼”ç¤ºä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLIDï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯æ¼”ç¤ºå™ªå£°å’Œè¦†ç›–é™åˆ¶ã€‚ç°æœ‰çš„æ•°æ®æ”¶é›†æ–¹æ³•è™½ç„¶æä¾›äº†æœ‰ä»·å€¼çš„äº¤äº’æ¼”ç¤ºï¼Œä½†å¾€å¾€å¯¼è‡´ç¨€ç–ã€æ–­è£‚å’Œå™ªå£°çš„è½¨è¿¹ï¼Œæ— æ³•å…¨é¢æ•æ‰æŠ€èƒ½å˜åŒ–å’Œè¿‡æ¸¡çš„å…¨è²Œã€‚æˆ‘ä»¬æå‡ºçš„å…³é”®è§è§£æ˜¯ï¼Œå°½ç®¡æ¼”ç¤ºå­˜åœ¨å™ªå£°å’Œç¨€ç–æ€§ï¼Œä½†ä»ç„¶å­˜åœ¨æ— é™çš„ç‰©ç†å¯è¡Œè½¨è¿¹ï¼Œå¯ä»¥è‡ªç„¶åœ°è¿æ¥æ¼”ç¤ºæŠ€èƒ½æˆ–ä»å…¶é‚»è¿‘çŠ¶æ€ä¸­äº§ç”Ÿã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜æŠ€èƒ½è·å–çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.01441",
            "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2505.01441",
            "abstract": "Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this work, we introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. Our results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.",
            "score": 14,
            "issue_id": 3603,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "4c0a590eccbb1960",
            "authors": [
                "Joykirat Singh",
                "Raghav Magazine",
                "Yash Pandya",
                "Akshay Nambi"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.01441.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ARTIST: ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ARTIST - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ARTIST Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ARTIST Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² LLM."
                },
                "en": {
                    "title": "Empowering LLMs with Dynamic Reasoning and Tool Integration",
                    "desc": "This paper presents ARTIST, a new framework that enhances large language models (LLMs) by integrating agentic reasoning, reinforcement learning (RL), and tool usage. Unlike traditional LLMs that rely on static knowledge, ARTIST allows models to dynamically decide when and how to use external tools during complex reasoning tasks. The framework employs outcome-based RL to improve the model's strategies for interacting with tools and environments, enabling it to learn without needing detailed step-by-step guidance. Experimental results demonstrate that ARTIST significantly outperforms existing models, achieving better reasoning and tool utilization in challenging scenarios."
                },
                "zh": {
                    "title": "ARTISTï¼šæ™ºèƒ½æ¨ç†ä¸å·¥å…·é›†æˆçš„æ–°å‰æ²¿",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç„¶å—åˆ°é™æ€å†…éƒ¨çŸ¥è¯†å’Œä»…åŸºäºæ–‡æœ¬æ¨ç†çš„é™åˆ¶ã€‚ç°å®ä¸–ç•Œçš„é—®é¢˜è§£å†³é€šå¸¸éœ€è¦åŠ¨æ€çš„å¤šæ­¥éª¤æ¨ç†ã€é€‚åº”æ€§å†³ç­–ä»¥åŠä¸å¤–éƒ¨å·¥å…·å’Œç¯å¢ƒçš„äº¤äº’èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ARTISTï¼ˆè‡ªæˆ‘æ”¹è¿›å˜æ¢å™¨ä¸­çš„ä»£ç†æ¨ç†å’Œå·¥å…·é›†æˆï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ä»£ç†æ¨ç†ã€å¼ºåŒ–å­¦ä¹ å’Œå·¥å…·é›†æˆç´§å¯†ç»“åˆçš„ç»Ÿä¸€æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARTISTåœ¨æ•°å­¦æ¨ç†å’Œå¤šè½®å‡½æ•°è°ƒç”¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æœ‰é«˜è¾¾22%çš„ç»å¯¹æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02471",
            "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction",
            "url": "https://huggingface.co/papers/2505.02471",
            "abstract": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.",
            "score": 9,
            "issue_id": 3602,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "fa1cacd261a625e3",
            "authors": [
                "Biao Gong",
                "Cheng Zou",
                "Dandan Zheng",
                "Hu Yu",
                "Jingdong Chen",
                "Jianxin Sun",
                "Junbo Zhao",
                "Jun Zhou",
                "Kaixiang Ji",
                "Lixiang Ru",
                "Libin Wang",
                "Qingpei Guo",
                "Rui Liu",
                "Weilong Chai",
                "Xinyu Xiao",
                "Ziyuan Huang"
            ],
            "affiliations": [
                "Ant Group",
                "Ming-Lite-Uni"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02471.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#multimodal",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ",
                    "desc": "Ming-Lite-Uni - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ming-Lite-Uni Ğ¸ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰ÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°."
                },
                "en": {
                    "title": "Unifying Vision and Language for Advanced AI",
                    "desc": "Ming-Lite-Uni is an innovative open-source framework designed to integrate vision and language through a unified visual generator and a multimodal autoregressive model. It introduces advanced techniques like multi-scale learnable tokens and representation alignment to enhance the interaction between text and images. By utilizing a fixed MLLM alongside a learnable diffusion model, it supports both text-to-image generation and image editing based on instructions. The framework's strong performance and interactive capabilities highlight its potential impact on the development of advanced AI systems, contributing to the journey towards artificial general intelligence (AGI)."
                },
                "zh": {
                    "title": "Ming-Lite-Uniï¼šç»Ÿä¸€è§†è§‰ä¸è¯­è¨€çš„å¤šæ¨¡æ€æ¡†æ¶",
                    "desc": "Ming-Lite-Uniæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨ç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ã€‚å®ƒå¼•å…¥äº†æ–°çš„è§†è§‰ç”Ÿæˆå™¨å’Œå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å¤šå°ºåº¦å¯å­¦ä¹ æ ‡è®°å’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ï¼Œæå‡äº†æ¨¡å‹çš„è¡¨ç°ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡éƒ½æ˜¯å¼€æºçš„ï¼Œé¼“åŠ±ç¤¾åŒºè¿›ä¸€æ­¥æ¢ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02370",
            "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
            "url": "https://huggingface.co/papers/2505.02370",
            "abstract": "Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.",
            "score": 9,
            "issue_id": 3602,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "5833f2e06b661a76",
            "authors": [
                "Ming Li",
                "Xin Gu",
                "Fan Chen",
                "Xiaoying Xing",
                "Longyin Wen",
                "Chen Chen",
                "Sijie Zhu"
            ],
            "affiliations": [
                "ByteDance Intelligent Creation (USA)",
                "Center for Research in Computer Vision, University of Central Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02370.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#training",
                    "#cv"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ñ… Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 9.19% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Real-Edit Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 30 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ² 13 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Image Editing with Accurate Instructions and Contrastive Supervision",
                    "desc": "This paper addresses the problem of noisy supervision in image editing tasks caused by mismatched editing instructions and image pairs. The authors propose a novel approach that involves creating more accurate editing instructions that align better with the original and edited images. They introduce contrastive editing instructions to enhance the effectiveness of these instructions and utilize a unified guide for vision-language models to rectify them. Their method improves supervision signals without relying on pre-training or VLM modules, achieving significant performance gains on benchmarks with less training data and smaller model sizes."
                },
                "zh": {
                    "title": "æå‡å›¾åƒç¼–è¾‘æ•ˆæœçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„å›¾åƒç¼–è¾‘æ¨¡å‹çš„ç›‘ç£ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºæ›´æœ‰æ•ˆçš„ç¼–è¾‘æŒ‡ä»¤ï¼Œä½¿å…¶ä¸åŸå§‹å’Œç¼–è¾‘åçš„å›¾åƒå¯¹æ›´å¥½åœ°å¯¹é½ï¼Œå¹¶å¼•å…¥å¯¹æ¯”ç¼–è¾‘æŒ‡ä»¤æ¥å¢å¼ºæ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œç¼–è¾‘æ¨¡å‹åœ¨ä¸åŒæ¨ç†æ­¥éª¤ä¸­è¡¨ç°å‡ºç‰¹å®šçš„ç”Ÿæˆå±æ€§ï¼Œè¿™äº›å±æ€§ä¸æ–‡æœ¬æ— å…³ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–äºä¹‹å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹æˆ–é¢„è®­ç»ƒä»»åŠ¡ï¼Œæä¾›äº†ä¸€ç§æ›´ç›´æ¥å’Œé«˜æ•ˆçš„ç›‘ç£ä¿¡å·ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç¼–è¾‘çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.01043",
            "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
            "url": "https://huggingface.co/papers/2505.01043",
            "abstract": "Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentsx2013such as weights, activations, and gradientsx2013each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.",
            "score": 9,
            "issue_id": 3602,
            "pub_date": "2025-05-02",
            "pub_date_card": {
                "ru": "2 Ğ¼Ğ°Ñ",
                "en": "May 2",
                "zh": "5æœˆ2æ—¥"
            },
            "hash": "b67e3ec75756e896",
            "authors": [
                "Zhiwei Hao",
                "Jianyuan Guo",
                "Li Shen",
                "Yong Luo",
                "Han Hu",
                "Guoxia Wang",
                "Dianhai Yu",
                "Yonggang Wen",
                "Dacheng Tao"
            ],
            "affiliations": [
                "Baidu Inc., Beijing 100000, China",
                "College of Computing and Data Science, Nanyang Technological University, 639798, Singapore",
                "Computer Science, City University of Hong Kong, Hong Kong 999077, China",
                "School of Computer Science, Wuhan University, Wuhan 430072, China",
                "School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, China",
                "School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.01043.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#survey",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¸Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¸ Ñ†ĞµĞ»Ñ‹Ñ… Ñ‡Ğ¸ÑĞµĞ», Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ¸ÑĞµĞ» Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ². Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ‡Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ±Ğ·Ğ¾Ñ€ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Optimizing Large Language Models with Low-Precision Training",
                    "desc": "This paper reviews low-precision training techniques for large language models (LLMs), which help improve training efficiency while reducing hardware resource demands. It categorizes these methods into three main groups based on numerical formats: fixed-point and integer-based, floating-point-based, and customized format-based methods. The survey also addresses quantization-aware training, which is closely related to low-precision training during model inference. By organizing the existing research, the paper aims to provide clarity and direction for future advancements in low-precision training."
                },
                "zh": {
                    "title": "ä½ç²¾åº¦è®­ç»ƒï¼šæå‡æ•ˆç‡çš„å…³é”®",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¡¨ç°ï¼Œä½†å…¶è®­ç»ƒæ‰€éœ€çš„ç¡¬ä»¶èµ„æºæå¤§ï¼Œæˆä¸ºæ•ˆç‡å’Œå¯æ‰©å±•æ€§çš„éšœç¢ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½ç²¾åº¦è®­ç»ƒæŠ€æœ¯è¢«å¹¿æ³›é‡‡ç”¨ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä½ç²¾åº¦è®­ç»ƒæ¶‰åŠå¤šä¸ªç»„ä»¶ï¼Œå¦‚æƒé‡ã€æ¿€æ´»å’Œæ¢¯åº¦ï¼Œæ¯ä¸ªç»„ä»¶å¯ä»¥ç”¨ä¸åŒçš„æ•°å€¼æ ¼å¼è¡¨ç¤ºï¼Œå¯¼è‡´ç ”ç©¶é¢†åŸŸçš„ç¢ç‰‡åŒ–ã€‚æœ¬æ–‡ç»¼è¿°äº†ç°æœ‰çš„ä½ç²¾åº¦è®­ç»ƒæ–¹æ³•ï¼Œå¹¶æ ¹æ®æ•°å€¼æ ¼å¼å°†å…¶ç³»ç»Ÿåœ°åˆ†ç±»ä¸ºä¸‰å¤§ç±»ï¼Œä»¥ä¾¿äºç ”ç©¶è€…ç†è§£å’Œå‚è€ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02625",
            "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
            "url": "https://huggingface.co/papers/2505.02625",
            "abstract": "Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.",
            "score": 7,
            "issue_id": 3602,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "2f8233e0d3083036",
            "authors": [
                "Qingkai Fang",
                "Yan Zhou",
                "Shoutao Guo",
                "Shaolei Zhang",
                "Yang Feng"
            ],
            "affiliations": [
                "Key Laboratory of AI Safety, Chinese Academy of Sciences",
                "Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02625.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#small_models",
                    "#benchmark"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaMA-Omni 2 - ÑĞµÑ€Ğ¸Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Qwen2.5 Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ° Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, LLaMA-Omni 2 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Speech Interaction with LLaMA-Omni 2",
                    "desc": "This paper presents LLaMA-Omni 2, a new series of speech language models designed for real-time speech interaction. These models, ranging from 0.5B to 14B parameters, utilize a speech encoder and an autoregressive streaming speech decoder to facilitate intelligent spoken chatbots. Remarkably, LLaMA-Omni 2 achieves high performance despite being trained on only 200K multi-turn speech dialogue samples. It outperforms previous models like GLM-4-Voice, which were trained on significantly larger datasets, showcasing the efficiency of the new architecture."
                },
                "zh": {
                    "title": "å®æ—¶æ™ºèƒ½è¯­éŸ³äº¤äº’çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†LLaMA-Omni 2ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»åˆ—çš„è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»0.5äº¿åˆ°14äº¿ï¼Œèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„å®æ—¶è¯­éŸ³äº¤äº’ã€‚è¯¥æ¨¡å‹åŸºäºQwen2.5ç³»åˆ—ï¼Œç»“åˆäº†è¯­éŸ³ç¼–ç å™¨å’Œè‡ªå›å½’æµå¼è¯­éŸ³è§£ç å™¨ã€‚å°½ç®¡ä»…åœ¨20ä¸‡å¤šè½®è¯­éŸ³å¯¹è¯æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒLLaMA-Omni 2åœ¨å¤šä¸ªè¯­éŸ³é—®ç­”å’Œè¯­éŸ³æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹GLM-4-Voiceã€‚è¿™è¡¨æ˜ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ•°æ®ä¹Ÿèƒ½è®­ç»ƒå‡ºé«˜æ•ˆçš„æ™ºèƒ½è¯­éŸ³èŠå¤©æœºå™¨äººã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.01583",
            "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action",
            "url": "https://huggingface.co/papers/2505.01583",
            "abstract": "Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.",
            "score": 6,
            "issue_id": 3602,
            "pub_date": "2025-05-02",
            "pub_date_card": {
                "ru": "2 Ğ¼Ğ°Ñ",
                "en": "May 2",
                "zh": "5æœˆ2æ—¥"
            },
            "hash": "d29565337415d11f",
            "authors": [
                "Jen-Hao Cheng",
                "Vivian Wang",
                "Huayu Wang",
                "Huapeng Zhou",
                "Yi-Hao Peng",
                "Hou-I Liu",
                "Hsiang-Wei Huang",
                "Kuang-Ming Chen",
                "Cheng-Yen Yang",
                "Wenhao Chai",
                "Yi-Ling Chen",
                "Vibhav Vineet",
                "Qin Cai",
                "Jenq-Neng Hwang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Microsoft",
                "National Yang Ming Chiao Tung University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.01583.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "TEMPURA: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹",
                    "desc": "TEMPURA - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. Ğ—Ğ°Ñ‚ĞµĞ¼ TEMPURA ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VER, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 1 Ğ¼Ğ»Ğ½ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ 500 Ñ‚Ñ‹Ñ. Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TEMPURA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "TEMPURA: Enhancing Video Understanding through Causal Reasoning and Temporal Segmentation",
                    "desc": "This paper introduces TEMPURA, a novel framework designed to improve how vision-language models understand the timing and relationships of events in videos. It addresses the limitations of existing methods that either lose important details by compressing video data or treat videos as continuous streams without clear boundaries. TEMPURA employs a two-stage training process that first predicts missing events to create causal explanations and then segments videos into distinct events with precise descriptions. The framework is trained on a large dataset, VER, and shows significant improvements in tasks like temporal grounding and highlight detection compared to existing models."
                },
                "zh": {
                    "title": "TEMPURAï¼šæå‡è§†é¢‘ç†è§£çš„å› æœæ¨ç†ä¸æ—¶é—´åˆ†å‰²ç»“åˆ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTEMPURAçš„æ¡†æ¶ï¼Œç”¨äºæé«˜è§†é¢‘çš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚TEMPURAé€šè¿‡æ©è”½äº‹ä»¶é¢„æµ‹æ¨ç†ï¼Œé‡å»ºç¼ºå¤±äº‹ä»¶å¹¶ç”Ÿæˆé€æ­¥çš„å› æœè§£é‡Šï¼Œä»è€Œæ›´å¥½åœ°ç†è§£è§†é¢‘ä¸­çš„äº‹ä»¶å…³ç³»ã€‚æ¥ç€ï¼Œå®ƒå­¦ä¹ è§†é¢‘åˆ†å‰²å’Œå¯†é›†æ ‡æ³¨ï¼Œå°†è§†é¢‘åˆ†è§£ä¸ºä¸é‡å çš„äº‹ä»¶ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ—¶é—´æˆ³å¯¹é½æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTEMPURAåœ¨æ—¶é—´å®šä½å’Œé«˜äº®æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†å› æœæ¨ç†ä¸ç»†ç²’åº¦æ—¶é—´åˆ†å‰²çš„ç»“åˆèƒ½å¤Ÿæå‡è§†é¢‘ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02823",
            "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing",
            "url": "https://huggingface.co/papers/2505.02823",
            "abstract": "Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.",
            "score": 3,
            "issue_id": 3603,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ",
                "en": "May 5",
                "zh": "5æœˆ5æ—¥"
            },
            "hash": "da9d48f7aea6c0bd",
            "authors": [
                "Zinan Guo",
                "Pengze Zhang",
                "Yanze Wu",
                "Chong Mou",
                "Songtao Zhao",
                "Qian He"
            ],
            "affiliations": [
                "Bytedance Intelligent Creation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02823.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#optimization",
                    "#dataset",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "MUSAR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ°Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "MUSAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ¸Ğ¿Ñ‚Ğ¸Ñ…Ğ°Ñ… Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. MUSAR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MUSAR Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "MUSAR: Unlocking Multi-Subject Customization with Single-Subject Data",
                    "desc": "The paper presents MUSAR, a novel framework designed to enhance multi-subject customization in machine learning. It addresses two main challenges: the scarcity of diverse multi-subject training data and the issue of attribute entanglement among different subjects. MUSAR employs debiased diptych learning to create training pairs from single-subject images, correcting biases through static attention routing and dual-branch LoRA. Additionally, it utilizes a dynamic attention routing mechanism to establish clear mappings between generated images and their corresponding subjects, leading to improved image quality and consistency across subjects."
                },
                "zh": {
                    "title": "MUSARï¼šå•ä¸€ä¸»é¢˜æ•°æ®ä¸‹çš„å¤šä¸»é¢˜å®šåˆ¶æ–°æ–¹æ³•",
                    "desc": "å½“å‰çš„å¤šä¸»é¢˜å®šåˆ¶æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè·å–å¤šæ ·åŒ–çš„å¤šä¸»é¢˜è®­ç»ƒæ•°æ®çš„å›°éš¾ï¼Œä»¥åŠä¸åŒä¸»é¢˜ä¹‹é—´å±æ€§çš„çº ç¼ ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MUSARæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä»…éœ€å•ä¸€ä¸»é¢˜è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°ç¨³å¥çš„å¤šä¸»é¢˜å®šåˆ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†å»åå·®çš„åŒè”å­¦ä¹ ï¼Œé€šè¿‡ä»å•ä¸€ä¸»é¢˜å›¾åƒæ„å»ºåŒè”è®­ç»ƒå¯¹ï¼Œä¿ƒè¿›å¤šä¸»é¢˜å­¦ä¹ ï¼Œå¹¶é€šè¿‡é™æ€æ³¨æ„åŠ›è·¯ç”±å’ŒåŒåˆ†æ”¯LoRAä¸»åŠ¨çº æ­£åŒè”æ„å»ºå¼•å…¥çš„åˆ†å¸ƒåå·®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€æ³¨æ„åŠ›è·¯ç”±æœºåˆ¶ï¼Œé€‚åº”æ€§åœ°å»ºç«‹ç”Ÿæˆå›¾åƒä¸æ¡ä»¶ä¸»é¢˜ä¹‹é—´çš„åŒå°„æ˜ å°„ï¼Œä»è€Œæ¶ˆé™¤è·¨ä¸»é¢˜çš„çº ç¼ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02005",
            "title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural\n  Radiance Fields",
            "url": "https://huggingface.co/papers/2505.02005",
            "abstract": "Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes (>6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.",
            "score": 3,
            "issue_id": 3611,
            "pub_date": "2025-05-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ",
                "en": "May 4",
                "zh": "5æœˆ4æ—¥"
            },
            "hash": "b0fc91d25884f885",
            "authors": [
                "Zhenxing Mi",
                "Ping Yin",
                "Xue Xiao",
                "Dan Xu"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR",
                "Inspur Cloud Information Technology Co, Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02005.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ NeRF Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑÑ†ĞµĞ½",
                    "desc": "Switch-NeRF++ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ (NeRF). Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½ÑƒÑ ÑĞ¼ĞµÑÑŒ Ñ…ĞµÑˆ-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (HMoHE) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… NeRF-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞµÑ‚ÑŒ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ…ĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ñ…ĞµÑˆ-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Switch-NeRF++ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Efficient Scene Decomposition for Scalable NeRFs",
                    "desc": "This paper presents Switch-NeRF++, a novel approach to improve the scalability and efficiency of Neural Radiance Fields (NeRF) for large-scale scenes. It introduces a Heterogeneous Mixture of Hash Experts (HMoHE) network that learns to decompose scenes into specialized components, allowing for better handling of scene heterogeneity. The framework utilizes a gating network that allocates 3D points to different NeRF experts, optimizing the learning process in an end-to-end manner. The results show significant improvements in both training and rendering speeds, achieving state-of-the-art accuracy on large-scale datasets."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSwitch-NeRF++çš„ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡åœºæ™¯ä¸­çš„åœºæ™¯åˆ†è§£ã€å»ºæ¨¡å¼‚è´¨æ€§å’Œå»ºæ¨¡æ•ˆç‡ç­‰é—®é¢˜ã€‚è¯¥ç½‘ç»œé‡‡ç”¨äº†å¼‚è´¨æ··åˆå“ˆå¸Œä¸“å®¶ï¼ˆHMoHEï¼‰æ¶æ„ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å­¦ä¹ å¼‚è´¨åˆ†è§£å’Œå¼‚è´¨NeRFã€‚é€šè¿‡ä¸€ä¸ªé—¨æ§ç½‘ç»œï¼Œç³»ç»Ÿèƒ½å¤Ÿå°†3Dç‚¹åˆ†é…ç»™ä¸“é—¨çš„NeRFä¸“å®¶ï¼Œä»è€Œå®ç°ç«¯åˆ°ç«¯çš„å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡ä¸­å…·æœ‰ä¼˜è¶Šçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè®­ç»ƒé€Ÿåº¦æå‡8å€ï¼Œæ¸²æŸ“é€Ÿåº¦æå‡16å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.01456",
            "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation",
            "url": "https://huggingface.co/papers/2505.01456",
            "abstract": "LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs.",
            "score": 2,
            "issue_id": 3608,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ",
                "en": "May 1",
                "zh": "5æœˆ1æ—¥"
            },
            "hash": "0201cbbcc6e52005",
            "authors": [
                "Vaidehi Patil",
                "Yi-Lin Sung",
                "Peter Hase",
                "Jie Peng",
                "Tianlong Chen",
                "Mohit Bansal"
            ],
            "affiliations": [
                "Department of Computer Science University of North Carolina at Chapel Hill",
                "School of Artificial Intelligence and Data Science University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.01456.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#benchmark",
                    "#interpretability",
                    "#security",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ—Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº UnLOK-VQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· MLLM. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑˆĞµÑÑ‚ÑŒ Ñ†ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ ÑĞµĞ¼ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°Ñ‚Ğ°Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ğ° Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ± Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Safety in Multimodal LLMs through Targeted Unlearning",
                    "desc": "This paper discusses the risks associated with large language models (LLMs) that are trained on extensive datasets, particularly the unintentional acquisition of sensitive information. It highlights the challenges of multimodal LLMs, which combine text and images, making them more vulnerable to adversarial attacks that can extract private details. The authors introduce a new benchmark called UnLOK-VQA for evaluating targeted unlearning in multimodal contexts, along with a framework for assessing methods to remove specific knowledge from these models. Their findings indicate that multimodal attacks are more effective than those targeting only text or images, and that larger models tend to be more robust against such attacks, emphasizing the need for improved unlearning techniques in multimodal settings."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ•æ„Ÿä¿¡æ¯é—å¿˜æŒ‘æˆ˜",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†æ•æ„Ÿä¿¡æ¯æ—¶çš„é£é™©ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒå’Œæ–‡æœ¬ç»“åˆçš„æƒ…å†µä¸‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨å¤šæ¨¡æ€æç¤ºæ¥æå–è¿™äº›æ•æ„Ÿä¿¡æ¯ï¼Œå› æ­¤éœ€è¦æœ‰æ•ˆçš„ç›®æ ‡æ€§é—å¿˜æœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•UnLOK-VQAï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªæ”»å‡»ä¸é˜²å¾¡æ¡†æ¶ï¼Œä»¥è¯„ä¼°ä»MLLMsä¸­åˆ é™¤ç‰¹å®šå¤šæ¨¡æ€çŸ¥è¯†çš„æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ¨¡æ€æ”»å‡»çš„æ•ˆæœä¼˜äºå•ä¸€æ–‡æœ¬æˆ–å›¾åƒæ”»å‡»ï¼Œè€Œæ›´å¤§çš„æ¨¡å‹åœ¨åæœŸç¼–è¾‘æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œè¡¨æ˜æ¨¡å‹è§„æ¨¡å¯¹å®‰å…¨æ€§æœ‰ç§¯æå½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02130",
            "title": "Attention Mechanisms Perspective: Exploring LLM Processing of\n  Graph-Structured Data",
            "url": "https://huggingface.co/papers/2505.02130",
            "abstract": "Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: https://github.com/millioniron/LLM_exploration{LLM4Exploration}",
            "score": 1,
            "issue_id": 3616,
            "pub_date": "2025-05-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ",
                "en": "May 4",
                "zh": "5æœˆ4æ—¥"
            },
            "hash": "a07adea642e1877d",
            "authors": [
                "Zhong Guan",
                "Likang Wu",
                "Hongke Zhao",
                "Ming He",
                "Jianpin Fan"
            ],
            "affiliations": [
                "AI Lab at Lenovo",
                "College of Management and Economics, Tianjin University, Tianjin, China",
                "Laboratory of Computation and Analytics of Complex Management Systems, Tianjin University, Tianjin, China",
                "ai-deepcube"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02130.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#training",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² LLM Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚, ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (LLM) Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ·Ğ»Ğ°Ğ¼Ğ¸. Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ LLM Ğ¿Ğ¾ ÑƒĞ·Ğ»Ğ°Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ½Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾ĞºĞ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking LLMs: Enhancing Graph Understanding with Attention",
                    "desc": "This paper investigates how large language models (LLMs) utilize attention mechanisms when processing graph-structured data. It highlights that while LLMs can identify interactions between text and graph nodes, they struggle with understanding the relationships between nodes due to their architectural limitations. The study reveals that the attention distribution in LLMs does not effectively reflect the underlying graph structure, leading to suboptimal performance. To address these issues, the authors propose using intermediate-state attention windows to enhance training and improve the transition to fully connected attention during inference."
                },
                "zh": {
                    "title": "æ¢ç´¢æ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾æ•°æ®ä¸­çš„è¡¨ç°",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†å›¾ç»“æ„æ•°æ®æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsèƒ½å¤Ÿè¯†åˆ«å›¾æ•°æ®å¹¶æ•æ‰æ–‡æœ¬ä¸èŠ‚ç‚¹ä¹‹é—´çš„äº¤äº’ï¼Œä½†åœ¨å»ºæ¨¡èŠ‚ç‚¹é—´å…³ç³»æ—¶å­˜åœ¨å›°éš¾ã€‚æ³¨æ„åŠ›åˆ†å¸ƒæœªèƒ½ä¸ç†æƒ³çš„ç»“æ„æ¨¡å¼å¯¹é½ï¼Œæ˜¾ç¤ºå‡ºå¯¹å›¾æ‹“æ‰‘çš„é€‚åº”æ€§ä¸è¶³ã€‚é€šè¿‡å¼•å…¥ä¸­é—´çŠ¶æ€çš„æ³¨æ„åŠ›çª—å£ï¼Œç ”ç©¶è¡¨æ˜å¯ä»¥æé«˜LLMsçš„è®­ç»ƒæ€§èƒ½ï¼Œå¹¶åœ¨æ¨ç†æ—¶æ— ç¼è¿‡æ¸¡åˆ°å®Œå…¨è¿æ¥çš„çª—å£ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-05.html",
    "link_next": "2025-05-07.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "05.05",
        "en": "05/05",
        "zh": "5æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "07.05",
        "en": "05/07",
        "zh": "5æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 3,
        "#benchmark": 10,
        "#agents": 3,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 2,
        "#video": 1,
        "#multimodal": 6,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 0,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 4,
        "#reasoning": 9,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 10,
        "#survey": 2,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºVoilaçš„è¯­éŸ³AIä»£ç†ã€‚å®ƒèƒ½è‡ªåŠ¨ã€å®æ—¶ä¸”å¯Œæœ‰æƒ…æ„Ÿåœ°ä¸äººäº’åŠ¨ã€‚Voilaé‡‡ç”¨ç«¯åˆ°ç«¯çš„æ¶æ„ï¼Œå®ç°ä½å»¶è¿Ÿçš„å…¨åŒå·¥å¯¹è¯ã€‚å®ƒç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¼ºå¤§çš„å£°å­¦å»ºæ¨¡ï¼Œæ”¯æŒè‡ªç„¶çš„è¯­éŸ³ç”Ÿæˆã€‚Voilaè¿˜æ”¯æŒè¶…è¿‡ä¸€ç™¾ä¸‡ç§é¢„è®¾å£°éŸ³å’Œé«˜æ•ˆçš„å®šåˆ¶æ–°å£°éŸ³ã€‚",
        "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng mÃ­ngwÃ¨i Voila de yÇ”yÄ«n AI dÃ ilÇ. TÄ nÃ©ng zÃ¬dÃ²ng, shÃ­shÃ­ qiÄ› fÃ¹yÇ’u qÃ­nggÇn de yÇ” rÃ©n hÃ¹dÃ²ng. Voila cÇiyÃ²ng duÄn dÃ o duÄn de jiÃ gÃ²u, shÃ­xiÃ n dÄ« yÃ¡nchÃ­ de quÃ¡n shuÄngxiÃ ng duÃ¬huÃ . TÄ jiÃ©hÃ© le dÃ  yÇ”yÃ¡n mÃ³xÃ­ng de tuÄ«lÇ nÃ©nglÃ¬ hÃ© qiÃ¡ngdÃ  de shÄ“ngxuÃ© jiÃ nmÃ³, zhÄ«chÃ­ zÃ¬rÃ¡n de yÇ”yÄ«n shÄ“ngchÃ©ng. Voila hÃ¡i zhÄ«chÃ­ chÄoguÃ² yÄ«bÇiwÃ n zhÇ’ng yÃ¹shÃ¨ shÄ“ngyÄ«n hÃ© gÄoxiÃ o de dÃ¬ngzhÃ¬ xÄ«n shÄ“ngyÄ«n.",
        "vocab": "[\n    {\"word\": \"è¯­éŸ³\", \"pinyin\": \"yÇ”yÄ«n\", \"trans\": \"voice\"},\n    {\"word\": \"AI\", \"pinyin\": \"Ä“i-Ã i\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ ilÇ\", \"trans\": \"agent\"},\n    {\"word\": \"è‡ªåŠ¨\", \"pinyin\": \"zÃ¬dÃ²ng\", \"trans\": \"automatic\"},\n    {\"word\": \"å®æ—¶\", \"pinyin\": \"shÃ­shÃ­\", \"trans\": \"real-time\"},\n    {\"word\": \"å¯Œæœ‰\", \"pinyin\": \"fÃ¹yÇ’u\", \"trans\": \"rich in\"},\n    {\"word\": \"æƒ…æ„Ÿ\", \"pinyin\": \"qÃ­nggÇn\", \"trans\": \"emotion\"},\n    {\"word\": \"äº’åŠ¨\", \"pinyin\": \"hÃ¹dÃ²ng\", \"trans\": \"interaction\"},\n    {\"word\": \"ç«¯åˆ°ç«¯\", \"pinyin\": \"duÄndÃ oduÄn\", \"trans\": \"end-to-end\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"ä½å»¶è¿Ÿ\", \"pinyin\": \"dÄ« yÃ¡nchÃ­\", \"trans\": \"low latency\"},\n    {\"word\": \"å…¨åŒå·¥\", \"pinyin\": \"quÃ¡n shuÄnggÅng\", \"trans\": \"full duplex\"},\n    {\"word\": \"å¯¹è¯\", \"pinyin\": \"duÃ¬huÃ \", \"trans\": \"dialogue\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ©hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©nglÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"å¼ºå¤§\", \"pinyin\": \"qiÃ¡ngdÃ \", \"trans\": \"powerful\"},\n    {\"word\": \"å£°å­¦\", \"pinyin\": \"shÄ“ngxuÃ©\", \"trans\": \"acoustics\"},\n    {\"word\": \"å»ºæ¨¡\", \"pinyin\": \"jiÃ nmÃ³\", \"trans\": \"modeling\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ«chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"è‡ªç„¶\", \"pinyin\": \"zÃ¬rÃ¡n\", \"trans\": \"natural\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generation\"},\n    {\"word\": \"è¶…è¿‡\", \"pinyin\": \"chÄoguÃ²\", \"trans\": \"exceed\"},\n    {\"word\": \"é¢„è®¾\", \"pinyin\": \"yÃ¹shÃ¨\", \"trans\": \"preset\"},\n    {\"word\": \"å£°éŸ³\", \"pinyin\": \"shÄ“ngyÄ«n\", \"trans\": \"sound\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄoxiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"å®šåˆ¶\", \"pinyin\": \"dÃ¬ngzhÃ¬\", \"trans\": \"customize\"},\n    {\"word\": \"æ–°\", \"pinyin\": \"xÄ«n\", \"trans\": \"new\"}\n]",
        "trans": "This article introduces a voice AI agent called Voila. It can interact with people automatically, in real-time, and with emotional richness. Voila employs an end-to-end architecture to achieve low-latency, full-duplex conversations. It combines the reasoning capabilities of large language models with powerful acoustic modeling to support natural voice generation. Voila also supports over a million preset voices and efficient customization of new voices.",
        "update_ts": "2025-05-06 09:12"
    }
}