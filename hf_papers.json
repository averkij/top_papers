{
    "date": {
        "ru": "26 ноября",
        "en": "November 26",
        "zh": "11月26日"
    },
    "time_utc": "2024-11-26 03:25",
    "weekday": 1,
    "issue_id": 777,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.15466",
            "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator",
            "url": "https://huggingface.co/papers/2411.15466",
            "abstract": "Subject-driven text-to-image generation aims to produce images of a new subject within a desired context by accurately capturing both the visual characteristics of the subject and the semantic content of a text prompt. Traditional methods rely on time- and resource-intensive fine-tuning for subject alignment, while recent zero-shot approaches leverage on-the-fly image prompting, often sacrificing subject alignment. In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Diptych Prompting arranges an incomplete diptych with the reference image in the left panel, and performs text-conditioned inpainting on the right panel. We further prevent unwanted content leakage by removing the background in the reference image and improve fine-grained details in the generated subject by enhancing attention weights between the panels during inpainting. Experimental results confirm that our approach significantly outperforms zero-shot image prompting methods, resulting in images that are visually preferred by users. Additionally, our method supports not only subject-driven generation but also stylized image generation and subject-driven image editing, demonstrating versatility across diverse image generation applications. Project page: https://diptychprompting.github.io/",
            "score": 9,
            "issue_id": 777,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 ноября",
                "en": "November 23",
                "zh": "11月23日"
            },
            "hash": "288600e8c54930f4",
            "authors": [
                "Chaehun Shin",
                "Jooyoung Choi",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15466.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Diptych Prompting: точная генерация изображений без дополнительного обучения",
                    "desc": "Статья представляет новый метод генерации изображений под названием Diptych Prompting. Этот подход использует свойство диптиха в крупномасштабных моделях text-to-image для точного воспроизведения субъекта в желаемом контексте. Метод интерпретирует задачу как инпейнтинг, размещая исходное изображение в левой части диптиха и генерируя правую часть на основе текстового промпта. Diptych Prompting превосходит существующие zero-shot методы и поддерживает различные приложения генерации изображений."
                },
                "en": {
                    "title": "Diptych Prompting: Zero-Shot Image Generation with Subject Precision",
                    "desc": "This paper presents Diptych Prompting, a new method for generating images from text prompts while maintaining accurate subject alignment. Unlike traditional methods that require extensive fine-tuning, this zero-shot approach treats the task as inpainting, using a diptych format with a reference image. By focusing on the left panel for the reference and performing text-conditioned inpainting on the right, the method enhances detail and prevents unwanted content from leaking into the generated image. The results show that Diptych Prompting not only improves visual quality but also allows for versatile applications in stylized image generation and editing."
                },
                "zh": {
                    "title": "Diptych Prompting：精准的主题驱动图像生成新方法",
                    "desc": "本文提出了一种新的零-shot方法，称为Diptych Prompting，旨在实现主题驱动的文本到图像生成。该方法通过将生成任务重新解释为图像修补，确保了主题的精确对齐。Diptych Prompting利用大型文本到图像模型的双联生成特性，左侧面板展示参考图像，右侧面板进行文本条件的修补。实验结果表明，该方法在视觉效果上优于传统的零-shot图像提示方法，且支持多种图像生成应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16657",
            "title": "DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation",
            "url": "https://huggingface.co/papers/2411.16657",
            "abstract": "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.",
            "score": 4,
            "issue_id": 777,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "02cf3312e8d1f6ca",
            "authors": [
                "Zun Wang",
                "Jialu Li",
                "Han Lin",
                "Jaehong Yoon",
                "Mohit Bansal"
            ],
            "affiliations": [
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16657.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#video",
                    "#story_generation"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "DreamRunner: От сценария к видео с помощью ИИ",
                    "desc": "DreamRunner - это новый метод генерации видео по текстовому сценарию, который использует большую языковую модель для структурирования входных данных. Он применяет адаптацию на основе извлечения информации для захвата целевых движений объектов в каждой сцене. DreamRunner также предлагает новый модуль пространственно-временного 3D-внимания и внедрения приоров для точного связывания объектов и движений. Метод демонстрирует передовые результаты в согласованности персонажей, соответствии тексту и плавных переходах."
                },
                "en": {
                    "title": "DreamRunner: Crafting Seamless Storytelling Videos from Text",
                    "desc": "This paper introduces DreamRunner, a new method for generating storytelling videos from text scripts. It addresses challenges in creating complex motions and maintaining object consistency across scenes by using a large language model for scene planning and a retrieval-augmented approach for motion customization. The method incorporates a novel spatial-temporal region-based attention module to ensure precise object-motion binding and semantic control in video frames. DreamRunner outperforms existing models in character consistency and smooth transitions, showcasing its effectiveness in generating multi-object interactions and adhering to compositional text prompts."
                },
                "zh": {
                    "title": "DreamRunner：创新的故事视频生成方法",
                    "desc": "故事视频生成（SVG）是一项新兴任务，旨在根据输入文本脚本创建长篇、多动作、多场景的视频。该方法面临着多个挑战，包括对象需要展现复杂的细微动作，以及多个对象在不同场景中的一致性。为了解决这些问题，我们提出了DreamRunner，这是一种新颖的故事到视频生成方法，利用大型语言模型（LLM）进行场景规划和对象布局。DreamRunner还引入了空间-时间区域基础的3D注意力机制，能够实现细粒度的对象动作绑定和逐帧语义控制，展现出在角色一致性和文本对齐方面的先进性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15138",
            "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
            "url": "https://huggingface.co/papers/2411.15138",
            "abstract": "We present Material Anything, a fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as a dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing a progressive material generation strategy guided by these confidence masks, along with a UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across a wide range of object categories and lighting conditions.",
            "score": 3,
            "issue_id": 776,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "34b8f6718115f1e3",
            "authors": [
                "Xin Huang",
                "Tengfei Wang",
                "Ziwei Liu",
                "Qing Wang"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15138.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная генерация материалов для 3D-объектов с помощью диффузии",
                    "desc": "В статье представлен Material Anything - полностью автоматизированный унифицированный фреймворк диффузии для генерации физически корректных материалов для 3D-объектов. В отличие от существующих методов, он предлагает надежное сквозное решение, адаптируемое к объектам в различных условиях освещения. Подход использует предобученную модель диффузии изображений с тройной архитектурой и функцией потерь рендеринга для улучшения стабильности и качества материалов. Также вводятся маски уверенности как динамический переключатель в модели диффузии, позволяющий эффективно обрабатывать объекты с текстурами и без них в различных условиях освещения."
                },
                "en": {
                    "title": "Automating Realistic Material Generation for 3D Objects",
                    "desc": "Material Anything is a novel framework that automates the generation of realistic materials for 3D objects using a unified diffusion approach. It simplifies the material creation process by eliminating the need for complex workflows and optimizations tailored to specific cases. The framework utilizes a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss, to ensure high-quality and stable material outputs. By incorporating confidence masks, it dynamically adapts to different object types and lighting scenarios, resulting in consistent and UV-ready materials across various conditions."
                },
                "zh": {
                    "title": "全自动材料生成，适应多种光照条件",
                    "desc": "本文介绍了一种名为Material Anything的全自动统一扩散框架，旨在为3D物体生成基于物理的材料。与现有方法依赖复杂流程或特定优化不同，Material Anything提供了一种稳健的端到端解决方案，适应不同光照条件下的物体。我们的方法利用了预训练的图像扩散模型，并通过三头架构和渲染损失来提高稳定性和材料质量。此外，我们引入了置信掩码作为扩散模型中的动态切换器，使其能够有效处理有纹理和无纹理的物体。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16085",
            "title": "Cautious Optimizers: Improving Training with One Line of Code",
            "url": "https://huggingface.co/papers/2411.16085",
            "abstract": "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to 1.47times. Code is available at https://github.com/kyleliang919/C-Optim",
            "score": 1,
            "issue_id": 777,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "48a2e1454fdde298",
            "authors": [
                "Kaizhao Liang",
                "Lizhang Chen",
                "Bo Liu",
                "Qiang Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.16085.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Осторожная оптимизация: простое изменение, большой результат",
                    "desc": "Статья представляет модификацию оптимизаторов на основе импульса, названную Cautious Optimizer. Авторы предлагают простое изменение в коде PyTorch, которое сохраняет функцию Гамильтона для Adam и гарантирует сходимость по анализу Ляпунова. Теоретический анализ раскрывает целое семейство новых оптимизаторов. Эмпирические эксперименты показывают ускорение предобучения Llama и MAE до 1.47 раз."
                },
                "en": {
                    "title": "Cautious Optimizer: Speeding Up Transformer Pretraining!",
                    "desc": "This paper introduces a new optimizer called the Cautious Optimizer, which is a simple modification of existing momentum-based optimizers like AdamW and Lion. The modification preserves the Hamiltonian function of Adam, ensuring that the convergence properties remain intact according to Lyapunov stability analysis. The authors demonstrate that this new optimizer can significantly speed up the pretraining of models like Llama and MAE, achieving improvements of up to 1.47 times. Additionally, the research opens the door to a new family of optimizers, expanding the options available for machine learning practitioners."
                },
                "zh": {
                    "title": "提升变换器预训练速度的新优化器",
                    "desc": "本文提出了一种新的优化器，称为Cautious Optimizer，旨在提高变换器预训练的速度和稳定性。通过对现有的动量优化器进行简单的修改，我们的理论分析表明，这种修改保持了Adam的哈密顿函数，并且在Lyapunov分析下不破坏收敛性保证。我们还揭示了一系列新的优化器，并选择了其中最简单的进行实证实验，结果显示在Llama和MAE预训练中速度提升可达1.47倍。相关代码已在GitHub上发布。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14486",
            "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
            "url": "https://huggingface.co/papers/2411.14486",
            "abstract": "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to acknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand challenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including both open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect responses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in fields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem difficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more challenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more prone to generate speculative answers when problems appear more tractable. The study also revealed significant variations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and NP-hard problems while performing relatively better on philosophical and psychological challenges. These results contribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the importance of uncertainty recognition as a critical component of future machine intelligence evaluation. This impossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing empirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting new directions for improving model training architectures and evaluation approaches.",
            "score": 1,
            "issue_id": 777,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 ноября",
                "en": "November 20",
                "zh": "11月20日"
            },
            "hash": "c30a94b30cace49a",
            "authors": [
                "David Noever",
                "Forrest McKee"
            ],
            "affiliations": [
                "PeopleTec, Inc., Huntsville, AL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14486.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#interpretability",
                    "#agi",
                    "#dataset"
                ],
                "emoji": "🤔",
                "ru": {
                    "title": "Признание незнания: ключевой аспект оценки искусственного интеллекта",
                    "desc": "Это исследование представляет новую систему оценки способности больших языковых моделей (LLM) признавать неопределенность на 675 принципиально нерешаемых проблемах. Двенадцать современных LLM были оценены на их склонность признавать незнание, а не генерировать правдоподобные, но неверные ответы. Лучшие модели показали точность 62-68% в признании неизвестности решения проблемы в различных областях. Исследование выявило обратную зависимость между сложностью проблемы и точностью модели, а также значительные различия между категориями проблем."
                },
                "en": {
                    "title": "Evaluating Uncertainty: A New Benchmark for Language Models",
                    "desc": "This research presents a new framework to evaluate how well large language models (LLMs) recognize their own uncertainty when faced with unsolvable problems. By testing twelve advanced LLMs on a set of graduate-level questions that have no answers, the study found that the best models could admit ignorance 62-68% of the time. Interestingly, the models were more likely to acknowledge uncertainty on harder problems, with GPT-4 showing a 35.8% acknowledgment rate on difficult questions. The findings emphasize the need for better training and evaluation methods to enhance LLMs' ability to recognize their knowledge limits, which is crucial for advancing artificial general intelligence (AGI)."
                },
                "zh": {
                    "title": "承认不确定性：评估大型语言模型的新视角",
                    "desc": "本研究提出了一种新的评估框架，用于评估大型语言模型（LLMs）在675个根本无法解决的问题上承认不确定性的能力。我们使用了一组经过精心挑选的研究生级别的重大挑战问题数据集，评估了包括开源和闭源模型在内的十二个最先进的LLMs，观察它们承认无知的倾向。结果显示，最佳模型在承认问题解决方案未知的准确率范围为62%到68%，并且在更具挑战性的问题上（如生物学、哲学和数学）表现出更高的不确定性承认率。研究还发现，不同问题类别之间存在显著差异，模型在承认发明和NP难题的不确定性时表现较差，而在哲学和心理学挑战中表现相对较好。"
                }
            }
        }
    ],
    "link_prev": "2024-11-25.html",
    "link_next": "2024-11-27.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "25.11",
        "en": "11/25",
        "zh": "11月25日"
    },
    "short_date_next": {
        "ru": "27.11",
        "en": "11/27",
        "zh": "11月27日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了大型扩散模型生成高质量图像，但难以学习新的个性化艺术风格。现有方法在微调时盲目使用预训练目标和噪声水平分布，导致风格对齐不佳。作者提出了风格友好的信噪比采样器，在微调时将信噪比分布偏向更高的噪声水平，以捕捉独特风格。这使模型能更好地生成具有高风格一致性的图像，并允许创建和共享新的风格模板。作者展示了生成水彩画、简笔画、3D渲染等多种风格的能力。",
        "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
        "pinyin": "这篇文章讨论了大型扩散模型生成高质量图像，但难以学习新的个性化艺术风格。现有方法在微调时盲目使用预训练目标和噪声水平分布，导致风格对齐不佳。作者提出了风格友好的信噪比采样器，在微调时将信噪比分布偏向更高的噪声水平，以捕捉独特风格。这使模型能更好地生成具有高风格一致性的图像，并允许创建和共享新的风格模板。作者展示了生成水彩画、简笔画、3D渲染等多种风格的能力。\n\nzhè piān wén zhāng tǎo lùn le dà xíng kuò sàn mó xíng shēng chéng gāo zhì liàng tú xiàng, dàn nán yǐ xué xí xīn de gè xìng huà yì shù fēng gé. xiàn yǒu fāng fǎ zài wēi tiáo shí māng mù shǐ yòng yù xùn liàn mù biāo hé zào shēng shuǐ píng fēn bù, dǎo zhì fēng gé duì qí bù jiā. zuò zhě tí chū le fēng gé yǒu hǎo de xìn zào bǐ cǎi yǎng qì, zài wēi tiáo shí jiāng xìn zào bǐ fēn bù piān xiàng gèng gāo de zào shēng shuǐ píng, yǐ bǔ zhuō dú tè fēng gé. zhè shǐ mó xíng néng gèng hǎo de shēng chéng jù yǒu gāo fēng gé yī zhì xìng de tú xiàng, bìng yǔn xǔ chuàng jiàn hé gòng xiǎng xīn de fēng gé mú bǎn. zuò zhě zhǎn shì le shēng chéng shuǐ cǎi huà, jiǎn bǐ huà, 3D xuàn rán děng duō zhǒng fēng gé de néng lì.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '个性化', 'pinyin': 'gè xìng huà', 'trans': 'personalized'}, {'word': '艺术', 'pinyin': 'yì shù', 'trans': 'art'}, {'word': '风格', 'pinyin': 'fēng gé', 'trans': 'style'}, {'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'}, {'word': '盲目', 'pinyin': 'máng mù', 'trans': 'blindly'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-trained'}, {'word': '目标', 'pinyin': 'mù biāo', 'trans': 'target'}, {'word': '噪声', 'pinyin': 'zào shēng', 'trans': 'noise'}, {'word': '水平', 'pinyin': 'shuǐ píng', 'trans': 'level'}, {'word': '分布', 'pinyin': 'fēn bù', 'trans': 'distribution'}, {'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'lead to'}, {'word': '对齐', 'pinyin': 'duì qí', 'trans': 'alignment'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '信噪比', 'pinyin': 'xìn zào bǐ', 'trans': 'signal-to-noise ratio'}, {'word': '采样器', 'pinyin': 'cǎi yàng qì', 'trans': 'sampler'}, {'word': '偏向', 'pinyin': 'piān xiàng', 'trans': 'bias towards'}, {'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'}, {'word': '独特', 'pinyin': 'dú tè', 'trans': 'unique'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '模板', 'pinyin': 'mú bǎn', 'trans': 'template'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '水彩画', 'pinyin': 'shuǐ cǎi huà', 'trans': 'watercolor painting'}, {'word': '简笔画', 'pinyin': 'jiǎn bǐ huà', 'trans': 'line drawing'}, {'word': '3D渲染', 'pinyin': '3D xuàn rán', 'trans': '3D rendering'}]",
        "trans": "This article discusses the ability of large diffusion models to generate high-quality images but their difficulty in learning new personalized artistic styles. Existing methods blindly use pre-trained objectives and noise level distributions during fine-tuning, leading to poor style alignment. The authors propose a style-friendly signal-to-noise ratio sampler that biases the signal-to-noise ratio distribution towards higher noise levels during fine-tuning to capture unique styles. This allows the model to generate images with higher style consistency and enables the creation and sharing of new style templates. The authors demonstrate the capability to generate various styles such as watercolor, sketch, and 3D rendering.",
        "update_ts": "2024-11-25 09:06"
    }
}