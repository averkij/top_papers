{
    "date": {
        "ru": "4 апреля",
        "en": "April 4",
        "zh": "4月4日"
    },
    "time_utc": "2025-04-04 02:20",
    "weekday": 4,
    "issue_id": 3063,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.02587",
            "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
            "url": "https://huggingface.co/papers/2504.02587",
            "abstract": "Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.",
            "score": 2,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "58300c3a6e30995f",
            "authors": [
                "Yan Ma",
                "Steffi Chern",
                "Xuyang Shen",
                "Yiran Zhong",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "Generative Artificial Intelligence Lab (GAIR)",
                "Minimax",
                "SII",
                "Shanghai Jiao Tong University (SJTU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02587.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прозрачное обучение с подкреплением для визуально-языковых моделей",
                    "desc": "Эта статья представляет новый подход к обучению с подкреплением (RL) для визуально-языковых моделей (VLM). Авторы предлагают прозрачную и воспроизводимую систему для применения RL в VLM, включающую четырехэтапный конвейер. Они также вводят стандартизированную схему оценки для анализа динамики обучения и рефлексивного поведения моделей. Эксперименты показывают, что RL превосходит обычное обучение с учителем в задачах визуального рассуждения и обобщения."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Vision-Language Models!",
                    "desc": "This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks."
                },
                "zh": {
                    "title": "建立可重复的强化学习框架",
                    "desc": "强化学习（RL）在提升大型语言模型的推理能力方面展现出强大的潜力，并正在积极扩展到视觉语言模型（VLMs）。然而，现有的RL应用往往依赖于复杂的框架，限制了可重复性和可访问性，同时缺乏标准化的评估协议，使得结果比较和训练动态解释变得困难。本文提出了一个透明的、从零开始的RL框架，提供了一个经过多个模型和数据集验证的最小功能四步流程。此外，提出了一种标准化的评估方案，以评估训练动态和反思行为。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02436",
            "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2504.02436",
            "abstract": "This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.",
            "score": 1,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "86b46513a72dbd76",
            "authors": [
                "Zhengcong Fei",
                "Debang Li",
                "Di Qiu",
                "Jiahua Wang",
                "Yikun Dou",
                "Rui Wang",
                "Jingtao Xu",
                "Mingyuan Fan",
                "Guibin Chen",
                "Yang Li",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02436.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#video",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Контролируемая генерация видео из отдельных элементов",
                    "desc": "SkyReels-A2 - это система генерации видео, способная собирать произвольные визуальные элементы в синтезированные видео на основе текстовых подсказок. Она использует модель совместного встраивания изображений и текста для сохранения согласованности элементов и глобальной связности. Авторы оптимизировали процесс вывода для скорости и стабильности, а также создали специальный набор данных для оценки. SkyReels-A2 является первой моделью с открытым исходным кодом коммерческого уровня для генерации видео из элементов (E2V)."
                },
                "en": {
                    "title": "SkyReels-A2: Mastering Video Generation with Element Control",
                    "desc": "This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation."
                },
                "zh": {
                    "title": "SkyReels-A2：可控视频生成的新突破",
                    "desc": "本文介绍了SkyReels-A2，一个可控的视频生成框架，能够根据文本提示将任意视觉元素（如角色、物体、背景）组合成合成视频，同时保持与每个元素的参考图像的一致性。我们将这一任务称为元素到视频（E2V），其主要挑战在于保持每个参考元素的真实性，确保场景的连贯性，以及实现自然的输出。为了解决这些问题，我们首先设计了一个全面的数据管道，以构建提示-参考-视频三元组用于模型训练。实验表明，我们的框架能够生成多样化、高质量的视频，并实现精确的元素控制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02119",
            "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
            "url": "https://huggingface.co/papers/2504.02119",
            "abstract": "Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.",
            "score": 1,
            "issue_id": 3063,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "7c31e20ce0a7813b",
            "authors": [
                "Wang Wei",
                "Tiankai Yang",
                "Hongjie Chen",
                "Ryan A. Rossi",
                "Yue Zhao",
                "Franck Dernoncourt",
                "Hoda Eldardiry"
            ],
            "affiliations": [
                "Adobe Research San Jose, CA, USA",
                "Adobe Research Seattle, WA, USA",
                "Department of Computer Science University of South California Los Angeles, CA, USA",
                "Department of Computer Science Virginia Tech Blacksburg, VA, USA",
                "Dolby Labs Atlanta, GA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02119.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LLM как эффективный инструмент выбора моделей в прогнозировании временных рядов",
                    "desc": "Статья предлагает использовать большие языковые модели (LLM) для автоматизации выбора моделей в прогнозировании временных рядов. Этот подход устраняет необходимость в предварительно созданных матрицах производительности, опираясь на внутренние знания и способности рассуждения LLM. Эксперименты с моделями LLaMA, GPT и Gemini показывают, что предложенный метод превосходит традиционные техники мета-обучения и эвристические базовые линии. Результаты подчеркивают потенциал LLM в эффективном выборе моделей для прогнозирования временных рядов."
                },
                "en": {
                    "title": "Revolutionizing Model Selection with Large Language Models",
                    "desc": "This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain."
                },
                "zh": {
                    "title": "利用大型语言模型优化时间序列预测的模型选择",
                    "desc": "本研究探讨了时间序列预测中的模型选择问题，传统方法需要在多个数据集上进行广泛的性能评估。我们提出了一种利用大型语言模型（LLMs）作为轻量级替代方案的方法，避免了构建昂贵的性能矩阵。通过与LLaMA、GPT和Gemini的广泛实验，我们的方法在性能上超越了传统的元学习技术和启发式基线，同时显著降低了计算开销。这些结果强调了LLMs在时间序列预测中高效模型选择的潜力。"
                }
            }
        }
    ],
    "link_prev": "2025-04-03.html",
    "link_next": "2025-04-07.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "03.04",
        "en": "04/03",
        "zh": "4月3日"
    },
    "short_date_next": {
        "ru": "07.04",
        "en": "04/07",
        "zh": "4月7日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的图像生成模型，叫做 MergeVQ。它结合了向量量化和 token 合并技术，旨在解决图像生成质量和表示学习效率之间的平衡问题。MergeVQ 在预训练阶段通过 token 合并模块提取语义信息，并在解码阶段恢复细节。实验结果显示，MergeVQ 在图像生成和表示学习任务中都表现出色，且效率高。代码和模型将在网上公开。",
        "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
        "pinyin": "这篇文章介绍了一种新的图像生成模型，叫做 MergeVQ。它结合了向量量化和 token 合并技术，旨在解决图像生成质量和表示学习效率之间的平衡问题。MergeVQ 在预训练阶段通过 token 合并模块提取语义信息，并在解码阶段恢复细节。实验结果显示，MergeVQ 在图像生成和表示学习任务中都表现出色，且效率高。代码和模型将在网上公开。\n\nZhè piān wénzhāng jièshào le yīzhǒng xīn de túxiàng shēngchéng móxíng, jiàozuò MergeVQ. Tā jiēhé le xiàngliàng liànggéhuà hé token hébìng jìshù, zhǐyú jiějué túxiàng shēngchéng zhìliàng hé biǎoshì xuéxí xiàoyì zhījiān de pínghéng wèntí. MergeVQ zài yùxùnliàn jiēduàn tōngguò token hébìng mókuài tíqǔ yǔyì xìnxī, bìng zài jiěmǎ jiēduàn huīfù xìjiè. Shíyàn jiéguǒ xiǎnshì, MergeVQ zài túxiàng shēngchéng hé biǎoshì xuéxí rènwù zhōng dōu biǎoxiàn chūsè, qiě xiàoyì gāo. Dàimǎ hé móxíng jiāng zài wǎngshàng gōngkāi.",
        "vocab": "[\n    {\"word\": \"向量量化\", \"pinyin\": \"xiàngliàng liàngzhì\", \"trans\": \"vector quantization\"},\n    {\"word\": \"token\", \"pinyin\": \"tōukèn\", \"trans\": \"token\"},\n    {\"word\": \"合并\", \"pinyin\": \"hébìng\", \"trans\": \"merge\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"平衡\", \"pinyin\": \"pínghéng\", \"trans\": \"balance\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùnliàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔyì\", \"trans\": \"semantic\"},\n    {\"word\": \"解码\", \"pinyin\": \"jiěmǎ\", \"trans\": \"decode\"},\n    {\"word\": \"恢复\", \"pinyin\": \"huīfù\", \"trans\": \"recover\"},\n    {\"word\": \"细节\", \"pinyin\": \"xìjié\", \"trans\": \"detail\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōngkāi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces a new image generation model called MergeVQ. It combines vector quantization and token merging techniques to address the balance between image generation quality and representation learning efficiency. MergeVQ extracts semantic information through a token merging module during the pre-training phase and recovers details during the decoding phase. Experimental results show that MergeVQ performs excellently in both image generation and representation learning tasks, with high efficiency. The code and model will be made publicly available online.",
        "update_ts": "2025-04-03 09:11"
    }
}