{
    "date": {
        "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 17",
        "zh": "4æœˆ17æ—¥"
    },
    "time_utc": "2025-04-17 03:30",
    "weekday": 3,
    "issue_id": 3281,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.12240",
            "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
            "url": "https://huggingface.co/papers/2504.12240",
            "abstract": "The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.",
            "score": 12,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "a237e12792a9a0c8",
            "authors": [
                "Junhao Zhuang",
                "Lingen Li",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Chun Yuan",
                "Ying Shan"
            ],
            "affiliations": [
                "Tencent ARC Lab, China",
                "The Chinese University of Hong Kong, China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12240.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#cv",
                    "#open_source",
                    "#architecture",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Cobra: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cobra Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Causal Sparse DiT Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°ÑĞºÑ€Ğ°ÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ 200 Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ². Cobra Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Cobra: Revolutionizing Line Art Colorization with Contextual Efficiency",
                    "desc": "This paper presents Cobra, a novel method for line art colorization in the comic production industry, which requires high accuracy and efficiency. Cobra utilizes a Causal Sparse DiT architecture that incorporates advanced techniques like causal sparse attention and positional encodings to handle over 200 reference images effectively. The method addresses challenges such as slow inference times and the need for flexible control, ensuring color identity consistency across diverse characters and backgrounds. Experimental results show that Cobra significantly improves the quality and speed of line art colorization, making it a valuable tool for artists."
                },
                "zh": {
                    "title": "Cobraï¼šé«˜æ•ˆçµæ´»çš„çº¿æ¡è‰ºæœ¯ä¸Šè‰²è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCobraçš„é«˜æ•ˆçº¿æ¡è‰ºæœ¯ä¸Šè‰²æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ¼«ç”»åˆ¶ä½œè¡Œä¸šä¸­å¯¹é«˜å‡†ç¡®æ€§å’Œçµæ´»æ§åˆ¶çš„éœ€æ±‚ã€‚Cobraèƒ½å¤Ÿå¤„ç†è¶…è¿‡200å¼ å‚è€ƒå›¾åƒï¼Œå¹¶ä¿æŒä½å»¶è¿Ÿï¼Œé€‚åº”å¤æ‚çš„è§’è‰²å’ŒèƒŒæ™¯ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†å› æœç¨€ç–DiTæ¶æ„ï¼Œåˆ©ç”¨ç‰¹æ®Šè®¾è®¡çš„ä½ç½®ç¼–ç å’Œå› æœç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆç®¡ç†é•¿ä¸Šä¸‹æ–‡å‚è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCobraåœ¨ä¸Šè‰²è´¨é‡å’Œæ¨ç†é€Ÿåº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œæ»¡è¶³äº†å·¥ä¸šç•Œçš„å…³é”®éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10514",
            "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
            "url": "https://huggingface.co/papers/2504.10514",
            "abstract": "Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.",
            "score": 12,
            "issue_id": 3281,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "c786e69f24be2f9e",
            "authors": [
                "Yijun Liang",
                "Ming Li",
                "Chenrui Fan",
                "Ziyue Li",
                "Dang Nguyen",
                "Kwesi Cobbina",
                "Shweta Bhardwaj",
                "Jiuhai Chen",
                "Fuxiao Liu",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10514.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸŒˆ",
                "ru": {
                    "title": "ColorBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ColorBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ†Ğ²ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 32 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ñ†Ğ²ĞµÑ‚Ğ°, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ° Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ColorBench Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Enhancing AI's Color Comprehension with ColorBench",
                    "desc": "This paper presents ColorBench, a benchmark designed to evaluate how vision-language models (VLMs) understand and utilize color in visual reasoning. It assesses various aspects of color perception, reasoning, and robustness through a series of real-world scenarios. The study finds that while larger models generally perform better, the existing VLMs show limited capabilities in color understanding, indicating a gap in their training. Additionally, the research highlights that while VLMs can use color cues effectively, they can also be misled by them, emphasizing the need for improved color comprehension in AI models."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„é¢œè‰²ç†è§£èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ColorBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é¢œè‰²ç†è§£æ–¹é¢èƒ½åŠ›çš„åŸºå‡†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æ›´å¤§çš„æ¨¡å‹åœ¨ColorBenchä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†è¯­è¨€æ¨¡å‹çš„ä½œç”¨æ¯”è§†è§‰ç¼–ç å™¨æ›´ä¸ºé‡è¦ã€‚ç°æœ‰çš„VLMsåœ¨é¢œè‰²ç†è§£æ–¹é¢çš„è¡¨ç°å·®è·è¾ƒå°ï¼Œè¡¨æ˜è¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†é‡è§†ã€‚æ­¤å¤–ï¼Œå°½ç®¡VLMsèƒ½å¤Ÿåˆ©ç”¨é¢œè‰²çº¿ç´¢ï¼Œä½†åœ¨æŸäº›ä»»åŠ¡ä¸­ä¹Ÿå¯èƒ½ä¼šå—åˆ°è¯¯å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12285",
            "title": "BitNet b1.58 2B4T Technical Report",
            "url": "https://huggingface.co/papers/2504.12285",
            "abstract": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.",
            "score": 2,
            "issue_id": 3281,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "cf67f70d9f122792",
            "authors": [
                "Shuming Ma",
                "Hongyu Wang",
                "Shaohan Huang",
                "Xingxing Zhang",
                "Ying Hu",
                "Ting Song",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12285.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#science",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸: 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ LLM Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ BitNet b1.58 2B4T - Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 4 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ° Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. BitNet b1.58 2B4T Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… LLM Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ½Ğ¾ Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Hugging Face Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ GPU Ğ¸ CPU."
                },
                "en": {
                    "title": "Efficient Language Understanding with BitNet: The 1-Bit Revolution",
                    "desc": "BitNet b1.58 2B4T is a groundbreaking 1-bit Large Language Model (LLM) with 2 billion parameters, making it the first of its kind to be open-source. It has been trained on an extensive dataset of 4 trillion tokens and evaluated on various benchmarks, showcasing its capabilities in language understanding, mathematical reasoning, coding, and conversation. Remarkably, BitNet achieves performance comparable to other leading full-precision LLMs while being more efficient in terms of memory usage, energy consumption, and decoding speed. The model's weights and inference implementations are made available on Hugging Face, promoting further research and practical applications."
                },
                "zh": {
                    "title": "å¼€æºé«˜æ•ˆçš„1ä½å¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†BitNet b1.58 2B4Tï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æºçš„ã€åŸç”Ÿçš„1ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡è¾¾åˆ°20äº¿ã€‚è¯¥æ¨¡å‹åœ¨4ä¸‡äº¿ä¸ªæ ‡è®°çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨è¯­è¨€ç†è§£ã€æ•°å­¦æ¨ç†ã€ç¼–ç¨‹èƒ½åŠ›å’Œå¯¹è¯èƒ½åŠ›ç­‰åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒBitNet b1.58 2B4Tåœ¨æ€§èƒ½ä¸Šä¸åŒç±»è§„æ¨¡çš„é¢†å…ˆå¼€æºå…¨ç²¾åº¦å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬æ˜¾è‘—å‡å°‘çš„å†…å­˜å ç”¨ã€èƒ½è€—å’Œè§£ç å»¶è¿Ÿã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨ï¼Œè¯¥æ¨¡å‹çš„æƒé‡é€šè¿‡Hugging Faceå‘å¸ƒï¼Œå¹¶æä¾›äº†é€‚ç”¨äºGPUå’ŒCPUæ¶æ„çš„å¼€æºæ¨ç†å®ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11952",
            "title": "Robust and Fine-Grained Detection of AI Generated Texts",
            "url": "https://huggingface.co/papers/2504.11952",
            "abstract": "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.",
            "score": 2,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "bdea465fe17b9401",
            "authors": [
                "Ram Mohan Rao Kadiyala",
                "Siddartha Pullakhandam",
                "Kanwal Mehreen",
                "Drishti Sharma",
                "Siddhant Gupta",
                "Jebish Purbey",
                "Ashay Srivastava",
                "Subhasya TippaReddy",
                "Arvind Reddy Bobbili",
                "Suraj Telugara Chandrashekhar",
                "Modabbir Adeeb",
                "Srinadh Vura",
                "Hamza Farooq"
            ],
            "affiliations": [
                "Cohere for AI Community",
                "IISc Bangalore",
                "IIT Roorkee",
                "M2ai.in",
                "Pulchowk Campus",
                "Stanford University",
                "Traversaal.ai",
                "University of California, Los Angeles",
                "University of Houston",
                "University of Maryland, College Park",
                "University of South Florida",
                "Vantager"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11952.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#hallucinations",
                    "#dataset",
                    "#benchmark",
                    "#multilingual",
                    "#security",
                    "#data"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ˜Ğ˜. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¸Ğ· Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ¾Ñ‚ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° 23 ÑĞ·Ñ‹ĞºĞ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¾Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğµ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Advancing Detection of Human-LLM Co-Authored Texts",
                    "desc": "This paper addresses the challenge of detecting machine-generated content, particularly in cases where texts are co-authored by humans and language models (LLMs). The authors developed a set of token classification models trained on a large dataset of 2.4 million co-authored texts, which allows for better detection across various domains and generators. The models demonstrated strong performance even with adversarial inputs and texts from non-native speakers. Additionally, the paper provides insights into how the models perform based on text length and characteristics compared to purely human-authored content."
                },
                "zh": {
                    "title": "æ„å»ºé«˜æ•ˆçš„æœºå™¨ç”Ÿæˆå†…å®¹æ£€æµ‹ç³»ç»Ÿ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç†æƒ³çš„æ£€æµ‹ç³»ç»Ÿï¼Œæ—¨åœ¨æœ‰æ•ˆè¯†åˆ«æœºå™¨ç”Ÿæˆçš„å†…å®¹ï¼Œå°¤å…¶æ˜¯åœ¨çŸ­æ–‡æœ¬ä¸­ã€‚ç°æœ‰ç³»ç»Ÿåœ¨è¯†åˆ«AIç”Ÿæˆå†…å®¹æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå› æ­¤æˆ‘ä»¬ä¸“æ³¨äºäººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…±åŒåˆ›ä½œçš„æ–‡æœ¬ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ç³»åˆ—ç”¨äºæ ‡è®°åˆ†ç±»çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤§é‡äººæœºå…±åˆ›æ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æœªè§é¢†åŸŸå’Œç”Ÿæˆå™¨çš„æ–‡æœ¬ä¸Šè¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å«240ä¸‡æ¡æ–‡æœ¬çš„æ–°æ•°æ®é›†ï¼Œä¸»è¦ç”±å¤šç§æµè¡Œçš„ä¸“æœ‰LLMå…±åŒåˆ›ä½œï¼Œæ¶µç›–23ç§è¯­è¨€ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-16.html",
    "link_next": "2025-04-18.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.04",
        "en": "04/18",
        "zh": "4æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„å…´è¶£ã€‚ç›®å‰çš„æ–¹æ³•ä¾èµ–ç›‘ç£ä¿¡å·ï¼Œå­˜åœ¨å¯æ‰©å±•æ€§å’Œé«˜æ ‡æ³¨æˆæœ¬é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„è‡ªè®­ç»ƒæ¡†æ¶ï¼Œåä¸ºGeniusã€‚Geniusé€šè¿‡æ­¥è¿›å¼é¢„æµ‹é‡é‡‡æ ·ç­–ç•¥å’Œä¼˜åŠ¿æ ¡å‡†ä¼˜åŒ–ï¼ˆACOï¼‰æŸå¤±å‡½æ•°ï¼Œå®ç°äº†æ— éœ€å¤–éƒ¨ç›‘ç£çš„LLMæ¨ç†èƒ½åŠ›æå‡ã€‚ä»£ç å°†åœ¨https://github.com/xufangzhi/Geniuså‘å¸ƒã€‚",
        "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„å…´è¶£ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le tÃ­shÄ“ng dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLM) tuÄ«lÇ nÃ©nglÃ¬ de xÃ¬ngqÃ¹.\n\nç›®å‰çš„æ–¹æ³•ä¾èµ–ç›‘ç£ä¿¡å·ï¼Œå­˜åœ¨å¯æ‰©å±•æ€§å’Œé«˜æ ‡æ³¨æˆæœ¬é—®é¢˜ã€‚\nMÃ¹qiÃ¡n de fÄngfÇ yÄ«lÃ i jiÃ ndÅ« xÃ¬nhÃ ",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­ shÄ“ng\", \"trans\": \"improve\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ  xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"è¯­è¨€æ¨¡å‹\", \"pinyin\": \"yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"language model\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"å…´è¶£\", \"pinyin\": \"xÃ¬ng qÃ¹\", \"trans\": \"interest\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ« lÃ i\", \"trans\": \"depend on\"},\n    {\"word\": \"ç›‘ç£\", \"pinyin\": \"jiÃ n dÅ«\", \"trans\": \"supervised\"},\n    {\"word\": \"ä¿¡å·\", \"pinyin\": \"xÃ¬n hÃ o\", \"trans\": \"signal\"},\n    {\"word\": \"å¯æ‰©å±•æ€§\", \"pinyin\": \"kÄ› kuÃ² zhÇn xÃ¬ng\", \"trans\": \"scalability\"},\n    {\"word\": \"é«˜\", \"pinyin\": \"gÄo\", \"trans\": \"high\"},\n    {\"word\": \"æ ‡æ³¨\", \"pinyin\": \"biÄo zhÃ¹\", \"trans\": \"annotation\"},\n    {\"word\": \"æˆæœ¬\", \"pinyin\": \"chÃ©ng bÄ›n\", \"trans\": \"cost\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"æ— ç›‘ç£\", \"pinyin\": \"wÃº jiÃ n dÅ«\", \"trans\": \"unsupervised\"},\n    {\"word\": \"è‡ªè®­ç»ƒ\", \"pinyin\": \"zÃ¬ xÃ¹n liÃ n\", \"trans\": \"self-training\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"åä¸º\", \"pinyin\": \"mÃ­ng wÃ©i\", \"trans\": \"named\"},\n    {\"word\": \"æ­¥è¿›å¼\", \"pinyin\": \"bÃ¹ jÃ¬n shÃ¬\", \"trans\": \"stepwise\"},\n    {\"word\": \"é¢„æµ‹\", \"pinyin\": \"yÃ¹ cÃ¨\", \"trans\": \"prediction\"},\n    {\"word\": \"é‡é‡‡æ ·\", \"pinyin\": \"chÃ³ng cÇi yÃ ng\", \"trans\": \"resampling\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"ä¼˜åŠ¿\", \"pinyin\": \"yÅu shÃ¬\", \"trans\": \"advantage\"},\n    {\"word\": \"æ ¡å‡†\", \"pinyin\": \"jiÃ o zhÇ”n\", \"trans\": \"calibration\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅu huÃ \", \"trans\": \"optimization\"},\n    {\"word\": \"æŸå¤±å‡½æ•°\", \"pinyin\": \"sÇ”n shÄ« hÃ¡n shÃ¹\", \"trans\": \"loss function\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"å¤–éƒ¨\", \"pinyin\": \"wÃ i bÃ¹\", \"trans\": \"external\"},\n    {\"word\": \"å‘å¸ƒ\", \"pinyin\": \"fÄ bÃ¹\", \"trans\": \"release\"}\n]",
        "trans": "This article discusses the interest in enhancing the reasoning capabilities of large language models (LLMs). Current methods rely on supervised signals, which present issues with scalability and high annotation costs. The authors",
        "update_ts": "2025-04-16 09:12"
    }
}