{
    "date": {
        "ru": "28 Ğ¼Ğ°Ñ",
        "en": "May 28",
        "zh": "5æœˆ28æ—¥"
    },
    "time_utc": "2025-05-28 06:17",
    "weekday": 2,
    "issue_id": 3994,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.18445",
            "title": "OmniConsistency: Learning Style-Agnostic Consistency from Paired\n  Stylization Data",
            "url": "https://huggingface.co/papers/2505.18445",
            "abstract": "OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose OmniConsistency, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.",
            "score": 45,
            "issue_id": 3990,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ",
                "en": "May 24",
                "zh": "5æœˆ24æ—¥"
            },
            "hash": "0a5e56835e542da2",
            "authors": [
                "Yiren Song",
                "Cheng Liu",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18445.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#diffusion",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ğ»Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "OmniConsistency - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ (DiTs) Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ»Ğ°Ğ³Ğ¸Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰ÑƒÑ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. OmniConsistency ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ñ‹Ğ¼Ğ¸ LoRA Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Achieving Consistent and High-Quality Image Stylization with OmniConsistency",
                    "desc": "OmniConsistency is a novel approach that improves the consistency and generalization of image stylization using large-scale Diffusion Transformers. It addresses two main challenges in image-to-image pipelines: ensuring consistent stylization across complex scenes and preventing degradation of style when using style LoRAs. The method introduces a learning framework that focuses on maintaining consistency while allowing for flexible style application. Experimental results demonstrate that OmniConsistency achieves visual quality and coherence comparable to leading commercial models."
                },
                "zh": {
                    "title": "OmniConsistencyï¼šæå‡å›¾åƒé£æ ¼ä¸€è‡´æ€§çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "OmniConsistency æ˜¯ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡æ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformersï¼‰æ¥å¢å¼ºå›¾åƒåˆ°å›¾åƒç®¡é“ä¸­çš„é£æ ¼ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤æ‚åœºæ™¯ä¸­ä¿æŒä¸€è‡´é£æ ¼å’Œé˜²æ­¢é£æ ¼é€€åŒ–çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚OmniConsistency æä¾›äº†ä¸€ç§åŸºäºå¯¹é½å›¾åƒå¯¹çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µçš„æ¸è¿›å­¦ä¹ ç­–ç•¥æ¥åˆ†ç¦»é£æ ¼å­¦ä¹ ä¸ä¸€è‡´æ€§ä¿æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniConsistency æ˜¾è‘—æé«˜äº†è§†è§‰è¿è´¯æ€§å’Œç¾å­¦è´¨é‡ï¼Œæ€§èƒ½å¯ä¸å•†ä¸šæœ€å…ˆè¿›æ¨¡å‹ GPT-4o ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21497",
            "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers",
            "url": "https://huggingface.co/papers/2505.21497",
            "abstract": "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.",
            "score": 37,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "7f740f76be754bce",
            "authors": [
                "Wei Pang",
                "Kevin Qinghong Lin",
                "Xiangru Jian",
                "Xi He",
                "Philip Torr"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Oxford",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21497.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#agents",
                    "#science"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ²: Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ñ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ PosterAgent - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Parser Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Planner Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ° Ğ¸ Painter-Commenter Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen-2.5 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ° 87% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼Ñƒ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Academic Poster Generation with PosterAgent",
                    "desc": "This paper addresses the challenge of generating academic posters from lengthy scientific documents by introducing a benchmark and metric suite for evaluation. It presents PosterAgent, a multi-agent pipeline that includes a Parser for structuring content, a Planner for layout design, and a Painter-Commenter loop for refining visuals based on feedback. The study evaluates the effectiveness of generated posters using metrics like visual quality, textual coherence, and the ability to convey core content through quizzes. The results show that their open-source approach significantly outperforms existing models while being more efficient in token usage, paving the way for future advancements in automated poster generation."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–å­¦æœ¯æµ·æŠ¥ç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å­¦æœ¯æµ·æŠ¥ç”ŸæˆåŸºå‡†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œæ—¨åœ¨å°†é•¿ç¯‡æ–‡æ¡£å‹ç¼©ä¸ºè§†è§‰ä¸Šè¿è´¯çš„å•é¡µæµ·æŠ¥ã€‚æˆ‘ä»¬æå‡ºäº†PosterAgentï¼Œä¸€ä¸ªå¤šä»£ç†ç®¡é“ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£æã€è§„åˆ’å’Œç»˜åˆ¶æµ·æŠ¥å†…å®¹ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è¾“å‡ºï¼Œæˆ‘ä»¬å‘ç°äººç±»è®¾è®¡çš„æµ·æŠ¥åœ¨è§†è§‰è¯­ä¹‰ä¸Šæ›´å…·å¸å¼•åŠ›ï¼Œè€ŒGPT-4oæ¨¡å‹è™½ç„¶å¤–è§‚ç¾è§‚ï¼Œä½†æ–‡æœ¬è´¨é‡å’Œä¿¡æ¯ä¼ è¾¾èƒ½åŠ›è¾ƒå·®ã€‚æˆ‘ä»¬çš„å¼€æºå˜ä½“åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰ç³»ç»Ÿï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„è®¡ç®—èµ„æºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21327",
            "title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
            "url": "https://huggingface.co/papers/2505.21327",
            "abstract": "MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  \t\t\t\t\tAI-generated summary \t\t\t\t Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.",
            "score": 30,
            "issue_id": 3991,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "a022fb524c5969bf",
            "authors": [
                "Jiakang Yuan",
                "Tianshuo Peng",
                "Yilei Jiang",
                "Yiting Lu",
                "Renrui Zhang",
                "Kaituo Feng",
                "Chaoyou Fu",
                "Tao Chen",
                "Lei Bai",
                "Bo Zhang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "Fudan University",
                "MMLab, The Chinese University of Hong Kong",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21327.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "MME-Reasoning - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ, Ğ´ĞµĞ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ°Ğ±Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ, Ğ° Ğ½Ğµ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ñ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº 'Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ' Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unveiling the Reasoning Gaps in Multimodal AI",
                    "desc": "The paper introduces MME-Reasoning, a benchmark designed to assess the logical reasoning abilities of multimodal large language models (MLLMs). It categorizes reasoning into three types: inductive, deductive, and abductive, addressing gaps in existing evaluations that often overlook these distinctions. The study reveals that even advanced MLLMs struggle with logical reasoning tasks, showing significant performance imbalances across the different reasoning types. Additionally, the paper analyzes common methods aimed at improving reasoning, highlighting the persistent limitations of current MLLMs in effectively handling diverse logical reasoning challenges."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›",
                    "desc": "MME-Reasoning æ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€»è¾‘æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œæ­ç¤ºäº†åœ¨å½’çº³ã€æ¼”ç»å’Œæº¯å› æ¨ç†ç±»å‹ä¸Šçš„æ˜¾è‘—å±€é™æ€§å’Œæ€§èƒ½ä¸å¹³è¡¡ã€‚å°½ç®¡å¤šæ¨¡æ€æ¨ç†å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰åŸºå‡†æœªèƒ½å…¨é¢è¯„ä¼°å…¶æ¨ç†èƒ½åŠ›ï¼Œç¼ºä¹å¯¹é€»è¾‘æ¨ç†ç±»å‹çš„æ˜ç¡®åˆ†ç±»ã€‚æˆ‘ä»¬è®¾è®¡äº† MME-Reasoningï¼Œæ¶µç›–æ‰€æœ‰ä¸‰ç§æ¨ç†ç±»å‹çš„é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªé—®é¢˜æœ‰æ•ˆè¯„ä¼°æ¨ç†èƒ½åŠ›ï¼Œè€Œéæ„ŸçŸ¥æŠ€èƒ½æˆ–çŸ¥è¯†å¹¿åº¦ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„ MLLMs åœ¨å…¨é¢çš„é€»è¾‘æ¨ç†è¯„ä¼°ä¸­è¡¨ç°æœ‰é™ï¼Œä¸”åœ¨ä¸åŒæ¨ç†ç±»å‹ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19000",
            "title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied\n  Iterative Policy Optimization",
            "url": "https://huggingface.co/papers/2505.19000",
            "abstract": "A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Applying Reinforcement Learning (RL) to Video Large Language Models (Video-LLMs) shows significant promise for complex video reasoning. However, popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group Relative Policy Optimization (GRPO), are limited by data preparation bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the quality of long chain-of-thoughts (CoTs) and downstream performance.To address these limitations, we propose VerIPO, a Verifier-guided Iterative Policy Optimization method designed to gradually improve video LLMs' capacity for generating deep, long-term reasoning chains. The core component is Rollout-Aware Verifier, positioned between the GRPO and Direct Preference Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop. This verifier leverages small LLMs as a judge to assess the reasoning logic of rollouts, enabling the construction of high-quality contrastive data, including reflective and contextually consistent CoTs. These curated preference samples drive the efficient DPO stage (7x faster than GRPO), leading to marked improvements in reasoning chain quality, especially in terms of length and contextual consistency. This training loop benefits from GRPO's expansive search and DPO's targeted optimization. Experimental results demonstrate: 1) Significantly faster and more effective optimization compared to standard GRPO variants, yielding superior performance; 2) Our trained models exceed the direct inference of large-scale instruction-tuned Video-LLMs, producing long and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long reasoning models (e.g., Video-R1), highlighting its effectiveness and stability.",
            "score": 30,
            "issue_id": 3991,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ",
                "en": "May 25",
                "zh": "5æœˆ25æ—¥"
            },
            "hash": "058fcf46b0f20cc6",
            "authors": [
                "Yunxin Li",
                "Xinyu Chen",
                "Zitao Li",
                "Zhenyu Liu",
                "Longyue Wang",
                "Wenhan Luo",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Alibaba International Group",
                "Division of AMC and Department of ECE, HKUST",
                "Harbin Institute of Technology, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19000.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#video",
                    "#optimization",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VerIPO: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ VerIPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Verifier Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°Ğ·Ğ°Ğ¼Ğ¸ GRPO Ğ¸ DPO Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. VerIPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 7 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GRPO. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ VerIPO Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Video Reasoning with Verifier-guided Optimization",
                    "desc": "This paper introduces VerIPO, a new method for improving Video Large Language Models (Video-LLMs) using a Verifier-guided Iterative Policy Optimization approach. It addresses the limitations of existing Reinforcement Fine-Tuning methods by incorporating a Rollout-Aware Verifier that enhances the quality of reasoning chains during training. By creating high-quality contrastive data, this method allows for faster and more effective optimization, achieving results that are significantly better than traditional methods. Experimental findings show that VerIPO not only speeds up the training process but also improves the contextual consistency and length of reasoning outputs in video tasks."
                },
                "zh": {
                    "title": "éªŒè¯è€…å¼•å¯¼çš„è¿­ä»£ä¼˜åŒ–ï¼Œæå‡è§†é¢‘æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVerIPOçš„éªŒè¯è€…å¼•å¯¼è¿­ä»£ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æå‡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨GRPOå’ŒDPOé˜¶æ®µä¹‹é—´å¼•å…¥ä¸€ä¸ªå›æ»šæ„ŸçŸ¥éªŒè¯å™¨ï¼Œå½¢æˆGRPO-éªŒè¯å™¨-DPOè®­ç»ƒå¾ªç¯ï¼Œä»è€Œå®ç°æ›´å¿«ä¸”æ›´æœ‰æ•ˆçš„ä¼˜åŒ–ã€‚éªŒè¯å™¨åˆ©ç”¨å°å‹è¯­è¨€æ¨¡å‹è¯„ä¼°æ¨ç†é€»è¾‘ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¯¹æ¯”æ•°æ®ï¼Œä¿ƒè¿›äº†é•¿é“¾æ¨ç†çš„ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVerIPOåœ¨ä¼˜åŒ–é€Ÿåº¦å’Œæ¨ç†è´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„GRPOæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21374",
            "title": "Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?",
            "url": "https://huggingface.co/papers/2505.21374",
            "abstract": "Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a \"Holmes-test\" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes.",
            "score": 24,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "0683137770e562ab",
            "authors": [
                "Junhao Cheng",
                "Yuying Ge",
                "Teng Wang",
                "Yixiao Ge",
                "Jing Liao",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "City University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21374.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ¨ĞµÑ€Ğ»Ğ¾Ğº Ğ¥Ğ¾Ğ»Ğ¼Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Video-Holmes - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹-ÑĞ°ÑĞ¿ĞµĞ½Ñ Ğ¸ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1837 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 270 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 7 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemini-2.5-Pro Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 45%, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Video-Holmes: A New Benchmark for Complex Video Reasoning",
                    "desc": "The Video-Holmes benchmark assesses the complex video reasoning abilities of Multimodal Language Models (MLLMs) using suspense short films. It highlights the challenges these models face in integrating information compared to human experts, particularly in real-world reasoning scenarios. The benchmark includes 1,837 questions based on 270 annotated films, requiring models to connect multiple visual clues across different segments. Despite advancements in reasoning techniques, the evaluation shows that even the best models struggle with accuracy, achieving only 45%, indicating significant room for improvement in multimodal reasoning."
                },
                "zh": {
                    "title": "Video-Holmesï¼šæ¿€åŠ±æ¨¡å‹æ›´åƒäººç±»æ¨ç†çš„åŸºå‡†æµ‹è¯•",
                    "desc": "Video-HolmesåŸºå‡†æµ‹è¯•è¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚è§†é¢‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ‚¬ç–‘çŸ­ç‰‡æ¥æ­ç¤ºä¸äººç±»ä¸“å®¶ç›¸æ¯”çš„ä¿¡æ¯æ•´åˆæŒ‘æˆ˜ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ª270éƒ¨æ‰‹åŠ¨æ³¨é‡Šçš„æ‚¬ç–‘çŸ­ç‰‡çš„1837ä¸ªé—®é¢˜ï¼Œè®¾è®¡äº†ä¸ƒä¸ªä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹ä¸»åŠ¨å¯»æ‰¾å’Œè¿æ¥åˆ†æ•£åœ¨ä¸åŒè§†é¢‘ç‰‡æ®µä¸­çš„å¤šä¸ªç›¸å…³è§†è§‰çº¿ç´¢ã€‚å°½ç®¡ç°æœ‰çš„MLLMsåœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¿¡æ¯æ•´åˆä¸Šå´é¢ä¸´é‡å¤§å›°éš¾ï¼Œè®¸å¤šæ¨¡å‹çš„å‡†ç¡®ç‡ä½äº40%ã€‚æˆ‘ä»¬å¸Œæœ›Video-Holmesèƒ½å¤Ÿæ¿€åŠ±æ¨¡å‹æ›´åƒäººç±»è¿›è¡Œæ¨ç†ï¼Œå¹¶å¼ºè°ƒè¿™ä¸€é¢†åŸŸçš„æŒç»­æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20355",
            "title": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient\n  Fine-Tuning",
            "url": "https://huggingface.co/papers/2505.20355",
            "abstract": "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git",
            "score": 23,
            "issue_id": 3990,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "d4035428ea14ce6b",
            "authors": [
                "Yeonjoon Jung",
                "Daehyun Ahn",
                "Hyungjun Kim",
                "Taesu Kim",
                "Eunhyeok Park"
            ],
            "affiliations": [
                "POSTECH",
                "SqueezeBits"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20355.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "GraLoRA: Ğ“Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ°Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Granular Low-Rank Adaptation (GraLoRA). GraLoRA Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Low-Rank Adaptation (LoRA), ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ½Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ±Ğ»Ğ¾ĞºĞ¸, ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ¾ ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GraLoRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ LoRA Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 8.5% Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Pass@1 Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ HumanEval+."
                },
                "en": {
                    "title": "GraLoRA: Unlocking the Power of Fine-Tuning with Granular Adaptation",
                    "desc": "This paper introduces Granular Low-Rank Adaptation (GraLoRA), a new method designed to improve the performance of Low-Rank Adaptation (LoRA) in fine-tuning generative models. LoRA is effective but struggles with overfitting when the rank is increased, leading to poor accuracy compared to full fine-tuning. GraLoRA addresses this issue by dividing weight matrices into smaller sub-blocks, allowing each to have its own low-rank adapter, which enhances gradient propagation and reduces entanglement. Experimental results demonstrate that GraLoRA significantly outperforms LoRA and other methods, achieving notable improvements in various benchmarks without increasing computational costs."
                },
                "zh": {
                    "title": "é¢—ç²’ä½ç§©é€‚åº”ï¼šè¶…è¶ŠLoRAçš„é«˜æ•ˆå¾®è°ƒ",
                    "desc": "ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§æµè¡Œçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œå› å…¶ç®€å•æœ‰æ•ˆè€Œå—åˆ°é‡è§†ã€‚å°½ç®¡æœ€è¿‘æœ‰æ‰€æ”¹è¿›ï¼ŒLoRAä»ç„¶é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§é™åˆ¶ï¼šå½“ç“¶é¢ˆåŠ å®½æ—¶å®¹æ˜“è¿‡æ‹Ÿåˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç»“æ„ï¼Œç§°ä¸ºé¢—ç²’ä½ç§©é€‚åº”ï¼ˆGraLoRAï¼‰ï¼Œå®ƒå°†æƒé‡çŸ©é˜µåˆ’åˆ†ä¸ºå­å—ï¼Œæ¯ä¸ªå­å—éƒ½æœ‰è‡ªå·±çš„ä½ç§©é€‚é…å™¨ï¼Œä»è€Œå…‹æœäº†LoRAçš„å±€é™æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒGraLoRAåœ¨ä»£ç ç”Ÿæˆå’Œå¸¸è¯†æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜äºLoRAï¼Œå…·æœ‰æ›´å¼ºçš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21333",
            "title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios",
            "url": "https://huggingface.co/papers/2505.21333",
            "abstract": "MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.",
            "score": 22,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "25639980b7f7add5",
            "authors": [
                "Yang Shi",
                "Huanqian Wang",
                "Wulin Xie",
                "Huanyao Zhang",
                "Lijie Zhao",
                "Yi-Fan Zhang",
                "Xinfeng Li",
                "Chaoyou Fu",
                "Zhuoer Wen",
                "Wenting Liu",
                "Zhuoran Zhang",
                "Xinlong Chen",
                "Bohan Zeng",
                "Sihan Yang",
                "Yuanxing Zhang",
                "Pengfei Wan",
                "Haotian Wang",
                "Wenjing Yang"
            ],
            "affiliations": [
                "CASIA",
                "CUHKSZ",
                "Kuaishou",
                "NTU",
                "PKU",
                "THU",
                "XJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21333.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#benchmark",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ OCR Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR) Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MME-VideoOCR, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 10 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 25 Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ MLLM Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ OCR. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 18 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ OCR Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Video OCR: Bridging the Gap in MLLM Performance",
                    "desc": "This paper discusses the challenges faced by Multimodal Large Language Models (MLLMs) in performing Optical Character Recognition (OCR) on videos. It highlights that factors like motion blur and temporal variations significantly reduce their accuracy compared to static images. To address these issues, the authors introduce the MME-VideoOCR benchmark, which includes a variety of tasks designed to test spatio-temporal reasoning and language understanding in video contexts. The evaluation of 18 MLLMs reveals that even the best models struggle with comprehensive video comprehension, particularly in scenarios requiring integration of information across multiple frames."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘OCRçš„å¤šæ¨¡æ€åŸºå‡†æŒ‘æˆ˜",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é™æ€å›¾åƒçš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è§†é¢‘OCRä¸­æ•ˆæœæ˜¾è‘—ä¸‹é™ã€‚è¿™æ˜¯ç”±äºè§†é¢‘å†…å®¹ä¸­çš„è¿åŠ¨æ¨¡ç³Šã€æ—¶é—´å˜åŒ–å’Œè§†è§‰æ•ˆæœç­‰å› ç´ å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MME-VideoOCRåŸºå‡†ï¼Œæ¶µç›–äº†å¤šç§è§†é¢‘OCRåº”ç”¨åœºæ™¯ï¼ŒåŒ…å«10ä¸ªä»»åŠ¡ç±»åˆ«å’Œ25ä¸ªå…·ä½“ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„MLLMsåœ¨å¤„ç†éœ€è¦æ•´ä½“è§†é¢‘ç†è§£çš„ä»»åŠ¡æ—¶èƒ½åŠ›æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨æ—¶ç©ºæ¨ç†å’Œè·¨å¸§ä¿¡æ¯æ•´åˆæ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21297",
            "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset",
            "url": "https://huggingface.co/papers/2505.21297",
            "abstract": "A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar.",
            "score": 18,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "1fdb1a9edd20c73b",
            "authors": [
                "Yifei Liu",
                "Li Lyna Zhang",
                "Yi Zhu",
                "Bingcheng Dong",
                "Xudong Zhou",
                "Ning Shang",
                "Fan Yang",
                "Mao Yang"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Microsoft Research Asia",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21297.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "rStar-Coder: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¾ ĞºĞ¾Ğ´Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ rStar-Coder - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¾ ĞºĞ¾Ğ´Ğµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 418 Ñ‚Ñ‹ÑÑÑ‡ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, 580 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ rStar-Coder Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ² Ğ¸Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ LLM Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ´Ğµ, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LiveCodeBench Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-14B ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° ÑĞ²Ğ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ñ 23.3% Ğ´Ğ¾ 62.5% Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° rStar-Coder."
                },
                "en": {
                    "title": "Unlocking Code Reasoning with rStar-Coder",
                    "desc": "The paper presents rStar-Coder, a large-scale dataset designed to enhance code reasoning capabilities in large language models (LLMs). It addresses the challenge of limited high-difficulty datasets by providing 418,000 verified code problems and 580,000 long-reasoning solutions, complete with diverse test cases. The dataset is created through a three-step process that includes curating competitive programming problems, synthesizing input-output test cases, and verifying solutions. Experiments show that models trained on rStar-Coder significantly outperform existing benchmarks, demonstrating its effectiveness in improving code reasoning tasks."
                },
                "zh": {
                    "title": "æå‡ä»£ç æ¨ç†èƒ½åŠ›çš„rStar-Coderæ•°æ®é›†",
                    "desc": "rStar-Coderæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«418,000ä¸ªç«äº‰çº§åˆ«çš„ä»£ç é—®é¢˜å’Œ580,000ä¸ªé•¿æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œé…å¤‡ä¸°å¯Œçš„æµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–ä¸åŒéš¾åº¦ã€‚é€šè¿‡ä¸‰é¡¹æ ¸å¿ƒè´¡çŒ®ï¼ŒrStar-Coderæä¾›äº†å¯éªŒè¯çš„è¾“å…¥è¾“å‡ºæµ‹è¯•æ¡ˆä¾‹ï¼Œç¡®ä¿äº†è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨rStar-Coderçš„æ•°æ®é›†ï¼ŒQwenæ¨¡å‹åœ¨å¤šä¸ªä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20292",
            "title": "OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for\n  Subject-to-Video Generation",
            "url": "https://huggingface.co/papers/2505.20292",
            "abstract": "Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and (ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, to accurately align human preferences with S2V benchmarks, we propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to separately quantify subject consistency, naturalness, and text relevance in generated videos. Building on this, we conduct a comprehensive evaluation of 16 representative S2V models, highlighting their strengths and weaknesses across different content. Moreover, we create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality 720P subject-text-video triples. Specifically, we ensure subject-information diversity in our dataset by (1) segmenting subjects and building pairing information via cross-video associations and (2) prompting GPT-Image-1 on raw frames to synthesize multi-view representations. Through OpenS2V-Nexus, we deliver a robust infrastructure to accelerate future S2V generation research.",
            "score": 16,
            "issue_id": 3990,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "e2a8d12789199cde",
            "authors": [
                "Shenghai Yuan",
                "Xianyi He",
                "Yufan Deng",
                "Yang Ye",
                "Jinfa Huang",
                "Bin Lin",
                "Chongyang Ma",
                "Jiebo Luo",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Rabbitpre AI",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20292.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "OpenS2V-Nexus: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OpenS2V-Nexus - Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ (Subject-to-Video, S2V). ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ OpenS2V-Eval - Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ OpenS2V-5M - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² ÑÑƒĞ±ÑŠĞµĞºÑ‚-Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°, ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° 16 Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… S2V Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‰Ğ°Ñ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Subject Fidelity",
                    "desc": "The paper introduces Subject-to-Video (S2V) generation, which focuses on creating videos that accurately reflect reference content. It presents OpenS2V-Nexus, a framework that includes OpenS2V-Eval, a detailed benchmark for evaluating video generation, and OpenS2V-5M, a large dataset of five million subject-text-video pairs. Unlike previous benchmarks, OpenS2V-Eval emphasizes the generation of videos that maintain subject consistency and natural appearance. The authors also propose three new metrics to assess generated videos based on subject fidelity, naturalness, and relevance to the input text, facilitating a comprehensive evaluation of various S2V models."
                },
                "zh": {
                    "title": "æ„å»ºè§†é¢‘ç”Ÿæˆçš„æ–°åŸºå‡†ä¸æ•°æ®é›†",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†Subject-to-Video (S2V) ç”Ÿæˆçš„åŸºç¡€è®¾æ–½OpenS2V-Nexusï¼Œæ—¨åœ¨åˆ›å»ºå¿ å®äºå‚è€ƒå†…å®¹çš„è§†é¢‘ã€‚æˆ‘ä»¬å¼•å…¥äº†OpenS2V-Evalï¼Œä¸€ä¸ªç»†ç²’åº¦çš„åŸºå‡†ï¼Œä¸“æ³¨äºç”Ÿæˆå…·æœ‰è‡ªç„¶å¤–è§‚å’Œèº«ä»½ä¿çœŸåº¦çš„ä¸€è‡´è§†é¢‘ã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆè§†é¢‘çš„è´¨é‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ç§è‡ªåŠ¨åŒ–æŒ‡æ ‡ï¼Œåˆ†åˆ«é‡åŒ–ä¸»é¢˜ä¸€è‡´æ€§ã€è‡ªç„¶æ€§å’Œæ–‡æœ¬ç›¸å…³æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«äº”ç™¾ä¸‡ä¸ªé«˜è´¨é‡720Pä¸»é¢˜-æ–‡æœ¬-è§†é¢‘ä¸‰å…ƒç»„çš„å¼€æ”¾æºä»£ç æ•°æ®é›†OpenS2V-5Mï¼Œä»¥æ”¯æŒæœªæ¥çš„S2Vç”Ÿæˆç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18943",
            "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems",
            "url": "https://huggingface.co/papers/2505.18943",
            "abstract": "MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) a Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.",
            "score": 16,
            "issue_id": 3990,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ",
                "en": "May 25",
                "zh": "5æœˆ25æ—¥"
            },
            "hash": "718f1062d34a47a7",
            "authors": [
                "Xuanming Zhang",
                "Yuxuan Chen",
                "Min-Hsuan Yeh",
                "Yixuan Li"
            ],
            "affiliations": [
                "Tsinghua University",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18943.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#agents",
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MetaMind: Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "MetaMind - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·, Ğ¸Ñ… ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². MetaMind Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 35.7% Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Ğ°Ğ³ĞµĞ½Ñ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑƒĞ¼ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."
                },
                "en": {
                    "title": "Empowering AI with Human-like Social Intelligence",
                    "desc": "MetaMind is a multi-agent framework that enhances large language models (LLMs) by improving their ability to understand human social interactions through Theory of Mind (ToM) tasks. It breaks down social understanding into three key stages: generating hypotheses about mental states, refining these hypotheses with cultural and ethical considerations, and producing contextually appropriate responses. This approach allows LLMs to achieve human-like performance in social reasoning, showing significant improvements in real-world scenarios and ToM reasoning tasks. The framework demonstrates the importance of each component in achieving a balance between contextual relevance and social appropriateness, paving the way for more empathetic AI interactions."
                },
                "zh": {
                    "title": "MetaMindï¼šæå‡AIçš„ç¤¾ä¼šæ™ºèƒ½",
                    "desc": "MetaMindæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œçµæ„Ÿæ¥æºäºå…ƒè®¤çŸ¥ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¿ƒæ™ºç†è®ºä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å°†ç¤¾ä¼šç†è§£åˆ†è§£ä¸ºä¸‰ä¸ªåä½œé˜¶æ®µï¼šé¦–å…ˆï¼Œå¿ƒæ™ºç†è®ºä»£ç†ç”Ÿæˆç”¨æˆ·å¿ƒç†çŠ¶æ€çš„å‡è®¾ï¼›å…¶æ¬¡ï¼Œé¢†åŸŸä»£ç†åˆ©ç”¨æ–‡åŒ–è§„èŒƒå’Œä¼¦ç†çº¦æŸæ¥ç»†åŒ–è¿™äº›å‡è®¾ï¼›æœ€åï¼Œå“åº”ä»£ç†ç”Ÿæˆç¬¦åˆä¸Šä¸‹æ–‡çš„é€‚å½“å›åº”ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMetaMindåœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé¦–æ¬¡ä½¿LLMsåœ¨å…³é”®çš„å¿ƒæ™ºç†è®ºä»»åŠ¡ä¸Šè¾¾åˆ°äººç±»æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18875",
            "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via\n  Semantic-Aware Permutation",
            "url": "https://huggingface.co/papers/2505.18875",
            "abstract": "SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30x and 1.89x speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively.",
            "score": 16,
            "issue_id": 3990,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ",
                "en": "May 24",
                "zh": "5æœˆ24æ—¥"
            },
            "hash": "bc68e232d8897ad4",
            "authors": [
                "Shuo Yang",
                "Haocheng Xi",
                "Yilong Zhao",
                "Muyang Li",
                "Jintao Zhang",
                "Han Cai",
                "Yujun Lin",
                "Xiuyu Li",
                "Chenfeng Xu",
                "Kelly Peng",
                "Jianfei Chen",
                "Song Han",
                "Kurt Keutzer",
                "Ion Stoica"
            ],
            "affiliations": [
                "MIT",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18875.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#video",
                    "#optimization"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "SVG2 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². SVG2 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ k-means Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° top-p Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2.30x Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Maximizing Video Generation Efficiency with SVG2",
                    "desc": "SVG2 is a novel framework designed to improve the efficiency and quality of video generation without the need for extensive training. It focuses on accurately identifying critical tokens through semantic-aware permutation, which groups tokens based on their meanings rather than just their positions. This method reduces computational waste by ensuring that critical tokens are processed together, optimizing GPU usage. By implementing dynamic budget control, SVG2 achieves significant speed improvements while maintaining high video quality, demonstrating a balance between performance and resource efficiency."
                },
                "zh": {
                    "title": "SVG2ï¼šæå‡è§†é¢‘ç”Ÿæˆæ•ˆç‡ä¸è´¨é‡çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "SVG2æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡å‡†ç¡®è¯†åˆ«å’Œå¤„ç†å…³é”®æ ‡è®°ï¼Œæå‡è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚å®ƒé‡‡ç”¨è¯­ä¹‰æ„ŸçŸ¥çš„æ’åˆ—å’ŒåŠ¨æ€é¢„ç®—æ§åˆ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—é¢„ç®—ä¸‹ç”Ÿæˆè´¨é‡ä¸ä½³çš„é—®é¢˜ã€‚SVG2é€šè¿‡k-meansèšç±»å’Œé‡æ–°æ’åˆ—æ ‡è®°ï¼Œç¡®ä¿äº†ç²¾ç¡®çš„èšç±»è¡¨ç¤ºï¼Œä»è€Œæé«˜äº†è¯†åˆ«å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘äº†è®¡ç®—æµªè´¹ã€‚è¯¥æ¡†æ¶åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾2.30å€çš„åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16459",
            "title": "MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks",
            "url": "https://huggingface.co/papers/2505.16459",
            "abstract": "The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems.",
            "score": 14,
            "issue_id": 3991,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "d18f80036a817d7c",
            "authors": [
                "Guiyao Tie",
                "Xueyang Zhou",
                "Tianhe Gu",
                "Ruihang Zhang",
                "Chaoran Hu",
                "Sizhe Zhang",
                "Mengqu Sun",
                "Yan Zhang",
                "Pan Zhou",
                "Lichao Sun"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Lehigh University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16459.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MMMR: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMMR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). MMMR Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 1083 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞµÑÑ‚ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RTEP). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLM Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ…, Ğ½Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Evaluating Multi-Modal Reasoning: The MMMR Benchmark",
                    "desc": "The MMMR benchmark is designed to evaluate the reasoning abilities of Multi-Modal Large Language Models (MLLMs) by focusing on their thinking quality across various reasoning types. It includes a challenging dataset with 1,083 questions that require complex reasoning, and a modular evaluation pipeline to assess reasoning quality beyond just accuracy. The study finds that while MLLMs with intermediate thinking traces perform better than those without, they still exhibit issues like inconsistency and overthinking. This benchmark aims to bridge the gap between accuracy and reasoning quality, providing a structured approach for future advancements in multi-modal reasoning systems."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†ï¼šMMMR",
                    "desc": "MMMRåŸºå‡†æµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé‡ç‚¹åœ¨äºé€šè¿‡å¤šæ ·çš„æ¨ç†ç±»å‹å’Œæ¨¡å—åŒ–è¯„ä¼°æµç¨‹æ¥è¯„ä¼°æ€ç»´è´¨é‡ã€‚å°½ç®¡MLLMsåœ¨è¯­è¨€ã€è§†è§‰å’Œç»“æ„åŒ–è¾“å…¥çš„ç»Ÿä¸€å¤„ç†ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†å…¶æ¨ç†èƒ½åŠ›ä»ç„¶ä¸å¤Ÿæ¸…æ™°ï¼Œç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åŸºå‡†ã€‚MMMRåŒ…å«ä¸€ä¸ªé«˜éš¾åº¦çš„æ•°æ®é›†å’Œä¸€ä¸ªæ¨ç†è¿½è¸ªè¯„ä¼°ç®¡é“ï¼Œæ—¨åœ¨è¶…è¶Šå‡†ç¡®æ€§è¯„ä¼°ï¼Œå…³æ³¨æ¨ç†çš„ç›¸å…³æ€§ã€ä¸€è‡´æ€§å’Œç»“æ„åŒ–é”™è¯¯æ³¨é‡Šã€‚é€šè¿‡å®è¯ç»“æœï¼ŒMMMRæ­ç¤ºäº†å‡†ç¡®æ€§ä¸æ¨ç†è´¨é‡ä¹‹é—´çš„å·®è·ï¼Œä¸ºæœªæ¥æ¨¡å‹çš„å‘å±•æä¾›äº†å¯æ“ä½œçš„è¯„ä¼°æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21505",
            "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective",
            "url": "https://huggingface.co/papers/2505.21505",
            "abstract": "The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual output transformation, and vocabulary space outputting.  \t\t\t\t\tAI-generated summary \t\t\t\t Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs.",
            "score": 10,
            "issue_id": 3994,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "b77701cf90420b4c",
            "authors": [
                "Shimao Zhang",
                "Zhejian Lai",
                "Xiang Liu",
                "Shuaijie She",
                "Xiao Liu",
                "Yeyun Gong",
                "Shujian Huang",
                "Jiajun Chen"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "National Key Laboratory for Novel Software Technology, Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21505.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#alignment"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°Ñ… LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ğ±Ñ‰ĞµĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM."
                },
                "en": {
                    "title": "Enhancing Multilingual Capabilities in LLMs through Neuron Identification",
                    "desc": "This research introduces a new algorithm for identifying neurons in large language models (LLMs) that are specific to certain languages as well as those that are language-agnostic. It explores how these neurons contribute to the model's ability to understand and generate text in multiple languages, enhancing multilingual alignment. The study categorizes the internal processes of LLMs into four key areas: understanding multiple languages, reasoning in a shared semantic space, transforming outputs across languages, and managing vocabulary. By analyzing the behavior of different types of neurons, the research provides insights into how LLMs can better support low-resource languages through learned multilingual capabilities."
                },
                "zh": {
                    "title": "ç»†ç²’åº¦ç¥ç»å…ƒè¯†åˆ«ï¼Œæå‡å¤šè¯­è¨€èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ›´ç»†ç²’åº¦çš„ç¥ç»å…ƒè¯†åˆ«ç®—æ³•ï¼Œç”¨äºæ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è¯­è¨€ç‰¹å®šç¥ç»å…ƒå’Œè¯­è¨€æ— å…³ç¥ç»å…ƒã€‚æˆ‘ä»¬åˆ†æäº†å¤šè¯­è¨€ç†è§£ã€å…±äº«è¯­ä¹‰æ¨ç†ã€å¤šè¯­è¨€è¾“å‡ºè½¬æ¢å’Œè¯æ±‡ç©ºé—´è¾“å‡ºç­‰æ–¹é¢å¯¹å¤šè¯­è¨€å¯¹é½å’Œèƒ½åŠ›çš„å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå­˜åœ¨åœ¨å¤„ç†ä¸åŒè¯­è¨€æ—¶é€‰æ‹©æ€§æ¿€æ´»çš„è¯­è¨€ç‰¹å®šç¥ç»å…ƒï¼Œè¿™ä¸ºæ·±å…¥ç†è§£LLMsåœ¨å¤šè¯­è¨€åœºæ™¯ä¸­çš„æœºåˆ¶æä¾›äº†æ–°è§†è§’ã€‚é€šè¿‡å¯¹ä¸åŒç±»å‹ç¥ç»å…ƒçš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬ä¸ºæ›´å¥½åœ°ç†è§£LLMsçš„å¤šè¯­è¨€å¯¹é½å’Œèƒ½åŠ›æä¾›äº†å®è¯ç»“æœå’Œæœ‰ä»·å€¼çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21496",
            "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\n  Mobile GUI Agents",
            "url": "https://huggingface.co/papers/2505.21496",
            "abstract": "In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie.",
            "score": 9,
            "issue_id": 3993,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "db6e0d226a2c500e",
            "authors": [
                "Han Xiao",
                "Guozhi Wang",
                "Yuxiang Chai",
                "Zimu Lu",
                "Weifeng Lin",
                "Hao He",
                "Lue Fan",
                "Liuyang Bian",
                "Rui Hu",
                "Liang Liu",
                "Shuai Ren",
                "Yafei Wen",
                "Xiaoxin Chen",
                "Aojun Zhou",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "CUHK MMLab",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21496.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#synthetic",
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "UI-Genie: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "UI-Genie - ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ UI-Genie-RM Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚, Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UI-Genie-RM-517k Ğ¸ UI-Genie-Agent-16k Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UI-Genie Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°."
                },
                "en": {
                    "title": "UI-Genie: Revolutionizing GUI Agents with Self-Improvement and Reward Models",
                    "desc": "This paper presents UI-Genie, a framework designed to improve GUI agents by tackling the challenges of verifying outcomes and scaling high-quality training data. It introduces a reward model, UI-Genie-RM, which uses an image-text interleaved architecture to effectively process historical data and combine different levels of rewards. The framework also includes innovative data generation strategies to create training data without manual effort, such as rule-based verification and hard negative mining. Experimental results indicate that UI-Genie outperforms existing methods in GUI agent tasks, showcasing the effectiveness of its self-improvement approach."
                },
                "zh": {
                    "title": "UI-Genieï¼šè‡ªæˆ‘æ”¹è¿›çš„GUIä»£ç†æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†UI-Genieï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³GUIä»£ç†ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè½¨è¿¹ç»“æœçš„éªŒè¯å›°éš¾å’Œé«˜è´¨é‡è®­ç»ƒæ•°æ®çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¥–åŠ±æ¨¡å‹å’Œè‡ªæˆ‘æ”¹è¿›ç®¡é“æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å¥–åŠ±æ¨¡å‹UI-Genie-RMé‡‡ç”¨å›¾åƒ-æ–‡æœ¬äº¤é”™æ¶æ„ï¼Œæœ‰æ•ˆå¤„ç†å†å²ä¸Šä¸‹æ–‡ï¼Œå¹¶ç»Ÿä¸€äº†åŠ¨ä½œçº§å’Œä»»åŠ¡çº§å¥–åŠ±ã€‚è‡ªæˆ‘æ”¹è¿›ç®¡é“é€šè¿‡å¥–åŠ±å¼•å¯¼æ¢ç´¢å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„ç»“æœéªŒè¯ï¼Œé€æ­¥æ‰©å±•å¯è§£å†³çš„å¤æ‚GUIä»»åŠ¡ï¼Œä»è€Œæå‡ä»£ç†å’Œå¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21457",
            "title": "Active-O3: Empowering Multimodal Large Language Models with Active\n  Perception via GRPO",
            "url": "https://huggingface.co/papers/2505.21457",
            "abstract": "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.",
            "score": 8,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "6d338aff50466f43",
            "authors": [
                "Muzhi Zhu",
                "Hao Zhong",
                "Canyu Zhao",
                "Zongze Du",
                "Zheng Huang",
                "Mingyu Liu",
                "Hao Chen",
                "Cheng Zou",
                "Jingdong Chen",
                "Ming Yang",
                "Chunhua Shen"
            ],
            "affiliations": [
                "Ant Group, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21457.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "ACTIVE-O3: ĞĞ°Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ MLLM Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ACTIVE-O3 - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´Ğ»Ñ MLLM Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ACTIVE-O3 Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ACTIVE-O3 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğµ V*."
                },
                "en": {
                    "title": "Empowering MLLMs with Active Perception for Smarter Decision-Making",
                    "desc": "This paper introduces ACTIVE-O3, a reinforcement learning framework designed to enhance Multimodal Large Language Models (MLLMs) with active perception capabilities. Active perception involves strategically selecting where to focus attention to gather relevant information, which is crucial for effective decision-making in robotics. The authors highlight the limitations of the existing GPT-o3 model in terms of search efficiency and region selection accuracy. By establishing a benchmark suite for evaluating ACTIVE-O3, the paper aims to advance research in active perception for MLLMs, demonstrating strong performance in various tasks without needing explicit reasoning data."
                },
                "zh": {
                    "title": "æå‡æœºå™¨äººä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "ä¸»åŠ¨è§†è§‰ï¼Œä¹Ÿç§°ä¸ºä¸»åŠ¨æ„ŸçŸ¥ï¼Œæ˜¯æŒ‡ä¸»åŠ¨é€‰æ‹©è§‚å¯Ÿçš„æ–¹å¼å’Œä½ç½®ï¼Œä»¥è·å–ä¸ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ã€‚æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æœºå™¨äººç³»ç»Ÿä¸­çš„ä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ¡†æ¶ACTIVE-O3ã€‚æˆ‘ä»¬å®šä¹‰äº†MLLMsçš„ä¸»åŠ¨æ„ŸçŸ¥ä»»åŠ¡ï¼Œå¹¶æŒ‡å‡ºç°æœ‰æ¨¡å‹åœ¨æœç´¢æ•ˆç‡å’ŒåŒºåŸŸé€‰æ‹©ä¸Šå­˜åœ¨ä¸è¶³ã€‚é€šè¿‡å»ºç«‹ç»¼åˆåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒACTIVE-O3åœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç¤ºäº†å¼ºå¤§çš„é›¶-shotæ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨äº†ä¸»åŠ¨æ„ŸçŸ¥çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20275",
            "title": "ImgEdit: A Unified Image Editing Dataset and Benchmark",
            "url": "https://huggingface.co/papers/2505.20275",
            "abstract": "Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce ImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2 million carefully curated edit pairs, which contain both novel and complex single-turn edits, as well as challenging multi-turn tasks. To ensure the data quality, we employ a multi-stage pipeline that integrates a cutting-edge vision-language model, a detection model, a segmentation model, alongside task-specific in-painting procedures and strict post-processing. ImgEdit surpasses existing datasets in both task novelty and data quality. Using ImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to process the reference image and editing prompt, which outperforms existing open-source models on multiple tasks, highlighting the value of ImgEdit and model design. For comprehensive evaluation, we introduce ImgEdit-Bench, a benchmark designed to evaluate image editing performance in terms of instruction adherence, editing quality, and detail preservation. It includes a basic testsuite, a challenging single-turn suite, and a dedicated multi-turn suite. We evaluate both open-source and proprietary models, as well as ImgEdit-E1, providing deep analysis and actionable insights into the current behavior of image-editing models. The source data are publicly available on https://github.com/PKU-YuanGroup/ImgEdit.",
            "score": 8,
            "issue_id": 3990,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "ab1a7940b7d4c31c",
            "authors": [
                "Yang Ye",
                "Xianyi He",
                "Zongjian Li",
                "Bin Lin",
                "Shenghai Yuan",
                "Zhiyuan Yan",
                "Bohan Hou",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Peng Cheng Laboratory",
                "Rabbitpre AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20275.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#optimization",
                    "#cv",
                    "#data"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ImgEdit: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ImgEdit - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1,2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ImgEdit-E1, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ImgEdit-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ImgEdit-E1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Open-Source Image Editing with ImgEdit Dataset",
                    "desc": "This paper presents ImgEdit, a new dataset designed to improve open-source image-editing models by providing 1.2 million high-quality image edit pairs. The dataset includes both simple and complex editing tasks, ensuring a wide range of challenges for model training. To maintain high data quality, a multi-stage pipeline is used, incorporating advanced models for vision-language processing, detection, and segmentation. The authors also introduce ImgEdit-E1, an editing model that outperforms existing open-source models, and ImgEdit-Bench, a benchmark for evaluating image editing performance across various tasks."
                },
                "zh": {
                    "title": "ImgEditï¼šé«˜è´¨é‡å›¾åƒç¼–è¾‘çš„çªç ´",
                    "desc": "æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•ä½¿å¾—é«˜ä¿çœŸæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå¼€æºå›¾åƒç¼–è¾‘æ¨¡å‹ä»ç„¶è½åäºä¸“æœ‰æ¨¡å‹ï¼Œä¸»è¦æ˜¯ç”±äºé«˜è´¨é‡æ•°æ®çš„ç¼ºä¹å’ŒåŸºå‡†æµ‹è¯•ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ImgEditï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„é«˜è´¨é‡å›¾åƒç¼–è¾‘æ•°æ®é›†ï¼ŒåŒ…å«120ä¸‡ä¸ªç²¾å¿ƒç­–åˆ’çš„ç¼–è¾‘å¯¹ï¼Œæ¶µç›–æ–°é¢–å’Œå¤æ‚çš„å•è½®ç¼–è¾‘ä»¥åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè½®ä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨å¤šé˜¶æ®µæµç¨‹ç¡®ä¿æ•°æ®è´¨é‡ï¼Œæ•´åˆäº†å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€æ£€æµ‹æ¨¡å‹ã€åˆ†å‰²æ¨¡å‹ä»¥åŠç‰¹å®šä»»åŠ¡çš„ä¿®å¤ç¨‹åºå’Œä¸¥æ ¼çš„åå¤„ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21500",
            "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2505.21500",
            "abstract": "Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.",
            "score": 6,
            "issue_id": 3993,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "c8cec2407a7def0e",
            "authors": [
                "Dingming Li",
                "Hongxing Li",
                "Zixuan Wang",
                "Yuchen Yan",
                "Hang Zhang",
                "Siqi Chen",
                "Guiyang Hou",
                "Shengpei Jiang",
                "Wenqi Zhang",
                "Yongliang Shen",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "University of Electronic Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21500.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#reasoning",
                    "#3d",
                    "#games"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ViewSpatial-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ°Ğ»Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 3D-ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ¢Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 46.24%."
                },
                "en": {
                    "title": "Enhancing VLMs with Multi-Viewpoint Spatial Reasoning",
                    "desc": "This paper addresses the limitations of vision-language models (VLMs) in understanding spatial relationships from different viewpoints. It highlights that while VLMs perform well in egocentric spatial reasoning, they struggle with allocentric perspectives, which are essential for tasks requiring understanding from another entity's viewpoint. The authors introduce ViewSpatial-Bench, a new benchmark for evaluating multi-viewpoint spatial localization, along with a 3D annotation pipeline for accurate directional labeling. By fine-tuning VLMs on this dataset, they demonstrate a significant performance improvement, emphasizing the importance of 3D spatial modeling in enhancing VLMs' spatial reasoning capabilities."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£å’Œæ¨ç†è§†è§‰å†…å®¹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦è·¨è§†è§’ç†è§£å’Œç©ºé—´æ¨ç†çš„ä»»åŠ¡ä¸­ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„VLMsä¸»è¦åœ¨è‡ªæˆ‘ä¸­å¿ƒçš„ç©ºé—´æ¨ç†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦é‡‡ç”¨å…¶ä»–å®ä½“çš„ç©ºé—´å‚è€ƒæ¡†æ¶æ—¶ï¼Œæ— æ³•å¾ˆå¥½åœ°æ¨å¹¿åˆ°ä»–å¿ƒä¸­å¿ƒè§†è§’ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ViewSpatial-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºå¤šè§†è§’ç©ºé—´å®šä½è¯†åˆ«è¯„ä¼°è®¾è®¡çš„ç»¼åˆåŸºå‡†ï¼Œæ”¯æŒè‡ªåŠ¨åŒ–çš„3Dæ ‡æ³¨æµç¨‹ç”Ÿæˆç²¾ç¡®çš„æ–¹å‘æ ‡ç­¾ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„å¤šè§†è§’ç©ºé—´æ•°æ®é›†ä¸Šå¾®è°ƒVLMsï¼Œæˆ‘ä»¬åœ¨å„é¡¹ä»»åŠ¡ä¸Šå®ç°äº†46.24%çš„æ•´ä½“æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21491",
            "title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2505.21491",
            "abstract": "Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce a new dataset curated semi-automatically, a comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines.",
            "score": 6,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "cf3b062e968f421f",
            "authors": [
                "Boyang Wang",
                "Xuweiyi Chen",
                "Matheus Gadelha",
                "Zezhou Cheng"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21491.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#games",
                    "#diffusion",
                    "#architecture",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ°Ğ´Ñ€Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Frame In Ğ¸ Frame Out, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ²Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ² ĞºĞ°Ğ´Ñ€ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ· Ğ½ĞµĞ³Ğ¾. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Diffusion Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Mastering Video Generation with Motion Control",
                    "desc": "This paper addresses key challenges in video generation, specifically focusing on controllability, temporal coherence, and detail synthesis. It introduces a novel technique called Frame In and Frame Out, allowing users to manipulate objects in a video scene based on specified motion trajectories. The authors present a new dataset and evaluation protocol tailored for this task, along with a Diffusion Transformer architecture that preserves identity while enabling motion control. Results demonstrate that their method significantly improves upon existing video generation models."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆçš„å¯æ§æ€§ä¸ä¸€è‡´æ€§",
                    "desc": "æœ¬è®ºæ–‡å…³æ³¨è§†é¢‘ç”Ÿæˆä¸­çš„å¯æ§æ€§ã€æ—¶é—´ä¸€è‡´æ€§å’Œç»†èŠ‚åˆæˆç­‰å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œå¸§è¿›å¸§å‡ºâ€çš„ç”µå½±æŠ€æœ¯ï¼Œå…è®¸ç”¨æˆ·æ§åˆ¶å›¾åƒä¸­çš„å¯¹è±¡è‡ªç„¶åœ°ç¦»å¼€æˆ–è¿›å…¥åœºæ™¯ã€‚ä¸ºæ”¯æŒè¿™ä¸€ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŠè‡ªåŠ¨ç­–åˆ’çš„æ–°æ•°æ®é›†ï¼Œå¹¶åˆ¶å®šäº†é’ˆå¯¹è¯¥è®¾ç½®çš„ç»¼åˆè¯„ä¼°åè®®ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19099",
            "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.19099",
            "abstract": "SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75\\%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\\% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.",
            "score": 6,
            "issue_id": 3991,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ",
                "en": "May 25",
                "zh": "5æœˆ25æ—¥"
            },
            "hash": "e7a5589d299cea0e",
            "authors": [
                "Kun Xiang",
                "Heng Li",
                "Terry Jingchen Zhang",
                "Yinya Huang",
                "Zirong Liu",
                "Peixin Qu",
                "Jixi He",
                "Jiaqi Chen",
                "Yu-Jie Yuan",
                "Jianhua Han",
                "Hang Xu",
                "Hanhui Li",
                "Mrinmaya Sachan",
                "Xiaodan Liang"
            ],
            "affiliations": [
                "ETH Zurich",
                "Huawei Noahs Ark Lab",
                "Sun Yat-sen University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19099.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#benchmark",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "SeePhys: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ LLM Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ",
                    "desc": "SeePhys - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 7 Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¾Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 21 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 75% Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 60% Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ."
                },
                "en": {
                    "title": "SeePhys: Bridging Visual Reasoning and Physics in LLMs",
                    "desc": "SeePhys is a new benchmark designed to test how well large language models (LLMs) can solve physics problems that involve visual reasoning. It includes a wide range of questions, from middle school to PhD level, and features many different types of diagrams that are crucial for finding the right answers. Unlike previous benchmarks, SeePhys requires models to extract visual information for 75% of the problems, making it essential for them to interpret diagrams accurately. The results show that even the best models struggle with these tasks, highlighting significant gaps in their ability to connect visual data with physics reasoning without relying heavily on text."
                },
                "zh": {
                    "title": "SeePhysï¼šæŒ‘æˆ˜è§†è§‰æ¨ç†ä¸ç‰©ç†é—®é¢˜è§£å†³çš„åŸºå‡†",
                    "desc": "SeePhysæ˜¯ä¸€ä¸ªå¤§å‹çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰©ç†é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œæ¶µç›–ä»ä¸­å­¦åˆ°åšå£«èµ„æ ¼è€ƒè¯•çš„èŒƒå›´ã€‚è¯¥åŸºå‡†æ¶‰åŠç‰©ç†å­¦çš„7ä¸ªåŸºæœ¬é¢†åŸŸï¼Œå¹¶åŒ…å«21ç±»é«˜åº¦å¼‚è´¨çš„å›¾è¡¨ã€‚ä¸ä»¥å¾€çš„ç ”ç©¶ä¸åŒï¼ŒSeePhysä¸­75%çš„é—®é¢˜éœ€è¦æå–è§†è§‰ä¿¡æ¯æ‰èƒ½å¾—å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œæ˜¾ç¤ºå‡ºè§†è§‰ä¿¡æ¯åœ¨è§£å†³é—®é¢˜ä¸­çš„é‡è¦æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„è§†è§‰æ¨ç†æ¨¡å‹ï¼Œå…¶å‡†ç¡®ç‡ä¹Ÿæœªèƒ½è¶…è¿‡60%ï¼Œæ­ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20322",
            "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering\n  Target Atoms",
            "url": "https://huggingface.co/papers/2505.20322",
            "abstract": "Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.",
            "score": 6,
            "issue_id": 3990,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "63cd6df71ddaefa4",
            "authors": [
                "Mengru Wang",
                "Ziwen Xu",
                "Shengyu Mao",
                "Shumin Deng",
                "Zhaopeng Tu",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "NUS-NCS Joint Lab, Singapore",
                "National University of Singapore",
                "Tencent AI Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20322.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#security",
                    "#training",
                    "#alignment"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Steering Target Atoms (STA) Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ STA Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑĞ¾Ğ±ÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Control in Language Models with Steering Target Atoms",
                    "desc": "This paper introduces a new method called Steering Target Atoms (STA) to improve the control over language model generation. It addresses the challenge of intertwined internal representations in large models, which can hinder precise steering and lead to unintended consequences. By isolating and manipulating specific knowledge components, STA enhances the safety and reliability of model outputs. The experiments show that this approach is effective, especially in adversarial situations, and it also improves reasoning control in large models."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå¼•å¯¼ç›®æ ‡åŸå­ï¼ˆSTAï¼‰ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚é€šè¿‡ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ†ç¦»å’Œæ“æ§é«˜ç»´ç©ºé—´ä¸­çš„çŸ¥è¯†ç»„ä»¶ï¼Œä»è€Œå¢å¼ºå¯¹æ¨¡å‹è¡Œä¸ºçš„æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTAåœ¨å¯¹æŠ—æ€§åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†è¿™ä¸€å¼•å¯¼ç­–ç•¥åº”ç”¨äºå¤§å‹æ¨ç†æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶åœ¨ç²¾ç¡®æ¨ç†æ§åˆ¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21473",
            "title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via\n  Next-Detail Prediction",
            "url": "https://huggingface.co/papers/2505.21473",
            "abstract": "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing a more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose a parallel inference mechanism with self-correction that accelerates generation speed by approximately 8x while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2x faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlow's superior generation quality and efficiency compared to existing state-of-the-art methods.",
            "score": 4,
            "issue_id": 3992,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "d4df44fe3325795e",
            "authors": [
                "Yiheng Liu",
                "Liao Qu",
                "Huichao Zhang",
                "Xu Wang",
                "Yi Jiang",
                "Yiming Gao",
                "Hu Ye",
                "Xian Li",
                "Shuai Wang",
                "Daniel K. Du",
                "Shu Cheng",
                "Zehuan Yuan",
                "Xinglong Wu"
            ],
            "affiliations": [
                "ByteDance Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21473.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#training",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ",
                    "desc": "DetailFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‰Ğ¸Ğ¼ÑÑ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 8 Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "DetailFlow: Efficient 1D Image Generation with Coarse-to-Fine Refinement",
                    "desc": "This paper introduces DetailFlow, a new method for generating images using a 1D autoregressive approach. It employs a next-detail prediction strategy that allows the model to start with a broad image structure and gradually add finer details. By using a resolution-aware token sequence and a parallel inference mechanism, DetailFlow significantly improves generation speed and reduces the number of tokens needed for high-quality image synthesis. The results show that DetailFlow outperforms existing models in both image quality and efficiency, achieving better performance with fewer resources."
                },
                "zh": {
                    "title": "DetailFlowï¼šé«˜æ•ˆçš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDetailFlowçš„ç²—åˆ°ç»†çš„1Dè‡ªå›å½’å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ä¸‹ä¸€ä¸ªç»†èŠ‚é¢„æµ‹ç­–ç•¥æ¥å»ºæ¨¡å›¾åƒã€‚é€šè¿‡å­¦ä¹ ä¸€ä¸ªåˆ†è¾¨ç‡æ„ŸçŸ¥çš„æ ‡è®°åºåˆ—ï¼Œå¹¶ä½¿ç”¨é€æ­¥é™çº§çš„å›¾åƒè¿›è¡Œç›‘ç£ï¼ŒDetailFlowä½¿ç”Ÿæˆè¿‡ç¨‹èƒ½å¤Ÿä»å…¨å±€ç»“æ„å¼€å§‹ï¼Œé€æ­¥ç»†åŒ–ç»†èŠ‚ã€‚è¯¥ç²—åˆ°ç»†çš„1Dæ ‡è®°åºåˆ—ä¸è‡ªå›å½’æ¨ç†æœºåˆ¶å¾ˆå¥½åœ°å¯¹é½ï¼Œä¸ºè‡ªå›å½’æ¨¡å‹ç”Ÿæˆå¤æ‚è§†è§‰å†…å®¹æä¾›äº†ä¸€ç§æ›´è‡ªç„¶å’Œé«˜æ•ˆçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDetailFlowåœ¨å›¾åƒåˆæˆè´¨é‡å’Œæ•ˆç‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21334",
            "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
            "url": "https://huggingface.co/papers/2505.21334",
            "abstract": "HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference.",
            "score": 2,
            "issue_id": 3994,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "b07c0c110f38f89c",
            "authors": [
                "Kele Shao",
                "Keda Tao",
                "Can Qin",
                "Haoxuan You",
                "Yang Sui",
                "Huan Wang"
            ],
            "affiliations": [
                "Columbia University",
                "Rice University",
                "Salesforce AI Research",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21334.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "HoliTom: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-LLM",
                    "desc": "HoliTom - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ¾Ğ±Ñ€ĞµĞ·ĞºÑƒ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ÑƒĞ´Ğ°ĞµÑ‚ÑÑ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ´Ğ¾ 6,9% Ğ¾Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ² 99,1% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "HoliTom: Efficient Video LLMs through Smart Token Pruning",
                    "desc": "HoliTom is a novel framework designed to enhance the efficiency of video large language models (LLMs) by reducing computational redundancy in video tokens. It combines outer-LLM pruning, which segments video data globally to identify and eliminate redundant tokens, with inner-LLM token merging based on similarity to further optimize performance. This dual approach allows for a significant reduction in visual tokens by over 90%, while still maintaining high performance levels, achieving 99.1% of the original output. The method also improves processing speed, reducing computational costs to just 6.9% of FLOPs and accelerating decoding throughput by 1.32 times, making it a practical solution for efficient video LLM inference."
                },
                "zh": {
                    "title": "HoliTomï¼šé«˜æ•ˆè§†é¢‘LLMçš„å…¨æ–°å‰ªæç­–ç•¥",
                    "desc": "HoliTomæ˜¯ä¸€ç§æ–°é¢–çš„è®­ç»ƒæ— å…³çš„æ•´ä½“ä»¤ç‰Œåˆå¹¶æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å…¨çƒæ—¶é—´åˆ†å‰²è¿›è¡Œå¤–éƒ¨LLMå‰ªæï¼Œæ˜¾è‘—å‡å°‘è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆè§†é¢‘LLMï¼‰çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤–éƒ¨LLMå‰ªæå’Œå†…éƒ¨LLMåŸºäºä»¤ç‰Œç›¸ä¼¼æ€§çš„åˆå¹¶ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå‡å°‘è§†è§‰ä»¤ç‰Œè¶…è¿‡90%ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒHoliTomæœ‰æ•ˆç¼“è§£äº†LLMçš„è®¡ç®—è´Ÿæ‹…ï¼ŒåŒæ—¶ä¿æŒäº†99.1%çš„åŸå§‹æ€§èƒ½ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬å’Œè§£ç é€Ÿåº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨é«˜æ•ˆè§†é¢‘LLMæ¨ç†ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21070",
            "title": "Minute-Long Videos with Dual Parallelisms",
            "url": "https://huggingface.co/papers/2505.21070",
            "abstract": "Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54times lower latency and 1.48times lower memory cost on 8timesRTX 4090 GPUs.",
            "score": 2,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "c777be11d4844462",
            "authors": [
                "Zeqing Wang",
                "Bowen Zheng",
                "Xingyi Yang",
                "Yuecong Xu",
                "Xinchao Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "National University of Singapore",
                "Xidian University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21070.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#video",
                    "#inference"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer (DiT), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ÑƒÑ DualParal. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ GPU Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 1025 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ´Ğ¾ 6,54 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¸ 1,48 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 8 GPU RTX 4090."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with DualPar Efficiency!",
                    "desc": "The paper presents a new method called DualPar for improving the efficiency of video generation using Diffusion Transformer (DiT) models. It addresses the high latency and memory costs associated with generating long videos by distributing the workload across multiple GPUs. The method uses a block-wise denoising approach to maintain synchronized noise levels while allowing for parallel processing of frames and model layers. Additionally, it introduces a feature cache and coordinated noise initialization to optimize performance, resulting in faster and more efficient video generation without artifacts."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆæ— é™é•¿è§†é¢‘çš„åˆ›æ–°ç­–ç•¥",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘ç”Ÿæˆæ—¶çš„å¤„ç†å»¶è¿Ÿå’Œå†…å­˜æˆæœ¬é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒå¼æ¨ç†ç­–ç•¥ï¼Œç§°ä¸ºDualParalï¼Œé€šè¿‡åœ¨å¤šä¸ªGPUä¸Šå¹¶è¡Œå¤„ç†æ—¶é—´å¸§å’Œæ¨¡å‹å±‚æ¥æé«˜æ•ˆç‡ã€‚ä¸ºäº†å…‹æœæ‰©æ•£æ¨¡å‹å¯¹å™ªå£°æ°´å¹³åŒæ­¥çš„è¦æ±‚ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å—çº§å»å™ªæ–¹æ¡ˆï¼Œä½¿å¾—æ¯ä¸ªGPUå¤„ç†ç‰¹å®šçš„å¸§å—å’Œå±‚ï¼ŒåŒæ—¶å®ç°å¼‚æ­¥è®¡ç®—å’Œé€šä¿¡ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ—¶æ˜¾è‘—é™ä½äº†å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆæ— é™é•¿çš„è§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19314",
            "title": "SoloSpeech: Enhancing Intelligibility and Quality in Target Speech\n  Extraction through a Cascaded Generative Pipeline",
            "url": "https://huggingface.co/papers/2505.19314",
            "abstract": "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.",
            "score": 2,
            "issue_id": 3990,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ",
                "en": "May 25",
                "zh": "5æœˆ25æ—¥"
            },
            "hash": "d58bb66dfe2fc291",
            "authors": [
                "Helin Wang",
                "Jiarui Hai",
                "Dongchao Yang",
                "Chen Chen",
                "Kai Li",
                "Junyi Peng",
                "Thomas Thebaud",
                "Laureano Moro Velazquez",
                "Jesus Villalba",
                "Najim Dehak"
            ],
            "affiliations": [
                "Brno University of Technology",
                "Johns Hopkins University",
                "Nanyang Technological University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19314.jpg",
            "data": {
                "categories": [
                    "#audio"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "SoloSpeech: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "SoloSpeech - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· ÑĞ¼ĞµÑĞ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ - ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾. SoloSpeech Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Libri2Mix, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "SoloSpeech: Revolutionizing Target Speech Extraction with Generative Techniques",
                    "desc": "This paper introduces SoloSpeech, a new approach for Target Speech Extraction (TSE) that effectively isolates a target speaker's voice from a mix of multiple speakers. Unlike traditional discriminative models that can produce artifacts and lack naturalness, SoloSpeech employs a cascaded generative pipeline that includes processes for compression, extraction, reconstruction, and correction. It utilizes a speaker-embedding-free target extractor that aligns the latent spaces of cue audio and mixture audio, enhancing performance and reducing discrepancies. Evaluated on the Libri2Mix dataset, SoloSpeech sets a new benchmark for intelligibility and quality in TSE, showing strong generalization capabilities in diverse real-world scenarios."
                },
                "zh": {
                    "title": "SoloSpeechï¼šæå‡ç›®æ ‡è¯­éŸ³æå–çš„æ–°æ–¹æ³•",
                    "desc": "ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ—¨åœ¨ä»å¤šä¸ªè¯´è¯è€…çš„æ··åˆéŸ³é¢‘ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯è€…çš„å£°éŸ³ï¼Œé€šå¸¸åˆ©ç”¨ä½œä¸ºè¾…åŠ©éŸ³é¢‘çš„è¯´è¯è€…ç‰¹å¾çº¿ç´¢ã€‚å°½ç®¡è¿‘æœŸçš„TSEè¿›å±•ä¸»è¦é‡‡ç”¨äº†é«˜æ„ŸçŸ¥è´¨é‡çš„åˆ¤åˆ«æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹å¸¸å¸¸å¼•å…¥ä¸å¿…è¦çš„ä¼ªå½±ï¼Œé™ä½è‡ªç„¶æ€§ï¼Œå¹¶å¯¹è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒä¹‹é—´çš„å·®å¼‚æ•æ„Ÿã€‚å¦ä¸€æ–¹é¢ï¼Œç”Ÿæˆæ¨¡å‹åœ¨æ„ŸçŸ¥è´¨é‡å’Œå¯æ‡‚æ€§æ–¹é¢æ»åã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SoloSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„çº§è”ç”Ÿæˆç®¡é“ï¼Œé›†æˆäº†å‹ç¼©ã€æå–ã€é‡å»ºå’Œä¿®æ­£è¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨ç›®æ ‡è¯­éŸ³æå–å’Œè¯­éŸ³åˆ†ç¦»ä»»åŠ¡ä¸­å®ç°æ–°çš„æœ€å…ˆè¿›çš„å¯æ‡‚æ€§å’Œè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21205",
            "title": "Sci-Fi: Symmetric Constraint for Frame Inbetweening",
            "url": "https://huggingface.co/papers/2505.21205",
            "abstract": "Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably can't make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines.",
            "score": 1,
            "issue_id": 3990,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "a20a9eb43b42208f",
            "authors": [
                "Liuhan Chen",
                "Xiaodong Cun",
                "Xiaoyu Li",
                "Xianyi He",
                "Shenghai Yuan",
                "Jie Chen",
                "Ying Shan",
                "Li Yuan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "GVC Lab, Great Bay University",
                "Rabbitpre Intelligence",
                "Shenzhen Graduate School, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21205.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Sci-Fi, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (I2V-DM). Sci-Fi Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ EF-Net Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¼ ĞºĞ°Ğ´Ñ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Achieving Harmony in Video Frame Synthesis with Sci-Fi",
                    "desc": "This paper presents a new approach called Sci-Fi for generating intermediate video sequences from given start and end frames. The authors identify a limitation in existing methods that treat the start and end frame constraints equally, which can lead to poor video quality. Sci-Fi introduces a novel mechanism using a lightweight module, EF-Net, to enhance the influence of the end frame, ensuring it has a similar impact as the start frame. The results show that Sci-Fi produces smoother and more consistent transitions in generated videos compared to current state-of-the-art techniques."
                },
                "zh": {
                    "title": "å®ç°èµ·å§‹å¸§ä¸ç»“æŸå¸§çš„å¯¹ç§°çº¦æŸ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºSci-Fiï¼Œç”¨äºåœ¨ç»™å®šçš„èµ·å§‹å¸§å’Œç»“æŸå¸§ä¹‹é—´åˆæˆä¸­é—´è§†é¢‘åºåˆ—ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºå¤§å‹é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆI2V-DMsï¼‰ï¼Œä½†åœ¨å¼•å…¥ç»“æŸå¸§çº¦æŸæ—¶å­˜åœ¨è®¾è®¡ç¼ºé™·ã€‚æˆ‘ä»¬å‘ç°ï¼Œç®€å•åœ°ä½¿ç”¨ä¸èµ·å§‹å¸§ç›¸åŒçš„æœºåˆ¶æ¥å¼•å…¥ç»“æŸå¸§çº¦æŸï¼Œå¯èƒ½æ— æ³•æœ‰æ•ˆå½±å“ä¸­é—´å†…å®¹ï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆå¸§çš„è¿åŠ¨ä¸ä¸€è‡´æˆ–å¤–è§‚å´©æºƒã€‚Sci-Fié€šè¿‡å¼•å…¥ä¸€ç§æ”¹è¿›çš„æœºåˆ¶å’Œè½»é‡çº§æ¨¡å—EF-Netï¼Œä½¿ç»“æŸå¸§çº¦æŸçš„å½±å“åŠ›ä¸èµ·å§‹å¸§ç›¸å½“ï¼Œä»è€Œå®ç°æ›´å’Œè°çš„è¿‡æ¸¡æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20561",
            "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.20561",
            "abstract": "BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL.",
            "score": 1,
            "issue_id": 3994,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "4b7b0470932e5c49",
            "authors": [
                "Shenao Zhang",
                "Yaqing Wang",
                "Yinxiao Liu",
                "Tianqi Liu",
                "Peter Grabowski",
                "Eugene Ie",
                "Zhaoran Wang",
                "Yunxuan Li"
            ],
            "affiliations": [
                "Google",
                "Google DeepMind",
                "Northwestern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20561.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "BARL: Ğ£Ğ¼Ğ½ĞµĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµĞ¼, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµĞ¼",
                    "desc": "BARL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ‘Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². BARL Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BARL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLMs with Reflective Exploration through BARL",
                    "desc": "The paper introduces BARL, a Bayes-Adaptive Reinforcement Learning framework designed to improve the performance of Large Language Models (LLMs) by incorporating reflective reasoning and efficient exploration strategies. Traditional Markovian RL methods limit exploration to the training phase and rely solely on current state information, which may hinder the emergence of reflective reasoning during training. BARL addresses this limitation by optimizing expected returns using a Bayesian approach, allowing the model to adaptively explore and exploit based on updated beliefs about the environment. Empirical results show that BARL significantly enhances token efficiency and effectiveness in reasoning tasks compared to standard Markovian RL methods."
                },
                "zh": {
                    "title": "BARLï¼šæå‡LLMæ€§èƒ½çš„è´å¶æ–¯è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "BARLæ˜¯ä¸€ç§è´å¶æ–¯è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆåæ€æ¨ç†å’Œé«˜æ•ˆæ¢ç´¢ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„é©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ é™åˆ¶äº†æ¢ç´¢è¿‡ç¨‹ï¼Œä»…ä¾èµ–å½“å‰çŠ¶æ€çš„å†å²ä¸Šä¸‹æ–‡æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œå¯¼è‡´åæ€æ¨ç†çš„å‡ºç°å’Œå…¶åœ¨æµ‹è¯•æ—¶çš„å¥½å¤„ä¸æ˜ç¡®ã€‚BARLæ¡†æ¶é€šè¿‡ä¼˜åŒ–é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„åéªŒåˆ†å¸ƒï¼Œé¼“åŠ±æ¨¡å‹åœ¨å¥–åŠ±æœ€å¤§åŒ–å’Œä¿¡æ¯æ”¶é›†ä¹‹é—´è¿›è¡Œå¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBARLåœ¨åˆæˆå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„é©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´é«˜çš„ä»¤ç‰Œæ•ˆç‡å’Œæ›´æœ‰æ•ˆçš„æ¢ç´¢èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20289",
            "title": "VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual\n  Tool Selection",
            "url": "https://huggingface.co/papers/2505.20289",
            "abstract": "VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dynamically explore, select, and combine tools from a diverse library based on empirical performance. Existing methods for tool-augmented reasoning either rely on training-free prompting or large-scale fine-tuning; both lack active tool exploration and typically assume limited tool diversity, and fine-tuning methods additionally demand extensive human supervision. In contrast, VisTA leverages end-to-end reinforcement learning to iteratively refine sophisticated, query-specific tool selection strategies, using task outcomes as feedback signals. Through Group Relative Policy Optimization (GRPO), our framework enables an agent to autonomously discover effective tool-selection pathways without requiring explicit reasoning supervision. Experiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. These results highlight VisTA's ability to enhance generalization, adaptively utilize diverse tools, and pave the way for flexible, experience-driven visual reasoning systems.",
            "score": 1,
            "issue_id": 3991,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "06184525a85012f4",
            "authors": [
                "Zeyi Huang",
                "Yuyang Ji",
                "Anirudh Sundara Rajan",
                "Zefan Cai",
                "Wen Xiao",
                "Junjie Hu",
                "Yong Jae Lee"
            ],
            "affiliations": [
                "Microsoft",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20289.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#benchmark",
                    "#optimization",
                    "#cv",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "VisTA: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "VisTA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², VisTA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VisTA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Empowering Visual Agents with Autonomous Tool Selection",
                    "desc": "VisTA is a reinforcement learning framework designed to improve visual reasoning by allowing agents to autonomously select and combine tools from a diverse library. Unlike traditional methods that either require extensive human supervision or lack active exploration, VisTA uses end-to-end reinforcement learning to refine tool selection strategies based on task outcomes. The framework employs Group Relative Policy Optimization (GRPO) to enable agents to discover effective pathways for tool selection without explicit reasoning guidance. Experiments show that VisTA significantly outperforms existing methods, particularly in challenging scenarios, demonstrating its potential for enhancing generalization and adaptability in visual reasoning tasks."
                },
                "zh": {
                    "title": "VisTAï¼šè‡ªä¸»é€‰æ‹©å·¥å…·çš„è§†è§‰æ¨ç†æ–°æ¡†æ¶",
                    "desc": "VisTAæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªä¸»é€‰æ‹©å’Œç»„åˆå¤šæ ·åŒ–å·¥å…·æ¥å¢å¼ºè§†è§‰æ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒVisTAä¸ä¾èµ–äºè®­ç»ƒå‰æç¤ºæˆ–å¤§è§„æ¨¡å¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–å·¥å…·é€‰æ‹©ç­–ç•¥ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä»»åŠ¡ç»“æœä½œä¸ºåé¦ˆä¿¡å·ï¼Œå…è®¸æ™ºèƒ½ä½“è‡ªä¸»å‘ç°æœ‰æ•ˆçš„å·¥å…·é€‰æ‹©è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisTAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åˆ†å¸ƒå¤–ç¤ºä¾‹æ—¶ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¢å¼ºæ³›åŒ–èƒ½åŠ›å’Œçµæ´»åˆ©ç”¨å¤šæ ·åŒ–å·¥å…·æ–¹é¢çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19973",
            "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in\n  Digital Forensics and Incident Response",
            "url": "https://huggingface.co/papers/2505.19973",
            "abstract": "DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.",
            "score": 1,
            "issue_id": 3994,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "c4a0e825312e2a47",
            "authors": [
                "Bilel Cherif",
                "Tamas Bisztray",
                "Richard A. Dubniczky",
                "Aaesha Aldahmani",
                "Saeed Alshehhi",
                "Norbert Tihanyi"
            ],
            "affiliations": [
                "EÃ¶tvÃ¶s LorÃ¡nd University, Budapest, Hungary",
                "Technology Innovation Institute, Abu Dhabi, UAE",
                "University of Oslo, Oslo, Norway"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19973.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#hallucinations",
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸ĞºĞ¸",
                    "desc": "DFIR-Metric - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸ĞºĞ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµÑÑ‚Ñ‹ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Benchmark Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ 14 LLM Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ - Task Understanding Score (TUS), Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Evaluating AI in Digital Forensics: The DFIR-Metric Benchmark",
                    "desc": "The paper introduces DFIR-Metric, a benchmark designed to evaluate Large Language Models (LLMs) in the field of Digital Forensics and Incident Response (DFIR). It consists of three main components: a Knowledge Assessment with expert-reviewed questions, Realistic Forensic Challenges that test reasoning and evidence correlation, and Practical Analysis using real forensic cases. The study also presents a new metric called the Task Understanding Score (TUS) to assess model performance in low-accuracy situations. By evaluating 14 LLMs, this benchmark aims to provide a reliable framework for improving AI applications in digital forensics."
                },
                "zh": {
                    "title": "DFIR-Metricï¼šæ•°å­—å–è¯ä¸­çš„è¯­è¨€æ¨¡å‹è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "DFIR-Metric æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­—å–è¯é¢†åŸŸè¡¨ç°çš„åŸºå‡†å·¥å…·ã€‚å®ƒåŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šçŸ¥è¯†è¯„ä¼°ã€ç°å®å–è¯æŒ‘æˆ˜å’Œå®é™…åˆ†ææ¡ˆä¾‹ï¼Œæ—¨åœ¨å…¨é¢æµ‹è¯•æ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ 14 ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œç ”ç©¶è€…åˆ†æäº†å®ƒä»¬åœ¨å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚æ–°å¼•å…¥çš„ä»»åŠ¡ç†è§£åˆ†æ•°ï¼ˆTUSï¼‰å¯ä»¥æ›´æœ‰æ•ˆåœ°è¯„ä¼°æ¨¡å‹åœ¨æ¥è¿‘é›¶å‡†ç¡®ç‡çš„åœºæ™¯ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17813",
            "title": "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.17813",
            "abstract": "Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive \"thinking\" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer \"thinking\" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.",
            "score": 1,
            "issue_id": 3992,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "d8d9d938b84c5ea6",
            "authors": [
                "Michael Hassid",
                "Gabriel Synnaeve",
                "Yossi Adi",
                "Roy Schwartz"
            ],
            "affiliations": [
                "FAIR Team, Meta",
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17813.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞšĞ¾Ñ€Ğ¾Ñ‡Ğµ Ğ¼Ñ‹ÑĞ»ÑŒ - Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ²Ñ‹Ğ²Ğ¾Ğ´: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ short-m@k, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ€ĞµĞ´Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… m Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Shorter Chains, Smarter Reasoning!",
                    "desc": "This paper investigates the effectiveness of reasoning in large language models (LLMs) by comparing long and short reasoning chains. The authors find that shorter reasoning chains can yield significantly more accurate answers, with improvements of up to 34.5% compared to longer chains. They introduce a new method called short-m@k, which allows for parallel processing of multiple reasoning chains and selects the final answer based on majority voting. Their results indicate that shorter reasoning processes not only enhance performance but also reduce computational costs and inference time, challenging the traditional belief that longer reasoning leads to better outcomes."
                },
                "zh": {
                    "title": "çŸ­æ€ç»´é“¾ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­ä¾èµ–é•¿æ€ç»´é“¾çš„å‡è®¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¾ƒçŸ­çš„æ¨ç†é“¾åœ¨å›ç­”é—®é¢˜æ—¶æ›´å¯èƒ½äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆï¼Œå‡†ç¡®ç‡æ¯”æœ€é•¿é“¾é«˜å‡º34.5%ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ–¹æ³•short-m@kï¼Œé€šè¿‡å¹¶è¡Œç”Ÿæˆkä¸ªç‹¬ç«‹çš„æ€ç»´è¿‡ç¨‹ï¼Œå¹¶åœ¨ç¬¬ä¸€ä¸ªmä¸ªå®Œæˆååœæ­¢è®¡ç®—ï¼Œæœ€ç»ˆç­”æ¡ˆé€šè¿‡å¤šæ•°æŠ•ç¥¨é€‰å‡ºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒçŸ­æ€ç»´é“¾çš„è®­ç»ƒå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼ŒæŒ‘æˆ˜äº†é•¿æ€ç»´é“¾å¿…ç„¶å¸¦æ¥æ›´å¥½æ¨ç†èƒ½åŠ›çš„ä¼ ç»Ÿè§‚å¿µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16901",
            "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks",
            "url": "https://huggingface.co/papers/2505.16901",
            "abstract": "Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.",
            "score": 1,
            "issue_id": 3994,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "a91f0c74f1c3c191",
            "authors": [
                "Hongyuan Tao",
                "Ying Zhang",
                "Zhenhao Tang",
                "Hongen Peng",
                "Xukun Zhu",
                "Bingchang Liu",
                "Yingguang Yang",
                "Ziyin Zhang",
                "Zhaogui Xu",
                "Haipeng Zhang",
                "Linchao Zhu",
                "Rui Wang",
                "Hang Yu",
                "Jianguo Li",
                "Peng Di"
            ],
            "affiliations": [
                "Ant Group, Hangzhou, China",
                "Shanghai Jiaotong University, Shanghai, China",
                "ShanghaiTech University, Shanghai, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16901.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#games",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ´Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Code Graph Models (CGM) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. CGM Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ±ĞµĞ·Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ¼ graph RAG, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Lite, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Empowering Code Generation with Open-Source Graph Models",
                    "desc": "This paper presents a novel approach to enhance repository-level code generation tasks using open-source Code Graph Models (CGMs). By integrating code graph structures into the attention mechanisms of Large Language Models (LLMs), the authors demonstrate that these models can effectively understand the relationships and dependencies within codebases. This method eliminates the need for agent-based solutions, which often compromise data privacy and customization. The results show a significant improvement in performance, achieving a top ranking among open-source models on the SWE-bench Lite benchmark."
                },
                "zh": {
                    "title": "å¼€æºä»£ç å›¾æ¨¡å‹æå‡ä»£ç ç”Ÿæˆæ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¼€æºä»£ç å›¾æ¨¡å‹ï¼ˆCode Graph Models, CGMsï¼‰ï¼Œæ—¨åœ¨æå‡ä»£ç ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚é€šè¿‡å°†ä»£ç å›¾ç»“æ„æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒCGMsèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ä»£ç åº“ä¸­çš„å‡½æ•°å’Œæ–‡ä»¶ã€‚ä¸ä¾èµ–ä»£ç†çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ä»£ç†ï¼Œç¡®ä¿äº†æ•°æ®éšç§å’Œæ¨¡å‹å®šåˆ¶çš„çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¼€æºQwen2.5-72Bæ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨SWE-bench LiteåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†43.00%çš„è§£å†³ç‡ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16673",
            "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large\n  Language Models via Share-GRPO",
            "url": "https://huggingface.co/papers/2505.16673",
            "abstract": "Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.",
            "score": 1,
            "issue_id": 3994,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "e90d190149bd97ed",
            "authors": [
                "Huanjin Yao",
                "Qixiang Yin",
                "Jingyi Zhang",
                "Min Yang",
                "Yibo Wang",
                "Wenhao Wu",
                "Fei Su",
                "Li Shen",
                "Minghui Qiu",
                "Dacheng Tao",
                "Jiaxing Huang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "ByteDance",
                "Nanyang Technological University",
                "The University of Sydney",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16673.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ MLLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Share-GRPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Share-GRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ³Ğ¾Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing MLLMs with Share-GRPO: Expanding Questions and Sharing Reasoning",
                    "desc": "This paper introduces Share-GRPO, a new reinforcement learning method designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). It addresses challenges like sparse rewards and advantage vanishing by expanding the question space and sharing diverse reasoning paths. The approach encourages MLLMs to explore various reasoning trajectories and share insights across different question variants. By hierarchically computing advantages, Share-GRPO enhances the stability of policy training and demonstrates superior performance on multiple reasoning benchmarks."
                },
                "zh": {
                    "title": "Share-GRPOï¼šæå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•Share-GRPOï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ‰©å±•é—®é¢˜ç©ºé—´å’Œå…±äº«å¤šæ ·çš„æ¨ç†è½¨è¿¹ï¼ŒShare-GRPOæœ‰æ•ˆåœ°è§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¨€ç–å¥–åŠ±å’Œä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡æ•°æ®è½¬æ¢æŠ€æœ¯æ‰©å±•ç»™å®šé—®é¢˜çš„ç©ºé—´ï¼Œç„¶åé¼“åŠ±MLLMåœ¨æ‰©å±•çš„é—®é¢˜ç©ºé—´ä¸­æ¢ç´¢å¤šæ ·çš„æ¨ç†è½¨è¿¹ï¼Œå¹¶åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å…±äº«è¿™äº›è½¨è¿¹ã€‚æ­¤å¤–ï¼ŒShare-GRPOåœ¨ä¼˜åŠ¿è®¡ç®—ä¸­å…±äº«å¥–åŠ±ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†ç›¸å¯¹ä¼˜åŠ¿çš„ä¼°è®¡å‡†ç¡®æ€§ï¼Œå¢å¼ºäº†ç­–ç•¥è®­ç»ƒçš„ç¨³å®šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21494",
            "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal\n  Alignment",
            "url": "https://huggingface.co/papers/2505.21494",
            "abstract": "Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.",
            "score": 0,
            "issue_id": 3992,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "9308405d203b4ba9",
            "authors": [
                "Xiaojun Jia",
                "Sensen Gao",
                "Simeng Qin",
                "Tianyu Pang",
                "Chao Du",
                "Yihao Huang",
                "Xinfeng Li",
                "Yiming Li",
                "Bo Li",
                "Yang Liu"
            ],
            "affiliations": [
                "MBZUAI, United Arab Emirates",
                "Nanyang Technological University, Singapore",
                "Sea AI Lab, Singapore",
                "University of Illinois Urbana-Champaign, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21494.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#security",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ° Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ FOA-Attack. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Adversarial Transferability with FOA-Attack",
                    "desc": "This paper addresses the vulnerability of multimodal large language models (MLLMs) to transferable adversarial examples. The authors introduce a new attack method called FOA-Attack, which focuses on aligning both global and local features to enhance the transferability of adversarial samples. By utilizing cosine similarity for global feature alignment and optimal transport for local feature alignment, the method improves the effectiveness of attacks on closed-source models. The proposed approach outperforms existing methods in various experiments, demonstrating its robustness in generating transferable adversarial examples."
                },
                "zh": {
                    "title": "æå‡å¯¹æŠ—æ ·æœ¬è½¬ç§»èƒ½åŠ›çš„FOA-Attackæ–¹æ³•",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å®¹æ˜“å—åˆ°å¯è½¬ç§»çš„å¯¹æŠ—æ ·æœ¬æ”»å‡»ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¯¹é½å…¨å±€ç‰¹å¾æ¥å®ç°ç›®æ ‡æ”»å‡»ï¼Œä½†å¿½è§†äº†å±€éƒ¨ä¿¡æ¯çš„ä¸°å¯Œæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾æœ€ä¼˜å¯¹é½çš„é’ˆå¯¹æ€§å¯è½¬ç§»å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œç§°ä¸ºFOA-Attackã€‚é€šè¿‡å¼•å…¥å…¨å±€ç‰¹å¾æŸå¤±å’Œå±€éƒ¨èšç±»æœ€ä¼˜ä¼ è¾“æŸå¤±ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†å¯¹æŠ—æ ·æœ¬çš„è½¬ç§»èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19377",
            "title": "Absolute Coordinates Make Motion Generation Easy",
            "url": "https://huggingface.co/papers/2505.19377",
            "abstract": "Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications.",
            "score": 0,
            "issue_id": 3994,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "d9415ccd47a86548",
            "authors": [
                "Zichong Meng",
                "Zeyu Han",
                "Xiaogang Peng",
                "Yiming Xie",
                "Huaizu Jiang"
            ],
            "affiliations": [
                "Northeastern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19377.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#games",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ² Ğ² Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Transformer. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€ÑˆĞ¸Ğ½ Ğ¼ĞµÑˆĞ° SMPL-H Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Text-to-Motion with Global Coordinates",
                    "desc": "This paper introduces a new approach to text-to-motion generation by using absolute joint coordinates in global space instead of the traditional local-relative motion representation. This change enhances motion fidelity, improves text alignment, and allows for better scalability, even when using a simple Transformer model. The authors demonstrate that their method supports various downstream tasks without needing complex reengineering or additional guidance. Overall, this work lays a solid foundation for future advancements in motion generation and related applications."
                },
                "zh": {
                    "title": "å…¨å±€ç»å¯¹åæ ‡æå‡æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆçš„æ•ˆæœ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œä½¿ç”¨å…¨å±€ç©ºé—´ä¸­çš„ç»å¯¹å…³èŠ‚åæ ‡æ¥æé«˜è¿åŠ¨çš„çœŸå®æ„Ÿã€æ–‡æœ¬å¯¹é½å’Œå¯æ‰©å±•æ€§ã€‚ä¼ ç»Ÿçš„è¿åŠ¨è¡¨ç¤ºæ–¹æ³•ä¾èµ–äºç›¸å¯¹è¿åŠ¨ï¼Œè™½ç„¶ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œä½†å¯¹æ‰©æ•£æ¨¡å‹çš„åº”ç”¨é€ æˆäº†é™åˆ¶ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§æ–°çš„è¡¨ç¤ºæ–¹æ³•åœ¨è¿åŠ¨çœŸå®æ„Ÿå’Œæ–‡æœ¬å¯¹é½æ–¹é¢æ˜¾è‘—æå‡ï¼Œä¸”èƒ½å¤Ÿæ”¯æŒä¸‹æ¸¸ä»»åŠ¡ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ä»æ–‡æœ¬ç›´æ¥ç”Ÿæˆè¿åŠ¨çš„SMPL-Hç½‘æ ¼é¡¶ç‚¹æ–¹é¢çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œè¿åŠ¨ç›¸å…³åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16340",
            "title": "Improving Chemical Understanding of LLMs via SMILES Parsing",
            "url": "https://huggingface.co/papers/2505.16340",
            "abstract": "Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark.",
            "score": 0,
            "issue_id": 3993,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "22065ebe729018b8",
            "authors": [
                "Yunhui Jang",
                "Jaehyung Kim",
                "Sungsoo Ahn"
            ],
            "affiliations": [
                "KAIST",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16340.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#graphs",
                    "#science",
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "CLEANMOL: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CLEANMOL - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). CLEANMOL Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€ SMILES-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ». ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ LLM Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CLEANMOL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Mol-Instructions."
                },
                "en": {
                    "title": "CLEANMOL: Enhancing LLMs for Molecular Understanding",
                    "desc": "This paper presents CLEANMOL, a new framework aimed at improving how large language models (LLMs) understand molecular structures represented in SMILES format. The authors identify that existing LLMs struggle with basic molecular tasks, such as counting rings in molecules. CLEANMOL addresses this by breaking down SMILES parsing into clear, structured tasks that enhance graph-level comprehension of molecular properties. The framework includes a pretraining dataset with varying difficulty levels, leading to improved performance on molecular understanding benchmarks."
                },
                "zh": {
                    "title": "CLEANMOLï¼šæå‡åˆ†å­ç»“æ„ç†è§£çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ†å­ç§‘å­¦çš„ç§‘å­¦å‘ç°ä¸­è¢«è¶Šæ¥è¶Šå¤šåœ°è®¤å¯ä¸ºå¼ºå¤§çš„å·¥å…·ã€‚ä¸ºäº†ä½¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ç†è§£åˆ†å­ç»“æ„ï¼Œæˆ‘ä»¬æå‡ºäº†CLEANMOLæ¡†æ¶ï¼Œå°†SMILESè§£æè½¬åŒ–ä¸ºä¸€ç³»åˆ—æ¸…æ™°ä¸”ç¡®å®šçš„ä»»åŠ¡ï¼Œä»¥ä¿ƒè¿›å›¾çº§åˆ†å­ç†è§£ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬å­å›¾åŒ¹é…å’Œå…¨å±€å›¾åŒ¹é…ï¼Œæä¾›ä¸åˆ†å­ç»“æ„ç‰¹æ€§ç›¸ä¸€è‡´çš„ç»“æ„åŒ–ç›‘ç£ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒCLEANMOLä¸ä»…å¢å¼ºäº†ç»“æ„ç†è§£èƒ½åŠ›ï¼Œè¿˜åœ¨Mol-InstructionsåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-27.html",
    "link_next": "2025-05-29.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "27.05",
        "en": "05/27",
        "zh": "5æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "29.05",
        "en": "05/29",
        "zh": "5æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 4,
        "#benchmark": 21,
        "#agents": 5,
        "#cv": 7,
        "#rl": 6,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 1,
        "#video": 9,
        "#multimodal": 14,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 13,
        "#robotics": 0,
        "#agi": 0,
        "#games": 5,
        "#interpretability": 1,
        "#reasoning": 16,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 15,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ã€‚ä»¥å‰ä¸»è¦é€šè¿‡å¢åŠ å‚æ•°æ•°é‡æ¥æé«˜æ€§èƒ½ï¼Œä½†ç°åœ¨ç¡¬ä»¶é™åˆ¶ä½¿å¾—è‡ªæ³¨æ„åŠ›æˆæœ¬æˆä¸ºä¸»è¦ç“¶é¢ˆã€‚æ–‡ç« æå‡ºç ”ç©¶é‡ç‚¹åº”ä»æ¨¡å‹å‹ç¼©è½¬å‘æ•°æ®å‹ç¼©ï¼Œç‰¹åˆ«æ˜¯ä»¤ç‰Œå‹ç¼©ã€‚ä»¤ç‰Œå‹ç¼©å¯ä»¥å‡å°‘è®­ç»ƒæˆ–æ¨ç†è¿‡ç¨‹ä¸­çš„ä»¤ç‰Œæ•°é‡ï¼Œä»è€Œæé«˜AIæ•ˆç‡ã€‚ä½œè€…åˆ†æäº†é•¿ä¸Šä¸‹æ–‡AIçš„å‘å±•ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œå¹¶æ¢è®¨äº†ä»¤ç‰Œå‹ç¼©çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ã€‚",
        "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ã€‚ä»¥å‰ä¸»è¦é€šè¿‡å¢åŠ å‚æ•°æ•°é‡æ¥æé«˜æ€§èƒ½ï¼Œä½†ç°åœ¨ç¡¬ä»¶é™åˆ¶ä½¿å¾—è‡ªæ³¨æ„åŠ›æˆæœ¬æˆä¸ºä¸»è¦ç“¶é¢ˆã€‚æ–‡ç« æå‡ºç ”ç©¶é‡ç‚¹åº”ä»æ¨¡å‹å‹ç¼©è½¬å‘æ•°æ®å‹ç¼©ï¼Œç‰¹åˆ«æ˜¯ä»¤ç‰Œå‹ç¼©ã€‚ä»¤ç‰Œå‹ç¼©å¯ä»¥å‡å°‘è®­ç»ƒæˆ–æ¨ç†è¿‡ç¨‹ä¸­çš„ä»¤ç‰Œæ•°é‡ï¼Œä»è€Œæé«˜AIæ•ˆç‡ã€‚ä½œè€…åˆ†æäº†é•¿ä¸Šä¸‹æ–‡AIçš„å‘å±•ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œå¹¶æ¢è®¨äº†ä»¤ç‰Œå‹ç¼©çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng hÃ© duÅ mÃ³shÃ¬ yÇ”yÃ¡n mÃ³xÃ­ng de kuÃ isÃ¹ fÄzhÇn. YÇqiÃ¡n zhÇ”yÃ o tÅngguÃ² zÄ“ngjiÄ cÄnshÃ¹ shÃ¹liÃ ng lÃ¡i tÃ­gÄo xÃ¬ngnÃ©ng, dÃ n xiÃ nzÃ i yÃ¬ngjiÃ n xiÃ nzhÃ¬ shÇdÃ© zÃ¬ zhÃ¹yÃ¬lÃ¬ chÃ©ngbÄ›n chÃ©ngwÃ©i zhÇ”yÃ o pÃ­ngtÇng. WÃ©nzhÄng tÃ­chÅ« yÃ¡njiÅ« zhÃ²ngdiÇn yÄ«ng cÃ³ng mÃ³xÃ­ng yÄsuÅ zhuÇnxiÃ ng shÃ¹jÃ¹ yÄsuÅ, tÃ¨biÃ© shÃ¬ lÃ¬ngpÃ¡i yÄsuÅ. LÃ¬ngpÃ¡i yÄsuÅ kÄ›yÇ jiÇnshÇo xÃ¹nliÃ n huÃ² tuÄ«lÇ guÃ²chÃ©ng zhÅng de lÃ¬ngpÃ¡i shÃ¹liÃ ng, cÃ³ng'Ã©r tÃ­gÄo AI xiÃ onÃ©ng. ZuÃ²zhÄ› fÄ“nxi le chÃ¡ng shÃ ngxÃ¬awÃ©nfÇ AI de fÄzhÇn, tÃ­chÅ« le yÄ«gÃ¨ tÇ’ngyÄ« de shÃ¹xuÃ© kuÃ ngjiÃ , bÃ¬ng tÃ ntÇo le lÃ¬ngpÃ¡i yÄsuÅ de yÅushÃ¬ hÃ© tiÇozhÃ n.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ  xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"è¯­è¨€æ¨¡å‹\", \"pinyin\": \"yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"language model\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å¿«é€Ÿ\", \"pinyin\": \"kuÃ i sÃ¹\", \"trans\": \"rapid\"},\n    {\"word\": \"å‘å±•\", \"pinyin\": \"fÄ zhÇn\", \"trans\": \"development\"},\n    {\"word\": \"ä¸»è¦\", \"pinyin\": \"zhÇ” yÃ o\", \"trans\": \"main\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅng guÃ²\", \"trans\": \"through\"},\n    {\"word\": \"å¢åŠ \", \"pinyin\": \"zÄ“ng jiÄ\", \"trans\": \"increase\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄn shÇ”\", \"trans\": \"parameter\"},\n    {\"word\": \"æ•°é‡\", \"pinyin\": \"shÃ¹ liÃ ng\", \"trans\": \"quantity\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­ gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ç¡¬ä»¶\", \"pinyin\": \"yÃ¬ng jiÃ n\", \"trans\": \"hardware\"},\n    {\"word\": \"é™åˆ¶\", \"pinyin\": \"xiÃ n zhÃ¬\", \"trans\": \"limit\"},\n    {\"word\": \"è‡ªæ³¨æ„åŠ›\", \"pinyin\": \"zÃ¬ zhÃ¹ yÃ¬ lÃ¬\", \"trans\": \"self-attention\"},\n    {\"word\": \"æˆæœ¬\", \"pinyin\": \"chÃ©ng bÄ›n\", \"trans\": \"cost\"},\n    {\"word\": \"ç“¶é¢ˆ\", \"pinyin\": \"pÃ­ng lÃ³ng\", \"trans\": \"bottleneck\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"é‡ç‚¹\", \"pinyin\": \"zhÃ²ng diÇn\", \"trans\": \"focus\"},\n    {\"word\": \"æ¨¡å‹å‹ç¼©\", \"pinyin\": \"mÃ³ xÃ­ng yÄ suÅ\", \"trans\": \"model compression\"},\n    {\"word\": \"è½¬å‘\", \"pinyin\": \"zhuÇn xiÃ ng\", \"trans\": \"turn to\"},\n    {\"word\": \"æ•°æ®å‹ç¼©\", \"pinyin\": \"shÃ¹ jÃ¹ yÄ suÅ\", \"trans\": \"data compression\"},\n    {\"word\": \"ä»¤ç‰Œ\", \"pinyin\": \"lÃ¬ng pÃ¡i\", \"trans\": \"token\"},\n    {\"word\": \"å‹ç¼©\", \"pinyin\": \"yÄ suÅ\", \"trans\": \"compression\"},\n    {\"word\": \"å‡å°‘\", \"pinyin\": \"jiÇn shÇo\", \"trans\": \"reduce\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"training\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"è¿‡ç¨‹\", \"pinyin\": \"guÃ² chÃ©ng\", \"trans\": \"process\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ o lÇœ\", \"trans\": \"efficiency\"},\n    {\"word\": \"ä½œè€…\", \"pinyin\": \"zuÃ² zhÄ›\", \"trans\": \"author\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“n xÄ«\", \"trans\": \"analyze\"},\n    {\"word\": \"é•¿ä¸Šä¸‹æ–‡\", \"pinyin\": \"chÃ¡ng shÃ ng xiÃ  wÃ©n\", \"trans\": \"long context\"},\n    {\"word\": \"æ•°å­¦æ¡†æ¶\", \"pinyin\": \"shÃ¹ xuÃ© kuÃ ng jiÃ \", \"trans\": \"mathematical framework\"},\n    {\"word\": \"æ¢è®¨\", \"pinyin\": \"tÃ n tÇo\", \"trans\": \"explore\"},\n    {\"word\": \"ä¼˜åŠ¿\", \"pinyin\": \"yÅu shÃ¬\", \"trans\": \"advantage\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"}\n]",
        "trans": "This article discusses the rapid development of large language models and multimodal language models. Previously, performance was mainly improved by increasing the number of parameters, but now hardware limitations make the cost of self-attention a major bottleneck. The article proposes that the focus of research should shift from model compression to data compression, particularly token compression. Token compression can reduce the number of tokens during training or inference, thereby improving AI efficiency. The author analyzes the development of long-context AI, proposes a unified mathematical framework, and explores the advantages and challenges of token compression.",
        "update_ts": "2025-05-27 09:12"
    }
}