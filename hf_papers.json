{
    "date": {
        "ru": "1 Ğ¸ÑĞ»Ñ",
        "en": "July 1",
        "zh": "7æœˆ1æ—¥"
    },
    "time_utc": "2025-07-01 04:31",
    "weekday": 1,
    "issue_id": 4571,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.23858",
            "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
            "url": "https://huggingface.co/papers/2506.23858",
            "abstract": "VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.",
            "score": 17,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "684efafe36e7bbc8",
            "authors": [
                "Jianzong Wu",
                "Liang Hou",
                "Haotian Yang",
                "Xin Tao",
                "Ye Tian",
                "Pengfei Wan",
                "Di Zhang",
                "Yunhai Tong"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23858.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VMoBA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "VMoBA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VDM). ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ. VMoBA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸ 1D-2D-3D, Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Accelerating Video Generation with Sparse Attention",
                    "desc": "VMoBA is a new sparse attention mechanism designed to improve Video Diffusion Models (VDMs) by reducing the computational complexity associated with full attention mechanisms. It addresses the challenges of generating long-duration, high-resolution videos while maintaining quality. The method incorporates a layer-wise recurrent block partition scheme, global block selection, and threshold-based block selection to optimize attention patterns specific to video data. Experimental results show that VMoBA significantly speeds up training and inference times while achieving comparable or superior video generation quality."
                },
                "zh": {
                    "title": "VMoBAï¼šåŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„æ–°ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶",
                    "desc": "VMoBAæ˜¯ä¸€ç§æ–°é¢–çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸“ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰è®¾è®¡ï¼Œæ—¨åœ¨è§£å†³å…¨æ³¨æ„åŠ›æœºåˆ¶çš„å¹³æ–¹å¤æ‚åº¦é—®é¢˜ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒè§†é¢‘å˜æ¢å™¨çš„æ³¨æ„åŠ›æ¨¡å¼è¿›è¡Œæ·±å…¥åˆ†æï¼ŒVMoBAèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è§†é¢‘æ•°æ®çš„æ—¶ç©ºç‰¹æ€§ã€‚è¯¥æœºåˆ¶é€šè¿‡ä¸‰é¡¹å…³é”®æ”¹è¿›æå‡äº†åŸæœ‰çš„MoBAæ¡†æ¶ï¼ŒåŒ…æ‹¬åŠ¨æ€é€‚åº”æ—¶ç©ºæ³¨æ„åŠ›æ¨¡å¼çš„åˆ†å±‚é€’å½’å—åˆ’åˆ†æ–¹æ¡ˆã€ä¼˜å…ˆé€‰æ‹©æœ€æ˜¾è‘—çš„æŸ¥è¯¢-é”®å—äº¤äº’çš„å…¨å±€å—é€‰æ‹©ï¼Œä»¥åŠåŸºäºé˜ˆå€¼çš„å—é€‰æ‹©æ¥åŠ¨æ€ç¡®å®šå…³æ³¨å—çš„æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVMoBAåœ¨é•¿åºåˆ—è®­ç»ƒä¸­æ˜¾è‘—åŠ é€ŸVDMsçš„è®­ç»ƒï¼ŒåŒæ—¶åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¸å…¨æ³¨æ„åŠ›æœºåˆ¶ç›¸å½“æˆ–æ›´ä¼˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.24123",
            "title": "Calligrapher: Freestyle Text Image Customization",
            "url": "https://huggingface.co/papers/2506.24123",
            "abstract": "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.",
            "score": 10,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "ee3cd26fec757450",
            "authors": [
                "Yue Ma",
                "Qingyan Bai",
                "Hao Ouyang",
                "Ka Leong Cheng",
                "Qiuyu Wang",
                "Hongyu Liu",
                "Zichen Liu",
                "Haofan Wang",
                "Jingye Chen",
                "Yujun Shen",
                "Qifeng Chen"
            ],
            "affiliations": [
                "Ant Group, China",
                "Hong Kong University of Science and Technology, China",
                "InstantX, Independent Research Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.24123.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "âœ’ï¸",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ°Ğ»Ğ»Ğ¸Ğ³Ñ€Ğ°Ñ„: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Calligrapher - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². Calligrapher Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¸Ñ„Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Digital Typography with Style Consistency",
                    "desc": "Calligrapher is a new framework that uses diffusion processes to create high-quality digital typography with a focus on style consistency. It addresses challenges in customizing typography by employing a self-distillation method that utilizes a pre-trained text-to-image model and a large language model to create a style-focused benchmark. Additionally, it features a localized style injection system that extracts style features from reference images using a trainable style encoder. The framework's in-context generation mechanism enhances the alignment of styles, allowing for precise and intricate typography suitable for various design applications."
                },
                "zh": {
                    "title": "Calligrapherï¼šè‡ªåŠ¨åŒ–é«˜è´¨é‡æ•°å­—æ’ç‰ˆçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "Calligrapher æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç»“åˆäº†è‡ªè’¸é¦å’Œå±€éƒ¨é£æ ¼æ³¨å…¥æŠ€æœ¯ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡ä¸”é£æ ¼ä¸€è‡´çš„æ•°å­—æ’ç‰ˆã€‚è¯¥æ¡†æ¶è§£å†³äº†æ’ç‰ˆå®šåˆ¶ä¸­é£æ ¼æ§åˆ¶å’Œæ•°æ®ä¾èµ–çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ã€‚é¦–å…ˆï¼Œå¼€å‘äº†ä¸€ç§è‡ªè’¸é¦æœºåˆ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè‡ªåŠ¨æ„å»ºä»¥é£æ ¼ä¸ºä¸­å¿ƒçš„æ’ç‰ˆåŸºå‡†ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¯è®­ç»ƒçš„é£æ ¼ç¼–ç å™¨å¼•å…¥å±€éƒ¨é£æ ¼æ³¨å…¥æ¡†æ¶ï¼Œæå–å‚è€ƒå›¾åƒä¸­çš„å¼ºå¥é£æ ¼ç‰¹å¾ï¼Œä»è€Œæå‡ç›®æ ‡é£æ ¼çš„å¯¹é½ç²¾åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23542",
            "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention",
            "url": "https://huggingface.co/papers/2506.23542",
            "abstract": "A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at https://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.",
            "score": 5,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "9c9286ea4d796818",
            "authors": [
                "Weida Wang",
                "Changyong He",
                "Jin Zeng",
                "Di Qiu"
            ],
            "affiliations": [
                "Google",
                "School of Computer Science and Technology, Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23542.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#dataset",
                    "#graphs",
                    "#synthetic",
                    "#open_source"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ToF: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ°Ñ… Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Time-of-Flight (ToF), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹. Ğ¡ĞµÑ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing ToF Depth Images with Motion-Invariant Graph Fusion",
                    "desc": "This paper presents a new method for denoising depth images captured by Time-of-Flight (ToF) sensors, which often suffer from noise. The proposed network utilizes motion-invariant graph fusion to improve both the temporal stability and spatial sharpness of the images. By leveraging the self-similarity of graph structures across frames, the method effectively addresses depth variations while maintaining consistency. The approach is formulated as a maximum a posterior problem, resulting in an interpretable network that outperforms existing methods on benchmark datasets."
                },
                "zh": {
                    "title": "æå‡ToFæ·±åº¦å›¾åƒçš„å»å™ªæ•ˆæœ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ—¶é—´é£è¡Œï¼ˆToFï¼‰æ·±åº¦å»å™ªç½‘ç»œï¼Œåˆ©ç”¨è¿åŠ¨ä¸å˜å›¾èåˆå’Œè‡ªé€‚åº”æ»¤æ³¢å™¨æ¥æé«˜æ—¶é—´ç¨³å®šæ€§å’Œç©ºé—´æ¸…æ™°åº¦ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•å¸§å¤„ç†æˆ–å¤šå¸§å¤„ç†ï¼Œä½†æœªè€ƒè™‘ä¸åŒå¸§ä¸­å¯¹åº”åƒç´ çš„æ·±åº¦å˜åŒ–ï¼Œå¯¼è‡´æ—¶é—´ä¸ä¸€è‡´å’Œç©ºé—´æ¨¡ç³Šã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å›¾ç»“æ„çš„æ—¶é—´è‡ªç›¸ä¼¼æ€§ï¼Œå®ç°è·¨å¸§å‡ ä½•æ³¨æ„åŠ›çš„å›¾èåˆï¼Œå¹¶ç»“åˆå›¾çš„å¹³æ»‘æ€§å…ˆéªŒå’ŒToFå™ªå£°åˆ†å¸ƒçš„æ•°æ®ä¿¡åº¦é¡¹ï¼Œæ„å»ºæœ€å¤§åéªŒå»å™ªé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨åˆæˆDVToFæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®Kinectv2æ•°æ®é›†ä¸Šå±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17417",
            "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?",
            "url": "https://huggingface.co/papers/2506.17417",
            "abstract": "Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated that inference-time computation techniques, such as decoding-time scaling and self-refinement, can significantly enhance reasoning capabilities without relying on external knowledge. A key driver of this success is the emergence of self-correction and self-verification behaviors, often elicited through reinforcement learning (RL). In this paper, we investigate whether these inference-time techniques extend effectively to vision-language models (VLMs), particularly those trained with RL. We find that while decoding strategies such as majority voting and best-of-N selection with self-verification all improve VLM reasoning performance, generation-reliant methods such as the former achieve significantly higher gains versus verification-reliant methods such as the latter. Additionally, the self-correction behavior often associated with RL-tuned models, such as aha moment, does not lead to measurable gains. We show via extensive experimentation within the inference-time scaling framework to identify a key root cause: RL-trained VLMs still lack robust self-verification capabilities across both visual and textual modalities.",
            "score": 5,
            "issue_id": 4570,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "57576e8287f4515e",
            "authors": [
                "Mingyuan Wu",
                "Meitang Li",
                "Jingcheng Yang",
                "Jize Jiang",
                "Kaizhuo Yan",
                "Zhaoheng Li",
                "Minjia Zhang",
                "Klara Nahrstedt"
            ],
            "affiliations": [
                "University of Illinois Urbana Champaign",
                "University of Michigan Ann Arbor"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17417.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¿Ğ¾Ğ±ĞµĞ´Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¼Ğ°Ğ¶Ğ¾Ñ€Ğ¸Ñ‚Ğ°Ñ€Ğ½Ğ¾Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñƒ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Boosting VLM Reasoning with Inference-Time Techniques",
                    "desc": "This paper explores how inference-time techniques can improve reasoning in vision-language models (VLMs). It highlights that methods based on generation, like decoding-time scaling, provide better performance than those based on verification. The study also reveals that reinforcement learning (RL) trained models do not exhibit the expected self-correction benefits. Ultimately, the findings suggest that RL-trained VLMs struggle with self-verification, which limits their reasoning capabilities."
                },
                "zh": {
                    "title": "æ¨ç†æ—¶æŠ€æœ¯æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ¨ç†æ—¶æŠ€æœ¯å¦‚ä½•æå‡è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè§£ç æ—¶é—´ç¼©æ”¾å’Œè‡ªæˆ‘ä¿®æ­£ç­‰æŠ€æœ¯åœ¨æ²¡æœ‰å¤–éƒ¨çŸ¥è¯†çš„æƒ…å†µä¸‹æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„è¡¨ç°ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ¨¡å‹æœªèƒ½æ˜¾ç¤ºå‡ºè‡ªæˆ‘ä¿®æ­£çš„ä¼˜åŠ¿ï¼Œä½†åŸºäºç”Ÿæˆçš„æ–¹æ³•åœ¨æå‡æ€§èƒ½æ–¹é¢æ˜æ˜¾ä¼˜äºåŸºäºéªŒè¯çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLè®­ç»ƒçš„VLMåœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¸Šä»ç¼ºä¹å¼ºå¤§çš„è‡ªæˆ‘éªŒè¯èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.24119",
            "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.24119",
            "abstract": "Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.",
            "score": 4,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "fbb4b5b047892d14",
            "authors": [
                "Bo Liu",
                "Leon Guertler",
                "Simon Yu",
                "Zichen Liu",
                "Penghui Qi",
                "Daniel Balcells",
                "Mickel Liu",
                "Cheston Tan",
                "Weiyan Shi",
                "Min Lin",
                "Wee Sun Lee",
                "Natasha Jaques"
            ],
            "affiliations": [
                "Centre for Frontier AI Research (CFAR), A*STAR",
                "National University of Singapore",
                "Northeastern University",
                "Plastic Labs",
                "Sea AI Lab",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.24119.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#games",
                    "#transfer_learning",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ¾Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SPIRAL - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñƒ Ğ² Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ²ĞµÑ€ÑĞ¸Ğ¹ ÑĞ°Ğ¼Ğ¸Ñ… ÑĞµĞ±Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼ ĞºÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ³Ñ€Ğµ ĞšÑƒĞ½-Ğ¿Ğ¾ĞºĞµÑ€ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-4B-Base Ğ½Ğ° 8.6% Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ 8.4% Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°: ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ, Ñ€Ğ°ÑÑ‡ĞµÑ‚ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ÑĞ¼."
                },
                "en": {
                    "title": "Empowering AI Reasoning through Self-Play in Zero-Sum Games",
                    "desc": "The paper presents SPIRAL, a self-play framework that enhances reasoning abilities in language models by allowing them to compete against improved versions of themselves in zero-sum games. This approach eliminates the need for human-generated problem-answer pairs and domain-specific rewards, enabling models to learn through an infinite curriculum of challenges. By implementing a multi-agent reinforcement learning system, SPIRAL stabilizes training and facilitates the development of reasoning skills that can be transferred across different tasks. The results show significant improvements in reasoning performance, demonstrating the effectiveness of self-play in fostering cognitive skills in AI models."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å¯¹å¼ˆï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSPIRALçš„è‡ªæˆ‘å¯¹å¼ˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›¶å’Œæ¸¸æˆæå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨¡å‹ä¸è‡ªèº«ä¸æ–­æ”¹è¿›çš„ç‰ˆæœ¬è¿›è¡Œå¤šè½®å¯¹å¼ˆï¼Œæ¶ˆé™¤äº†å¯¹äººå·¥ç›‘ç£çš„éœ€æ±‚ã€‚SPIRALèƒ½å¤Ÿç”Ÿæˆæ— é™çš„é€æ­¥æŒ‘æˆ˜é—®é¢˜ï¼Œä½¿æ¨¡å‹å¿…é¡»é€‚åº”æ›´å¼ºçš„å¯¹æ‰‹ï¼Œä»è€Œå®ç°è‡ªæˆ‘æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SPIRALè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨æ•°å­¦å’Œä¸€èˆ¬æ¨ç†æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†é›¶å’Œæ¸¸æˆåœ¨è‡ªä¸»æ¨ç†å‘å±•ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17930",
            "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
            "url": "https://huggingface.co/papers/2506.17930",
            "abstract": "A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into \"gibberish\" can improve large language model performance across various tasks, surpassing state-of-the-art methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent \"gibberish\" can remarkably improve performance across diverse tasks. Notably, the \"gibberish\" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.",
            "score": 4,
            "issue_id": 4570,
            "pub_date": "2025-06-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ½Ñ",
                "en": "June 22",
                "zh": "6æœˆ22æ—¥"
            },
            "hash": "346a389b3fbfb2bd",
            "authors": [
                "Jianyu Wang",
                "Zhiqiang Hu",
                "Lidong Bing"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "MiroMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17930.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ‘ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PromptQuine. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ 'Ğ±ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ñ‹' Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². PromptQuine Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking LLM Potential with 'Gibberish' Prompts!",
                    "desc": "This paper introduces PromptQuine, a new approach to designing prompts for large language models (LLMs). Instead of using well-structured instructions, it shows that transforming random examples into 'gibberish' can enhance model performance on various tasks. The authors propose an evolutionary search framework that autonomously finds effective pruning strategies, even with limited data. Their results indicate that this unconventional method can outperform traditional prompt optimization techniques, suggesting new directions for improving in-context learning."
                },
                "zh": {
                    "title": "èƒ¡è¨€ä¹±è¯­ï¼Œæå‡æ¨¡å‹è¡¨ç°çš„ç§˜å¯†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºè®¾è®¡èŒƒå¼ï¼Œç§°ä¸ºPromptQuineï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç¤ºæ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°†éšæœºç¤ºä¾‹ä¿®å‰ªæˆçœ‹ä¼¼æ— æ„ä¹‰çš„â€œèƒ¡è¨€ä¹±è¯­â€å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œç”šè‡³è¶…è¶Šäº†ç°æœ‰çš„æœ€ä½³æ–¹æ³•ã€‚å°½ç®¡å‘ç°æœ‰æ•ˆçš„ä¿®å‰ªç­–ç•¥å¹¶ä¸ç®€å•ï¼Œä½†PromptQuineæ¡†æ¶é€šè¿‡è‡ªæˆ‘å‘ç°çš„æ–¹å¼ï¼Œè‡ªåŠ¨æœç´¢ä¿®å‰ªç­–ç•¥ï¼Œåˆ©ç”¨ä½æ•°æ®ç¯å¢ƒè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¸Œæœ›èƒ½ä¸ºåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æœºåˆ¶ç ”ç©¶æä¾›æŒ‡å¯¼ï¼Œå¹¶æ¨åŠ¨æ›´å¼€æ”¾çš„æœç´¢ç®—æ³•çš„å‘å±•ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„LLMæç¤ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23394",
            "title": "Teaching a Language Model to Speak the Language of Tools",
            "url": "https://huggingface.co/papers/2506.23394",
            "abstract": "A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.  \t\t\t\t\tAI-generated summary \t\t\t\t External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.",
            "score": 0,
            "issue_id": 4570,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ½Ñ",
                "en": "June 29",
                "zh": "6æœˆ29æ—¥"
            },
            "hash": "19b24559e749a260",
            "authors": [
                "Simeon Emanuilov"
            ],
            "affiliations": [
                "Department of Software Technologies, Faculty of Mathematics and Informatics, Sofia University St. Kliment Ohridski"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23394.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»Ğ³Ğ°Ñ€ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµÑ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ BgGPT Ğ½Ğ° Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° TUCAN Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering Multilingual Models for Effective Tool Use",
                    "desc": "This paper presents a new method to enhance language models for better tool usage in various languages, focusing on Bulgarian. It addresses the challenges faced by multilingual models in accurately using functions and generating structured outputs, especially in lower-resource languages. The authors introduce TUCAN, a model that significantly improves function-calling accuracy by training on a specialized bilingual dataset. This approach not only boosts performance but also ensures that the outputs are clean and usable, paving the way for better multilingual applications."
                },
                "zh": {
                    "title": "æå‡å¤šè¯­è¨€å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­å·¥å…·ä½¿ç”¨çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯ä¿åŠ åˆ©äºšè¯­çš„åŠŸèƒ½è°ƒç”¨å‡†ç¡®æ€§ã€‚å¤§å¤šæ•°å¤šè¯­è¨€æ¨¡å‹åœ¨éè‹±è¯­è¯­è¨€ä¸­ç¼ºä¹å¯é çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­è¡¨ç°ä¸ä½³ã€‚ç ”ç©¶é€šè¿‡å¯¹BgGPTæ¨¡å‹ç³»åˆ—è¿›è¡ŒæŒç»­è®­ç»ƒï¼Œä½¿ç”¨ä¸€ä¸ªåŒ…å«10,035ä¸ªåŠŸèƒ½è°ƒç”¨ç¤ºä¾‹çš„åŒè¯­æ•°æ®é›†ï¼Œæ¥å¢å¼ºæ¨¡å‹çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œç ”ç©¶æ¨å‡ºçš„TUCANæ¨¡å‹åœ¨åŠŸèƒ½è°ƒç”¨å‡†ç¡®æ€§ä¸Šæ¯”åŸºç¡€æ¨¡å‹æé«˜äº†28.75%ï¼Œå¹¶ä¸”åœ¨å“åº”æ ¼å¼ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-30.html",
    "link_next": "2025-07-02.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "30.06",
        "en": "06/30",
        "zh": "6æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "02.07",
        "en": "07/02",
        "zh": "7æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}