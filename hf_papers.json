{
    "date": {
        "ru": "28 августа",
        "en": "August 28",
        "zh": "8月28日"
    },
    "time_utc": "2025-08-28 02:21",
    "weekday": 3,
    "issue_id": 5585,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.19652",
            "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
            "url": "https://huggingface.co/papers/2508.19652",
            "abstract": "Vision-SR1 uses reinforcement learning to enhance visual reasoning in vision-language models by decomposing the process into visual perception and language reasoning stages, improving accuracy and reducing hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.",
            "score": 1,
            "issue_id": 5585,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "990502a2cc19d192",
            "authors": [
                "Zongxia Li",
                "Wenhao Yu",
                "Chengsong Huang",
                "Rui Liu",
                "Zhenwen Liang",
                "Fuxiao Liu",
                "Jingxi Che",
                "Dian Yu",
                "Jordan Boyd-Graber",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab, Seattle",
                "University of Maryland, College Park",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19652.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#hallucinations",
                    "#training",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение визуального мышления ИИ без внешних аннотаций",
                    "desc": "Vision-SR1 - это метод улучшения визуального мышления в мультимодальных моделях с помощью обучения с подкреплением. Он разделяет процесс на этапы визуального восприятия и языкового рассуждения, что повышает точность и уменьшает галлюцинации. Метод использует самовознаграждение, не полагаясь на внешние визуальные аннотации. Эксперименты показывают, что Vision-SR1 улучшает визуальное мышление, снижает визуальные галлюцинации и уменьшает зависимость от языковых шаблонов в различных задачах компьютерного зрения и обработки естественного языка."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Self-Rewarding Learning",
                    "desc": "Vision-SR1 is a novel approach that enhances visual reasoning in vision-language models (VLMs) by using reinforcement learning. It breaks down the reasoning process into two distinct stages: visual perception and language reasoning, allowing for more focused training. By generating self-contained visual perceptions, the model can validate its understanding without needing to refer back to the original image. This method reduces visual hallucinations and reliance on language shortcuts, leading to improved accuracy in various vision-language tasks."
                },
                "zh": {
                    "title": "Vision-SR1：提升视觉推理的自我奖励方法",
                    "desc": "Vision-SR1是一种利用强化学习的方法，旨在增强视觉语言模型中的视觉推理能力。该方法将推理过程分解为视觉感知和语言推理两个阶段，从而提高了模型的准确性并减少了幻觉现象。通过自我奖励机制，Vision-SR1不依赖外部视觉监督，能够有效地训练模型进行更好的视觉感知和语言推理。实验结果表明，Vision-SR1在多种视觉语言任务中显著改善了视觉推理能力，降低了对语言捷径的依赖。"
                }
            }
        }
    ],
    "link_prev": "2025-08-27.html",
    "link_next": "2025-08-29.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "27.08",
        "en": "08/27",
        "zh": "8月27日"
    },
    "short_date_next": {
        "ru": "29.08",
        "en": "08/29",
        "zh": "8月29日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}