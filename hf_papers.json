{
    "date": {
        "ru": "7 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 7",
        "zh": "3æœˆ7æ—¥"
    },
    "time_utc": "2025-03-07 07:10",
    "weekday": 4,
    "issue_id": 2582,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.04625",
            "title": "START: Self-taught Reasoner with Tools",
            "url": "https://huggingface.co/papers/2503.04625",
            "abstract": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.",
            "score": 28,
            "issue_id": 2579,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "8961f69e1eda24ad",
            "authors": [
                "Chengpeng Li",
                "Mingfeng Xue",
                "Zhenru Zhang",
                "Jiaxi Yang",
                "Beichen Zhang",
                "Xiang Wang",
                "Bowen Yu",
                "Binyuan Hui",
                "Junyang Lin",
                "Dayiheng Liu"
            ],
            "affiliations": [
                "Alibaba Group",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04625.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#rl",
                    "#training",
                    "#architecture",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "START: Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ START - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. START Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Hint-infer Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ Hint Rejection Sampling Fine-Tuning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ QwQ-32B Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. START Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ½Ğ°ÑƒĞºĞµ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ PhD Ğ¸ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Reasoning with External Tools: Introducing START",
                    "desc": "This paper presents START, a new long Chain-of-thought reasoning model that integrates external tools to improve reasoning capabilities. Traditional large reasoning models often struggle with hallucinations and inefficiencies, but START addresses these issues by allowing the model to perform complex computations and self-debugging. The innovation lies in two techniques: Hint-infer, which uses designed hints to encourage tool usage, and Hint Rejection Sampling Fine-Tuning, which refines the model's reasoning paths. START demonstrates superior performance on various benchmarks, surpassing its predecessor and competing effectively with state-of-the-art models."
                },
                "zh": {
                    "title": "å·¥å…·æ•´åˆï¼Œæ¨ç†æ›´å¼ºï¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é•¿é“¾æ¨ç†æ¨¡å‹STARTï¼ˆè‡ªæˆ‘å­¦ä¹ æ¨ç†å™¨ä¸å·¥å…·ï¼‰ï¼Œå®ƒé€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚STARTåˆ©ç”¨ä»£ç æ‰§è¡Œè¿›è¡Œå¤æ‚è®¡ç®—ã€è‡ªæˆ‘æ£€æŸ¥ã€æ¢ç´¢å¤šç§æ–¹æ³•å’Œè‡ªæˆ‘è°ƒè¯•ï¼Œä»è€Œå…‹æœäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸¸è§çš„å¹»è§‰å’Œä½æ•ˆé—®é¢˜ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºè‡ªæˆ‘å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æç¤ºæ¨ç†ï¼ˆHint-inferï¼‰å’Œæç¤ºæ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆHint-RFTï¼‰ä¸¤ç§æŠ€æœ¯ï¼Œå‰è€…é€šè¿‡æ’å…¥è®¾è®¡çš„æç¤ºæ¥æ¿€å‘æ¨¡å‹ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ã€‚ç»è¿‡å¾®è°ƒï¼ŒSTARTåœ¨å¤šä¸ªç§‘å­¦é—®ç­”å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºåŸºç¡€æ¨¡å‹QwQ-32Bã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03803",
            "title": "EgoLife: Towards Egocentric Life Assistant",
            "url": "https://huggingface.co/papers/2503.03803",
            "abstract": "We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.",
            "score": 8,
            "issue_id": 2581,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 5",
                "zh": "3æœˆ5æ—¥"
            },
            "hash": "52396234365a3fb0",
            "authors": [
                "Jingkang Yang",
                "Shuai Liu",
                "Hongming Guo",
                "Yuhao Dong",
                "Xiamengwei Zhang",
                "Sicheng Zhang",
                "Pengyun Wang",
                "Zitang Zhou",
                "Binzhu Xie",
                "Ziyue Wang",
                "Bei Ouyang",
                "Zhengyu Lin",
                "Marco Cominelli",
                "Zhongang Cai",
                "Yuanhan Zhang",
                "Peiyuan Zhang",
                "Fangzhou Hong",
                "Joerg Widmer",
                "Francesco Gringoli",
                "Lei Yang",
                "Bo Li",
                "Ziwei Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.03803.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#long_context",
                    "#dataset",
                    "#open_source",
                    "#agents",
                    "#multimodal",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ‘“",
                "ru": {
                    "title": "EgoLife: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¾Ñ‡ĞºĞ¾Ğ²",
                    "desc": "ĞŸÑ€Ğ¾ĞµĞºÑ‚ EgoLife Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‡ĞºĞ¾Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… EgoLife Dataset, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 300 Ñ‡Ğ°ÑĞ¾Ğ² ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸ ÑˆĞµÑÑ‚Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ EgoLifeQA Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° EgoButler, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EgoGPT Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ EgoRAG."
                },
                "en": {
                    "title": "Empowering Daily Life with Egocentric AI Assistance",
                    "desc": "The paper presents EgoLife, an AI-powered life assistant designed to enhance personal efficiency through wearable glasses that capture egocentric video data. A comprehensive dataset, the EgoLife Dataset, was created by recording daily activities of participants over a week, resulting in 300 hours of multimodal data with detailed annotations. The study introduces EgoLifeQA, a set of question-answering tasks that utilize this dataset to assist users with practical life questions and personalized recommendations. To tackle challenges in visual-audio model development and long-context question answering, the authors propose EgoButler, which includes EgoGPT for video understanding and EgoRAG for retrieval-based answering."
                },
                "zh": {
                    "title": "æ™ºèƒ½ç”Ÿæ´»åŠ©æ‰‹ï¼Œæå‡ä¸ªäººæ•ˆç‡",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†EgoLifeé¡¹ç›®ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç”Ÿæ´»åŠ©æ‰‹ï¼Œé€šè¿‡AIé©±åŠ¨çš„å¯ç©¿æˆ´çœ¼é•œæå‡ä¸ªäººæ•ˆç‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„æ•°æ®æ”¶é›†ç ”ç©¶ï¼Œå…­åå‚ä¸è€…å…±åŒç”Ÿæ´»ä¸€å‘¨ï¼Œä½¿ç”¨AIçœ¼é•œè®°å½•æ—¥å¸¸æ´»åŠ¨ï¼Œå½¢æˆäº†EgoLifeæ•°æ®é›†ï¼ŒåŒ…å«300å°æ—¶çš„å¤šè§†è§’ã€å¤šæ¨¡æ€æ—¥å¸¸ç”Ÿæ´»æ•°æ®ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EgoLifeQAï¼Œä¸€ä¸ªé’ˆå¯¹ç”Ÿæ´»çš„é•¿æ–‡æœ¬é—®ç­”ä»»åŠ¡ï¼Œæ—¨åœ¨æä¾›å®ç”¨çš„æ—¥å¸¸ç”Ÿæ´»å¸®åŠ©ã€‚ä¸ºäº†è§£å†³å…³é”®æŠ€æœ¯æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EgoButlerç³»ç»Ÿï¼ŒåŒ…æ‹¬EgoGPTå’ŒEgoRAGï¼Œå‰è€…åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåè€…æ”¯æŒè¶…é•¿æ–‡æœ¬é—®é¢˜çš„å›ç­”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04598",
            "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
            "url": "https://huggingface.co/papers/2503.04598",
            "abstract": "Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose HybridNorm, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm.",
            "score": 5,
            "issue_id": 2579,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "78b05ed4e27a874b",
            "authors": [
                "Zhijian Zhuo",
                "Yutao Zeng",
                "Ya Wang",
                "Sijun Zhang",
                "Jian Yang",
                "Xiaoqing Li",
                "Xun Zhou",
                "Jinwen Ma"
            ],
            "affiliations": [
                "Capital University of Economics and Business",
                "School of Mathematical Sciences, Peking University",
                "SeedFoundation-Model, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04598.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "HybridNorm: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ HybridNorm. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Pre-Norm Ğ¸ Post-Norm ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ QKV Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Post-Norm Ğ² FFN ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. HybridNorm Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ HybridNorm Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "HybridNorm: The Best of Both Normalization Worlds for Transformers",
                    "desc": "This paper introduces HybridNorm, a new normalization strategy for training deep transformer networks, which combines the strengths of Pre-Norm and Post-Norm methods. Pre-Norm structures help with training stability but often lead to lower performance, while Post-Norm structures typically perform better but can complicate training. HybridNorm applies QKV normalization in the attention mechanism and Post-Norm in the feed-forward network, resulting in improved training stability and performance. Experiments demonstrate that HybridNorm outperforms both Pre-Norm and Post-Norm across various benchmarks, making it a promising approach for enhancing large language models."
                },
                "zh": {
                    "title": "HybridNormï¼šæå‡å˜æ¢å™¨æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ä¸æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ï¼Œç§°ä¸ºHybridNormï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å˜æ¢å™¨ç½‘ç»œè®­ç»ƒä¸­çš„å±‚å½’ä¸€åŒ–ä½ç½®é—®é¢˜ã€‚HybridNormç»“åˆäº†Pre-Normå’ŒPost-Normçš„ä¼˜ç‚¹ï¼Œåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨QKVå½’ä¸€åŒ–ï¼Œè€Œåœ¨å‰é¦ˆç½‘ç»œä¸­ä½¿ç”¨Post-Normã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHybridNormåœ¨ç¨ å¯†å’Œç¨€ç–æ¶æ„ä¸­å‡ä¼˜äºä¼ ç»Ÿçš„Pre-Normå’ŒPost-Normæ–¹æ³•ï¼Œæå‡äº†å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºæ·±åº¦å˜æ¢å™¨æ¨¡å‹çš„è®­ç»ƒå’Œæ€§èƒ½æ”¹è¿›æä¾›äº†æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04222",
            "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
            "url": "https://huggingface.co/papers/2503.04222",
            "abstract": "We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.",
            "score": 5,
            "issue_id": 2578,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "c7d793a0b91efd5d",
            "authors": [
                "Ziyi Yang",
                "Fanqi Wan",
                "Longguang Zhong",
                "Canbin Huang",
                "Guosheng Liang",
                "Xiaojun Quan"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Sun Yat-sen University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04222.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#small_models",
                    "#rlhf",
                    "#transfer_learning",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ñ‰Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ",
                    "desc": "FuseChat-3.0 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… LLM Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FuseChat-3.0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Llama-3.1-8B-Instruct Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 6,8 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ 14 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Fusing Strengths for Smarter, Smaller Models",
                    "desc": "FuseChat-3.0 is a collection of large language models (LLMs) that combines the strengths of various larger source models into smaller, more efficient target models. The training process involves supervised fine-tuning to align the target models with the source models, followed by Direct Preference Optimization to enhance performance based on preferences from multiple sources. This innovative approach leads to significant improvements in tasks like instruction following, general knowledge, mathematics, and coding. The results show an average performance boost of 6.8 points across 14 benchmarks, with particularly impressive gains in instruction-following tasks."
                },
                "zh": {
                    "title": "èåˆå¤šæºæ¨¡å‹ï¼Œæå‡è¯­è¨€ç†è§£èƒ½åŠ›",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†FuseChat-3.0ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ•´åˆä¸åŒæ¥æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¼˜åŠ¿è€Œå¼€å‘çš„æ›´ç´§å‡‘çš„ç›®æ ‡LLMå¥—ä»¶ã€‚æºæ¨¡å‹åŒ…æ‹¬å¼ºå¤§çš„Gemma-2-27B-itã€Mistral-Large-Instruct-2407ã€Qwen-2.5-72B-Instructå’ŒLlama-3.1-70B-Instructã€‚ç›®æ ‡æ¨¡å‹åˆ™é›†ä¸­åœ¨ä¸‰ç§å¹¿æ³›ä½¿ç”¨çš„å°å‹å˜ä½“ä¸Šï¼Œä»¥åŠä¸¤ä¸ªè¶…ç´§å‡‘é€‰é¡¹ã€‚é€šè¿‡ä¸“é—¨çš„æ•°æ®æ„å»ºåè®®å’Œä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼ŒFuseChat-3.0åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04094",
            "title": "PokÃ©Champ: an Expert-level Minimax Language Agent",
            "url": "https://huggingface.co/papers/2503.04094",
            "abstract": "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\\'emon battles. Built on a general framework for two-player competitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently outperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o, with a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the Pok\\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.",
            "score": 4,
            "issue_id": 2580,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "bffe348d8443d4c2",
            "authors": [
                "Seth Karten",
                "Andy Luu Nguyen",
                "Chi Jin"
            ],
            "affiliations": [
                "Department of Computer Science, Princeton University",
                "Department of Electrical and Computer Engineering, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04094.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#games"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "PokeChamp: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PokeChamp - Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞºÑĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ±Ğ¾ĞµĞ² Ğ² PokÃ©mon. PokeChamp Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LLM Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ğ³Ñ€Ñ‹ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ GPT-4o Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ±ĞµĞ´ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ² Ñ‚Ğ¾Ğ¿-30% - 10% Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ PokÃ©mon Showdown. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ğ¾ĞµĞ² PokÃ©mon Ğ¸ ÑĞµÑ€Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ñ."
                },
                "en": {
                    "title": "Pok'eChamp: Elevating PokÃ©mon Battles with LLMs",
                    "desc": "Pok'eChamp is a minimax agent designed for PokÃ©mon battles that utilizes Large Language Models (LLMs) to improve its decision-making process. It replaces traditional components of the minimax algorithm with LLMs for player action sampling, opponent modeling, and value function estimation, allowing it to better understand gameplay history and human strategies. The agent demonstrates impressive performance, achieving a 76% win rate against top LLM-based bots and a 64% win rate with a smaller model, showcasing its effectiveness in competitive play. Additionally, Pok'eChamp compiles a large dataset of over 3 million PokÃ©mon battles to create benchmarks for evaluating battling skills and encourages further research in integrating LLMs with game-theoretic approaches."
                },
                "zh": {
                    "title": "PokÃ©Champï¼šå®å¯æ¢¦å¯¹æˆ˜ä¸­çš„æ™ºèƒ½ä»£ç†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†PokÃ©Champï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æå°æå¤§ç®—æ³•ä»£ç†ï¼Œç”¨äºå®å¯æ¢¦å¯¹æˆ˜ã€‚è¯¥ä»£ç†åˆ©ç”¨LLMsçš„é€šç”¨èƒ½åŠ›ï¼Œå¢å¼ºäº†æå°æå¤§æ ‘æœç´¢ï¼Œæ›¿ä»£äº†ç©å®¶åŠ¨ä½œé‡‡æ ·ã€å¯¹æ‰‹å»ºæ¨¡å’Œä»·å€¼å‡½æ•°ä¼°è®¡ç­‰å…³é”®æ¨¡å—ã€‚PokÃ©Champåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ¸¸æˆå†å²å’Œäººç±»çŸ¥è¯†ï¼Œå‡å°‘æœç´¢ç©ºé—´å¹¶è§£å†³éƒ¨åˆ†å¯è§‚æµ‹æ€§é—®é¢˜ã€‚ç»è¿‡è¯„ä¼°ï¼ŒPokÃ©Champåœ¨å®å¯æ¢¦å¯¹æˆ˜ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèµ¢å¾—äº†76%çš„èƒœç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ™ºèƒ½ä½“é—®é¢˜ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04130",
            "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
            "url": "https://huggingface.co/papers/2503.04130",
            "abstract": "Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5\\% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to 8times and the decoding latency by 2.4-2.9times for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm",
            "score": 3,
            "issue_id": 2580,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "f1d1afacd41dc0c7",
            "authors": [
                "Jindong Jiang",
                "Xiuyu Li",
                "Zhijian Liu",
                "Muyang Li",
                "Guo Chen",
                "Zhiqi Li",
                "De-An Huang",
                "Guilin Liu",
                "Zhiding Yu",
                "Kurt Keutzer",
                "Sungjin Ahn",
                "Jan Kautz",
                "Hongxu Yin",
                "Yao Lu",
                "Song Han",
                "Wonmin Byeon"
            ],
            "affiliations": [
                "KAIST",
                "MIT",
                "NVIDIA",
                "Nanjing University",
                "Rutgers University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04130.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#long_context",
                    "#architecture",
                    "#video",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "ğŸŒªï¸",
                "ru": {
                    "title": "STORM: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "STORM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mamba State Space Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ LLM. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. STORM Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ STORM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 5% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ 8 Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "STORM: Revolutionizing Video Understanding with Temporal Insights",
                    "desc": "This paper presents STORM, a new architecture designed to enhance video understanding in multimodal large language models (LLMs) by incorporating a temporal encoder. Unlike previous methods that treat video frames independently, STORM uses the Mamba State Space Model to capture the dynamics between frames, resulting in richer representations of video content. The architecture also implements token reduction strategies that significantly lower computational costs and improve processing speed without losing important temporal information. Evaluations demonstrate that STORM outperforms existing models on long video benchmarks while achieving substantial reductions in computation and latency."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šSTORMæ¨¡å‹",
                    "desc": "æœ€è¿‘ï¼ŒåŸºäºè§†é¢‘çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®¸å¤šç°æœ‰æ–¹æ³•åœ¨è§†è§‰éª¨å¹²ä¸­ç‹¬ç«‹å¤„ç†å¸§ï¼Œç¼ºä¹æ˜ç¡®çš„æ—¶é—´å»ºæ¨¡ï¼Œé™åˆ¶äº†æ•æ‰åŠ¨æ€æ¨¡å¼çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†STORMï¼ˆæ—¶ç©ºä»¤ç‰Œå‡å°‘æ¨¡å‹ï¼‰ï¼Œå®ƒåœ¨å›¾åƒç¼–ç å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ä¹‹é—´å¼•å…¥äº†ä¸“é—¨çš„æ—¶é—´ç¼–ç å™¨ã€‚æˆ‘ä»¬çš„æ—¶é—´ç¼–ç å™¨åˆ©ç”¨MambaçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå°†æ—¶é—´ä¿¡æ¯æ•´åˆåˆ°å›¾åƒä»¤ç‰Œä¸­ï¼Œç”Ÿæˆä¸°å¯Œçš„è¡¨ç¤ºï¼Œä¿ç•™æ•´ä¸ªè§†é¢‘åºåˆ—ä¸­çš„å¸§é—´åŠ¨æ€ã€‚é€šè¿‡è¿™äº›æŠ€æœ¯çš„æ•´åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—éœ€æ±‚å’Œæ¨ç†å»¶è¿Ÿï¼Œå®ç°äº†å¯¹é•¿è§†é¢‘çš„é«˜æ•ˆç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04725",
            "title": "L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling",
            "url": "https://huggingface.co/papers/2503.04725",
            "abstract": "We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L^2M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.",
            "score": 2,
            "issue_id": 2582,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "51e8f1332666da31",
            "authors": [
                "Zhuo Chen",
                "Oriol MaynÃ© i Comas",
                "Zhuotao Jin",
                "Di Luo",
                "Marin SoljaÄiÄ‡"
            ],
            "affiliations": [
                "Harvard University",
                "Massachusetts Institute of Technology",
                "NSF AI Institute for Artificial Intelligence and Fundamental Interactions",
                "Polytechnic University of Catalonia",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04725.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#math",
                    "#long_context"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ - ĞºĞ»ÑÑ‡ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰ĞµĞµÑÑ Ğ½Ğ° Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ L^2M Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµĞµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Unlocking Long-Range Dependencies in Language Models",
                    "desc": "This paper introduces a new scaling law for bipartite mutual information that is crucial for understanding long-range dependencies in natural language processing. Unlike traditional two-point mutual information, this scaling law operates independently and is essential for effective long-context language modeling. The authors propose the Long-context Language Modeling (L^2M) condition, which connects a model's ability to handle long contexts with the size of its latent state for retaining past information. Experimental validation on transformers and state space models supports the theoretical framework, paving the way for advancements in large language models with extended context lengths."
                },
                "zh": {
                    "title": "é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„æ–°æ³•åˆ™",
                    "desc": "æœ¬æ–‡ä¸¥è°¨åœ°å»ºç«‹äº†è‡ªç„¶è¯­è¨€ä¸­çš„åŒå‘äº’ä¿¡æ¯ç¼©æ”¾æ³•åˆ™ï¼Œè¯¥æ³•åˆ™æ§åˆ¶ç€é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ä¸€ç¼©æ”¾æ³•åˆ™ä¸ä¼ ç»Ÿçš„ä¸¤ç‚¹äº’ä¿¡æ¯ä¸åŒï¼Œå¹¶ä¸”ç‹¬ç«‹ç¼©æ”¾ï¼Œæ˜¯ç†è§£é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡çš„å…³é”®ã€‚é€šè¿‡è¿™ä¸€ç¼©æ”¾æ³•åˆ™ï¼Œæˆ‘ä»¬æå‡ºäº†é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡ï¼ˆL^2Mï¼‰æ¡ä»¶ï¼Œå°†æ¨¡å‹æœ‰æ•ˆå»ºæ¨¡é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„èƒ½åŠ›ä¸å…¶å­˜å‚¨è¿‡å»ä¿¡æ¯çš„æ½œåœ¨çŠ¶æ€å¤§å°çš„ç¼©æ”¾è”ç³»èµ·æ¥ã€‚æˆ‘ä»¬çš„ç»“æœé€šè¿‡å¯¹å˜æ¢å™¨å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹çš„å®éªŒå¾—åˆ°äº†éªŒè¯ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å‘æ›´é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„å‘å±•å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03983",
            "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities",
            "url": "https://huggingface.co/papers/2503.03983",
            "abstract": "Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/.",
            "score": 2,
            "issue_id": 2581,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "952e4cb72ec34df8",
            "authors": [
                "Sreyan Ghosh",
                "Zhifeng Kong",
                "Sonal Kumar",
                "S Sakshi",
                "Jaehyeon Kim",
                "Wei Ping",
                "Rafael Valle",
                "Dinesh Manocha",
                "Bryan Catanzaro"
            ],
            "affiliations": [
                "NVIDIA, Santa Clara, CA, USA",
                "University of Maryland, College Park, MD, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03983.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#long_context",
                    "#dataset",
                    "#small_models",
                    "#reasoning",
                    "#synthetic",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "AF2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "Audio Flamingo 2 (AF2) - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (ALM) Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ± Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CLAP, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Audio QA Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. AF2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 20 Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LongAudio Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ALM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾."
                },
                "en": {
                    "title": "Revolutionizing Audio Understanding with Audio Flamingo 2",
                    "desc": "This paper presents Audio Flamingo 2 (AF2), an advanced Audio-Language Model (ALM) designed to enhance audio understanding and reasoning. AF2 utilizes a custom CLAP model and synthetic Audio QA data to improve fine-grained audio reasoning, along with a multi-stage curriculum learning approach. The model achieves state-of-the-art results with a relatively small 3B parameter language model, outperforming larger models on over 20 benchmarks. Additionally, it introduces LongAudio, a new dataset for training on long audio segments, and demonstrates exceptional performance on the LongAudioBench for evaluating long audio understanding."
                },
                "zh": {
                    "title": "éŸ³é¢‘ç†è§£çš„æ–°çªç ´ï¼šAudio Flamingo 2",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Audio Flamingo 2ï¼ˆAF2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰å…ˆè¿›éŸ³é¢‘ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰ã€‚AF2åˆ©ç”¨äº†å®šåˆ¶çš„CLAPæ¨¡å‹ã€åˆæˆçš„éŸ³é¢‘é—®ç­”æ•°æ®ä»¥åŠå¤šé˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚AF2åœ¨ä»…ä½¿ç”¨ä¸€ä¸ª3Bå‚æ•°çš„å°å‹è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†20å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¤§å‹å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒAF2é¦–æ¬¡æ‰©å±•äº†å¯¹é•¿éŸ³é¢‘ç‰‡æ®µï¼ˆ30ç§’åˆ°5åˆ†é’Ÿï¼‰çš„ç†è§£ï¼Œå¹¶æå‡ºäº†LongAudioæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒALMåœ¨é•¿éŸ³é¢‘æ ‡æ³¨å’Œé—®ç­”ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02191",
            "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
            "url": "https://huggingface.co/papers/2503.02191",
            "abstract": "Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub.   To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants.   Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches.",
            "score": 1,
            "issue_id": 2581,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 4",
                "zh": "3æœˆ4æ—¥"
            },
            "hash": "1d0bc1069249c9e1",
            "authors": [
                "Mia Mohammad Imran",
                "Robert Zita",
                "Rebekah Copeland",
                "Preetha Chatterjee",
                "Rahat Rizvi Rahman",
                "Kostadin Damevski"
            ],
            "affiliations": [
                "Drexel University, Philadelphia, PA, USA",
                "Eastern Mennonite University, Harrisonburg, VA, USA",
                "Elmhurst University, Elmhurst, IL, USA",
                "Missouri University of Science and Technology, Rolla, MO, USA",
                "Virginia Commonwealth University, Richmond, VA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02191.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#dataset",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ‚Ğ¼Ğ¾ÑÑ„ĞµÑ€Ñ‹ Ğ² open-source ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ğ½Ğ° GitHub. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 202 Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 696 Ğ½ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ², Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹ Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´ĞµÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 69% F1-Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Proactive Moderation: Detecting Toxicity Before It Escalates",
                    "desc": "This paper investigates how toxic language in GitHub conversations can deter contributors and newcomers. It introduces a dataset of 202 toxic conversations with marked derailment points, alongside 696 non-toxic examples for comparison. The study identifies specific linguistic features and conversational dynamics that signal potential toxicity. Using large language models (LLMs), the authors propose a proactive moderation strategy that summarizes conversation trajectories to detect early signs of derailment, achieving a 69% F1-Score in predictions."
                },
                "zh": {
                    "title": "ä¸»åŠ¨ç®¡ç†ï¼Œé˜²æ­¢å¯¹è¯åç¦»ä¸æœ‰æ¯’è¯­è¨€",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åœ¨GitHubä¸Šé¢„æµ‹å’Œç†è§£å¯¹è¯åç¦»å¯¼è‡´çš„æœ‰æ¯’è¯­è¨€ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…å«202ä¸ªæœ‰æ¯’å¯¹è¯å’Œ696ä¸ªéæœ‰æ¯’å¯¹è¯ï¼Œå¹¶æ ‡æ³¨äº†åç¦»ç‚¹ã€‚é€šè¿‡åˆ†æè¿™äº›å¯¹è¯çš„è¯­è¨€ç‰¹å¾å’ŒåŠ¨æ€æ¨¡å¼ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæœ‰æ¯’å¯¹è¯çš„ç‹¬ç‰¹ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸»åŠ¨çš„ç®¡ç†ç­–ç•¥ï¼Œåˆ©ç”¨ç°ä»£å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ£€æµ‹å’Œå¤„ç†æ½œåœ¨çš„æœ‰å®³å¯¹è¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20258",
            "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
            "url": "https://huggingface.co/papers/2502.20258",
            "abstract": "As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the \"broken telephone\" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.",
            "score": 1,
            "issue_id": 2580,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "5b26055854ae3d90",
            "authors": [
                "Amr Mohamed",
                "Mingmeng Geng",
                "Michalis Vazirgiannis",
                "Guokan Shang"
            ],
            "affiliations": [
                "Ecole Polytechnique",
                "MBZUAI",
                "SISSA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20258.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#data",
                    "#long_context",
                    "#alignment",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚ 'Ğ¸ÑĞ¿Ğ¾Ñ€Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ°' Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ±ĞµĞ¶Ğ½Ğ°, Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ LLM Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…."
                },
                "en": {
                    "title": "Mitigating Distortion in Iterative LLM Outputs",
                    "desc": "This paper explores how large language models (LLMs) can distort information when they repeatedly process their own outputs, similar to the 'broken telephone' effect seen in human communication. The researchers conducted translation-based experiments to observe how information degradation occurs over time, which is affected by the choice of language and the complexity of the generation chain. They found that while some level of distortion is unavoidable, it can be reduced by using specific prompting strategies. These insights highlight the potential risks of relying on LLMs for generating content in iterative processes, raising concerns about the accuracy of AI-generated information."
                },
                "zh": {
                    "title": "æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯æ‰­æ›²ä¸å¯é æ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åå¤å¤„ç†è‡ªèº«è¾“å‡ºæ—¶æ˜¯å¦ä¼šæ‰­æ›²ä¿¡æ¯ï¼Œç±»ä¼¼äºäººç±»æ²Ÿé€šä¸­çš„â€œç ´ç”µè¯â€æ•ˆåº”ã€‚é€šè¿‡åŸºäºç¿»è¯‘çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°ä¿¡æ¯æ‰­æ›²ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œç´¯ç§¯ï¼Œå—è¯­è¨€é€‰æ‹©å’Œé“¾æ¡å¤æ‚æ€§çš„å½±å“ã€‚å°½ç®¡ä¿¡æ¯é€€åŒ–æ˜¯ä¸å¯é¿å…çš„ï¼Œä½†é€šè¿‡æˆ˜ç•¥æ€§æç¤ºæŠ€æœ¯å¯ä»¥å‡è½»è¿™ç§å½±å“ã€‚ç ”ç©¶ç»“æœä¸ºAIä»‹å¯¼çš„ä¿¡æ¯ä¼ æ’­çš„é•¿æœŸå½±å“æä¾›äº†é‡è¦è§è§£ï¼Œæå‡ºäº†å…³äºLLMç”Ÿæˆå†…å®¹åœ¨è¿­ä»£å·¥ä½œæµç¨‹ä¸­å¯é æ€§çš„é‡è¦é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04606",
            "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
            "url": "https://huggingface.co/papers/2503.04606",
            "abstract": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a sim14,000times compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
            "score": 1,
            "issue_id": 2580,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "c635690f3edc1186",
            "authors": [
                "Aoxiong Yin",
                "Kai Shen",
                "Yichong Leng",
                "Xu Tan",
                "Xinyu Zhou",
                "Juncheng Li",
                "Siliang Tang"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University, Hangzhou, China",
                "Moonshot AI, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04606.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#benchmark",
                    "#long_context",
                    "#architecture",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "LanDiff: Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LanDiff - Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LanDiff Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ LanDiff Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VBench T2V. Ğ¢Ğ°ĞºĞ¶Ğµ LanDiff Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "LanDiff: Bridging Language and Visuals for Superior Video Generation",
                    "desc": "This paper introduces LanDiff, a novel framework for text-to-video (T2V) generation that combines the strengths of autoregressive language models and diffusion models. It addresses the limitations of each approach by using a coarse-to-fine generation strategy, which enhances both visual quality and semantic understanding. Key innovations include a semantic tokenizer for efficient compression of visual features, a language model that generates meaningful semantic tokens, and a streaming diffusion model that refines these tokens into high-quality videos. The results demonstrate that LanDiff outperforms existing state-of-the-art models in T2V tasks, particularly in generating long videos."
                },
                "zh": {
                    "title": "LanDiffï¼šæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLanDiffçš„æ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆè‡ªå›å½’è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œä»¥å®ç°æ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ç²—åˆ°ç»†çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå…‹æœäº†å„è‡ªçš„å±€é™æ€§ï¼Œæå‡äº†è§†è§‰è´¨é‡å’Œè¯­ä¹‰ç†è§£ã€‚LanDiffå¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼ŒåŒ…æ‹¬é«˜æ•ˆçš„è¯­ä¹‰å‹ç¼©æŠ€æœ¯ã€ç”Ÿæˆé«˜å±‚è¯­ä¹‰å…³ç³»çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠå°†ç²—ç•¥è¯­ä¹‰ç²¾ç‚¼ä¸ºé«˜ä¿çœŸè§†é¢‘çš„æµå¼æ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLanDiffåœ¨VBench T2VåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04378",
            "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks",
            "url": "https://huggingface.co/papers/2503.04378",
            "abstract": "Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.",
            "score": 1,
            "issue_id": 2578,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "926e56aa51a0fefe",
            "authors": [
                "Zhilin Wang",
                "Jiaqi Zeng",
                "Olivier Delalleau",
                "Daniel Egert",
                "Ellie Evans",
                "Hoo-Chang Shin",
                "Felipe Soares",
                "Yi Dong",
                "Oleksii Kuchaiev"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04378.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#inference",
                    "#reasoning",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¾Ñ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ° Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ğ´Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, Ğ° Ñ‚Ñ€ĞµÑ‚ÑŒÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Arena Hard. ĞŸÑ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Llama 3 Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 70B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ OpenAI o1 Ğ¸ DeepSeek R1."
                },
                "en": {
                    "title": "Enhancing Open-Ended Task Performance through Feedback and Editing",
                    "desc": "This paper discusses a novel approach to improve inference-time scaling in machine learning models, particularly for open-ended tasks. It introduces a system where one model generates an initial response, a second model provides feedback, and a third model edits the response based on that feedback. This method allows for more flexible applications beyond traditional tasks that require verifiable answers, such as math or coding. The authors demonstrate that by optimizing the number of drafts, feedback, and edits, their model achieves state-of-the-art performance on the Arena Hard benchmark, outperforming previous models."
                },
                "zh": {
                    "title": "æ¨ç†æ—¶é—´æ‰©å±•ï¼šæå‡å¼€æ”¾æ€§ä»»åŠ¡çš„æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ¨ç†æ—¶é—´æ‰©å±•åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯OpenAI o1å’ŒDeepSeek R1ç­‰æ¨¡å‹ã€‚è®¸å¤šç°æœ‰æŠ€æœ¯è¦æ±‚ä»»åŠ¡çš„ç­”æ¡ˆå¯éªŒè¯ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å€Ÿé‰´äººç±»å¦‚ä½•è¿›è¡Œåˆæ­¥å°è¯•ã€è¯·æ±‚åé¦ˆå¹¶æ ¹æ®åé¦ˆè¿›è¡Œæ”¹è¿›çš„è¿‡ç¨‹ï¼Œå¼€å‘äº†ä¸“é—¨çš„åé¦ˆå’Œç¼–è¾‘æ¨¡å‹ã€‚é€šè¿‡ä¼˜åŒ–åˆå§‹å“åº”è‰ç¨¿ã€æœ‰æ•ˆåé¦ˆå’Œç¼–è¾‘å“åº”çš„æ•°é‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨Arena HardåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†92.7çš„æœ€æ–°æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-06.html",
    "link_next": "2025-03-10.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "06.03",
        "en": "03/06",
        "zh": "3æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "10.03",
        "en": "03/10",
        "zh": "3æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 8,
        "#agents": 2,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 7,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼Œä½†å¼€æºçš„å¤šè¯­è¨€LLMsä»ç„¶ç¨€ç¼ºï¼Œç°æœ‰æ¨¡å‹é€šå¸¸è¯­è¨€è¦†ç›–æœ‰é™ã€‚è¿™äº›æ¨¡å‹é€šå¸¸ä¼˜å…ˆè€ƒè™‘èµ„æºä¸°å¯Œçš„è¯­è¨€ï¼Œè€Œå¿½ç•¥äº†å¹¿æ³›ä½¿ç”¨ä½†èµ„æºåŒ®ä¹çš„è¯­è¨€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬ä»‹ç»äº†Babelï¼Œä¸€ä¸ªå¼€æ”¾çš„å¤šè¯­è¨€LLMï¼Œæ¶µç›–äº†æŒ‰ä½¿ç”¨äººæ•°æ’åå‰25çš„è¯­è¨€ï¼Œæ”¯æŒè¶…è¿‡90%çš„å…¨çƒäººå£ï¼Œå¹¶åŒ…æ‹¬è®¸å¤šå…¶ä»–å¼€æ”¾å¤šè¯­è¨€LLMså¿½ç•¥çš„è¯­è¨€ã€‚ä¸ä¼ ç»Ÿçš„ç»§ç»­é¢„è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒBabelé€šè¿‡ä¸€ç§å±‚æ‰©å±•æŠ€æœ¯å¢åŠ å…¶å‚æ•°æ•°é‡ï¼Œä»è€Œæé«˜äº†Babelçš„æ€§èƒ½ä¸Šé™ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå˜ä½“ï¼šBabel-9Bï¼Œç”¨äºé«˜æ•ˆæ¨ç†å’Œå¾®è°ƒï¼Œä»¥åŠBabel-83Bï¼Œä¸ºå¼€æ”¾å¤šè¯­è¨€LLMsè®¾å®šäº†æ–°æ ‡å‡†ã€‚å¹¿æ³›çš„å¤šè¯­è¨€ä»»åŠ¡è¯„ä¼°è¯æ˜äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¼€æºç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼ŒBabelå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼ŒBabel-9B-Chatåœ¨10Bå¤§å°çš„LLMsä¸­é¢†å…ˆï¼ŒBabel-83B-Chatåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è®¾å®šäº†æ–°æ ‡å‡†ï¼Œè¾¾åˆ°äº†å•†ä¸šæ¨¡å‹çš„æ°´å¹³ã€‚",
        "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
        "pinyin": "DÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) chÃ¨dÇ gÇibiÃ nle zÃ¬rÃ¡n yÇ”yÃ¡n chÇ”lÇ (NLP), dÃ n kÄiyuÃ¡n de duÅyÇ”yÃ¡n LLMs rÃ©ngrÃ¡n xÄ«quÄ“, xiÃ nyÇ’u mÃ³xÃ­ng tÅngchÃ¡ng yÇ”yÃ¡n fÃºgÃ i yÇ’uxiÃ n. ZhÃ¨xiÄ“ mÃ³xÃ­ng tÅngchÃ¡ng yÅuxiÄn kÇolÇœ zÄ«yuÃ¡n fÄ“ngfÃ¹ de yÇ”yÃ¡n, Ã©r hÅ«lÃ¼e le guÇngfÃ n shÇyÃ²ng dÃ n zÄ«yuÃ¡n kuÃ¬fÃ¡ de yÇ”yÃ¡n. WÃ¨ile jiÄ›juÃ© zhÃ¨ yÄ« chÄjÃ¹, wÇ’men jiÃ¨shÃ o le Babel, yÄ«gÃ¨ kÄifÃ ng de duÅyÇ”yÃ¡n LLM, hÃ nhuÃ²le Ã n shÇyÃ²ng rÃ©nshÃ¹ pÃ¡imÃ­ng qiÃ¡n 25 de yÇ”yÃ¡n, zhÄ«chÃ­ chÄoguÃ² 90% de quÃ¡nqiÃº rÃ©nkÇ’u, bÃ¬ng bÄokuÃ² xÇ”duÅ qÃ­tÄ kÄifÃ ng duÅyÇ”yÃ¡n LLMs hÅ«lÃ¼e de yÇ”yÃ¡n. YÇ” chuÃ¡ntÇ’ng de jÃ¬xÃ¹ yÃ¹xÃ¹n fÄngfÇ bÃ¹tÃ³ng, Babel tÅngguÃ² yÄ«zhÇ’ng cÃ©ng kuÃ²zhÇn jÃ¬shÃ¹ zÄ“ngjiÄ qÃ­ cÄnshÃ¹ shÃ¹liÃ ng, dÃ ngrÃ¡n tÃ­gÄole Babel de xÃ¬ngnÃ©ng shÃ ngxiÃ n. WÇ’men yÇnrÃ¹le liÇnggÃ¨ biÃ ntÇ: Babel-9B, yÃ²ngyÃº gÄoxiÃ o tuÄ«lÇ hÃ© wÄ“itiÃ¡o, yÇjiÇ Babel-83B, wÃ¨i kÄifÃ ng duÅyÇ”yÃ¡n LLMs shÃ¨dÃ¬ngle xÄ«n biÄozhÇ”n. GuÇngfÃ n de duÅyÇ”yÃ¡n rÃ¨nwÃ¹ pÃ­nggÅ« zhÃ¨ngmÃ­ngle qÃ­ yÅubiÃ¨ de xÃ¬ngnÃ©ng. CÇwÃ i, shÇyÃ²ng kÄiyuÃ¡n jiÃ nshÇ wÄ“itiÃ¡o shÃ¹jÃºjÃ­, Babel quÃ¨dÃ©le xiÇnzhÃ¹ de xÃ¬ngnÃ©ng, Babel-9B-Chat zÃ i 10B dÃ xÃ¬ng de LLMs zhÅng lÇngxiÄn, Babel-83B-Chat zÃ i duÅyÇ”yÃ¡n rÃ¨nwÃ¹ zhÅng shÃ¨dÃ¬ngle xÄ«n biÄozhÇ”n, dÃ¡le shÄngyÃ¨ mÃ³xÃ­ng de shuÇpÃ­ng.",
        "vocab": "[\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"å½»åº•\", \"pinyin\": \"chÃ¨dÇ\", \"trans\": \"thoroughly\"},\n    {\"word\": \"è‡ªç„¶è¯­è¨€å¤„ç†\", \"pinyin\": \"zÃ¬rÃ¡n yÇ”yÃ¡n chÇ”lÇ\", \"trans\": \"Natural Language Processing\"},\n    {\"word\": \"ç¨€ç¼º\", \"pinyin\": \"xÄ«quÄ“\", \"trans\": \"scarce\"},\n    {\"word\": \"è¦†ç›–\", \"pinyin\": \"fÃ¹gÃ i\", \"trans\": \"cover\"},\n    {\"word\": \"æœ‰é™\", \"pinyin\": \"yÇ’uxiÃ n\", \"trans\": \"limited\"},\n    {\"word\": \"ä¼˜å…ˆ\", \"pinyin\": \"yÅuxiÄn\", \"trans\": \"prioritize\"},\n    {\"word\": \"èµ„æº\", \"pinyin\": \"zÄ«yuÃ¡n\", \"trans\": \"resources\"},\n    {\"word\": \"ä¸°å¯Œ\", \"pinyin\": \"fÄ“ngfÃ¹\", \"trans\": \"abundant\"},\n    {\"word\": \"åŒ®ä¹\", \"pinyin\": \"kuÃ¬fÃ¡\", \"trans\": \"scarce\"},\n    {\"word\": \"å·®è·\", \"pinyin\": \"chÄjÃ¹\", \"trans\": \"gap\"},\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"æ¶µç›–\", \"pinyin\": \"hÃ¡ngÃ i\", \"trans\": \"cover\"},\n    {\"word\": \"æŒ‰\", \"pinyin\": \"Ã n\", \"trans\": \"according to\"},\n    {\"word\": \"æ’å\", \"pinyin\": \"pÃ¡imÃ­ng\", \"trans\": \"ranking\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ«chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"å…¨çƒ\", \"pinyin\": \"quÃ¡nqiÃº\", \"trans\": \"global\"},\n    {\"word\": \"äººå£\", \"pinyin\": \"rÃ©nkÇ’u\", \"trans\": \"population\"},\n    {\"word\": \"ç»§ç»­\", \"pinyin\": \"jÃ¬xÃ¹\", \"trans\": \"continue\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹nliÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"å±‚\", \"pinyin\": \"cÃ©ng\", \"trans\": \"layer\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ²zhÇn\", \"trans\": \"expand\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬shÃ¹\", \"trans\": \"technology\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄnshÇ”\", \"trans\": \"parameters\"},\n    {\"word\": \"æ•°é‡\", \"pinyin\": \"shÃ¹liÃ ng\", \"trans\": \"quantity\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ngnÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ä¸Šé™\", \"pinyin\": \"shÃ ngxiÃ n\", \"trans\": \"upper limit\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇnrÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"å˜ä½“\", \"pinyin\": \"biÃ ntÇ\", \"trans\": \"variants\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄoxiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“itiÃ¡o\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"è®¾å®š\", \"pinyin\": \"shÃ¨dÃ¬ng\", \"trans\": \"set\"},\n    {\"word\": \"æ ‡å‡†\", \"pinyin\": \"biÄozhÇ”n\", \"trans\": \"standard\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"è¯æ˜\", \"pinyin\": \"zhÃ¨ngmÃ­ng\", \"trans\": \"prove\"},\n    {\"word\": \"ä¼˜è¶Š\", \"pinyin\": \"yÅuyuÃ¨\", \"trans\": \"superior\"},\n    {\"word\": \"ç›‘ç£\", \"pinyin\": \"jiÃ ndÅ«\", \"trans\": \"supervised\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇnzhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"é¢†å…ˆ\", \"pinyin\": \"lÇngxiÄn\", \"trans\": \"lead\"},\n    {\"word\": \"å•†ä¸š\", \"pinyin\": \"shÄngyÃ¨\", \"trans\": \"commercial\"},\n    {\"word\": \"æ°´å¹³\", \"pinyin\": \"shuÇpÃ­ng\", \"trans\": \"level\"}\n]",
        "trans": "Large language models (LLMs) have revolutionized natural language processing (NLP), but open-source multilingual LLMs remain scarce, with existing models often having limited language coverage. These models typically prioritize resource-rich languages while neglecting widely used but resource-scarce languages. To address this gap, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supporting over 90% of the global population and including many languages overlooked by other open multilingual LLMs. Unlike traditional continued pre-training methods, Babel enhances its parameter count through a layer expansion technique, raising Babel's performance ceiling. We introduce two variants: Babel-9B for efficient inference and fine-tuning, and Babel-83B, setting a new standard for open multilingual LLMs. Extensive multilingual task evaluations demonstrate its superior performance. Additionally, using open-source supervised fine-tuning datasets, Babel achieves significant performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting new standards in multilingual tasks, reaching the level of commercial models.",
        "update_ts": "2025-03-06 09:11"
    }
}