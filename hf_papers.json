{
    "date": {
        "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 18",
        "zh": "4æœˆ18æ—¥"
    },
    "time_utc": "2025-04-18 04:13",
    "weekday": 4,
    "issue_id": 3305,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.13146",
            "title": "Antidistillation Sampling",
            "url": "https://huggingface.co/papers/2504.13146",
            "abstract": "Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.",
            "score": 24,
            "issue_id": 3304,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "6aa117b5c441eb3d",
            "authors": [
                "Yash Savani",
                "Asher Trockman",
                "Zhili Feng",
                "Avi Schwarzschild",
                "Alexander Robey",
                "Marc Finzi",
                "J. Zico Kolter"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13146.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ñ‚Ğ¸Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ñ‚Ğ¸Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ½ĞµĞµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ°Ñ Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ."
                },
                "en": {
                    "title": "Protecting Model Knowledge with Antidistillation Sampling",
                    "desc": "This paper discusses a method called antidistillation sampling, which aims to protect advanced models from being easily distilled into simpler versions. Distillation is a process where a complex model's knowledge is transferred to a smaller model, but this can be exploited if the complex model generates detailed reasoning traces. Antidistillation sampling works by altering the probability distribution of the next token generated by the model, making the reasoning less useful for distillation. This approach allows the model to maintain its performance while reducing the risk of its knowledge being easily extracted."
                },
                "zh": {
                    "title": "æŠ—è’¸é¦é‡‡æ ·ï¼šä¿æŠ¤æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°ç­–ç•¥",
                    "desc": "å‰æ²¿æ¨¡å‹ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ä¼šäº§ç”Ÿä¸°å¯Œçš„æ ‡è®°åºåˆ—ï¼Œè¿™äº›åºåˆ—å¯ä»¥å¸®åŠ©æ¨¡å‹è’¸é¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æ¼æ´ï¼Œæ¨¡å‹æ‹¥æœ‰è€…å¯èƒ½ä¼šå¯»æ‰¾é‡‡æ ·ç­–ç•¥ï¼Œä»¥é™åˆ¶è’¸é¦çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¸å½±å“æ¨¡å‹æ€§èƒ½ã€‚æŠ—è’¸é¦é‡‡æ ·æ­£æ˜¯æä¾›è¿™ç§èƒ½åŠ›çš„ç­–ç•¥ã€‚é€šè¿‡æˆ˜ç•¥æ€§åœ°ä¿®æ”¹æ¨¡å‹çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¦‚ç‡åˆ†å¸ƒï¼ŒæŠ—è’¸é¦é‡‡æ ·å¯ä»¥ç ´åæ¨ç†è½¨è¿¹ï¼Œä½¿å…¶åœ¨è’¸é¦ä¸­å˜å¾—ä¸é‚£ä¹ˆæœ‰æ•ˆï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®é™…æ•ˆç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13122",
            "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
            "url": "https://huggingface.co/papers/2504.13122",
            "abstract": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) Instance Level, aligning overall video content with responses; ii) Temporal Level, aligning video temporal semantics with event descriptions; and iii) Perceptive Level, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination. The code and data are available at https://github.com/HaroldChen19/VistaDPO.",
            "score": 9,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "efbc3b240498ce70",
            "authors": [
                "Haojian Huang",
                "Haodong Chen",
                "Shengqiong Wu",
                "Meng Luo",
                "Jinlan Fu",
                "Xinya Du",
                "Hanwang Zhang",
                "Hao Fei"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13122.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#alignment",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VistaDPO: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…",
                    "desc": "VistaDPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…: Ğ¾Ğ±Ñ‰ĞµĞ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VistaDPO-7k Ñ 7200 Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ€Ğ³Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ¼ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VistaDPO Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Aligning Video and Language: Introducing VistaDPO",
                    "desc": "This paper presents VistaDPO, a new framework designed to improve the alignment between video content and human language understanding in Large Video Models (LVMs). It operates on three hierarchical levels: aligning overall video content with user responses, matching temporal semantics of videos with event descriptions, and correlating spatial objects with language tokens. To support this framework, the authors created a dataset called VistaDPO-7k, which includes 7.2K question-answer pairs with detailed annotations for better preference alignment. The results show that VistaDPO enhances the performance of LVMs by reducing issues related to video hallucination and misalignment with human intuition."
                },
                "zh": {
                    "title": "VistaDPOï¼šæå‡è§†é¢‘ç†è§£çš„åå¥½å¯¹é½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ¡†æ¶VistaDPOï¼Œç”¨äºè§†é¢‘å±‚æ¬¡ç©ºé—´-æ—¶é—´ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è§†é¢‘æ¨¡å‹ï¼ˆLVMsï¼‰åœ¨è§†é¢‘ç†è§£ä¸­çš„äººç±»ç›´è§‰ä¸ä¸€è‡´å’Œè§†é¢‘å¹»è§‰é—®é¢˜ã€‚VistaDPOé€šè¿‡ä¸‰ä¸ªå±‚æ¬¡å¢å¼ºæ–‡æœ¬-è§†é¢‘åå¥½å¯¹é½ï¼šå®ä¾‹å±‚ã€æ—¶é—´å±‚å’Œæ„ŸçŸ¥å±‚ï¼Œåˆ†åˆ«å¯¹é½è§†é¢‘å†…å®¹ã€æ—¶é—´è¯­ä¹‰å’Œç©ºé—´å¯¹è±¡ã€‚ä¸ºäº†æ”¯æŒç»†ç²’åº¦è§†é¢‘-è¯­è¨€åå¥½å¯¹é½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†VistaDPO-7kæ•°æ®é›†ï¼ŒåŒ…å«7200ä¸ªé—®ç­”å¯¹åŠå…¶ç©ºé—´-æ—¶é—´ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVistaDPOæ˜¾è‘—æå‡äº†ç°æœ‰LVMsçš„æ€§èƒ½ï¼Œæœ‰æ•ˆå‡è½»äº†è§†é¢‘-è¯­è¨€çš„ä¸ä¸€è‡´æ€§å’Œå¹»è§‰ç°è±¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13169",
            "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
            "url": "https://huggingface.co/papers/2504.13169",
            "abstract": "Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io.",
            "score": 8,
            "issue_id": 3304,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "f1ebb64cfce24e47",
            "authors": [
                "Tsung-Han Wu",
                "Heekyung Lee",
                "Jiaxin Ge",
                "Joseph E. Gonzalez",
                "Trevor Darrell",
                "David M. Chan"
            ],
            "affiliations": [
                "POSTECH",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13169.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#hallucinations",
                    "#data",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "REVERSE: ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ VLM Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ REVERSE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. REVERSE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 1,3 Ğ¼Ğ»Ğ½ Ğ¿Ğ¾Ğ»ÑƒÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ REVERSE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 12-28% Ğ¿Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "REVERSE: Correcting Visual Hallucinations in VLMs Dynamically",
                    "desc": "This paper addresses the issue of visual hallucinations in Vision-Language Models (VLMs), where models incorrectly describe non-existent elements. The authors propose a new framework called REVERSE, which combines hallucination-aware training with real-time self-verification to improve the accuracy of VLM outputs. By utilizing a large dataset of semi-synthetic samples, REVERSE can identify and correct hallucinations during the generation process. The results demonstrate that this approach significantly reduces hallucinations, outperforming existing methods in various benchmarks."
                },
                "zh": {
                    "title": "REVERSEï¼šåŠ¨æ€ä¿®æ­£è§†è§‰å¹»è§‰çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å‡ºç°è§†è§‰å¹»è§‰ï¼Œå³ç”Ÿæˆä¸å­˜åœ¨çš„ç‰©ä½“ã€åŠ¨ä½œæˆ–æ¦‚å¿µçš„æè¿°ï¼Œè¿™åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­å¸¦æ¥äº†é‡å¤§é£é™©ã€‚ç°æœ‰çš„å¹»è§‰ç¼“è§£æ–¹æ³•é€šå¸¸åˆ†ä¸ºä¸¤ç±»ï¼šç”Ÿæˆè°ƒæ•´å’ŒåæœŸéªŒè¯ã€‚ç”Ÿæˆè°ƒæ•´æ–¹æ³•ä¾èµ–å¯å‘å¼è§„åˆ™ï¼Œç¼ºä¹æœ‰æ•ˆçš„ä¿®æ­£æœºåˆ¶ï¼Œè€ŒåæœŸéªŒè¯åˆ™å¤æ‚ï¼Œé€šå¸¸éœ€è¦å¤šä¸ªæ¨¡å‹ï¼Œå¹¶å€¾å‘äºæ‹’ç»è¾“å‡ºè€Œä¸æ˜¯è¿›è¡Œä¿®æ­£ã€‚æˆ‘ä»¬æå‡ºçš„REVERSEæ¡†æ¶ç»“åˆäº†å¹»è§‰æ„ŸçŸ¥è®­ç»ƒå’Œå®æ—¶è‡ªæˆ‘éªŒè¯ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ£€æµ‹å¹»è§‰å¹¶åŠ¨æ€ä¿®æ­£ï¼Œä»è€Œæ˜¾è‘—é™ä½å¹»è§‰çš„å‘ç”Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12626",
            "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
            "url": "https://huggingface.co/papers/2504.12626",
            "abstract": "We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. The FramePack compresses input frames to make the transformer context length a fixed number regardless of the video length. As a result, we are able to process a large number of frames using video diffusion with computation bottleneck similar to image diffusion. This also makes the training video batch sizes significantly higher (batch sizes become comparable to image diffusion training). We also propose an anti-drifting sampling method that generates frames in inverted temporal order with early-established endpoints to avoid exposure bias (error accumulation over iterations). Finally, we show that existing video diffusion models can be finetuned with FramePack, and their visual quality may be improved because the next-frame prediction supports more balanced diffusion schedulers with less extreme flow shift timesteps.",
            "score": 8,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "fd1688a4e26dbb32",
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12626.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "FramePack: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "FramePack - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. FramePack Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾."
                },
                "en": {
                    "title": "FramePack: Efficient Video Generation with Next-Frame Prediction",
                    "desc": "The paper introduces FramePack, a novel neural network architecture designed for predicting the next frame in video generation. By compressing input frames, FramePack ensures that the transformer can handle a fixed context length, making it efficient for processing long videos. This approach allows for larger training batch sizes, similar to those used in image diffusion, while maintaining computational efficiency. Additionally, the authors present an anti-drifting sampling method to mitigate exposure bias, enhancing the quality of generated frames and improving existing video diffusion models through fine-tuning."
                },
                "zh": {
                    "title": "FramePackï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ä¸‹ä¸€å¸§é¢„æµ‹èƒ½åŠ›",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¥ç»ç½‘ç»œç»“æ„ï¼ŒFramePackï¼Œç”¨äºè®­ç»ƒè§†é¢‘ç”Ÿæˆçš„ä¸‹ä¸€å¸§é¢„æµ‹æ¨¡å‹ã€‚FramePacké€šè¿‡å‹ç¼©è¾“å…¥å¸§ï¼Œä½¿å¾—å˜æ¢å™¨çš„ä¸Šä¸‹æ–‡é•¿åº¦å›ºå®šï¼Œæ— è®ºè§†é¢‘é•¿åº¦å¦‚ä½•ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨ä¸å›¾åƒæ‰©æ•£ç›¸ä¼¼çš„è®¡ç®—ç“¶é¢ˆå¤„ç†å¤§é‡å¸§ï¼Œä»è€Œæ˜¾è‘—æé«˜è§†é¢‘è®­ç»ƒçš„æ‰¹é‡å¤§å°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åæ¼‚ç§»é‡‡æ ·æ–¹æ³•ï¼Œä»¥é¿å…è¿­ä»£è¿‡ç¨‹ä¸­çš„æ›å…‰åå·®ï¼Œä»è€Œæé«˜ç”Ÿæˆå¸§çš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12369",
            "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
            "url": "https://huggingface.co/papers/2504.12369",
            "abstract": "World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing a memory attention mechanism that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.",
            "score": 6,
            "issue_id": 3303,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "79cf162a1b60f887",
            "authors": [
                "Zeqi Xiao",
                "Yushi Lan",
                "Yifan Zhou",
                "Wenqi Ouyang",
                "Shuai Yang",
                "Yanhong Zeng",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12369.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#long_context"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "WorldMem: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
                    "desc": "WorldMem - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ°Ğ½ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ¸Ğ»Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚ĞºĞ°Ñ…. Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€, Ğ½Ğ¾ Ğ¸ ĞµĞ³Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Enhancing World Simulation with Memory-Driven Consistency",
                    "desc": "This paper introduces WorldMem, a novel framework designed to improve world simulation by utilizing a memory bank that stores various memory frames and states. The framework addresses the challenge of maintaining long-term consistency in 3D spatial representations, which is often hindered by limited temporal context. By implementing a memory attention mechanism, WorldMem can retrieve relevant information from stored memory frames, allowing for accurate scene reconstruction despite changes in viewpoint or time. Additionally, the integration of timestamps enables the framework to model both static and dynamic aspects of the environment, enhancing interaction and perception in simulated worlds."
                },
                "zh": {
                    "title": "å¢å¼ºä¸–ç•Œæ¨¡æ‹Ÿçš„ä¸€è‡´æ€§ä¸åŠ¨æ€æ€§",
                    "desc": "ä¸–ç•Œæ¨¡æ‹Ÿå› å…¶å»ºæ¨¡è™šæ‹Ÿç¯å¢ƒå’Œé¢„æµ‹è¡Œä¸ºåæœçš„èƒ½åŠ›è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œæœ‰é™çš„æ—¶é—´ä¸Šä¸‹æ–‡çª—å£å¸¸å¸¸å¯¼è‡´é•¿æœŸä¸€è‡´æ€§ç»´æŠ¤çš„å¤±è´¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒä¸‰ç»´ç©ºé—´ä¸€è‡´æ€§æ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WorldMemæ¡†æ¶ï¼Œé€šè¿‡ä¸€ä¸ªåŒ…å«è®°å¿†å•å…ƒçš„è®°å¿†åº“æ¥å¢å¼ºåœºæ™¯ç”Ÿæˆï¼Œè¿™äº›è®°å¿†å•å…ƒå­˜å‚¨è®°å¿†å¸§å’ŒçŠ¶æ€ï¼ˆä¾‹å¦‚ï¼Œå§¿åŠ¿å’Œæ—¶é—´æˆ³ï¼‰ã€‚é€šè¿‡é‡‡ç”¨è®°å¿†æ³¨æ„æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®é‡å»ºå…ˆå‰è§‚å¯Ÿåˆ°çš„åœºæ™¯ï¼Œå³ä½¿åœ¨æ˜¾è‘—çš„è§†è§’æˆ–æ—¶é—´é—´éš”ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆæå–ç›¸å…³ä¿¡æ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05506",
            "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
            "url": "https://huggingface.co/papers/2504.05506",
            "abstract": "Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
            "score": 4,
            "issue_id": 3303,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "a727d08eac22e920",
            "authors": [
                "Ahmed Masry",
                "Mohammed Saidul Islam",
                "Mahir Ahmed",
                "Aayush Bajaj",
                "Firoz Kabir",
                "Aaryaman Kartha",
                "Md Tahmid Rahman Laskar",
                "Mizanur Rahman",
                "Shadikur Rahman",
                "Mehrad Shahmohammadi",
                "Megh Thakkar",
                "Md Rizwan Parvez",
                "Enamul Hoque",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Dialpad Inc., Canada",
                "MILA - Quebec AI Institute, Canada",
                "Nanyang Technological University, Singapore",
                "Qatar Computing Research Institute (QCRI)",
                "RBC, Canada",
                "Salesforce Research, USA",
                "York University, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05506.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ChartQAPro: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²",
                    "desc": "ChartQAPro - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1300 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 2000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ChartQAPro Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ñ… Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "ChartQAPro: Elevating Chart Understanding for AI",
                    "desc": "This paper introduces ChartQAPro, a new benchmark designed to improve Chart Question Answering (CQA) systems by providing a more diverse and realistic set of charts and questions. Unlike previous benchmarks, ChartQAPro includes 1,341 charts from 157 sources and features various question types, which better reflect the complexities of real-world data analysis. The study reveals that modern large vision-language models (LVLMs) struggle significantly with this new benchmark, demonstrating a performance drop from 90.5% on the previous ChartQA to only 55.81% on ChartQAPro. Through error analyses and ablation studies, the authors identify challenges in chart reasoning that can guide future improvements in LVLMs."
                },
                "zh": {
                    "title": "æå‡å›¾è¡¨é—®ç­”ç³»ç»Ÿçš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ChartQAProï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æé«˜å›¾è¡¨é—®ç­”ç³»ç»Ÿçš„æ€§èƒ½ã€‚å®ƒåŒ…å«æ¥è‡ª157ä¸ªä¸åŒæ¥æºçš„1,341ä¸ªå›¾è¡¨ï¼Œæ¶µç›–å¤šç§å›¾è¡¨ç±»å‹ï¼Œå¹¶æä¾›1,948ä¸ªå¤šæ ·åŒ–çš„é—®é¢˜ã€‚é€šè¿‡å¯¹21ä¸ªæ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ç°ä»£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ChartQAProä¸Šçš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºå›¾è¡¨æ¨ç†çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜åŒ…æ‹¬è¯¦ç»†çš„é”™è¯¯åˆ†æå’Œæ¶ˆèç ”ç©¶ï¼Œä»¥è¯†åˆ«å›¾è¡¨ç†è§£å’Œæ¨ç†ä¸­çš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13079",
            "title": "Retrieval-Augmented Generation with Conflicting Evidence",
            "url": "https://huggingface.co/papers/2504.13079",
            "abstract": "Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.",
            "score": 3,
            "issue_id": 3304,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "13305db862567e7f",
            "authors": [
                "Han Wang",
                "Archiki Prasad",
                "Elias Stengel-Eskin",
                "Mohit Bansal"
            ],
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13079.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#dataset",
                    "#rag",
                    "#optimization",
                    "#agents",
                    "#hallucinations"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ RAMDocs, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ MADAM-RAG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². MADAM-RAG Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 11.40% Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ AmbigDocs Ğ¸ Ğ½Ğ° 15.80% Ğ½Ğ° FaithEval Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RAG. ĞĞ´Ğ½Ğ°ĞºĞ¾, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ, Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Accuracy through Debate and Retrieval",
                    "desc": "This paper discusses the challenges faced by large language model (LLM) agents when using retrieval-augmented generation (RAG) to provide accurate responses to user queries. It introduces RAMDocs, a new dataset designed to simulate complex scenarios involving ambiguity, misinformation, and noise in documents. The authors propose MADAM-RAG, a multi-agent system where LLMs debate answers over multiple rounds, allowing for better aggregation of responses while filtering out inaccuracies. The results show that MADAM-RAG significantly improves performance on tasks requiring disambiguation and misinformation suppression compared to traditional RAG methods, although challenges remain in handling conflicting evidence."
                },
                "zh": {
                    "title": "å¤šä»£ç†è¾©è®ºï¼šæå‡è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥æé«˜å›ç­”çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåœ¨å¤„ç†æ¨¡ç³Šç”¨æˆ·æŸ¥è¯¢å’Œæ¥è‡ªå¤šä¸ªæ¥æºçš„æ½œåœ¨å†²çªä¿¡æ¯æ—¶ï¼Œå¸¸å¸¸éœ€è¦æŠ‘åˆ¶æ¥è‡ªå˜ˆæ‚æˆ–æ— å…³æ–‡æ¡£çš„ä¸å‡†ç¡®ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†RAMDocsæ•°æ®é›†ï¼Œæ¨¡æ‹Ÿäº†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢åœºæ™¯ï¼Œå¹¶æå‡ºäº†MADAM-RAGå¤šä»£ç†æ–¹æ³•ï¼Œé€šè¿‡å¤šè½®è¾©è®ºæ¥å¤„ç†ç­”æ¡ˆçš„ä¼˜åŠ£ï¼Œä»è€Œæœ‰æ•ˆæ•´åˆä¸åŒæ¥æºçš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMADAM-RAGåœ¨å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢å’ŒæŠ‘åˆ¶é”™è¯¯ä¿¡æ¯æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13145",
            "title": "Exploring Expert Failures Improves LLM Agent Tuning",
            "url": "https://huggingface.co/papers/2504.13145",
            "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.",
            "score": 2,
            "issue_id": 3305,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "597cd9806c8a07ff",
            "authors": [
                "Li-Cheng Lan",
                "Andrew Bai",
                "Minhao Cheng",
                "Ruochen Wang",
                "Cho-Jui Hsieh",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "OpenAI",
                "Pennsylvania State University",
                "UCLA",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13145.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Exploring Expert Failures (EEF) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. EEF Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Rejection Sampling Fine-Tuning (RFT), Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… WebShop Ğ¸ SciWorld. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Learning from Mistakes: Enhancing LLMs with Expert Failures",
                    "desc": "This paper introduces Exploring Expert Failures (EEF), a novel approach to enhance the performance of Large Language Models (LLMs) in complex tasks. EEF builds on Rejection Sampling Fine-Tuning (RFT) by utilizing insights from previously failed expert trajectories to identify beneficial actions that can improve agent exploration and skill acquisition. By integrating these valuable actions into the training dataset while excluding harmful ones, EEF addresses previously unsolvable subtasks and boosts overall agent performance. The results demonstrate that EEF outperforms existing methods, achieving a 62% win rate in WebShop and setting new benchmarks in task performance."
                },
                "zh": {
                    "title": "ä»å¤±è´¥ä¸­å­¦ä¹ ï¼Œæå‡æ™ºèƒ½ä½“èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®æ¨ç†å’Œäº¤äº’ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆRFTï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡æ¨¡ä»¿ä¸“å®¶ç”Ÿæˆçš„æˆåŠŸè½¨è¿¹å¹¶åœ¨è‡ªç”Ÿæˆçš„æˆåŠŸè½¨è¿¹ä¸Šè¿›è¡Œè¿­ä»£å¾®è°ƒæ¥æå‡æ¨¡å‹çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºRFTåå‘äºç®€å•åœºæ™¯ï¼Œè®¸å¤šå¤æ‚å­ä»»åŠ¡ä»ç„¶æœªèƒ½è§£å†³ã€‚æˆ‘ä»¬æå‡ºçš„æ¢ç´¢ä¸“å®¶å¤±è´¥ï¼ˆEEFï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä»å¤±è´¥çš„ä¸“å®¶è½¨è¿¹ä¸­æå–æœ‰ç›Šçš„è¡ŒåŠ¨ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¢ç´¢æ•ˆç‡å’Œå…³é”®æŠ€èƒ½çš„è·å–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12782",
            "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
            "url": "https://huggingface.co/papers/2504.12782",
            "abstract": "Ensuring the ethical deployment of text-to-image models requires effective techniques to prevent the generation of harmful or inappropriate content. While concept erasure methods offer a promising solution, existing finetuning-based approaches suffer from notable limitations. Anchor-free methods risk disrupting sampling trajectories, leading to visual artifacts, while anchor-based methods rely on the heuristic selection of anchor concepts. To overcome these shortcomings, we introduce a finetuning framework, dubbed ANT, which Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is built on a key insight: reversing the condition direction of classifier-free guidance during mid-to-late denoising stages enables precise content modification without sacrificing early-stage structural integrity. This inspires a trajectory-aware objective that preserves the integrity of the early-stage score function field, which steers samples toward the natural image manifold, without relying on heuristic anchor concept selection. For single-concept erasure, we propose an augmentation-enhanced weight saliency map to precisely identify the critical parameters that most significantly contribute to the unwanted concept, enabling more thorough and efficient erasure. For multi-concept erasure, our objective function offers a versatile plug-and-play solution that significantly boosts performance. Extensive experiments demonstrate that ANT achieves state-of-the-art results in both single and multi-concept erasure, delivering high-quality, safe outputs without compromising the generative fidelity. Code is available at https://github.com/lileyang1210/ANT",
            "score": 1,
            "issue_id": 3305,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "6c74497f6fd8b96b",
            "authors": [
                "Leyang Li",
                "Shilin Lu",
                "Yan Ren",
                "Adams Wai-Kin Kong"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12782.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#training",
                    "#optimization",
                    "#ethics"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ANT: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ANT Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ANT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞºĞ¾Ñ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. ANT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "ANT: Ethical Image Generation Through Smart Concept Erasure",
                    "desc": "This paper presents ANT, a new finetuning framework designed to improve the ethical deployment of text-to-image models by effectively erasing harmful or inappropriate content. ANT addresses the limitations of existing methods by guiding denoising trajectories to avoid unwanted concepts without disrupting the image quality. It introduces a trajectory-aware objective that maintains the integrity of early-stage image features while allowing for precise content modification. The framework also includes an innovative weight saliency map for identifying critical parameters in single-concept erasure and offers a versatile solution for multi-concept erasure, achieving state-of-the-art results in generating safe and high-quality images."
                },
                "zh": {
                    "title": "ANTï¼šé«˜æ•ˆå»é™¤ä¸å½“å†…å®¹çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºANTçš„å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºç¡®ä¿æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ä¼¦ç†éƒ¨ç½²ï¼Œç‰¹åˆ«æ˜¯é˜²æ­¢ç”Ÿæˆæœ‰å®³æˆ–ä¸å½“å†…å®¹ã€‚ANTé€šè¿‡è‡ªåŠ¨å¼•å¯¼å»å™ªè½¨è¿¹ï¼Œé¿å…äº†ç°æœ‰æ–¹æ³•ä¸­çš„ä¸€äº›å±€é™æ€§ï¼Œå¦‚é”šç‚¹æ–¹æ³•çš„å¯å‘å¼é€‰æ‹©å’Œæ— é”šç‚¹æ–¹æ³•å¯¼è‡´çš„è§†è§‰ä¼ªå½±ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº†åœ¨å»å™ªä¸­åæœŸåè½¬åˆ†ç±»å™¨æ— æŒ‡å¯¼çš„æ¡ä»¶æ–¹å‘çš„å…³é”®è§è§£ï¼Œä»è€Œå®ç°äº†ç²¾ç¡®çš„å†…å®¹ä¿®æ”¹ï¼ŒåŒæ—¶ä¿æŒäº†æ—©æœŸé˜¶æ®µçš„ç»“æ„å®Œæ•´æ€§ã€‚é€šè¿‡å¢å¼ºçš„æƒé‡æ˜¾è‘—æ€§å›¾ï¼ŒANTèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¹¶å»é™¤å•ä¸€æˆ–å¤šä¸ªä¸å½“æ¦‚å¿µï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å»é™¤æ•ˆæœå’Œç”Ÿæˆè´¨é‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-17.html",
    "link_next": "2025-04-21.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "17.04",
        "en": "04/17",
        "zh": "4æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "21.04",
        "en": "04/21",
        "zh": "4æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "This article discusses the role of color in how humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark for testing VLMs' color understanding skills. The study finds that while larger models perform better, current VLMs generally neglect color understanding. It also shows that using Chain of Thought (CoT) reasoning can improve color task accuracy. The authors hope ColorBench will help advance research in this area.",
        "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "pinyin": "Sure, here is the pinyin transcription for the text:\n\nZhÃ¨ wÃ©nzhÄng tÇolÃ¹n zhÃ¨ zhÇ’ng yÇnsÃ¨ zÃ i rÃ©nlÃ¨i hÃ© shÃ¬yÇn yÇ”yÃ¡n mÃ³xÃ­ng (VLMs) zhÄ«jiÃ n hÃ© lÇjiÄ› shÃ¬jiÃ¨ zhÅng de zuÃ²yÃ²ng. TÄ jiÃ¨shÃ o YÇnsÃ¨ BÄ›nch, yÄ«gÃ¨ xÄ«n de bÄ›nchmark yÇ cÃ¨shÃ¬ VLMs de yÇnsÃ¨ lÇjiÄ› jÃ¬nÃ©ng. YÃ¡njiÅ« fÄxiÃ n zhÇyÇ’u dÃ xÃ­ng mÃ³xÃ­ng biÇoxiÃ n gÃ¨ng hÇo, dÄngqiÃ¡n VLMs yÄ«bÄn shÅ« huÇng yÇnsÃ¨ lÇjiÄ›. TÄ yÄ› shuÅmÃ­ng yÃ²ng LiÃ¡n de SÄ«xiÇng (CoT) tuÇlÇ nÃ©ng gÇishÃ n yÇnsÃ¨ rÃ¨nwÃ¹ de zhÇ”nquÃ¨du. ZhÃ¨xiÄ“ zuÃ²zhÄ› xÄ«wÃ ng YÇnsÃ¨ BÄ›nch huÃ¬ bÄngzhÃ¹ tuÄ«jÃ¬n zhÃ¨ ge lÇngyÃ¹ de yÃ¡njiÅ«.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific word choices.",
        "vocab": "[\n    {\"word\": \"perceive\", \"pinyin\": \"pÉ™rËˆsiËv\", \"trans\": \"æ„ŸçŸ¥\"},\n    {\"word\": \"vision-language models\", \"pinyin\": \"ËˆvÉªÊ’É™n ËˆlÃ¦Å‹É¡wÉªdÊ’ mÉ’dÉ™lz\", \"trans\": \"è§†è§‰-è¯­è¨€æ¨¡å‹\"},\n    {\"word\": \"benchmark\", \"pinyin\": \"ËˆbÉ›nÊ§mÉ‘Ëk\", \"trans\": \"åŸºå‡†\"},\n    {\"word\": \"neglect\", \"pinyin\": \"nÉªËˆÉ¡lÉ›kt\", \"trans\": \"å¿½è§†\"},\n    {\"word\": \"Chain of Thought\", \"pinyin\": \"Ê§eÉªn É’v Î¸É”Ët\", \"trans\": \"æ€ç»´é“¾\"},\n    {\"word\": \"reasoning\", \"pinyin\": \"ËˆriËz(É™)nÉªÅ‹\", \"trans\": \"æ¨ç†\"},\n    {\"word\": \"accuracy\", \"pinyin\": \"ËˆÃ¦kjÉ™rÉ™si\", \"trans\": \"å‡†ç¡®æ€§\"},\n    {\"word\": \"advance\", \"pinyin\": \"Ã¦dËˆvÉ‘Ëns\", \"trans\": \"æ¨è¿›\"}\n]",
        "trans": "This article explores the role of color in how both humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark designed to evaluate VLMs' ability to understand color. The study reveals that while larger models tend to perform better, current VLMs generally overlook color understanding. Additionally, the research demonstrates that employing Chain of Thought (CoT) reasoning can enhance the accuracy of color-related tasks. The authors express hope that ColorBench will contribute to further advancements in this field of research.",
        "update_ts": "2025-04-17 09:12"
    }
}