{
    "date": {
        "ru": "2 октября",
        "en": "October 2",
        "zh": "10月2日"
    },
    "time_utc": "2025-10-02 02:16",
    "weekday": 3,
    "issue_id": 6198,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.00184",
            "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
            "url": "https://huggingface.co/papers/2510.00184",
            "abstract": "Reverse-engineering a model that learns multi-digit multiplication via implicit chain-of-thought reveals that it uses attention to encode long-range dependencies and represents partial products efficiently, insights that help address limitations in standard fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via implicit chain-of-thought, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.",
            "score": 5,
            "issue_id": 6198,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "27d7ce536d31aa04",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "🔢",
                "ru": {
                    "title": "Как нейросети учатся умножать: разгадка механизма длинных зависимостей",
                    "desc": "Исследователи провели обратную инженерию модели, которая научилась многозначному умножению через неявную цепочку рассуждений (chain-of-thought). Оказалось, что модель использует механизм attention для построения направленного ациклического графа, кэширующего промежуточные произведения, и представляет числа через базис Фурье и суммы Минковского. Стандартное fine-tuning застревает в локальном оптимуме из-за неспособности уловить необходимые длинные зависимости между разрядами чисел. Добавление вспомогательной функции потерь для предсказания промежуточных сумм создаёт правильное индуктивное смещение и позволяет модели успешно освоить умножение."
                },
                "en": {
                    "title": "Unlocking Multi-Digit Multiplication with Attention and Inductive Bias",
                    "desc": "This paper investigates how a model learns to perform multi-digit multiplication using an implicit chain-of-thought approach. It reveals that the model effectively encodes long-range dependencies through attention mechanisms, allowing it to manage partial products efficiently. The authors demonstrate that standard fine-tuning methods often fail to capture these dependencies, leading to suboptimal performance. By introducing an auxiliary loss that predicts running sums, they provide a solution to enhance learning dynamics and improve the model's ability to handle complex multiplication tasks."
                },
                "zh": {
                    "title": "揭示多位数乘法学习的关键机制",
                    "desc": "本研究通过逆向工程一个成功学习多位数乘法的模型，揭示了其使用注意力机制编码长距离依赖关系的方式。研究发现，该模型通过构建有向无环图来缓存和检索成对的部分积，从而有效地表示部分积。模型在注意力头中通过形成闵可夫斯基和来实现部分积，并使用傅里叶基表示数字，这些都是标准微调模型所缺乏的直观且高效的表示方式。通过引入辅助损失来预测“运行和”，我们为模型提供了一个归纳偏置，使其能够成功学习多位数乘法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00526",
            "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum",
            "url": "https://huggingface.co/papers/2510.00526",
            "abstract": "Research identifies probability-based objectives that outperform negative log likelihood for fine-tuning large language models, depending on model capability.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., -p, -p^{10}, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
            "score": 4,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "9f43fe314cbe69af",
            "authors": [
                "Gaotang Li",
                "Ruizhong Qiu",
                "Xiusi Chen",
                "Heng Ji",
                "Hanghang Tong"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00526.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Выбор функции потерь зависит от силы модели",
                    "desc": "Исследование показывает, что стандартная функция потерь negative log likelihood (NLL) не всегда оптимальна для файн-тюнинга больших языковых моделей. Авторы изучили семейство вероятностных функций потерь и обнаружили критическую закономерность: эффективность функции зависит от capability модели. Для сильных моделей лучше работают функции, которые снижают вес токенов с низкой вероятностью (например, -p или -p^10), для слабых моделей эффективнее остаётся классический NLL. Эксперименты на 7 архитектурах, 14 бенчмарках и 3 доменах подтверждают существование континуума model-capability, который определяет выбор оптимальной функции потерь."
                },
                "en": {
                    "title": "Optimizing Fine-Tuning: Beyond Negative Log Likelihood",
                    "desc": "This paper explores how different training objectives can improve the fine-tuning of large language models (LLMs) beyond the traditional negative log likelihood (NLL). It identifies that NLL may not be optimal for models that have already been pre-trained, as they possess inherent task-relevant knowledge. The authors propose a range of probability-based objectives that adapt to the model's capability, showing that stronger models benefit from objectives that prioritize high-probability tokens. Through extensive experiments, they demonstrate that the effectiveness of these objectives varies along a continuum of model strength, providing insights into how to select the best training objective based on model performance."
                },
                "zh": {
                    "title": "超越负对数似然的微调目标",
                    "desc": "本研究探讨了基于概率的目标函数在微调大型语言模型时的表现，发现其在不同模型能力下优于负对数似然（NLL）。传统的监督微调方法常常受限于NLL这一训练目标，而在后训练阶段，模型已经具备了任务相关的先验知识。我们通过大量实验和消融研究，揭示了模型能力的连续性对目标函数表现的影响。在强模型端，倾向于先验的目标函数表现优于NLL，而在弱模型端则是NLL占优，提供了根据模型能力调整目标函数的理论基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01174",
            "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
            "url": "https://huggingface.co/papers/2510.01174",
            "abstract": "Code2Video generates educational videos using a code-centric agent framework, improving coherence and interpretability compared to direct code generation.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.",
            "score": 3,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "26c2c9dd6c370251",
            "authors": [
                "Yanzhe Chen",
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01174.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#video",
                    "#games",
                    "#optimization",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "Генерация обучающих видео через программный код",
                    "desc": "Code2Video — это фреймворк на основе агентов для создания образовательных видео через исполняемый Python-код. Система использует трёх совместно работающих агентов: планировщик структурирует контент лекции, программист преобразует инструкции в код с автоматическим исправлением ошибок, а критик на основе vision-language моделей улучшает визуальную компоновку. Для оценки качества предложена метрика TeachQuiz, которая измеряет, насколько хорошо LLM может восстановить знания после просмотра сгенерированного видео. Подход показывает улучшение на 40% по сравнению с прямой генерацией кода и создаёт видео, сопоставимые с профессиональными обучающими материалами."
                },
                "en": {
                    "title": "Code2Video: Crafting Coherent Educational Videos with Code",
                    "desc": "Code2Video is a framework designed to create educational videos using a code-centric approach, which enhances coherence and interpretability compared to traditional methods. It consists of three main agents: the Planner organizes content into logical sequences, the Coder translates these sequences into executable Python code, and the Critic refines the visual layout using vision-language models. This method addresses the challenges of generating professional educational videos by allowing precise control over visual elements and transitions. The framework has shown a significant improvement in video quality and coherence, outperforming direct code generation by 40%."
                },
                "zh": {
                    "title": "Code2Video：教育视频生成的新方法",
                    "desc": "Code2Video 是一个基于代码的代理框架，用于生成教育视频，提升了视频的一致性和可解释性。该框架包含三个协作代理：规划者负责将讲座内容结构化并准备视觉资产；编码器将结构化指令转换为可执行的 Python 代码，并通过范围引导自动修复提高效率；评论者利用视觉语言模型优化空间布局，确保清晰度。通过建立专业制作的教育视频基准MMMC，我们评估了Code2Video在美学评分、代码效率和知识恢复等多个维度的表现，结果显示其在视频生成上优于直接代码生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00232",
            "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model\n  Responses",
            "url": "https://huggingface.co/papers/2510.00232",
            "abstract": "BiasFreeBench evaluates bias mitigation techniques in large language models using a unified benchmark and response-level metric to ensure fair and safe outputs in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.",
            "score": 2,
            "issue_id": 6198,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "7a6ed8974cc83369",
            "authors": [
                "Xin Xu",
                "Xunzhi He",
                "Churan Zhi",
                "Ruizhe Chen",
                "Julian McAuley",
                "Zexue He"
            ],
            "affiliations": [
                "Columbia University",
                "MIT-IBM Watson Lab",
                "UC San Diego",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00232.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Единый бенчмарк для честной оценки методов борьбы с предвзятостью в LLM",
                    "desc": "Существующие исследования методов устранения предвзятости в больших языковых моделях используют разные базовые подходы и метрики, что затрудняет их сравнение. В работе представлен BiasFreeBench — унифицированный бенчмарк, который сравнивает восемь основных техник устранения bias (четыре на основе промптинга и четыре на основе обучения) в реальных сценариях взаимодействия с пользователями. Авторы вводят метрику уровня ответов Bias-Free Score, которая измеряет справедливость, безопасность и антистереотипность ответов LLM. Бенчмарк систематически сравнивает эффективность различных методов с учётом парадигмы (промптинг vs обучение), размера модели и способности обобщаться на новые типы предвзятости."
                },
                "en": {
                    "title": "Unifying Bias Mitigation Evaluation for Safer AI Outputs",
                    "desc": "BiasFreeBench is a new benchmark designed to evaluate bias mitigation techniques in large language models (LLMs). It addresses the inconsistency in previous studies by providing a unified framework for comparing various debiasing methods. The benchmark includes a novel response-level metric called Bias-Free Score, which assesses the fairness and safety of model outputs in real-world scenarios. By systematically analyzing different debiasing strategies, BiasFreeBench aims to enhance the reliability of LLMs in producing equitable and safe responses."
                },
                "zh": {
                    "title": "统一评估偏见缓解技术的基准工具",
                    "desc": "BiasFreeBench 是一个评估大型语言模型偏见缓解技术的基准工具，旨在确保模型输出在现实场景中公平和安全。该研究通过统一的查询-响应设置，比较了八种主流的偏见缓解方法，包括四种基于提示和四种基于训练的方法。我们引入了一个新的响应级别指标——无偏分数，来衡量模型响应的公平性、安全性和反刻板印象程度。该基准的发布将为偏见缓解研究提供一个统一的测试平台。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01051",
            "title": "GEM: A Gym for Agentic LLMs",
            "url": "https://huggingface.co/papers/2510.01051",
            "abstract": "GEM, an open-source environment simulator, facilitates experience-based learning for large language models by providing a standardized framework and diverse environments for training and benchmarking reinforcement learning algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.",
            "score": 1,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "6d69bb75ee0b2258",
            "authors": [
                "Zichen Liu",
                "Anya Sims",
                "Keyu Duan",
                "Changyu Chen",
                "Simon Yu",
                "Xiangxin Zhou",
                "Haotian Xu",
                "Shaopan Xiong",
                "Bo Liu",
                "Chenmien Tan",
                "Chuen Yang Beh",
                "Weixun Wang",
                "Hao Zhu",
                "Weiyan Shi",
                "Diyi Yang",
                "Michael Shieh",
                "Yee Whye Teh",
                "Wee Sun Lee",
                "Min Lin"
            ],
            "affiliations": [
                "NUS",
                "Northeastern",
                "OpenRLHF",
                "Oxford",
                "RL2",
                "ROLL",
                "SMU",
                "Sea AI Lab",
                "Stanford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01051.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#open_source",
                    "#benchmark",
                    "#training",
                    "#games",
                    "#agents"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "GEM: спортзал для тренировки LLM-агентов через reinforcement learning",
                    "desc": "В статье представлен GEM (General Experience Maker) — открытая среда-симулятор для обучения больших языковых моделей через взаимодействие с окружением. Это аналог OpenAI Gym, но специально разработанный для эпохи LLM, предоставляющий стандартизированный интерфейс между агентом и средой с поддержкой асинхронного векторизованного выполнения. GEM включает 24 разнообразные среды и базовые результаты с использованием алгоритма REINFORCE с Return Batch Normalization (ReBN), который лучше справляется с credit assignment по сравнению с GRPO. Авторы также проводят сравнительный анализ популярных RL-алгоритмов (PPO, GRPO, REINFORCE) и позиционируют GEM как инструмент для ускорения исследований агентных LLM."
                },
                "en": {
                    "title": "GEM: Empowering LLMs with Experience-Based Learning",
                    "desc": "GEM (General Experience Maker) is an open-source simulator designed to enhance experience-based learning for large language models (LLMs) by providing a standardized framework for training and benchmarking reinforcement learning (RL) algorithms. It allows agents to learn by interacting with various complex environments, moving away from static datasets. GEM includes features like asynchronous vectorized execution for efficient processing and flexible wrappers for easy customization. Additionally, it offers a suite of environments and tools for evaluating different RL algorithms, aiming to accelerate research in agentic LLMs."
                },
                "zh": {
                    "title": "GEM：加速大型语言模型的经验学习",
                    "desc": "GEM（通用经验生成器）是一个开源环境模拟器，旨在为大型语言模型提供基于经验的学习体验。它为强化学习算法的训练和基准测试提供了标准化框架和多样化环境，类似于传统强化学习中的OpenAI-Gym。GEM支持异步向量化执行，具有高吞吐量，并提供灵活的包装器以便于扩展。此外，GEM还包含多种环境、强大的集成工具和示例脚本，帮助研究人员加速未来的智能体语言模型研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00777",
            "title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.00777",
            "abstract": "In-place feedback allows users to directly edit LLM responses, improving performance and reducing token usage in multi-turn reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine their outputs based on user-provided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM's previous response, and the model conditions on this modified response to generate its revision. Empirical evaluations on diverse reasoning-intensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using 79.1% fewer tokens. Complementary analyses on controlled environments further demonstrate that in-place feedback resolves a core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content. These findings suggest that in-place feedback offers a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks.",
            "score": 1,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "be36c5f4f29f17e4",
            "authors": [
                "Youngbin Choi",
                "Minjong Lee",
                "Saemi Moon",
                "Seunghyuk Cho",
                "Chaehyeon Chung",
                "MoonJeong Park",
                "Dongwoo Kim"
            ],
            "affiliations": [
                "Computer Science and Engineering, POSTECH",
                "Graduate School of Artificial Intelligence, POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00777.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "✏️",
                "ru": {
                    "title": "Редактируй прямо здесь: эффективная обратная связь для LLM",
                    "desc": "Исследователи предложили новый подход взаимодействия с языковыми моделями под названием in-place feedback, при котором пользователи напрямую редактируют ответы LLM вместо отправки новых сообщений. Эксперименты на задачах многошагового рассуждения показали, что этот метод улучшает производительность модели и снижает использование токенов на 79,1% по сравнению с традиционной многошаговой обратной связью. Ключевое преимущество заключается в том, что модели лучше применяют исправления именно к ошибочным частям ответа, избегая появления новых ошибок в ранее корректном содержании. Подход демонстрирует более естественный и эффективный механизм управления LLM в задачах, требующих сложных рассуждений."
                },
                "en": {
                    "title": "In-Place Feedback: Direct Edits for Smarter LLMs",
                    "desc": "This paper presents a new method called in-place feedback for improving large language models (LLMs) during multi-turn reasoning tasks. Instead of sending new messages for feedback, users can directly edit the model's previous responses, allowing the model to learn from these modifications. The results show that this approach not only enhances the model's performance but also significantly reduces the number of tokens used by 79.1%. Overall, in-place feedback addresses the limitations of traditional feedback methods by enabling more precise corrections and reducing the introduction of new errors."
                },
                "zh": {
                    "title": "就地反馈：提升LLM推理的有效新方式",
                    "desc": "本研究提出了一种新的交互模式——就地反馈，允许用户直接编辑大型语言模型（LLM）的响应。这种方法在多轮推理任务中表现出色，能够显著提高模型的性能，同时减少79.1%的令牌使用。通过实证评估，我们发现就地反馈比传统的多轮反馈更有效，能够更准确地应用用户的反馈，避免了模型在修正错误时引入新的错误。总的来说，就地反馈为指导LLM在复杂推理任务中提供了一种更自然和有效的机制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00553",
            "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
            "url": "https://huggingface.co/papers/2510.00553",
            "abstract": "Two fundamental properties of reinforcement learning-induced parameter updates in large language models are identified, leading to a plug-in acceleration framework that significantly speeds up training without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.",
            "score": 1,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "75c2581875112809",
            "authors": [
                "Yuchen Cai",
                "Ding Cao",
                "Xin Xu",
                "Zijun Yao",
                "Yuqing Huang",
                "Zhenyu Tan",
                "Benyi Zhang",
                "Guiquan Liu",
                "Junfeng Fang"
            ],
            "affiliations": [
                "HKUST",
                "NUS",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00553.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение обучения LLM через ранговую экстраполяцию",
                    "desc": "Исследование выявляет два фундаментальных свойства обновлений параметров в больших языковых моделях при reinforcement learning: доминирование ранга-1 и линейную динамику главного подпространства. Оказывается, что верхнее сингулярное подпространство матрицы обновлений параметров определяет более 99% улучшения способностей к рассуждению. На основе этих находок предложен фреймворк AlphaRL, который экстраполирует финальное обновление параметров, используя короткое окно раннего обучения. Метод обеспечивает ускорение до 2.5x с сохранением более 96% производительности без дополнительных модулей или настройки гиперпараметров."
                },
                "en": {
                    "title": "Accelerating RL Training in LLMs with AlphaRL",
                    "desc": "This paper explores how reinforcement learning (RL) affects the training of large language models (LLMs) by identifying two key properties of parameter updates. The first property, Rank-1 Dominance, shows that a specific part of the parameter update matrix is crucial for improving reasoning capabilities, capturing over 99% of performance gains. The second property, Rank-1 Linear Dynamics, indicates that this important part changes in a predictable way during training, allowing for accurate predictions from early training stages. Based on these insights, the authors introduce AlphaRL, a framework that accelerates training by predicting final updates from early data, achieving significant speed improvements while maintaining high performance."
                },
                "zh": {
                    "title": "加速强化学习训练的有效工具",
                    "desc": "本文识别了强化学习在大型语言模型（LLMs）中引起的参数更新的两个基本特性。这些特性包括：1）秩-1主导性，意味着参数更新矩阵的主导子空间几乎完全决定了推理的改进；2）秩-1线性动态，表明这个主导子空间在训练过程中线性演变。基于这些发现，提出了AlphaRL加速框架，可以在不牺牲性能的情况下显著加快训练速度。实验表明，该框架在多个大型语言模型和算法中具有广泛的适用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00536",
            "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
            "url": "https://huggingface.co/papers/2510.00536",
            "abstract": "GUI-KV, a KV cache compression method for GUI agents, improves efficiency by exploiting spatial and temporal redundancies, reducing computational cost while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.",
            "score": 1,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "6687d7079b2d54e2",
            "authors": [
                "Kung-Hsiang Huang",
                "Haoyi Qiu",
                "Yutong Dai",
                "Caiming Xiong",
                "Chien-Sheng Wu"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00536.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Сжатие памяти для AI-агентов в графических интерфейсах",
                    "desc": "Исследователи представили GUI-KV — метод сжатия KV-кэша для AI-агентов, работающих с графическими интерфейсами. Метод использует две техники: пространственную оценку важности визуальных токенов через L2-норму скрытых состояний и удаление избыточной информации между последовательными скриншотами. В отличие от обработки естественных изображений, внимание в GUI равномерно разрежено во всех слоях трансформера, что позволяет использовать единую стратегию распределения бюджета памяти. На бенчмарке AgentNetBench метод снижает вычислительные затраты на 38.9% при улучшении точности на 4.1%, не требуя дополнительного обучения модели."
                },
                "en": {
                    "title": "Efficient GUI Agents with GUI-KV Cache Compression",
                    "desc": "The paper presents GUI-KV, a method for compressing key-value (KV) caches specifically designed for graphical user interface (GUI) agents. It addresses the inefficiencies in processing high-resolution screenshots by leveraging spatial and temporal redundancies, which reduces computational costs while maintaining accuracy. The authors analyze attention patterns in GUI workloads and propose a uniform budget allocation strategy that outperforms more complex methods. By introducing techniques like spatial saliency guidance and temporal redundancy scoring, GUI-KV achieves significant improvements in efficiency and accuracy across various benchmarks."
                },
                "zh": {
                    "title": "高效的GUI代理缓存压缩方法",
                    "desc": "GUI-KV是一种针对图形用户界面（GUI）代理的键值（KV）缓存压缩方法，通过利用空间和时间冗余来提高效率。该方法在处理高分辨率截图和长时间任务时，能够降低计算成本，同时保持准确性。研究表明，GUI代理的注意力模式与自然图像不同，所有变换器层的注意力稀疏性均较高，这促使我们提出了一种简单的均匀预算分配策略。GUI-KV结合了空间显著性引导和时间冗余评分两种新技术，显著提高了缓存压缩的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00510",
            "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
            "url": "https://huggingface.co/papers/2510.00510",
            "abstract": "A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.",
            "score": 1,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "4a1605179598a812",
            "authors": [
                "Jiarun Liu",
                "Shiyue Xu",
                "Shangkun Liu",
                "Yang Li",
                "Wen Liu",
                "Min Liu",
                "Xiaoqing Zhou",
                "Hanmin Wang",
                "Shilin Jia",
                "zhen Wang",
                "Shaohua Tian",
                "Hanhao Li",
                "Junbo Zhang",
                "Yongli Yu",
                "Peng Cao",
                "Haofen Wang"
            ],
            "affiliations": [
                "GAIA JINGDONG CHO-EI Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00510.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#optimization",
                    "#architecture",
                    "#multimodal",
                    "#agents",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Универсальный AI-агент через интеграцию памяти, планирования и инструментов",
                    "desc": "В статье предлагается универсальная архитектура AI-агента, которая объединяет три ключевых компонента для решения сложных задач. Первый компонент — это мультиагентная система с планированием, исполнением и голосованием критических моделей. Второй — иерархическая память, включающая рабочий, семантический и процедурный уровни. Третий — набор инструментов для поиска, выполнения кода и мультимодального парсинга. Система превосходит open-source решения и приближается к производительности проприетарных систем."
                },
                "en": {
                    "title": "Empowering AI with Integrated Generalist Agent Architecture",
                    "desc": "This paper presents a new architecture for generalist agents that enhances their performance in various tasks. It combines multi-agent planning, where different agents work together to make decisions, with a hierarchical memory system that organizes information at different levels. Additionally, it includes a refined tool suite that allows the agent to perform tasks like searching and executing code. The proposed system shows significant improvements over existing models, indicating that integrating these components leads to more robust and adaptable AI assistants."
                },
                "zh": {
                    "title": "通用智能体架构：提升AI助手的适应性与鲁棒性",
                    "desc": "本文提出了一种通用智能体架构，结合了多智能体规划、分层记忆和精细化工具套件，能够在多种任务中超越现有系统。该架构整合了集体多智能体框架、分层记忆系统以及用于搜索、代码执行和多模态解析的工具。通过全面的基准测试，我们的框架在性能上持续优于开源基线，并接近专有系统的表现。这些结果表明系统级集成的重要性，并为可扩展、弹性和适应性强的人工智能助手指明了方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00406",
            "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified\n  Rewards in World Simulators",
            "url": "https://huggingface.co/papers/2510.00406",
            "abstract": "VLA-RFT uses a data-driven world model to fine-tune VLA models efficiently, reducing sample requirements and improving robustness under perturbations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.",
            "score": 1,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "8dead5a43b0cdc61",
            "authors": [
                "Hengtao Li",
                "Pengxiang Ding",
                "Runze Suo",
                "Yihao Wang",
                "Zirui Ge",
                "Dongyuan Zang",
                "Kexian Yu",
                "Mingyang Sun",
                "Hongyin Zhang",
                "Donglin Wang",
                "Weihua Su"
            ],
            "affiliations": [
                "BUPT",
                "Fudan University",
                "Hebei University of Technology",
                "OpenHelix Team",
                "Westlake University",
                "Zhejiang University",
                "Zhengzhou University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00406.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение робота через мир-симулятор: эффективно и надёжно",
                    "desc": "Статья представляет VLA-RFT — метод дообучения Vision-Language-Action моделей с помощью reinforcement learning. Вместо дорогостоящих взаимодействий с реальным миром используется data-driven world model, которая предсказывает будущие визуальные наблюдения на основе действий агента. Метод требует менее 400 шагов fine-tuning для достижения результатов лучше, чем supervised baseline, и показывает высокую устойчивость к возмущениям. VLA-RFT решает проблему накопления ошибок в imitation learning и предлагает практичный подход к улучшению генерализации роботизированных политик."
                },
                "en": {
                    "title": "Enhancing VLA Models with Efficient Reinforcement Fine-Tuning",
                    "desc": "VLA-RFT is a framework that improves Vision-Language-Action (VLA) models by using a data-driven world model for reinforcement fine-tuning. This approach reduces the number of samples needed for training and enhances the model's ability to handle unexpected changes in the environment. By simulating future visual observations based on actions, it provides a more effective learning signal that aligns with the desired outcomes. The results show that VLA-RFT not only outperforms traditional supervised methods but also maintains strong performance even when conditions are altered."
                },
                "zh": {
                    "title": "利用世界模型提升VLA模型的鲁棒性与效率",
                    "desc": "VLA-RFT是一种强化学习微调框架，利用数据驱动的世界模型作为可控模拟器，从而提高VLA模型的效率。该框架通过真实交互数据训练，能够预测基于动作的未来视觉观察，提供密集的轨迹级奖励信号。与传统的监督学习方法相比，VLA-RFT在样本需求上大幅降低，且在少于400步的微调后超越了强大的基线模型。它在扰动条件下表现出强大的鲁棒性，确保任务执行的稳定性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23250",
            "title": "Training Vision-Language Process Reward Models for Test-Time Scaling in\n  Multimodal Reasoning: Key Insights and Lessons Learned",
            "url": "https://huggingface.co/papers/2509.23250",
            "abstract": "Hybrid data synthesis and perception-focused supervision improve the reliability of Vision-Language Process Reward Models (VL-PRMs) in guiding VLMs across diverse multimodal benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs.",
            "score": 1,
            "issue_id": 6198,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "c59cc3e092f9a705",
            "authors": [
                "Brandon Ong",
                "Tej Deep Pala",
                "Vernon Toh",
                "William Chandra Tjhi",
                "Soujanya Poria"
            ],
            "affiliations": [
                "AI Singapore",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23250.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#games",
                    "#multimodal",
                    "#data",
                    "#dataset"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Учим мультимодальные модели проверять свои рассуждения пошагово",
                    "desc": "Статья представляет улучшенные Vision-Language Process Reward Models (VL-PRMs), которые обеспечивают пошаговый контроль качества рассуждений в мультимодальных языковых моделях. Авторы предлагают гибридный подход к синтезу данных, комбинируя MCTS с оценками сильной VLM, и вводят специальную supervision на уровне визуального восприятия. Эксперименты на пяти бенчмарках показывают, что даже небольшие VL-PRMs могут эффективно выявлять ошибки в рассуждениях и улучшать производительность базовых моделей во время инференса. Ключевое открытие: VL-PRMs как Outcome Reward Models превосходят пошаговую селекцию, а supervision на уровне восприятия существенно повышает качество test-time scaling."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Hybrid Supervision and Data Synthesis",
                    "desc": "This paper discusses improvements in Vision-Language Process Reward Models (VL-PRMs) to enhance their effectiveness in guiding Vision Language Models (VLMs). The authors introduce a hybrid data synthesis method that merges Monte Carlo Tree Search with insights from a robust VLM, resulting in more precise step-level supervision. They also propose a perception-focused supervision approach that helps the model identify errors during visual reasoning. Through extensive testing on various multimodal benchmarks, the study demonstrates that these enhancements lead to better performance and reliability in VLMs, even in complex reasoning tasks."
                },
                "zh": {
                    "title": "混合数据合成与感知监督提升视觉语言模型的可靠性",
                    "desc": "本研究提出了一种混合数据合成框架，结合了蒙特卡洛树搜索（MCTS）和强大的视觉语言模型（VLM）的判断，以生成更准确的步骤级标签。我们还引入了以感知为中心的监督，帮助过程奖励模型（PRM）在推理的视觉基础阶段明确检测错误。通过系统评估多种测试时扩展策略，我们的实验表明，视觉语言过程奖励模型（VL-PRM）能够可靠地引导VLM朝向更准确的解决方案。我们的研究结果为进一步研究和视觉语言模型的进步提供了重要的见解。"
                }
            }
        }
    ],
    "link_prev": "2025-10-01.html",
    "link_next": "2025-10-03.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "01.10",
        "en": "10/01",
        "zh": "10月1日"
    },
    "short_date_next": {
        "ru": "03.10",
        "en": "10/03",
        "zh": "10月3日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 5,
        "#cv": 0,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}