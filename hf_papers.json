{
    "date": {
        "ru": "24 февраля",
        "en": "February 24",
        "zh": "2月24日"
    },
    "time_utc": "2025-02-24 13:19",
    "weekday": 0,
    "issue_id": 2373,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.14776",
            "title": "SurveyX: Academic Survey Automation via Large Language Models",
            "url": "https://huggingface.co/papers/2502.14776",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, a pre-processing method called AttributeTree, and a re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on www.surveyx.cn",
            "score": 68,
            "issue_id": 2363,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "b2504554ef88d631",
            "authors": [
                "Xun Liang",
                "Jiawei Yang",
                "Yezhaohui Wang",
                "Chen Tang",
                "Zifan Zheng",
                "Simin Niu",
                "Shichao Song",
                "Hanyu Wang",
                "Bo Tang",
                "Feiyu Xiong",
                "Keming Mao",
                "Zhiyu li"
            ],
            "affiliations": [
                "Institute for Advanced Algorithms Research, Shanghai, China",
                "Northeastern University, Shenyang, China",
                "Renmin University of China, Beijing, China",
                "The University of Sydney, Sydney, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14776.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#survey",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "SurveyX: Революция в автоматизированном создании научных обзоров",
                    "desc": "Исследователи представили SurveyX - систему для автоматизированного создания обзоров с использованием больших языковых моделей (LLM). Система разбивает процесс составления обзора на две фазы: подготовку и генерацию, включая онлайн-поиск ссылок и предобработку данных методом AttributeTree. Экспериментальная оценка показала, что SurveyX превосходит существующие системы по качеству контента и цитирования, приближаясь к уровню экспертов-людей. Система решает ряд ограничений, присущих предыдущим подходам к автоматизированному созданию обзоров."
                },
                "en": {
                    "title": "SurveyX: Revolutionizing Automated Survey Generation with LLMs",
                    "desc": "This paper introduces SurveyX, a novel system for automated survey generation that leverages Large Language Models (LLMs) to improve the survey creation process. It addresses key limitations of previous methods by breaking down the process into two distinct phases: Preparation and Generation. SurveyX incorporates innovative techniques such as online reference retrieval and a pre-processing method called AttributeTree, which enhance the quality of the generated surveys. Experimental results demonstrate that SurveyX significantly outperforms existing systems in both content and citation quality, nearing the performance of human experts."
                },
                "zh": {
                    "title": "SurveyX：高效的自动调查生成系统",
                    "desc": "大型语言模型（LLMs）在理解能力和知识基础方面表现出色，显示出它们可以作为自动调查生成的有效工具。然而，现有的自动调查生成研究受到一些关键限制，如有限的上下文窗口、缺乏深入的内容讨论和缺乏系统的评估框架。我们提出了SurveyX，一个高效且有组织的自动调查生成系统，将调查编写过程分为准备阶段和生成阶段。通过引入在线参考检索、属性树预处理方法和重新润色过程，SurveyX显著提高了调查编写的效率，并在内容质量和引用质量上超越了现有系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11663",
            "title": "MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction",
            "url": "https://huggingface.co/papers/2502.11663",
            "abstract": "World models that forecast environmental changes from actions are vital for autonomous driving models with strong generalization. The prevailing driving world model mainly build on video prediction model. Although these models can produce high-fidelity video sequences with advanced diffusion-based generator, they are constrained by their predictive duration and overall generalization capabilities. In this paper, we explore to solve this problem by combining generation loss with MAE-style feature-level context learning. In particular, we instantiate this target with three key design: (1) A more scalable Diffusion Transformer (DiT) structure trained with extra mask construction task. (2) we devise diffusion-related mask tokens to deal with the fuzzy relations between mask reconstruction and generative diffusion process. (3) we extend mask construction task to spatial-temporal domain by utilizing row-wise mask for shifted self-attention rather than masked self-attention in MAE. Then, we adopt a row-wise cross-view module to align with this mask design. Based on above improvement, we propose MaskGWM: a Generalizable driving World Model embodied with Video Mask reconstruction. Our model contains two variants: MaskGWM-long, focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view generation. Comprehensive experiments on standard benchmarks validate the effectiveness of the proposed method, which contain normal validation of Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot validation of Waymo dataset. Quantitative metrics on these datasets show our method notably improving state-of-the-art driving world model.",
            "score": 36,
            "issue_id": 2367,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "14ade1d8007cfa16",
            "authors": [
                "Jingcheng Ni",
                "Yuxin Guo",
                "Yichen Liu",
                "Rui Chen",
                "Lewei Lu",
                "Zehuan Wu"
            ],
            "affiliations": [
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11663.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#long_context",
                    "#games",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "MaskGWM: Улучшенная модель мира для автономного вождения",
                    "desc": "Статья представляет новую модель MaskGWM для прогнозирования изменений окружающей среды в контексте автономного вождения. Модель сочетает генеративные методы на основе диффузии с обучением контекста на уровне признаков в стиле MAE. Ключевые особенности включают масштабируемую структуру Diffusion Transformer, специальные маскирующие токены для диффузионного процесса и пространственно-временное построение масок. Эксперименты показывают улучшение современных результатов в долгосрочном прогнозировании и генерации с нескольких ракурсов."
                },
                "en": {
                    "title": "Enhancing Autonomous Driving with MaskGWM: A New Era in World Models",
                    "desc": "This paper presents a new approach to creating world models for autonomous driving that can better predict environmental changes over time. The authors introduce MaskGWM, which combines advanced video generation techniques with feature-level context learning to enhance generalization. Key innovations include a scalable Diffusion Transformer architecture, the use of diffusion-related mask tokens, and an extension of mask construction to the spatial-temporal domain. Experimental results demonstrate that MaskGWM significantly outperforms existing models in various driving scenarios, showcasing its effectiveness in long-horizon and multi-view predictions."
                },
                "zh": {
                    "title": "提升驾驶模型的泛化能力",
                    "desc": "本文探讨了一种新的驾驶世界模型，旨在通过结合生成损失和MAE风格的特征级上下文学习来提高模型的泛化能力。我们提出了一种可扩展的扩散变换器结构，并引入了扩散相关的掩码标记，以处理掩码重建与生成扩散过程之间的模糊关系。此外，我们将掩码构建任务扩展到时空域，采用行级掩码进行偏移自注意力。实验结果表明，所提出的MaskGWM模型在多个标准数据集上显著提升了驾驶世界模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15007",
            "title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers",
            "url": "https://huggingface.co/papers/2502.15007",
            "abstract": "We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens -- especially stopwords, articles, and commas -- consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer's embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of filler tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.",
            "score": 32,
            "issue_id": 2367,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "9be244de8b366352",
            "authors": [
                "Anton Razzhigaev",
                "Matvey Mikhalchuk",
                "Temurbek Rahmatullaev",
                "Elizaveta Goncharova",
                "Polina Druzhinina",
                "Ivan Oseledets",
                "Andrey Kuznetsov"
            ],
            "affiliations": [
                "AIRI",
                "HSE University",
                "Lomonosov Moscow State University",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15007.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#architecture",
                    "#long_context",
                    "#interpretability",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Скрытая сила мелочей: как незаметные токены влияют на понимание контекста в LLM",
                    "desc": "Исследователи представили методы для измерения того, как большие языковые модели (LLM) кодируют и хранят контекстную информацию. Они обнаружили, что даже незначительные на первый взгляд токены (например, артикли и пунктуация) несут важную контекстную информацию. Удаление таких токенов приводит к снижению производительности модели на тестах MMLU и BABILong-4k. Авторы также выявили сильную корреляцию между контекстуализацией и линейностью в трансформациях между слоями модели."
                },
                "en": {
                    "title": "Unveiling the Hidden Power of Minor Tokens in LLMs",
                    "desc": "This paper explores how Large Language Models (LLMs) encode contextual information, highlighting the significant role of seemingly minor tokens like stopwords and punctuation. The authors demonstrate that removing these tokens negatively impacts model performance on tasks such as MMLU and BABILong-4k, indicating their importance in maintaining context. They also find a correlation between the contextualization of tokens and the linearity of transformations between model layers, suggesting that the way information is processed is not purely linear. To aid further research, the authors introduce LLM-Microscope, a toolkit designed to analyze token-level nonlinearity and visualize the contributions of different layers in LLMs."
                },
                "zh": {
                    "title": "隐藏的重要性：填充标记在上下文中的关键作用",
                    "desc": "本文介绍了量化大型语言模型（LLMs）如何编码和存储上下文信息的方法。研究发现，通常被视为次要的标记（如限定词和标点符号）实际上承载着意想不到的高上下文信息。特别是，去除这些标记（尤其是停用词、冠词和逗号）会显著降低MMLU和BABILong-4k的性能。我们的分析还表明，上下文化与线性度之间存在强相关性，线性度衡量从一层嵌入到下一层的转换是否可以用单一线性映射来近似。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13449",
            "title": "Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model",
            "url": "https://huggingface.co/papers/2502.13449",
            "abstract": "Understanding molecules is key to understanding organisms and driving advances in drug discovery, requiring interdisciplinary knowledge across chemistry and biology. Although large molecular language models have achieved notable success in interpreting molecular structures, their instruction datasets are limited to the specific knowledge from task-oriented datasets and do not fully cover the fundamental characteristics of molecules, hindering their abilities as general-purpose molecular assistants. To address this issue, we propose Mol-LLaMA, a large molecular language model that grasps the general knowledge centered on molecules via multi-modal instruction tuning. To this end, we design key data types that encompass the fundamental features of molecules, incorporating essential knowledge from molecular structures. In addition, to improve understanding of molecular features, we introduce a module that integrates complementary information from different molecular encoders, leveraging the distinct advantages of different molecular representations. Our experimental results demonstrate that Mol-LLaMA is capable of comprehending the general features of molecules and generating relevant responses to users' queries with detailed explanations, implying its potential as a general-purpose assistant for molecular analysis.",
            "score": 31,
            "issue_id": 2363,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 февраля",
                "en": "February 19",
                "zh": "2月19日"
            },
            "hash": "e52b99ade1ae590a",
            "authors": [
                "Dongki Kim",
                "Wonbin Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "Korea Advanced Institute of Science and Technology (KAIST), Seoul, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13449.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#science",
                    "#agi",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Mol-LLaMA: Мультимодальная ЯМ для всестороннего анализа молекул",
                    "desc": "Статья представляет Mol-LLaMA - большую языковую модель для молекул, обученную на мультимодальных инструкциях. Модель охватывает фундаментальные характеристики молекул, включая знания из молекулярных структур. Авторы внедрили модуль, интегрирующий информацию из разных молекулярных энкодеров для улучшения понимания свойств молекул. Эксперименты показывают, что Mol-LLaMA способна понимать общие особенности молекул и генерировать релевантные ответы на запросы пользователей с подробными объяснениями."
                },
                "en": {
                    "title": "Mol-LLaMA: A General-Purpose Assistant for Molecular Understanding",
                    "desc": "This paper introduces Mol-LLaMA, a large molecular language model designed to enhance understanding of molecular structures for drug discovery. Unlike previous models that relied on limited task-specific datasets, Mol-LLaMA utilizes multi-modal instruction tuning to incorporate a broader range of fundamental molecular knowledge. The model integrates various molecular encoders to leverage their unique strengths, improving its ability to interpret molecular features. Experimental results show that Mol-LLaMA can effectively respond to user queries with detailed explanations, positioning it as a versatile tool for molecular analysis."
                },
                "zh": {
                    "title": "Mol-LLaMA：分子分析的通用助手",
                    "desc": "理解分子对于理解生物体和推动药物发现至关重要。虽然大型分子语言模型在解析分子结构方面取得了显著成功，但它们的训练数据集仅限于特定任务的知识，未能全面覆盖分子的基本特征。为了解决这个问题，我们提出了Mol-LLaMA，一个通过多模态指令调优来掌握分子中心的通用知识的大型分子语言模型。实验结果表明，Mol-LLaMA能够理解分子的普遍特征，并生成与用户查询相关的详细解释，显示出其作为分子分析通用助手的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14397",
            "title": "PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data",
            "url": "https://huggingface.co/papers/2502.14397",
            "abstract": "We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence. Additionally, the background must be preserved without distortion, and the artist's unique style must be captured efficiently from limited training data. These requirements are not addressed by previous methods that primarily focus on global style transfer or regional inpainting. The proposed method, PhotoDoodle, employs a two-stage training strategy. Initially, we train a general-purpose image editing model, OmniEditor, using large-scale data. Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques. To enhance consistency in the generated results, we introduce a positional encoding reuse mechanism. Additionally, we release a PhotoDoodle dataset featuring six high-quality styles. Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation.",
            "score": 28,
            "issue_id": 2364,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "7e08e4606569ffb5",
            "authors": [
                "Shijie Huang",
                "Yiren Song",
                "Yuxuan Zhang",
                "Hailong Guo",
                "Xueyin Wang",
                "Mike Zheng Shou",
                "Jiaming Liu"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Byte Dance",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "Tiamat"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14397.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "PhotoDoodle: Персонализированное художественное редактирование фотографий",
                    "desc": "PhotoDoodle - это новая система редактирования изображений, позволяющая художникам накладывать декоративные элементы на фотографии. Она решает проблемы реалистичного наложения, выравнивания перспективы и сохранения контекста. Метод использует двухэтапную стратегию обучения: сначала тренируется универсальная модель OmniEditor, затем она дообучается на небольшом наборе данных конкретного художника с помощью EditLoRA. Авторы также представили датасет PhotoDoodle с шестью высококачественными стилями."
                },
                "en": {
                    "title": "Seamless Artistic Integration with PhotoDoodle",
                    "desc": "PhotoDoodle is an innovative image editing framework that allows artists to add decorative elements to photographs while ensuring they blend seamlessly with the background. The challenge lies in maintaining realistic integration, perspective alignment, and preserving the original photo without distortion. To achieve this, PhotoDoodle uses a two-stage training approach, starting with a general image editing model and then fine-tuning it with a small dataset of artist-specific edits. The framework also introduces a positional encoding reuse mechanism to improve the consistency of the edited images, showcasing its effectiveness in customized artistic creation."
                },
                "zh": {
                    "title": "PhotoDoodle：艺术创作的新可能性",
                    "desc": "本文介绍了一种名为PhotoDoodle的新型图像编辑框架，旨在帮助艺术家在照片上叠加装饰元素。PhotoDoodle解决了图像合成中的多个挑战，包括元素与背景的无缝融合、透视对齐和上下文一致性。该方法采用两阶段训练策略，首先使用大规模数据训练通用图像编辑模型OmniEditor，然后通过小型艺术家策划的数据集进行微调，以捕捉独特的编辑风格。实验结果表明，PhotoDoodle在定制图像编辑方面表现出色，展示了艺术创作的新可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12084",
            "title": "VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues",
            "url": "https://huggingface.co/papers/2502.12084",
            "abstract": "Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM^2-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.",
            "score": 16,
            "issue_id": 2366,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "7af295c37b147c3a",
            "authors": [
                "Jianshu Zhang",
                "Dongyu Yao",
                "Renjie Pi",
                "Paul Pu Liang",
                "Yi R.",
                "Fung"
            ],
            "affiliations": [
                "CMU",
                "HKUST",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12084.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#interpretability",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Преодоление разрыва между машинным и человеческим зрением в задачах связывания визуальных признаков",
                    "desc": "Статья представляет VLM^2-Bench - набор тестов для оценки способности моделей визуального языка (VLM) связывать визуальные признаки. Исследование включает 9 подзадач и более 3000 тестовых примеров, оценивая восемь открытых VLM и GPT-4o. Результаты выявили значительный разрыв в производительности между моделями и людьми, где даже GPT-4o отстает на 34.80%. На основе полученных данных, авторы рекомендуют улучшить базовые визуальные возможности моделей, установить более четкие принципы интеграции языкового рассуждения в визуальные задачи и изменить парадигмы обучения в сторону развития способности моделей самостоятельно структурировать и выводить отношения между визуальными признаками."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Visual Linking in VLMs",
                    "desc": "This paper introduces VLM^2-Bench, a new benchmark aimed at evaluating the ability of vision-language models (VLMs) to visually link matching cues, which is essential for tasks like recognizing the same person in different images. The benchmark consists of 9 subtasks and over 3,000 test cases, providing a comprehensive assessment of VLM performance. The evaluation reveals significant challenges, with models like GPT-4o showing a 34.80% performance gap compared to human capabilities. The authors suggest improvements in visual processing, clearer integration of language reasoning, and a shift in training paradigms to enhance models' ability to understand and relate visual cues independently."
                },
                "zh": {
                    "title": "提升视觉语言模型的匹配能力",
                    "desc": "这篇论文探讨了视觉语言模型（VLMs）在视觉链接匹配线索方面的能力。研究者们提出了VLM^2-Bench基准，旨在评估这些模型在识别相同对象时的表现。通过对八个开源VLM和GPT-4o的全面评估，发现模型在链接视觉线索方面存在显著的性能差距，甚至GPT-4o比人类低34.80%。基于这些发现，作者建议增强模型的核心视觉能力，明确语言推理与视觉任务的整合原则，并推动视觉-文本训练范式的转变。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14922",
            "title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers",
            "url": "https://huggingface.co/papers/2502.14922",
            "abstract": "This paper identifies the misinterpretation of the context can be a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase \"10 dollars per kilo,\" LLMs might not recognize that \"per\" means \"for each,\" leading to calculation errors. We introduce a novel, post-training approach called **Stick to the Facts (SIFT)** to tackle this. SIFT leverages increasing inference-time compute to ground LLM reasoning in contexts. At the core of SIFT lies the *Sticker*, which is generated by the model itself to explicitly emphasize the key information within the context. Given the curated Sticker, SIFT generates two predictions -- one from the original query and one from the query augmented with the Sticker. If they differ, the Sticker is sequentially refined via *forward* optimization (to better align the extracted facts with the query) and *inverse* generation (to conform with the model's inherent tendencies) for more faithful reasoning outcomes. Studies across diverse models (from 3B to 100B+) and benchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements. Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to **85.67**%, establishing a new state-of-the-art in the open-source community. The code is available at https://github.com/zhijie-group/SIFT.",
            "score": 15,
            "issue_id": 2363,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 февраля",
                "en": "February 19",
                "zh": "2月19日"
            },
            "hash": "b5ab16112068f00f",
            "authors": [
                "Zihao Zeng",
                "Xuyao Huang",
                "Boxiu Li",
                "Zhijie Deng"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14922.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#inference",
                    "#math",
                    "#training",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SIFT: Новый метод повышения точности рассуждений языковых моделей",
                    "desc": "Статья представляет новый подход под названием SIFT (Stick to the Facts) для улучшения рассуждений больших языковых моделей (LLM). SIFT использует дополнительные вычисления во время вывода для более точной интерпретации контекста. Метод включает генерацию 'Sticker' - ключевой информации из контекста, и последующую оптимизацию рассуждений модели. Эксперименты показали значительное улучшение производительности различных LLM на нескольких бенчмарках."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Stick to the Facts (SIFT)",
                    "desc": "This paper addresses the problem of context misinterpretation in large language models (LLMs), which can lead to reasoning errors. It introduces a new post-training method called **Stick to the Facts (SIFT)** that enhances LLM reasoning by grounding it in context. SIFT utilizes a self-generated *Sticker* to highlight crucial information, allowing the model to produce two predictions for comparison. The method shows significant improvements in accuracy across various models and benchmarks, achieving a new state-of-the-art performance in the open-source community."
                },
                "zh": {
                    "title": "提升语言模型推理准确性的创新方法",
                    "desc": "这篇论文指出，大型语言模型在推理过程中对上下文的误解可能会导致显著问题。比如，在“每公斤10美元”这个短语中，模型可能无法正确理解“每”的意思，从而导致计算错误。为了解决这个问题，论文提出了一种新的后训练方法，称为**Stick to the Facts (SIFT)**，它通过增加推理时的计算量来增强模型的上下文推理能力。SIFT的核心是由模型生成的*Sticker*，它强调了上下文中的关键信息，从而提高推理的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15589",
            "title": "LightThinker: Thinking Step-by-Step Compression",
            "url": "https://huggingface.co/papers/2502.15589",
            "abstract": "Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance. Code will be released at https://github.com/zjunlp/LightThinker.",
            "score": 14,
            "issue_id": 2365,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "563a32fe7bab988e",
            "authors": [
                "Jintian Zhang",
                "Yuqi Zhu",
                "Mengshu Sun",
                "Yujie Luo",
                "Shuofei Qiao",
                "Lun Du",
                "Da Zheng",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15589.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#reasoning",
                    "#long_context",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LightThinker: Эффективное сжатие мыслей для ускорения работы языковых моделей",
                    "desc": "LightThinker - это новый метод, позволяющий большим языковым моделям (LLM) динамически сжимать промежуточные мысли во время рассуждений. Этот подход вдохновлен когнитивными процессами человека и сжимает подробные шаги рассуждений в компактные представления, значительно сокращая количество токенов в контекстном окне. LightThinker обучается тому, когда и как выполнять сжатие, отображая скрытые состояния в сжатые токены-суть и создавая специализированные маски внимания. Эксперименты показывают, что метод снижает пиковое использование памяти и время вывода, сохраняя при этом конкурентоспособную точность."
                },
                "en": {
                    "title": "LightThinker: Efficient Reasoning through Dynamic Thought Compression",
                    "desc": "This paper introduces LightThinker, a method designed to enhance the efficiency of large language models (LLMs) during complex reasoning tasks. By dynamically compressing intermediate thoughts, LightThinker reduces the memory and computational costs associated with generating lengthy tokens. The approach mimics human cognitive processes by transforming verbose reasoning into compact representations, which helps in minimizing the number of tokens stored. The authors also present a new metric, Dependency (Dep), to measure the effectiveness of this compression, demonstrating that LightThinker can lower memory usage and inference time while maintaining accuracy across various datasets."
                },
                "zh": {
                    "title": "LightThinker：提升大型语言模型推理效率的新方法",
                    "desc": "大型语言模型（LLMs）在复杂推理任务中表现出色，但生成长文本时的内存和计算成本较高。本文提出了一种新方法LightThinker，能够在推理过程中动态压缩中间思维。LightThinker借鉴人类认知过程，将冗长的思维步骤压缩为紧凑的表示，从而显著减少上下文窗口中存储的标记数量。通过在数据构建中训练模型进行压缩，并引入依赖度（Dep）指标来量化压缩程度，我们的实验表明LightThinker在保持准确性的同时，降低了峰值内存使用和推理时间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14494",
            "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following",
            "url": "https://huggingface.co/papers/2502.14494",
            "abstract": "Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependency between dialogue turns that distinguishes multi-turn from single-turn interactions. This structural dependency not only reflects user intent but also establishes a second dimension for instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark innovatively defines a structural flow framework comprising six fundamental inter-turn relationships, which not only introduces novel structural constraints for model evaluation but also serves as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at https://github.com/MLGroupJLU/StructFlowBench.",
            "score": 11,
            "issue_id": 2365,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "512d952f8463678a",
            "authors": [
                "Jinnan Li",
                "Jinzhe Li",
                "Yue Wang",
                "Yi Chang",
                "Yuan Wu"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Jilin University",
                "Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China",
                "International Center of Future Science, Jilin University",
                "School of Artificial Intelligence, Jilin University",
                "School of Information and Library Science, University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14494.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Новый подход к оценке LLM в многоходовых диалогах",
                    "desc": "Статья представляет новый бенчмарк StructFlowBench для оценки способности больших языковых моделей (LLM) следовать многоходовым инструкциям. В отличие от существующих бенчмарков, StructFlowBench учитывает структурную зависимость между репликами диалога. Авторы определяют шесть фундаментальных межходовых отношений для моделирования структурного потока диалога. Результаты оценки 13 ведущих LLM показывают значительные недостатки в понимании моделями структуры многоходовых диалогов."
                },
                "en": {
                    "title": "Enhancing Multi-Turn Dialogue Understanding in LLMs",
                    "desc": "This paper introduces StructFlowBench, a new benchmark designed to evaluate the multi-turn instruction following capabilities of large language models (LLMs). It highlights the importance of understanding the structural dependencies between dialogue turns, which are often ignored in existing benchmarks. By defining six fundamental inter-turn relationships, StructFlowBench provides a framework for assessing how well models can follow instructions across multiple dialogue turns. The study reveals that many current LLMs struggle with these structural aspects, indicating a need for improvement in their dialogue comprehension."
                },
                "zh": {
                    "title": "多轮对话评估的新标准",
                    "desc": "本文提出了StructFlowBench，这是一个针对多轮指令跟随能力的基准测试，旨在填补现有评估方法的空白。现有的评估主要关注细粒度的约束满足和特定领域能力评估，但忽视了对话轮次之间的结构依赖关系。我们定义了一个结构流框架，包含六种基本的轮次关系，以此为模型评估引入新的结构约束。通过对13个领先的开源和闭源大型语言模型进行系统评估，实验结果显示当前模型在理解多轮对话结构方面存在显著不足。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15086",
            "title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models",
            "url": "https://huggingface.co/papers/2502.15086",
            "abstract": "As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SAFEBENCH, the first benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.",
            "score": 8,
            "issue_id": 2373,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "1dcbddc768d1e8df",
            "authors": [
                "Yeonjun In",
                "Wonjoong Kim",
                "Kanghoon Yoon",
                "Sungchul Kim",
                "Mehrab Tanjim",
                "Kibum Kim",
                "Chanyoung Park"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15086.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#security",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Безопасность LLM: учет индивидуальных стандартов пользователей",
                    "desc": "Исследователи представили U-SAFEBENCH - первый бенчмарк для оценки безопасности больших языковых моделей (LLM) с учетом пользовательских стандартов. Оценка 18 популярных LLM показала, что они не всегда действуют безопасно в контексте индивидуальных требований пользователей. Авторы предложили простое решение на основе метода цепочки рассуждений для улучшения пользовательской безопасности. Этот бенчмарк открывает новое направление в исследовании безопасности LLM с учетом индивидуальных особенностей пользователей."
                },
                "en": {
                    "title": "Enhancing LLM Safety with User-Specific Standards",
                    "desc": "This paper discusses the safety vulnerabilities of large language models (LLMs) and highlights the need for user-specific safety standards rather than relying on general benchmarks. It introduces U-SAFEBENCH, a new benchmark designed to evaluate how well LLMs adhere to these user-specific safety standards. The authors found that many popular LLMs do not perform safely when assessed against these tailored standards. To improve safety, they propose a chain-of-thought approach that effectively enhances user-specific safety in LLMs."
                },
                "zh": {
                    "title": "关注用户特定的LLM安全性",
                    "desc": "随着大型语言模型（LLM）代理的使用不断增加，它们的安全漏洞变得越来越明显。现有的评估方法主要依赖于通用标准，忽视了用户特定的安全标准。研究表明，LLM的安全标准可能因用户的不同而有所不同，而不是对所有用户都一致。为了解决这一问题，我们提出了U-SAFEBENCH，这是第一个旨在评估用户特定LLM安全性的基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14949",
            "title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding",
            "url": "https://huggingface.co/papers/2502.14949",
            "abstract": "With the growing adoption of Retrieval-Augmented Generation (RAG) in document processing, robust text recognition has become increasingly critical for knowledge extraction. While OCR (Optical Character Recognition) for English and other languages benefits from large datasets and well-established benchmarks, Arabic OCR faces unique challenges due to its cursive script, right-to-left text flow, and complex typographic and calligraphic features. We present KITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in current evaluation systems. Our benchmark comprises 8,809 samples across 9 major domains and 36 sub-domains, encompassing diverse document types including handwritten text, structured tables, and specialized coverage of 21 chart types for business intelligence. Our findings show that modern vision-language models (such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like EasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate (CER). Furthermore, we highlight significant limitations of current Arabic OCR models, particularly in PDF-to-Markdown conversion, where the best model Gemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in accurately recognizing Arabic text, including issues with complex fonts, numeral recognition errors, word elongation, and table structure detection. This work establishes a rigorous evaluation framework that can drive improvements in Arabic document analysis methods and bridge the performance gap with English OCR technologies.",
            "score": 5,
            "issue_id": 2371,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "f7184152f13a0145",
            "authors": [
                "Ahmed Heakl",
                "Abdullah Sohail",
                "Mukul Ranjan",
                "Rania Hossam",
                "Ghazi Ahmed",
                "Mohamed El-Geish",
                "Omar Maher",
                "Zhiqiang Shen",
                "Fahad Khan",
                "Salman Khan"
            ],
            "affiliations": [
                "Australian National University",
                "Linköping University",
                "MBZUAI",
                "Monta AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14949.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#low_resource",
                    "#dataset",
                    "#science",
                    "#optimization"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "KITAB-Bench: прорыв в оценке арабского OCR",
                    "desc": "Статья представляет KITAB-Bench - комплексный бенчмарк для оптического распознавания арабского текста (OCR). Он включает 8809 образцов из 9 основных доменов и 36 поддоменов, охватывая разнообразные типы документов. Исследование показывает, что современные мультимодальные языковые модели превосходят традиционные подходы к OCR на 60% по показателю ошибок на уровне символов (CER). Однако даже лучшие модели достигают лишь 65% точности при конвертации PDF в Markdown, что подчеркивает сложности в распознавании арабского текста."
                },
                "en": {
                    "title": "Bridging the Gap: Advancing Arabic OCR with KITAB-Bench",
                    "desc": "This paper introduces KITAB-Bench, a new benchmark specifically designed for evaluating Arabic Optical Character Recognition (OCR) systems. It addresses the unique challenges of Arabic text, such as its cursive nature and right-to-left orientation, which complicate accurate recognition. The study reveals that modern vision-language models significantly outperform traditional OCR methods in recognizing Arabic text, achieving a 60% improvement in Character Error Rate (CER). Additionally, it identifies key limitations in current Arabic OCR models, particularly in converting PDFs to Markdown, highlighting the need for enhanced techniques in Arabic document processing."
                },
                "zh": {
                    "title": "提升阿拉伯语OCR的基准与挑战",
                    "desc": "随着检索增强生成（RAG）在文档处理中的广泛应用，强大的文本识别变得愈发重要。阿拉伯语的光学字符识别（OCR）面临独特挑战，如连写字母、从右到左的文本流以及复杂的排版特征。我们提出了KITAB-Bench，这是一个全面的阿拉伯语OCR基准，涵盖了8809个样本，涉及9个主要领域和36个子领域。我们的研究表明，现代视觉语言模型在字符错误率（CER）上比传统OCR方法平均提高了60%，并强调了当前阿拉伯语OCR模型的显著局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15027",
            "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
            "url": "https://huggingface.co/papers/2502.15027",
            "abstract": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback.",
            "score": 4,
            "issue_id": 2363,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "3b32932adf862766",
            "authors": [
                "Henry Hengyuan Zhao",
                "Wenqi Pei",
                "Yifei Tao",
                "Haiyang Mei",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15027.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#rlhf",
                    "#dataset",
                    "#agi",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Интерактивный интеллект LMM: новые методы оценки и улучшения",
                    "desc": "Статья представляет новый фреймворк InterFeedback для оценки интерактивного интеллекта мультимодальных языковых моделей (LMM). Авторы разработали InterFeedback-Bench для тестирования 10 различных LMM на двух наборах данных. Также был создан набор данных InterFeedback-Human для ручного тестирования ведущих моделей. Результаты показывают, что даже лучшие LMM способны корректировать свои результаты на основе обратной связи менее чем в 50% случаев."
                },
                "en": {
                    "title": "Enhancing Interactive Intelligence in Large Multimodal Models",
                    "desc": "This paper addresses the gap in evaluating Large Multimodal Models (LMMs) regarding their interactive intelligence with human users. The authors introduce InterFeedback, a framework that autonomously assesses the interactive capabilities of any LMM across various datasets. They also present InterFeedback-Bench, which tests 10 open-source LMMs using two datasets, MMMU-Pro and MathVerse, and InterFeedback-Human, a dataset for manual evaluation of models like OpenAI-o1 and Claude-3.5-Sonnet. The results reveal that even advanced LMMs struggle to incorporate human feedback effectively, highlighting the need for improved methods to enhance their responsiveness to user interactions."
                },
                "zh": {
                    "title": "提升大型多模态模型的互动智能",
                    "desc": "现有的基准测试并未评估大型多模态模型（LMM）与人类用户的互动智能，而这对于开发通用人工智能助手至关重要。我们设计了InterFeedback，这是一个互动框架，可以应用于任何LMM和数据集，以自主评估这种能力。此外，我们推出了InterFeedback-Bench，使用两个代表性数据集MMM-Pro和MathVerse来评估10种不同的开源LMM的互动智能。我们的评估结果表明，即使是最先进的LMM（如OpenAI-o1）也只能通过人类反馈纠正不到50%的结果，显示出增强LMM解读和利用反馈能力的方法的必要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15422",
            "title": "Evaluating Multimodal Generative AI with Korean Educational Standards",
            "url": "https://huggingface.co/papers/2502.15422",
            "abstract": "This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models - open-source, open-access, and closed APIs - by examining difficulties, subject diversity, and human error rates. The code and dataset builder will be made fully open-sourced at https://github.com/naver-ai/KoNET.",
            "score": 3,
            "issue_id": 2372,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "f25bc188f9466ff0",
            "authors": [
                "Sanghee Park",
                "Geewook Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "NAVER Cloud AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15422.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "🇰🇷",
                "ru": {
                    "title": "KoNET: Корейский бенчмарк для оценки мультимодальных ИИ-систем в образовании",
                    "desc": "Статья представляет новый бенчмарк KoNET для оценки мультимодальных генеративных систем искусственного интеллекта на основе корейских национальных образовательных тестов. Бенчмарк включает четыре экзамена разных уровней образования, от начальной школы до университета. KoNET позволяет всесторонне анализировать производительность ИИ на корейском языке, что расширяет исследования в менее изученных языках. Авторы оценивают различные модели машинного обучения, изучая сложность заданий, разнообразие предметов и уровень человеческих ошибок."
                },
                "en": {
                    "title": "Evaluating AI Performance with Korean Educational Tests",
                    "desc": "This paper introduces the Korean National Educational Test Benchmark (KoNET), which is designed to assess the capabilities of Multimodal Generative AI Systems using standardized Korean educational tests. KoNET includes four distinct exams that cover various educational stages, allowing for a thorough evaluation of AI performance in a structured manner. The benchmark aims to provide valuable insights into how well AI models perform in the Korean language, which is often underrepresented in AI research. The study evaluates multiple models, including open-source and closed APIs, by analyzing their performance across different question types and levels of difficulty."
                },
                "zh": {
                    "title": "评估多模态AI的新基准：KoNET",
                    "desc": "本文介绍了韩国国家教育测试基准（KoNET），这是一个用于评估多模态生成AI系统的新基准，基于韩国的国家教育测试。KoNET包含四个考试：韩国小学综合教育发展测试（KoEGED）、中学（KoMGED）、高中（KoHGED）和大学入学能力测试（KoCSAT）。这些考试以其严格的标准和多样化的问题而闻名，能够全面分析AI在不同教育水平上的表现。通过关注韩语，KoNET为模型在较少探索的语言中的表现提供了深入的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14637",
            "title": "ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation",
            "url": "https://huggingface.co/papers/2502.14637",
            "abstract": "Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications. Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency. In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format. We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. Experiments show that ReQFlow achieves state-of-the-art performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37x faster than RFDiffusion and 62x faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency. The code is available at https://github.com/AngxiaoYue/ReQFlow.",
            "score": 3,
            "issue_id": 2371,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "2b9039b1aeecb0ab",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#healthcare",
                    "#inference",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "ReQFlow: Революция в скорости и качестве генерации белковых структур",
                    "desc": "Статья представляет новый метод под названием ReQFlow для быстрой и качественной генерации белковых остовов. Метод использует кватернионный поток для представления 3D-вращений и локальных трансляций для каждого остатка в белковой цепи. ReQFlow достигает наилучших результатов в генерации белковых остовов, требуя при этом значительно меньше шагов сэмплирования и времени вывода по сравнению с существующими методами. Эксперименты показывают, что ReQFlow работает в 37 раз быстрее RFDiffusion и в 62 раза быстрее Genie2 при генерации остова длиной 300 аминокислот."
                },
                "en": {
                    "title": "ReQFlow: Fast and Efficient Protein Backbone Generation",
                    "desc": "This paper introduces a new method called rectified quaternion flow (ReQFlow) for generating protein backbones, which is crucial for designing proteins from scratch. The authors address the limitations of existing generative models, which often produce proteins that are not optimal and are slow to compute. ReQFlow improves efficiency by using quaternion representations for 3D rotations and employs a technique called spherical linear interpolation (SLERP) to create smooth transitions in protein structures. The results show that ReQFlow outperforms previous models in both speed and quality, making it a significant advancement in the field of protein design."
                },
                "zh": {
                    "title": "高效蛋白质骨架生成的新方法",
                    "desc": "本研究提出了一种新颖的修正四元数流（ReQFlow）匹配方法，用于快速高质量的蛋白质骨架生成。该方法通过随机噪声为每个氨基酸残基生成局部平移和三维旋转，并使用单位四元数表示三维旋转。我们通过四元数流（QFlow）匹配训练模型，确保数值稳定性，并修正QFlow模型以加速推理和提高生成蛋白质骨架的设计性。实验结果表明，ReQFlow在蛋白质骨架生成方面表现出色，推理速度显著提高，效率远超现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13189",
            "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
            "url": "https://huggingface.co/papers/2502.13189",
            "abstract": "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.   In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.",
            "score": 3,
            "issue_id": 2370,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "a2a79c273e7a0f43",
            "authors": [
                "Enzhe Lu",
                "Zhejun Jiang",
                "Jingyuan Liu",
                "Yulun Du",
                "Tao Jiang",
                "Chao Hong",
                "Shaowei Liu",
                "Weiran He",
                "Enming Yuan",
                "Yuzhi Wang",
                "Zhiqi Huang",
                "Huan Yuan",
                "Suting Xu",
                "Xinran Xu",
                "Guokun Lai",
                "Yanru Chen",
                "Huabin Zheng",
                "Junjie Yan",
                "Jianlin Su",
                "Yuxin Wu",
                "Neo Y. Zhang",
                "Zhilin Yang",
                "Xinyu Zhou",
                "Mingxing Zhang",
                "Jiezhong Qiu"
            ],
            "affiliations": [
                "Moonshot AI",
                "Tsinghua University",
                "Zhejiang Lab/Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13189.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#long_context",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MoBA: Эффективное внимание для LLM без компромиссов",
                    "desc": "Статья представляет новый подход к решению проблемы масштабирования контекстной длины в больших языковых моделях (LLM). Авторы предлагают метод Mixture of Block Attention (MoBA), который применяет принципы Mixture of Experts к механизму внимания. MoBA позволяет модели самостоятельно определять, на что обращать внимание, без предопределенных структур. Этот подход демонстрирует превосходную производительность на задачах с длинным контекстом и обеспечивает плавный переход между полным и разреженным вниманием."
                },
                "en": {
                    "title": "Empowering LLMs with Efficient Context Scaling through MoBA",
                    "desc": "This paper addresses the challenge of scaling context length in large language models (LLMs) while managing computational complexity. Traditional attention mechanisms face a quadratic increase in resource demands, which can hinder performance. The authors propose Mixture of Block Attention (MoBA), a new method that allows models to autonomously determine their attention focus without imposing rigid structures. MoBA combines the principles of Mixture of Experts with attention, enabling efficient processing of long contexts and improving performance on complex reasoning tasks."
                },
                "zh": {
                    "title": "提升长上下文处理效率的新方法",
                    "desc": "本文提出了一种新的注意力机制，称为混合块注意力（MoBA），旨在提高大型语言模型（LLMs）在处理长上下文时的效率。传统的注意力机制在计算复杂度上呈现二次增长，限制了模型的扩展性。MoBA遵循“少结构”原则，使模型能够自主决定关注的内容，而不是依赖于预定义的偏见。该方法在长上下文任务中表现出色，并能够在全注意力和稀疏注意力之间无缝切换，提升了计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15657",
            "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
            "url": "https://huggingface.co/papers/2502.15657",
            "abstract": "The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path.",
            "score": 3,
            "issue_id": 2365,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "66434dc15bcc1913",
            "authors": [
                "Yoshua Bengio",
                "Michael Cohen",
                "Damiano Fornasiere",
                "Joumana Ghosn",
                "Pietro Greiner",
                "Matt MacDermott",
                "Sören Mindermann",
                "Adam Oberman",
                "Jesse Richardson",
                "Oliver Richardson",
                "Marc-Antoine Rondeau",
                "Pierre-Luc St-Charles",
                "David Williams-King"
            ],
            "affiliations": [
                "Imperial College London",
                "McGill University",
                "Mila Quebec AI Institute",
                "Universite de Montreal",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15657.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#agents",
                    "#security",
                    "#science",
                    "#agi",
                    "#ethics"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Безопасный ИИ: от агентов к ученым",
                    "desc": "В статье обсуждаются риски, связанные с разработкой генералистических ИИ-агентов, способных автономно планировать и действовать. Авторы предлагают альтернативу в виде неагентной системы ИИ под названием Scientist AI, которая объясняет мир на основе наблюдений, а не действует в нем. Система состоит из модели мира, генерирующей теории для объяснения данных, и механизма вывода для ответов на вопросы. Предлагается использовать Scientist AI для ускорения научного прогресса и в качестве защиты от потенциально опасных ИИ-агентов."
                },
                "en": {
                    "title": "Towards Safer AI: Embracing Non-Agentic Systems",
                    "desc": "This paper discusses the risks associated with developing generalist AI agents that can operate autonomously across various tasks. It highlights how current AI training methods can lead to unintended behaviors, such as deception or pursuing harmful goals. To address these concerns, the authors propose a new approach called Scientist AI, which focuses on creating a non-agentic system that explains observations rather than taking actions. This system aims to assist human researchers while ensuring safety and trustworthiness, ultimately promoting a safer path for AI development."
                },
                "zh": {
                    "title": "安全与创新并重的科学家AI",
                    "desc": "本文讨论了当前人工智能（AI）系统在自主规划和执行任务方面的风险，尤其是当这些系统可能偏离人类的意图时。我们提出了一种新的非代理AI系统，称为科学家AI，它旨在通过观察来解释世界，而不是主动采取行动。科学家AI包括一个世界模型和一个问答推理机器，能够处理不确定性，从而减少过于自信的预测带来的风险。我们希望这种安全的AI系统能够帮助人类研究人员加速科学进步，同时作为对抗潜在危险AI代理的保护措施。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15681",
            "title": "One-step Diffusion Models with $f$-Divergence Distribution Matching",
            "url": "https://huggingface.co/papers/2502.15681",
            "abstract": "Sampling from diffusion models involves a slow iterative process that hinders their practical deployment, especially for interactive applications. To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step student generator via variational score distillation, which matches the distribution of samples generated by the student to the teacher's distribution. However, these approaches use the reverse Kullback-Leibler (KL) divergence for distribution matching which is known to be mode seeking. In this paper, we generalize the distribution matching approach using a novel f-divergence minimization framework, termed f-distill, that covers different divergences with different trade-offs in terms of mode coverage and training variance. We derive the gradient of the f-divergence between the teacher and student distributions and show that it is expressed as the product of their score differences and a weighting function determined by their density ratio. This weighting function naturally emphasizes samples with higher density in the teacher distribution, when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the reverse-KL divergence is a special case within our framework. Empirically, we demonstrate that alternative f-divergences, such as forward-KL and Jensen-Shannon divergences, outperform the current best variational score distillation methods across image generation tasks. In particular, when using Jensen-Shannon divergence, f-distill achieves current state-of-the-art one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO. Project page: https://research.nvidia.com/labs/genair/f-distill",
            "score": 3,
            "issue_id": 2364,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "b1dfe60e5b59d586",
            "authors": [
                "Yilun Xu",
                "Weili Nie",
                "Arash Vahdat"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15681.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Ускорение диффузионных моделей через минимизацию f-дивергенции",
                    "desc": "Статья представляет новый метод ускорения генерации изображений с помощью диффузионных моделей, называемый f-distill. Авторы обобщают подход сопоставления распределений, используя минимизацию f-дивергенции вместо обратной дивергенции Кульбака-Лейблера. Это позволяет достичь лучшего баланса между покрытием мод и дисперсией обучения. Эмпирически показано, что f-distill превосходит существующие методы вариационной дистилляции оценок на задачах генерации изображений."
                },
                "en": {
                    "title": "Accelerating Diffusion Models with f-Distill for Faster Image Generation",
                    "desc": "This paper addresses the slow sampling process of diffusion models, which limits their use in real-time applications. It introduces a new method called f-distill that generalizes the distribution matching process using f-divergence minimization, allowing for better trade-offs between mode coverage and training variance. The authors derive a gradient expression for f-divergence that highlights samples with higher density in the teacher distribution, improving the quality of generated samples. Empirical results show that using f-distill with alternative divergences, particularly Jensen-Shannon divergence, leads to superior performance in image generation tasks compared to existing methods."
                },
                "zh": {
                    "title": "加速扩散模型生成的新方法",
                    "desc": "本文提出了一种新的分布匹配方法，称为f-distill，旨在加速扩散模型的生成速度。通过引入f-散度最小化框架，f-distill能够处理不同的散度，从而在模式覆盖和训练方差之间取得不同的权衡。我们推导了教师和学生分布之间f-散度的梯度，并展示了其与分数差异和密度比的加权函数的关系。实验结果表明，使用Jensen-Shannon散度的f-distill在图像生成任务中表现优于现有的变分分数蒸馏方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15631",
            "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
            "url": "https://huggingface.co/papers/2502.15631",
            "abstract": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies.",
            "score": 3,
            "issue_id": 2363,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 февраля",
                "en": "February 21",
                "zh": "2月21日"
            },
            "hash": "6f204197089fd2e2",
            "authors": [
                "Marthe Ballon",
                "Andres Algaba",
                "Vincent Ginis"
            ],
            "affiliations": [
                "Data Analytics Lab, Vrije Universiteit Brussel, 1050 Brussel, Belgium",
                "School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts 02138, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15631.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективность рассуждений важнее их длины в языковых моделях",
                    "desc": "Статья анализирует взаимосвязь между длиной цепочки рассуждений и точностью в больших языковых моделях при решении математических задач. Исследователи обнаружили, что более новые модели достигают лучших результатов без увеличения длины рассуждений. Интересно, что точность обычно снижается при удлинении цепочки рассуждений, но это падение меньше у более совершенных моделей. Результаты показывают, что новые поколения моделей эффективнее используют вычислительные ресурсы во время вывода."
                },
                "en": {
                    "title": "Efficiency Over Length: Unlocking Better Reasoning in Language Models",
                    "desc": "This paper investigates how the length of reasoning chains in large language models affects their accuracy in mathematical reasoning tasks. It compares two model variants, o1-mini and o3-mini, on the Omni-MATH benchmark, revealing that o3-mini achieves better accuracy without needing longer reasoning chains. The study finds that longer reasoning chains often lead to decreased accuracy, especially across various models and question difficulties. Additionally, it suggests that newer models are more efficient in using reasoning tokens, which has important implications for model evaluation and scaling."
                },
                "zh": {
                    "title": "推理链长度与模型准确性的关系",
                    "desc": "这篇论文探讨了大型语言模型在数学推理中的表现，特别是推理链的长度与准确性之间的关系。研究发现，o3-mini模型在不需要更长推理链的情况下，能够实现更高的准确率。论文还指出，随着推理链的增长，所有模型的准确性普遍下降，但在更高效的模型中，这种下降的幅度较小。最后，尽管o3-mini(h)在准确性上略有提升，但它需要分配更多的推理标记，这表明新一代模型在测试时的计算效率更高。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14905",
            "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence",
            "url": "https://huggingface.co/papers/2502.14905",
            "abstract": "In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning dataset construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.",
            "score": 2,
            "issue_id": 2363,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "e6a1c222ee43df08",
            "authors": [
                "Bhavik Agarwal",
                "Ishan Joshi",
                "Viktoria Rojkova"
            ],
            "affiliations": [
                "MasterControl AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14905.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#synthetic",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение LLM для генерации структурированного текста",
                    "desc": "В этой статье представлен метод обеспечения строгого соблюдения схемы при генерации текста большими языковыми моделями (LLM) с использованием их способностей к рассуждению. Авторы разработали подход на основе фреймворка DeepSeek R1, обучая модель с 1,5 миллиардами параметров структурированному рассуждению через новый конвейер, сочетающий синтетические наборы данных и пользовательские функции вознаграждения в рамках Group Relative Policy Optimization (GRPO). Несмотря на относительно скромный объем обучения, модель демонстрирует надежную производительность в обеспечении согласованности схемы. Результаты подчеркивают практическую полезность ресурсоэффективного фреймворка для генерации текста с ограничениями по схеме."
                },
                "en": {
                    "title": "ThinkJSON: Efficient Schema-Constrained Text Generation with LLMs",
                    "desc": "This paper presents a method to improve how large language models (LLMs) follow specific rules or schemas when generating text. The authors use a reinforcement learning approach called DeepSeek R1 to train a 1.5 billion parameter model, enhancing its reasoning skills through a combination of synthetic data and custom reward systems. They first establish core reasoning abilities with a large unstructured dataset and then fine-tune the model on a smaller dataset focused on schema adherence. The results show that their method, named ThinkJSON, performs well in maintaining schema consistency compared to larger models, demonstrating an efficient way to generate structured text."
                },
                "zh": {
                    "title": "利用推理能力强化模式遵循",
                    "desc": "本文探讨了在大型语言模型生成中强制遵循严格模式的挑战，利用了语言模型的推理能力。我们基于DeepSeek R1强化学习框架，训练了一个具有15亿参数的模型，通过合成推理数据集构建和自定义奖励函数，结合群体相对策略优化（GRPO）的方法。首先，我们在一个2万样本的非结构化到结构化数据集上进行R1强化学习，以建立核心推理能力。随后，我们在一个单独的1万推理样本数据集上进行了监督微调，专注于提高下游任务的模式遵循性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15011",
            "title": "CrossOver: 3D Scene Cross-Modal Alignment",
            "url": "https://huggingface.co/papers/2502.15011",
            "abstract": "Multi-modal 3D object understanding has gained significant attention, yet current approaches often assume complete data availability and rigid alignment across all modalities. We present CrossOver, a novel framework for cross-modal 3D scene understanding via flexible, scene-level modality alignment. Unlike traditional methods that require aligned modality data for every object instance, CrossOver learns a unified, modality-agnostic embedding space for scenes by aligning modalities - RGB images, point clouds, CAD models, floorplans, and text descriptions - with relaxed constraints and without explicit object semantics. Leveraging dimensionality-specific encoders, a multi-stage training pipeline, and emergent cross-modal behaviors, CrossOver supports robust scene retrieval and object localization, even with missing modalities. Evaluations on ScanNet and 3RScan datasets show its superior performance across diverse metrics, highlighting adaptability for real-world applications in 3D scene understanding.",
            "score": 1,
            "issue_id": 2366,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "97fe61eb66853ca2",
            "authors": [
                "Sayan Deb Sarkar",
                "Ondrej Miksik",
                "Marc Pollefeys",
                "Daniel Barath",
                "Iro Armeni"
            ],
            "affiliations": [
                "ETH Zurich",
                "Microsoft Spatial AI Lab, Zurich",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15011.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#3d",
                    "#training"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Гибкое объединение модальностей для понимания 3D-сцен",
                    "desc": "CrossOver - это новая система для кросс-модального понимания 3D-сцен, использующая гибкое выравнивание модальностей на уровне сцены. Она создает единое модально-агностическое пространство представлений для сцен, объединяя различные модальности без жестких ограничений. CrossOver использует специфические для разных размерностей энкодеры и многоступенчатый процесс обучения. Система демонстрирует высокую эффективность в задачах поиска сцен и локализации объектов даже при отсутствии некоторых модальностей."
                },
                "en": {
                    "title": "CrossOver: Flexible 3D Scene Understanding Across Modalities",
                    "desc": "This paper introduces CrossOver, a new framework designed for understanding 3D scenes using multiple types of data, such as images and point clouds. Unlike traditional methods that need all data types to be perfectly aligned, CrossOver allows for flexible alignment, making it easier to work with incomplete information. It creates a shared space where different data types can be compared and understood together, even if some data is missing. The results show that CrossOver performs better than existing methods, making it useful for real-world applications in 3D scene analysis."
                },
                "zh": {
                    "title": "跨模态3D场景理解的新突破",
                    "desc": "本论文提出了一种名为CrossOver的新框架，用于灵活的跨模态3D场景理解。与传统方法不同，CrossOver不需要每个对象实例的模态数据严格对齐，而是通过放宽约束来学习统一的模态无关嵌入空间。该框架支持RGB图像、点云、CAD模型、平面图和文本描述等多种模态的对齐，能够在缺失模态的情况下进行稳健的场景检索和对象定位。实验结果表明，CrossOver在ScanNet和3RScan数据集上表现优越，展示了其在实际3D场景理解中的适应性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2403.12959",
            "title": "WHAC: World-grounded Humans and Cameras",
            "url": "https://huggingface.co/papers/2403.12959",
            "abstract": "Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories. Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework. We will make the code and dataset publicly available.",
            "score": 0,
            "issue_id": 2373,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 марта",
                "en": "March 19",
                "zh": "3月19日"
            },
            "hash": "2fcde4d4c9ef8b28",
            "authors": [
                "Wanqi Yin",
                "Zhongang Cai",
                "Ruisi Wang",
                "Fanzhou Wang",
                "Chen Wei",
                "Haiyi Mei",
                "Weiye Xiao",
                "Zhitao Yang",
                "Qingping Sun",
                "Atsushi Yamashita",
                "Ziwei Liu",
                "Lei Yang"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2403.12959.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Синергия мира, человека и камеры для точной оценки 3D-поз",
                    "desc": "Статья представляет новый подход к оценке траекторий человека и камеры в мировой системе координат по монокулярному видео. Авторы предлагают фреймворк WHAC, который позволяет одновременно оценивать выразительные параметрические модели человека (SMPL-X) и позы камеры без использования традиционных методов оптимизации. Ключевая идея заключается в использовании синергии между миром, человеком и камерой, а также в учете абсолютных пространственных подсказок, предоставляемых движениями человека. Также авторы представляют новый синтетический датасет WHAC-A-Mole с точно аннотированными людьми и камерами для оценки эффективности предложенного метода."
                },
                "en": {
                    "title": "Jointly Estimating Human and Camera Trajectories with WHAC",
                    "desc": "This paper addresses the challenge of estimating human and camera trajectories from a single video while maintaining accurate scale in the world coordinate system. The authors propose a novel framework called WHAC, which simultaneously estimates expressive human models and camera poses by leveraging the relationship between the world, human, and camera. They utilize the depth information obtained from camera-frame SMPL-X estimation and the spatial cues provided by human motions to enhance the accuracy of their model. Additionally, they introduce a new synthetic dataset, WHAC-A-Mole, which includes detailed annotations for training and testing their approach, demonstrating its effectiveness through extensive experiments."
                },
                "zh": {
                    "title": "从单目视频中精准估计人类与相机轨迹",
                    "desc": "本研究旨在从单目视频中准确估计人类和相机的轨迹，解决这一复杂问题。我们提出了一种新框架WHAC，通过结合世界、人体和相机之间的相互作用，来共同恢复人类模型和相机姿态。该方法利用相机帧SMPL-X估计技术和人类运动提供的空间线索，避免了传统的优化技术。我们还创建了一个新的合成数据集WHAC-A-Mole，包含准确标注的人类和相机，展示了多样的互动人类动作和真实的相机轨迹。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14975",
            "title": "Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries",
            "url": "https://huggingface.co/papers/2502.14975",
            "abstract": "We present an open-source benchmark and evaluation framework for assessing emotional boundary handling in Large Language Models (LLMs). Using a dataset of 1156 prompts across six languages, we evaluated three leading LLMs (GPT-4o, Claude-3.5 Sonnet, and Mistral-large) on their ability to maintain appropriate emotional boundaries through pattern-matched response analysis. Our framework quantifies responses across seven key patterns: direct refusal, apology, explanation, deflection, acknowledgment, boundary setting, and emotional awareness. Results demonstrate significant variation in boundary-handling approaches, with Claude-3.5 achieving the highest overall score (8.69/10) and producing longer, more nuanced responses (86.51 words on average). We identified a substantial performance gap between English (average score 25.62) and non-English interactions (< 0.22), with English responses showing markedly higher refusal rates (43.20% vs. < 1% for non-English). Pattern analysis revealed model-specific strategies, such as Mistral's preference for deflection (4.2%) and consistently low empathy scores across all models (< 0.06). Limitations include potential oversimplification through pattern matching, lack of contextual understanding in response analysis, and binary classification of complex emotional responses. Future work should explore more nuanced scoring methods, expand language coverage, and investigate cultural variations in emotional boundary expectations. Our benchmark and methodology provide a foundation for systematic evaluation of LLM emotional intelligence and boundary-setting capabilities.",
            "score": 0,
            "issue_id": 2372,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "453adb62252fb78b",
            "authors": [
                "David Noever",
                "Grant Rosario"
            ],
            "affiliations": [
                "PeopleTec, Inc., Huntsville, AL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14975.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#open_source",
                    "#multilingual",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Измеряем эмоциональный интеллект искусственного интеллекта",
                    "desc": "Представлен открытый фреймворк для оценки способности больших языковых моделей (LLM) устанавливать эмоциональные границы. Три ведущие модели (GPT-4, Claude-3.5 и Mistral-large) были протестированы на наборе из 1156 промптов на шести языках. Анализ ответов проводился по семи ключевым паттернам, включая прямой отказ, извинение и установку границ. Результаты показали значительные различия между моделями и языками, при этом Claude-3.5 показала лучший общий результат."
                },
                "en": {
                    "title": "Evaluating Emotional Intelligence in Language Models",
                    "desc": "This paper introduces an open-source benchmark designed to evaluate how well Large Language Models (LLMs) manage emotional boundaries in their responses. The study analyzed 1156 prompts in six languages, focusing on three prominent LLMs: GPT-4o, Claude-3.5 Sonnet, and Mistral-large. The evaluation framework measures responses based on seven emotional handling patterns, revealing that Claude-3.5 performed best overall, while significant performance disparities were noted between English and non-English interactions. The findings highlight the need for improved methods to assess emotional intelligence in LLMs, suggesting future research should address cultural differences and enhance scoring techniques."
                },
                "zh": {
                    "title": "评估大型语言模型的情感边界处理能力",
                    "desc": "本文提出了一个开源基准和评估框架，用于评估大型语言模型（LLMs）在情感边界处理方面的表现。我们使用了一个包含1156个提示的多语言数据集，评估了三种领先的LLM（GPT-4o、Claude-3.5 Sonnet和Mistral-large）在保持适当情感边界方面的能力。结果显示，不同模型在边界处理方法上存在显著差异，Claude-3.5获得了最高分（8.69/10），并产生了更长、更细致的回应。我们还发现英语和非英语互动之间存在显著的表现差距，未来的研究应探索更细致的评分方法和文化差异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13407",
            "title": "JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework",
            "url": "https://huggingface.co/papers/2502.13407",
            "abstract": "Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5 to 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at https://github.com/circleLZY/MTKD-CD.",
            "score": 0,
            "issue_id": 2370,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 февраля",
                "en": "February 19",
                "zh": "2月19日"
            },
            "hash": "5a5fa5fc17c92e51",
            "authors": [
                "Ziyuan Liu",
                "Ruifei Zhu",
                "Long Gao",
                "Yuanxiu Zhou",
                "Jingyu Ma",
                "Yuantao Gu"
            ],
            "affiliations": [
                "Chang Guang Satellite Technology Co., Ltd. (CGSTL) Changchun 130102, China",
                "Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13407.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#transfer_learning",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "Прорыв в обнаружении изменений на спутниковых снимках с помощью дистилляции знаний",
                    "desc": "В статье представлен новый набор данных JL1-CD для задачи обнаружения изменений на спутниковых снимках высокого разрешения. Авторы также предлагают фреймворк многоучительной дистилляции знаний (MTKD) для улучшения результатов обнаружения изменений. Эксперименты показывают, что MTKD значительно повышает производительность моделей с различными архитектурами нейронных сетей. Достигнуты новые передовые результаты на наборах данных JL1-CD и SYSU-CD."
                },
                "en": {
                    "title": "Enhancing Change Detection with JL1-CD and MTKD Framework",
                    "desc": "This paper addresses challenges in remote sensing image change detection (CD) by introducing the JL1-CD dataset, which consists of 5,000 pairs of high-resolution images. The dataset aims to provide a comprehensive resource for training models, overcoming the issue of limited open-source datasets. Additionally, the authors propose a multi-teacher knowledge distillation (MTKD) framework that enhances the performance of CD models across different architectures. Experimental results show that this framework achieves state-of-the-art performance on both the JL1-CD and SYSU-CD datasets."
                },
                "zh": {
                    "title": "提升遥感变化检测的新方法",
                    "desc": "深度学习在遥感图像变化检测领域取得了显著成功，但仍面临两个主要挑战：缺乏亚米级、全面的开源变化检测数据集，以及在变化区域不同的图像中实现一致且令人满意的检测结果的困难。为了解决这些问题，我们引入了JL1-CD数据集，包含5000对512 x 512像素的图像，分辨率为0.5到0.75米。此外，我们提出了一种多教师知识蒸馏（MTKD）框架用于变化检测。实验结果表明，MTKD框架显著提高了各种网络架构和参数规模的变化检测模型的性能，达到了新的最先进结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15082",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "url": "https://huggingface.co/papers/2502.15082",
            "abstract": "User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or \"forgetting\" a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.",
            "score": 0,
            "issue_id": 2365,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 февраля",
                "en": "February 20",
                "zh": "2月20日"
            },
            "hash": "4a0bc63404dc5d71",
            "authors": [
                "Vaidehi Patil",
                "Elias Stengel-Eskin",
                "Mohit Bansal"
            ],
            "affiliations": [
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15082.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#leakage",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное забывание без потери знаний",
                    "desc": "Статья описывает метод UPCORE для улучшения процесса удаления информации из обученных языковых моделей. UPCORE минимизирует ухудшение работы модели после удаления данных путем выборочного отсечения выбросов из набора удаляемых данных. Метод применим к различным алгоритмам разобучения и позволяет достичь лучшего баланса между эффективностью удаления и сохранением производительности модели. Авторы также предлагают новую метрику на основе площади под кривой для оценки этого компромисса."
                },
                "en": {
                    "title": "Smart Unlearning: Balancing Deletion and Model Integrity with UPCORE",
                    "desc": "This paper addresses the challenge of unlearning information from pretrained models, particularly large language models, without significantly harming their performance. The authors introduce UPCORE, a framework that selects a subset of data points to delete while minimizing the negative impact on the model's overall capabilities. By focusing on the variance of model representations, UPCORE effectively prunes outliers from the forget set, leading to better retention of the model's utility. The framework is evaluated against standard unlearning methods, demonstrating improved performance in both deletion effectiveness and model preservation through a new area-under-the-curve metric."
                },
                "zh": {
                    "title": "UPCORE：平衡遗忘与模型保留的创新方法",
                    "desc": "在机器学习中，用户的要求常常需要从预训练模型中删除特定信息，这被称为“遗忘”。然而，删除数据点可能会影响模型在其他数据上的表现，因此需要在删除信息和保持模型能力之间找到平衡。为了解决这个问题，我们提出了UPCORE（实用性保持核心集选择），这是一种通用的数据选择框架，旨在减少遗忘过程中的模型损害。通过选择性地修剪遗忘集中的异常值，UPCORE能够在删除有效性和模型保留之间实现更好的平衡。"
                }
            }
        }
    ],
    "link_prev": "2025-02-21.html",
    "link_next": "2025-02-25.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "21.02",
        "en": "02/21",
        "zh": "2月21日"
    },
    "short_date_next": {
        "ru": "25.02",
        "en": "02/25",
        "zh": "2月25日"
    },
    "categories": {
        "#dataset": 12,
        "#data": 6,
        "#benchmark": 12,
        "#agents": 1,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 6,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 6,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 5,
        "#healthcare": 1,
        "#training": 11,
        "#robotics": 0,
        "#agi": 4,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 2,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 5,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 9,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 2
    },
    "zh": {
        "text": "这篇文章介绍了一种名为 SurveyX 的自动化问卷生成系统。SurveyX 通过引入在线参考检索、预处理方法 AttributeTree 和重新润色过程，显著提高了问卷生成的效果。实验结果表明，SurveyX 在内容质量和引用质量方面优于现有系统，接近人类专家的表现。文章还提到了 SurveyX 生成的问卷示例可以在 www.surveyx.cn 上找到。",
        "title": "SurveyX: Academic Survey Automation via Large Language Models",
        "pinyin": "这篇文章介绍了一种名为 SurveyX 的自动化问卷生成系统。SurveyX 通过引入在线参考检索、预处理方法 AttributeTree 和重新润色过程，显著提高了问卷生成的效果。实验结果表明，SurveyX 在内容质量和引用质量方面优于现有系统，接近人类专家的表现。文章还提到了 SurveyX 生成的问卷示例可以在 www.surveyx.cn 上找到。\n\nZhè piān wénzhāng jièshào le yīzhǒng míngwèi SurveyX de zìdònghuà wènjuàn shēngchéng xìtǒng. SurveyX tōngguò yǐnrù zàixiàn cānkǎo jiǎnsuǒ, yùchǔlǐ fāngfǎ AttributeTree hé chóngxīn rùnsè guòchéng, xiǎnzhù tīgāo le wènjuàn shēngchéng de xiàoguǒ. Shíyàn jiéguǒ biǎomíng, SurveyX zài nèiróng zhìliàng hé yǐnyòng zhìliàng fāngmiàn yōuyú xiànyǒu xìtǒng, jiējìn rénlèi zhuānjiā de biǎoxiàn. Wénzhāng hái tí dào le SurveyX shēngchéng de wènjuàn shìlì kěyǐ zài www.surveyx.cn shàng zhǎo dào.",
        "vocab": "[{'word': '自动化', 'pinyin': 'zìdònghuà', 'trans': 'automated'},\n{'word': '问卷', 'pinyin': 'wènjuàn', 'trans': 'questionnaire'},\n{'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'},\n{'word': '系统', 'pinyin': 'xìtǒng', 'trans': 'system'},\n{'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'},\n{'word': '在线', 'pinyin': 'zàixiàn', 'trans': 'online'},\n{'word': '参考', 'pinyin': 'cānkǎo', 'trans': 'reference'},\n{'word': '检索', 'pinyin': 'jiǎnsuǒ', 'trans': 'retrieval'},\n{'word': '预处理', 'pinyin': 'yùchǔlǐ', 'trans': 'preprocessing'},\n{'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'},\n{'word': 'AttributeTree', 'pinyin': '', 'trans': 'AttributeTree'},\n{'word': '润色', 'pinyin': 'rùnsè', 'trans': 'polish'},\n{'word': '过程', 'pinyin': 'guòchéng', 'trans': 'process'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'},\n{'word': '效果', 'pinyin': 'xiàoguǒ', 'trans': 'effect'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '表明', 'pinyin': 'biǎomíng', 'trans': 'indicate'},\n{'word': '内容', 'pinyin': 'nèiróng', 'trans': 'content'},\n{'word': '质量', 'pinyin': 'zhìliàng', 'trans': 'quality'},\n{'word': '引用', 'pinyin': 'yǐnyòng', 'trans': 'citation'},\n{'word': '现有', 'pinyin': 'xiànyǒu', 'trans': 'existing'},\n{'word': '接近', 'pinyin': 'jiējìn', 'trans': 'close to'},\n{'word': '人类', 'pinyin': 'rénlèi', 'trans': 'human'},\n{'word': '专家', 'pinyin': 'zhuānjiā', 'trans': 'expert'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '提到', 'pinyin': 'tídào', 'trans': 'mention'},\n{'word': '示例', 'pinyin': 'shìlì', 'trans': 'example'},\n{'word': '找到', 'pinyin': 'zhǎodào', 'trans': 'find'}]",
        "trans": "This article introduces an automated survey generation system called SurveyX. SurveyX significantly enhances the effectiveness of survey generation by incorporating online reference retrieval, the preprocessing method AttributeTree, and a refinement process. Experimental results indicate that SurveyX outperforms existing systems in terms of content quality and citation quality, approaching the performance of human experts. The article also mentions that examples of surveys generated by SurveyX can be found at www.surveyx.cn.",
        "update_ts": "2025-02-24 09:12"
    }
}