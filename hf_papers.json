{
    "date": {
        "ru": "4 ноября",
        "en": "November 4",
        "zh": "11月4日"
    },
    "time_utc": "2024-11-04 02:50",
    "weekday": 0,
    "issue_id": 407,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.00412",
            "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation",
            "url": "https://huggingface.co/papers/2411.00412",
            "abstract": "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but often produce hallucinations for complex ones. While integrating LLMs with tools can increase reliability, this approach typically results in over-reliance on tools, diminishing the model's ability to solve simple problems through basic reasoning. In contrast, human experts first assess problem complexity using domain knowledge before choosing an appropriate solution approach. Inspired by this human problem-solving process, we propose a novel two-component fine-tuning method. In the first component World Knowledge Distillation (WKD), LLMs learn directly from solutions generated using tool's information to internalize domain knowledge. In the second component Tool Usage Adaptation (TUA), we partition problems into easy and hard categories based on the model's direct answering accuracy. While maintaining the same alignment target for easy problems as in WKD, we train the model to intelligently switch to tool usage for more challenging problems. We validate our method on six scientific benchmark datasets, spanning mathematics, climate science and epidemiology. On average, our models demonstrate a 28.18% improvement in answer accuracy and a 13.89% increase in tool usage precision across all datasets, surpassing state-of-the-art models including GPT-4o and Claude-3.5.",
            "score": 2,
            "issue_id": 407,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 ноября",
                "en": "November 1",
                "zh": "11月1日"
            },
            "hash": "27e4deefc7d09df0",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умное переключение: как научить ИИ эффективно решать задачи разной сложности",
                    "desc": "Исследование посвящено улучшению способности больших языковых моделей (LLM) решать научные задачи. Авторы предлагают двухкомпонентный метод дообучения: дистилляция мировых знаний и адаптация использования инструментов. Этот подход позволяет LLM эффективно решать простые задачи с помощью базовых рассуждений, а для сложных - прибегать к инструментам. Метод показал значительное улучшение точности ответов и точности использования инструментов на шести научных наборах данных."
                },
                "en": {
                    "title": "Enhancing LLMs: Smart Tool Use for Complex Problems",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) in solving complex scientific problems, which often lead to inaccuracies or 'hallucinations'. The authors propose a two-component fine-tuning method that mimics human problem-solving strategies by first assessing problem complexity. The first component, World Knowledge Distillation (WKD), allows LLMs to learn from solutions that utilize external tools, while the second component, Tool Usage Adaptation (TUA), helps the model categorize problems as easy or hard and decide when to use tools. The proposed method shows significant improvements in accuracy and tool usage precision across various scientific datasets, outperforming existing models."
                },
                "zh": {
                    "title": "智能切换，提升模型解决问题的能力",
                    "desc": "大型语言模型（LLMs）在解决简单科学问题方面表现出色，但在复杂问题上常常出现幻觉。我们提出了一种新颖的双组件微调方法，模仿人类专家的解决问题过程。第一个组件是世界知识蒸馏（WKD），使LLMs从工具生成的解决方案中学习领域知识。第二个组件是工具使用适应（TUA），根据模型的直接回答准确性将问题分为简单和困难两类，从而提高模型在复杂问题上的工具使用能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.23775",
            "title": "In-Context LoRA for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2410.23775",
            "abstract": "Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20sim 100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA",
            "score": 1,
            "issue_id": 407,
            "pub_date": "2024-10-31",
            "pub_date_card": {
                "ru": "31 октября",
                "en": "October 31",
                "zh": "10月31日"
            },
            "hash": "748dab03a37a21a4",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Раскрытие скрытого потенциала DiT для многозадачной генерации изображений",
                    "desc": "Исследование предлагает новый подход к использованию диффузионных трансформеров (DiT) для генерации изображений в различных задачах. Авторы обнаружили, что существующие модели DiT для преобразования текста в изображение уже обладают способностью к контекстной генерации без дополнительной настройки. На основе этого наблюдения они разработали простой pipeline, включающий конкатенацию изображений, совместное создание подписей и применение LoRA-тюнинга на небольших датасетах. Предложенный метод, названный IC-LoRA, позволяет генерировать высококачественные наборы изображений, лучше соответствующие заданным промптам."
                },
                "en": {
                    "title": "Unlocking In-Context Generation with IC-LoRA",
                    "desc": "This paper investigates the use of diffusion transformers (DiTs) for generating images without being tied to specific tasks. The authors propose that DiTs can generate images effectively with minimal adjustments, leveraging their inherent in-context generation capabilities. They introduce a new method called In-Context LoRA (IC-LoRA), which simplifies the process by concatenating images and using joint captioning, along with small dataset tuning. This approach enhances the quality of generated images while maintaining a flexible architecture that can adapt to various tasks without extensive retraining."
                },
                "zh": {
                    "title": "激活上下文生成能力，提升图像生成质量",
                    "desc": "本研究探讨了扩散变换器（DiTs）在无任务特定的图像生成中的应用。我们提出，文本到图像的DiTs本身具备上下文生成能力，只需少量调整即可激活。通过实验，我们展示了现有的文本到图像DiTs能够在不调整的情况下有效进行上下文生成。我们提出了一种简单的流程，利用DiTs的上下文能力，生成高保真度的图像集，且不需要对原始模型进行修改。"
                }
            }
        }
    ],
    "link_prev": "2024-11-01.html",
    "link_next": "2024-11-05.html",
    "short_date_prev": {
        "ru": "01.11",
        "en": "11/01",
        "zh": "11月1日"
    },
    "short_date_next": {
        "ru": "05.11",
        "en": "11/05",
        "zh": "11月5日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "这篇文章讨论了稀疏自编码器（SAEs）在大语言模型（LLMs）中的应用。SAEs可以将不易解释的中间表示分解成易解释的特征，从而更好地控制和分析。然而，类似的分析和方法在文本到图像的模型中尚缺乏研究。作者研究了在少步文本到图像扩散模型（如SDXL Turbo）中使用SAEs学习可解释特征的可能性。结果发现，SAEs学到的特征可解释，并对生成过程产生因果影响，揭示了模型内部的专业化。具体来说，有一个模块主要处理图像组合，一个负责添加细节，另一个负责颜色、光照和风格。因此，这项工作是理解生成文本到图像模型内部的重要一步，展示了SAEs在视觉领域的潜力。",
        "title": "Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders",
        "pinyin": "这篇文章讨论了稀疏自编码器（SAEs）在大语言模型（LLMs）中的应用。SAEs可以将不易解释的中间表示分解成易解释的特征，从而更好地控制和分析。然而，类似的分析和方法在文本到图像的模型中尚缺乏研究。作者研究了在少步文本到图像扩散模型（如SDXL Turbo）中使用SAEs学习可解释特征的可能性。结果发现，SAEs学到的特征可解释，并对生成过程产生因果影响，揭示了模型内部的专业化。具体来说，有一个模块主要处理图像组合，一个负责添加细节，另一个负责颜色、光照和风格。因此，这项工作是理解生成文本到图像模型内部的重要一步，展示了SAEs在视觉领域的潜力。\n\nPinyin transcription:\n\nZhè piān wénzhāng tǎolùn le xīshū zìbiǎnmǎqì (SAEs) zài dà yǔyán móxíng (LLMs) zhōng de yìngyòng. SAEs kěyǐ jiāng bùyì jiěshì de zhōngjiān biǎoshì fēnjiě chéng yì jiěshì de tèzhēng, cóng'ér gèng hǎo de kòngzhì hé fēnxī. Rán'ér, lèisì de fēnxī hé fāngfǎ zài wénběn dào túxiàng de móxíng zhōng shàng quēfá yánjiū. Zuòzhě yánjiū le zài shǎo bù wénběn dào túxiàng kuòsàn móxíng (rú SDXL Turbo) zhōng shǐyòng SAEs xuéxí kě jiěshì tèzhēng de kěnéngxìng. Jiéguǒ fāxiàn, SAEs xué dào de tèzhēng kě jiěshì, bìng duì shēngchéng guòchéng chǎnshēng yīnguǒ yǐngxiǎng, jiēshì le móxíng nèibù de zhuānménhuà. Jùtǐ lái shuō, yǒu yīgè mókuài zhǔyào chǔlǐ túxiàng zǔhé, yīgè fùzé tiānjiǎ xìjiě, lìng yīgè fùzé yánsè, guāngzhào hé fēnggé. Yīncǐ, zhè xiàng gōngzuò shì liǎojiě shēngchéng wénběn dào túxiàng móxíng nèibù de zhòngyào yī bù, zhǎnshì le SAEs zài shìjué lǐngyù de qiánlì.",
        "vocab": "[\n    {\"word\": \"稀疏自编码器\", \"pinyin\": \"xī shū zì biān mǎ qì\", \"trans\": \"Sparse Autoencoder\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"中间表示\", \"pinyin\": \"zhōng jiān biǎo shì\", \"trans\": \"Intermediate Representation\"},\n    {\"word\": \"特征\", \"pinyin\": \"tè zhēng\", \"trans\": \"Feature\"},\n    {\"word\": \"分解\", \"pinyin\": \"fēn jiě\", \"trans\": \"Decompose\"},\n    {\"word\": \"控制\", \"pinyin\": \"kòng zhì\", \"trans\": \"Control\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēn xī\", \"trans\": \"Analyze\"},\n    {\"word\": \"文本到图像\", \"pinyin\": \"wén běn dào tú xiàng\", \"trans\": \"Text-to-Image\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"Model\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"Research\"},\n    {\"word\": \"少步\", \"pinyin\": \"shǎo bù\", \"trans\": \"Few-Step\"},\n    {\"word\": \"扩散模型\", \"pinyin\": \"kuò sàn mó xíng\", \"trans\": \"Diffusion Model\"},\n    {\"word\": \"可解释\", \"pinyin\": \"kě jiě shì\", \"trans\": \"Interpretable\"},\n    {\"word\": \"因果影响\", \"pinyin\": \"yīn guǒ yǐng xiǎng\", \"trans\": \"Causal Effect\"},\n    {\"word\": \"揭示\", \"pinyin\": \"jiē shì\", \"trans\": \"Reveal\"},\n    {\"word\": \"专业化\", \"pinyin\": \"zhuān yè huà\", \"trans\": \"Specialization\"},\n    {\"word\": \"模块\", \"pinyin\": \"mó kuài\", \"trans\": \"Module\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"Process\"},\n    {\"word\": \"图像组合\", \"pinyin\": \"tú xiàng zǔ hé\", \"trans\": \"Image Composition\"},\n    {\"word\": \"细节\", \"pinyin\": \"xì jiě\", \"trans\": \"Detail\"},\n    {\"word\": \"颜色\", \"pinyin\": \"yán sè\", \"trans\": \"Color\"},\n    {\"word\": \"光照\", \"pinyin\": \"guāng zhào\", \"trans\": \"Lighting\"},\n    {\"word\": \"风格\", \"pinyin\": \"fēng gē\", \"trans\": \"Style\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"Generate\"},\n    {\"word\": \"视觉领域\", \"pinyin\": \"shì jué lǐng yù\", \"trans\": \"Visual Domain\"},\n    {\"word\": \"潜力\", \"pinyin\": \"qián lì\", \"trans\": \"Potential\"}\n]",
        "trans": "This article discusses the application of Sparse Autoencoders (SAEs) in large language models (LLMs). SAEs can decompose hard-to-interpret intermediate representations into interpretable features, thereby enabling better control and analysis. However, similar analyses and methods in text-to-image models are still lacking in research. The authors investigated the possibility of using SAEs to learn interpretable features in few-step text-to-image diffusion models (such as SDXL Turbo). The results showed that the features learned by SAEs are interpretable and have a causal impact on the generation process, revealing specialization within the model. Specifically, one module mainly handles image composition, another is responsible for adding details, and another deals with color, lighting, and style. Therefore, this work is an important step towards understanding the internal workings of text-to-image generation models and demonstrates the potential of SAEs in the visual domain.",
        "update_ts": "2024-11-03 09:12"
    }
}