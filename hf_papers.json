{
    "date": {
        "ru": "27 Ğ¼Ğ°Ñ",
        "en": "May 27",
        "zh": "5æœˆ27æ—¥"
    },
    "time_utc": "2025-05-27 02:29",
    "weekday": 1,
    "issue_id": 3967,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.18675",
            "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual\n  Reasoning from Transit Maps",
            "url": "https://huggingface.co/papers/2505.18675",
            "abstract": "Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.",
            "score": 4,
            "issue_id": 3967,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ",
                "en": "May 24",
                "zh": "5æœˆ24æ—¥"
            },
            "hash": "61fe1af51f08d30f",
            "authors": [
                "Sicheng Feng",
                "Song Wang",
                "Shuyi Ouyang",
                "Lingdong Kong",
                "Zikai Song",
                "Jianke Zhu",
                "Huan Wang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "National University of Singapore",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18675.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "ReasonMap: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ReasonMap - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ¸Ğ· 30 Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ğ¸ 1008 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ñƒ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ğ¾-Ğ¿Ñ€ĞµĞ¶Ğ½ĞµĞ¼Ñƒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ."
                },
                "en": {
                    "title": "Evaluating Visual Reasoning in Multimodal Models with ReasonMap",
                    "desc": "This paper introduces ReasonMap, a benchmark aimed at evaluating the fine-grained visual understanding and spatial reasoning capabilities of multimodal large language models (MLLMs). The benchmark includes high-resolution transit maps and a set of question-answer pairs to rigorously test the models' reasoning abilities. The study finds that base models often outperform reasoning variants in open-source settings, while closed-source models show the opposite trend. Additionally, the results indicate that masking visual inputs generally leads to decreased performance, highlighting the importance of genuine visual perception in complex reasoning tasks."
                },
                "zh": {
                    "title": "ç»†ç²’åº¦è§†è§‰æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç»†ç²’åº¦è§†è§‰ç†è§£çš„æ¨ç†ä»»åŠ¡ä¸Šä»ç„¶ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReasonMapï¼Œä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsçš„ç»†ç²’åº¦è§†è§‰ç†è§£å’Œç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ReasonMapåŒ…å«æ¥è‡ª13ä¸ªå›½å®¶30ä¸ªåŸå¸‚çš„é«˜åˆ†è¾¨ç‡äº¤é€šåœ°å›¾ï¼Œå¹¶è®¾è®¡äº†ä¸¤çº§è¯„ä¼°æµç¨‹æ¥å‡†ç¡®è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œè´¨é‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªåç›´è§‰çš„æ¨¡å¼ï¼šåœ¨å¼€æºæ¨¡å‹ä¸­ï¼ŒåŸºç¡€æ¨¡å‹çš„è¡¨ç°ä¼˜äºæ¨ç†æ¨¡å‹ï¼Œè€Œåœ¨é—­æºæ¨¡å‹ä¸­åˆ™ç›¸åã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18545",
            "title": "B-score: Detecting biases in large language models using response\n  history",
            "url": "https://huggingface.co/papers/2505.18545",
            "abstract": "Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types of questions invite more biased answers, we test LLMs on our proposed set of questions that span 9 topics and belong to three types: (1) Subjective; (2) Random; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves in a multi-turn conversation in response to questions that seek an Random, unbiased answer. Furthermore, we propose B-score, a novel metric that is effective in detecting biases to Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e, accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or the frequency of single-turn answers alone. Code and data are available at: https://b-score.github.io.",
            "score": 3,
            "issue_id": 3967,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ",
                "en": "May 24",
                "zh": "5æœˆ24æ—¥"
            },
            "hash": "60113080c6881b99",
            "authors": [
                "An Vo",
                "Mohammad Reza Taesiri",
                "Daeyoung Kim",
                "Anh Totti Nguyen"
            ],
            "affiliations": [
                "Auburn University, USA",
                "KAIST, South Korea",
                "University of Alberta, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18545.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#ethics",
                    "#hallucinations",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞµĞµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 9 Ñ‚ĞµĞ¼Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ, ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾, Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ B-score Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "De-biasing LLMs Through Multi-Turn Conversations",
                    "desc": "This paper explores how large language models (LLMs) can reduce their biases during multi-turn conversations by referencing their previous answers. The authors categorize questions into three types: Subjective, Random, and Objective, and find that LLMs can effectively 'de-bias' themselves when responding to Random questions. They introduce a new metric called B-score, which helps identify biases in LLM responses across various question types. The results show that using B-score enhances the accuracy of verifying LLM answers compared to traditional methods like confidence scores."
                },
                "zh": {
                    "title": "å¤šè½®å¯¹è¯ä¸­çš„å»åè§èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å¸¸è¡¨ç°å‡ºæ˜æ˜¾çš„åè§ï¼Œä¾‹å¦‚å¯¹å¥³æ€§çš„åè§æˆ–å¯¹æ•°å­—7çš„åå¥½ã€‚æˆ‘ä»¬ç ”ç©¶äº†åœ¨å¤šè½®å¯¹è¯ä¸­ï¼ŒLLMsæ˜¯å¦èƒ½å¤Ÿåœ¨è§‚å¯Ÿåˆ°è‡ªå·±ä¹‹å‰çš„å›ç­”åï¼Œè¾“å‡ºæ›´å°‘åè§çš„ç­”æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç»„æ¶µç›–9ä¸ªä¸»é¢˜çš„æµ‹è¯•é—®é¢˜ï¼Œåˆ†ä¸ºä¸»è§‚ã€éšæœºå’Œå®¢è§‚ä¸‰ç§ç±»å‹ï¼Œä»¥äº†è§£å“ªäº›é—®é¢˜æ›´å®¹æ˜“å¼•å‘åè§ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨é¢å¯¹å¯»æ±‚éšæœºã€ä¸åè§ç­”æ¡ˆçš„é—®é¢˜æ—¶ï¼Œèƒ½å¤Ÿè‡ªæˆ‘â€œå»åè§â€ï¼Œå¹¶ä¸”æˆ‘ä»¬æå‡ºçš„B-scoreæŒ‡æ ‡åœ¨æ£€æµ‹åè§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†LLMç­”æ¡ˆçš„éªŒè¯å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13426",
            "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.13426",
            "abstract": "VLM-Gym addresses the \"knowing-doing\" gap in Vision-Language Models by training them in a diverse RL environment, leading to enhanced perception and reasoning abilities that surpass existing models in interactive games.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This ``knowing-doing'' gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at https://github.com/chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents.",
            "score": 3,
            "issue_id": 3967,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "8cfc8c8f1a7e5589",
            "authors": [
                "Liang Chen",
                "Hongcheng Gao",
                "Tianyu Liu",
                "Zhiqi Huang",
                "Flood Sung",
                "Xinyu Zhou",
                "Yuxin Wu",
                "Baobao Chang"
            ],
            "affiliations": [
                "Moonshot AI",
                "Peking University",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13426.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#open_source",
                    "#agents",
                    "#games",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ² Vision-Language Models Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VLM-Gym - ÑÑ€ĞµĞ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Vision-Language Models, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼. VLM-Gym Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹ Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñƒ ÑÑ€ĞµĞ´Ñƒ, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ G0 Ğ¸ G1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ G1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Bridging the Knowing-Doing Gap in Vision-Language Models",
                    "desc": "VLM-Gym is a new training environment designed to improve Vision-Language Models (VLMs) by bridging the gap between their knowledge and practical application in interactive games. Traditional VLMs excel in tasks involving text and images but struggle with decision-making in dynamic environments. By using reinforcement learning (RL) in a diverse set of visual games, VLM-Gym enables models to develop better perception and reasoning skills. The G1 models trained in this environment outperform existing models, demonstrating that enhanced perception and reasoning can support each other during training."
                },
                "zh": {
                    "title": "VLM-Gymï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å†³ç­–èƒ½åŠ›",
                    "desc": "VLM-Gym æ˜¯ä¸€ä¸ªé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œæ—¨åœ¨è§£å†³å®ƒä»¬åœ¨äº’åŠ¨æ¸¸æˆä¸­çš„å†³ç­–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡åœ¨å¤šæ ·åŒ–çš„æ¸¸æˆç¯å¢ƒä¸­è®­ç»ƒï¼ŒVLM-Gym æå‡äº†æ¨¡å‹çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ç®€å•æ¸¸æˆä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬å¼€å‘äº† G0 å’Œ G1 æ¨¡å‹ï¼Œå…¶ä¸­ G1 æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¹‹å‰è¿›è¡Œäº†æ„ŸçŸ¥å¢å¼ºï¼Œä»¥åº”å¯¹æ¸¸æˆå¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ€ç»ˆï¼ŒG1 æ¨¡å‹åœ¨æ‰€æœ‰æ¸¸æˆä¸­å‡è¶…è¶Šäº†å…¶æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19706",
            "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with\n  Error-Aware Hierarchical Supervision",
            "url": "https://huggingface.co/papers/2505.19706",
            "abstract": "PathFinder-PRM, a hierarchical and error-aware Process Reward Model, improves mathematical problem-solving by fine-grained error classification and step correctness estimation, achieving state-of-the-art PRMScore with reduced data usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are prone to hallucination, especially during multi-hop and reasoning-intensive tasks such as mathematical problem solving. While Outcome Reward Models verify only final answers, Process Reward Models (PRMs) score each intermediate step to steer generation toward coherent solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware discriminative PRM that first classifies math and consistency errors at each step, then combines these fine-grained signals to estimate step correctness. To train PathFinder-PRM, we construct a 400K-sample dataset by enriching the human-annotated PRM800K corpus and RLHFlow Mistral traces with three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while using 3 times less data. When applied to reward guided greedy search, our model yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results demonstrate that decoupled error detection and reward estimation not only boost fine-grained error detection but also substantially improve end-to-end, reward-guided mathematical reasoning with greater data efficiency.",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "e60e6086053e62d0",
            "authors": [
                "Tej Deep Pala",
                "Panshul Sharma",
                "Amir Zadeh",
                "Chuan Li",
                "Soujanya Poria"
            ],
            "affiliations": [
                "Lambda Labs",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19706.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#hallucinations",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ñ PathFinder-PRM",
                    "desc": "PathFinder-PRM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ°. PathFinder-PRM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ PRMScore, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "PathFinder-PRM: Enhancing Math Problem-Solving with Fine-Grained Error Detection",
                    "desc": "PathFinder-PRM is a new model designed to enhance mathematical problem-solving by focusing on detailed error classification and assessing the correctness of each step in the solution process. Unlike traditional Outcome Reward Models that only evaluate final answers, this model uses a hierarchical and error-aware approach to score intermediate steps, which helps guide the generation of coherent solutions. It was trained on a large dataset that includes fine-grained labels for errors, allowing it to achieve a state-of-the-art PRMScore while using significantly less data. The results show that this model not only improves error detection but also enhances overall performance in reward-guided reasoning tasks."
                },
                "zh": {
                    "title": "æå‡æ•°å­¦æ¨ç†çš„é”™è¯¯æ„ŸçŸ¥æ¨¡å‹",
                    "desc": "PathFinder-PRMæ˜¯ä¸€ç§å±‚æ¬¡åŒ–ä¸”å…·å¤‡é”™è¯¯æ„ŸçŸ¥çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ç»†è‡´çš„é”™è¯¯åˆ†ç±»å’Œæ­¥éª¤æ­£ç¡®æ€§ä¼°è®¡æ¥æå‡æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹æ¯ä¸ªæ­¥éª¤çš„æ•°å­¦é”™è¯¯å’Œä¸€è‡´æ€§é”™è¯¯è¿›è¡Œåˆ†ç±»ï¼Œç»“åˆè¿™äº›ç»†è‡´çš„ä¿¡å·æ¥è¯„ä¼°æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«40ä¸‡æ ·æœ¬çš„æ•°æ®é›†ï¼ŒPathFinder-PRMåœ¨PRMBenchä¸Šè¾¾åˆ°äº†67.7çš„æœ€æ–°çŠ¶æ€ï¼Œä½¿ç”¨çš„æ•°æ®é‡æ¯”ä¹‹å‰å‡å°‘äº†ä¸‰å€ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè§£è€¦çš„é”™è¯¯æ£€æµ‹å’Œå¥–åŠ±ä¼°è®¡ä¸ä»…æå‡äº†ç»†ç²’åº¦é”™è¯¯æ£€æµ‹çš„èƒ½åŠ›ï¼Œè¿˜æ˜¾è‘—æ”¹å–„äº†åŸºäºå¥–åŠ±çš„æ•°å­¦æ¨ç†æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19630",
            "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue",
            "url": "https://huggingface.co/papers/2505.19630",
            "abstract": "Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. https://github.com/JarvisUSTC/DoctorAgent-RL",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "ab312c010a92062f",
            "authors": [
                "Yichun Feng",
                "Jiawei Wang",
                "Lu Zhou",
                "Yixue Li"
            ],
            "affiliations": [
                "Department of EEIS, University of Science and Technology of China",
                "Guangzhou National Laboratory",
                "School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences",
                "Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19630.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#science",
                    "#healthcare",
                    "#dataset",
                    "#games",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑ‚Ğ¾Ñ€: Ğ˜Ğ˜ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ²ĞµÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ñ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DoctorAgent-RL - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚-Ğ²Ñ€Ğ°Ñ‡ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ±Ğ¾Ñ€ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DoctorAgent-RL Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Revolutionizing Clinical Consultations with Reinforcement Learning",
                    "desc": "This paper introduces DoctorAgent-RL, a novel framework that enhances biomedical question answering by using reinforcement learning (RL) to improve multi-turn medical consultations. Unlike traditional systems that rely on static data, DoctorAgent-RL allows a doctor agent to adaptively optimize its questioning strategy through dynamic interactions with a patient agent. The framework is designed to intelligently extract relevant clinical information, addressing the limitations of vague patient descriptions. Additionally, the authors present MTMedDialog, a new dataset for simulating patient interactions, which supports the framework's effectiveness in real-world clinical settings."
                },
                "zh": {
                    "title": "æ™ºèƒ½åŒ»ç–—å’¨è¯¢çš„æ–°çªç ´",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦é—®ç­”é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å®é™…ä¸´åºŠå’¨è¯¢ä¸­çš„åº”ç”¨ä»é¢ä¸´æ ¸å¿ƒæŒ‘æˆ˜ã€‚ç°æœ‰ç³»ç»Ÿä¾èµ–å•å‘ä¿¡æ¯ä¼ é€’æ¨¡å¼ï¼Œæ‚£è€…å¿…é¡»åœ¨ä¸€æ¬¡æ€§æè¿°ç—‡çŠ¶ï¼Œå¯¼è‡´æ¨¡ç³ŠæŠ•è¯‰æ—¶çš„è¯Šæ–­å»ºè®®ä¸å¤Ÿå…·ä½“ã€‚ä¼ ç»Ÿçš„åŸºäºç›‘ç£å­¦ä¹ çš„å¤šè½®å¯¹è¯æ–¹æ³•å—é™äºé™æ€æ•°æ®é©±åŠ¨çš„èŒƒå¼ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼Œéš¾ä»¥æ™ºèƒ½æå–å…³é”®ä¸´åºŠä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DoctorAgent-RLï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œå°†åŒ»ç–—å’¨è¯¢å»ºæ¨¡ä¸ºä¸ç¡®å®šæ€§ä¸‹çš„åŠ¨æ€å†³ç­–è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19443",
            "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications\n  of Agentic AI",
            "url": "https://huggingface.co/papers/2505.19443",
            "abstract": "A review contrasts vibe coding and agentic coding paradigms, highlighting their differences in interaction, autonomy, and application areas in AI-assisted software development.  \t\t\t\t\tAI-generated summary \t\t\t\t This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "04b7019c8d659b76",
            "authors": [
                "Ranjan Sapkota",
                "Konstantinos I. Roumeliotis",
                "Manoj Karkee"
            ],
            "affiliations": [
                "Cornell University, Department of Biological and Environmental Engineering, USA",
                "University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, Greece"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19443.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#agents",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ°Ğ¹Ğ± vs ĞĞ³ĞµĞ½Ñ‚: ĞĞ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜: Ğ²Ğ°Ğ¹Ğ±-ĞºĞ¾Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ğ½Ğ³Ğ°. Ğ’Ğ°Ğ¹Ğ±-ĞºĞ¾Ğ´Ğ¸Ğ½Ğ³ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ğ½Ğ³, Ğ½Ğ°Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ†ĞµĞ»ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼."
                },
                "en": {
                    "title": "Harmonizing Vibe and Agentic Coding for AI Development",
                    "desc": "This paper reviews two coding paradigms in AI-assisted software development: vibe coding and agentic coding. Vibe coding focuses on human interaction and creativity, using conversational prompts to aid in ideation and experimentation. In contrast, agentic coding allows for more autonomous development, where AI agents can plan and execute tasks with little human input. The authors propose a taxonomy to compare these paradigms and suggest that the future of AI software engineering will benefit from integrating both approaches for a more effective development process."
                },
                "zh": {
                    "title": "èåˆæƒ…æ„Ÿç¼–ç ä¸è‡ªä¸»ç¼–ç çš„æœªæ¥è½¯ä»¶å¼€å‘",
                    "desc": "æœ¬æ–‡å¯¹ä¸¤ç§æ–°å…´çš„äººå·¥æ™ºèƒ½è¾…åŠ©è½¯ä»¶å¼€å‘èŒƒå¼è¿›è¡Œäº†å…¨é¢åˆ†æï¼šæƒ…æ„Ÿç¼–ç å’Œè‡ªä¸»ç¼–ç ã€‚æƒ…æ„Ÿç¼–ç å¼ºè°ƒé€šè¿‡åŸºäºæç¤ºçš„å¯¹è¯å·¥ä½œæµç¨‹å®ç°ç›´è§‚çš„äººæœºäº¤äº’ï¼Œé€‚åˆäºåˆ›æ„æ¢ç´¢å’Œå®éªŒã€‚è€Œè‡ªä¸»ç¼–ç åˆ™é€šè¿‡ç›®æ ‡é©±åŠ¨çš„æ™ºèƒ½ä½“å®ç°è‡ªä¸»è½¯ä»¶å¼€å‘ï¼Œèƒ½å¤Ÿåœ¨æœ€å°äººç±»å¹²é¢„ä¸‹è¿›è¡Œè§„åˆ’ã€æ‰§è¡Œå’Œæµ‹è¯•ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒæˆåŠŸçš„äººå·¥æ™ºèƒ½è½¯ä»¶å·¥ç¨‹å°†ä¾èµ–äºå°†è¿™ä¸¤ç§èŒƒå¼çš„ä¼˜åŠ¿ç»“åˆåœ¨ä¸€ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18536",
            "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal\n  Large Language Models",
            "url": "https://huggingface.co/papers/2505.18536",
            "abstract": "Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ",
                "en": "May 24",
                "zh": "5æœˆ24æ—¥"
            },
            "hash": "03c065b99c9707fe",
            "authors": [
                "Haoyuan Sun",
                "Jiaqi Wu",
                "Bo Xia",
                "Yifu Luo",
                "Yifei Zhao",
                "Kai Qin",
                "Xufei Lv",
                "Tiantian Zhang",
                "Yongzhe Chang",
                "Xueqian Wang"
            ],
            "affiliations": [
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18536.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#benchmark",
                    "#rlhf",
                    "#agi",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RFT: ĞºĞ»ÑÑ‡ Ğº ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ MLLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RFT, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Reinforcement Fine-Tuning: Powering Reasoning in Multimodal AI",
                    "desc": "This paper discusses the role of reinforcement fine-tuning (RFT) in improving the reasoning abilities of multimodal large language models (MLLMs). It highlights how RFT has contributed to the development of advanced AI models and emphasizes its importance in enhancing reasoning across various tasks and domains. The authors summarize five key improvements brought by RFT, including better training algorithms and diverse benchmarks. They also suggest future research directions to further explore the potential of RFT in the quest for Artificial General Intelligence (AGI)."
                },
                "zh": {
                    "title": "å¼ºåŒ–å¾®è°ƒï¼šæ¨åŠ¨å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "åœ¨2025å¹´ï¼Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œå¹¶æ¨åŠ¨äº†å¦‚OpenAI-o1å’ŒDeepSeek-R1ç­‰å‰æ²¿AIæ¨¡å‹çš„å‘å±•ã€‚RFTåœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„é«˜æ•ˆåº”ç”¨å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†ç ”ç©¶è€…åº”äº†è§£çš„åŸºç¡€çŸ¥è¯†ï¼Œå¹¶æ€»ç»“äº†RFTåœ¨æå‡MLLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„äº”ä¸ªå…³é”®æ”¹è¿›ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†äº”ä¸ªæœªæ¥ç ”ç©¶çš„æœ‰å‰æ™¯æ–¹å‘ï¼Œä»¥æœŸä¸ºAGIçš„è¿›æ­¥æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16972",
            "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for\n  Speech Recognition",
            "url": "https://huggingface.co/papers/2505.16972",
            "abstract": "Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a scalable pipeline that improves multilingual ASR models by converting large-scale text corpora into synthetic speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just tens of hours of real transcribed speech can effectively train TTS models to generate synthetic speech at hundreds of times the original volume while maintaining high quality. To evaluate synthetic speech quality, we develop an intelligibility-based assessment framework and establish clear thresholds for when synthetic data benefits ASR training. Using Speech Back-Translation, we generate more than 500,000 hours of synthetic speech in ten languages and continue pre-training Whisper-large-v3, achieving average transcription error reductions of over 30\\%. These results highlight the scalability and effectiveness of Speech Back-Translation for enhancing multilingual ASR systems.",
            "score": 1,
            "issue_id": 3967,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "784a648ae844599f",
            "authors": [
                "Tianduo Wang",
                "Lu Xu",
                "Wei Lu",
                "Shanbo Cheng"
            ],
            "affiliations": [
                "ByteDance Seed",
                "StatNLP Research Group, Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16972.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multilingual",
                    "#low_resource",
                    "#dataset",
                    "#audio",
                    "#synthetic"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµÑ‡ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ASR",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Speech Back-Translation Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑÑ‹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-speech (TTS) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ TTS Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Whisper-large-v3 Ğ½Ğ° 500 000 Ñ‡Ğ°ÑĞ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° 10 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 30%."
                },
                "en": {
                    "title": "Scaling ASR with Synthetic Speech: Speech Back-Translation",
                    "desc": "This paper presents a method called Speech Back-Translation to enhance Automatic Speech Recognition (ASR) systems for multiple languages, especially those with limited resources. By using text-to-speech (TTS) models, the authors convert large text corpora into synthetic speech, significantly increasing the amount of training data available. They show that even a small amount of real speech can train TTS models to produce high-quality synthetic speech at a much larger scale. The results indicate that this approach can improve ASR performance, achieving over 30% reduction in transcription errors across ten languages."
                },
                "zh": {
                    "title": "è¯­éŸ³åå‘ç¿»è¯‘ï¼šæå‡å¤šè¯­è¨€ASRçš„æœ‰æ•ˆåˆ©å™¨",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºè¯­éŸ³åå‘ç¿»è¯‘ï¼ˆSpeech Back-Translationï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“è½¬æ¢ä¸ºåˆæˆè¯­éŸ³ï¼Œåˆ©ç”¨ç°æˆçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œè§£å†³äº†èµ„æºæœ‰é™è¯­è¨€çš„è¦†ç›–é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…éœ€æ•°åå°æ—¶çš„çœŸå®è½¬å½•è¯­éŸ³ï¼Œå°±èƒ½æœ‰æ•ˆè®­ç»ƒTTSæ¨¡å‹ç”Ÿæˆæ•°ç™¾å€çš„åˆæˆè¯­éŸ³ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è¶…è¿‡50ä¸‡å°æ—¶çš„åˆæˆè¯­éŸ³ï¼Œå¹¶åœ¨å¤šè¯­è¨€ASRç³»ç»Ÿä¸­å®ç°äº†è¶…è¿‡30%çš„è½¬å½•é”™è¯¯ç‡é™ä½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-26.html",
    "link_next": "2025-05-28.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "26.05",
        "en": "05/26",
        "zh": "5æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "28.05",
        "en": "05/28",
        "zh": "5æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 1,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºTabSTARçš„æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ç§ç”¨äºè¡¨æ ¼æ•°æ®çš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ç‰¹å¾ã€‚TabSTARé€šè¿‡è¿ç§»å­¦ä¹ å®ç°äº†æœ€å…ˆè¿›çš„åˆ†ç±»ä»»åŠ¡æ€§èƒ½ï¼Œè€Œä¸éœ€è¦ç‰¹å®šäºæ•°æ®é›†çš„å‚æ•°ã€‚å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œç»“åˆç›®æ ‡ä»¤ç‰Œï¼Œå­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„åµŒå…¥ã€‚TabSTARåœ¨ä¸­ç­‰å’Œå¤§å‹æ•°æ®é›†çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶å±•ç¤ºäº†é¢„è®­ç»ƒé˜¶æ®µçš„æ‰©å±•è§„å¾‹ã€‚",
        "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºTabSTARçš„æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ç§ç”¨äºè¡¨æ ¼æ•°æ®çš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ç‰¹å¾ã€‚TabSTARé€šè¿‡è¿ç§»å­¦ä¹ å®ç°äº†æœ€å…ˆè¿›çš„åˆ†ç±»ä»»åŠ¡æ€§èƒ½ï¼Œè€Œä¸éœ€è¦ç‰¹å®šäºæ•°æ®é›†çš„å‚æ•°ã€‚å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œç»“åˆç›®æ ‡ä»¤ç‰Œï¼Œå­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„åµŒå…¥ã€‚TabSTARåœ¨ä¸­ç­‰å’Œå¤§å‹æ•°æ®é›†çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶å±•ç¤ºäº†é¢„è®­ç»ƒé˜¶æ®µçš„æ‰©å±•è§„å¾‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ©i TabSTAR de mÃ³ xÃ­ng. tÄ shÃ¬ yÄ« zhÇ’ng yÃ²ng yÃº biÇo gÃ© shÃ¹ jÃ¹ de jÄ« chÇ” mÃ³ xÃ­ng, nÃ©ng gÃ²u chÇ” lÇ wÃ©n bÄ›n tÃ¨ zhÃ¨ng. TabSTAR tÅng guÃ² qiÄn yÃ­ xuÃ© xÃ­ shÃ­ xiÃ n le zuÃ¬ xiÄn jÃ¬n de fÄ“n lÃ¨i rÃ¨n wÃ¹ xÃ­ng nÃ©ng, Ã©r bÃ¹ xÅ« yÃ o tÃ¨ dÃ¬ng yÃº shÃ¹ jÃ¹ de cÄn shÃ¹. tÄ lÃ¬ yÃ²ng yÃ¹ xÃ¹n liÃ n de wÃ©n bÄ›n biÄn mÇ qÃ¬, jiÃ© hÃ© mÃ¹ biÄo lÃ¬ng pÃ¡i, xuÃ© xÃ­ rÃ¨n wÃ¹ tÃ¨ dÃ¬ng de qiÃ n rÃ¹. TabSTAR zÃ i zhÅng dÄ›ng hÃ© dÃ  xÃ­ng shÃ¹ jÃ¹ de fÄ“n lÃ¨i rÃ¨n wÃ¹ zhÅng biÇo xiÃ n chÅ« sÃ¨, bÃ¬ng zhÇn shÃ¬ le yÃ¹ xÃ¹n liÃ n jiÄ“ duÃ n de kuÃ² zhÇn guÄ« lÇœ.",
        "vocab": "[\n    {\"word\": \"è¿ç§»å­¦ä¹ \", \"pinyin\": \"qiÄn yÃ­ xuÃ© xÃ­\", \"trans\": \"transfer learning\"},\n    {\"word\": \"åˆ†ç±»ä»»åŠ¡\", \"pinyin\": \"fÄ“n lÃ¨i rÃ¨n wÃ¹\", \"trans\": \"classification task\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄn shÇ”\", \"trans\": \"parameter\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"æ–‡æœ¬ç¼–ç å™¨\", \"pinyin\": \"wÃ©n bÄ›n biÄn mÇ qÃ¬\", \"trans\": \"text encoder\"},\n    {\"word\": \"ç›®æ ‡ä»¤ç‰Œ\", \"pinyin\": \"mÃ¹ biÄo lÃ¬ng pÃ¡i\", \"trans\": \"target token\"},\n    {\"word\": \"åµŒå…¥\", \"pinyin\": \"qiÃ n rÃ¹\", \"trans\": \"embedding\"},\n    {\"word\": \"è¡¨ç°å‡ºè‰²\", \"pinyin\": \"biÇo xiÃ n chÅ« sÃ¨\", \"trans\": \"perform excellently\"},\n    {\"word\": \"æ‰©å±•è§„å¾‹\", \"pinyin\": \"kuÃ² zhÇn guÄ« lÇœ\", \"trans\": \"expansion pattern\"}\n]",
        "trans": "This article introduces a model called TabSTAR. It is a foundational model for tabular data that can handle textual features. TabSTAR achieves state-of-the-art performance in classification tasks through transfer learning, without requiring parameters specific to the dataset. It leverages a pre-trained text encoder combined with target tokens to learn task-specific embeddings. TabSTAR performs exceptionally well in classification tasks on medium and large datasets and demonstrates scalability during the pre-training phase.",
        "update_ts": "2025-05-26 09:39"
    }
}