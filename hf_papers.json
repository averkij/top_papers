{
    "date": {
        "ru": "5 ноября",
        "en": "November 5",
        "zh": "11月5日"
    },
    "time_utc": "2024-11-05 02:42",
    "weekday": 1,
    "issue_id": 420,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.00836",
            "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
            "url": "https://huggingface.co/papers/2411.00836",
            "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.",
            "score": 4,
            "issue_id": 420,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "e73ae00a5621a2b9",
            "data": {
                "categories": [
                    "#benchmark",
                    "#math",
                    "#cv"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "DynaMath: Новый подход к оценке математических способностей ИИ",
                    "desc": "Статья представляет DynaMath - динамический визуальный математический бенчмарк для оценки робастности рассуждений Vision-Language Models (VLM) в задачах математики. Исследователи обнаружили, что современные VLM, такие как GPT-4V, часто не способны применять шаги решения к похожим задачам с небольшими изменениями. DynaMath включает 501 исходный вопрос в виде Python-программ, позволяющих генерировать множество вариаций для тестирования обобщающей способности моделей. Результаты оценки 14 современных VLM показали, что их точность в худшем случае значительно ниже средней точности."
                },
                "en": {
                    "title": "Enhancing VLMs: Unveiling Mathematical Reasoning Limitations with DynaMath",
                    "desc": "This paper explores the limitations of Vision-Language Models (VLMs) in performing mathematical reasoning tasks that involve visual elements. It highlights that while humans can adapt their problem-solving strategies to slight changes in problems, current state-of-the-art VLMs like GPT-4o struggle with this adaptability. To address this issue, the authors introduce DynaMath, a dynamic benchmark that generates a wide variety of mathematical questions to rigorously test VLMs' reasoning capabilities. The findings reveal that VLMs exhibit significantly lower accuracy in worst-case scenarios compared to average cases, underscoring the importance of evaluating their robustness in mathematical reasoning."
                },
                "zh": {
                    "title": "提升视觉语言模型的数学推理能力",
                    "desc": "本文探讨了视觉语言模型（VLMs）在数学推理任务中的表现，尤其是在视觉上下文的影响下。研究发现，尽管人类能够灵活应对相似问题的变化，当前的最先进模型如GPT-4o在面对这些变化时却表现不佳，显示出其数学推理能力的局限性。为了解决这一问题，本文提出了DynaMath，一个动态视觉数学基准，旨在深入评估VLMs的推理稳健性。通过对501个高质量种子问题的自动生成，DynaMath能够评估模型在不同输入条件下的泛化能力，结果显示模型在最坏情况下的准确率显著低于平均情况。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00918",
            "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
            "url": "https://huggingface.co/papers/2411.00918",
            "abstract": "Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.",
            "score": 2,
            "issue_id": 420,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 ноября",
                "en": "November 1",
                "zh": "11月1日"
            },
            "hash": "a406640433a3de34",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LibMoE: Демократизация исследований Mixture of Experts в больших языковых моделях",
                    "desc": "Статья представляет LibMoE - комплексную модульную систему для исследования, обучения и оценки алгоритмов Mixture of Experts (MoE) в больших языковых моделях (LLM). LibMoE основан на трех принципах: модульный дизайн, эффективное обучение и всесторонняя оценка. Используя LibMoE, авторы провели обширное сравнение пяти современных алгоритмов MoE на трех различных LLM и 11 наборах данных в режиме zero-shot. Результаты показали, что, несмотря на уникальные характеристики, все алгоритмы MoE показывают примерно одинаковые результаты при усреднении по широкому спектру задач."
                },
                "en": {
                    "title": "LibMoE: Streamlining Mixture of Experts for Large Language Models",
                    "desc": "This paper introduces LibMoE, a framework designed to facilitate research on Mixture of Experts (MoE) algorithms for large language models (LLMs). It emphasizes a modular design, efficient training processes, and comprehensive evaluation methods to make MoE more accessible to researchers. The authors benchmark five leading MoE algorithms across three LLMs and 11 datasets, revealing that their performance is generally similar across various tasks. LibMoE aims to standardize the training and evaluation of MoE, helping researchers advance the development of future LLMs."
                },
                "zh": {
                    "title": "LibMoE：让混合专家算法更易于研究和应用",
                    "desc": "混合专家（MoE）在大型语言模型（LLMs）的高效和有效发展中扮演着重要角色。由于资源需求巨大，许多研究者难以研究大规模的MoE算法。本文开发了LibMoE，这是一个全面且模块化的框架，旨在简化MoE算法的研究、训练和评估。通过模块化设计、高效训练和全面评估，LibMoE使得MoE在LLMs中的应用对更多研究者变得可及。"
                }
            }
        }
    ],
    "link_prev": "2024-11-04.html",
    "link_next": "2024-11-06.html",
    "short_date_prev": {
        "ru": "04.11",
        "en": "11/04",
        "zh": "11月4日"
    },
    "short_date_next": {
        "ru": "06.11",
        "en": "11/06",
        "zh": "11月6日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#medicine": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "这篇文章讨论了构建图形用户界面（GUI）代理的现有努力。目前主要依赖于强大的商业视觉语言模型（VLMs），如GPT-4o和GeminiProVision。开源VLMs在GUI理解和未见分布（OOD）场景中表现较差，因此实践者不太愿意使用。为了推动这一领域的研究，作者开发了OS-Atlas，这是一个擅长GUI理解和OOD任务的基础模型。他们还发布了一个包含1300多万GUI元素的跨平台数据集，并证明了OS-Atlas在多个基准测试中的显著性能提升。",
        "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
        "pinyin": "Zhè piān wénzhāng tǎolùn le gòujiàn túxíng yònghù jiēmiàn (GUI) dàilǐ de xiàn yǒu nǔlì. Dāngqián zhǔyào yīlài zài qiángdà de shāngyè shìjué yǔyán móxíng (VLMs) shàng, rú GPT-4o hé GeminiProVision. Kāiyuán VLMs zài GUI lǐjiě hé wèi jiàn fēnbù (OOD) chǎngjǐng zhōng biǎoxiǎn jiàochǎ, yīncǐ shíjiànzhě bù tài yuànyì shǐyòng. Wèile tuīdòng zhè yī lǐngyù de yánjiū, zuòzhě kāifāle OS-Atlas, zhè shì yīgè shàncháng GUI lǐjiě hé OOD rènwù de jīchǔ móxíng. Tāmen hái fābùle yīgè bāohán 1300 duō wàn GUI yuánsù de kuà píngtái shùjùjí, bìng zhèngmíngle OS-Atlas zài duōgè jīzhǔn cèshì zhōng de xiǎnzhù xiàonéng tǐshēng.",
        "vocab": "[\n    {\"word\": \"构建\", \"pinyin\": \"gòu jiàn\", \"trans\": \"construct\"},\n    {\"word\": \"图形用户界面\", \"pinyin\": \"tú xíng yòng hù jiè miàn\", \"trans\": \"graphical user interface\"},\n    {\"word\": \"代理\", \"pinyin\": \"dài lǐ\", \"trans\": \"agent\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiàn yǒu\", \"trans\": \"existing\"},\n    {\"word\": \"努力\", \"pinyin\": \"nǔ lì\", \"trans\": \"efforts\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"rely on\"},\n    {\"word\": \"强大\", \"pinyin\": \"qiáng dà\", \"trans\": \"powerful\"},\n    {\"word\": \"商业\", \"pinyin\": \"shāng yè\", \"trans\": \"commercial\"},\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shì jué yǔ yán mó xíng\", \"trans\": \"visual language model\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open-source\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"未见分布\", \"pinyin\": \"wèi jiàn fēn bù\", \"trans\": \"out-of-distribution\"},\n    {\"word\": \"场景\", \"pinyin\": \"chǎng jǐng\", \"trans\": \"scenario\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"较差\", \"pinyin\": \"jiào chà\", \"trans\": \"poor\"},\n    {\"word\": \"实践者\", \"pinyin\": \"shí jiàn zhě\", \"trans\": \"practitioner\"},\n    {\"word\": \"不太愿意\", \"pinyin\": \"bù tài yuàn yì\", \"trans\": \"not very willing\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuī dòng\", \"trans\": \"promote\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"field\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"开发\", \"pinyin\": \"kāi fā\", \"trans\": \"develop\"},\n    {\"word\": \"基础模型\", \"pinyin\": \"jī chǔ mó xíng\", \"trans\": \"foundation model\"},\n    {\"word\": \"擅长\", \"pinyin\": \"shàn cháng\", \"trans\": \"proficient\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"发布\", \"pinyin\": \"fā bù\", \"trans\": \"release\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"跨平台\", \"pinyin\": \"kuà píng tái\", \"trans\": \"cross-platform\"},\n    {\"word\": \"元素\", \"pinyin\": \"yuán sù\", \"trans\": \"element\"},\n    {\"word\": \"证明\", \"pinyin\": \"zhèng míng\", \"trans\": \"demonstrate\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jī zhǔn cè shì\", \"trans\": \"benchmark test\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improvement\"}\n]",
        "trans": "This article discusses current efforts in building graphical user interface (GUI) agents. Currently, these efforts primarily rely on powerful commercial vision language models (VLMs) such as GPT-4o and GeminiProVision. Open-source VLMs perform poorly in GUI understanding and out-of-distribution (OOD) scenarios, making practitioners reluctant to use them. To advance research in this field, the authors developed OS-Atlas, a foundational model adept at GUI understanding and OOD tasks. They also released a cross-platform dataset containing over 13 million GUI elements and demonstrated significant performance improvements of OS-Atlas across multiple benchmark tests.",
        "update_ts": "2024-11-04 10:14"
    }
}