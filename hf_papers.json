{
    "date": {
        "ru": "4 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 4",
        "zh": "11æœˆ4æ—¥"
    },
    "time_utc": "2024-11-04 04:15",
    "weekday": 0,
    "issue_id": 408,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.00776",
            "title": "Randomized Autoregressive Visual Generation",
            "url": "https://huggingface.co/papers/2411.00776",
            "abstract": "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer",
            "score": 4,
            "issue_id": 408,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 1",
                "zh": "11æœˆ1æ—¥"
            },
            "hash": "0cc2c0f19f735f79",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Randomized AutoRegressive modeling (RAR) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. RAR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet-256 Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼ FID 1.48. RAR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Randomized AutoRegressive Modeling",
                    "desc": "This paper introduces Randomized AutoRegressive modeling (RAR), a novel approach for generating images that enhances performance while remaining compatible with existing language modeling techniques. RAR employs a unique training method where the input sequence is randomly shuffled during the autoregressive training process, allowing the model to learn from various factorization orders. This strategy helps the model to better understand and utilize bidirectional contexts, leading to improved image generation capabilities. The results show that RAR achieves a remarkable FID score of 1.48 on the ImageNet-256 benchmark, outperforming previous state-of-the-art methods in both autoregressive and diffusion-based image generation."
                },
                "zh": {
                    "title": "éšæœºè‡ªå›å½’å»ºæ¨¡ï¼šå›¾åƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§éšæœºè‡ªå›å½’å»ºæ¨¡ï¼ˆRARï¼‰æ–¹æ³•ç”¨äºè§†è§‰ç”Ÿæˆï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶ä¸è¯­è¨€å»ºæ¨¡æ¡†æ¶å®Œå…¨å…¼å®¹ã€‚RARæ–¹æ³•ç®€å•ï¼šåœ¨æ ‡å‡†çš„è‡ªå›å½’è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¾“å…¥åºåˆ—é€šå¸¸æŒ‰å…‰æ …å½¢å¼æ’åˆ—ï¼Œä½†ä»¥æ¦‚ç‡réšæœºæ‰“ä¹±ä¸ºä¸åŒçš„å› å­åŒ–é¡ºåºï¼Œrä»1å¼€å§‹ï¼Œéšç€è®­ç»ƒçº¿æ€§è¡°å‡åˆ°0ã€‚è¿™ç§é€€ç«è®­ç»ƒç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æœ€å¤§åŒ–æ‰€æœ‰å› å­åŒ–é¡ºåºçš„æœŸæœ›ä¼¼ç„¶ï¼Œä»è€Œæœ‰æ•ˆæé«˜æ¨¡å‹å»ºæ¨¡åŒå‘ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚RARä¿æŒäº†è‡ªå›å½’å»ºæ¨¡æ¡†æ¶çš„å®Œæ•´æ€§ï¼Œç¡®ä¿ä¸è¯­è¨€å»ºæ¨¡çš„å®Œå…¨å…¼å®¹ï¼ŒåŒæ—¶åœ¨å›¾åƒç”Ÿæˆä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00412",
            "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation",
            "url": "https://huggingface.co/papers/2411.00412",
            "abstract": "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but often produce hallucinations for complex ones. While integrating LLMs with tools can increase reliability, this approach typically results in over-reliance on tools, diminishing the model's ability to solve simple problems through basic reasoning. In contrast, human experts first assess problem complexity using domain knowledge before choosing an appropriate solution approach. Inspired by this human problem-solving process, we propose a novel two-component fine-tuning method. In the first component World Knowledge Distillation (WKD), LLMs learn directly from solutions generated using tool's information to internalize domain knowledge. In the second component Tool Usage Adaptation (TUA), we partition problems into easy and hard categories based on the model's direct answering accuracy. While maintaining the same alignment target for easy problems as in WKD, we train the model to intelligently switch to tool usage for more challenging problems. We validate our method on six scientific benchmark datasets, spanning mathematics, climate science and epidemiology. On average, our models demonstrate a 28.18% improvement in answer accuracy and a 13.89% increase in tool usage precision across all datasets, surpassing state-of-the-art models including GPT-4o and Claude-3.5.",
            "score": 4,
            "issue_id": 407,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 1",
                "zh": "11æœˆ1æ—¥"
            },
            "hash": "27e4deefc7d09df0",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… - Ğ¿Ñ€Ğ¸Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLMs: Smart Tool Use for Complex Problems",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) in solving complex scientific problems, which often lead to inaccuracies or 'hallucinations'. The authors propose a two-component fine-tuning method that mimics human problem-solving strategies by first assessing problem complexity. The first component, World Knowledge Distillation (WKD), allows LLMs to learn from solutions that utilize external tools, while the second component, Tool Usage Adaptation (TUA), helps the model categorize problems as easy or hard and decide when to use tools. The proposed method shows significant improvements in accuracy and tool usage precision across various scientific datasets, outperforming existing models."
                },
                "zh": {
                    "title": "æ™ºèƒ½åˆ‡æ¢ï¼Œæå‡æ¨¡å‹è§£å†³é—®é¢˜çš„èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³ç®€å•ç§‘å­¦é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚é—®é¢˜ä¸Šå¸¸å¸¸å‡ºç°å¹»è§‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒç»„ä»¶å¾®è°ƒæ–¹æ³•ï¼Œæ¨¡ä»¿äººç±»ä¸“å®¶çš„è§£å†³é—®é¢˜è¿‡ç¨‹ã€‚ç¬¬ä¸€ä¸ªç»„ä»¶æ˜¯ä¸–ç•ŒçŸ¥è¯†è’¸é¦ï¼ˆWKDï¼‰ï¼Œä½¿LLMsä»å·¥å…·ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆä¸­å­¦ä¹ é¢†åŸŸçŸ¥è¯†ã€‚ç¬¬äºŒä¸ªç»„ä»¶æ˜¯å·¥å…·ä½¿ç”¨é€‚åº”ï¼ˆTUAï¼‰ï¼Œæ ¹æ®æ¨¡å‹çš„ç›´æ¥å›ç­”å‡†ç¡®æ€§å°†é—®é¢˜åˆ†ä¸ºç®€å•å’Œå›°éš¾ä¸¤ç±»ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸Šçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.23775",
            "title": "In-Context LoRA for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2410.23775",
            "abstract": "Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20sim 100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA",
            "score": 2,
            "issue_id": 407,
            "pub_date": "2024-10-31",
            "pub_date_card": {
                "ru": "31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 31",
                "zh": "10æœˆ31æ—¥"
            },
            "hash": "748dab03a37a21a4",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° DiT Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DiT Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ pipeline, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ LoRA-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ IC-LoRA, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼."
                },
                "en": {
                    "title": "Unlocking In-Context Generation with IC-LoRA",
                    "desc": "This paper investigates the use of diffusion transformers (DiTs) for generating images without being tied to specific tasks. The authors propose that DiTs can generate images effectively with minimal adjustments, leveraging their inherent in-context generation capabilities. They introduce a new method called In-Context LoRA (IC-LoRA), which simplifies the process by concatenating images and using joint captioning, along with small dataset tuning. This approach enhances the quality of generated images while maintaining a flexible architecture that can adapt to various tasks without extensive retraining."
                },
                "zh": {
                    "title": "æ¿€æ´»ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰åœ¨æ— ä»»åŠ¡ç‰¹å®šçš„å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„DiTsæœ¬èº«å…·å¤‡ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œåªéœ€å°‘é‡è°ƒæ•´å³å¯æ¿€æ´»ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒDiTsèƒ½å¤Ÿåœ¨ä¸è°ƒæ•´çš„æƒ…å†µä¸‹æœ‰æ•ˆè¿›è¡Œä¸Šä¸‹æ–‡ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æµç¨‹ï¼Œåˆ©ç”¨DiTsçš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œç”Ÿæˆé«˜ä¿çœŸåº¦çš„å›¾åƒé›†ï¼Œä¸”ä¸éœ€è¦å¯¹åŸå§‹æ¨¡å‹è¿›è¡Œä¿®æ”¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.22901",
            "title": "HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models",
            "url": "https://huggingface.co/papers/2410.22901",
            "abstract": "We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (https://songkey.github.io/hellomeme).",
            "score": 1,
            "issue_id": 408,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "801963cbdcf75d7b",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¼Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ² Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¼Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ SD1.5, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Models with Adapter Integration",
                    "desc": "This paper presents a novel method for integrating adapters into text-to-image foundation models, allowing them to perform complex tasks while maintaining their ability to generalize. The method focuses on optimizing the attention mechanism associated with 2D feature maps, which significantly boosts the performance of the adapters. The effectiveness of this approach was demonstrated through the task of meme video generation, yielding impressive results. The authors aim to contribute to the open-source community by sharing their code and providing insights for post-training tasks in large text-to-image models."
                },
                "zh": {
                    "title": "é€‚é…å™¨æ’å…¥ï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†é€‚é…å™¨æ’å…¥æ–‡æœ¬åˆ°å›¾åƒçš„åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»è€Œåœ¨æ‰§è¡Œå¤æ‚çš„ä¸‹æ¸¸ä»»åŠ¡æ—¶ä¿æŒåŸºç¡€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¼˜åŒ–ä¸äºŒç»´ç‰¹å¾å›¾ç›¸å…³çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œå¢å¼ºé€‚é…å™¨çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ç”Ÿæˆè¡¨æƒ…åŒ…è§†é¢‘çš„ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºå¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åè®­ç»ƒä»»åŠ¡æä¾›ä¸€äº›è§è§£ï¼Œå¹¶ä¸ºå¼€æºç¤¾åŒºå¸¦æ¥ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.21157",
            "title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation",
            "url": "https://huggingface.co/papers/2410.21157",
            "abstract": "Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.",
            "score": 1,
            "issue_id": 408,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "d6a0779456870cae",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#plp",
                    "#multilingual"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº M2RC-EVAL Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 18 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… M2RC-INSTRUCT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LLM Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Multilingual Code Completion with M2RC-EVAL and M2RC-INSTRUCT",
                    "desc": "This paper introduces a new benchmark called M2RC-EVAL for repository-level code completion that supports 18 programming languages, addressing the limitations of existing benchmarks that only cover a few languages. It provides fine-grained annotations based on abstract syntax trees, allowing for a more detailed evaluation of code completion scenarios. Additionally, the authors present the M2RC-INSTRUCT dataset to enhance the performance of code Large Language Models (LLMs) in multilingual contexts. Experimental results show that both M2RC-EVAL and M2RC-INSTRUCT significantly improve the capabilities of existing code LLMs."
                },
                "zh": {
                    "title": "å¤šè¯­è¨€ä»£ç è¡¥å…¨çš„æ–°åŸºå‡†æµ‹è¯•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šè¯­è¨€ä»£ç è¡¥å…¨åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºM2RC-EVALï¼Œæ¶µç›–äº†18ç§ç¼–ç¨‹è¯­è¨€ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸åªå…³æ³¨å°‘æ•°å‡ ç§è¯­è¨€ï¼Œæ— æ³•å…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸­çš„ä»£ç æ™ºèƒ½èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒM2RC-EVALæä¾›äº†ç»†ç²’åº¦çš„æ³¨é‡Šï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£æ¨¡å‹åœ¨ä¸åŒè¡¥å…¨åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ä»£ç è¡¥å…¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªå¤šè¯­è¨€æŒ‡ä»¤æ•°æ®é›†M2RC-INSTRUCTã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-01.html",
    "link_next": "2024-11-05.html",
    "short_date_prev": {
        "ru": "01.11",
        "en": "11/01",
        "zh": "11æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "05.11",
        "en": "11/05",
        "zh": "11æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 2,
        "#medicine": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ã€‚SAEså¯ä»¥å°†ä¸æ˜“è§£é‡Šçš„ä¸­é—´è¡¨ç¤ºåˆ†è§£æˆæ˜“è§£é‡Šçš„ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°æ§åˆ¶å’Œåˆ†æã€‚ç„¶è€Œï¼Œç±»ä¼¼çš„åˆ†æå’Œæ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹ä¸­å°šç¼ºä¹ç ”ç©¶ã€‚ä½œè€…ç ”ç©¶äº†åœ¨å°‘æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚SDXL Turboï¼‰ä¸­ä½¿ç”¨SAEså­¦ä¹ å¯è§£é‡Šç‰¹å¾çš„å¯èƒ½æ€§ã€‚ç»“æœå‘ç°ï¼ŒSAEså­¦åˆ°çš„ç‰¹å¾å¯è§£é‡Šï¼Œå¹¶å¯¹ç”Ÿæˆè¿‡ç¨‹äº§ç”Ÿå› æœå½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹å†…éƒ¨çš„ä¸“ä¸šåŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªæ¨¡å—ä¸»è¦å¤„ç†å›¾åƒç»„åˆï¼Œä¸€ä¸ªè´Ÿè´£æ·»åŠ ç»†èŠ‚ï¼Œå¦ä¸€ä¸ªè´Ÿè´£é¢œè‰²ã€å…‰ç…§å’Œé£æ ¼ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œæ˜¯ç†è§£ç”Ÿæˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å†…éƒ¨çš„é‡è¦ä¸€æ­¥ï¼Œå±•ç¤ºäº†SAEsåœ¨è§†è§‰é¢†åŸŸçš„æ½œåŠ›ã€‚",
        "title": "Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ã€‚SAEså¯ä»¥å°†ä¸æ˜“è§£é‡Šçš„ä¸­é—´è¡¨ç¤ºåˆ†è§£æˆæ˜“è§£é‡Šçš„ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°æ§åˆ¶å’Œåˆ†æã€‚ç„¶è€Œï¼Œç±»ä¼¼çš„åˆ†æå’Œæ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹ä¸­å°šç¼ºä¹ç ”ç©¶ã€‚ä½œè€…ç ”ç©¶äº†åœ¨å°‘æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚SDXL Turboï¼‰ä¸­ä½¿ç”¨SAEså­¦ä¹ å¯è§£é‡Šç‰¹å¾çš„å¯èƒ½æ€§ã€‚ç»“æœå‘ç°ï¼ŒSAEså­¦åˆ°çš„ç‰¹å¾å¯è§£é‡Šï¼Œå¹¶å¯¹ç”Ÿæˆè¿‡ç¨‹äº§ç”Ÿå› æœå½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹å†…éƒ¨çš„ä¸“ä¸šåŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªæ¨¡å—ä¸»è¦å¤„ç†å›¾åƒç»„åˆï¼Œä¸€ä¸ªè´Ÿè´£æ·»åŠ ç»†èŠ‚ï¼Œå¦ä¸€ä¸ªè´Ÿè´£é¢œè‰²ã€å…‰ç…§å’Œé£æ ¼ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œæ˜¯ç†è§£ç”Ÿæˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å†…éƒ¨çš„é‡è¦ä¸€æ­¥ï¼Œå±•ç¤ºäº†SAEsåœ¨è§†è§‰é¢†åŸŸçš„æ½œåŠ›ã€‚\n\nPinyin transcription:\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le xÄ«shÅ« zÃ¬biÇnmÇqÃ¬ (SAEs) zÃ i dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zhÅng de yÃ¬ngyÃ²ng. SAEs kÄ›yÇ jiÄng bÃ¹yÃ¬ jiÄ›shÃ¬ de zhÅngjiÄn biÇoshÃ¬ fÄ“njiÄ› chÃ©ng yÃ¬ jiÄ›shÃ¬ de tÃ¨zhÄ“ng, cÃ³ng'Ã©r gÃ¨ng hÇo de kÃ²ngzhÃ¬ hÃ© fÄ“nxÄ«. RÃ¡n'Ã©r, lÃ¨isÃ¬ de fÄ“nxÄ« hÃ© fÄngfÇ zÃ i wÃ©nbÄ›n dÃ o tÃºxiÃ ng de mÃ³xÃ­ng zhÅng shÃ ng quÄ“fÃ¡ yÃ¡njiÅ«. ZuÃ²zhÄ› yÃ¡njiÅ« le zÃ i shÇo bÃ¹ wÃ©nbÄ›n dÃ o tÃºxiÃ ng kuÃ²sÃ n mÃ³xÃ­ng (rÃº SDXL Turbo) zhÅng shÇyÃ²ng SAEs xuÃ©xÃ­ kÄ› jiÄ›shÃ¬ tÃ¨zhÄ“ng de kÄ›nÃ©ngxÃ¬ng. JiÃ©guÇ’ fÄxiÃ n, SAEs xuÃ© dÃ o de tÃ¨zhÄ“ng kÄ› jiÄ›shÃ¬, bÃ¬ng duÃ¬ shÄ“ngchÃ©ng guÃ²chÃ©ng chÇnshÄ“ng yÄ«nguÇ’ yÇngxiÇng, jiÄ“shÃ¬ le mÃ³xÃ­ng nÃ¨ibÃ¹ de zhuÄnmÃ©nhuÃ . JÃ¹tÇ lÃ¡i shuÅ, yÇ’u yÄ«gÃ¨ mÃ³kuÃ i zhÇ”yÃ o chÇ”lÇ tÃºxiÃ ng zÇ”hÃ©, yÄ«gÃ¨ fÃ¹zÃ© tiÄnjiÇ xÃ¬jiÄ›, lÃ¬ng yÄ«gÃ¨ fÃ¹zÃ© yÃ¡nsÃ¨, guÄngzhÃ o hÃ© fÄ“nggÃ©. YÄ«ncÇ, zhÃ¨ xiÃ ng gÅngzuÃ² shÃ¬ liÇojiÄ› shÄ“ngchÃ©ng wÃ©nbÄ›n dÃ o tÃºxiÃ ng mÃ³xÃ­ng nÃ¨ibÃ¹ de zhÃ²ngyÃ o yÄ« bÃ¹, zhÇnshÃ¬ le SAEs zÃ i shÃ¬juÃ© lÇngyÃ¹ de qiÃ¡nlÃ¬.",
        "vocab": "[\n    {\"word\": \"ç¨€ç–è‡ªç¼–ç å™¨\", \"pinyin\": \"xÄ« shÅ« zÃ¬ biÄn mÇ qÃ¬\", \"trans\": \"Sparse Autoencoder\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"ä¸­é—´è¡¨ç¤º\", \"pinyin\": \"zhÅng jiÄn biÇo shÃ¬\", \"trans\": \"Intermediate Representation\"},\n    {\"word\": \"ç‰¹å¾\", \"pinyin\": \"tÃ¨ zhÄ“ng\", \"trans\": \"Feature\"},\n    {\"word\": \"åˆ†è§£\", \"pinyin\": \"fÄ“n jiÄ›\", \"trans\": \"Decompose\"},\n    {\"word\": \"æ§åˆ¶\", \"pinyin\": \"kÃ²ng zhÃ¬\", \"trans\": \"Control\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“n xÄ«\", \"trans\": \"Analyze\"},\n    {\"word\": \"æ–‡æœ¬åˆ°å›¾åƒ\", \"pinyin\": \"wÃ©n bÄ›n dÃ o tÃº xiÃ ng\", \"trans\": \"Text-to-Image\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"Model\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"Research\"},\n    {\"word\": \"å°‘æ­¥\", \"pinyin\": \"shÇo bÃ¹\", \"trans\": \"Few-Step\"},\n    {\"word\": \"æ‰©æ•£æ¨¡å‹\", \"pinyin\": \"kuÃ² sÃ n mÃ³ xÃ­ng\", \"trans\": \"Diffusion Model\"},\n    {\"word\": \"å¯è§£é‡Š\", \"pinyin\": \"kÄ› jiÄ› shÃ¬\", \"trans\": \"Interpretable\"},\n    {\"word\": \"å› æœå½±å“\", \"pinyin\": \"yÄ«n guÇ’ yÇng xiÇng\", \"trans\": \"Causal Effect\"},\n    {\"word\": \"æ­ç¤º\", \"pinyin\": \"jiÄ“ shÃ¬\", \"trans\": \"Reveal\"},\n    {\"word\": \"ä¸“ä¸šåŒ–\", \"pinyin\": \"zhuÄn yÃ¨ huÃ \", \"trans\": \"Specialization\"},\n    {\"word\": \"æ¨¡å—\", \"pinyin\": \"mÃ³ kuÃ i\", \"trans\": \"Module\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"Process\"},\n    {\"word\": \"å›¾åƒç»„åˆ\", \"pinyin\": \"tÃº xiÃ ng zÇ” hÃ©\", \"trans\": \"Image Composition\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬ jiÄ›\", \"trans\": \"Detail\"},\n    {\"word\": \"é¢œè‰²\", \"pinyin\": \"yÃ¡n sÃ¨\", \"trans\": \"Color\"},\n    {\"word\": \"å…‰ç…§\", \"pinyin\": \"guÄng zhÃ o\", \"trans\": \"Lighting\"},\n    {\"word\": \"é£æ ¼\", \"pinyin\": \"fÄ“ng gÄ“\", \"trans\": \"Style\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"Generate\"},\n    {\"word\": \"è§†è§‰é¢†åŸŸ\", \"pinyin\": \"shÃ¬ juÃ© lÇng yÃ¹\", \"trans\": \"Visual Domain\"},\n    {\"word\": \"æ½œåŠ›\", \"pinyin\": \"qiÃ¡n lÃ¬\", \"trans\": \"Potential\"}\n]",
        "trans": "This article discusses the application of Sparse Autoencoders (SAEs) in large language models (LLMs). SAEs can decompose hard-to-interpret intermediate representations into interpretable features, thereby enabling better control and analysis. However, similar analyses and methods in text-to-image models are still lacking in research. The authors investigated the possibility of using SAEs to learn interpretable features in few-step text-to-image diffusion models (such as SDXL Turbo). The results showed that the features learned by SAEs are interpretable and have a causal impact on the generation process, revealing specialization within the model. Specifically, one module mainly handles image composition, another is responsible for adding details, and another deals with color, lighting, and style. Therefore, this work is an important step towards understanding the internal workings of text-to-image generation models and demonstrates the potential of SAEs in the visual domain.",
        "update_ts": "2024-11-03 09:12"
    }
}