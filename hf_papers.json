{
    "date": {
        "ru": "8 сентября",
        "en": "September 8",
        "zh": "9月8日"
    },
    "time_utc": "2025-09-08 19:09",
    "weekday": 0,
    "issue_id": 5777,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.04664",
            "title": "Why Language Models Hallucinate",
            "url": "https://huggingface.co/papers/2509.04664",
            "abstract": "Language models produce incorrect statements due to training and evaluation procedures that reward guessing over acknowledging uncertainty, leading to a need for socio-technical changes in benchmark scoring.  \t\t\t\t\tAI-generated summary \t\t\t\t Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art systems and undermine trust. We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious -- they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. We then argue that hallucinations persist due to the way most evaluations are graded -- language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This \"epidemic\" of penalizing uncertain responses can only be addressed through a socio-technical mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems.",
            "score": 66,
            "issue_id": 5763,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "a9af1f035c82b958",
            "authors": [
                "Adam Tauman Kalai",
                "Ofir Nachum",
                "Santosh S. Vempala",
                "Edwin Zhang"
            ],
            "affiliations": [
                "Georgia Tech",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04664.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#training",
                    "#ethics",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Переосмысление оценки языковых моделей для борьбы с галлюцинациями",
                    "desc": "Статья рассматривает проблему галлюцинаций в языковых моделях, когда они генерируют правдоподобные, но неверные утверждения вместо признания неопределенности. Авторы утверждают, что это происходит из-за процедур обучения и оценки, которые поощряют угадывание. Они анализируют статистические причины галлюцинаций в современном процессе обучения моделей. Предлагается изменить систему оценки существующих бенчмарков, чтобы стимулировать разработку более надежных систем ИИ."
                },
                "en": {
                    "title": "Transforming AI Trustworthiness by Addressing Hallucinations",
                    "desc": "This paper discusses how large language models often produce incorrect statements, known as 'hallucinations', due to their training and evaluation methods. These models are rewarded for guessing answers even when they are uncertain, which leads to plausible but false outputs. The authors analyze how these hallucinations stem from errors in binary classification and the statistical pressures in the training process. They propose that to reduce these hallucinations, the scoring systems of benchmarks should be modified to discourage guessing and promote acknowledgment of uncertainty, ultimately fostering more reliable AI systems."
                },
                "zh": {
                    "title": "改进评分机制，提升语言模型可信度",
                    "desc": "这篇论文讨论了语言模型在训练和评估过程中产生错误陈述的原因。由于现有的评分机制奖励猜测而非承认不确定性，导致模型在面对不确定时倾向于猜测，从而产生虚假信息。作者分析了现代训练流程中导致这些“幻觉”的统计原因，并指出这些错误源于二元分类中的错误。为了提高语言模型的可信度，论文建议对现有基准的评分方式进行社会技术上的调整，而不是增加新的评估方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.05208",
            "title": "Symbolic Graphics Programming with Large Language Models",
            "url": "https://huggingface.co/papers/2509.05208",
            "abstract": "LLMs generate SVGs from natural-language descriptions using a reinforcement learning approach with verifiable rewards, improving performance and scene coherence.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable vector graphics (SVGs). We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards approach, where a format-validity gate ensures renderable SVG, and a cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers a precise and interpretable lens on cross-modal grounding.",
            "score": 27,
            "issue_id": 5767,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "c1d059781774f265",
            "authors": [
                "Yamei Chen",
                "Haoquan Zhang",
                "Yangyi Huang",
                "Zeju Qiu",
                "Kaipeng Zhang",
                "Yandong Wen",
                "Weiyang Liu"
            ],
            "affiliations": [
                "Max Planck Institute for Intelligent Systems",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.05208.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#games",
                    "#interpretability",
                    "#optimization",
                    "#rl",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "ИИ рисует векторную графику по текстовым описаниям",
                    "desc": "Статья исследует способность больших языковых моделей (LLM) генерировать программы символьной графики (SGP) из текстовых описаний, фокусируясь на масштабируемой векторной графике (SVG). Авторы представляют бенчмарк SGP-GenBench для оценки качества генерации SGP и обнаруживают значительное превосходство проприетарных моделей над открытыми. Предлагается подход обучения с подкреплением (RL) с проверяемыми наградами для улучшения генерации SVG, используя кросс-модальные награды на основе сильных энкодеров изображений. Результаты показывают, что RL улучшает декомпозицию объектов и контекстуальные детали, повышая качество и семантическую согласованность сгенерированных SVG."
                },
                "en": {
                    "title": "Enhancing SVG Generation with Reinforcement Learning",
                    "desc": "This paper explores how large language models (LLMs) can generate scalable vector graphics (SVGs) from natural-language descriptions using a reinforcement learning (RL) approach. The authors introduce SGP-GenBench, a benchmark that evaluates the quality of symbolic graphics programs (SGPs) based on object fidelity, scene fidelity, and compositionality. They find that proprietary models outperform open-source ones, and they propose a method that uses verifiable rewards to enhance the generation of SVGs. The results show that their approach significantly improves the quality and coherence of the generated graphics, providing insights into how LLMs understand visual content."
                },
                "zh": {
                    "title": "用强化学习提升SVG生成质量",
                    "desc": "本文研究了大型语言模型（LLMs）在从自然语言描述生成符号图形程序（SGPs）方面的能力，特别是可缩放矢量图形（SVGs）。我们提出了SGP-GenBench基准，评估对象保真度、场景保真度和组合性等指标，发现前沿的专有模型在生成SGPs方面显著优于开源模型。为了解决这一差距，我们采用了带有可验证奖励的强化学习方法，确保生成的SVG格式有效，并通过强大的视觉编码器对文本和渲染图像进行对齐。实验结果表明，我们的方法显著提高了SVG生成的质量和语义，达到了与前沿系统相当的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04185",
            "title": "Set Block Decoding is a Language Model Inference Accelerator",
            "url": "https://huggingface.co/papers/2509.04185",
            "abstract": "Set Block Decoding accelerates language model generation by integrating next token prediction and masked token prediction, enabling parallel sampling of future tokens and reducing computational cost without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training.",
            "score": 20,
            "issue_id": 5770,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "43590e8d94e403ca",
            "authors": [
                "Itai Gat",
                "Heli Ben-Hamu",
                "Marton Havasi",
                "Daniel Haziza",
                "Jeremy Reizenstein",
                "Gabriel Synnaeve",
                "David Lopez-Paz",
                "Brian Karrer",
                "Yaron Lipman"
            ],
            "affiliations": [
                "FAIR at Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04185.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#inference"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Set Block Decoding: параллельная генерация для ускорения языковых моделей",
                    "desc": "Статья представляет новый метод ускорения генерации текста языковыми моделями под названием Set Block Decoding (SBD). SBD объединяет предсказание следующего токена и предсказание маскированных токенов, позволяя параллельно сэмплировать несколько будущих токенов. Это снижает вычислительные затраты без потери точности. Метод не требует изменений в архитектуре модели и совместим с точным KV-кэшированием."
                },
                "en": {
                    "title": "Accelerate Language Generation with Set Block Decoding!",
                    "desc": "Set Block Decoding (SBD) is a novel approach that enhances the efficiency of language model generation by combining next token prediction (NTP) and masked token prediction (MATP) in one framework. This method allows for the parallel sampling of multiple future tokens, which significantly reduces the computational and memory costs associated with inference. By leveraging advanced solvers from discrete diffusion techniques, SBD achieves faster generation speeds without compromising the model's accuracy. Importantly, SBD can be implemented without altering the model architecture or requiring additional training parameters, making it a practical solution for existing models like Llama-3.1 and Qwen-3."
                },
                "zh": {
                    "title": "集合块解码：加速语言模型生成的创新方法",
                    "desc": "本文介绍了一种名为集合块解码（Set Block Decoding, SBD）的方法，旨在加速语言模型的生成过程。SBD通过将下一标记预测（Next Token Prediction, NTP）和掩码标记预测（Masked Token Prediction, MATP）结合在一个架构中，实现了并行采样多个未来标记。与以往的加速方法不同，SBD允许模型同时生成多个不一定连续的标记，从而显著降低计算成本。通过对现有模型进行微调，SBD在保持准确性的同时，减少了生成所需的前向传递次数，提升了生成效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04744",
            "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning",
            "url": "https://huggingface.co/papers/2509.04744",
            "abstract": "WildScore evaluates MLLMs' symbolic music reasoning through a benchmark of real-world music scores and user-generated queries, revealing both strengths and challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, their reasoning abilities in the multimodal symbolic music domain remain largely unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to interpret real-world music scores and answer complex musicological queries. Each instance in WildScore is sourced from genuine musical compositions and accompanied by authentic user-generated questions and discussions, capturing the intricacies of practical music analysis. To facilitate systematic evaluation, we propose a systematic taxonomy, comprising both high-level and fine-grained musicological ontologies. Furthermore, we frame complex music reasoning as multiple-choice question answering, enabling controlled and scalable assessment of MLLMs' symbolic music understanding. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns in their visual-symbolic reasoning, uncovering both promising directions and persistent challenges for MLLMs in symbolic music reasoning and analysis. We release the dataset and code.",
            "score": 8,
            "issue_id": 5761,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "44d75a7c2c61a026",
            "authors": [
                "Gagan Mundada",
                "Yash Vishe",
                "Amit Namburi",
                "Xin Xu",
                "Zachary Novack",
                "Julian McAuley",
                "Junda Wu"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04744.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#survey"
                ],
                "emoji": "🎼",
                "ru": {
                    "title": "WildScore: новый рубеж в понимании музыки искусственным интеллектом",
                    "desc": "Статья представляет WildScore - первый бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) в области анализа и рассуждений о символической музыке. Бенчмарк состоит из реальных музыкальных партитур и вопросов пользователей, охватывая сложности практического музыкального анализа. Авторы предлагают систематическую таксономию и формулируют задачу как ответы на вопросы с множественным выбором. Эмпирическое тестирование современных MLLM на WildScore выявляет как перспективные направления, так и сохраняющиеся проблемы в области рассуждений о символической музыке."
                },
                "en": {
                    "title": "WildScore: Unlocking MLLMs' Music Reasoning Potential",
                    "desc": "WildScore is a benchmark designed to assess the reasoning abilities of Multimodal Large Language Models (MLLMs) in the context of symbolic music. It evaluates how well these models can interpret real-world music scores and respond to complex questions about music. The benchmark includes genuine musical compositions and user-generated queries, providing a realistic setting for analysis. By framing music reasoning as multiple-choice questions, WildScore allows for systematic evaluation of MLLMs' understanding of music, revealing both their strengths and areas needing improvement."
                },
                "zh": {
                    "title": "WildScore：音乐推理的新基准",
                    "desc": "WildScore是一个评估多模态大型语言模型（MLLMs）在符号音乐推理能力的基准测试。它通过真实的音乐乐谱和用户生成的查询，揭示了这些模型在音乐分析中的优势和挑战。该基准测试采用了系统的分类法，涵盖了高层次和细粒度的音乐学本体。通过将复杂的音乐推理框架化为多项选择题回答，WildScore为MLLMs的符号音乐理解提供了可控和可扩展的评估方式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.05263",
            "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
            "url": "https://huggingface.co/papers/2509.05263",
            "abstract": "LatticeWorld, a 3D world generation framework using lightweight LLMs and Unreal Engine 5, creates dynamic, interactive environments from textual and visual inputs, achieving high accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a 90times increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18",
            "score": 6,
            "issue_id": 5761,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "027e61af3a0f5c1a",
            "authors": [
                "Yinglin Duan",
                "Zhengxia Zou",
                "Tongwei Gu",
                "Wei Jia",
                "Zhan Zhao",
                "Luyi Xu",
                "Xinzhu Liu",
                "Hao Jiang",
                "Kang Chen",
                "Shuang Qiu"
            ],
            "affiliations": [
                "Beihang University, China",
                "City University of Hong Kong, China",
                "NetEase, Inc., China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.05263.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#optimization",
                    "#agents",
                    "#3d"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Генерация 3D-миров на основе ИИ: быстро, точно, реалистично",
                    "desc": "LatticeWorld - это фреймворк для создания трёхмерных миров, использующий облегчённые языковые модели и Unreal Engine 5. Система принимает текстовые и визуальные инструкции для генерации динамических интерактивных сред. LatticeWorld достигает высокой точности в создании планировки сцен и визуальной достоверности. Фреймворк значительно повышает эффективность промышленного производства по сравнению с традиционными ручными методами."
                },
                "en": {
                    "title": "Revolutionizing 3D World Generation with LatticeWorld",
                    "desc": "LatticeWorld is a 3D world generation framework that utilizes lightweight large language models (LLMs) and Unreal Engine 5 to create interactive environments from both textual and visual inputs. This framework aims to enhance the realism of simulations, bridging the gap between simulated and real-world scenarios, which is crucial for applications like autonomous driving and embodied AI. By employing generative methods, LatticeWorld can produce large-scale 3D worlds with dynamic agents, showcasing high-fidelity physics and real-time rendering capabilities. The results demonstrate a significant increase in production efficiency, achieving over 90 times faster output compared to traditional modeling techniques while maintaining high visual quality."
                },
                "zh": {
                    "title": "LatticeWorld：高效生成动态3D世界的创新框架",
                    "desc": "LatticeWorld是一个使用轻量级大语言模型和虚幻引擎5的3D世界生成框架。它能够根据文本和视觉输入创建动态、互动的环境，具有高准确性和效率。该框架通过多模态输入生成大规模的3D互动世界，支持动态代理和高保真物理模拟。与传统手动建模方法相比，LatticeWorld在工业生产效率上提高了90倍，同时保持了高创意质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.03680",
            "title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer",
            "url": "https://huggingface.co/papers/2509.03680",
            "abstract": "LuxDiT, a video diffusion transformer fine-tuned with low-rank adaptation, generates accurate HDR environment maps from visual input, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.",
            "score": 6,
            "issue_id": 5766,
            "pub_date": "2025-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "aa0dec923ff8c683",
            "authors": [
                "Ruofan Liang",
                "Kai He",
                "Zan Gojcic",
                "Igor Gilitschenski",
                "Sanja Fidler",
                "Nandita Vijaykumar",
                "Zian Wang"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.03680.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#synthetic",
                    "#cv",
                    "#dataset",
                    "#training",
                    "#video"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "LuxDiT: Трансформер для реалистичного HDR-освещения из одного изображения",
                    "desc": "LuxDiT - это новый подход к оценке освещения сцены из одного изображения или видео. Метод основан на видео-диффузионном трансформере, дообученном с помощью низкоранговой адаптации. LuxDiT генерирует точные HDR-карты окружения на основе визуального ввода, превосходя существующие методы. Модель обучена на большом синтетическом наборе данных с разнообразными условиями освещения и эффективно обобщается на реальные сцены."
                },
                "en": {
                    "title": "LuxDiT: Revolutionizing HDR Lighting Estimation with Video Diffusion Transformers",
                    "desc": "LuxDiT is a novel machine learning model that generates high dynamic range (HDR) environment maps from images or videos. It uses a video diffusion transformer that has been fine-tuned with low-rank adaptation to improve the accuracy of lighting predictions. The model is trained on a large synthetic dataset, allowing it to learn diverse lighting conditions and effectively generalize to real-world scenarios. By enhancing the semantic alignment between input visuals and the generated maps, LuxDiT surpasses existing methods in both quality and performance."
                },
                "zh": {
                    "title": "LuxDiT：高效生成HDR环境图的创新方法",
                    "desc": "LuxDiT是一种新型的数据驱动方法，利用视频扩散变换器生成高动态范围（HDR）环境图。该模型通过低秩适应微调，能够从视觉输入中推断照明信息，克服了传统方法在真实场景中的局限性。它在一个包含多样化光照条件的大型合成数据集上进行训练，能够有效地从间接视觉线索中学习。与现有技术相比，LuxDiT在定量和定性评估中均表现出色，生成的照明预测具有真实的高频细节。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.05296",
            "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
            "url": "https://huggingface.co/papers/2509.05296",
            "abstract": "WinT3R, a feed-forward reconstruction model, achieves high-quality camera pose estimation and real-time performance using a sliding window mechanism and a global camera token pool.  \t\t\t\t\tAI-generated summary \t\t\t\t We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.",
            "score": 3,
            "issue_id": 5764,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "b6ac447839602a03",
            "authors": [
                "Zizun Li",
                "Jianjun Zhou",
                "Yifan Wang",
                "Haoyu Guo",
                "Wenzheng Chang",
                "Yang Zhou",
                "Haoyi Zhu",
                "Junyi Chen",
                "Chunhua Shen",
                "Tong He"
            ],
            "affiliations": [
                "SII",
                "Shanghai AI Lab",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.05296.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "WinT3R: Революция в реконструкции камер в реальном времени",
                    "desc": "WinT3R - это модель прямого распространения для реконструкции, которая обеспечивает высококачественную оценку положения камеры и работу в режиме реального времени. Модель использует механизм скользящего окна для обмена информацией между кадрами, что улучшает качество геометрических предсказаний. WinT3R также применяет компактное представление камер и глобальный пул токенов камеры для повышения надежности оценки положения. Эти инновации позволяют модели достичь передовых результатов в онлайн-реконструкции, оценке положения камеры и скорости реконструкции."
                },
                "en": {
                    "title": "WinT3R: Real-Time Camera Pose Estimation with High Precision",
                    "desc": "WinT3R is a feed-forward reconstruction model designed for accurate camera pose estimation and efficient real-time performance. It utilizes a sliding window mechanism to facilitate effective information sharing among frames, enhancing the quality of geometric predictions while minimizing computational load. Additionally, the model incorporates a global camera token pool, which improves the reliability of pose estimation without compromising speed. As a result, WinT3R achieves state-of-the-art performance in online reconstruction tasks, as demonstrated through extensive testing on various datasets."
                },
                "zh": {
                    "title": "WinT3R：高效精准的相机姿态估计",
                    "desc": "WinT3R是一种前馈重建模型，能够实时预测精确的相机姿态和高质量的点云地图。以往的方法在重建质量和实时性能之间存在权衡。为了解决这个问题，我们引入了滑动窗口机制，确保窗口内帧之间的信息充分交流，从而提高几何预测的质量。通过维护一个全局相机令牌池，WinT3R在不牺牲效率的情况下增强了相机姿态估计的可靠性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.03800",
            "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting",
            "url": "https://huggingface.co/papers/2509.03800",
            "abstract": "MedVista3D is a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis that addresses local-global understanding, report variability, and achieves state-of-the-art performance in disease classification, report retrieval, and medical visual question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Radiologic diagnostic errors-under-reading errors, inattentional blindness, and communication failures-remain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointly, lacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce a Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released.",
            "score": 3,
            "issue_id": 5762,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "ad0922456cbd778e",
            "authors": [
                "Yuheng Li",
                "Yenho Chen",
                "Yuxiang Lai",
                "Jike Zhong",
                "Vanessa Wildman",
                "Xiaofeng Yang"
            ],
            "affiliations": [
                "Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA",
                "Department of Computer Science, University of Southern California, Los Angeles, CA",
                "Department of Machine Learning, Georgia Institute of Technology, Atlanta, GA",
                "Department of Radiation Oncology, Emory University School of Medicine, Atlanta, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.03800.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#healthcare",
                    "#transfer_learning",
                    "#3d"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Улучшение анализа КТ с помощью многомасштабного обучения компьютерного зрения и обработки естественного языка",
                    "desc": "MedVista3D - это фреймворк для предварительного обучения многомасштабных семантически обогащенных моделей компьютерного зрения и обработки естественного языка для анализа 3D КТ-изображений. Он решает проблемы локально-глобального понимания и вариативности медицинских отчетов. MedVista3D использует выравнивание изображения и текста на локальном и глобальном уровнях для детального представления в контексте полного объема. Фреймворк достигает передовых результатов в классификации заболеваний, поиске отчетов и медицинских вопросно-ответных системах."
                },
                "en": {
                    "title": "Revolutionizing 3D CT Analysis with MedVista3D",
                    "desc": "MedVista3D is a new framework designed to improve the analysis of 3D CT scans by combining vision and language understanding. It tackles common problems in radiology, such as missing details and inconsistent report language, by enhancing local and global context in image analysis. The framework uses advanced techniques for aligning images with text, allowing for better disease detection and interpretation of medical reports. MedVista3D has shown to outperform existing models in tasks like disease classification and report retrieval, making it a significant advancement in medical imaging technology."
                },
                "zh": {
                    "title": "MedVista3D：提升3D CT分析的智能框架",
                    "desc": "MedVista3D是一个多尺度语义增强的视觉-语言预训练框架，专门用于3D CT分析。它解决了局部与全局理解、报告变异性等问题，并在疾病分类、报告检索和医学视觉问答中达到了最先进的性能。该框架通过局部和全局图像-文本对齐，实现了细粒度的表示学习，并引入了语义匹配库来处理报告的变异性。MedVista3D在零样本疾病分类和器官分割等任务中表现优异，展示了其在医学影像分析中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04013",
            "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
            "url": "https://huggingface.co/papers/2509.04013",
            "abstract": "LLMs show reduced effectiveness on paraphrased benchmark questions, indicating limitations in handling linguistic variability and suggesting the need for more robust evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in a fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmark-based evaluations provide a reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture a model's robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios.",
            "score": 2,
            "issue_id": 5764,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "32f0ad5327f657e2",
            "authors": [
                "Riccardo Lunardi",
                "Vincenzo Della Mea",
                "Stefano Mizzaro",
                "Kevin Roitero"
            ],
            "affiliations": [
                "University of Udine, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04013.jpg",
            "data": {
                "categories": [
                    "#evaluation",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Языковые модели спотыкаются о перефразированные вопросы",
                    "desc": "Исследование показало, что большие языковые модели (LLM) менее эффективны при работе с перефразированными вопросами из стандартных тестов. Это указывает на ограничения LLM в обработке лингвистических вариаций. Результаты ставят под сомнение надежность оценки моделей на основе существующих бенчмарков. Исследователи подчеркивают необходимость разработки более устойчивых методов оценки, которые лучше отражают реальные сценарии использования LLM."
                },
                "en": {
                    "title": "Evaluating LLMs: Beyond Fixed Benchmarks to Real-World Language Variability",
                    "desc": "This paper investigates how well Large Language Models (LLMs) perform when faced with paraphrased questions, highlighting their limitations in dealing with linguistic variability. The authors found that while the rankings of LLMs remained stable, their effectiveness scores dropped significantly when questions were reworded. This indicates that current benchmark evaluations may not accurately reflect a model's ability to generalize to real-world language use. The study calls for the development of more robust evaluation methods that account for diverse question phrasing to better assess LLM capabilities."
                },
                "zh": {
                    "title": "提升LLMs鲁棒性，重塑评估标准",
                    "desc": "大型语言模型（LLMs）在处理同一问题的不同表述时效果较差，显示出其在语言变异性方面的局限性。这项研究系统地评估了LLMs对改写基准问题的鲁棒性，并探讨了基于基准的评估是否可靠。研究发现，尽管LLMs在不同表述下的排名相对稳定，但其绝对有效性得分显著下降。这表明LLMs在应对真实世界的语言变异时存在困难，呼吁开发更能反映实际应用场景的评估方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02437",
            "title": "U-ARM : Ultra low-cost general teleoperation interface for robot\n  manipulation",
            "url": "https://huggingface.co/papers/2509.02437",
            "abstract": "U-Arm is a low-cost, adaptable teleoperation framework for robotic arms that optimizes mechanical design and control logic to enhance data collection efficiency and task success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose U-Arm, a low-cost and rapidly adaptable leader-follower teleoperation framework designed to interface with most of commercially available robotic arms. Our system supports teleoperation through three structurally distinct 3D-printed leader arms that share consistent control logic, enabling seamless compatibility with diverse commercial robot configurations. Compared with previous open-source leader-follower interfaces, we further optimized both the mechanical design and servo selection, achieving a bill of materials (BOM) cost of only \\50.5 for the 6-DoF leader arm and 56.8 for the 7-DoF version. To enhance usability, we mitigate the common challenge in controlling redundant degrees of freedom by %engineering methods mechanical and control optimizations. Experimental results demonstrate that U-Arm achieves 39\\% higher data collection efficiency and comparable task success rates across multiple manipulation scenarios compared with Joycon, another low-cost teleoperation interface. We have open-sourced all CAD models of three configs and also provided simulation support for validating teleoperation workflows. We also open-sourced real-world manipulation data collected with U-Arm. The project website is https://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.",
            "score": 1,
            "issue_id": 5767,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "328b26798c2ef081",
            "authors": [
                "Yanwen Zou",
                "Zhaoye Zhou",
                "Chenyang Shi",
                "Zewei Ye",
                "Junda Huang",
                "Yan Ding",
                "Bo Zhao"
            ],
            "affiliations": [
                "EvoMind Tech",
                "IAAR-Shanghai",
                "Independent Researcher",
                "School of AI, SJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02437.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#robotics",
                    "#dataset"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "Доступное и универсальное телеуправление роботами",
                    "desc": "U-Arm - это недорогая и адаптируемая система телеуправления для роботизированных манипуляторов. Она оптимизирует механическую конструкцию и логику управления для повышения эффективности сбора данных и успешности выполнения задач. Система совместима с большинством коммерчески доступных роботов и поддерживает телеуправление через три различных 3D-печатных ведущих манипулятора. По сравнению с предыдущими открытыми интерфейсами, U-Arm достигает более низкой стоимости комплектующих и решает проблему управления избыточными степенями свободы."
                },
                "en": {
                    "title": "U-Arm: Affordable and Efficient Teleoperation for Robotic Arms",
                    "desc": "U-Arm is a cost-effective teleoperation framework designed for robotic arms, enhancing both mechanical design and control logic. It features three distinct 3D-printed leader arms that maintain consistent control, allowing compatibility with various commercial robots. The system has been optimized to reduce costs significantly while improving data collection efficiency by 39% compared to existing interfaces. Additionally, U-Arm addresses the challenges of controlling redundant degrees of freedom through engineering optimizations, making it a versatile tool for robotic manipulation tasks."
                },
                "zh": {
                    "title": "U-Arm：低成本高效的遥操作解决方案",
                    "desc": "U-Arm是一个低成本、可快速适应的遥操作框架，专为机器人手臂设计。它通过三种不同结构的3D打印领导臂，提供一致的控制逻辑，支持与多种商业机器人兼容。与之前的开源遥操作接口相比，U-Arm在机械设计和伺服选择上进行了优化，使得6自由度和7自由度的领导臂成本分别仅为50.5美元和56.8美元。实验结果表明，U-Arm在数据收集效率上提高了39%，并在多种操作场景中达到了与Joycon相当的任务成功率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04504",
            "title": "Behavioral Fingerprinting of Large Language Models",
            "url": "https://huggingface.co/papers/2509.04504",
            "abstract": "A Behavioral Fingerprinting framework evaluates Large Language Models using a Diagnostic Prompt Suite and automated pipeline, revealing divergent alignment behaviors and clustering patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated Diagnostic Prompt Suite and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting",
            "score": 1,
            "issue_id": 5763,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "c8bc7caa6cf21161",
            "authors": [
                "Zehua Pei",
                "Hui-Ling Zhen",
                "Ying Zhang",
                "Zhiyuan Yang",
                "Xing Li",
                "Xianzhi Yu",
                "Mingxuan Yuan",
                "Bei Yu"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04504.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#alignment",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Поведенческий отпечаток: новый взгляд на оценку языковых моделей",
                    "desc": "Статья представляет новую методологию оценки больших языковых моделей (LLM), названную 'Поведенческим отпечатком'. Этот подход использует набор диагностических промптов и автоматизированный конвейер оценки, где мощная LLM выступает в роли беспристрастного судьи. Исследование выявило значительные различия в поведении моделей, связанном с выравниванием (alignment), несмотря на сходство в базовых когнитивных способностях. Результаты показывают, что интерактивная природа модели является прямым следствием конкретных стратегий выравнивания, а не просто побочным эффектом масштаба или мощности рассуждений."
                },
                "en": {
                    "title": "Unveiling the Hidden Behaviors of Language Models",
                    "desc": "This paper presents a new framework called 'Behavioral Fingerprinting' to evaluate Large Language Models (LLMs) beyond just their performance metrics. It uses a Diagnostic Prompt Suite and an automated evaluation pipeline to analyze the cognitive and interactive styles of various models. The study finds that while core reasoning abilities are becoming similar among top models, their alignment behaviors, such as sycophancy and semantic robustness, show significant differences. This indicates that a model's behavior is influenced more by the developers' alignment strategies than by its size or reasoning capabilities."
                },
                "zh": {
                    "title": "揭示大型语言模型的行为特征",
                    "desc": "本文提出了一种新的“行为指纹”框架，用于评估大型语言模型（LLMs），超越传统的性能指标，关注模型的行为特征。通过使用精心设计的诊断提示套件和自动化评估流程，分析了十八种不同能力层次的模型。研究发现，尽管顶级模型在抽象和因果推理等核心能力上趋于一致，但在对齐相关的行为（如谄媚和语义稳健性）上却存在显著差异。该框架为揭示模型之间深层次的行为差异提供了一种可重复和可扩展的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04575",
            "title": "Bootstrapping Task Spaces for Self-Improvement",
            "url": "https://huggingface.co/papers/2509.04575",
            "abstract": "Exploratory Iteration (ExIt) is an autocurriculum RL method that trains LLMs to perform multi-step self-improvement at inference-time by selectively sampling informative intermediate histories, enabling strong self-improvement on unseen tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Progress in many task domains emerges from repeated revisions to previous solution attempts. Training agents that can reliably self-improve over such sequences at inference-time is a natural target for reinforcement learning (RL), yet the naive approach assumes a fixed maximum iteration depth, which can be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family of autocurriculum RL methods that directly exploits the recurrent structure of self-improvement tasks to train LLMs to perform multi-step self-improvement at inference-time while only training on the most informative single-step iterations. ExIt grows a task space by selectively sampling the most informative intermediate, partial histories encountered during an episode for continued iteration, treating these starting points as new self-iteration task instances to train a self-improvement policy. ExIt can further pair with explicit exploration mechanisms to sustain greater task diversity. Across several domains, encompassing competition math, multi-turn tool-use, and machine learning engineering, we demonstrate that ExIt strategies, starting from either a single or many task instances, can produce policies exhibiting strong inference-time self-improvement on held-out task instances, and the ability to iterate towards higher performance over a step budget extending beyond the average iteration depth encountered during training.",
            "score": 0,
            "issue_id": 5768,
            "pub_date": "2025-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "86e95856d92a57a9",
            "authors": [
                "Minqi Jiang",
                "Andrei Lupu",
                "Yoram Bachrach"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04575.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "ExIt: Самосовершенствование языковых моделей через исследовательские итерации",
                    "desc": "Exploratory Iteration (ExIt) - это метод обучения с подкреплением, который тренирует языковые модели для многошагового самосовершенствования во время вывода. ExIt выборочно отбирает наиболее информативные промежуточные истории для продолжения итерации, рассматривая их как новые экземпляры задач самоулучшения. Этот подход позволяет моделям эффективно улучшаться на невиданных ранее задачах, не ограничиваясь фиксированной глубиной итераций. ExIt продемонстрировал сильные результаты в различных областях, включая математические соревнования, многоэтапное использование инструментов и инженерию машинного обучения."
                },
                "en": {
                    "title": "Empowering Self-Improvement in LLMs with Exploratory Iteration",
                    "desc": "Exploratory Iteration (ExIt) is a novel reinforcement learning method designed to enhance the self-improvement capabilities of large language models (LLMs) during inference. It focuses on training agents to iteratively refine their solutions by selectively sampling the most informative intermediate steps from previous attempts. This approach allows LLMs to tackle unseen tasks more effectively by treating these sampled histories as new tasks for further improvement. The results show that ExIt can significantly boost performance across various domains by enabling agents to explore and iterate beyond their initial training experiences."
                },
                "zh": {
                    "title": "探索性迭代：强化学习中的自我改进新方法",
                    "desc": "探索性迭代（ExIt）是一种自适应课程强化学习方法，旨在训练大型语言模型（LLM）在推理时进行多步自我改进。该方法通过选择性地采样信息丰富的中间历史，帮助模型在未见过的任务上实现强大的自我提升。ExIt利用自我改进任务的递归结构，专注于最具信息量的单步迭代，从而扩展任务空间。通过与明确的探索机制结合，ExIt能够维持更大的任务多样性，展示出在多个领域的强大自我改进能力。"
                }
            }
        }
    ],
    "link_prev": "2025-09-05.html",
    "link_next": "2025-09-09.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "05.09",
        "en": "09/05",
        "zh": "9月5日"
    },
    "short_date_next": {
        "ru": "09.09",
        "en": "09/09",
        "zh": "9月9日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 3,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0,
        "#evaluation": 1
    }
}