{
    "date": {
        "ru": "3 марта",
        "en": "March 3",
        "zh": "3月3日"
    },
    "time_utc": "2025-03-03 08:15",
    "weekday": 0,
    "issue_id": 2491,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.20730",
            "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
            "url": "https://huggingface.co/papers/2502.20730",
            "abstract": "Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.",
            "score": 7,
            "issue_id": 2486,
            "pub_date": "2025-02-28",
            "pub_date_card": {
                "ru": "28 февраля",
                "en": "February 28",
                "zh": "2月28日"
            },
            "hash": "e9ef168e304ec240",
            "authors": [
                "Zhuoqun Li",
                "Haiyang Yu",
                "Xuanang Chen",
                "Hongyu Lin",
                "Yaojie Lu",
                "Fei Huang",
                "Xianpei Han",
                "Yongbin Li",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "Tongyi Lab",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20730.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "Умная система для проектирования сложных инженерных решений",
                    "desc": "Статья представляет новый бенчмарк SolutionBench для оценки способности систем генерировать решения инженерных задач с множественными ограничениями. Авторы предлагают систему SolutionRAG, использующую древовидное исследование и механизм двухточечного мышления для создания надежных решений. Экспериментальные результаты показывают, что SolutionRAG достигает наилучших показателей на SolutionBench. Это демонстрирует потенциал системы для улучшения автоматизации и надежности проектирования сложных инженерных решений в реальных приложениях."
                },
                "en": {
                    "title": "Revolutionizing Engineering Design with SolutionRAG",
                    "desc": "This paper addresses the need for effective solutions in complex engineering design tasks, which have been overlooked in previous research on retrieval-augmented generation (RAG). It introduces a new benchmark called SolutionBench, aimed at evaluating the generation of feasible solutions under multiple constraints. The authors propose a novel system named SolutionRAG, which utilizes tree-based exploration and bi-point thinking to improve solution reliability. Experimental results show that SolutionRAG outperforms existing methods, indicating its potential to automate and enhance the design process in engineering applications."
                },
                "zh": {
                    "title": "提升复杂工程设计的自动化与可靠性",
                    "desc": "本论文提出了一个新的基准测试，称为SolutionBench，用于评估系统在生成复杂工程问题的完整和可行解决方案方面的能力。我们还提出了一种新系统SolutionRAG，利用树形探索和双点思维机制来生成可靠的解决方案。通过大量实验结果，SolutionRAG在SolutionBench上达到了最先进的性能，显示了其在实际应用中提高复杂工程解决方案设计的自动化和可靠性的潜力。此研究填补了以往在检索增强生成（RAG）领域中对复杂工程解决方案设计任务的研究空白。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18600",
            "title": "Chain of Draft: Thinking Faster by Writing Less",
            "url": "https://huggingface.co/papers/2502.18600",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.",
            "score": 5,
            "issue_id": 2491,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 февраля",
                "en": "February 25",
                "zh": "2月25日"
            },
            "hash": "739d903f5735d9eb",
            "authors": [
                "Silei Xu",
                "Wenhao Xie",
                "Lingxiao Zhao",
                "Pengcheng He"
            ],
            "affiliations": [
                "Zoom Communications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18600.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "✍️",
                "ru": {
                    "title": "Эффективное рассуждение: краткость - сестра точности",
                    "desc": "Статья представляет новый подход к решению сложных задач с помощью больших языковых моделей - Chain of Draft (CoD). В отличие от метода Chain-of-Thought (CoT), который использует подробные рассуждения, CoD имитирует человеческий подход, генерируя краткие промежуточные мысли. Этот метод позволяет достичь той же или лучшей точности, что и CoT, но при этом использует значительно меньше токенов. CoD демонстрирует эффективность в различных задачах, требующих рассуждений, снижая затраты и задержки."
                },
                "en": {
                    "title": "Streamlining Reasoning: Less is More with Chain of Draft",
                    "desc": "This paper introduces Chain of Draft (CoD), a new approach for Large Language Models (LLMs) that mimics human reasoning by generating concise intermediate thoughts. Unlike the traditional Chain-of-Thought (CoT) prompting, which relies on verbose explanations, CoD focuses on delivering essential information in a minimalistic format. The authors demonstrate that CoD can achieve comparable or even superior accuracy to CoT while using significantly fewer tokens, leading to reduced computational costs and faster processing times. This innovative method enhances the efficiency of LLMs in tackling complex reasoning tasks."
                },
                "zh": {
                    "title": "草稿链：高效推理的新方法",
                    "desc": "大型语言模型（LLMs）在解决复杂推理任务方面表现出色，尤其是通过链式思维（CoT）提示，强调逐步推理的详细过程。然而，人类通常采用更高效的策略：草拟简洁的中间思考，只捕捉关键信息。本文提出了一种新范式——草稿链（CoD），灵感来源于人类的认知过程，使LLMs在解决任务时生成简约而信息丰富的中间推理输出。通过减少冗长并专注于关键见解，CoD在准确性上与CoT相匹配或超越，同时仅使用7.6%的标记，显著降低了各种推理任务的成本和延迟。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.18017",
            "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
            "url": "https://huggingface.co/papers/2502.18017",
            "abstract": "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.",
            "score": 3,
            "issue_id": 2487,
            "pub_date": "2025-02-25",
            "pub_date_card": {
                "ru": "25 февраля",
                "en": "February 25",
                "zh": "2月25日"
            },
            "hash": "4202273d8c895c2a",
            "authors": [
                "Qiuchen Wang",
                "Ruixue Ding",
                "Zehui Chen",
                "Weiqi Wu",
                "Shihang Wang",
                "Pengjun Xie",
                "Feng Zhao"
            ],
            "affiliations": [
                "MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, USTC",
                "Shanghai Jiao Tong University",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.18017.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#rag",
                    "#games",
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ViDoRAG: Новый подход к извлечению информации из визуальных документов",
                    "desc": "Статья представляет новый набор данных ViDoSeek для оценки эффективности методов извлечения информации с добавлением генерации (RAG) на визуально насыщенных документах. Авторы выявляют ограничения существующих подходов RAG в обработке мультимодальной информации и сложных рассуждений. Для решения этих проблем предлагается новая мультиагентная система ViDoRAG, использующая гибридную стратегию на основе гауссовых смесей для мультимодального поиска. ViDoRAG демонстрирует улучшение производительности более чем на 10% по сравнению с существующими методами на бенчмарке ViDoSeek."
                },
                "en": {
                    "title": "Enhancing RAG for Complex Visual Reasoning with ViDoRAG",
                    "desc": "This paper addresses the challenges of understanding complex information in visually rich documents using Retrieval-Augmented Generation (RAG) methods. It introduces ViDoSeek, a new dataset that tests RAG performance on documents that require advanced reasoning skills. The authors highlight limitations in current RAG techniques, such as difficulties in integrating visual and textual data and inadequate reasoning capabilities. To overcome these issues, they propose ViDoRAG, a multi-agent framework that enhances retrieval and reasoning through a hybrid strategy and an iterative workflow, demonstrating significant improvements in performance on the ViDoSeek benchmark."
                },
                "zh": {
                    "title": "提升视觉文档理解的RAG新框架",
                    "desc": "理解视觉丰富文档中的信息对传统的增强检索生成（RAG）方法来说仍然是一个重大挑战。现有的基准主要集中在基于图像的问题回答（QA），而忽视了在密集视觉文档中高效检索、理解和推理的基本挑战。为了解决这些问题，我们引入了ViDoSeek，这是一个新颖的数据集，旨在评估RAG在需要复杂推理的视觉丰富文档上的表现。我们提出的ViDoRAG框架采用混合策略，结合多模态检索和迭代代理工作流，以提高模型的推理能力，并在ViDoSeek基准上显著超越现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20545",
            "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
            "url": "https://huggingface.co/papers/2502.20545",
            "abstract": "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems.",
            "score": 2,
            "issue_id": 2486,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 февраля",
                "en": "February 27",
                "zh": "2月27日"
            },
            "hash": "fb32f9423103ece9",
            "authors": [
                "Kechen Li",
                "Wenqi Zhu",
                "Coralia Cartis",
                "Tianbo Ji",
                "Shiwei Liu"
            ],
            "affiliations": [
                "Mathematical Institute, University of Oxford",
                "Nanjing University of Aeronautics and Astronautics",
                "School of Transportation and Civil Engineering, Nantong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20545.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Большие языковые модели преодолевают границы математического мышления",
                    "desc": "В этой статье исследуется способность больших языковых моделей (LLM) решать сложные математические задачи, в частности, определение неотрицательности многомерных полиномов. Авторы создали датасет SoS-1K из 1000 полиномов и разработали инструкции по рассуждению для моделей. Эксперименты показали, что без структурированного руководства модели работают немного лучше случайного угадывания, но с качественными инструкциями точность повышается до 81%. Модель SoS-7B, дообученная на SoS-1K, превзошла более крупные модели по точности и скорости вычислений."
                },
                "en": {
                    "title": "Unlocking Mathematical Reasoning in LLMs with Structured Guidance",
                    "desc": "This paper explores the limitations of Large Language Models (LLMs) in solving complex mathematical problems, specifically the challenge of determining if a multivariate polynomial is nonnegative. The authors introduce a new dataset called SoS-1K, which contains around 1,000 polynomials and structured reasoning instructions to guide the models. They demonstrate that LLMs perform poorly without guidance, achieving only slightly above random guessing, but can significantly improve their accuracy with high-quality instructions. Notably, their fine-tuned model, SoS-7B, surpasses larger models in performance while being more computationally efficient, showcasing the potential of LLMs in addressing NP-hard problems."
                },
                "zh": {
                    "title": "推动数学推理的边界",
                    "desc": "大型语言模型（LLMs）在多种任务中达到了人类水平的能力，但在严格的数学问题解决方面仍然面临挑战。本文研究了一个基本但计算上难以处理的问题：判断给定的多变量多项式是否非负。我们引入了SoS-1K数据集，包含约1000个多项式，并设计了基于五个逐步挑战标准的推理指导。实验表明，经过高质量的推理指导后，模型的准确率显著提高，最高可达81%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20396",
            "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
            "url": "https://huggingface.co/papers/2502.20396",
            "abstract": "Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.",
            "score": 1,
            "issue_id": 2486,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 февраля",
                "en": "February 27",
                "zh": "2月27日"
            },
            "hash": "41439b4f54e02c9b",
            "authors": [
                "Toru Lin",
                "Kartik Sachdev",
                "Linxi Fan",
                "Jitendra Malik",
                "Yuke Zhu"
            ],
            "affiliations": [
                "NVIDIA",
                "UC Berkeley",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20396.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#rl",
                    "#games",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Ловкость робота: от симуляции к реальности",
                    "desc": "Статья описывает применение обучения с подкреплением для решения задач манипуляции объектами с помощью человекоподобного робота. Авторы представляют новые методы для преодоления ключевых проблем, включая автоматическую настройку симуляции, обобщенную схему проектирования наград и смешанное представление объектов. Исследование демонстрирует успешные результаты на трех задачах ловкой манипуляции, достигая надежной генерализации и высокой производительности. Работа предлагает эффективный подход к обучению человекоподобных роботов сложным манипуляциям без необходимости в демонстрациях человека."
                },
                "en": {
                    "title": "Reinforcement Learning for Dexterous Robot Manipulation: Bridging Sim and Real Worlds",
                    "desc": "This paper addresses the challenges of using reinforcement learning for complex robot manipulation tasks that involve physical contact. The authors propose innovative methods to enhance the performance of humanoid robots in these tasks, including a system to align simulated environments with real-world conditions. They also introduce a new reward design that simplifies the process of creating effective rewards for long tasks and a technique to improve learning efficiency in difficult scenarios. The results demonstrate that their approach allows robots to learn dexterous manipulation effectively, achieving high performance without requiring human guidance."
                },
                "zh": {
                    "title": "突破类人机器人灵巧操作的强化学习挑战",
                    "desc": "本研究探讨了在类人机器人灵巧操作任务中应用强化学习的关键挑战。我们提出了新技术来克服这些挑战，包括自动化的真实到模拟调优模块，以缩小模拟环境与现实世界的差距。我们还设计了一种通用的奖励机制，简化了长时间接触丰富的操作任务的奖励工程。通过对三项类人灵巧操作任务的实验，我们展示了在不需要人类示范的情况下，使用模拟到真实的强化学习实现了稳健的泛化和高性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20811",
            "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
            "url": "https://huggingface.co/papers/2502.20811",
            "abstract": "Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. HAICTrain comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, HAICBench includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.",
            "score": 1,
            "issue_id": 2486,
            "pub_date": "2025-02-28",
            "pub_date_card": {
                "ru": "28 февраля",
                "en": "February 28",
                "zh": "2月28日"
            },
            "hash": "806f6aacd5ee2f8a",
            "authors": [
                "Xiao Wang",
                "Jingyun Hua",
                "Weihong Lin",
                "Yuanxing Zhang",
                "Fuzheng Zhang",
                "Jianlong Wu",
                "Di Zhang",
                "Liqiang Nie"
            ],
            "affiliations": [
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20811.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#synthetic"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Улучшение понимания человеческих действий в видео с помощью аннотированных данных",
                    "desc": "Статья представляет новый подход к улучшению понимания видео с человеческими действиями для мультимодальных больших языковых моделей (MLLM). Авторы разработали двухэтапный процесс аннотации данных, включающий сбор релевантных видео и их стандартизированное описание. В результате были созданы два набора данных: HAICTrain для обучения и HAICBench для оценки моделей. Эксперименты показали, что обучение на HAICTrain значительно улучшает способности моделей понимать человеческие действия на видео."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Curated Human Action Datasets",
                    "desc": "This paper presents a solution to improve video understanding in Multi-modal Large Language Models (MLLMs) by addressing the scarcity of high-quality data on human actions. The authors introduce a two-stage data annotation pipeline that first collects videos with clear human actions from the Internet and then annotates them using a standardized caption format. This results in two curated datasets: HAICTrain, which contains 126K video-caption pairs for training, and HAICBench, which includes 500 annotated pairs for evaluation. Experimental results show that using HAICTrain significantly boosts human action understanding and enhances text-to-video generation capabilities across multiple benchmarks."
                },
                "zh": {
                    "title": "提升视频理解的创新数据标注流程",
                    "desc": "最近的多模态大型语言模型（MLLMs）在视频理解方面取得了显著进展。然而，它们在涉及人类动作的视频上的表现仍然受到高质量数据缺乏的限制。为了解决这个问题，我们提出了一个两阶段的数据标注流程，首先从互联网收集包含清晰人类动作的视频，然后使用标准化的字幕格式对视频进行标注。通过这个流程，我们创建了两个数据集HAICTrain和HAICBench，实验结果表明，使用HAICTrain进行训练显著提升了人类动作理解能力，并改善了文本到视频生成的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20583",
            "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
            "url": "https://huggingface.co/papers/2502.20583",
            "abstract": "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper, rely on deep encoder-decoder architectures, and their encoders are a critical bottleneck for efficient deployment due to high computational intensity. We introduce LiteASR, a low-rank compression scheme for ASR encoders that significantly reduces inference costs while maintaining transcription accuracy. Our approach leverages the strong low-rank properties observed in intermediate activations: by applying principal component analysis (PCA) with a small calibration dataset, we approximate linear transformations with a chain of low-rank matrix multiplications, and further optimize self-attention to work in the reduced dimension. Evaluation results show that our method can compress Whisper large-v3's encoder size by over 50%, matching Whisper medium's size with better transcription accuracy, thereby establishing a new Pareto-optimal frontier of efficiency and performance. The code of LiteASR is available at https://github.com/efeslab/LiteASR.",
            "score": 1,
            "issue_id": 2486,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 февраля",
                "en": "February 27",
                "zh": "2月27日"
            },
            "hash": "3c7268c0881fa426",
            "authors": [
                "Keisuke Kamahori",
                "Jungo Kasai",
                "Noriyuki Kojima",
                "Baris Kasikci"
            ],
            "affiliations": [
                "Kotoba Technologies Inc.",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20583.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#audio"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "LiteASR: Эффективное сжатие энкодеров ASR без потери точности",
                    "desc": "LiteASR - это метод сжатия энкодеров для систем автоматического распознавания речи (ASR), который значительно уменьшает вычислительные затраты при сохранении точности транскрипции. Метод использует анализ главных компонент (PCA) для аппроксимации линейных преобразований цепочкой умножений матриц низкого ранга. LiteASR позволяет сжать размер энкодера модели Whisper large-v3 более чем на 50%, достигая размера Whisper medium с лучшей точностью транскрипции. Это устанавливает новую Парето-оптимальную границу эффективности и производительности для моделей ASR."
                },
                "en": {
                    "title": "LiteASR: Efficient ASR with Low-Rank Compression",
                    "desc": "This paper presents LiteASR, a novel low-rank compression technique designed to enhance the efficiency of automatic speech recognition (ASR) models, particularly focusing on the encoder component. By utilizing principal component analysis (PCA) on intermediate activations, LiteASR reduces the computational load during inference while preserving transcription accuracy. The method achieves over 50% reduction in the encoder size of OpenAI's Whisper large-v3 model, aligning its performance with that of the medium version but with improved accuracy. This work sets a new standard for balancing efficiency and performance in ASR systems."
                },
                "zh": {
                    "title": "LiteASR：高效的低秩压缩方案",
                    "desc": "现代自动语音识别（ASR）模型，如OpenAI的Whisper，依赖于深度编码器-解码器架构，而编码器的计算强度是高效部署的瓶颈。我们提出了LiteASR，这是一种针对ASR编码器的低秩压缩方案，能够显著降低推理成本，同时保持转录准确性。我们的方法利用了中间激活中的强低秩特性，通过使用小型校准数据集的主成分分析（PCA），用低秩矩阵乘法链来近似线性变换，并进一步优化自注意力机制以适应降维后的数据。评估结果表明，我们的方法可以将Whisper large-v3的编码器大小压缩超过50%，并在转录准确性上优于Whisper medium，从而建立了效率与性能的新帕累托最优边界。"
                }
            }
        }
    ],
    "link_prev": "2025-02-28.html",
    "link_next": "2025-03-04.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "28.02",
        "en": "02/28",
        "zh": "2月28日"
    },
    "short_date_next": {
        "ru": "04.03",
        "en": "03/04",
        "zh": "3月4日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章研究了自我奖励推理的大语言模型（LLMs）。这种模型可以在推理过程中同时生成逐步推理并评估其输出的正确性，无需外部反馈。研究重点是自我纠正任务，模型可以自主检测和修正错误，决定何时终止迭代修正循环。作者提出了一个两阶段的算法框架，使用自我生成的数据来构建这种模型。实验结果显示，这种方法在Llama-3和Qwen-2.5上表现优异，超越了内在自我纠正能力，达到了与依赖外部奖励模型系统相当的性能。",
        "title": "Self-rewarding correction for mathematical reasoning",
        "pinyin": "这篇文章研究了自我奖励推理的大语言模型（LLMs）。\nZhè piān wénzhāng yánjiū le zìwǒ jiǎnglì tuīlǐ de dà yǔyán móxíng (LLMs).\n\n这种模型可以在推理过程中同时生成逐步推理并评估其输出的正确性，无需外部反馈。\nZhè zhǒng móxíng kěyǐ zài tuīlǐ guòchéng zhōng tóngshí shēngchéng zhúbù tuīlǐ bìng píngjià qí shūchū de zhèngquèxìng, wúxū wàibù fǎnkuì.\n\n研究重点是自我纠正任务，模型可以自主检测和修正错误，决定何时终止迭代修正循环。\nYánjiū zhòngdiǎn shì zìwǒ jiūzhèng rènwù, móxíng kěyǐ zìzhǔ jiǎncè hé xiūzhèng cuòwù, juédìng héshí zhōngzhǐ diécì xiūzhèng xúnhuán.\n\n作者提出了一个两阶段的算法框架，使用自我生成的数据来构建这种模型。\nZuòzhě tíchū le yīgè liǎng jiēduàn de suànfǎ kuàngjià, shǐyòng zìwǒ shēngchéng de shùjù lái gòujiàn zhè zhǒng móxíng.\n\n实验结果显示，这种方法在Llama-3和Qwen-2.5上表现优异，超越了内在自我纠正能力，达到了与依赖外部奖励模型系统相当的性能。\nShíyàn jiéguǒ xiǎnshì, zhè zhǒng fāngfǎ zài Llama-3 hé Qwen-2.5 shàng biǎoxiàn yōuyì, chāoyuè le nèizài zìwǒ jiūzhèng nénglì, dádào le yǔ yīlài wàibù jiǎnglì móxíng xìtǒng xiāngdāng de xíngnéng.",
        "vocab": "[{'word': '自我奖励推理', 'pinyin': 'zì wǒ jiǎng lì tuī lǐ', 'trans': 'self-rewarding reasoning'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '逐步推理', 'pinyin': 'zhú bù tuī lǐ', 'trans': 'step-by-step reasoning'},\n{'word': '自我纠正', 'pinyin': 'zì wǒ jiū zhèng', 'trans': 'self-correction'},\n{'word': '自主检测', 'pinyin': 'zì zhǔ jiǎn cè', 'trans': 'autonomous detection'},\n{'word': '迭代修正', 'pinyin': 'dié dài xiū zhèng', 'trans': 'iterative correction'},\n{'word': '算法框架', 'pinyin': 'suàn fǎ kuàng jià', 'trans': 'algorithmic framework'},\n{'word': '自我生成', 'pinyin': 'zì wǒ shēng chéng', 'trans': 'self-generation'},\n{'word': '内在自我纠正', 'pinyin': 'nèi zài zì wǒ jiū zhèng', 'trans': 'intrinsic self-correction'},\n{'word': '依赖外部奖励', 'pinyin': 'yī lài wài bù jiǎng lì', 'trans': 'reliant on external rewards'}]",
        "trans": "This article investigates large language models (LLMs) with self-rewarding reasoning capabilities. These models can generate step-by-step reasoning during the inference process and evaluate the correctness of their outputs without external feedback. The research focuses on self-correcting tasks, where the model can autonomously detect and correct errors, deciding when to terminate the iterative correction loop. The authors propose a two-stage algorithmic framework that uses self-generated data to build such models. Experimental results demonstrate that this method performs exceptionally well on Llama-3 and Qwen-2.5, surpassing inherent self-correcting capabilities and achieving performance comparable to systems that rely on external reward models.",
        "update_ts": "2025-03-02 12:38"
    }
}