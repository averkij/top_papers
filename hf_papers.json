{
    "date": {
        "ru": "31 июля",
        "en": "July 31",
        "zh": "7月31日"
    },
    "time_utc": "2025-07-31 03:02",
    "weekday": 3,
    "issue_id": 5102,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.21493",
            "title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics",
            "url": "https://huggingface.co/papers/2507.21493",
            "abstract": "BANG is a generative approach that uses latent diffusion models and temporal attention to enable intuitive part-level decomposition and manipulation of 3D objects, enhancing 3D creation workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence.   BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows.   The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition.",
            "score": 16,
            "issue_id": 5102,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 июля",
                "en": "July 29",
                "zh": "7月29日"
            },
            "hash": "28c2cc57bc1db4c8",
            "authors": [
                "Longwen Zhang",
                "Qixuan Zhang",
                "Haoran Jiang",
                "Yinuo Bai",
                "Wei Yang",
                "Lan Xu",
                "Jingyi Yu"
            ],
            "affiliations": [
                "Deemos Technology Co., Ltd., China",
                "Huazhong University of Science and Technology, China",
                "ShanghaiTech University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21493.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#3d",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "BANG: Интуитивная декомпозиция 3D-объектов для творческого моделирования",
                    "desc": "BANG - это генеративный подход, использующий латентные диффузионные модели и временное внимание для интуитивной декомпозиции и манипуляции 3D-объектами на уровне частей. Он основан на концепции 'Generative Exploded Dynamics', создающей плавную последовательность разобранных состояний входной геометрии. BANG использует предобученную крупномасштабную латентную диффузионную модель, дообученную для разобранной динамики с помощью легковесного адаптера. Подход позволяет улучшить рабочие процессы 3D-создания, предлагая интуитивно понятный способ трансформации концепций в детализированные 3D-активы."
                },
                "en": {
                    "title": "BANG: Intuitive 3D Creation through Generative Decomposition",
                    "desc": "BANG is a generative approach that enhances 3D creation by allowing users to intuitively decompose and manipulate 3D objects at a part level. It employs latent diffusion models and a temporal attention mechanism to ensure smooth transitions and maintain the coherence of geometric and semantic properties during the decomposition process. The system allows for precise control through spatial prompts, enabling users to specify which parts to manipulate, and can integrate with multimodal models for enhanced creativity. BANG's capabilities support detailed geometry generation and are particularly useful in applications like 3D printing, where it facilitates the creation of separable parts for easy assembly."
                },
                "zh": {
                    "title": "BANG：直观的3D对象分解与创作新方法",
                    "desc": "BANG是一种生成性方法，利用潜在扩散模型和时间注意力机制，实现3D对象的直观部分级分解和操作，提升3D创作流程。它通过“生成性爆炸动态”技术，创建输入几何体的平滑爆炸状态序列，逐步分离部件，同时保持几何和语义的一致性。BANG使用经过预训练的大规模潜在扩散模型，并通过轻量级的爆炸视图适配器进行微调，确保分解过程的精确控制。该方法还结合了时间注意力模块，确保时间上的平滑过渡和一致性，极大地增强了3D创作的灵活性和直观性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22886",
            "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\n  Segmentation",
            "url": "https://huggingface.co/papers/2507.22886",
            "abstract": "Omnimodal Referring Audio-Visual Segmentation (OmniAVS) and Omnimodal Instructed Segmentation Assistant (OISA) advance audio-visual segmentation by integrating complex multimodal expressions and leveraging MLLM for reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks.",
            "score": 3,
            "issue_id": 5102,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "99373a83d84e0212",
            "authors": [
                "Kaining Ying",
                "Henghui Ding",
                "Guanquan Jie",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Fudan University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22886.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#cv",
                    "#reasoning",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Мультимодальная сегментация: новый уровень понимания аудио-визуального контента",
                    "desc": "OmniAVS - это новый набор данных для сегментации аудио-визуального контента, включающий 2,098 видео и 59,458 мультимодальных выражений. Датасет отличается гибким сочетанием текста, речи, звука и визуальных подсказок, а также акцентом на понимание аудиоконтента и сложные рассуждения. OISA - это модель, использующая мультимодальную языковую модель (MLLM) для понимания сложных сигналов и выполнения сегментации на основе рассуждений. Эксперименты показывают, что OISA превосходит существующие методы на OmniAVS и достигает конкурентоспособных результатов в других связанных задачах."
                },
                "en": {
                    "title": "Revolutionizing Audio-Visual Segmentation with OmniAVS and OISA",
                    "desc": "This paper introduces Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset designed to enhance audio-visual segmentation by incorporating diverse multimodal expressions. It features 2,098 videos and 59,458 multimodal referring expressions, which include text, speech, sound, and visual cues. The study also presents the Omnimodal Instructed Segmentation Assistant (OISA), which utilizes a Multimodal Large Language Model (MLLM) to improve reasoning and understanding of complex audiovisual content. Experimental results demonstrate that OISA significantly outperforms existing segmentation methods on the OmniAVS dataset and shows competitive performance on related tasks."
                },
                "zh": {
                    "title": "全模态音视频分割的创新与突破",
                    "desc": "本文提出了全模态引用音视频分割（OmniAVS）和全模态指令分割助手（OISA），旨在提升音视频分割的能力。OmniAVS是一个新数据集，包含2098个视频和59458个多模态引用表达，具有8种灵活结合文本、语音、声音和视觉线索的多模态表达类型。OISA利用多语言大模型（MLLM）来理解复杂线索并进行推理分割，从而解决多模态推理和音视频内容的细致理解问题。实验结果表明，OISA在OmniAVS上优于现有方法，并在其他相关任务中也取得了竞争性结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22448",
            "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance",
            "url": "https://huggingface.co/papers/2507.22448",
            "abstract": "Falcon-H1, a new series of large language models with a hybrid architecture combining Transformer-based attention and State Space Models, achieves state-of-the-art performance and efficiency across various tasks and sizes.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.",
            "score": 0,
            "issue_id": 5102,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "1023abbaabd95fa0",
            "authors": [
                "Jingwei Zuo",
                "Maksim Velikanov",
                "Ilyas Chahed",
                "Younes Belkada",
                "Dhia Eddine Rhayem",
                "Guillaume Kunsch",
                "Hakim Hacid",
                "Hamza Yous",
                "Brahim Farhat",
                "Ibrahim Khadraoui",
                "Mugariya Farooq",
                "Giulia Campesan",
                "Ruxandra Cojocaru",
                "Yasser Djilali",
                "Shi Hu",
                "Iheb Chaabane",
                "Puneesh Khanna",
                "Mohamed El Amine Seddik",
                "Ngoc Dung Huynh",
                "Phuc Le Khac",
                "Leen AlQadi",
                "Billel Mokeddem",
                "Mohamed Chami",
                "Abdalgader Abubaker",
                "Mikhail Lubinets",
                "Kacper Piskorski",
                "Slim Frikha"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.22448.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#agi",
                    "#architecture",
                    "#science",
                    "#long_context",
                    "#dataset",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "🦅",
                "ru": {
                    "title": "Falcon-H1: Гибридная мощь в мире языковых моделей",
                    "desc": "Falcon-H1 - это новая серия больших языковых моделей с гибридной архитектурой, сочетающей внимание на основе трансформеров и модели пространства состояний. Модели демонстрируют передовую производительность и эффективность в различных задачах и размерах. Falcon-H1 доступен в нескольких конфигурациях, включая базовые и инструктированные варианты с количеством параметров от 0,5B до 34B. Модели превосходят аналоги с большим количеством параметров и показывают отличные результаты в рассуждениях, математике, многоязычных задачах и научных знаниях."
                },
                "en": {
                    "title": "Falcon-H1: Redefining Efficiency in Language Models",
                    "desc": "Falcon-H1 introduces a new series of large language models that utilize a hybrid architecture, merging Transformer-based attention with State Space Models for enhanced performance and efficiency. This innovative design allows the models to handle long-context memory better while maintaining computational efficiency. The models are available in various sizes and configurations, demonstrating state-of-the-art capabilities across multiple tasks, including reasoning and multilingual processing. By outperforming larger models with fewer parameters, Falcon-H1 sets a new standard in the field of AI language models."
                },
                "zh": {
                    "title": "Falcon-H1：高效与性能的完美结合",
                    "desc": "Falcon-H1是一系列新的大型语言模型，采用混合架构，结合了基于Transformer的注意力机制和状态空间模型（SSM）。这种设计使得Falcon-H1在多种任务中表现出色，具有高效的计算能力和优越的长时记忆能力。与之前的Falcon模型不同，Falcon-H1提供了多种配置，能够在参数较少的情况下与更大规模的模型竞争。所有模型都以开放源代码的方式发布，体现了我们对可及性和影响力的承诺。"
                }
            }
        }
    ],
    "link_prev": "2025-07-30.html",
    "link_next": "2025-08-01.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "30.07",
        "en": "07/30",
        "zh": "7月30日"
    },
    "short_date_next": {
        "ru": "01.08",
        "en": "08/01",
        "zh": "8月1日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}