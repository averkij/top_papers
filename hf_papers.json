{
    "date": {
        "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 28",
        "zh": "11æœˆ28æ—¥"
    },
    "time_utc": "2024-11-28 11:09",
    "weekday": 3,
    "issue_id": 836,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.17949",
            "title": "ROICtrl: Boosting Instance Control for Visual Generation",
            "url": "https://huggingface.co/papers/2411.17949",
            "abstract": "Natural language often struggles to accurately associate positional and attribute information with multiple instances, which limits current text-based visual generation models to simpler compositions featuring only a few dominant instances. To address this limitation, this work enhances diffusion models by introducing regional instance control, where each instance is governed by a bounding box paired with a free-form caption. Previous methods in this area typically rely on implicit position encoding or explicit attention masks to separate regions of interest (ROIs), resulting in either inaccurate coordinate injection or large computational overhead. Inspired by ROI-Align in object detection, we introduce a complementary operation called ROI-Unpool. Together, ROI-Align and ROI-Unpool enable explicit, efficient, and accurate ROI manipulation on high-resolution feature maps for visual generation. Building on ROI-Unpool, we propose ROICtrl, an adapter for pretrained diffusion models that enables precise regional instance control. ROICtrl is compatible with community-finetuned diffusion models, as well as with existing spatial-based add-ons (\\eg, ControlNet, T2I-Adapter) and embedding-based add-ons (\\eg, IP-Adapter, ED-LoRA), extending their applications to multi-instance generation. Experiments show that ROICtrl achieves superior performance in regional instance control while significantly reducing computational costs.",
            "score": 54,
            "issue_id": 827,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "55ffbda778f2f640",
            "authors": [
                "Yuchao Gu",
                "Yipin Zhou",
                "Yunfan Ye",
                "Yixin Nie",
                "Licheng Yu",
                "Pingchuan Ma",
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "GenAI, Meta",
                "MIT",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17949.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - ROICtrl. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. ROICtrl Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ROI-Align Ğ¸ ROI-Unpool Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Enhancing Visual Generation with Precise Regional Control",
                    "desc": "This paper presents a new method to improve text-based visual generation models by allowing better control over multiple instances in images. It introduces a technique called ROICtrl, which uses bounding boxes and captions to manage regions of interest (ROIs) more effectively. The method combines ROI-Align and a new operation called ROI-Unpool to manipulate high-resolution feature maps accurately and efficiently. Experiments demonstrate that ROICtrl enhances performance in generating images with multiple instances while lowering computational costs."
                },
                "zh": {
                    "title": "åŒºåŸŸå®ä¾‹æ§åˆ¶ï¼Œæå‡è§†è§‰ç”Ÿæˆç²¾åº¦",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥åŒºåŸŸå®ä¾‹æ§åˆ¶æ¥è§£å†³è‡ªç„¶è¯­è¨€åœ¨å¤šå®ä¾‹ä½ç½®å’Œå±æ€§ä¿¡æ¯å…³è”ä¸Šçš„ä¸è¶³ã€‚æ¯ä¸ªå®ä¾‹ç”±ä¸€ä¸ªè¾¹ç•Œæ¡†å’Œä¸€ä¸ªè‡ªç”±å½¢å¼çš„æè¿°é…å¯¹ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„æ§åˆ¶ã€‚è®ºæ–‡ä¸­ä»‹ç»çš„ROI-Unpoolæ“ä½œä¸ROI-Alignç›¸ç»“åˆï¼Œä½¿å¾—åœ¨é«˜åˆ†è¾¨ç‡ç‰¹å¾å›¾ä¸Šè¿›è¡ŒåŒºåŸŸæ“ä½œå˜å¾—é«˜æ•ˆä¸”å‡†ç¡®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROICtrlåœ¨åŒºåŸŸå®ä¾‹æ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17188",
            "title": "Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment",
            "url": "https://huggingface.co/papers/2411.17188",
            "abstract": "Many real-world user queries (e.g. \"How do to make egg fried rice?\") could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook. Models designed to generate interleaved text and images face challenges in ensuring consistency within and across these modalities. To address these challenges, we present ISG, a comprehensive evaluation framework for interleaved text-and-image generation. ISG leverages a scene graph structure to capture relationships between text and image blocks, evaluating responses on four levels of granularity: holistic, structural, block-level, and image-specific. This multi-tiered evaluation allows for a nuanced assessment of consistency, coherence, and accuracy, and provides interpretable question-answer feedback. In conjunction with ISG, we introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8 categories and 21 subcategories. This benchmark dataset includes complex language-vision dependencies and golden answers to evaluate models effectively on vision-centric tasks such as style transfer, a challenging area for current models. Using ISG-Bench, we demonstrate that recent unified vision-language models perform poorly on generating interleaved content. While compositional approaches that combine separate language and image models show a 111% improvement over unified models at the holistic level, their performance remains suboptimal at both block and image levels. To facilitate future work, we develop ISG-Agent, a baseline agent employing a \"plan-execute-refine\" pipeline to invoke tools, achieving a 122% performance improvement.",
            "score": 16,
            "issue_id": 828,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "dac4ab78d1cc7e28",
            "authors": [
                "Dongping Chen",
                "Ruoxi Chen",
                "Shu Pu",
                "Zhaoyi Liu",
                "Yanru Wu",
                "Caixi Chen",
                "Benlin Liu",
                "Yue Huang",
                "Yao Wan",
                "Pan Zhou",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "HUST",
                "University of Notre Dame",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17188.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#agents",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ISG: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ISG - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ISG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ñ€Ğ°Ñ„Ğ° ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ISG-Bench Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ…Ğ¾Ñ‚Ñ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ, Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ´Ğ°Ğ»ĞµĞºĞ¸ Ğ¾Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Text-Image Generation with ISG Framework",
                    "desc": "This paper introduces ISG, a framework designed to evaluate models that generate interleaved text and images, which is useful for tasks like creating step-by-step cooking guides. It uses a scene graph structure to analyze the relationships between text and images, providing a detailed assessment across different levels of granularity. The authors also present ISG-Bench, a benchmark dataset with 1,150 samples that helps evaluate models on complex language-vision tasks. The findings reveal that while compositional models outperform unified models significantly, there is still room for improvement, leading to the development of ISG-Agent, which enhances performance through a structured approach."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬ä¸å›¾åƒç”Ÿæˆä¸€è‡´æ€§çš„è¯„ä¼°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºISGçš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒäº¤é”™çš„å“åº”ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆä¸€è‡´æ€§çš„é—®é¢˜ã€‚ISGåˆ©ç”¨åœºæ™¯å›¾ç»“æ„æ•æ‰æ–‡æœ¬ä¸å›¾åƒå—ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åœ¨æ•´ä½“ã€ç»“æ„ã€å—çº§å’Œå›¾åƒç‰¹å®šå››ä¸ªå±‚é¢è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡ISG-BenchåŸºå‡†æ•°æ®é›†ï¼Œç ”ç©¶è¡¨æ˜ç°æœ‰çš„ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆäº¤é”™å†…å®¹æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œè€Œç»„åˆæ–¹æ³•åˆ™åœ¨æ•´ä½“å±‚é¢ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚æœ€åï¼Œå¼€å‘çš„ISG-Agenté€šè¿‡â€œè®¡åˆ’-æ‰§è¡Œ-ä¼˜åŒ–â€æµç¨‹å®ç°äº†æ›´é«˜çš„æ€§èƒ½ï¼Œæ¨åŠ¨äº†æœªæ¥çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18613",
            "title": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models",
            "url": "https://huggingface.co/papers/2411.18613",
            "abstract": "We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. See our project page for results and interactive demos: cat-4d.github.io.",
            "score": 14,
            "issue_id": 829,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "736c7f565ac43a96",
            "authors": [
                "Rundi Wu",
                "Ruiqi Gao",
                "Ben Poole",
                "Alex Trevithick",
                "Changxi Zheng",
                "Jonathan T. Barron",
                "Aleksander Holynski"
            ],
            "affiliations": [
                "Columbia University",
                "Google DeepMind",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18613.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#optimization",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ 2D-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-ÑÑ†ĞµĞ½Ñ‹",
                    "desc": "CAT4D - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. CAT4D Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğµ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Transforming Monocular Videos into Dynamic 4D Scenes",
                    "desc": "CAT4D is a new method that creates dynamic 3D scenes from a single video. It uses a multi-view video diffusion model, which is trained on various datasets, to generate new views from different camera angles and times. The approach includes a unique sampling technique that allows for the transformation of a single video into a multi-view format, facilitating the reconstruction of 4D scenes using a flexible 3D Gaussian model. The results show that CAT4D performs well in generating new views and reconstructing dynamic scenes, showcasing its potential for creative applications."
                },
                "zh": {
                    "title": "CAT4Dï¼šä»å•ç›®è§†é¢‘ç”ŸæˆåŠ¨æ€3Dåœºæ™¯çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "CAT4Dæ˜¯ä¸€ç§ä»å•ç›®è§†é¢‘åˆ›å»º4DåŠ¨æ€3Dåœºæ™¯çš„æ–¹æ³•ã€‚å®ƒåˆ©ç”¨å¤šè§†è§’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç»è¿‡å¤šç§æ•°æ®é›†çš„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨æŒ‡å®šçš„ç›¸æœºä½ç½®å’Œæ—¶é—´æˆ³ä¸‹è¿›è¡Œæ–°è§†è§’åˆæˆã€‚é€šè¿‡ä¸€ç§æ–°é¢–çš„é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ¨¡å‹å¯ä»¥å°†å•ä¸ªå•ç›®è§†é¢‘è½¬æ¢ä¸ºå¤šè§†è§’è§†é¢‘ï¼Œä»è€Œé€šè¿‡ä¼˜åŒ–å¯å˜å½¢çš„3Dé«˜æ–¯è¡¨ç¤ºå®ç°ç¨³å¥çš„4Dé‡å»ºã€‚æˆ‘ä»¬åœ¨æ–°è§†è§’åˆæˆå’ŒåŠ¨æ€åœºæ™¯é‡å»ºåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶å¼ºè°ƒäº†ä»çœŸå®æˆ–ç”Ÿæˆè§†é¢‘ä¸­ç”Ÿæˆ4Dåœºæ™¯çš„åˆ›é€ èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17945",
            "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
            "url": "https://huggingface.co/papers/2411.17945",
            "abstract": "Generating high-fidelity 3D content from text prompts remains a significant challenge in computer vision due to the limited size, diversity, and annotation depth of the existing datasets. To address this, we introduce MARVEL-40M+, an extensive dataset with 40 million text annotations for over 8.9 million 3D assets aggregated from seven major 3D datasets. Our contribution is a novel multi-stage annotation pipeline that integrates open-source pretrained multi-view VLMs and LLMs to automatically produce multi-level descriptions, ranging from detailed (150-200 words) to concise semantic tags (10-20 words). This structure supports both fine-grained 3D reconstruction and rapid prototyping. Furthermore, we incorporate human metadata from source datasets into our annotation pipeline to add domain-specific information in our annotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D, a two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our annotations and use a pretrained image-to-3D network to generate 3D textured meshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly outperforms existing datasets in annotation quality and linguistic diversity, achieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators.",
            "score": 11,
            "issue_id": 827,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "360ba2514eea161e",
            "authors": [
                "Sankalp Sinha",
                "Mohammad Sadil Khan",
                "Muhammad Usama",
                "Shino Sam",
                "Didier Stricker",
                "Sk Aziz Ali",
                "Muhammad Zeshan Afzal"
            ],
            "affiliations": [
                "BITS Pilani, Hyderabad",
                "DFKI",
                "MindGarage",
                "RPTU Kaiserslautern-Landau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17945.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#diffusion",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "ğŸŒŸ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MARVEL-40M+, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ 40 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 8,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MARVEL-FX3D - Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking 3D Creation with MARVEL-40M+: A New Era in Text-to-3D Generation",
                    "desc": "This paper presents MARVEL-40M+, a new dataset designed to improve the generation of 3D content from text prompts. It includes 40 million text annotations for 8.9 million 3D assets, created using a multi-stage annotation pipeline that leverages pretrained vision-language models (VLMs) and large language models (LLMs). The dataset enhances 3D reconstruction and prototyping by providing detailed and concise descriptions, while also incorporating human metadata to reduce inaccuracies in the generated annotations. Additionally, the authors introduce MARVEL-FX3D, a text-to-3D pipeline that efficiently generates 3D meshes, demonstrating superior annotation quality and diversity compared to existing datasets."
                },
                "zh": {
                    "title": "MARVEL-40M+: é«˜è´¨é‡3Då†…å®¹ç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†MARVEL-40M+æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«4000ä¸‡æ¡æ–‡æœ¬æ³¨é‡Šï¼Œæ¶µç›–890ä¸‡ä»¶3Dèµ„äº§ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡å’Œå¤šæ ·æ€§ä¸Šçš„ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šé˜¶æ®µæ³¨é‡Šæµç¨‹ï¼Œç»“åˆäº†å¼€æºçš„å¤šè§†è§’è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šå±‚æ¬¡çš„æè¿°ã€‚é€šè¿‡å¼•å…¥äººç±»å…ƒæ•°æ®ï¼Œæˆ‘ä»¬å¢å¼ºäº†æ³¨é‡Šçš„é¢†åŸŸç‰¹å®šä¿¡æ¯ï¼Œå‡å°‘äº†VLMçš„å¹»è§‰ç°è±¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†MARVEL-FX3Dï¼Œä¸€ä¸ªä¸¤é˜¶æ®µçš„æ–‡æœ¬åˆ°3Dç”Ÿæˆç®¡é“ï¼Œèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18279",
            "title": "Large Language Model-Brained GUI Agents: A Survey",
            "url": "https://huggingface.co/papers/2411.18279",
            "abstract": "GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.   To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.",
            "score": 9,
            "issue_id": 831,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "5c5e1a276f559eb5",
            "authors": [
                "Chaoyun Zhang",
                "Shilin He",
                "Jiaxu Qian",
                "Bowen Li",
                "Liqun Li",
                "Si Qin",
                "Yu Kang",
                "Minghua Ma",
                "Qingwei Lin",
                "Saravan Rajmohan",
                "Dongmei Zhang",
                "Qi Zhang"
            ],
            "affiliations": [
                "Microsoft AI, Microsoft, China",
                "M365 Research, Microsoft, USA",
                "Shanghai Artificial Intelligence Laboratory, China",
                "Peking University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18279.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#dataset",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼ Ñ‡ĞµÑ€ĞµĞ· GUI",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (GUI), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ­Ñ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ GUI Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ GUI. Ğ˜ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ GUI, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing User Interaction with LLM-Brained GUI Agents",
                    "desc": "This paper surveys the development of LLM-brained GUI agents, which utilize large language models to automate interactions with graphical user interfaces. These agents can understand natural language commands and perform complex tasks across various platforms, enhancing user experience significantly. The study explores the historical context, core components, and advanced techniques involved in creating these agents, as well as the data requirements for training them. It also identifies research gaps and proposes a roadmap for future advancements in this rapidly evolving field."
                },
                "zh": {
                    "title": "LLMé©±åŠ¨çš„GUIä»£ç†ï¼šé©æ–°ç”¨æˆ·äº¤äº’ä½“éªŒ",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„æœ€æ–°è¿›å±•ã€‚è¿™äº›ä»£ç†èƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶è‡ªåŠ¨æ‰§è¡Œå¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ï¼Œæå¤§åœ°æå‡äº†ç”¨æˆ·ä¸è½¯ä»¶çš„äº¤äº’ä½“éªŒã€‚è®ºæ–‡å›é¡¾äº†è¿™ä¸€é¢†åŸŸçš„å†å²æ¼”å˜ã€æ ¸å¿ƒç»„ä»¶å’Œå…ˆè¿›æŠ€æœ¯ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚é€šè¿‡å¯¹ç°æœ‰æ¡†æ¶å’Œè¯„ä¼°æ ‡å‡†çš„åˆ†æï¼Œæœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›æŒ‡å¯¼ï¼Œä»¥å……åˆ†å‘æŒ¥LLMé©±åŠ¨çš„GUIä»£ç†çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17787",
            "title": "Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient",
            "url": "https://huggingface.co/papers/2411.17787",
            "abstract": "In the rapidly advancing field of image generation, Visual Auto-Regressive (VAR) modeling has garnered considerable attention for its innovative next-scale prediction approach. This paradigm offers substantial improvements in efficiency, scalability, and zero-shot generalization. Yet, the inherently coarse-to-fine nature of VAR introduces a prolonged token sequence, leading to prohibitive memory consumption and computational redundancies. To address these bottlenecks, we propose Collaborative Decoding (CoDe), a novel efficient decoding strategy tailored for the VAR framework. CoDe capitalizes on two critical observations: the substantially reduced parameter demands at larger scales and the exclusive generation patterns across different scales. Based on these insights, we partition the multi-scale inference process into a seamless collaboration between a large model and a small model. The large model serves as the 'drafter', specializing in generating low-frequency content at smaller scales, while the smaller model serves as the 'refiner', solely focusing on predicting high-frequency details at larger scales. This collaboration yields remarkable efficiency with minimal impact on quality: CoDe achieves a 1.7x speedup, slashes memory usage by around 50%, and preserves image quality with only a negligible FID increase from 1.95 to 1.98. When drafting steps are further decreased, CoDe can achieve an impressive 2.9x acceleration ratio, reaching 41 images/s at 256x256 resolution on a single NVIDIA 4090 GPU, while preserving a commendable FID of 2.27. The code is available at https://github.com/czg1225/CoDe",
            "score": 9,
            "issue_id": 828,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "0ac2e8ea4bbd89ad",
            "authors": [
                "Zigeng Chen",
                "Xinyin Ma",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17787.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#optimization",
                    "#small_models",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (VAR) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Collaborative Decoding (CoDe). CoDe Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸: Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ğ° Ğ¼Ğ°Ğ»Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 2.9 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ FID."
                },
                "en": {
                    "title": "Collaborative Decoding: Boosting Efficiency in Image Generation",
                    "desc": "This paper introduces Collaborative Decoding (CoDe), an efficient decoding strategy for Visual Auto-Regressive (VAR) models in image generation. CoDe optimizes the multi-scale inference process by utilizing a large model to generate low-frequency content and a smaller model to refine high-frequency details. This collaboration significantly reduces memory usage by about 50% and increases processing speed by up to 2.9 times, while maintaining image quality with only a slight increase in FID score. The proposed method demonstrates a promising approach to overcoming the computational challenges associated with VAR modeling."
                },
                "zh": {
                    "title": "åä½œè§£ç ï¼šæå‡å›¾åƒç”Ÿæˆæ•ˆç‡çš„æ–°ç­–ç•¥",
                    "desc": "åœ¨å¿«é€Ÿå‘å±•çš„å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œè§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰å»ºæ¨¡å› å…¶åˆ›æ–°çš„ä¸‹ä¸€æ­¥é¢„æµ‹æ–¹æ³•è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚VARçš„ç²—åˆ°ç»†ç‰¹æ€§å¯¼è‡´äº†è¾ƒé•¿çš„ä»¤ç‰Œåºåˆ—ï¼Œä»è€Œé€ æˆäº†é«˜æ˜‚çš„å†…å­˜æ¶ˆè€—å’Œè®¡ç®—å†—ä½™ã€‚ä¸ºäº†è§£å†³è¿™äº›ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆè§£ç ç­–ç•¥â€”â€”åä½œè§£ç ï¼ˆCoDeï¼‰ï¼Œå®ƒé€šè¿‡å¤§æ¨¡å‹å’Œå°æ¨¡å‹çš„æ— ç¼åä½œæ¥ä¼˜åŒ–å¤šå°ºåº¦æ¨ç†è¿‡ç¨‹ã€‚CoDeåœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†1.7å€çš„åŠ é€Ÿå’Œçº¦50%çš„å†…å­˜ä½¿ç”¨å‡å°‘ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•ˆç‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15139",
            "title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
            "url": "https://huggingface.co/papers/2411.15139",
            "abstract": "Recently, the diffusion model has emerged as a powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions. Leveraging its capability for end-to-end autonomous driving is a promising direction. However, the numerous denoising steps in the robotic diffusion policy and the more dynamic, open-world nature of traffic scenes pose substantial challenges for generating diverse driving actions at a real-time speed. To address these challenges, we propose a novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution. Additionally, we design an efficient cascade diffusion decoder for enhanced interaction with conditional scene context. The proposed model, DiffusionDrive, demonstrates 10times reduction in denoising steps compared to vanilla diffusion policy, delivering superior diversity and quality in just 2 steps. On the planning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone, DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new record, while running at a real-time speed of 45 FPS on an NVIDIA 4090. Qualitative results on challenging scenarios further confirm that DiffusionDrive can robustly generate diverse plausible driving actions. Code and model will be available at https://github.com/hustvl/DiffusionDrive.",
            "score": 8,
            "issue_id": 828,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "cef89e5425478782",
            "authors": [
                "Bencheng Liao",
                "Shaoyu Chen",
                "Haoran Yin",
                "Bo Jiang",
                "Cheng Wang",
                "Sixu Yan",
                "Xinbang Zhang",
                "Xiangyu Li",
                "Ying Zhang",
                "Qian Zhang",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Institute of Artificial Intelligence, Huazhong University of Science & Technology",
                "School of EIC, Huazhong University of Science & Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15139.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#robotics",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DiffusionDrive Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑƒÑĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ ÑĞºĞ¾Ñ€Ñ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ ÑÑ†ĞµĞ½Ñ‹. DiffusionDrive Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ 10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 2 ÑˆĞ°Ğ³Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Robotic Driving with Efficient Diffusion Models",
                    "desc": "This paper introduces DiffusionDrive, a novel approach to robotic policy learning using diffusion models. It addresses the challenges of generating diverse driving actions in dynamic traffic environments by implementing a truncated diffusion policy that utilizes prior multi-mode anchors. The model significantly reduces the number of denoising steps required, achieving high-quality action generation in just 2 steps while maintaining real-time performance. Experimental results show that DiffusionDrive outperforms existing methods in both diversity and quality of driving actions, setting a new benchmark in the field."
                },
                "zh": {
                    "title": "æ‰©æ•£é©±åŠ¨ï¼šå®æ—¶å¤šæ ·åŒ–é©¾é©¶ç­–ç•¥çš„çªç ´",
                    "desc": "æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§å¼ºå¤§çš„ç”ŸæˆæŠ€æœ¯ï¼Œåœ¨æœºå™¨äººç­–ç•¥å­¦ä¹ ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œèƒ½å¤Ÿå»ºæ¨¡å¤šæ¨¡æ€çš„åŠ¨ä½œåˆ†å¸ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æˆªæ–­æ‰©æ•£ç­–ç•¥ï¼Œç»“åˆäº†å…ˆå‰çš„å¤šæ¨¡æ€é”šç‚¹ï¼Œå¹¶ç¼©çŸ­äº†æ‰©æ•£è°ƒåº¦ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿä»é”šå®šçš„é«˜æ–¯åˆ†å¸ƒå­¦ä¹ å»å™ªå£°ï¼Œç”Ÿæˆå¤šæ¨¡æ€é©¾é©¶åŠ¨ä½œåˆ†å¸ƒã€‚è¯¥æ¨¡å‹DiffusionDriveåœ¨å»å™ªæ­¥éª¤ä¸Šå‡å°‘äº†10å€ï¼Œèƒ½å¤Ÿåœ¨ä»…éœ€2ä¸ªæ­¥éª¤å†…æä¾›æ›´é«˜çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusionDriveåœ¨å®æ—¶é€Ÿåº¦ä¸‹è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç¨³å¥åœ°ç”Ÿæˆå¤šæ ·åŒ–çš„åˆç†é©¾é©¶åŠ¨ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17786",
            "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching",
            "url": "https://huggingface.co/papers/2411.17786",
            "abstract": "Personalized image generation requires text-to-image generative models that capture the core features of a reference subject to allow for controlled generation across different contexts. Existing methods face challenges due to complex training requirements, high inference costs, limited flexibility, or a combination of these issues. In this paper, we introduce DreamCache, a scalable approach for efficient and high-quality personalized image generation. By caching a small number of reference image features from a subset of layers and a single timestep of the pretrained diffusion denoiser, DreamCache enables dynamic modulation of the generated image features through lightweight, trained conditioning adapters. DreamCache achieves state-of-the-art image and text alignment, utilizing an order of magnitude fewer extra parameters, and is both more computationally effective and versatile than existing models.",
            "score": 7,
            "issue_id": 833,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "14d0175202b8957e",
            "authors": [
                "Emanuele Aiello",
                "Umberto Michieli",
                "Diego Valsesia",
                "Mete Ozay",
                "Enrico Magli"
            ],
            "affiliations": [
                "Politecnico di Torino",
                "Samsung R&D Institute UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17786.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "DreamCache: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DreamCache - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑĞ»Ğ¾ĞµĞ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¸Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ². DreamCache Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ° Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ĞµĞ½ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "DreamCache: Efficient Personalized Image Generation Made Easy",
                    "desc": "This paper presents DreamCache, a novel method for personalized image generation using text-to-image generative models. DreamCache improves efficiency by caching features from reference images and utilizing lightweight conditioning adapters for dynamic modulation. It addresses the limitations of existing methods, such as high computational costs and inflexibility, while achieving superior image and text alignment. Overall, DreamCache offers a scalable solution that requires significantly fewer parameters and resources compared to traditional approaches."
                },
                "zh": {
                    "title": "DreamCacheï¼šé«˜æ•ˆä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆéœ€è¦æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿæ•æ‰å‚è€ƒå¯¹è±¡çš„æ ¸å¿ƒç‰¹å¾ï¼Œä»¥ä¾¿åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ§åˆ¶ç”Ÿæˆã€‚ç°æœ‰æ–¹æ³•é¢ä¸´å¤æ‚çš„è®­ç»ƒè¦æ±‚ã€é«˜æ¨ç†æˆæœ¬å’Œæœ‰é™çš„çµæ´»æ€§ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†DreamCacheï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆä¸”é«˜è´¨é‡åœ°ç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒã€‚é€šè¿‡ç¼“å­˜å°‘é‡å‚è€ƒå›¾åƒç‰¹å¾å’Œé¢„è®­ç»ƒæ‰©æ•£å»å™ªå™¨çš„å•ä¸ªæ—¶é—´æ­¥ï¼ŒDreamCacheå®ç°äº†ç”Ÿæˆå›¾åƒç‰¹å¾çš„åŠ¨æ€è°ƒèŠ‚ï¼Œä¸”æ‰€éœ€é¢å¤–å‚æ•°æ˜¾è‘—å‡å°‘ï¼Œè®¡ç®—æ•ˆç‡å’Œçµæ´»æ€§å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17440",
            "title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition",
            "url": "https://huggingface.co/papers/2411.17440",
            "abstract": "Identity-preserving text-to-video (IPT2V) generation aims to create high-fidelity videos with consistent human identity. It is an important task in video generation but remains an open problem for generative models. This paper pushes the technical frontier of IPT2V in two directions that have not been resolved in literature: (1) A tuning-free pipeline without tedious case-by-case finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V model to keep human identity consistent in the generated video. Inspired by prior findings in frequency analysis of diffusion transformers, it employs identity-control signals in the frequency domain, where facial features can be decomposed into low-frequency global features and high-frequency intrinsic features. First, from a low-frequency perspective, we introduce a global facial extractor, which encodes reference images and facial key points into a latent space, generating features enriched with low-frequency information. These features are then integrated into shallow layers of the network to alleviate training challenges associated with DiT. Second, from a high-frequency perspective, we design a local facial extractor to capture high-frequency details and inject them into transformer blocks, enhancing the model's ability to preserve fine-grained features. We propose a hierarchical training strategy to leverage frequency information for identity preservation, transforming a vanilla pre-trained video generation model into an IPT2V model. Extensive experiments demonstrate that our frequency-aware heuristic scheme provides an optimal control solution for DiT-based models. Thanks to this scheme, our ConsisID generates high-quality, identity-preserving videos, making strides towards more effective IPT2V.",
            "score": 5,
            "issue_id": 827,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "28823ea0e7fa6b0c",
            "authors": [
                "Shenghai Yuan",
                "Jinfa Huang",
                "Xianyi He",
                "Yunyuan Ge",
                "Yujun Shi",
                "Liuhan Chen",
                "Jiebo Luo",
                "Li Yuan"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "Peng Cheng Laboratory",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17440.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ConsisID - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ facial features Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ»Ğ¸Ñ† Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "ConsisID: Seamless Identity Preservation in Video Generation",
                    "desc": "This paper presents a novel approach to identity-preserving text-to-video (IPT2V) generation, focusing on maintaining consistent human identity in videos. The authors introduce ConsisID, a controllable model that operates without the need for extensive fine-tuning, simplifying the process of video generation. By utilizing a frequency-aware heuristic, the model effectively separates and processes low-frequency global features and high-frequency intrinsic details of facial characteristics. The proposed hierarchical training strategy enhances the model's performance, allowing it to generate high-fidelity videos while preserving individual identities throughout the video sequence."
                },
                "zh": {
                    "title": "é«˜ä¿çœŸèº«ä»½ä¿æŒè§†é¢‘ç”Ÿæˆçš„çªç ´",
                    "desc": "èº«ä»½ä¿æŒçš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼ˆIPT2Vï¼‰æ—¨åœ¨åˆ›å»ºå…·æœ‰ä¸€è‡´äººç±»èº«ä»½çš„é«˜ä¿çœŸè§†é¢‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºConsisIDçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦ç¹çå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä¿æŒç”Ÿæˆè§†é¢‘ä¸­çš„äººç±»èº«ä»½ä¸€è‡´æ€§ã€‚è¯¥æ¨¡å‹åˆ©ç”¨é¢‘ç‡åˆ†æçš„æ–¹æ³•ï¼Œå°†äººè„¸ç‰¹å¾åˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘ä¿¡æ¯ï¼Œä»è€Œæœ‰æ•ˆåœ°æå–å’Œä¿ç•™ç»†è‡´çš„é¢éƒ¨ç‰¹å¾ã€‚é€šè¿‡å±‚æ¬¡åŒ–çš„è®­ç»ƒç­–ç•¥ï¼ŒConsisIDåœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„èº«ä»½ä¿æŒæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17769",
            "title": "Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis",
            "url": "https://huggingface.co/papers/2411.17769",
            "abstract": "In this work, we introduce a single parameter omega, to effectively control granularity in diffusion-based synthesis. This parameter is incorporated during the denoising steps of the diffusion model's reverse process. Our approach does not require model retraining, architectural modifications, or additional computational overhead during inference, yet enables precise control over the level of details in the generated outputs. Moreover, spatial masks or denoising schedules with varying omega values can be applied to achieve region-specific or timestep-specific granularity control. Prior knowledge of image composition from control signals or reference images further facilitates the creation of precise omega masks for granularity control on specific objects. To highlight the parameter's role in controlling subtle detail variations, the technique is named Omegance, combining \"omega\" and \"nuance\". Our method demonstrates impressive performance across various image and video synthesis tasks and is adaptable to advanced diffusion models. The code is available at https://github.com/itsmag11/Omegance.",
            "score": 4,
            "issue_id": 832,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "3fee8ae6759c9578",
            "authors": [
                "Xinyu Hou",
                "Zongsheng Yue",
                "Xiaoming Li",
                "Chen Change Loy"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17769.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#video",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¼ĞµĞ³Ğ°-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€: Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ Ğ¾Ğ¼ĞµĞ³Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ¼ĞµĞ³Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Omegance, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Omegance: Precision Control in Diffusion Synthesis",
                    "desc": "This paper presents a new parameter called omega that allows for fine control over the detail level in diffusion-based image and video synthesis. By integrating omega into the denoising steps of the diffusion model's reverse process, the authors achieve granularity control without needing to retrain the model or change its architecture. The method supports the use of spatial masks and varying omega values to target specific regions or time steps, enhancing the precision of generated outputs. Named Omegance, this technique leverages prior knowledge from control signals or reference images to create tailored omega masks for detailed object synthesis."
                },
                "zh": {
                    "title": "é€šè¿‡Omeganceå®ç°ç»†èŠ‚æ§åˆ¶çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå•ä¸€å‚æ•°omegaï¼Œç”¨äºæœ‰æ•ˆæ§åˆ¶åŸºäºæ‰©æ•£çš„åˆæˆä¸­çš„ç»†èŠ‚ç²’åº¦ã€‚è¯¥å‚æ•°åœ¨æ‰©æ•£æ¨¡å‹çš„å»å™ªæ­¥éª¤ä¸­è¢«å¼•å…¥ï¼Œå…è®¸åœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–ä¿®æ”¹æ¶æ„çš„æƒ…å†µä¸‹ï¼Œç²¾ç¡®æ§åˆ¶ç”Ÿæˆè¾“å‡ºçš„ç»†èŠ‚æ°´å¹³ã€‚é€šè¿‡åº”ç”¨ç©ºé—´æ©ç æˆ–ä¸åŒomegaå€¼çš„å»å™ªè°ƒåº¦ï¼Œå¯ä»¥å®ç°åŒºåŸŸç‰¹å®šæˆ–æ—¶é—´æ­¥ç‰¹å®šçš„ç²’åº¦æ§åˆ¶ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æ§åˆ¶ä¿¡å·æˆ–å‚è€ƒå›¾åƒçš„å…ˆéªŒçŸ¥è¯†ï¼Œå¯ä»¥åˆ›å»ºç²¾ç¡®çš„omegaæ©ç ï¼Œä»¥ä¾¿å¯¹ç‰¹å®šå¯¹è±¡è¿›è¡Œç²’åº¦æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16781",
            "title": "UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing",
            "url": "https://huggingface.co/papers/2411.16781",
            "abstract": "Human pose plays a crucial role in the digital age. While recent works have achieved impressive progress in understanding and generating human poses, they often support only a single modality of control signals and operate in isolation, limiting their application in real-world scenarios. This paper presents UniPose, a framework employing Large Language Models (LLMs) to comprehend, generate, and edit human poses across various modalities, including images, text, and 3D SMPL poses. Specifically, we apply a pose tokenizer to convert 3D poses into discrete pose tokens, enabling seamless integration into the LLM within a unified vocabulary. To further enhance the fine-grained pose perception capabilities, we facilitate UniPose with a mixture of visual encoders, among them a pose-specific visual encoder. Benefiting from a unified learning strategy, UniPose effectively transfers knowledge across different pose-relevant tasks, adapts to unseen tasks, and exhibits extended capabilities. This work serves as the first attempt at building a general-purpose framework for pose comprehension, generation, and editing. Extensive experiments highlight UniPose's competitive and even superior performance across various pose-relevant tasks.",
            "score": 4,
            "issue_id": 830,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "015e61d2dcffef60",
            "authors": [
                "Yiheng Li",
                "Ruibing Hou",
                "Hong Chang",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "affiliations": [
                "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China",
                "University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16781.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#transfer_learning",
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "UniPose: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM",
                    "desc": "UniPose - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚ Ğ¸ 3D-Ğ¿Ğ¾Ğ·Ñ‹ SMPL. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ· Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ¿Ğ¾Ğ· Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ² LLM Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¼ĞµÑÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ·, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. UniPose Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "UniPose: Unifying Human Pose Understanding Across Modalities",
                    "desc": "This paper introduces UniPose, a novel framework that utilizes Large Language Models (LLMs) to understand, generate, and edit human poses using multiple modalities such as images, text, and 3D SMPL poses. By employing a pose tokenizer, UniPose converts 3D poses into discrete tokens, allowing for integration into the LLM with a unified vocabulary. The framework enhances pose perception through a mixture of visual encoders, including a specialized pose encoder, enabling it to adapt to various pose-related tasks. UniPose represents a significant advancement in creating a versatile system for pose manipulation, demonstrating superior performance in extensive experiments across different applications."
                },
                "zh": {
                    "title": "UniPoseï¼šå¤šæ¨¡æ€äººç±»å§¿æ€ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†UniPoseæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘äººç±»å§¿æ€ã€‚UniPoseæ”¯æŒå¤šç§æ§åˆ¶ä¿¡å·çš„æ¨¡æ€ï¼ŒåŒ…æ‹¬å›¾åƒã€æ–‡æœ¬å’Œ3D SMPLå§¿æ€ï¼Œå…‹æœäº†ä»¥å¾€æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡ä½¿ç”¨å§¿æ€æ ‡è®°å™¨å°†3Då§¿æ€è½¬æ¢ä¸ºç¦»æ•£çš„å§¿æ€æ ‡è®°ï¼ŒUniPoseå®ç°äº†ä¸LLMçš„æ— ç¼é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniPoseåœ¨å¤šç§å§¿æ€ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†å…¶é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18462",
            "title": "Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding",
            "url": "https://huggingface.co/papers/2411.18462",
            "abstract": "Speculative Decoding (SD) has become an important technique in accelerating the inference speed of large language models. Conventional SD methods employ a fixed draft length, which ignores the token generation difficulty across tasks. Consequently, in this paper, we address such an issue and introduce SVIP - a difficulty-aware dynamic draft length policy for speculative decoding systems. Based on a theoretical lower bound of draft token acceptance rate and its inference-time approximation, SVIP adaptively determines the lengths of draft sequences based on the entropy of each draft token distribution. Experimental results on mainstream SD benchmarks and frameworks demonstrate the superior performance of SVIP, achieving up to 20\\% walltime speedup on SpecBench over baseline SD methods and 60\\% speedup on MT-Bench for long-form generation of up to 8K tokens. Moreover, SVIP is totally training-free and compatible with any existing SD methods that generate draft tokens autoregressively. Experimental results also show that SVIP yields consistent walltime improvement on top of GliDe & CaPE and EAGLE-2.",
            "score": 4,
            "issue_id": 828,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "b9bc0d66b9a257c9",
            "authors": [
                "Ziyin Zhang",
                "Jiahao Xu",
                "Tian Liang",
                "Xingyu Chen",
                "Zhiwei He",
                "Rui Wang",
                "Zhaopeng Tu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18462.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SVIP - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. SVIP Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - Ğ´Ğ¾ 20% Ğ½Ğ° SpecBench Ğ¸ Ğ´Ğ¾ 60% Ğ½Ğ° MT-Bench Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². SVIP Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Dynamic Draft Length for Faster Language Model Inference",
                    "desc": "This paper presents SVIP, a new method for speculative decoding (SD) that improves the efficiency of large language models by adapting the draft length based on task difficulty. Traditional SD approaches use a fixed draft length, which can be inefficient as it does not account for the varying complexity of token generation. SVIP utilizes the entropy of draft token distributions to dynamically adjust the length of draft sequences, leading to faster inference times. Experimental results indicate that SVIP can achieve significant speedups in processing time, making it a valuable enhancement for existing SD frameworks without requiring additional training."
                },
                "zh": {
                    "title": "åŠ¨æ€è‰ç¨¿é•¿åº¦ï¼Œæå‡æ¨ç†é€Ÿåº¦ï¼",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨æµ‹è§£ç æŠ€æœ¯ï¼Œç§°ä¸ºSVIPï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚ä¼ ç»Ÿçš„æ¨æµ‹è§£ç æ–¹æ³•ä½¿ç”¨å›ºå®šçš„è‰ç¨¿é•¿åº¦ï¼Œæœªèƒ½è€ƒè™‘ä¸åŒä»»åŠ¡çš„ç”Ÿæˆéš¾åº¦ã€‚SVIPé€šè¿‡åˆ†ææ¯ä¸ªè‰ç¨¿ä»¤ç‰Œåˆ†å¸ƒçš„ç†µï¼ŒåŠ¨æ€è°ƒæ•´è‰ç¨¿åºåˆ—çš„é•¿åº¦ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSVIPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæ˜¾è‘—åŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17991",
            "title": "VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format",
            "url": "https://huggingface.co/papers/2411.17991",
            "abstract": "Recent researches on video large language models (VideoLLM) predominantly focus on model architectures and training datasets, leaving the interaction format between the user and the model under-explored. In existing works, users often interact with VideoLLMs by using the entire video and a query as input, after which the model generates a response. This interaction format constrains the application of VideoLLMs in scenarios such as live-streaming comprehension where videos do not end and responses are required in a real-time manner, and also results in unsatisfactory performance on time-sensitive tasks that requires localizing video segments. In this paper, we focus on a video-text duet interaction format. This interaction format is characterized by the continuous playback of the video, and both the user and the model can insert their text messages at any position during the video playback. When a text message ends, the video continues to play, akin to the alternative of two performers in a duet. We construct MMDuetIT, a video-text training dataset designed to adapt VideoLLMs to video-text duet interaction format. We also introduce the Multi-Answer Grounded Video Question Answering (MAGQA) task to benchmark the real-time response ability of VideoLLMs. Trained on MMDuetIT, MMDuet demonstrates that adopting the video-text duet interaction format enables the model to achieve significant improvements in various time-sensitive tasks (76% CIDEr on YouCook2 dense video captioning, 90\\% mAP on QVHighlights highlight detection and 25% R@0.5 on Charades-STA temporal video grounding) with minimal training efforts, and also enable VideoLLMs to reply in a real-time manner as the video plays. Code, data and demo are available at: https://github.com/yellow-binary-tree/MMDuet.",
            "score": 3,
            "issue_id": 835,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "8c18e594b9993e25",
            "authors": [
                "Yueqian Wang",
                "Xiaojun Meng",
                "Yuxuan Wang",
                "Jianxin Liang",
                "Jiansheng Wei",
                "Huishuai Zhang",
                "Dongyan Zhao"
            ],
            "affiliations": [
                "Beijing Institute for General Artificial Intelligence",
                "Huawei Noahs Ark Lab",
                "National Key Laboratory of General Artificial Intelligence",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17991.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#games",
                    "#video"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ´ÑƒÑÑ‚: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ VideoLLM",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (VideoLLM), Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 'Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ´ÑƒÑÑ‚Ğ¾Ğ¼'. Ğ’ ÑÑ‚Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾, Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MMDuetIT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ VideoLLM ÑÑ‚Ğ¾Ğ¼Ñƒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Multi-Answer Grounded Video Question Answering (MAGQA) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Video Interaction: The Duet Format for Real-Time Responses",
                    "desc": "This paper introduces a new interaction format for video large language models (VideoLLMs) called video-text duet interaction. Unlike traditional methods where users input a full video and a query, this format allows users and the model to exchange text messages while the video plays continuously. This approach is particularly beneficial for real-time applications, such as live-streaming, where immediate responses are necessary. The authors present a new dataset, MMDuetIT, and a benchmarking task, MAGQA, demonstrating that this interaction format significantly enhances the performance of VideoLLMs on time-sensitive tasks with minimal additional training."
                },
                "zh": {
                    "title": "è§†é¢‘æ–‡æœ¬äºŒé‡å¥ï¼šå®æ—¶äº¤äº’çš„æ–°æ–¹å¼",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ä¸€ç§è§†é¢‘æ–‡æœ¬äºŒé‡å¥äº¤äº’æ ¼å¼ï¼Œä»¥æ”¹å–„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMï¼‰çš„ç”¨æˆ·äº¤äº’ä½“éªŒã€‚ä¼ ç»Ÿçš„äº¤äº’æ–¹å¼è¦æ±‚ç”¨æˆ·åœ¨è§†é¢‘ç»“æŸåè¾“å…¥æŸ¥è¯¢ï¼Œè¿™é™åˆ¶äº†å®æ—¶å“åº”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†MMDuetITæ•°æ®é›†ï¼Œä»¥é€‚åº”è¿™ç§æ–°æ ¼å¼ï¼Œå¹¶å¼•å…¥äº†å¤šç­”æ¡ˆåŸºç¡€è§†é¢‘é—®ç­”ï¼ˆMAGQAï¼‰ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹çš„å®æ—¶å“åº”èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è§†é¢‘æ–‡æœ¬äºŒé‡å¥äº¤äº’æ ¼å¼åï¼Œæ¨¡å‹åœ¨å¤šä¸ªæ—¶é—´æ•æ„Ÿä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œä¸”èƒ½å¤Ÿåœ¨è§†é¢‘æ’­æ”¾æ—¶å®æ—¶å›å¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18412",
            "title": "Adaptive Blind All-in-One Image Restoration",
            "url": "https://huggingface.co/papers/2411.18412",
            "abstract": "Blind all-in-one image restoration models aim to recover a high-quality image from an input degraded with unknown distortions. However, these models require all the possible degradation types to be defined during the training stage while showing limited generalization to unseen degradations, which limits their practical application in complex cases. In this paper, we propose a simple but effective adaptive blind all-in-one restoration (ABAIR) model, which can address multiple degradations, generalizes well to unseen degradations, and efficiently incorporate new degradations by training a small fraction of parameters. First, we train our baseline model on a large dataset of natural images with multiple synthetic degradations, augmented with a segmentation head to estimate per-pixel degradation types, resulting in a powerful backbone able to generalize to a wide range of degradations. Second, we adapt our baseline model to varying image restoration tasks using independent low-rank adapters. Third, we learn to adaptively combine adapters to versatile images via a flexible and lightweight degradation estimator. Our model is both powerful in handling specific distortions and flexible in adapting to complex tasks, it not only outperforms the state-of-the-art by a large margin on five- and three-task IR setups, but also shows improved generalization to unseen degradations and also composite distortions.",
            "score": 3,
            "issue_id": 835,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "eda508d3cf03675b",
            "authors": [
                "David Serrano-Lozano",
                "Luis Herranz",
                "Shaolin Su",
                "Javier Vazquez-Corral"
            ],
            "affiliations": [
                "Computer Vision Center",
                "Universidad Autonoma de Madrid",
                "Universitat Aut`onoma de Barcelona"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18412.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ABAIR Ğ´Ğ»Ñ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ´Ñ‹ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ABAIR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Adaptive Restoration: Flexibility Meets Performance in Image Recovery",
                    "desc": "The paper introduces an Adaptive Blind All-in-One Restoration (ABAIR) model designed to improve image restoration from various unknown distortions. Unlike traditional models that require predefined degradation types, ABAIR can adapt to new and unseen distortions by training only a small number of parameters. It utilizes a segmentation head to identify per-pixel degradation types, enhancing its ability to generalize across different tasks. The model demonstrates superior performance compared to existing methods, particularly in handling complex and composite distortions."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”ç›²å…¨èƒ½ä¿®å¤ï¼Œçµæ´»åº”å¯¹å›¾åƒæŸä¼¤",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç›²å…¨èƒ½å›¾åƒä¿®å¤æ¨¡å‹ï¼ˆABAIRï¼‰ï¼Œæ—¨åœ¨ä»å—æŸå›¾åƒä¸­æ¢å¤é«˜è´¨é‡å›¾åƒã€‚ä¸ä¼ ç»Ÿæ¨¡å‹éœ€è¦åœ¨è®­ç»ƒé˜¶æ®µå®šä¹‰æ‰€æœ‰å¯èƒ½çš„æŸä¼¤ç±»å‹ä¸åŒï¼ŒABAIRèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šç§æŸä¼¤ï¼Œå¹¶ä¸”å¯¹æœªè§è¿‡çš„æŸä¼¤å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡è®­ç»ƒä¸€ä¸ªå¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç‹¬ç«‹çš„ä½ç§©é€‚é…å™¨æ¥é€‚åº”ä¸åŒçš„å›¾åƒä¿®å¤ä»»åŠ¡ï¼Œä»è€Œå®ç°çµæ´»çš„æŸä¼¤ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒABAIRåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶åœ¨å¤„ç†å¤åˆæŸä¼¤æ—¶è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18197",
            "title": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters",
            "url": "https://huggingface.co/papers/2411.18197",
            "abstract": "3D characters are essential to modern creative industries, but making them animatable often demands extensive manual work in tasks like rigging and skinning. Existing automatic rigging tools face several limitations, including the necessity for manual annotations, rigid skeleton topologies, and limited generalization across diverse shapes and poses. An alternative approach is to generate animatable avatars pre-bound to a rigged template mesh. However, this method often lacks flexibility and is typically limited to realistic human shapes. To address these issues, we present Make-It-Animatable, a novel data-driven method to make any 3D humanoid model ready for character animation in less than one second, regardless of its shapes and poses. Our unified framework generates high-quality blend weights, bones, and pose transformations. By incorporating a particle-based shape autoencoder, our approach supports various 3D representations, including meshes and 3D Gaussian splats. Additionally, we employ a coarse-to-fine representation and a structure-aware modeling strategy to ensure both accuracy and robustness, even for characters with non-standard skeleton structures. We conducted extensive experiments to validate our framework's effectiveness. Compared to existing methods, our approach demonstrates significant improvements in both quality and speed.",
            "score": 3,
            "issue_id": 832,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "f87c0e285ba351c9",
            "authors": [
                "Zhiyang Guo",
                "Jinxu Xiang",
                "Kai Ma",
                "Wengang Zhou",
                "Houqiang Li",
                "Ran Zhang"
            ],
            "affiliations": [
                "CAS Key Laboratory of Technology in GIPAS, EEIS Department, University of Science and Technology of China",
                "Tencent PCG"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18197.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Make-It-Animatable Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğº Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ·, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞµÑ‚ĞºĞ¸ Ğ¸ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‹ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¸Ñ†. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼ Ñ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸. Make-It-Animatable Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñƒ, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ĞµĞµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹."
                },
                "en": {
                    "title": "Instant Animation for Any 3D Character!",
                    "desc": "The paper introduces Make-It-Animatable, a new method for quickly preparing 3D humanoid models for animation. This approach overcomes limitations of existing rigging tools by eliminating the need for manual annotations and allowing for diverse shapes and poses. It utilizes a particle-based shape autoencoder to generate blend weights, bones, and pose transformations efficiently. The framework is designed to be accurate and robust, even for characters with unconventional skeletons, and shows significant improvements in quality and speed over previous methods."
                },
                "zh": {
                    "title": "å¿«é€Ÿç”Ÿæˆå¯åŠ¨ç”»çš„3Dè§’è‰²æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMake-It-Animatableçš„æ–°æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…ä½¿ä»»ä½•3Däººå½¢æ¨¡å‹å‡†å¤‡å¥½è¿›è¡Œè§’è‰²åŠ¨ç”»ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„æ··åˆæƒé‡ã€éª¨éª¼å’Œå§¿æ€å˜æ¢ï¼Œè§£å†³äº†ç°æœ‰è‡ªåŠ¨ç»‘å®šå·¥å…·çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå¤šç§3Dè¡¨ç¤ºå½¢å¼ï¼ŒåŒ…æ‹¬ç½‘æ ¼å’Œ3Dé«˜æ–¯ç‚¹äº‘ï¼Œå¹¶é‡‡ç”¨äº†ç²—åˆ°ç»†çš„è¡¨ç¤ºå’Œç»“æ„æ„ŸçŸ¥å»ºæ¨¡ç­–ç•¥ï¼Œä»¥ç¡®ä¿å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15872",
            "title": "Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024 SSA and Pediatrics",
            "url": "https://huggingface.co/papers/2411.15872",
            "abstract": "Identifying key pathological features in brain MRIs is crucial for the long-term survival of glioma patients. However, manual segmentation is time-consuming, requiring expert intervention and is susceptible to human error. Therefore, significant research has been devoted to developing machine learning methods that can accurately segment tumors in 3D multimodal brain MRI scans. Despite their progress, state-of-the-art models are often limited by the data they are trained on, raising concerns about their reliability when applied to diverse populations that may introduce distribution shifts. Such shifts can stem from lower quality MRI technology (e.g., in sub-Saharan Africa) or variations in patient demographics (e.g., children). The BraTS-2024 challenge provides a platform to address these issues. This study presents our methodology for segmenting tumors in the BraTS-2024 SSA and Pediatric Tumors tasks using MedNeXt, comprehensive model ensembling, and thorough postprocessing. Our approach demonstrated strong performance on the unseen validation set, achieving an average Dice Similarity Coefficient (DSC) of 0.896 on the BraTS-2024 SSA dataset and an average DSC of 0.830 on the BraTS Pediatric Tumor dataset. Additionally, our method achieved an average Hausdorff Distance (HD95) of 14.682 on the BraTS-2024 SSA dataset and an average HD95 of 37.508 on the BraTS Pediatric dataset. Our GitHub repository can be accessed here: Project Repository : https://github.com/python-arch/BioMbz-Optimizing-Brain-Tumor-Segmentation-with-MedNeXt-BraTS-2024-SSA-and-Pediatrics",
            "score": 3,
            "issue_id": 830,
            "pub_date": "2024-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "64cec64b56df2942",
            "authors": [
                "Sarim Hashmi",
                "Juan Lugo",
                "Abdelrahman Elsayed",
                "Dinesh Saggurthi",
                "Mohammed Elseiagy",
                "Alikhan Nurkamal",
                "Jaskaran Walia",
                "Fadillah Adamsyah Maani",
                "Mohammad Yaqub"
            ],
            "affiliations": [
                "Mohammed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, United Arab Emirates"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15872.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#healthcare",
                    "#optimization",
                    "#cv",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MedNeXt: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ½Ğ° ĞœĞ Ğ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MedNeXt, Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ BraTS-2024 SSA Ğ¸ Pediatric Tumors. Ğ˜Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ”Ğ°Ğ¹ÑĞ° 0,896 Ğ¸ 0,830 Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Brain Tumor Segmentation with Advanced Machine Learning",
                    "desc": "This paper focuses on improving the segmentation of glioma tumors in brain MRIs using advanced machine learning techniques. The authors highlight the challenges of manual segmentation, which is slow and prone to errors, and propose a solution through model ensembling and postprocessing methods. They tested their approach on the BraTS-2024 challenge datasets, achieving high accuracy as measured by the Dice Similarity Coefficient and Hausdorff Distance metrics. The results indicate that their method is robust and effective, even when applied to diverse populations and varying MRI quality."
                },
                "zh": {
                    "title": "æå‡è„‘è‚¿ç˜¤MRIåˆ†å‰²çš„æœºå™¨å­¦ä¹ æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ—¨åœ¨æé«˜èƒ¶è´¨ç˜¤æ‚£è€…è„‘éƒ¨MRIå›¾åƒä¸­è‚¿ç˜¤çš„è‡ªåŠ¨åˆ†å‰²ç²¾åº¦ã€‚æˆ‘ä»¬é‡‡ç”¨äº†MedNeXtæ¨¡å‹å’Œç»¼åˆæ¨¡å‹é›†æˆçš„æ–¹æ³•ï¼Œé’ˆå¯¹BraTS-2024æŒ‘æˆ˜ä¸­çš„SSAå’Œå„¿ç«¥è‚¿ç˜¤ä»»åŠ¡è¿›è¡Œç ”ç©¶ã€‚é€šè¿‡å¯¹æœªè§éªŒè¯é›†çš„æµ‹è¯•ï¼Œæˆ‘ä»¬åœ¨BraTS-2024 SSAæ•°æ®é›†ä¸Šè¾¾åˆ°äº†å¹³å‡Diceç›¸ä¼¼ç³»æ•°0.896ï¼Œåœ¨å„¿ç«¥è‚¿ç˜¤æ•°æ®é›†ä¸Šè¾¾åˆ°äº†0.830ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ä¸åŒäººç¾¤å’Œè®¾å¤‡è´¨é‡çš„MRIæ•°æ®æ—¶å…·æœ‰è‰¯å¥½çš„é€‚åº”æ€§å’Œå¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18363",
            "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
            "url": "https://huggingface.co/papers/2411.18363",
            "abstract": "Perception and understanding are two pillars of computer vision. While multimodal large language models (MLLM) have demonstrated remarkable visual understanding capabilities, they arguably lack accurate perception abilities, e.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on the COCO dataset, limiting many tasks requiring the combination of perception and understanding. In this work, we aim to bridge this perception gap from both model designing and data development perspectives. We first introduce ChatRex, an MLLM with a decoupled perception design. Instead of having the LLM directly predict box coordinates, we feed the output boxes from a universal proposal network into the LLM, allowing it to output the corresponding box indices to represent its detection results, turning the regression task into a retrieval-based task that LLM handles more proficiently. From the data perspective, we build a fully automated data engine and construct the Rexverse-2M dataset which possesses multiple granularities to support the joint training of perception and understanding. After standard two-stage training, ChatRex demonstrates strong perception capabilities while preserving multimodal understanding performance. The combination of these two capabilities simultaneously unlocks many attractive applications, demonstrating the complementary roles of both perception and understanding in MLLM. Code is available at https://github.com/IDEA-Research/ChatRex.",
            "score": 2,
            "issue_id": 831,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "8938f77b5606e172",
            "authors": [
                "Qing Jiang",
                "Gen luo",
                "Yuqin Yang",
                "Yuda Xiong",
                "Yihao Chen",
                "Zhaoyang Zeng",
                "Tianhe Ren",
                "Lei Zhang"
            ],
            "affiliations": [
                "International Digital Economy Academy (IDEA)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18363.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#architecture",
                    "#multimodal",
                    "#dataset",
                    "#cv",
                    "#optimization",
                    "#games"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ChatRex: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ChatRex - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ³Ğ´Ğµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾ĞºÑÑ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Rexverse-2M Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ChatRex Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Perception in Multimodal Language Models",
                    "desc": "This paper addresses the limitations of multimodal large language models (MLLM) in visual perception, particularly highlighting the low recall rates of existing models like Qwen2-VL. The authors introduce ChatRex, a new MLLM that separates perception from understanding by using a universal proposal network to generate box coordinates, which the LLM then processes as indices. Additionally, they develop the Rexverse-2M dataset to enhance training by providing diverse data for both perception and understanding tasks. The results show that ChatRex achieves improved perception capabilities while maintaining strong multimodal understanding, showcasing the importance of integrating both aspects in MLLM applications."
                },
                "zh": {
                    "title": "æ„ŸçŸ¥ä¸ç†è§£çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰ä¸­çš„æ„ŸçŸ¥ä¸ç†è§£ä¸¤ä¸ªé‡è¦æ–¹é¢ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨è§†è§‰ç†è§£ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ„ŸçŸ¥èƒ½åŠ›ä¸Šä»æ˜¾ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ChatRexæ¨¡å‹ï¼Œé€šè¿‡è§£è€¦çš„æ„ŸçŸ¥è®¾è®¡ï¼Œå°†æ£€æµ‹ä»»åŠ¡è½¬åŒ–ä¸ºæ£€ç´¢ä»»åŠ¡ï¼Œä»è€Œæé«˜äº†æ„ŸçŸ¥èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ„å»ºäº†Rexverse-2Mæ•°æ®é›†ï¼Œæ”¯æŒæ„ŸçŸ¥ä¸ç†è§£çš„è”åˆè®­ç»ƒï¼Œæœ€ç»ˆå®ç°äº†è¿™ä¸¤ç§èƒ½åŠ›çš„å¼ºå¤§ç»“åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18104",
            "title": "Training and Evaluating Language Models with Template-based Data Generation",
            "url": "https://huggingface.co/papers/2411.18104",
            "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these models often struggle with tasks requiring complex reasoning, particularly in mathematical problem-solving, due in part to the scarcity of large-scale, high-quality, domain-specific datasets necessary for training sophisticated reasoning abilities. To address this limitation, we introduce Template-based Data Generation (TDG), a novel approach that leverages LLMs (GPT-4) to automatically generate parameterized meta-templates, which are then used to synthesize a vast array of high-quality problems and solutions. Leveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset comprising over 7 million synthetically generated grade school math problems--each accompanied by code-based and natural language solutions--with the potential to generate an effectively unlimited number more. This dataset alleviates the scarcity of large-scale mathematical datasets and serves as a valuable resource for pre-training, fine-tuning, and evaluating LLMs in mathematical reasoning. Our method not only enables the generation of virtually infinite data but also elevates data augmentation to a new level by using GPT-4 for meta-template generation, ensuring diverse and high-quality problem structures. The TemplateMath Part I: TemplateGSM dataset is publicly available at https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available at https://github.com/iiis-ai/TemplateMath.",
            "score": 1,
            "issue_id": 830,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "63ac26a6bf01c100",
            "authors": [
                "Yifan Zhang"
            ],
            "affiliations": [
                "IIIS, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18104.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#data",
                    "#reasoning",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Template-based Data Generation (TDG) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ GPT-4 Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ°-ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ TDG Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ TemplateMath Part I: TemplateGSM, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering LLMs with Infinite Math Problems through Template Generation",
                    "desc": "This paper discusses the limitations of large language models (LLMs) like GPT-3 and GPT-4 in performing complex reasoning tasks, especially in mathematics. To overcome this challenge, the authors propose a new method called Template-based Data Generation (TDG), which uses LLMs to create a wide range of high-quality mathematical problems and solutions. They introduce a dataset named TemplateMath Part I: TemplateGSM, containing over 7 million synthetically generated grade school math problems, which can be expanded infinitely. This dataset aims to enhance the training and evaluation of LLMs in mathematical reasoning by providing a rich resource for pre-training and fine-tuning."
                },
                "zh": {
                    "title": "åˆ©ç”¨æ¨¡æ¿ç”Ÿæˆæ— é™æ•°å­¦é—®é¢˜",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºåŸºäºæ¨¡æ¿çš„æ•°æ®ç”Ÿæˆï¼ˆTDGï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³ã€‚é€šè¿‡åˆ©ç”¨GPT-4ï¼ŒTDGèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå‚æ•°åŒ–çš„å…ƒæ¨¡æ¿ï¼Œä»è€Œåˆæˆå‡ºå¤§é‡é«˜è´¨é‡çš„æ•°å­¦é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åˆ›å»ºäº†TemplateMath Part I: TemplateGSMæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡700ä¸‡ä¸ªåˆæˆçš„æ•°å­¦é—®é¢˜ï¼Œæå¤§åœ°ç¼“è§£äº†å¤§è§„æ¨¡æ•°å­¦æ•°æ®é›†çš„ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ•°æ®é›†ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒã€å¾®è°ƒå’Œè¯„ä¼°æä¾›äº†å®è´µçš„èµ„æºï¼Œæ¨åŠ¨äº†æ•°æ®å¢å¼ºçš„è¿›æ­¥ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-27.html",
    "link_next": "2024-11-29.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "27.11",
        "en": "11/27",
        "zh": "11æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "29.11",
        "en": "11/29",
        "zh": "11æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 3,
        "#cv": 8,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 5,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 1,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 7,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è‡ªç„¶è¯­è¨€åœ¨å¤„ç†å¤šå®ä¾‹çš„ä½ç½®å’Œå±æ€§ä¿¡æ¯æ—¶çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸå®ä¾‹æ§åˆ¶çš„æ‰©æ•£æ¨¡å‹ã€‚ä»–ä»¬å¼•å…¥äº†ROI-Unpoolæ“ä½œï¼Œä¸ROI-Alignç»“åˆï¼Œå®ç°äº†é«˜æ•ˆå’Œå‡†ç¡®çš„åŒºåŸŸæ“ä½œã€‚åŸºäºè¿™ä¸€æŠ€æœ¯ï¼Œä»–ä»¬æå‡ºäº†ROICtrlï¼Œä¸€ä¸ªé€‚é…å™¨ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶åŒºåŸŸå®ä¾‹ã€‚å®éªŒè¡¨æ˜ï¼ŒROICtrlåœ¨åŒºåŸŸå®ä¾‹æ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚",
        "title": "ROICtrl: Boosting Instance Control for Visual Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è‡ªç„¶è¯­è¨€åœ¨å¤„ç†å¤šå®ä¾‹çš„ä½ç½®å’Œå±æ€§ä¿¡æ¯æ—¶çš„å±€é™æ€§ã€‚\nZhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le zÃ¬ rÃ¡n yÇ” yÃ¡n zÃ i chÇ” lÇ duÅ shÃ­ lÃ¬ de wÃ¨i zhÃ¬ hÃ© shÇ” xÃ¬ng xÃ¬n xÄ« shÃ­ de jÃº xiÃ n xÃ¬ng.\n\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸå®ä¾‹æ§åˆ¶çš„æ‰©æ•£æ¨¡å‹ã€‚\nWÃ¨i le jiÄ› juÃ© zhÃ¨ gÃ¨ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng jÄ« yÃº qÅ« yÃ¹ shÃ­ lÃ¬ kÃ²ng zhÃ¬ de kuÃ² sÃ n mÃ³ xÃ­ng.\n\nä»–ä»¬å¼•å…¥äº†ROI-Unpoolæ“ä½œï¼Œä¸ROI-Alignç»“åˆï¼Œå®ç°äº†é«˜æ•ˆå’Œå‡†ç¡®çš„åŒºåŸŸæ“ä½œã€‚\nTÄ men yÇn rÃ¹ le ROI-Unpool cÄo zuÃ², yÇ” ROI-Align jiÃ© hÃ©, shÃ­ xiÃ n le gÄo xiÃ o hÃ© zhÇ”n quÃ¨ de qÅ« yÃ¹ cÄo zuÃ².\n\nåŸºäºè¿™ä¸€æŠ€æœ¯ï¼Œä»–ä»¬æå‡ºäº†ROICtrlï¼Œä¸€ä¸ªé€‚é…å™¨ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶åŒºåŸŸå®ä¾‹ã€‚\nJÄ« yÃº zhÃ¨ yÄ« jÃ¬ shÃ¹, tÄ men tÃ­ chÅ« le ROICtrl, yÄ« gÃ¨ shÃ¬ pÃ¨i qÃ¬, nÃ©ng gÃ²u jÄ«ng quÃ¨ kÃ²ng zhÃ¬ qÅ« yÃ¹ shÃ­ lÃ¬.\n\nå®éªŒè¡¨æ˜ï¼ŒROICtrlåœ¨åŒºåŸŸå®ä¾‹æ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚\nShÃ­ yÃ n biÇo mÃ­ng, ROICtrl zÃ i qÅ« yÃ¹ shÃ­ lÃ¬ kÃ²ng zhÃ¬ fÄng miÃ n biÇo xiÃ n yÅu yÃ¬, bÃ¬ng xiÇn zhÃ¹ jiÃ ng dÄ« le jÃ¬ suÃ n chÃ©ng bÄ›n.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"è‡ªç„¶è¯­è¨€\", \"pinyin\": \"zÃ¬ rÃ¡n yÇ” yÃ¡n\", \"trans\": \"natural language\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"å¤šå®ä¾‹\", \"pinyin\": \"duÅ shÃ­ lÃ¬\", \"trans\": \"multiple instances\"},\n    {\"word\": \"ä½ç½®\", \"pinyin\": \"wÃ¨i zhÃ¬\", \"trans\": \"position\"},\n    {\"word\": \"å±æ€§\", \"pinyin\": \"shÇ” xÃ¬ng\", \"trans\": \"attribute\"},\n    {\"word\": \"ä¿¡æ¯\", \"pinyin\": \"xÃ¬n xÄ«\", \"trans\": \"information\"},\n    {\"word\": \"å±€é™æ€§\", \"pinyin\": \"jÃº xiÃ n xÃ¬ng\", \"trans\": \"limitation\"},\n    {\"word\": \"è§£å†³\", \"pinyin\": \"jiÄ› juÃ©\", \"trans\": \"solve\"},\n    {\"word\": \"é—®é¢˜\", \"pinyin\": \"wÃ¨n tÃ­\", \"trans\": \"problem\"},\n    {\"word\": \"ä½œè€…\", \"pinyin\": \"zuÃ² zhÄ›\", \"trans\": \"author\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"åŒºåŸŸ\", \"pinyin\": \"qÅ« yÃ¹\", \"trans\": \"region\"},\n    {\"word\": \"å®ä¾‹\", \"pinyin\": \"shÃ­ lÃ¬\", \"trans\": \"instance\"},\n    {\"word\": \"æ§åˆ¶\", \"pinyin\": \"kÃ²ng zhÃ¬\", \"trans\": \"control\"},\n    {\"word\": \"æ‰©æ•£\", \"pinyin\": \"kuÃ² sÃ n\", \"trans\": \"diffusion\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"ROI-Unpool\", \"pinyin\": \"ROI-Unpool\", \"trans\": \"ROI-Unpool\"},\n    {\"word\": \"æ“ä½œ\", \"pinyin\": \"cÄo zuÃ²\", \"trans\": \"operation\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ© hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄo xiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"å‡†ç¡®\", \"pinyin\": \"zhÇ”n quÃ¨\", \"trans\": \"accurate\"},\n    {\"word\": \"é€‚é…å™¨\", \"pinyin\": \"shÃ¬ pÃ¨i qÃ¬\", \"trans\": \"adapter\"},\n    {\"word\": \"ç²¾ç¡®\", \"pinyin\": \"jÄ«ng quÃ¨\", \"trans\": \"precise\"},\n    {\"word\": \"è¡¨æ˜\", \"pinyin\": \"biÇo mÃ­ng\", \"trans\": \"indicate\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"ä¼˜å¼‚\", \"pinyin\": \"yÅu yÃ¬\", \"trans\": \"excellent\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"é™ä½\", \"pinyin\": \"jiÃ ng dÄ«\", \"trans\": \"reduce\"},\n    {\"word\": \"è®¡ç®—\", \"pinyin\": \"jÃ¬ suÃ n\", \"trans\": \"computation\"},\n    {\"word\": \"æˆæœ¬\", \"pinyin\": \"chÃ©ng bÄ›n\", \"trans\": \"cost\"}\n]",
        "trans": "This article discusses the limitations of natural language in handling multi-instance positional and attribute information. To address this issue, the authors propose a diffusion model based on region instance control. They introduce the ROI-Unpool operation, which, when combined with ROI-Align, achieves efficient and accurate regional operations. Based on this technology, they present ROICtrl, an adapter that can precisely control regional instances. Experiments show that ROICtrl performs exceptionally well in regional instance control and significantly reduces computational costs.",
        "update_ts": "2024-11-28 09:11"
    }
}