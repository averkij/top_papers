{
    "date": {
        "ru": "2 Ğ¸ÑĞ½Ñ",
        "en": "June 2",
        "zh": "6æœˆ2æ—¥"
    },
    "time_utc": "2025-06-02 02:47",
    "weekday": 0,
    "issue_id": 4066,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.24863",
            "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
            "url": "https://huggingface.co/papers/2505.24863",
            "abstract": "This paper presents AlphaOne (alpha1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. alpha1 first introduces alpha moment, which represents the scaled thinking phase with a universal parameter alpha. Within this scaled pre-alpha moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the alpha moment, alpha1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate alpha1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/",
            "score": 13,
            "issue_id": 4066,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "a30c2004fdd2d154",
            "authors": [
                "Junyu Zhang",
                "Runpei Dong",
                "Han Wang",
                "Xuying Ning",
                "Haoran Geng",
                "Peihao Li",
                "Xialin He",
                "Yutong Bai",
                "Jitendra Malik",
                "Saurabh Gupta",
                "Huan Zhang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24863.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "AlphaOne: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜",
                    "desc": "AlphaOne (alpha1) - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ°Ğ»ÑŒÑ„Ğ°-Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„Ğ°Ğ·Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼ Ğ°Ğ»ÑŒÑ„Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ²ÑÑ‚Ğ°Ğ²ĞºÑƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ‘ĞµÑ€Ğ½ÑƒĞ»Ğ»Ğ¸. AlphaOne Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "AlphaOne: Revolutionizing Reasoning in Large Models",
                    "desc": "This paper introduces AlphaOne, a framework designed to enhance the reasoning capabilities of large reasoning models (LRMs) during testing. It introduces the concept of the alpha moment, which allows for a controlled thinking phase using a universal parameter. By employing a Bernoulli stochastic process, AlphaOne dynamically manages the transition from slow to fast reasoning, optimizing the model's performance. Empirical results show that AlphaOne outperforms existing methods in various complex tasks, demonstrating its effectiveness in improving reasoning efficiency."
                },
                "zh": {
                    "title": "çµæ´»è°ƒèŠ‚æ¨ç†è¿›ç¨‹çš„AlphaOneæ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†AlphaOneï¼ˆalpha1ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨æµ‹è¯•æ—¶è°ƒèŠ‚å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ¨ç†è¿›ç¨‹çš„é€šç”¨æ¡†æ¶ã€‚alpha1é¦–å…ˆå¼•å…¥äº†alphaæ—¶åˆ»ï¼Œè¡¨ç¤ºå¸¦æœ‰é€šç”¨å‚æ•°alphaçš„ç¼©æ”¾æ€ç»´é˜¶æ®µã€‚åœ¨è¿™ä¸ªç¼©æ”¾çš„å‰alphaæ—¶åˆ»é˜¶æ®µä¸­ï¼Œå®ƒé€šè¿‡å°†æ¨ç†è¿‡æ¸¡æ ‡è®°çš„æ’å…¥å»ºæ¨¡ä¸ºä¼¯åŠªåˆ©éšæœºè¿‡ç¨‹ï¼ŒåŠ¨æ€è°ƒåº¦ç¼“æ…¢æ€ç»´çš„è¿‡æ¸¡ã€‚åœ¨alphaæ—¶åˆ»ä¹‹åï¼Œalpha1é€šè¿‡æ€ç»´ç»“æŸæ ‡è®°ç¡®å®šæ€§åœ°ç»ˆæ­¢ç¼“æ…¢æ€ç»´ï¼Œä»è€Œä¿ƒè¿›å¿«é€Ÿæ¨ç†å’Œé«˜æ•ˆç­”æ¡ˆç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24417",
            "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering",
            "url": "https://huggingface.co/papers/2505.24417",
            "abstract": "Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration.",
            "score": 2,
            "issue_id": 4066,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "f28c426fafe8156a",
            "authors": [
                "Runnan Lu",
                "Yuxuan Zhang",
                "Jailing Liu",
                "Haifa Wang",
                "Yiren Song"
            ],
            "affiliations": [
                "Liblib AI",
                "National University of Singapore",
                "The Chinese University of Hong Kong",
                "Tiamat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24417.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#multilingual",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "EasyText: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EasyText - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ DiT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°."
                },
                "en": {
                    "title": "EasyText: Multilingual Text Rendering Made Simple",
                    "desc": "This paper presents EasyText, a novel framework for generating multilingual text using diffusion models. It leverages a Diffusion Transformer (DiT) to connect denoising latents with multilingual character tokens, addressing the challenge of rendering text in various languages. The authors introduce innovative techniques such as character positioning encoding and position encoding interpolation to enhance the control and precision of text rendering. They also create a large-scale dataset with 1 million multilingual image-text pairs, which significantly improves the model's performance in multilingual text rendering and visual quality."
                },
                "zh": {
                    "title": "å¤šè¯­è¨€æ–‡æœ¬æ¸²æŸ“çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºEasyTextçš„æ–‡æœ¬æ¸²æŸ“æ¡†æ¶ï¼ŒåŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æŠ€æœ¯ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å»å™ªæ½œå˜é‡ä¸å¤šè¯­è¨€å­—ç¬¦ä»¤ç‰Œè¿æ¥ï¼Œå®ç°äº†å¯¹å¤šè¯­è¨€æ–‡æœ¬çš„ç²¾ç¡®æ¸²æŸ“ã€‚æˆ‘ä»¬æå‡ºäº†å­—ç¬¦ä½ç½®ç¼–ç å’Œä½ç½®ç¼–ç æ’å€¼æŠ€æœ¯ï¼Œä»¥å®ç°å¯æ§å’Œç²¾ç¡®çš„æ–‡æœ¬æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«100ä¸‡æ¡å¤šè¯­è¨€å›¾åƒ-æ–‡æœ¬æ³¨é‡Šçš„å¤§è§„æ¨¡åˆæˆæ–‡æœ¬å›¾åƒæ•°æ®é›†ï¼Œç”¨äºé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šè¯­è¨€æ–‡æœ¬æ¸²æŸ“å’Œè§†è§‰è´¨é‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24293",
            "title": "Large Language Models are Locally Linear Mappings",
            "url": "https://huggingface.co/papers/2505.24293",
            "abstract": "We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.",
            "score": 1,
            "issue_id": 4066,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "42a9e20ff9742560",
            "authors": [
                "James R. Golden"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.24293.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ğ² ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ»Ğ¸ ĞºÑƒÑĞ¾Ñ‡Ğ½ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒ, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Llama 3, Gemma 3 Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ¾Ğ±Ğ¸Ğ°Ğ½Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ LLM Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¸Ğ· ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ñ… ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Unlocking LLMs: Linear Insights into Complex Predictions",
                    "desc": "This paper shows that the inference processes of large language models (LLMs) can be represented as linear systems without changing the model's weights or outputs. By modifying the gradient calculations for next-token predictions, the authors create a Jacobian that closely mirrors the model's predictions using linear methods. They analyze various LLMs and find that these models operate in low-dimensional spaces, where significant singular vectors correspond to key concepts for predicting the next token. This method allows for a deeper understanding of how each layer functions and reveals interpretable semantic structures in the predictions of LLMs."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„çº¿æ€§æœ¬è´¨",
                    "desc": "æœ¬æ–‡å±•ç¤ºäº†å¤šä¸ªå¼€æ”¾æƒé‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ“ä½œå¯ä»¥æ˜ å°„åˆ°ä¸€ä¸ªå®Œå…¨ç­‰ä»·çš„çº¿æ€§ç³»ç»Ÿï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡æˆ–æ”¹å˜è¾“å‡ºé¢„æµ‹ã€‚æˆ‘ä»¬å€Ÿé‰´äº†å›¾åƒæ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°æ”¹å˜ç›¸å¯¹äºç»™å®šè¾“å…¥åºåˆ—çš„æ¢¯åº¦è®¡ç®—ï¼Œä½¿å¾—æ¨¡å‹çš„é›…å¯æ¯”çŸ©é˜µå‡ ä¹å®Œå…¨é‡ç°äº†çº¿æ€§ç³»ç»Ÿçš„å‰å‘é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ¨¡å‹ä¸ŠéªŒè¯äº†è¿™ç§æ–¹æ³•ï¼Œå¹¶é€šè¿‡å¯¹åˆ†ç¦»é›…å¯æ¯”çŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£ï¼Œå‘ç°è¿™äº›LLMsåœ¨æä½ç»´çš„å­ç©ºé—´ä¸­æ“ä½œï¼Œè®¸å¤šæœ€å¤§çš„å¥‡å¼‚å‘é‡è§£ç å‡ºä¸æœ€å¯èƒ½è¾“å‡ºæ ‡è®°ç›¸å…³çš„æ¦‚å¿µã€‚å°½ç®¡ç°ä»£LLMså…·æœ‰å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›å’Œå…¨å±€éçº¿æ€§ï¼Œä½†å¯ä»¥é€šè¿‡å‡ ä¹ç²¾ç¡®çš„å±€éƒ¨çº¿æ€§åˆ†è§£è¿›è¡Œè§£é‡Šï¼Œä»è€Œæä¾›å¯¹å…¶å†…éƒ¨è¡¨ç¤ºçš„æ´å¯Ÿï¼Œå¹¶æ­ç¤ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è¿‡ç¨‹ä¸­çš„å¯è§£é‡Šè¯­ä¹‰ç»“æ„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23844",
            "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation",
            "url": "https://huggingface.co/papers/2505.23844",
            "abstract": "Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at https://github.com/ZLKong/LLM_Integration",
            "score": 1,
            "issue_id": 4066,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "252af3d7c602c2c3",
            "authors": [
                "Zhenglun Kong",
                "Zheng Zhan",
                "Shiyue Hou",
                "Yifan Gong",
                "Xin Meng",
                "Pengwei Sui",
                "Peiyan Dong",
                "Xuan Shen",
                "Zifeng Wang",
                "Pu Zhao",
                "Hao Tang",
                "Stratis Ioannidis",
                "Yanzhi Wang"
            ],
            "affiliations": [
                "Google",
                "Harvard University",
                "Northeastern University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23844.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑ‡ĞµÑ‚Ğ° ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° 50% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Adaptive Knowledge Aggregation for Enhanced LLM Performance",
                    "desc": "This paper presents a new framework for improving large language models (LLMs) by adaptively selecting and aggregating knowledge from multiple specialized LLMs. Traditional methods like ensemble and weight merging are limited by high memory usage and performance degradation due to knowledge interference. The proposed approach includes an adaptive selection network that identifies the most relevant LLMs and a dynamic weighted fusion strategy that leverages the strengths of these models. Experimental results show that this method significantly reduces knowledge interference and enhances the stability and scalability of knowledge aggregation."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”çŸ¥è¯†èšåˆï¼Œæ„å»ºæ›´å¼ºå¤§çš„è¯­è¨€æ¨¡å‹",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†é€šè¿‡ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•æŒç»­æ”¹è¿›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ•´åˆå…¶ä»–ä¸“ä¸šLLMsçš„èƒ½åŠ›æ—¶ã€‚ç°æœ‰çš„æ–¹æ³•å¦‚é›†æˆå’Œæƒé‡åˆå¹¶éœ€è¦å¤§é‡å†…å­˜ï¼Œå¹¶ä¸”éš¾ä»¥é€‚åº”å˜åŒ–çš„æ•°æ®ç¯å¢ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©å’Œèšåˆæ¥è‡ªä¸åŒLLMsçš„çŸ¥è¯†ï¼Œä»¥æ„å»ºä¸€ä¸ªæ›´å¼ºå¤§çš„å•ä¸€æ¨¡å‹ï¼Œé¿å…äº†é›†æˆæ–¹æ³•çš„é«˜å†…å­˜å¼€é”€å’Œæƒé‡åˆå¹¶çš„çµæ´»æ€§ä¸è¶³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°æ›´ç¨³å®šå’Œå¯æ‰©å±•çš„çŸ¥è¯†èšåˆè¿‡ç¨‹ï¼ŒåŒæ—¶å°†çŸ¥è¯†å¹²æ‰°å‡å°‘äº†50%ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-30.html",
    "link_next": "2025-06-03.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "30.05",
        "en": "05/30",
        "zh": "5æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "03.06",
        "en": "06/03",
        "zh": "6æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸¤ç§åè®­ç»ƒç­–ç•¥ï¼Œè’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ï¼Œç”¨äºè¡¨æ ¼æ¨ç†ä»»åŠ¡çš„æ¨ç†æ—¶ç¼©æ”¾ã€‚è¿™äº›ç­–ç•¥åˆ›å»ºäº†ä¸€ä¸ªåä¸ºTable-R1-Zeroçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„å‚æ•°åŒ¹é…GPT-4.1çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨å¤šç§è¡¨æ ¼æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬çŸ­ç­”é—®ç­”ã€äº‹å®éªŒè¯å’Œè‡ªç”±å½¢å¼é—®ç­”ã€‚ç»“æœæ˜¾ç¤ºï¼ŒTable-R1-Zeroæ¨¡å‹åœ¨ä½¿ç”¨è¾ƒå°‘å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½åŒ¹é…æˆ–è¶…è¶Šäº†GPT-4.1å’ŒDeepSeek-R1ã€‚",
        "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le liÇng zhÇ’ng hÃ²u xÃ¹nliÃ n cÃ¨lÃ¼Ã¨, zhÄ“ngliÃº hÃ© qiÃ¡ngzhÃ¬ xuÃ©xÃ­ yÇ” kÄ› yÃ nzhÃ¨ng jiÇnglÃ¬ (RLVR), yÃ²ngyÃº biÇogÃ© tuÄ«lÇ rÃ¨nwÃ¹ de tuÄ«lÇ shÃ­ suÅfÃ ng. ZhÃ¨xiÄ“ cÃ¨lÃ¼Ã¨ chuÃ ngjiÃ n le yÄ«gÃ¨ mÃ­ngyÇ Table-R1-Zero de mÃ³xÃ­ng, gÄi mÃ³xÃ­ng shÇyÃ²ng jiÃ o shÇo de cÄnshÃ¹ pÇpÃ¨i GPT-4.1 de xÃ­ngnÃ©ng, bÃ¬ng zhÃ nshÃ¬ chÅ« qiÃ¡ngdÃ  de fÃ nhuÃ  nÃ©nglÃ¬. YÃ¡njiÅ« tuÃ¡nduÃ¬ pÃ­nggÅ« le zhÃ¨xiÄ“ mÃ³xÃ­ng zÃ i duÅ zhÇ’ng biÇogÃ© tuÄ«lÇ rÃ¨nwÃ¹ zhÅng de biÇoxiÃ n, bÄokuÃ² duÇn dÃ¡ wÃ¨ndÃ¡, shÃ¬shÃ­ yÃ nzhÃ¨ng hÃ© zÃ¬yÃ³u xÃ­ngshÃ¬ wÃ¨ndÃ¡. JiÃ©guÇ’ xiÇnshÃ¬, Table-R1-Zero mÃ³xÃ­ng zÃ i shÇyÃ²ng jiÃ o shÇo cÄnshÃ¹ de qÃ­ngkuÃ ng xiÃ , xÃ­ngnÃ©ng pÇpÃ¨i huÃ² chÄoyuÃ¨ le GPT-4.1 hÃ© DeepSeek-R1.",
        "vocab": "[\n    {\"word\": \"è’¸é¦\", \"pinyin\": \"zhÄ“ngliÃº\", \"trans\": \"distillation\"},\n    {\"word\": \"å¼ºåŒ–å­¦ä¹ \", \"pinyin\": \"qiÃ¡ng huÃ  xuÃ© xÃ­\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"å¯éªŒè¯å¥–åŠ±\", \"pinyin\": \"kÄ› yÃ n zhÃ¨ng jiÇng lÃ¬\", \"trans\": \"verifiable reward\"},\n    {\"word\": \"è¡¨æ ¼æ¨ç†\", \"pinyin\": \"biÇo gÃ© tuÄ« lÇ\", \"trans\": \"table reasoning\"},\n    {\"word\": \"æ¨ç†æ—¶ç¼©æ”¾\", \"pinyin\": \"tuÄ« lÇ shÃ­ suÅ fÃ ng\", \"trans\": \"inference-time scaling\"},\n    {\"word\": \"æ³›åŒ–èƒ½åŠ›\", \"pinyin\": \"fÃ n huÃ  nÃ©ng lÃ¬\", \"trans\": \"generalization capability\"},\n    {\"word\": \"çŸ­ç­”é—®ç­”\", \"pinyin\": \"duÇn dÃ¡ wÃ¨n dÃ¡\", \"trans\": \"short answer Q&A\"},\n    {\"word\": \"äº‹å®éªŒè¯\", \"pinyin\": \"shÃ¬ shÃ­ yÃ n zhÃ¨ng\", \"trans\": \"fact verification\"},\n    {\"word\": \"è‡ªç”±å½¢å¼é—®ç­”\", \"pinyin\": \"zÃ¬ yÃ³u xÃ­ng shÃ¬ wÃ¨n dÃ¡\", \"trans\": \"free-form Q&A\"}\n]",
        "trans": "This article introduces two post-training strategies, distillation and reinforcement learning with verifiable rewards (RLVR), for scaling inference-time reasoning in table reasoning tasks. These strategies create a model called Table-R1-Zero, which matches the performance of GPT-4.1 with fewer parameters and demonstrates strong generalization capabilities. The research team evaluated the performance of these models on various table reasoning tasks, including short answer question-answering, fact verification, and free-form question-answering. The results show that the Table-R1-Zero model matches or outperforms GPT-4.1 and DeepSeek-R1 with fewer parameters.",
        "update_ts": "2025-06-01 12:47"
    }
}