{
    "date": {
        "ru": "19 сентября",
        "en": "September 19",
        "zh": "9月19日"
    },
    "time_utc": "2025-09-21 06:33",
    "weekday": 4,
    "issue_id": 6002,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.15221",
            "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
            "url": "https://huggingface.co/papers/2509.15221",
            "abstract": "ScaleCUA, a large-scale dataset and model for computer use agents, achieves state-of-the-art performance across multiple platforms and tasks by leveraging data-driven scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.",
            "score": 92,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "6081b7f60504d5ef",
            "authors": [
                "Zhaoyang Liu",
                "JingJing Xie",
                "Zichen Ding",
                "Zehao Li",
                "Bowen Yang",
                "Zhenyu Wu",
                "Xuehui Wang",
                "Qiushi Sun",
                "Shi Liu",
                "Weiyun Wang",
                "Shenglong Ye",
                "Qingyun Li",
                "Zeyue Tian",
                "Gen Luo",
                "Xiangyu Yue",
                "Biqing Qi",
                "Kai Chen",
                "Bowen Zhou",
                "Yu Qiao",
                "Qifeng Chen",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15221.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#cv"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "ScaleCUA: Масштабирование агентов компьютерного использования на основе данных",
                    "desc": "ScaleCUA представляет собой масштабный датасет и модель для агентов компьютерного использования. Модель достигает передовых результатов на различных платформах и задачах благодаря масштабированию, основанному на данных. ScaleCUA включает в себя большой набор данных, охватывающий 6 операционных систем и 3 области задач, созданный с помощью замкнутого цикла, объединяющего автоматизированных агентов и экспертов-людей. Обученная на этих расширенных данных, модель ScaleCUA демонстрирует значительные улучшения по сравнению с базовыми моделями и устанавливает новые рекорды производительности в различных тестах."
                },
                "en": {
                    "title": "Empowering Computer Use Agents with ScaleCUA",
                    "desc": "ScaleCUA is a groundbreaking dataset and model designed for computer use agents (CUAs) that enhances their ability to operate graphical user interfaces (GUIs) autonomously. By integrating a large-scale dataset that covers six operating systems and three task domains, ScaleCUA significantly improves the performance of CUAs through data-driven scaling techniques. The model demonstrates impressive results, surpassing previous benchmarks and achieving state-of-the-art performance on various tasks. This work not only advances the capabilities of CUAs but also provides open-source resources to foster further research in the field."
                },
                "zh": {
                    "title": "数据驱动扩展，助力计算机使用代理的未来",
                    "desc": "ScaleCUA是一个大型数据集和模型，专为计算机使用代理（CUAs）设计，能够在多个平台和任务上实现最先进的性能。该数据集涵盖了六种操作系统和三个任务领域，通过将自动化代理与人类专家结合的闭环流程构建而成。经过大规模数据训练，ScaleCUA能够在不同平台上无缝操作，并在多个基准测试中取得显著提升。研究结果强调了数据驱动扩展在通用计算机使用代理中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15207",
            "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
            "url": "https://huggingface.co/papers/2509.15207",
            "abstract": "FlowRL enhances LLM reinforcement learning by matching the full reward distribution through flow balancing, improving diversity and performance over reward-maximizing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.",
            "score": 81,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "e1295c2f57ad673c",
            "authors": [
                "Xuekai Zhu",
                "Daixuan Cheng",
                "Dinghuai Zhang",
                "Hengli Li",
                "Kaiyan Zhang",
                "Che Jiang",
                "Youbang Sun",
                "Ermo Hua",
                "Yuxin Zuo",
                "Xingtai Lv",
                "Qizheng Zhang",
                "Lin Chen",
                "Fanghao Shao",
                "Bo Xue",
                "Yunchong Song",
                "Zhenjie Yang",
                "Ganqu Cui",
                "Ning Ding",
                "Jianfeng Gao",
                "Xiaodong Liu",
                "Bowen Zhou",
                "Hongyuan Mei",
                "Zhouhan Lin"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University",
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "Toyota Technological Institute at Chicago",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15207.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#rl"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "FlowRL: баланс потоков для разнообразного обучения языковых моделей",
                    "desc": "Статья представляет FlowRL - новый метод обучения с подкреплением для больших языковых моделей. В отличие от традиционных подходов максимизации награды, FlowRL сопоставляет полное распределение наград через балансировку потоков. Это позволяет улучшить разнообразие и производительность модели по сравнению с методами максимизации награды. Эксперименты на задачах математических и кодовых рассуждений показывают значительное улучшение результатов с использованием FlowRL."
                },
                "en": {
                    "title": "FlowRL: Enhancing LLMs through Reward Distribution Matching",
                    "desc": "FlowRL is a novel approach in reinforcement learning for large language models (LLMs) that focuses on matching the entire reward distribution rather than just maximizing rewards. By using flow balancing, it addresses the issue of over-optimization seen in traditional methods like PPO and GRPO, which often ignore less frequent but valid reasoning paths. This method transforms scalar rewards into a normalized target distribution and minimizes the reverse KL divergence, promoting diverse exploration and better reasoning. Experiments show that FlowRL significantly outperforms existing methods on math and code reasoning tasks, demonstrating the importance of reward distribution-matching for effective learning."
                },
                "zh": {
                    "title": "FlowRL：通过流平衡实现多样化推理",
                    "desc": "FlowRL是一种增强大型语言模型（LLM）强化学习的方法，通过流平衡匹配完整的奖励分布，而不是单纯最大化奖励。传统的奖励最大化方法（如PPO和GRPO）往往过度优化主要的奖励信号，忽视了较少出现但有效的推理路径，从而降低了多样性。FlowRL通过可学习的分区函数将标量奖励转化为标准化的目标分布，并最小化策略与目标分布之间的反向KL散度。实验结果表明，FlowRL在数学基准测试中平均提高了10.0%，在代码推理任务中表现也更为优越，强调了匹配奖励分布在LLM强化学习中实现高效探索和多样化推理的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14760",
            "title": "Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration",
            "url": "https://huggingface.co/papers/2509.14760",
            "abstract": "Align3, a lightweight method using Test-Time Deliberation, enhances specification alignment in large language models across diverse scenarios with minimal overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.",
            "score": 47,
            "issue_id": 5978,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "e9e06e0e548bde84",
            "authors": [
                "Haoran Zhang",
                "Yafu Li",
                "Xuyang Hu",
                "Dongrui Liu",
                "Zhilin Wang",
                "Bo Li",
                "Yu Cheng"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "University of Illinois at Urbana-Champaign",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14760.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#architecture",
                    "#benchmark",
                    "#training",
                    "#alignment"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Align3: Точная настройка языковых моделей под пользовательские требования",
                    "desc": "Статья представляет метод Align3, который использует Test-Time Deliberation для улучшения соответствия спецификациям в больших языковых моделях (LLM). Авторы вводят понятие 'specification alignment' - способность LLM следовать динамическим, сценарно-специфичным спецификациям с точки зрения поведения и безопасности. Для оценки этой способности создан бенчмарк SpecBench, охватывающий 5 сценариев, 103 спецификации и 1500 промптов. Эксперименты показали, что Test-Time Deliberation повышает соответствие спецификациям, а Align3 улучшает баланс между безопасностью и полезностью с минимальными накладными расходами."
                },
                "en": {
                    "title": "Aligning Language Models with Dynamic Specifications Efficiently",
                    "desc": "The paper introduces Align3, a method that improves how large language models (LLMs) align with specific behavioral and safety requirements in various scenarios. It uses Test-Time Deliberation (TTD) to help models reflect on and adjust their responses according to dynamic specifications. The authors also present SpecBench, a benchmark designed to evaluate how well models adhere to these specifications across multiple scenarios and prompts. The findings demonstrate that Align3 not only enhances alignment but also balances safety and helpfulness with minimal computational cost."
                },
                "zh": {
                    "title": "Align3：轻量级的规范对齐方法",
                    "desc": "Align3是一种轻量级的方法，利用测试时深思（Test-Time Deliberation）来增强大型语言模型在多种场景下的规范对齐能力。该方法关注模型在动态、特定场景下遵循用户或组织定制的行为和安全规范的能力。通过引入层次反思和修正，Align3能够在规范边界上进行推理，确保模型的输出符合预期。我们还提出了SpecBench，一个统一的基准，用于测量规范对齐，涵盖了多种场景和规范，帮助识别对齐差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15194",
            "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
            "url": "https://huggingface.co/papers/2509.15194",
            "abstract": "EVOL-RL, a label-free reinforcement learning method, enhances large language models by balancing stability and variation, preventing entropy collapse and improving generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.",
            "score": 29,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "271ee6d47770b19f",
            "authors": [
                "Yujun Zhou",
                "Zhenwen Liang",
                "Haolin Liu",
                "Wenhao Yu",
                "Kishan Panaganti",
                "Linfeng Song",
                "Dian Yu",
                "Xiangliang Zhang",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "University of Notre Dame",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15194.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Эволюция языковых моделей без меток",
                    "desc": "Метод EVOL-RL предлагает новый подход к обучению с подкреплением без использования меток для улучшения больших языковых моделей. Он сочетает стабильность и вариативность, предотвращая коллапс энтропии и улучшая обобщающую способность моделей. EVOL-RL использует голосование большинством для стабильности и награду за новизну для поддержания разнообразия. Эксперименты показывают, что EVOL-RL превосходит базовые методы на различных задачах и доменах."
                },
                "en": {
                    "title": "Evolving Language Models with Label-Free Reinforcement Learning",
                    "desc": "EVOL-RL is a novel reinforcement learning method designed to enhance large language models without relying on labeled data. It addresses the problem of entropy collapse, where models become less diverse and informative over time. By combining stability through majority voting with a novelty-aware reward system, EVOL-RL encourages exploration and variation in model responses. This approach not only improves generalization across different tasks but also significantly boosts performance metrics compared to existing methods."
                },
                "zh": {
                    "title": "EVOL-RL：无标签强化学习的进化之路",
                    "desc": "EVOL-RL是一种无标签的强化学习方法，旨在增强大型语言模型的稳定性和多样性。它通过防止熵崩溃，保持生成内容的多样性，从而提高模型的泛化能力。与传统方法不同，EVOL-RL结合了稳定性和变化性，确保模型在没有外部标签的情况下自我改进。实验结果表明，EVOL-RL在多个任务上表现优于现有的无标签强化学习基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15185",
            "title": "Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation",
            "url": "https://huggingface.co/papers/2509.15185",
            "abstract": "Self-guided Training for AutoRegressive models (ST-AR) enhances image understanding and generation quality in autoregressive models by addressing key visual semantics challenges through self-supervised objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.",
            "score": 26,
            "issue_id": 5976,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "3da8f7302a159d5f",
            "authors": [
                "Xiaoyu Yue",
                "Zidong Wang",
                "Yuqing Wang",
                "Wenlong Zhang",
                "Xihui Liu",
                "Wanli Ouyang",
                "Lei Bai",
                "Luping Zhou"
            ],
            "affiliations": [
                "Chinese University of Hong Kong",
                "Shanghai AI Laboratory",
                "University of Hong Kong",
                "University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15185.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Самообучение авторегрессионных моделей улучшает генерацию изображений",
                    "desc": "Статья представляет новый метод обучения авторегрессионных моделей для улучшения понимания и генерации изображений - Self-guided Training for AutoRegressive models (ST-AR). Авторы выявили три ключевые проблемы, мешающие обучению высокоуровневой визуальной семантики в таких моделях. Предложенный подход решает эти проблемы с помощью самоконтролируемых целей обучения. ST-AR значительно улучшает качество генерации изображений без использования предобученных моделей представлений."
                },
                "en": {
                    "title": "Enhancing Image Generation with Self-Guided Training",
                    "desc": "The paper introduces Self-guided Training for AutoRegressive models (ST-AR), a new framework aimed at improving image understanding and generation in autoregressive models. It identifies three main challenges in applying next-token prediction to visual data: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. By incorporating self-supervised objectives during training, ST-AR effectively addresses these challenges, enhancing the model's ability to learn high-level visual semantics. The results show significant improvements in image generation quality, with FID scores increasing by approximately 42% and 49% for different model versions without using pre-trained representations."
                },
                "zh": {
                    "title": "自指导训练提升图像生成与理解",
                    "desc": "自指导训练（ST-AR）通过自监督目标解决了自回归模型在图像理解和生成中的关键视觉语义挑战，从而提升了图像理解和生成质量。研究表明，高质量的视觉表示在图像生成中至关重要，而自回归模型在图像理解方面存在局限性。本文首次系统性地探讨了将下一个标记预测范式应用于视觉领域的机制，并识别出影响高层视觉语义学习的三个关键特性。通过在训练过程中引入自监督目标，ST-AR显著提高了自回归模型的图像理解能力，并改善了生成质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13160",
            "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
            "url": "https://huggingface.co/papers/2509.13160",
            "abstract": "FinSearchComp is an open-source benchmark for evaluating financial search and reasoning capabilities of end-to-end agents, featuring realistic tasks and professional annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Search has emerged as core infrastructure for LLM-based agents and is widely viewed as critical on the path toward more general intelligence. Finance is a particularly demanding proving ground: analysts routinely conduct complex, multi-step searches over time-sensitive, domain-specific data, making it ideal for assessing both search proficiency and knowledge-grounded reasoning. Yet no existing open financial datasets evaluate data searching capability of end-to-end agents, largely because constructing realistic, complicated tasks requires deep financial expertise and time-sensitive data is hard to evaluate. We present FinSearchComp, the first fully open-source agent benchmark for realistic, open-domain financial search and reasoning. FinSearchComp comprises three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and Complex Historical Investigation -- closely reproduce real-world financial analyst workflows. To ensure difficulty and reliability, we engage 70 professional financial experts for annotation and implement a rigorous multi-stage quality-assurance pipeline. The benchmark includes 635 questions spanning global and Greater China markets, and we evaluate 21 models (products) on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy. DouBao (web) leads on the Greater China subset. Experimental analyses show that equipping agents with web search and financial plugins substantially improves results on FinSearchComp, and the country origin of models and tools impact performance significantly.By aligning with realistic analyst tasks and providing end-to-end evaluation, FinSearchComp offers a professional, high-difficulty testbed for complex financial search and reasoning.",
            "score": 24,
            "issue_id": 5975,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 сентября",
                "en": "September 16",
                "zh": "9月16日"
            },
            "hash": "a1faa8bf123c24e6",
            "authors": [
                "Liang Hu",
                "Jianpeng Jiao",
                "Jiashuo Liu",
                "Yanle Ren",
                "Zhoufutu Wen",
                "Kaiyuan Zhang",
                "Xuanliang Zhang",
                "Xiang Gao",
                "Tianci He",
                "Fei Hu",
                "Yali Liao",
                "Zaiyuan Wang",
                "Chenghao Yang",
                "Qianyu Yang",
                "Mingren Yin",
                "Zhiyuan Zeng",
                "Ge Zhang",
                "Xinyi Zhang",
                "Xiying Zhao",
                "Zhenwei Zhu",
                "Hongseok Namkoong",
                "Wenhao Huang",
                "Yuwen Tang"
            ],
            "affiliations": [
                "ByteDance",
                "Columbia Business School"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13160.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#open_source",
                    "#agents",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "💹",
                "ru": {
                    "title": "FinSearchComp: профессиональный тест для оценки финансового ИИ",
                    "desc": "FinSearchComp - это открытый эталонный тест для оценки возможностей финансового поиска и рассуждений агентов на основе нейросетей. Он включает три задачи, имитирующие реальные рабочие процессы финансовых аналитиков: поиск актуальных данных, простой исторический поиск и сложное историческое исследование. Тест содержит 635 вопросов по глобальным и китайским рынкам, разработанных с участием 70 профессиональных финансовых экспертов. Эксперименты показали, что оснащение агентов веб-поиском и финансовыми плагинами значительно улучшает результаты на FinSearchComp."
                },
                "en": {
                    "title": "FinSearchComp: Benchmarking AI in Financial Search and Reasoning",
                    "desc": "FinSearchComp is an innovative benchmark designed to assess the financial search and reasoning abilities of end-to-end AI agents. It includes realistic tasks that mimic the workflows of financial analysts, such as fetching time-sensitive data and conducting historical investigations. The benchmark is supported by professional annotations from financial experts, ensuring high-quality and relevant evaluation criteria. By testing various models on this benchmark, researchers can better understand the effectiveness of AI in handling complex financial queries and improve their performance through enhanced search capabilities."
                },
                "zh": {
                    "title": "金融搜索与推理的专业基准测试",
                    "desc": "FinSearchComp是一个开源基准，用于评估端到端智能体在金融搜索和推理方面的能力。该基准包含三个任务，模拟真实的金融分析师工作流程，确保任务的复杂性和可靠性。通过与70位专业金融专家合作进行标注，FinSearchComp提供了635个问题，涵盖全球及大中华市场。实验结果表明，结合网络搜索和金融插件的智能体在FinSearchComp上的表现显著提升，展示了其在复杂金融搜索和推理中的专业性和高难度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15130",
            "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance",
            "url": "https://huggingface.co/papers/2509.15130",
            "abstract": "WorldForge, a training-free framework, enhances video diffusion models with precise motion control and photorealistic content generation through recursive refinement, flow-gated latent fusion, and dual-path self-corrective guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.",
            "score": 20,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "f91495ad752f8344",
            "authors": [
                "Chenxi Song",
                "Yanming Yang",
                "Tong Zhao",
                "Ruibo Li",
                "Chi Zhang"
            ],
            "affiliations": [
                "AGI Lab, School of Engineering, Westlake University, Hangzhou, China",
                "The College of Computing and Data Science, Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15130.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#inference",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "WorldForge: Точный контроль движения в видео-диффузионных моделях без переобучения",
                    "desc": "WorldForge - это фреймворк для улучшения видео-диффузионных моделей без дополнительного обучения. Он обеспечивает точный контроль движения и фотореалистичную генерацию контента с помощью рекурсивного уточнения, слияния латентных пространств на основе оптического потока и двухпутевого самокорректирующего управления. WorldForge решает проблемы ограниченной управляемости и геометрической несогласованности существующих моделей. Этот подход позволяет эффективно использовать богатые латентные представления предобученных моделей для задач пространственного интеллекта."
                },
                "en": {
                    "title": "WorldForge: Training-Free Control for Photorealistic Video Synthesis",
                    "desc": "WorldForge is a novel framework that enhances video diffusion models without the need for retraining. It introduces three key modules: Intra-Step Recursive Refinement for optimizing predictions during inference, Flow-Gated Latent Fusion for separating motion from appearance, and Dual-Path Self-Corrective Guidance to correct trajectory drift. These components work together to provide precise motion control and generate photorealistic content. The framework demonstrates significant improvements in realism and consistency, making it a valuable tool for controllable video synthesis."
                },
                "zh": {
                    "title": "无训练的视频合成新范式",
                    "desc": "WorldForge是一个无需训练的框架，通过递归优化、流门控潜在融合和双路径自我校正指导，增强了视频扩散模型的运动控制和真实感内容生成。该方法解决了现有视频扩散模型在可控性和几何一致性方面的不足，避免了重新训练带来的知识退化和高计算成本。通过在推理过程中引入递归优化机制，WorldForge能够精确地注入运动轨迹。实验结果表明，该方法在真实感、轨迹一致性和视觉保真度方面优于现有技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15212",
            "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
            "url": "https://huggingface.co/papers/2509.15212",
            "abstract": "RynnVLA-001, a vision-language-action model, uses a two-stage pretraining approach and ActionVAE to achieve superior performance on robotics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.",
            "score": 18,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "0f4ee15179f6aeef",
            "authors": [
                "Yuming Jiang",
                "Siteng Huang",
                "Shengke Xue",
                "Yaxi Zhao",
                "Jun Cen",
                "Sicong Leng",
                "Kehan Li",
                "Jiayan Guo",
                "Kexiang Wang",
                "Mingxiu Chen",
                "Fan Wang",
                "Deli Zhao",
                "Xin Li"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15212.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#optimization",
                    "#training",
                    "#cv",
                    "#games",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "RynnVLA-001: Передовая модель зрения-языка-действия для робототехники",
                    "desc": "RynnVLA-001 - это модель зрения-языка-действия (VLA), разработанная для задач робототехники. Модель использует двухэтапный подход предобучения на большом масштабе видеоданных с человеческими демонстрациями. Первый этап включает генеративное предобучение на эгоцентрических видео, а второй этап добавляет предсказание траекторий ключевых точек. Дополнительно предложен вариационный автоэнкодер ActionVAE для улучшения представления действий."
                },
                "en": {
                    "title": "Revolutionizing Robotics with RynnVLA-001: A Vision-Language-Action Breakthrough!",
                    "desc": "RynnVLA-001 is a vision-language-action model designed to improve robotics tasks through a two-stage pretraining approach. The first stage involves Ego-Centric Video Generative Pretraining, where the model learns to predict future video frames based on initial frames and language instructions using a large dataset of manipulation videos. The second stage, Human-Centric Trajectory-Aware Modeling, enhances the model's ability to predict keypoint trajectories, linking visual predictions with action outcomes. Additionally, the introduction of ActionVAE helps to simplify the action representation by compressing action sequences into compact latent embeddings, leading to better performance on robotics tasks compared to existing models."
                },
                "zh": {
                    "title": "RynnVLA-001：提升机器人任务的视觉-语言-动作模型",
                    "desc": "RynnVLA-001是一种视觉-语言-动作模型，采用两阶段预训练方法和ActionVAE来提升机器人任务的表现。第一阶段是以自我为中心的视频生成预训练，利用1200万段自我中心的操作视频训练图像到视频模型，以根据初始帧和语言指令预测未来帧。第二阶段是以人为中心的轨迹感知建模，通过联合预测未来关键点轨迹，有效地将视觉帧预测与动作预测结合起来。此外，ActionVAE作为变分自编码器，压缩动作序列为紧凑的潜在嵌入，降低了VLA输出空间的复杂性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14476",
            "title": "AToken: A Unified Tokenizer for Vision",
            "url": "https://huggingface.co/papers/2509.14476",
            "abstract": "AToken, a unified visual tokenizer, achieves high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets using a 4D transformer architecture with adversarial-free training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.",
            "score": 17,
            "issue_id": 5975,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "4904c9c747d48b89",
            "authors": [
                "Jiasen Lu",
                "Liangchen Song",
                "Mingze Xu",
                "Byeongjoo Ahn",
                "Yanjun Wang",
                "Chen Chen",
                "Afshin Dehghan",
                "Yinfei Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2509.14476.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#3d",
                    "#architecture",
                    "#video",
                    "#training",
                    "#cv",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Единый токенизатор для всех визуальных данных",
                    "desc": "AToken - это унифицированный визуальный токенизатор, способный обрабатывать изображения, видео и 3D-объекты. Он использует архитектуру трансформера с 4D-позиционным кодированием для создания общего латентного пространства. AToken обучается без использования генеративно-состязательных сетей, применяя перцептивные потери и потери матрицы Грама. Модель демонстрирует высокие результаты как в задачах реконструкции, так и в задачах семантического понимания для различных визуальных модальностей."
                },
                "en": {
                    "title": "Unified Visual Tokenization for Next-Gen AI",
                    "desc": "AToken is a novel visual tokenizer that integrates high-fidelity reconstruction and semantic understanding for images, videos, and 3D assets using a 4D transformer architecture. It encodes various visual inputs into a shared latent space, allowing it to handle multiple modalities simultaneously. The model employs an adversarial-free training approach, utilizing perceptual and Gram matrix losses to ensure stable and high-quality outputs. AToken's performance is demonstrated through impressive metrics across different tasks, paving the way for advanced multimodal AI applications."
                },
                "zh": {
                    "title": "统一视觉标记，重建与理解的完美结合",
                    "desc": "AToken是一种统一的视觉标记器，能够在图像、视频和3D资产中实现高保真重建和语义理解。与现有的专注于单一模态的标记器不同，AToken将多样的视觉输入编码到一个共享的4D潜在空间中，从而统一了重建和理解任务。该方法采用纯变换器架构和4D旋转位置嵌入，能够处理任意分辨率和时间长度的视觉输入。通过无对抗训练目标，结合感知损失和Gram矩阵损失，AToken在多个基准测试中表现出色，推动了下一代多模态AI系统的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14638",
            "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks",
            "url": "https://huggingface.co/papers/2509.14638",
            "abstract": "MultiEdit, a comprehensive dataset with over 107K high-quality image editing samples, improves performance on sophisticated editing tasks using a novel pipeline with multi-modal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at https://huggingface.co/datasets/inclusionAI/MultiEdit.",
            "score": 7,
            "issue_id": 5975,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "35e0066dc539ba98",
            "authors": [
                "Mingsong Li",
                "Lin Liu",
                "Hongjun Wang",
                "Haoxing Chen",
                "Xijun Gu",
                "Shizhan Liu",
                "Dong Gong",
                "Junbo Zhao",
                "Zhenzhong Lan",
                "Jianguo Li"
            ],
            "affiliations": [
                "Inclusion AI",
                "The University of Hong Kong",
                "University of New South Wales",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14638.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "MultiEdit: революция в редактировании изображений с помощью ИИ",
                    "desc": "Статья представляет MultiEdit - новый набор данных, содержащий более 107 тысяч высококачественных образцов редактирования изображений. Набор данных охватывает 6 сложных задач редактирования через 18 типов редактирования без переноса стиля и 38 операций переноса стиля. Для создания набора данных используется новый конвейер с двумя мультимодальными большими языковыми моделями для генерации инструкций по редактированию и создания отредактированных изображений. Эксперименты показывают, что дообучение моделей на MultiEdit значительно улучшает их производительность в сложных задачах редактирования."
                },
                "en": {
                    "title": "MultiEdit: Elevating Image Editing with a Rich Dataset",
                    "desc": "The paper introduces MultiEdit, a new dataset designed to enhance instruction-based image editing (IBIE) methods by providing over 107,000 high-quality image editing samples. It addresses the limitations of existing datasets, which often contain noisy image-caption pairs and lack diversity in editing tasks. MultiEdit includes six challenging editing tasks and a variety of editing types, from style transfer to complex semantic operations. By utilizing multi-modal large language models for dataset construction, the paper demonstrates that fine-tuning models with MultiEdit significantly improves their performance on complex editing tasks while maintaining their effectiveness on standard benchmarks."
                },
                "zh": {
                    "title": "MultiEdit：提升复杂图像编辑能力的全新数据集",
                    "desc": "MultiEdit是一个包含超过107K高质量图像编辑样本的综合数据集，旨在提升复杂编辑任务的性能。该数据集涵盖了6种具有挑战性的编辑任务，包含18种非风格转移编辑类型和38种风格转移操作。通过使用多模态大型语言模型（MLLMs），我们构建了一种新颖的数据集生成管道，以生成视觉适应的编辑指令并制作高保真编辑图像。实验结果表明，使用MultiEdit训练集微调基础开源模型显著提高了模型在复杂编辑任务上的表现，同时有效保留了其在标准编辑基准上的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14233",
            "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments",
            "url": "https://huggingface.co/papers/2509.14233",
            "abstract": "Apertus is a suite of open large language models that ensure data compliance and multilingual representation through ethical data sourcing, the Goldfish objective, and comprehensive artifact release.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting robots.txt exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.",
            "score": 7,
            "issue_id": 5988,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "a50d6618d5feca99",
            "authors": [
                "Alejandro Hernández-Cano",
                "Alexander Hägele",
                "Allen Hao Huang",
                "Angelika Romanou",
                "Antoni-Joan Solergibert",
                "Barna Pasztor",
                "Bettina Messmer",
                "Dhia Garbaya",
                "Eduard Frank Ďurech",
                "Ido Hakimi",
                "Juan García Giraldo",
                "Mete Ismayilzada",
                "Negar Foroutan",
                "Skander Moalla",
                "Tiancheng Chen",
                "Vinko Sabolčec",
                "Yixuan Xu",
                "Michael Aerni",
                "Badr AlKhamissi",
                "Ines Altemir Marinas",
                "Mohammad Hossein Amani",
                "Matin Ansaripour",
                "Ilia Badanin",
                "Harold Benoit",
                "Emanuela Boros",
                "Nicholas Browning",
                "Fabian Bösch",
                "Maximilian Böther",
                "Niklas Canova",
                "Camille Challier",
                "Clement Charmillot",
                "Jonathan Coles",
                "Jan Deriu",
                "Arnout Devos",
                "Lukas Drescher",
                "Daniil Dzenhaliou",
                "Maud Ehrmann",
                "Dongyang Fan",
                "Simin Fan",
                "Silin Gao",
                "Miguel Gila",
                "María Grandury",
                "Diba Hashemi",
                "Alexander Hoyle",
                "Jiaming Jiang",
                "Mark Klein",
                "Andrei Kucharavy",
                "Anastasiia Kucherenko",
                "Frederike Lübeck",
                "Roman Machacek",
                "Theofilos Manitaras",
                "Andreas Marfurt",
                "Kyle Matoba",
                "Simon Matrenok",
                "Henrique Mendoncça",
                "Fawzi Roberto Mohamed",
                "Syrielle Montariol",
                "Luca Mouchel",
                "Sven Najem-Meyer",
                "Jingwei Ni",
                "Gennaro Oliva",
                "Matteo Pagliardini",
                "Elia Palme",
                "Andrei Panferov",
                "Léo Paoletti",
                "Marco Passerini",
                "Ivan Pavlov",
                "Auguste Poiroux",
                "Kaustubh Ponkshe",
                "Nathan Ranchin",
                "Javi Rando",
                "Mathieu Sauser",
                "Jakhongir Saydaliev",
                "Muhammad Ali Sayfiddinov",
                "Marian Schneider",
                "Stefano Schuppli",
                "Marco Scialanga",
                "Andrei Semenov",
                "Kumar Shridhar",
                "Raghav Singhal",
                "Anna Sotnikova",
                "Alexander Sternfeld",
                "Ayush Kumar Tarun",
                "Paul Teiletche",
                "Jannis Vamvas",
                "Xiaozhe Yao",
                "Hao Zhao Alexander Ilic",
                "Ana Klimovic",
                "Andreas Krause",
                "Caglar Gulcehre",
                "David Rosenthal",
                "Elliott Ash",
                "Florian Tramèr",
                "Joost VandeVondele",
                "Livio Veraldi",
                "Martin Rajman",
                "Thomas Schulthess",
                "Torsten Hoefler",
                "Antoine Bosselut",
                "Martin Jaggi",
                "Imanol Schlag"
            ],
            "affiliations": [
                "CSCS",
                "EPFL",
                "ETH Zurich",
                "HES-SO Valais-Wallis",
                "HSLU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14233.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#open_source",
                    "#ethics",
                    "#dataset",
                    "#training",
                    "#data",
                    "#low_resource"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Этичные и многоязычные LLM для всех",
                    "desc": "Apertus - это набор открытых больших языковых моделей (LLM), разработанных для решения проблем соответствия данных и многоязычного представления. Модели обучаются исключительно на открытых данных, соблюдая права владельцев контента и фильтруя неприемлемый контент. Для снижения рисков запоминания используется объективная функция Goldfish, подавляющая дословное воспроизведение данных. Модели Apertus охватывают более 1800 языков и демонстрируют результаты на уровне современных открытых моделей в многоязычных тестах."
                },
                "en": {
                    "title": "Apertus: Ethical and Multilingual Large Language Models for All",
                    "desc": "Apertus is a collection of open large language models (LLMs) that focus on ethical data sourcing and multilingual capabilities. It ensures data compliance by using only openly available data and respecting content-owner rights, while also filtering out harmful content. The models employ the Goldfish objective to reduce the risk of memorization, allowing them to perform well on various tasks without recalling specific training data verbatim. With training on a vast amount of multilingual data, Apertus achieves competitive results on multilingual benchmarks and promotes transparency by releasing all development artifacts."
                },
                "zh": {
                    "title": "Apertus：开放与合规的多语言模型",
                    "desc": "Apertus是一套开放的大型语言模型，旨在解决当前开放模型生态系统中的数据合规性和多语言表示问题。与许多之前的模型不同，Apertus模型仅在公开可用的数据上进行预训练，并遵循robots.txt的排除规则，过滤掉不允许的、有毒的和个人可识别的信息。为了减少记忆风险，我们在预训练过程中采用了金鱼目标，强烈抑制逐字回忆数据，同时保持下游任务的性能。Apertus模型在多语言覆盖方面也有所扩展，使用来自1800多种语言的15T标记进行训练，约40%的预训练数据分配给非英语内容。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15178",
            "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding",
            "url": "https://huggingface.co/papers/2509.15178",
            "abstract": "A zero-shot framework using multimodal large language models for spatio-temporal video grounding employs decomposed spatio-temporal highlighting and temporal-augmented assembling strategies to improve grounding accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as grounding tokens, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (e.g., attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
            "score": 5,
            "issue_id": 5976,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "f4b09bebd69f88b8",
            "authors": [
                "Zaiquan Yang",
                "Yuhao Liu",
                "Gerhard Hancke",
                "Rynson W. H. Lau"
            ],
            "affiliations": [
                "Department of Computer Science, City University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15178.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшение пространственно-временной локализации в видео с помощью мультимодальных языковых моделей",
                    "desc": "Статья представляет новый подход к задаче пространственно-временной локализации объектов в видео (STVG) с использованием мультимодальных больших языковых моделей (MLLM). Авторы предлагают безэталонную (zero-shot) систему, включающую стратегии декомпозированного пространственно-временного выделения (DSTH) и сборки с временным расширением (TAS). DSTH разделяет запрос на подзапросы по атрибутам и действиям, используя модуль LRA для обучения латентных переменных. TAS улучшает временную согласованность, собирая предсказания с использованием оригинальных и временно-расширенных кадров."
                },
                "en": {
                    "title": "Enhancing Video Grounding with Multimodal Language Models",
                    "desc": "This paper presents a zero-shot framework for spatio-temporal video grounding (STVG) using multimodal large language models (MLLMs). The authors identify that MLLMs can dynamically assign grounding tokens but often struggle with integrating all relevant cues from text queries. To address this, they introduce two innovative strategies: decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS), which enhance the model's reasoning capabilities. The proposed methods improve grounding accuracy by effectively separating and utilizing attributes and actions from queries while ensuring temporal consistency in predictions."
                },
                "zh": {
                    "title": "多模态模型助力时空视频精准定位",
                    "desc": "这篇论文提出了一种基于多模态大型语言模型的零-shot框架，用于时空视频定位。研究表明，多模态大型语言模型在处理文本查询时，能够动态分配特定的标记来进行定位，但在整合文本中的线索时常常表现不佳。为了解决这个问题，论文引入了分解时空高亮和时间增强组装策略，以提高定位的准确性。通过这些创新方法，研究展示了该框架在多个基准测试中超越了现有的最先进技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10397",
            "title": "RecoWorld: Building Simulated Environments for Agentic Recommender\n  Systems",
            "url": "https://huggingface.co/papers/2509.10397",
            "abstract": "RecoWorld is a simulated environment for agentic recommender systems that uses a dual-view architecture with user and recommender interactions, leveraging LLMs and multi-turn RL to enhance user retention and engagement.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RecoWorld, a blueprint for building simulated environments tailored to agentic recommender systems. Such environments give agents a proper training space where they can learn from errors without impacting real users. RecoWorld distinguishes itself with a dual-view architecture: a simulated user and an agentic recommender engage in multi-turn interactions aimed at maximizing user retention. The user simulator reviews recommended items, updates its mindset, and when sensing potential user disengagement, generates reflective instructions. The agentic recommender adapts its recommendations by incorporating these user instructions and reasoning traces, creating a dynamic feedback loop that actively engages users. This process leverages the exceptional reasoning capabilities of modern LLMs. We explore diverse content representations within the simulator, including text-based, multimodal, and semantic ID modeling, and discuss how multi-turn RL enables the recommender to refine its strategies through iterative interactions. RecoWorld also supports multi-agent simulations, allowing creators to simulate the responses of targeted user populations. It marks an important first step toward recommender systems where users and agents collaboratively shape personalized information streams. We envision new interaction paradigms where \"user instructs, recommender responds,\" jointly optimizing user retention and engagement.",
            "score": 5,
            "issue_id": 5986,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "071b32228f73fa1f",
            "authors": [
                "Fei Liu",
                "Xinyu Lin",
                "Hanchao Yu",
                "Mingyuan Wu",
                "Jianyu Wang",
                "Qiang Zhang",
                "Zhuokai Zhao",
                "Yinglong Xia",
                "Yao Zhang",
                "Weiwei Li",
                "Mingze Gao",
                "Qifan Wang",
                "Lizhu Zhang",
                "Benyu Zhang",
                "Xiangjun Fan"
            ],
            "affiliations": [
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10397.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#games",
                    "#reasoning",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "RecoWorld: Виртуальная лаборатория для обучения умных рекомендательных систем",
                    "desc": "RecoWorld - это симулированная среда для агентных рекомендательных систем, использующая двойную архитектуру взаимодействия пользователя и рекомендателя. Она применяет большие языковые модели и многоходовое обучение с подкреплением для улучшения удержания и вовлечения пользователей. Система включает в себя симулятор пользователя, который обновляет свое состояние и генерирует инструкции, и агентный рекомендатель, адаптирующий свои рекомендации на основе этих инструкций. RecoWorld поддерживает различные представления контента и многоагентные симуляции, открывая путь к новым парадигмам взаимодействия в рекомендательных системах."
                },
                "en": {
                    "title": "RecoWorld: Enhancing User Engagement through Dynamic Recommender Interactions",
                    "desc": "RecoWorld is a simulated environment designed for training agentic recommender systems, allowing them to learn from mistakes without affecting real users. It features a dual-view architecture where a simulated user interacts with the recommender in multi-turn dialogues to enhance user retention. The user simulator provides feedback by reviewing recommendations and generating instructions when it detects disengagement, which the recommender uses to adjust its suggestions. This setup utilizes large language models (LLMs) and multi-turn reinforcement learning (RL) to create a dynamic feedback loop that fosters user engagement and optimizes personalized content delivery."
                },
                "zh": {
                    "title": "智能推荐系统的新纪元：用户与推荐者的协作",
                    "desc": "RecoWorld是一个为智能推荐系统设计的模拟环境，采用双视角架构，专注于用户与推荐者之间的互动。该环境允许智能体在不影响真实用户的情况下，通过错误学习来提升性能。用户模拟器会评估推荐项目并更新其思维，当感知到用户可能 disengagement 时，会生成反思指令。智能推荐者则根据这些指令和推理轨迹调整推荐，形成一个动态反馈循环，从而增强用户的参与度和留存率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09307",
            "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
            "url": "https://huggingface.co/papers/2509.09307",
            "abstract": "MatCha is a benchmark for evaluating the performance of multimodal large language models in understanding materials characterization images, revealing significant limitations compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.",
            "score": 5,
            "issue_id": 5988,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "dab6381d366ed933",
            "authors": [
                "Zhengzhao Lai",
                "Youbin Zheng",
                "Zhenyang Cai",
                "Haonan Lyu",
                "Jinpu Yang",
                "Hongqing Liang",
                "Yan Hu",
                "Benyou Wang"
            ],
            "affiliations": [
                "Northeastern University",
                "The Chinese University of Hong Kong, Shenzhen",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09307.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#science",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "MatCha: раскрывая пределы ИИ в анализе материалов",
                    "desc": "MatCha - это новый эталонный тест для оценки производительности мультимодальных больших языковых моделей в понимании изображений характеризации материалов. Исследование показало значительные ограничения этих моделей по сравнению с экспертами-людьми в задачах, требующих высокого уровня экспертизы и сложного визуального восприятия. Тест включает 1500 вопросов по 21 различной задаче, охватывающих четыре ключевых этапа исследования материалов. Результаты подчеркивают ограниченную адаптивность существующих мультимодальных моделей к реальным сценариям характеризации материалов."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating MLLMs in Materials Characterization",
                    "desc": "MatCha is a new benchmark designed to assess how well multimodal large language models (MLLMs) can interpret materials characterization images. It includes 1,500 expert-level questions across 21 tasks that reflect real challenges in materials science. The study shows that current MLLMs significantly underperform compared to human experts, especially in tasks requiring advanced expertise and visual understanding. This research aims to highlight the limitations of MLLMs in practical applications and encourage further advancements in the field."
                },
                "zh": {
                    "title": "MatCha：材料表征图像理解的新基准",
                    "desc": "MatCha是一个用于评估多模态大型语言模型在理解材料表征图像方面性能的基准。该基准包含1500个问题，要求具备专家级的领域知识，涵盖材料研究的四个关键阶段和21个不同任务。我们的评估显示，现有的多模态大型语言模型在处理需要高水平专业知识和复杂视觉感知的问题时，表现明显不如人类专家。MatCha的研究结果强调了现有模型在真实材料表征场景中的适应性仍然有限。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06216",
            "title": "Agentic Software Engineering: Foundational Pillars and a Research\n  Roadmap",
            "url": "https://huggingface.co/papers/2509.06216",
            "abstract": "Agentic Software Engineering introduces a dual modality approach with human and agent collaboration, redefining software engineering processes and tools to achieve complex, goal-oriented objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic Software Engineering (SE 3.0) represents a new era where intelligent agents are tasked not with simple code generation, but with achieving complex, goal-oriented SE objectives. To harness these new capabilities while ensuring trustworthiness, we must recognize a fundamental duality within the SE field in the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE for Agents. This duality demands a radical reimagining of the foundational pillars of SE (actors, processes, tools, and artifacts) which manifest differently across each modality. We propose two purpose-built workbenches to support this vision. The Agent Command Environment (ACE) serves as a command center where humans orchestrate and mentor agent teams, handling outputs such as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The Agent Execution Environment (AEE) is a digital workspace where agents perform tasks while invoking human expertise when facing ambiguity or complex trade-offs. This bi-directional partnership, which supports agent-initiated human callbacks and handovers, gives rise to new, structured engineering activities (i.e., processes) that redefine human-AI collaboration, elevating the practice from agentic coding to true agentic software engineering. This paper presents the Structured Agentic Software Engineering (SASE) vision, outlining several of the foundational pillars for the future of SE. The paper culminates in a research roadmap that identifies a few key challenges and opportunities while briefly discussing the resulting impact of this future on SE education. Our goal is not to offer a definitive solution, but to provide a conceptual scaffold with structured vocabulary to catalyze a community-wide dialogue, pushing the SE community to think beyond its classic, human-centric tenets toward a disciplined, scalable, and trustworthy agentic future.",
            "score": 5,
            "issue_id": 5986,
            "pub_date": "2025-09-07",
            "pub_date_card": {
                "ru": "7 сентября",
                "en": "September 7",
                "zh": "9月7日"
            },
            "hash": "ace980da7b8fafa9",
            "authors": [
                "Ahmed E. Hassan",
                "Hao Li",
                "Dayi Lin",
                "Bram Adams",
                "Tse-Hsun Chen",
                "Yutaro Kashiwa",
                "Dong Qiu"
            ],
            "affiliations": [
                "Concordia University, Canada",
                "Huawei Canada, Canada",
                "Nara Institute of Science and Technology, Japan",
                "Queens University, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06216.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Агентный программный инжиниринг: новая эра сотрудничества человека и ИИ",
                    "desc": "Статья представляет концепцию Агентного программного инжиниринга (SE 3.0), где интеллектуальные агенты выполняют сложные, целенаправленные задачи разработки ПО. Авторы предлагают двойственный подход, включающий SE для людей и SE для агентов, что требует переосмысления основ программной инженерии. Представлены две специализированные среды: Agent Command Environment (ACE) для оркестрации агентов людьми и Agent Execution Environment (AEE) для выполнения задач агентами. Статья завершается дорожной картой исследований, определяющей ключевые вызовы и возможности в этой новой парадигме."
                },
                "en": {
                    "title": "Revolutionizing Software Engineering with Human-Agent Collaboration",
                    "desc": "Agentic Software Engineering (SE 3.0) introduces a collaborative framework where humans and intelligent agents work together to achieve complex software engineering goals. This approach emphasizes a dual modality, distinguishing between software engineering processes designed for humans and those tailored for agents. The paper proposes two specialized environments: the Agent Command Environment (ACE) for human oversight and the Agent Execution Environment (AEE) for agent task execution, fostering a bi-directional partnership. By redefining foundational aspects of software engineering, this vision aims to enhance human-AI collaboration and set the stage for future developments in the field."
                },
                "zh": {
                    "title": "代理软件工程：人机协作的新纪元",
                    "desc": "代理软件工程（SE 3.0）引入了一种人类与智能代理协作的双重模式，重新定义了软件工程的过程和工具，以实现复杂的目标导向任务。该方法强调人类与代理之间的双向合作，提出了两个专门的工作环境：代理指挥环境（ACE）和代理执行环境（AEE）。ACE作为指挥中心，帮助人类协调代理团队，而AEE则是代理执行任务的数字工作空间。通过这种合作，软件工程的实践从简单的编码提升到真正的代理软件工程，推动了软件工程的未来发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13399",
            "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable,\n  Fine-Grained Evaluation of Multi-Turn Editing",
            "url": "https://huggingface.co/papers/2509.13399",
            "abstract": "EdiVal-Agent is an automated evaluation framework for instruction-based image editing that integrates VLMs, object detectors, and human preference models to assess instruction following, content consistency, and visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images -- resulting in limited coverage and inheriting biases from prior generative models -- or (ii) rely solely on zero-shot vision-language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise.   To address this, we introduce EdiVal-Agent, an automated, scalable, and fine-grained evaluation framework for multi-turn instruction-based editing from an object-centric perspective, supported by a suite of expert tools. Given an image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions. For evaluation, it integrates VLMs with open-vocabulary object detectors to assess instruction following, uses semantic-level feature extractors to evaluate content consistency, and leverages human preference models to judge visual quality. We show that combining VLMs with object detectors yields stronger agreement with human judgments in instruction-following evaluation compared to using VLMs alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows future tools to be seamlessly integrated, enhancing evaluation accuracy over time.   Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types and 11 state-of-the-art editing models spanning autoregressive (AR) (including Nano Banana, GPT-Image-1), flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models. Project page: https://tianyucodings.github.io/EdiVAL-page/.",
            "score": 3,
            "issue_id": 5994,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 сентября",
                "en": "September 16",
                "zh": "9月16日"
            },
            "hash": "d68786190b03b715",
            "authors": [
                "Tianyu Chen",
                "Yasi Zhang",
                "Zhi Zhang",
                "Peiyu Yu",
                "Shu Wang",
                "Zhendong Wang",
                "Kevin Lin",
                "Xiaofei Wang",
                "Zhengyuan Yang",
                "Linjie Li",
                "Chung-Ching Lin",
                "Jianwen Xie",
                "Oscar Leong",
                "Lijuan Wang",
                "Ying Nian Wu",
                "Mingyuan Zhou"
            ],
            "affiliations": [
                "Lambda, Inc",
                "Microsoft",
                "University of California, Los Angeles",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13399.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#multimodal",
                    "#optimization",
                    "#cv",
                    "#interpretability"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "EdiVal-Agent: Объективная оценка редактирования изображений с помощью ИИ",
                    "desc": "EdiVal-Agent - это автоматизированная система оценки редактирования изображений на основе инструкций. Она объединяет визуально-языковые модели (VLM), детекторы объектов и модели человеческих предпочтений для оценки выполнения инструкций, согласованности контента и визуального качества. EdiVal-Agent разбивает изображение на семантически значимые объекты и синтезирует разнообразные контекстно-зависимые инструкции по редактированию. Система показывает лучшее согласие с человеческими оценками по сравнению с использованием только VLM и метрик на основе CLIP."
                },
                "en": {
                    "title": "Revolutionizing Image Editing Evaluation with EdiVal-Agent",
                    "desc": "EdiVal-Agent is a new framework designed to evaluate instruction-based image editing by combining various machine learning techniques. It uses vision-language models (VLMs) and object detectors to assess how well images follow editing instructions, maintain content consistency, and achieve visual quality. By breaking down images into meaningful objects and generating context-aware editing instructions, EdiVal-Agent provides a more accurate evaluation compared to previous methods. The framework's modular design allows for the integration of new tools, improving evaluation accuracy and helping to identify weaknesses in current editing models."
                },
                "zh": {
                    "title": "EdiVal-Agent：智能图像编辑的评估新标准",
                    "desc": "EdiVal-Agent 是一个自动化评估框架，专门用于基于指令的图像编辑。它结合了视觉语言模型（VLMs）、物体检测器和人类偏好模型，以评估指令遵循、内容一致性和视觉质量。该框架通过将图像分解为语义上有意义的对象，并生成多样的上下文感知编辑指令，从而实现精细化评估。EdiVal-Agent 的模块化设计使得未来的工具可以无缝集成，随着时间的推移提高评估的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15020",
            "title": "Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs",
            "url": "https://huggingface.co/papers/2509.15020",
            "abstract": "Tokenizing the space with the answer letter in multiple-choice question answering improves LLM accuracy and calibration.  \t\t\t\t\tAI-generated summary \t\t\t\t When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string \"Answer:\" to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy -- tokenizing the space together with the answer letter -- as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model's confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results.",
            "score": 2,
            "issue_id": 5984,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "6e87639367d48430",
            "authors": [
                "Mario Sanz-Guerrero",
                "Minh Duc Bui",
                "Katharina von der Wense"
            ],
            "affiliations": [
                "Johannes Gutenberg University Mainz, Germany",
                "University of Colorado Boulder, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15020.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#data",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Маленькая токенизация - большая разница в оценке языковых моделей",
                    "desc": "Исследование показывает, что токенизация пробела вместе с буквой ответа в задачах множественного выбора значительно улучшает точность и калибровку больших языковых моделей (LLM). Эта, казалось бы, незначительная деталь может привести к разнице в точности до 11% и изменить рейтинги моделей. Авторы рекомендуют использовать эту стратегию токенизации для более надежной оценки LLM. Исследование подчеркивает важность тщательного проектирования методов оценки и стандартизации протоколов для обеспечения сопоставимых результатов."
                },
                "en": {
                    "title": "Tokenization Matters: Boosting LLM Accuracy in MCQA!",
                    "desc": "This paper investigates the impact of tokenization strategies on the performance of large language models (LLMs) in multiple-choice question answering (MCQA). It reveals that the way space is tokenized after the prompt can lead to significant accuracy differences, affecting model rankings by up to 11%. The authors propose a specific method of tokenizing the space along with the answer letter, which consistently improves both accuracy and model calibration. These findings emphasize the necessity for standardized evaluation practices to ensure the reliability of LLM comparisons."
                },
                "zh": {
                    "title": "优化分词提升LLM准确性与校准性",
                    "desc": "本文探讨了在多选题问答中，如何处理冒号后空格的分词对大型语言模型（LLM）准确性和校准的影响。研究发现，采用不同的分词方式可能导致准确率差异高达11%，并且可能改变模型排名，影响LLM比较的可靠性。我们推荐将空格与答案字母一起分词，这种方法在性能上表现出一致且显著的提升，同时也改善了模型的校准性。我们的研究强调了评估设计的重要性，并呼吁建立标准化和透明的评估协议，以确保结果的可靠性和可比性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14977",
            "title": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal\n  Ultrasound Intelligence",
            "url": "https://huggingface.co/papers/2509.14977",
            "abstract": "EchoVLM, a vision-language model with a Mixture of Experts architecture, improves ultrasound report generation and diagnosis by leveraging data from multiple anatomical regions.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at https://github.com/Asunatan/EchoVLM.",
            "score": 2,
            "issue_id": 5980,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "36663a681181ac46",
            "authors": [
                "Chaoyin She",
                "Ruifang Lu",
                "Lida Chen",
                "Wei Wang",
                "Qinghua Huang"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "The First Affiliated Hospital of Sun Yat-Sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14977.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#architecture",
                    "#games",
                    "#cv",
                    "#open_source",
                    "#training",
                    "#science",
                    "#dataset"
                ],
                "emoji": "🔊",
                "ru": {
                    "title": "EchoVLM: Умный помощник для ультразвуковой диагностики",
                    "desc": "EchoVLM - это модель машинного обучения для анализа ультразвуковых изображений, использующая архитектуру Mixture of Experts. Она обучена на данных из семи анатомических областей и способна выполнять несколько задач, включая генерацию отчетов, диагностику и визуальный вопросно-ответный анализ. EchoVLM значительно превосходит существующие модели в задаче генерации ультразвуковых отчетов, улучшая показатели BLEU-1 и ROUGE-1. Эта модель имеет большой потенциал для повышения точности диагностики в ультразвуковой визуализации."
                },
                "en": {
                    "title": "Revolutionizing Ultrasound Diagnosis with EchoVLM",
                    "desc": "EchoVLM is a specialized vision-language model designed to enhance ultrasound report generation and diagnosis by utilizing a Mixture of Experts (MoE) architecture. This model is trained on diverse data from seven anatomical regions, allowing it to effectively handle multiple tasks such as report generation, diagnosis, and visual question-answering. The results show that EchoVLM significantly outperforms existing models, achieving notable improvements in BLEU-1 and ROUGE-1 scores for ultrasound report generation. This advancement indicates that EchoVLM can greatly improve diagnostic accuracy in ultrasound imaging, making it a promising tool for clinical applications."
                },
                "zh": {
                    "title": "EchoVLM：提升超声诊断的智能助手",
                    "desc": "EchoVLM是一种专门为超声医学成像设计的视觉-语言模型，采用混合专家架构。该模型通过利用来自七个解剖区域的数据，显著提高了超声报告生成和诊断的效率。实验结果显示，EchoVLM在超声报告生成任务中，相较于Qwen2-VL，BLEU-1和ROUGE-1得分分别提高了10.15和4.77分。这表明EchoVLM在提高超声成像诊断准确性方面具有重要潜力，为未来的临床应用提供了可行的技术解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10402",
            "title": "Developer-LLM Conversations: An Empirical Study of Interactions and\n  Generated Code Quality",
            "url": "https://huggingface.co/papers/2509.10402",
            "abstract": "Analysis of real-world developer-LLM conversations reveals patterns in task outcomes, code quality, and common issues across multiple programming languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are becoming integral to modern software development workflows, assisting developers with code generation, API explanation, and iterative problem-solving through natural language conversations. Despite widespread adoption, there is limited understanding of how developers interact with LLMs in practice and how these conversational dynamics influence task outcomes, code quality, and software engineering workflows. To address this, we leverage CodeChat, a large dataset comprising 82,845 real-world developer-LLM conversations, containing 368,506 code snippets generated across over 20 programming languages, derived from the WildChat dataset. We find that LLM responses are substantially longer than developer prompts, with a median token-length ratio of 14:1. Multi-turn conversations account for 68% of the dataset and often evolve due to shifting requirements, incomplete prompts, or clarification requests. Topic analysis identifies web design (9.6% of conversations) and neural network training (8.7% of conversations) as the most frequent LLM-assisted tasks. Evaluation across five languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and language-specific issues in LLM-generated code: generated Python and JavaScript code often include undefined variables (83.4% and 75.3% of code snippets, respectively); Java code lacks required comments (75.9%); C++ code frequently omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a conversation, syntax and import errors persist across turns; however, documentation quality in Java improves by up to 14.7%, and import handling in Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code generated in prior turns and explicitly request a fix are most effective for resolving errors.",
            "score": 0,
            "issue_id": 5986,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "fd7d34991fe810de",
            "authors": [
                "Suzhen Zhong",
                "Ying Zou",
                "Bram Adams"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, Queens University, Kingston, ON K7L 3N6, Canada",
                "Maintenance, Construction and Intelligence of Software Lab (MCIS), School of Computing, Queens University, Kingston, ON K7L 3N6, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10402.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#plp",
                    "#games",
                    "#optimization",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Анализ диалогов человек-ИИ раскрывает нюансы взаимодействия разработчиков с языковыми моделями",
                    "desc": "Статья анализирует реальные диалоги разработчиков с языковыми моделями (LLM) на основе большого набора данных CodeChat. Исследование выявляет закономерности в результатах задач, качестве кода и распространенных проблемах для разных языков программирования. Авторы обнаружили, что многоходовые беседы составляют 68% датасета, а наиболее частые задачи связаны с веб-дизайном и обучением нейронных сетей. Анализ показал специфические для каждого языка проблемы в генерируемом коде, такие как неопределенные переменные в Python и JavaScript или отсутствие комментариев в Java."
                },
                "en": {
                    "title": "Enhancing Code Quality through Developer-LLM Conversations",
                    "desc": "This paper analyzes how developers interact with Large Language Models (LLMs) during coding tasks, revealing important patterns in the outcomes and quality of the generated code. By examining a dataset of over 82,000 real-world conversations, the study highlights that LLM responses are typically much longer than the prompts given by developers. It identifies common issues in the generated code across various programming languages, such as undefined variables in Python and JavaScript, and missing comments in Java. The findings suggest that multi-turn conversations can improve code quality, especially when developers explicitly request corrections for previous errors."
                },
                "zh": {
                    "title": "开发者与LLM对话的深度分析",
                    "desc": "本研究分析了开发者与大型语言模型（LLM）之间的对话，揭示了任务结果、代码质量和常见问题的模式。我们利用包含82,845个真实开发者-LLM对话的数据集，发现LLM的响应通常比开发者的提示长得多，且多轮对话占据了68%的数据集。通过主题分析，我们发现网页设计和神经网络训练是最常见的LLM辅助任务。研究还表明，不同编程语言生成的代码存在特定问题，例如Python和JavaScript代码常常包含未定义的变量，而Java代码缺少必要的注释。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06482",
            "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution\n  Remote Sensing Change Detection",
            "url": "https://huggingface.co/papers/2509.06482",
            "abstract": "FSG-Net addresses false alarms and semantic gaps in change detection by using a frequency-spatial synergistic approach with wavelet interaction, attention mechanisms, and gated fusion.  \t\t\t\t\tAI-generated summary \t\t\t\t Change detection from high-resolution remote sensing images lies as a cornerstone of Earth observation applications, yet its efficacy is often compromised by two critical challenges. First, false alarms are prevalent as models misinterpret radiometric variations from temporal shifts (e.g., illumination, season) as genuine changes. Second, a non-negligible semantic gap between deep abstract features and shallow detail-rich features tends to obstruct their effective fusion, culminating in poorly delineated boundaries. To step further in addressing these issues, we propose the Frequency-Spatial Synergistic Gated Network (FSG-Net), a novel paradigm that aims to systematically disentangle semantic changes from nuisance variations. Specifically, FSG-Net first operates in the frequency domain, where a Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates pseudo-changes by discerningly processing different frequency components. Subsequently, the refined features are enhanced in the spatial domain by a Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the saliency of genuine change regions. To finally bridge the semantic gap, a Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to selectively gate and integrate crucial details from shallow layers. Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at https://github.com/zxXie-Air/FSG-Net after a possible publication.",
            "score": 0,
            "issue_id": 5976,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "89822775d7f8c973",
            "authors": [
                "Zhongxiang Xie",
                "Shuangxi Miao",
                "Yuhan Jiang",
                "Zhewei Zhang",
                "Jing Yao",
                "Xuecao Li",
                "Jianxi Huang",
                "Pedram Ghamisi"
            ],
            "affiliations": [
                "Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China",
                "College of Land Science and Technology, China Agricultural University, Beijing 100193, China",
                "Faculty of Geosciences and Engineering, Southwest Jiaotong University, Chengdu 60031, China",
                "Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, 09599 Freiberg, Germany",
                "Lancaster Environment Centre, Lancaster University, LA1 4YR Lancaster, U.K."
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06482.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "FSG-Net: точное обнаружение изменений на спутниковых снимках",
                    "desc": "FSG-Net - это новая нейросетевая архитектура для обнаружения изменений на спутниковых снимках высокого разрешения. Она использует вейвлет-преобразование и механизмы внимания для уменьшения ложных срабатываний и улучшения семантического анализа. FSG-Net включает в себя модули DAWIM для обработки частотных компонент, STSAM для усиления важных пространственных областей и LGFU для объединения признаков разного уровня. Эксперименты показали, что FSG-Net превосходит существующие методы на нескольких наборах данных по обнаружению изменений."
                },
                "en": {
                    "title": "Enhancing Change Detection with FSG-Net: Bridging Gaps and Reducing False Alarms",
                    "desc": "FSG-Net is a novel approach designed to improve change detection in high-resolution remote sensing images by addressing false alarms and semantic gaps. It utilizes a frequency-spatial synergistic method that includes a Discrepancy-Aware Wavelet Interaction Module to reduce misinterpretations caused by radiometric variations. The model further enhances feature representation through a Synergistic Temporal-Spatial Attention Module, which focuses on highlighting genuine changes. Finally, a Lightweight Gated Fusion Unit effectively integrates high-level semantic information with detailed features, achieving state-of-the-art performance on several benchmarks."
                },
                "zh": {
                    "title": "FSG-Net：精准变化检测的新方法",
                    "desc": "FSG-Net是一种新颖的网络模型，旨在解决高分辨率遥感图像变化检测中的假警报和语义差距问题。该模型采用频率-空间协同的方法，通过小波交互模块和注意力机制，有效区分真实变化与干扰变化。FSG-Net首先在频率域中处理不同频率成分，以减少伪变化的影响，然后在空间域中增强真实变化区域的显著性。最后，通过轻量级门控融合单元，FSG-Net将高层语义与低层细节有效结合，显著提高了变化检测的准确性。"
                }
            }
        }
    ],
    "link_prev": "2025-09-18.html",
    "link_next": "2025-09-22.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "18.09",
        "en": "09/18",
        "zh": "9月18日"
    },
    "short_date_next": {
        "ru": "22.09",
        "en": "09/22",
        "zh": "9月22日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 10,
        "#agents": 4,
        "#cv": 9,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 1,
        "#agi": 2,
        "#games": 8,
        "#interpretability": 3,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 10,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 1
    }
}