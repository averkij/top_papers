{
    "date": {
        "ru": "14 января",
        "en": "January 14",
        "zh": "1月14日"
    },
    "time_utc": "2025-01-14 22:09",
    "weekday": 1,
    "issue_id": 1668,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.07301",
            "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2501.07301",
            "abstract": "Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models.",
            "score": 46,
            "issue_id": 1651,
            "pub_date": "2025-01-13",
            "pub_date_card": {
                "ru": "13 января",
                "en": "January 13",
                "zh": "1月13日"
            },
            "hash": "98f46bb1e2772efc",
            "authors": [
                "Zhenru Zhang",
                "Chujie Zheng",
                "Yangzhen Wu",
                "Beichen Zhang",
                "Runji Lin",
                "Bowen Yu",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07301.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#data",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Усовершенствование Process Reward Models для более точного контроля математических рассуждений",
                    "desc": "Статья посвящена Process Reward Models (PRM) для контроля процесса математических рассуждений в больших языковых моделях. Авторы выявили проблемы в существующих методах синтеза данных и оценки PRMs, таких как Monte Carlo и Best-of-N. Они предложили новый механизм фильтрации на основе консенсуса, объединяющий MC-оценку с подходом LLM-as-a-judge. В результате исследователи создали улучшенную PRM, превосходящую существующие open-source альтернативы."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with Process Reward Models",
                    "desc": "This paper introduces Process Reward Models (PRMs) as a method to enhance the reasoning capabilities of Large Language Models (LLMs) by identifying and correcting errors in their reasoning processes. The authors highlight the limitations of traditional Monte Carlo estimation methods for data synthesis, which often lead to poor performance in evaluating reasoning steps. They also point out biases in the Best-of-N evaluation strategies that can misalign with the goals of PRMs, particularly in how they assess the correctness of reasoning processes versus final answers. To overcome these issues, the paper proposes a new consensus filtering mechanism that combines different evaluation methods, resulting in improved model performance and more accurate error identification."
                },
                "zh": {
                    "title": "提升过程监督模型的有效性",
                    "desc": "本文探讨了过程奖励模型（PRMs）在大型语言模型（LLMs）数学推理中的应用，旨在识别和减少推理过程中的中间错误。研究表明，传统的基于蒙特卡洛估计的数据合成方法在性能和泛化能力上不如使用LLM作为评判者和人工标注的方法。我们还发现，现有的最佳选择（BoN）评估策略存在偏差，导致评估标准与PRM的过程验证目标不一致。为了解决这些问题，本文提出了一种共识过滤机制，结合了蒙特卡洛估计和LLM评判者，显著提高了模型性能和数据效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06425",
            "title": "Tensor Product Attention Is All You Need",
            "url": "https://huggingface.co/papers/2501.06425",
            "abstract": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.",
            "score": 35,
            "issue_id": 1651,
            "pub_date": "2025-01-11",
            "pub_date_card": {
                "ru": "11 января",
                "en": "January 11",
                "zh": "1月11日"
            },
            "hash": "f723487eccf1ccfe",
            "authors": [
                "Yifan Zhang",
                "Yifeng Liu",
                "Huizhuo Yuan",
                "Zhen Qin",
                "Yang Yuan",
                "Quanquan Gu",
                "Andrew Chi-Chih Yao"
            ],
            "affiliations": [
                "IIIS, Tsinghua University",
                "Shanghai Qi Zhi Institute",
                "TapTap",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06425.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#optimization",
                    "#inference",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное внимание: компактные трансформеры для длинных последовательностей",
                    "desc": "В статье представлен новый механизм внимания - Tensor Product Attention (TPA), использующий тензорные разложения для компактного представления запросов, ключей и значений. TPA значительно уменьшает размер кэша ключ-значение при выводе, что повышает эффективность использования памяти. На основе TPA авторы разработали новую архитектуру модели - Tensor ProducT ATTenTion Transformer (T6). Эмпирические исследования показали, что T6 превосходит стандартные базовые модели Transformer по различным метрикам. TPA позволяет обрабатывать значительно более длинные последовательности при фиксированных ресурсах, решая важную проблему масштабируемости современных языковых моделей."
                },
                "en": {
                    "title": "Efficient Attention for Longer Sequences with TPA",
                    "desc": "This paper introduces Tensor Product Attention (TPA), a new attention mechanism designed to reduce memory usage during inference in language models. TPA achieves this by using tensor decompositions to compactly represent queries, keys, and values, which allows for smaller key-value caches. The authors present the Tensor ProducT ATTenTion Transformer (T6), a model that integrates TPA and shows improved performance on language modeling tasks compared to traditional Transformer architectures. T6 not only enhances model quality but also enables the processing of longer input sequences efficiently, addressing a key limitation in current language models."
                },
                "zh": {
                    "title": "张量乘积注意力：高效处理长序列的创新方案",
                    "desc": "本文提出了一种新的注意力机制，称为张量乘积注意力（TPA），旨在解决长输入序列处理中的内存开销问题。TPA通过张量分解技术，紧凑地表示查询、键和值，从而显著减少推理时的KV缓存大小。该机制结合了上下文低秩分解和RoPE，提升了模型质量和内存效率。基于TPA，我们还引入了一种新的模型架构——张量乘积注意力变换器（T6），在语言建模任务中表现优于传统的Transformer基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06252",
            "title": "$\\text{Transformer}^2$: Self-adaptive LLMs",
            "url": "https://huggingface.co/papers/2501.06252",
            "abstract": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce \\implname, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, \\implname employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \"expert\" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. \\implname demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. \\implname represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.",
            "score": 19,
            "issue_id": 1651,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 января",
                "en": "January 9",
                "zh": "1月9日"
            },
            "hash": "935c31e095aeeec8",
            "authors": [
                "Qi Sun",
                "Edoardo Cetin",
                "Yujin Tang"
            ],
            "affiliations": [
                "Institute of Science Tokyo, Japan",
                "Sakana AI, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06252.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agi",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самоадаптация языковых моделей в реальном времени",
                    "desc": "Статья представляет новый фреймворк самоадаптации для больших языковых моделей (LLM), который позволяет адаптироваться к новым задачам в реальном времени. Метод использует двухэтапный механизм: сначала определяются свойства задачи, затем применяются специальные векторы экспертов для настройки поведения модели. Подход превосходит традиционные методы вроде LoRA, используя меньше параметров и работая эффективнее. Фреймворк демонстрирует универсальность для разных архитектур LLM и модальностей, включая задачи компьютерного зрения."
                },
                "en": {
                    "title": "Dynamic Adaptation for Language Models",
                    "desc": "This paper presents a new framework called \textit{implname} that enhances large language models (LLMs) by allowing them to adapt to new tasks in real-time without the heavy computational costs of traditional fine-tuning. Instead of adjusting the entire model, \textit{implname} selectively modifies specific components of the model's weight matrices, making it more efficient. The framework uses a two-step process during inference: first, it identifies the task requirements, and then it combines specialized 'expert' vectors, which are optimized through reinforcement learning, to tailor the model's response. This approach not only improves performance compared to existing methods like LoRA but also works across various LLM architectures and tasks, including those involving both text and images."
                },
                "zh": {
                    "title": "自适应LLMs：高效应对多样化任务的未来",
                    "desc": "自适应大型语言模型（LLMs）旨在解决传统微调方法的挑战，这些方法通常计算密集且在处理多样化任务时能力有限。我们介绍了一种新颖的自适应框架\textit{implname}，它通过选择性调整权重矩阵的单个组件，实时适应LLMs以应对未见过的任务。在推理过程中，\textit{implname}采用双重机制：首先，调度系统识别任务属性，然后动态混合经过强化学习训练的任务特定“专家”向量，以获得针对输入提示的目标行为。我们的研究方法在参数更少且效率更高的情况下，超越了广泛使用的方法，如LoRA，展示了在不同LLM架构和模态（包括视觉-语言任务）中的多样性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06173",
            "title": "VideoAuteur: Towards Long Narrative Video Generation",
            "url": "https://huggingface.co/papers/2501.06173",
            "abstract": "Recent video generation models have shown promising results in producing high-quality video clips lasting several seconds. However, these models face challenges in generating long sequences that convey clear and informative events, limiting their ability to support coherent narrations. In this paper, we present a large-scale cooking video dataset designed to advance long-form narrative generation in the cooking domain. We validate the quality of our proposed dataset in terms of visual fidelity and textual caption accuracy using state-of-the-art Vision-Language Models (VLMs) and video generation models, respectively. We further introduce a Long Narrative Video Director to enhance both visual and semantic coherence in generated videos and emphasize the role of aligning visual embeddings to achieve improved overall video quality. Our method demonstrates substantial improvements in generating visually detailed and semantically aligned keyframes, supported by finetuning techniques that integrate text and image embeddings within the video generation process. Project page: https://videoauteur.github.io/",
            "score": 18,
            "issue_id": 1653,
            "pub_date": "2025-01-10",
            "pub_date_card": {
                "ru": "10 января",
                "en": "January 10",
                "zh": "1月10日"
            },
            "hash": "e110fbe840c50afa",
            "authors": [
                "Junfei Xiao",
                "Feng Cheng",
                "Lu Qi",
                "Liangke Gui",
                "Jiepeng Cen",
                "Zhibei Ma",
                "Alan Yuille",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance",
                "ByteDance Seed",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06173.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#story_generation",
                    "#dataset",
                    "#long_context",
                    "#training",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🍳",
                "ru": {
                    "title": "Готовим длинные видео: новый подход к генерации нарративного контента",
                    "desc": "Статья представляет новый датасет видеороликов о приготовлении пищи для улучшения генерации длинных нарративных видео. Авторы проверяют качество датасета с помощью современных моделей компьютерного зрения и генерации видео. Они также предлагают метод Long Narrative Video Director для повышения визуальной и семантической согласованности генерируемых видео. Результаты показывают значительное улучшение в генерации детализированных и семантически согласованных ключевых кадров."
                },
                "en": {
                    "title": "Enhancing Long-Form Video Generation with Coherent Narratives",
                    "desc": "This paper addresses the limitations of current video generation models in creating long, coherent videos, particularly in the cooking domain. It introduces a large-scale dataset specifically designed for generating long-form cooking videos, ensuring high visual quality and accurate textual descriptions. The authors propose a Long Narrative Video Director that improves both the visual and semantic coherence of the generated content by aligning visual embeddings. Their approach shows significant advancements in producing detailed keyframes and enhancing overall video quality through the integration of text and image embeddings."
                },
                "zh": {
                    "title": "推动烹饪视频的长篇叙事生成",
                    "desc": "最近的视频生成模型在生成持续几秒的高质量视频片段方面取得了良好效果。然而，这些模型在生成长序列时面临挑战，难以传达清晰且信息丰富的事件，限制了它们支持连贯叙述的能力。本文提出了一个大规模的烹饪视频数据集，旨在推动烹饪领域的长篇叙事生成。我们引入了一种长叙事视频导演，增强生成视频的视觉和语义一致性，并强调对齐视觉嵌入在提高整体视频质量中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07572",
            "title": "WebWalker: Benchmarking LLMs in Web Traversal",
            "url": "https://huggingface.co/papers/2501.07572",
            "abstract": "Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.",
            "score": 14,
            "issue_id": 1651,
            "pub_date": "2025-01-13",
            "pub_date_card": {
                "ru": "13 января",
                "en": "January 13",
                "zh": "1月13日"
            },
            "hash": "1dd4e60432c1ca54",
            "authors": [
                "Jialong Wu",
                "Wenbiao Yin",
                "Yong Jiang",
                "Zhenglin Wang",
                "Zekun Xi",
                "Runnan Fang",
                "Deyu Zhou",
                "Pengjun Xie",
                "Fei Huang"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07572.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#benchmark",
                    "#agi",
                    "#optimization",
                    "#games",
                    "#interpretability",
                    "#agents",
                    "#survey"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "WebWalker: умная навигация по веб-страницам для улучшения вопросно-ответных систем",
                    "desc": "В статье представлен новый подход к решению задач открытого вопросно-ответного поиска - WebWalkerQA. Эта система оценивает способность языковых моделей систематически исследовать подстраницы веб-сайтов для извлечения качественной информации. Авторы предлагают фреймворк WebWalker, использующий мультиагентный подход для имитации человеческой навигации по веб-страницам. Экспериментальные результаты демонстрируют эффективность комбинации RAG и WebWalker в реальных сценариях."
                },
                "en": {
                    "title": "Enhancing LLMs with Human-like Web Navigation for Better Information Retrieval",
                    "desc": "This paper introduces WebWalkerQA, a benchmark for evaluating large language models (LLMs) in open-domain question-answering tasks. It addresses the limitations of traditional search engines that often retrieve superficial content, which hinders LLMs from accessing complex information. The proposed WebWalker framework uses a multi-agent system that simulates human-like web navigation, allowing LLMs to systematically traverse subpages of a website to gather high-quality data. Experimental results indicate that combining retrieval-augmented generation (RAG) with WebWalker enhances the models' performance in real-world scenarios by enabling deeper information extraction."
                },
                "zh": {
                    "title": "WebWalkerQA：提升问答系统的网页导航能力",
                    "desc": "检索增强生成（RAG）在开放领域问答任务中表现出色，但传统搜索引擎可能只检索到表面内容，限制了大型语言模型（LLMs）处理复杂信息的能力。为了解决这个问题，我们引入了WebWalkerQA，这是一个评估LLMs进行网页遍历能力的基准。它评估LLMs系统性地遍历网站子页面以提取高质量数据的能力。我们提出了WebWalker，这是一个多代理框架，通过探索-评估范式模拟人类的网页导航。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06458",
            "title": "O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning",
            "url": "https://huggingface.co/papers/2501.06458",
            "abstract": "Building upon our previous investigations of O1 replication (Part 1: Journey Learning [Qin et al., 2024] and Part 2: Distillation [Huang et al., 2024]), this work explores the potential of inference-time scaling in large language models (LLMs) for medical reasoning tasks, ranging from diagnostic decision-making to treatment planning. Through extensive experiments on medical benchmarks of varying complexity (MedQA, Medbullets, and JAMA Clinical Challenges), our investigation reveals several key insights: (1) Increasing inference time does lead to improved performance. With a modest training set of 500 samples, our model yields substantial performance improvements of 6%-11%. (2) Task complexity directly correlates with the required length of reasoning chains, confirming the necessity of extended thought processes for challenging problems. (3) The differential diagnoses generated by our model adhere to the principles of the hypothetico-deductive method, producing a list of potential conditions that may explain a patient's symptoms and systematically narrowing these possibilities by evaluating the evidence. These findings demonstrate the promising synergy between inference-time scaling and journey learning in advancing LLMs' real-world clinical reasoning capabilities.",
            "score": 14,
            "issue_id": 1651,
            "pub_date": "2025-01-11",
            "pub_date_card": {
                "ru": "11 января",
                "en": "January 11",
                "zh": "1月11日"
            },
            "hash": "c95817afd181bd85",
            "authors": [
                "Zhongzhen Huang",
                "Gui Geng",
                "Shengyi Hua",
                "Zhen Huang",
                "Haoyang Zou",
                "Shaoting Zhang",
                "Pengfei Liu",
                "Xiaofan Zhang"
            ],
            "affiliations": [
                "Generative AI Research Lab (GAIR)",
                "SII",
                "SPIRAL Lab",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06458.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#inference",
                    "#healthcare",
                    "#reasoning"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Масштабирование времени вывода LLM улучшает медицинские рассуждения",
                    "desc": "Данная работа исследует потенциал масштабирования времени вывода в больших языковых моделях (LLM) для задач медицинского рассуждения. Эксперименты на медицинских бенчмарках показали, что увеличение времени вывода приводит к улучшению производительности модели. Сложность задачи напрямую коррелирует с необходимой длиной цепочек рассуждений. Дифференциальные диагнозы, генерируемые моделью, соответствуют принципам гипотетико-дедуктивного метода."
                },
                "en": {
                    "title": "Enhancing Medical Reasoning in LLMs through Inference-Time Scaling",
                    "desc": "This paper investigates how increasing inference time can enhance the performance of large language models (LLMs) in medical reasoning tasks. The authors conducted experiments on various medical benchmarks and found that longer inference times lead to significant performance improvements, even with a small training dataset. They also discovered that more complex tasks require longer reasoning chains, highlighting the importance of extended thought processes. Additionally, the model's differential diagnoses align with the hypothetico-deductive method, showcasing its ability to systematically evaluate potential conditions based on patient symptoms."
                },
                "zh": {
                    "title": "推理时间扩展助力医学推理能力提升",
                    "desc": "本研究基于我们之前对O1复制的研究，探讨了在大型语言模型（LLMs）中推理时间扩展对医学推理任务的潜力。通过在不同复杂度的医学基准（如MedQA、Medbullets和JAMA临床挑战）上进行广泛实验，我们发现增加推理时间确实能提高模型性能，尤其是在仅有500个样本的训练集上，性能提升可达6%-11%。此外，任务的复杂性与所需推理链的长度直接相关，表明对于复杂问题需要更长的思考过程。最后，我们的模型生成的差异性诊断遵循假设演绎法的原则，系统地评估证据以缩小可能的病症范围。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06282",
            "title": "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction",
            "url": "https://huggingface.co/papers/2501.06282",
            "abstract": "Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations. Previous models for voice interactions are categorized as native and aligned. Native models integrate speech and text processing in one framework but struggle with issues like differing sequence lengths and insufficient pre-training. Aligned models maintain text LLM capabilities but are often limited by small datasets and a narrow focus on speech tasks. In this work, we introduce MinMo, a Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction. We address the main limitations of prior aligned multimodal models. We train MinMo through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and a broad range of speech tasks. After the multi-stage training, MinMo achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system. Moreover, we propose a novel and simple voice decoder that outperforms prior models in voice generation. The enhanced instruction-following capabilities of MinMo supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices. For MinMo, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice. The MinMo project web page is https://funaudiollm.github.io/minmo, and the code and models will be released soon.",
            "score": 13,
            "issue_id": 1651,
            "pub_date": "2025-01-10",
            "pub_date_card": {
                "ru": "10 января",
                "en": "January 10",
                "zh": "1月10日"
            },
            "hash": "2bd352453760208e",
            "authors": [
                "Qian Chen",
                "Yafeng Chen",
                "Yanni Chen",
                "Mengzhe Chen",
                "Yingda Chen",
                "Chong Deng",
                "Zhihao Du",
                "Ruize Gao",
                "Changfeng Gao",
                "Zhifu Gao",
                "Yabin Li",
                "Xiang Lv",
                "Jiaqing Liu",
                "Haoneng Luo",
                "Bin Ma",
                "Chongjia Ni",
                "Xian Shi",
                "Jialong Tang",
                "Hui Wang",
                "Hao Wang",
                "Wen Wang",
                "Yuxuan Wang",
                "Yunlan Xu",
                "Fan Yu",
                "Zhijie Yan",
                "Yexin Yang",
                "Baosong Yang",
                "Xian Yang",
                "Guanrou Yang",
                "Tianyu Zhao",
                "Qinglin Zhang",
                "Shiliang Zhang",
                "Nan Zhao",
                "Pei Zhang",
                "Chong Zhang",
                "Jinren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06282.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "MinMo: революция в голосовом ИИ-взаимодействии",
                    "desc": "Статья представляет MinMo - мультимодальную большую языковую модель для беспрепятственного голосового взаимодействия. Модель обучена на 1,4 миллионах часов разнообразных речевых данных и широком спектре речевых задач через несколько этапов выравнивания речи и текста. MinMo достигает передовых результатов в понимании и генерации речи, сохраняя при этом возможности текстовых ЯБМ. Модель также поддерживает полнодуплексное общение и управляемую генерацию речи с различными нюансами, включая эмоции, диалекты и темп речи."
                },
                "en": {
                    "title": "MinMo: Revolutionizing Voice Interactions with Multimodal Learning",
                    "desc": "This paper presents MinMo, a Multimodal Large Language Model designed for seamless voice interactions, featuring around 8 billion parameters. It overcomes limitations of previous aligned models by employing a multi-stage training approach that includes speech-to-text, text-to-speech, and duplex interaction alignments, utilizing a vast dataset of 1.4 million hours of diverse speech. MinMo achieves state-of-the-art performance in voice comprehension and generation, enabling full-duplex conversations and enhanced instruction-following capabilities for nuanced speech generation. Additionally, it introduces a novel voice decoder that significantly improves voice generation quality compared to earlier models."
                },
                "zh": {
                    "title": "MinMo：无缝语音交互的新突破",
                    "desc": "本文介绍了一种名为MinMo的多模态大型语言模型，旨在实现无缝的语音交互。MinMo具有约80亿个参数，通过多阶段的对齐训练，克服了以往模型在语音理解和生成方面的局限性。该模型能够支持全双工对话，允许用户与系统进行实时的双向交流。MinMo还具备根据用户指令生成语音的能力，能够调整情感、方言和语速等细节。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06842",
            "title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training",
            "url": "https://huggingface.co/papers/2501.06842",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across diverse tasks, yet their training remains highly resource-intensive and susceptible to critical challenges such as training instability. A predominant source of this instability stems from gradient and loss spikes, which disrupt the learning process, often leading to costly interventions like checkpoint recovery and experiment restarts, further amplifying inefficiencies. This paper presents a comprehensive investigation into gradient spikes observed during LLM training, revealing their prevalence across multiple architectures and datasets. Our analysis shows that these spikes can be up to 1000times larger than typical gradients, substantially deteriorating model performance. To address this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a novel optimizer designed to counteract gradient spikes through momentum reset and spike-aware gradient clipping. Extensive experiments, including both pre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam and its variants across various tasks, including (1) LLM pre-training from 60M to 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time Series Forecasting. Additionally, SPAM facilitates memory-efficient training by enabling sparse momentum, where only a subset of momentum terms are maintained and updated. When operating under memory constraints, SPAM outperforms state-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our work underscores the importance of mitigating gradient spikes in LLM training and introduces an effective optimization strategy that enhances both training stability and resource efficiency at scale. Code is available at https://github.com/TianjinYellow/SPAM-Optimizer.git",
            "score": 10,
            "issue_id": 1658,
            "pub_date": "2025-01-12",
            "pub_date_card": {
                "ru": "12 января",
                "en": "January 12",
                "zh": "1月12日"
            },
            "hash": "d5fec659e34cf867",
            "authors": [
                "Tianjin Huang",
                "Ziquan Zhu",
                "Gaojie Jin",
                "Lu Liu",
                "Zhangyang Wang",
                "Shiwei Liu"
            ],
            "affiliations": [
                "Eindhoven University of Technology",
                "University of Exeter",
                "University of Leicester",
                "University of Oxford",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06842.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "SPAM: Стабильное и эффективное обучение языковых моделей",
                    "desc": "Исследователи представили новый оптимизатор SPAM (Spike-Aware Adam with Momentum Reset) для обучения больших языковых моделей (LLM). SPAM предназначен для решения проблемы резких скачков градиентов, которые могут быть в 1000 раз больше обычных и нарушают процесс обучения. Оптимизатор использует сброс импульса и адаптивное ограничение градиента для противодействия этим скачкам. Эксперименты показали, что SPAM превосходит Adam и его варианты в различных задачах, включая предобучение LLM, обучение с подкреплением и прогнозирование временных рядов."
                },
                "en": {
                    "title": "Taming Gradient Spikes for Stable LLM Training with SPAM",
                    "desc": "This paper investigates the issue of gradient spikes during the training of Large Language Models (LLMs), which can lead to instability and inefficiencies. These spikes can be significantly larger than normal gradients, negatively impacting model performance and requiring costly interventions. To combat this problem, the authors propose a new optimizer called Spike-Aware Adam with Momentum Reset (SPAM), which incorporates momentum reset and spike-aware gradient clipping. Experimental results show that SPAM outperforms traditional optimizers like Adam in various tasks while also being more memory-efficient."
                },
                "zh": {
                    "title": "应对梯度波动，提升训练稳定性！",
                    "desc": "大型语言模型（LLMs）在多种任务中表现出色，但其训练过程资源消耗大且容易出现不稳定性。研究发现，梯度和损失的剧烈波动是导致训练不稳定的主要原因，这会影响学习过程并增加干预成本。本文提出了一种新型优化器——Spike-Aware Adam with Momentum Reset（SPAM），旨在通过动量重置和梯度剪切来应对梯度波动。实验结果表明，SPAM在多种任务中均优于传统的Adam优化器，显著提高了训练的稳定性和资源效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07574",
            "title": "UnCommon Objects in 3D",
            "url": "https://huggingface.co/papers/2501.07574",
            "abstract": "We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for 3D deep learning and 3D generative AI. uCO3D is the largest publicly-available collection of high-resolution videos of objects with 3D annotations that ensures full-360^{circ} coverage. uCO3D is significantly more diverse than MVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of higher quality, due to extensive quality checks of both the collected videos and the 3D annotations. Similar to analogous datasets, uCO3D contains annotations for 3D camera poses, depth maps and sparse point clouds. In addition, each object is equipped with a caption and a 3D Gaussian Splat reconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D and obtain superior results using the latter, showing that uCO3D is better for learning applications.",
            "score": 7,
            "issue_id": 1651,
            "pub_date": "2025-01-13",
            "pub_date_card": {
                "ru": "13 января",
                "en": "January 13",
                "zh": "1月13日"
            },
            "hash": "79c40f6997052ddd",
            "authors": [
                "Xingchen Liu",
                "Piyush Tayal",
                "Jianyuan Wang",
                "Jesus Zarzar",
                "Tom Monnier",
                "Konstantinos Tertikas",
                "Jiali Duan",
                "Antoine Toisoul",
                "Jason Y. Zhang",
                "Natalia Neverova",
                "Andrea Vedaldi",
                "Roman Shapovalov",
                "David Novotny"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "KAUST",
                "Meta AI",
                "NKUA, Greece"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07574.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "uCO3D: Новый стандарт для 3D-данных в машинном обучении",
                    "desc": "Авторы представляют новый набор данных uCO3D для глубокого обучения и генеративного ИИ в 3D. Этот датасет содержит высококачественные видео объектов с полным 360-градусным охватом и 3D-аннотациями. uCO3D превосходит аналоги по разнообразию, охватывая более 1000 категорий объектов, и качеству благодаря тщательным проверкам. Помимо стандартных аннотаций, датасет включает подписи к объектам и 3D-реконструкции на основе гауссовых сплатов."
                },
                "en": {
                    "title": "Unlocking 3D Learning with uCO3D: A New Era of Object-Centric Datasets",
                    "desc": "The paper presents Uncommon Objects in 3D (uCO3D), a comprehensive dataset designed for advancing 3D deep learning and generative AI. This dataset features high-resolution videos with full 360-degree coverage and includes over 1,000 diverse object categories, making it larger and more varied than existing datasets like MVImgNet and CO3Dv2. uCO3D provides detailed annotations such as 3D camera poses, depth maps, and sparse point clouds, along with captions and 3D Gaussian Splat reconstructions for each object. Experiments demonstrate that training large 3D models on uCO3D yields superior performance compared to other datasets, highlighting its effectiveness for learning applications."
                },
                "zh": {
                    "title": "uCO3D：提升3D学习的全新数据集",
                    "desc": "我们介绍了一个新的3D深度学习和生成AI数据集，名为Uncommon Objects in 3D（uCO3D）。uCO3D是一个公开可用的高分辨率视频集合，包含360度的3D注释，涵盖超过1000个物体类别，具有更高的多样性和质量。该数据集提供了3D相机姿态、深度图和稀疏点云的注释，并为每个物体配备了描述和3D高斯点云重建。通过在多个数据集上训练大型3D模型，我们发现uCO3D在学习应用中表现更优。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07171",
            "title": "BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature",
            "url": "https://huggingface.co/papers/2501.07171",
            "abstract": "The development of vision-language models (VLMs) is driven by large-scale and diverse multimodal datasets. However, progress toward generalist biomedical VLMs is limited by the lack of annotated, publicly accessible datasets across biology and medicine. Existing efforts are restricted to narrow domains, missing the full diversity of biomedical knowledge encoded in scientific literature. To address this gap, we introduce BIOMEDICA, a scalable, open-source framework to extract, annotate, and serialize the entirety of the PubMed Central Open Access subset into an easy-to-use, publicly accessible dataset.Our framework produces a comprehensive archive with over 24 million unique image-text pairs from over 6 million articles. Metadata and expert-guided annotations are also provided. We demonstrate the utility and accessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style models continuously pre-trained on the BIOMEDICA dataset via streaming, eliminating the need to download 27 TB of data locally.On average, our models achieve state-of-the-art performance across 40 tasks - spanning pathology, radiology, ophthalmology, dermatology, surgery, molecular biology, parasitology, and cell biology - excelling in zero-shot classification with a 6.56% average improvement (as high as 29.8% and 17.5% in dermatology and ophthalmology, respectively), and stronger image-text retrieval, all while using 10x less compute. To foster reproducibility and collaboration, we release our codebase and dataset for the broader research community.",
            "score": 3,
            "issue_id": 1656,
            "pub_date": "2025-01-13",
            "pub_date_card": {
                "ru": "13 января",
                "en": "January 13",
                "zh": "1月13日"
            },
            "hash": "07db2230e08b0fde",
            "authors": [
                "Alejandro Lozano",
                "Min Woo Sun",
                "James Burgess",
                "Liangyu Chen",
                "Jeffrey J Nirschl",
                "Jeffrey Gu",
                "Ivan Lopez",
                "Josiah Aklilu",
                "Austin Wolfgang Katzer",
                "Collin Chiu",
                "Anita Rau",
                "Xiaohan Wang",
                "Yuhui Zhang",
                "Alfred Seunghoon Song",
                "Robert Tibshirani",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Department of Biomedical Data Science, Stanford University",
                "Department of Computer Science, Stanford University",
                "Department of Electrical Engineering, Stanford University",
                "Department of Pathology, Stanford University",
                "Department of Statistics, Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07171.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#cv",
                    "#dataset",
                    "#science",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "BIOMEDICA: Прорыв в обработке биомедицинских данных с помощью ИИ",
                    "desc": "Статья представляет BIOMEDICA - масштабируемый фреймворк с открытым исходным кодом для извлечения и аннотирования биомедицинских данных из научной литературы. Фреймворк создал обширный архив из более чем 24 миллионов уникальных пар изображение-текст из более 6 миллионов статей. На основе этого датасета были обучены модели BMCA-CLIP, достигшие state-of-the-art результатов в 40 биомедицинских задачах. Модели показали значительное улучшение в zero-shot классификации и поиске изображений по тексту при использовании в 10 раз меньших вычислительных ресурсов."
                },
                "en": {
                    "title": "Unlocking Biomedical Knowledge with BIOMEDICA",
                    "desc": "This paper presents BIOMEDICA, a new framework designed to create a large, open-source dataset from the PubMed Central Open Access subset, which includes over 24 million image-text pairs from scientific articles. The framework addresses the challenge of limited annotated datasets in the biomedical field, enabling the development of generalist vision-language models (VLMs) that can understand diverse biomedical knowledge. The authors also introduce BMCA-CLIP, a set of models that are continuously pre-trained on this dataset, achieving state-of-the-art performance across various medical tasks with significant improvements in zero-shot classification and image-text retrieval. By making their codebase and dataset publicly available, they aim to enhance reproducibility and collaboration in biomedical research."
                },
                "zh": {
                    "title": "推动生物医学领域的视觉语言模型发展",
                    "desc": "本文介绍了BIOMEDICA，一个可扩展的开源框架，用于提取、注释和序列化PubMed Central开放获取子集的全部内容。该框架生成了一个包含超过2400万个独特图像-文本对的综合档案，来自超过600万篇文章。我们还提供了元数据和专家指导的注释，并展示了BMCA-CLIP模型在40个医学任务中的优越性能，尤其在零样本分类和图像-文本检索方面表现突出。通过发布代码库和数据集，我们促进了研究的可重复性和合作。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06590",
            "title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
            "url": "https://huggingface.co/papers/2501.06590",
            "abstract": "Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science. Our code can be found at https://github.com/gersteinlab/chemagent",
            "score": 3,
            "issue_id": 1651,
            "pub_date": "2025-01-11",
            "pub_date_card": {
                "ru": "11 января",
                "en": "January 11",
                "zh": "1月11日"
            },
            "hash": "c217e826245ef357",
            "authors": [
                "Xiangru Tang",
                "Tianyu Hu",
                "Muyang Ye",
                "Yanjun Shao",
                "Xunjian Yin",
                "Siru Ouyang",
                "Wangchunshu Zhou",
                "Pan Lu",
                "Zhuosheng Zhang",
                "Yilun Zhao",
                "Arman Cohan",
                "Mark Gerstein"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Stanford University",
                "UIUC",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06590.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#multimodal",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "ChemAgent: Умный помощник для LLM в химических задачах",
                    "desc": "ChemAgent - это новая система, улучшающая работу больших языковых моделей (LLM) в задачах химического рассуждения. Она использует динамически обновляемую библиотеку, созданную путем декомпозиции химических задач на подзадачи. При решении новых проблем ChemAgent извлекает и уточняет релевантную информацию из библиотеки, что позволяет эффективно декомпозировать задачи и генерировать решения. Система показала значительное превосходство над существующими методами, улучшив производительность LLM до 46% на четырех наборах данных по химическому рассуждению."
                },
                "en": {
                    "title": "Empowering LLMs for Chemical Reasoning with ChemAgent",
                    "desc": "This paper introduces ChemAgent, a new framework that enhances large language models (LLMs) for chemical reasoning tasks. It addresses the challenges LLMs face with complex chemical calculations and domain-specific formulas by creating a dynamic library of decomposed sub-tasks. ChemAgent retrieves and refines relevant information from this library, allowing for better task decomposition and solution generation. Experimental results show that ChemAgent significantly improves performance on chemical reasoning datasets, indicating its potential for applications in drug discovery and materials science."
                },
                "zh": {
                    "title": "ChemAgent：提升化学推理的智能助手",
                    "desc": "化学推理通常涉及复杂的多步骤过程，需要精确的计算，哪怕是微小的错误也可能导致严重的后果。大型语言模型（LLMs）在处理特定领域的公式、准确执行推理步骤和有效整合代码时面临困难。为了解决这些问题，我们提出了ChemAgent，一个通过动态自更新库来提升LLMs性能的新框架。该框架通过将化学任务分解为子任务，并将这些子任务编译成结构化的集合，以便在未来查询时参考，从而实现有效的任务分解和解决方案生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06708",
            "title": "Evaluating Sample Utility for Data Selection by Mimicking Model Weights",
            "url": "https://huggingface.co/papers/2501.06708",
            "abstract": "Foundation models rely on large-scale web-crawled datasets, which frequently contain noisy data, biases, and irrelevant content. Existing data selection techniques typically use human heuristics, downstream evaluation datasets, or specialized scoring models, and can overlook samples' utility in the training process. Instead, we propose a new approach, Mimic Score, a data quality metric that uses a pretrained reference model as a guide to assess the usefulness of data samples for training a new model. It relies on the alignment between the gradient of the new model parameters and the vector pointing toward the reference model in weight space. Samples that misalign with this direction are considered low-value and can be filtered out. Motivated by the Mimic score, we develop Grad-Mimic, a data selection framework that identifies and prioritizes useful samples, automating the selection process to create effective filters. Empirically, using Mimic scores to guide model training results in consistent performance gains across six image datasets and enhances the performance of CLIP models. Moreover, Mimic scores and their associated filters improve upon existing filtering methods and offer accurate estimation of dataset quality.",
            "score": 2,
            "issue_id": 1661,
            "pub_date": "2025-01-12",
            "pub_date_card": {
                "ru": "12 января",
                "en": "January 12",
                "zh": "1月12日"
            },
            "hash": "7560c17a0e1b7234",
            "authors": [
                "Tzu-Heng Huang",
                "Manjot Bilkhu",
                "Frederic Sala",
                "Javier Movellan"
            ],
            "affiliations": [
                "Apple Inc.",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06708.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#ethics",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умный отбор данных для эффективного обучения моделей",
                    "desc": "Предложен новый подход к оценке качества данных для обучения моделей машинного обучения - Mimic Score. Этот метод использует предобученную эталонную модель для оценки полезности образцов данных, анализируя выравнивание градиента параметров новой модели с вектором, указывающим на эталонную модель в пространстве весов. На основе Mimic Score разработан фреймворк Grad-Mimic для автоматизированного отбора полезных образцов данных. Эксперименты показали, что использование Mimic Score приводит к улучшению производительности моделей на нескольких наборах данных изображений и моделей CLIP."
                },
                "en": {
                    "title": "Enhancing Data Selection with Mimic Score for Better Model Training",
                    "desc": "This paper introduces a new method called Mimic Score to improve data selection for training foundation models. It uses a pretrained reference model to evaluate the usefulness of data samples by analyzing the alignment of gradients in weight space. Samples that do not align well with the reference model are deemed low-value and can be removed from the training dataset. The proposed Grad-Mimic framework automates this selection process, leading to better model performance across various image datasets and outperforming existing data filtering techniques."
                },
                "zh": {
                    "title": "Mimic Score：提升数据选择的新方法",
                    "desc": "基础模型依赖于大规模的网络爬取数据集，这些数据集常常包含噪声数据、偏见和无关内容。现有的数据选择技术通常使用人工启发式方法、下游评估数据集或专门的评分模型，可能会忽视样本在训练过程中的实用性。我们提出了一种新的方法，称为Mimic Score，这是一种数据质量指标，利用预训练的参考模型来评估数据样本对新模型训练的有用性。基于Mimic Score，我们开发了Grad-Mimic数据选择框架，自动识别和优先选择有用样本，从而提高模型训练的效果。"
                }
            }
        }
    ],
    "link_prev": "2025-01-13.html",
    "link_next": "2025-01-15.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "13.01",
        "en": "01/13",
        "zh": "1月13日"
    },
    "short_date_next": {
        "ru": "15.01",
        "en": "01/15",
        "zh": "1月15日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 2,
        "#training": 6,
        "#robotics": 0,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的方法，叫做过程奖励模型（PRMs），用于大型语言模型（LLMs）在数学推理中的过程监督。目标是识别和减少推理过程中的错误。研究发现，常用的蒙特卡罗（MC）估计方法效果不佳，因为它依赖完成模型评估当前步骤的正确性，导致步骤验证不准确。文章还指出了传统Best-of-N（BoN）评估策略的偏差，并提出了一种共识过滤机制，结合MC估计和LLM-as-a-judge，改进了模型性能和数据效率。最后，文章发布了一个新的最先进的PRM，并提供了未来研究的实用指南。",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
        "pinyin": "这篇文章介绍了一种新的方法，叫做过程奖励模型（PRMs），用于大型语言模型（LLMs）在数学推理中的过程监督。目标是识别和减少推理过程中的错误。研究发现，常用的蒙特卡罗（MC）估计方法效果不佳，因为它依赖完成模型评估当前步骤的正确性，导致步骤验证不准确。文章还指出了传统Best-of-N（BoN）评估策略的偏差，并提出了一种共识过滤机制，结合MC估计和LLM-as-a-judge，改进了模型性能和数据效率。最后，文章发布了一个新的最先进的PRM，并提供了未来研究的实用指南。\n\nzhè piān wénzhāng jièshào le yī zhǒng xīn de fāngfǎ, jiàozuò guòchéng jiǎnglì móxíng (PRMs), yòngyú dàxíng yǔyán móxíng (LLMs) zài shùxué tuīlǐ zhōng de guòchéng jiàndū. Mùbiāo shì shíbié hé jiǎnshǎo tuīlǐ guòchéng zhōng de cuòwù. Yánjiū fāxiàn, chángyòng de méngtèkǎluó (MC) gūjì fāngfǎ xiàojià, yīnwèi tā yīlài wánchéng móxíng píngjià dāngqián bùzhòu de zhèngquèxìng, dǎozhì bùzhòu yànzhèng bù zhǔnquè. Wénzhāng hái zhǐchū le chuántǒng Best-of-N (BoN) píngjià cèlüè de piānchā, bìng tíchū le yī zhǒng gòngshì guòlǜ jīzhì, jiéhé MC gūjì hé LLM-as-a-judge, gǎijìn le móxíng xìngnéng hé shùjù xiàoyòng. Zuìhòu, wénzhāng fābù le yīgè xīn de zuì xiānjìn de PRM, bìng tígōng le wèilái yánjiū de shíyòng zhǐnán.",
        "vocab": "[\n    {\"word\": \"过程奖励模型\", \"pinyin\": \"guòchéng jiǎnglì móxíng\", \"trans\": \"Process Reward Model\"},\n    {\"word\": \"大型语言模型\", \"pinyin\": \"dàxíng yǔyán móxíng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"数学推理\", \"pinyin\": \"shùxué tuīlǐ\", \"trans\": \"Mathematical Reasoning\"},\n    {\"word\": \"过程监督\", \"pinyin\": \"guòchéng jiàndū\", \"trans\": \"Process Supervision\"},\n    {\"word\": \"蒙特卡罗\", \"pinyin\": \"méngtèkǎluó\", \"trans\": \"Monte Carlo\"},\n    {\"word\": \"估计\", \"pinyin\": \"gūjì\", \"trans\": \"Estimation\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yīlài\", \"trans\": \"Depend\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"Evaluate\"},\n    {\"word\": \"步骤验证\", \"pinyin\": \"bùzhòu yànzhèng\", \"trans\": \"Step Verification\"},\n    {\"word\": \"偏差\", \"pinyin\": \"piānchā\", \"trans\": \"Bias\"},\n    {\"word\": \"共识过滤机制\", \"pinyin\": \"gòngshí guòlǜ jīzhì\", \"trans\": \"Consensus Filtering Mechanism\"},\n    {\"word\": \"LLM-as-a-judge\", \"pinyin\": \"LLM-as-a-judge\", \"trans\": \"LLM-as-a-judge\"},\n    {\"word\": \"最先进\", \"pinyin\": \"zuìxiānjìn\", \"trans\": \"State-of-the-art\"},\n    {\"word\": \"实用指南\", \"pinyin\": \"shíyòng zhǐnán\", \"trans\": \"Practical Guide\"}\n]",
        "trans": "This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research.",
        "update_ts": "2025-01-14 09:10"
    }
}