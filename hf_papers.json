{
    "date": {
        "ru": "28 ноября",
        "en": "November 28",
        "zh": "11月28日"
    },
    "time_utc": "2024-11-28 11:09",
    "weekday": 3,
    "issue_id": 836,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.17949",
            "title": "ROICtrl: Boosting Instance Control for Visual Generation",
            "url": "https://huggingface.co/papers/2411.17949",
            "abstract": "Natural language often struggles to accurately associate positional and attribute information with multiple instances, which limits current text-based visual generation models to simpler compositions featuring only a few dominant instances. To address this limitation, this work enhances diffusion models by introducing regional instance control, where each instance is governed by a bounding box paired with a free-form caption. Previous methods in this area typically rely on implicit position encoding or explicit attention masks to separate regions of interest (ROIs), resulting in either inaccurate coordinate injection or large computational overhead. Inspired by ROI-Align in object detection, we introduce a complementary operation called ROI-Unpool. Together, ROI-Align and ROI-Unpool enable explicit, efficient, and accurate ROI manipulation on high-resolution feature maps for visual generation. Building on ROI-Unpool, we propose ROICtrl, an adapter for pretrained diffusion models that enables precise regional instance control. ROICtrl is compatible with community-finetuned diffusion models, as well as with existing spatial-based add-ons (\\eg, ControlNet, T2I-Adapter) and embedding-based add-ons (\\eg, IP-Adapter, ED-LoRA), extending their applications to multi-instance generation. Experiments show that ROICtrl achieves superior performance in regional instance control while significantly reducing computational costs.",
            "score": 54,
            "issue_id": 827,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "55ffbda778f2f640",
            "authors": [
                "Yuchao Gu",
                "Yipin Zhou",
                "Yunfan Ye",
                "Yixin Nie",
                "Licheng Yu",
                "Pingchuan Ma",
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "GenAI, Meta",
                "MIT",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17949.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Точный контроль над областями в генеративных моделях изображений",
                    "desc": "Статья представляет новый метод улучшения диффузионных моделей для генерации изображений - ROICtrl. Он позволяет точно контролировать отдельные области изображения с помощью ограничивающих рамок и подписей. ROICtrl использует операции ROI-Align и ROI-Unpool для эффективной работы с высокоразрешающими картами признаков. Метод совместим с существующими дополнениями к диффузионным моделям и значительно снижает вычислительные затраты при генерации сложных многообъектных сцен."
                },
                "en": {
                    "title": "Enhancing Visual Generation with Precise Regional Control",
                    "desc": "This paper presents a new method to improve text-based visual generation models by allowing better control over multiple instances in images. It introduces a technique called ROICtrl, which uses bounding boxes and captions to manage regions of interest (ROIs) more effectively. The method combines ROI-Align and a new operation called ROI-Unpool to manipulate high-resolution feature maps accurately and efficiently. Experiments demonstrate that ROICtrl enhances performance in generating images with multiple instances while lowering computational costs."
                },
                "zh": {
                    "title": "区域实例控制，提升视觉生成精度",
                    "desc": "这篇论文提出了一种改进扩散模型的方法，通过引入区域实例控制来解决自然语言在多实例位置和属性信息关联上的不足。每个实例由一个边界框和一个自由形式的描述配对，从而实现更精确的控制。论文中介绍的ROI-Unpool操作与ROI-Align相结合，使得在高分辨率特征图上进行区域操作变得高效且准确。实验结果表明，ROICtrl在区域实例控制方面表现优越，同时显著降低了计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17188",
            "title": "Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment",
            "url": "https://huggingface.co/papers/2411.17188",
            "abstract": "Many real-world user queries (e.g. \"How do to make egg fried rice?\") could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook. Models designed to generate interleaved text and images face challenges in ensuring consistency within and across these modalities. To address these challenges, we present ISG, a comprehensive evaluation framework for interleaved text-and-image generation. ISG leverages a scene graph structure to capture relationships between text and image blocks, evaluating responses on four levels of granularity: holistic, structural, block-level, and image-specific. This multi-tiered evaluation allows for a nuanced assessment of consistency, coherence, and accuracy, and provides interpretable question-answer feedback. In conjunction with ISG, we introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8 categories and 21 subcategories. This benchmark dataset includes complex language-vision dependencies and golden answers to evaluate models effectively on vision-centric tasks such as style transfer, a challenging area for current models. Using ISG-Bench, we demonstrate that recent unified vision-language models perform poorly on generating interleaved content. While compositional approaches that combine separate language and image models show a 111% improvement over unified models at the holistic level, their performance remains suboptimal at both block and image levels. To facilitate future work, we develop ISG-Agent, a baseline agent employing a \"plan-execute-refine\" pipeline to invoke tools, achieving a 122% performance improvement.",
            "score": 16,
            "issue_id": 828,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "dac4ab78d1cc7e28",
            "authors": [
                "Dongping Chen",
                "Ruoxi Chen",
                "Shu Pu",
                "Zhaoyi Liu",
                "Yanru Wu",
                "Caixi Chen",
                "Benlin Liu",
                "Yue Huang",
                "Yao Wan",
                "Pan Zhou",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "HUST",
                "University of Notre Dame",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17188.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#agents",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "ISG: Новый подход к оценке генерации мультимодального контента",
                    "desc": "Статья представляет ISG - комплексную систему оценки для генерации чередующегося текста и изображений. ISG использует структуру графа сцены для оценки согласованности и точности на четырех уровнях детализации. Авторы также вводят набор данных ISG-Bench для эффективной оценки моделей в задачах, связанных со зрением. Исследование показывает, что современные унифицированные модели плохо справляются с генерацией чередующегося контента, а композиционные подходы, хотя и лучше, все еще далеки от оптимальных результатов."
                },
                "en": {
                    "title": "Enhancing Text-Image Generation with ISG Framework",
                    "desc": "This paper introduces ISG, a framework designed to evaluate models that generate interleaved text and images, which is useful for tasks like creating step-by-step cooking guides. It uses a scene graph structure to analyze the relationships between text and images, providing a detailed assessment across different levels of granularity. The authors also present ISG-Bench, a benchmark dataset with 1,150 samples that helps evaluate models on complex language-vision tasks. The findings reveal that while compositional models outperform unified models significantly, there is still room for improvement, leading to the development of ISG-Agent, which enhances performance through a structured approach."
                },
                "zh": {
                    "title": "提升文本与图像生成一致性的评估框架",
                    "desc": "本文提出了一种名为ISG的评估框架，用于生成文本和图像交错的响应，旨在解决生成一致性的问题。ISG利用场景图结构捕捉文本与图像块之间的关系，并在整体、结构、块级和图像特定四个层面进行评估。通过ISG-Bench基准数据集，研究表明现有的统一视觉语言模型在生成交错内容方面表现不佳，而组合方法则在整体层面上有显著提升。最后，开发的ISG-Agent通过“计划-执行-优化”流程实现了更高的性能，推动了未来的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18613",
            "title": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models",
            "url": "https://huggingface.co/papers/2411.18613",
            "abstract": "We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. See our project page for results and interactive demos: cat-4d.github.io.",
            "score": 14,
            "issue_id": 829,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "736c7f565ac43a96",
            "authors": [
                "Rundi Wu",
                "Ruiqi Gao",
                "Ben Poole",
                "Alex Trevithick",
                "Changxi Zheng",
                "Jonathan T. Barron",
                "Aleksander Holynski"
            ],
            "affiliations": [
                "Columbia University",
                "Google DeepMind",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18613.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#optimization",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Превращение 2D-видео в динамичные 3D-сцены",
                    "desc": "CAT4D - это метод создания динамических 3D-сцен из монокулярного видео с использованием многоракурсной видео-диффузионной модели. Модель обучена на разнообразных наборах данных и позволяет синтезировать новые ракурсы для заданных положений камеры и временных меток. CAT4D преобразует одно монокулярное видео в многоракурсное, что делает возможной надежную 4D-реконструкцию путем оптимизации деформируемого 3D гауссового представления. Метод демонстрирует конкурентоспособные результаты в синтезе новых ракурсов и реконструкции динамических сцен, а также открывает возможности для генерации 4D-сцен из реальных или сгенерированных видео."
                },
                "en": {
                    "title": "Transforming Monocular Videos into Dynamic 4D Scenes",
                    "desc": "CAT4D is a new method that creates dynamic 3D scenes from a single video. It uses a multi-view video diffusion model, which is trained on various datasets, to generate new views from different camera angles and times. The approach includes a unique sampling technique that allows for the transformation of a single video into a multi-view format, facilitating the reconstruction of 4D scenes using a flexible 3D Gaussian model. The results show that CAT4D performs well in generating new views and reconstructing dynamic scenes, showcasing its potential for creative applications."
                },
                "zh": {
                    "title": "CAT4D：从单目视频生成动态3D场景的创新方法",
                    "desc": "CAT4D是一种从单目视频创建4D动态3D场景的方法。它利用多视角视频扩散模型，经过多种数据集的训练，能够在指定的相机位置和时间戳下进行新视角合成。通过一种新颖的采样方法，该模型可以将单个单目视频转换为多视角视频，从而通过优化可变形的3D高斯表示实现稳健的4D重建。我们在新视角合成和动态场景重建基准测试中展示了竞争力的性能，并强调了从真实或生成视频中生成4D场景的创造能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17945",
            "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
            "url": "https://huggingface.co/papers/2411.17945",
            "abstract": "Generating high-fidelity 3D content from text prompts remains a significant challenge in computer vision due to the limited size, diversity, and annotation depth of the existing datasets. To address this, we introduce MARVEL-40M+, an extensive dataset with 40 million text annotations for over 8.9 million 3D assets aggregated from seven major 3D datasets. Our contribution is a novel multi-stage annotation pipeline that integrates open-source pretrained multi-view VLMs and LLMs to automatically produce multi-level descriptions, ranging from detailed (150-200 words) to concise semantic tags (10-20 words). This structure supports both fine-grained 3D reconstruction and rapid prototyping. Furthermore, we incorporate human metadata from source datasets into our annotation pipeline to add domain-specific information in our annotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D, a two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our annotations and use a pretrained image-to-3D network to generate 3D textured meshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly outperforms existing datasets in annotation quality and linguistic diversity, achieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators.",
            "score": 11,
            "issue_id": 827,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "360ba2514eea161e",
            "authors": [
                "Sankalp Sinha",
                "Mohammad Sadil Khan",
                "Muhammad Usama",
                "Shino Sam",
                "Didier Stricker",
                "Sk Aziz Ali",
                "Muhammad Zeshan Afzal"
            ],
            "affiliations": [
                "BITS Pilani, Hyderabad",
                "DFKI",
                "MindGarage",
                "RPTU Kaiserslautern-Landau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17945.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#diffusion",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "🌟",
                "ru": {
                    "title": "Революция в 3D: от текста к реалистичным объектам",
                    "desc": "Статья представляет MARVEL-40M+, масштабный набор данных с 40 миллионами текстовых аннотаций для более чем 8,9 миллионов 3D-объектов. Авторы разработали многоэтапный конвейер аннотаций, использующий предобученные мультимодальные языковые модели и большие языковые модели для создания многоуровневых описаний. Набор данных значительно превосходит существующие по качеству аннотаций и лингвистическому разнообразию. Также представлен MARVEL-FX3D - двухэтапный конвейер для генерации 3D-контента из текстовых запросов."
                },
                "en": {
                    "title": "Unlocking 3D Creation with MARVEL-40M+: A New Era in Text-to-3D Generation",
                    "desc": "This paper presents MARVEL-40M+, a new dataset designed to improve the generation of 3D content from text prompts. It includes 40 million text annotations for 8.9 million 3D assets, created using a multi-stage annotation pipeline that leverages pretrained vision-language models (VLMs) and large language models (LLMs). The dataset enhances 3D reconstruction and prototyping by providing detailed and concise descriptions, while also incorporating human metadata to reduce inaccuracies in the generated annotations. Additionally, the authors introduce MARVEL-FX3D, a text-to-3D pipeline that efficiently generates 3D meshes, demonstrating superior annotation quality and diversity compared to existing datasets."
                },
                "zh": {
                    "title": "MARVEL-40M+: 高质量3D内容生成的新纪元",
                    "desc": "本文介绍了MARVEL-40M+数据集，该数据集包含4000万条文本注释，涵盖890万件3D资产，旨在解决现有数据集在规模和多样性上的不足。我们提出了一种新颖的多阶段注释流程，结合了开源的多视角视觉语言模型（VLM）和大型语言模型（LLM），自动生成多层次的描述。通过引入人类元数据，我们增强了注释的领域特定信息，减少了VLM的幻觉现象。此外，我们开发了MARVEL-FX3D，一个两阶段的文本到3D生成管道，能够快速生成高质量的3D网格。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18279",
            "title": "Large Language Model-Brained GUI Agents: A Survey",
            "url": "https://huggingface.co/papers/2411.18279",
            "abstract": "GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.   To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.",
            "score": 9,
            "issue_id": 831,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "5c5e1a276f559eb5",
            "authors": [
                "Chaoyun Zhang",
                "Shilin He",
                "Jiaxu Qian",
                "Bowen Li",
                "Liqun Li",
                "Si Qin",
                "Yu Kang",
                "Minghua Ma",
                "Qingwei Lin",
                "Saravan Rajmohan",
                "Dongmei Zhang",
                "Qi Zhang"
            ],
            "affiliations": [
                "Microsoft AI, Microsoft, China",
                "M365 Research, Microsoft, USA",
                "Shanghai Artificial Intelligence Laboratory, China",
                "Peking University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18279.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#dataset",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LLM-агенты революционизируют взаимодействие человека с компьютером через GUI",
                    "desc": "Статья представляет обзор агентов с графическим интерфейсом пользователя (GUI), основанных на больших языковых моделях (LLM). Эти агенты способны интерпретировать сложные элементы GUI и автономно выполнять действия на основе инструкций на естественном языке. В работе рассматривается историческая эволюция, основные компоненты и передовые методы LLM-агентов для GUI. Исследуются вопросы создания специализированных моделей действий для задач GUI, а также методы оценки эффективности таких агентов."
                },
                "en": {
                    "title": "Revolutionizing User Interaction with LLM-Brained GUI Agents",
                    "desc": "This paper surveys the development of LLM-brained GUI agents, which utilize large language models to automate interactions with graphical user interfaces. These agents can understand natural language commands and perform complex tasks across various platforms, enhancing user experience significantly. The study explores the historical context, core components, and advanced techniques involved in creating these agents, as well as the data requirements for training them. It also identifies research gaps and proposes a roadmap for future advancements in this rapidly evolving field."
                },
                "zh": {
                    "title": "LLM驱动的GUI代理：革新用户交互体验",
                    "desc": "本论文探讨了基于大型语言模型（LLM）的图形用户界面（GUI）代理的最新进展。这些代理能够理解自然语言指令，并自动执行复杂的多步骤任务，极大地提升了用户与软件的交互体验。论文回顾了这一领域的历史演变、核心组件和先进技术，并提出了未来研究的方向。通过对现有框架和评估标准的分析，本文旨在为研究人员和从业者提供指导，以充分发挥LLM驱动的GUI代理的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17787",
            "title": "Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient",
            "url": "https://huggingface.co/papers/2411.17787",
            "abstract": "In the rapidly advancing field of image generation, Visual Auto-Regressive (VAR) modeling has garnered considerable attention for its innovative next-scale prediction approach. This paradigm offers substantial improvements in efficiency, scalability, and zero-shot generalization. Yet, the inherently coarse-to-fine nature of VAR introduces a prolonged token sequence, leading to prohibitive memory consumption and computational redundancies. To address these bottlenecks, we propose Collaborative Decoding (CoDe), a novel efficient decoding strategy tailored for the VAR framework. CoDe capitalizes on two critical observations: the substantially reduced parameter demands at larger scales and the exclusive generation patterns across different scales. Based on these insights, we partition the multi-scale inference process into a seamless collaboration between a large model and a small model. The large model serves as the 'drafter', specializing in generating low-frequency content at smaller scales, while the smaller model serves as the 'refiner', solely focusing on predicting high-frequency details at larger scales. This collaboration yields remarkable efficiency with minimal impact on quality: CoDe achieves a 1.7x speedup, slashes memory usage by around 50%, and preserves image quality with only a negligible FID increase from 1.95 to 1.98. When drafting steps are further decreased, CoDe can achieve an impressive 2.9x acceleration ratio, reaching 41 images/s at 256x256 resolution on a single NVIDIA 4090 GPU, while preserving a commendable FID of 2.27. The code is available at https://github.com/czg1225/CoDe",
            "score": 9,
            "issue_id": 828,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "0ac2e8ea4bbd89ad",
            "authors": [
                "Zigeng Chen",
                "Xinyin Ma",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17787.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#optimization",
                    "#small_models",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Эффективное сотрудничество моделей для быстрой генерации изображений",
                    "desc": "Статья представляет новую стратегию декодирования для визуального авторегрессионного моделирования (VAR) под названием Collaborative Decoding (CoDe). CoDe разделяет процесс генерации изображения на две части: большая модель генерирует низкочастотный контент, а малая модель уточняет детали. Это позволяет значительно ускорить генерацию и снизить потребление памяти при минимальном влиянии на качество изображений. Авторы демонстрируют ускорение до 2.9 раз при сохранении хорошего значения метрики FID."
                },
                "en": {
                    "title": "Collaborative Decoding: Boosting Efficiency in Image Generation",
                    "desc": "This paper introduces Collaborative Decoding (CoDe), an efficient decoding strategy for Visual Auto-Regressive (VAR) models in image generation. CoDe optimizes the multi-scale inference process by utilizing a large model to generate low-frequency content and a smaller model to refine high-frequency details. This collaboration significantly reduces memory usage by about 50% and increases processing speed by up to 2.9 times, while maintaining image quality with only a slight increase in FID score. The proposed method demonstrates a promising approach to overcoming the computational challenges associated with VAR modeling."
                },
                "zh": {
                    "title": "协作解码：提升图像生成效率的新策略",
                    "desc": "在快速发展的图像生成领域，视觉自回归（VAR）建模因其创新的下一步预测方法而受到广泛关注。VAR的粗到细特性导致了较长的令牌序列，从而造成了高昂的内存消耗和计算冗余。为了解决这些瓶颈，我们提出了一种新的高效解码策略——协作解码（CoDe），它通过大模型和小模型的无缝协作来优化多尺度推理过程。CoDe在保持图像质量的同时，实现了1.7倍的加速和约50%的内存使用减少，展示了其在效率上的显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15139",
            "title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
            "url": "https://huggingface.co/papers/2411.15139",
            "abstract": "Recently, the diffusion model has emerged as a powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions. Leveraging its capability for end-to-end autonomous driving is a promising direction. However, the numerous denoising steps in the robotic diffusion policy and the more dynamic, open-world nature of traffic scenes pose substantial challenges for generating diverse driving actions at a real-time speed. To address these challenges, we propose a novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution. Additionally, we design an efficient cascade diffusion decoder for enhanced interaction with conditional scene context. The proposed model, DiffusionDrive, demonstrates 10times reduction in denoising steps compared to vanilla diffusion policy, delivering superior diversity and quality in just 2 steps. On the planning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone, DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new record, while running at a real-time speed of 45 FPS on an NVIDIA 4090. Qualitative results on challenging scenarios further confirm that DiffusionDrive can robustly generate diverse plausible driving actions. Code and model will be available at https://github.com/hustvl/DiffusionDrive.",
            "score": 8,
            "issue_id": 828,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "cef89e5425478782",
            "authors": [
                "Bencheng Liao",
                "Shaoyu Chen",
                "Haoran Yin",
                "Bo Jiang",
                "Cheng Wang",
                "Sixu Yan",
                "Xinbang Zhang",
                "Xiangyu Li",
                "Ying Zhang",
                "Qian Zhang",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Institute of Artificial Intelligence, Huazhong University of Science & Technology",
                "School of EIC, Huazhong University of Science & Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15139.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#robotics",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Ускоренная диффузионная модель для разнообразного и эффективного автономного вождения",
                    "desc": "Статья представляет новую модель DiffusionDrive для автономного вождения, основанную на усеченной диффузионной политике. Авторы предлагают использовать предварительные многомодовые якоря и сокращенное расписание диффузии для ускорения генерации разнообразных действий вождения. Модель включает каскадный диффузионный декодер для улучшенного взаимодействия с контекстом сцены. DiffusionDrive демонстрирует 10-кратное сокращение шагов шумоподавления по сравнению с обычной диффузионной политикой, обеспечивая высокое качество и разнообразие действий всего за 2 шага."
                },
                "en": {
                    "title": "Revolutionizing Robotic Driving with Efficient Diffusion Models",
                    "desc": "This paper introduces DiffusionDrive, a novel approach to robotic policy learning using diffusion models. It addresses the challenges of generating diverse driving actions in dynamic traffic environments by implementing a truncated diffusion policy that utilizes prior multi-mode anchors. The model significantly reduces the number of denoising steps required, achieving high-quality action generation in just 2 steps while maintaining real-time performance. Experimental results show that DiffusionDrive outperforms existing methods in both diversity and quality of driving actions, setting a new benchmark in the field."
                },
                "zh": {
                    "title": "扩散驱动：实时多样化驾驶策略的突破",
                    "desc": "最近，扩散模型作为一种强大的生成技术，在机器人策略学习中得到了广泛应用，能够建模多模态的动作分布。我们提出了一种新颖的截断扩散策略，结合了先前的多模态锚点，并缩短了扩散调度，从而使模型能够从锚定的高斯分布学习去噪声，生成多模态驾驶动作分布。该模型DiffusionDrive在去噪步骤上减少了10倍，能够在仅需2个步骤内提供更高的多样性和质量。实验结果表明，DiffusionDrive在实时速度下表现出色，能够稳健地生成多样化的合理驾驶动作。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17786",
            "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching",
            "url": "https://huggingface.co/papers/2411.17786",
            "abstract": "Personalized image generation requires text-to-image generative models that capture the core features of a reference subject to allow for controlled generation across different contexts. Existing methods face challenges due to complex training requirements, high inference costs, limited flexibility, or a combination of these issues. In this paper, we introduce DreamCache, a scalable approach for efficient and high-quality personalized image generation. By caching a small number of reference image features from a subset of layers and a single timestep of the pretrained diffusion denoiser, DreamCache enables dynamic modulation of the generated image features through lightweight, trained conditioning adapters. DreamCache achieves state-of-the-art image and text alignment, utilizing an order of magnitude fewer extra parameters, and is both more computationally effective and versatile than existing models.",
            "score": 7,
            "issue_id": 833,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "14d0175202b8957e",
            "authors": [
                "Emanuele Aiello",
                "Umberto Michieli",
                "Diego Valsesia",
                "Mete Ozay",
                "Enrico Magli"
            ],
            "affiliations": [
                "Politecnico di Torino",
                "Samsung R&D Institute UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17786.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#cv",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "DreamCache: эффективная персонализация генерации изображений",
                    "desc": "DreamCache - это новый подход к персонализированной генерации изображений, использующий кэширование признаков из подмножества слоев предобученной модели диффузии. Метод позволяет динамически модулировать генерируемые признаки изображения с помощью легких обучаемых адаптеров. DreamCache достигает наилучших результатов в соответствии изображений и текста, используя на порядок меньше дополнительных параметров. Он также более эффективен вычислительно и универсален по сравнению с существующими моделями."
                },
                "en": {
                    "title": "DreamCache: Efficient Personalized Image Generation Made Easy",
                    "desc": "This paper presents DreamCache, a novel method for personalized image generation using text-to-image generative models. DreamCache improves efficiency by caching features from reference images and utilizing lightweight conditioning adapters for dynamic modulation. It addresses the limitations of existing methods, such as high computational costs and inflexibility, while achieving superior image and text alignment. Overall, DreamCache offers a scalable solution that requires significantly fewer parameters and resources compared to traditional approaches."
                },
                "zh": {
                    "title": "DreamCache：高效个性化图像生成的新方法",
                    "desc": "个性化图像生成需要文本到图像的生成模型，这些模型能够捕捉参考对象的核心特征，以便在不同上下文中进行控制生成。现有方法面临复杂的训练要求、高推理成本和有限的灵活性等挑战。本文介绍了DreamCache，这是一种可扩展的方法，能够高效且高质量地生成个性化图像。通过缓存少量参考图像特征和预训练扩散去噪器的单个时间步，DreamCache实现了生成图像特征的动态调节，且所需额外参数显著减少，计算效率和灵活性均优于现有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17440",
            "title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition",
            "url": "https://huggingface.co/papers/2411.17440",
            "abstract": "Identity-preserving text-to-video (IPT2V) generation aims to create high-fidelity videos with consistent human identity. It is an important task in video generation but remains an open problem for generative models. This paper pushes the technical frontier of IPT2V in two directions that have not been resolved in literature: (1) A tuning-free pipeline without tedious case-by-case finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V model to keep human identity consistent in the generated video. Inspired by prior findings in frequency analysis of diffusion transformers, it employs identity-control signals in the frequency domain, where facial features can be decomposed into low-frequency global features and high-frequency intrinsic features. First, from a low-frequency perspective, we introduce a global facial extractor, which encodes reference images and facial key points into a latent space, generating features enriched with low-frequency information. These features are then integrated into shallow layers of the network to alleviate training challenges associated with DiT. Second, from a high-frequency perspective, we design a local facial extractor to capture high-frequency details and inject them into transformer blocks, enhancing the model's ability to preserve fine-grained features. We propose a hierarchical training strategy to leverage frequency information for identity preservation, transforming a vanilla pre-trained video generation model into an IPT2V model. Extensive experiments demonstrate that our frequency-aware heuristic scheme provides an optimal control solution for DiT-based models. Thanks to this scheme, our ConsisID generates high-quality, identity-preserving videos, making strides towards more effective IPT2V.",
            "score": 5,
            "issue_id": 827,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "28823ea0e7fa6b0c",
            "authors": [
                "Shenghai Yuan",
                "Jinfa Huang",
                "Xianyi He",
                "Yunyuan Ge",
                "Yujun Shi",
                "Liuhan Chen",
                "Jiebo Luo",
                "Li Yuan"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "Peng Cheng Laboratory",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17440.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Сохранение идентичности в видео через частотный анализ",
                    "desc": "Статья представляет ConsisID - модель для генерации видео с сохранением идентичности человека на основе диффузионных трансформеров. Авторы предлагают частотно-ориентированный подход, разделяя facial features на низкочастотные глобальные и высокочастотные локальные компоненты. Модель использует глобальный и локальный экстракторы лиц для сохранения идентичности в разных частотных диапазонах. Предложенная иерархическая стратегия обучения позволяет эффективно использовать частотную информацию для сохранения идентичности в генерируемом видео."
                },
                "en": {
                    "title": "ConsisID: Seamless Identity Preservation in Video Generation",
                    "desc": "This paper presents a novel approach to identity-preserving text-to-video (IPT2V) generation, focusing on maintaining consistent human identity in videos. The authors introduce ConsisID, a controllable model that operates without the need for extensive fine-tuning, simplifying the process of video generation. By utilizing a frequency-aware heuristic, the model effectively separates and processes low-frequency global features and high-frequency intrinsic details of facial characteristics. The proposed hierarchical training strategy enhances the model's performance, allowing it to generate high-fidelity videos while preserving individual identities throughout the video sequence."
                },
                "zh": {
                    "title": "高保真身份保持视频生成的突破",
                    "desc": "身份保持的文本到视频生成（IPT2V）旨在创建具有一致人类身份的高保真视频。本文提出了一种名为ConsisID的模型，能够在不需要繁琐微调的情况下，保持生成视频中的人类身份一致性。该模型利用频率分析的方法，将人脸特征分解为低频和高频信息，从而有效地提取和保留细致的面部特征。通过层次化的训练策略，ConsisID在视频生成任务中实现了更好的身份保持效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17769",
            "title": "Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis",
            "url": "https://huggingface.co/papers/2411.17769",
            "abstract": "In this work, we introduce a single parameter omega, to effectively control granularity in diffusion-based synthesis. This parameter is incorporated during the denoising steps of the diffusion model's reverse process. Our approach does not require model retraining, architectural modifications, or additional computational overhead during inference, yet enables precise control over the level of details in the generated outputs. Moreover, spatial masks or denoising schedules with varying omega values can be applied to achieve region-specific or timestep-specific granularity control. Prior knowledge of image composition from control signals or reference images further facilitates the creation of precise omega masks for granularity control on specific objects. To highlight the parameter's role in controlling subtle detail variations, the technique is named Omegance, combining \"omega\" and \"nuance\". Our method demonstrates impressive performance across various image and video synthesis tasks and is adaptable to advanced diffusion models. The code is available at https://github.com/itsmag11/Omegance.",
            "score": 4,
            "issue_id": 832,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "3fee8ae6759c9578",
            "authors": [
                "Xinyu Hou",
                "Zongsheng Yue",
                "Xiaoming Li",
                "Chen Change Loy"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17769.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#video",
                    "#inference"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Омега-параметр: тонкая настройка детализации в диффузионных моделях",
                    "desc": "Статья представляет новый параметр омега для контроля детализации в диффузионных моделях. Этот параметр внедряется в процесс денойзинга без необходимости переобучения модели или изменения архитектуры. Омега позволяет точно управлять уровнем деталей в генерируемых изображениях, в том числе для отдельных регионов или временных шагов. Метод, названный Omegance, демонстрирует впечатляющие результаты в различных задачах синтеза изображений и видео."
                },
                "en": {
                    "title": "Omegance: Precision Control in Diffusion Synthesis",
                    "desc": "This paper presents a new parameter called omega that allows for fine control over the detail level in diffusion-based image and video synthesis. By integrating omega into the denoising steps of the diffusion model's reverse process, the authors achieve granularity control without needing to retrain the model or change its architecture. The method supports the use of spatial masks and varying omega values to target specific regions or time steps, enhancing the precision of generated outputs. Named Omegance, this technique leverages prior knowledge from control signals or reference images to create tailored omega masks for detailed object synthesis."
                },
                "zh": {
                    "title": "通过Omegance实现细节控制的创新方法",
                    "desc": "本文提出了一个单一参数omega，用于有效控制基于扩散的合成中的细节粒度。该参数在扩散模型的去噪步骤中被引入，允许在不需要重新训练模型或修改架构的情况下，精确控制生成输出的细节水平。通过应用空间掩码或不同omega值的去噪调度，可以实现区域特定或时间步特定的粒度控制。此外，利用控制信号或参考图像的先验知识，可以创建精确的omega掩码，以便对特定对象进行粒度控制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16781",
            "title": "UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing",
            "url": "https://huggingface.co/papers/2411.16781",
            "abstract": "Human pose plays a crucial role in the digital age. While recent works have achieved impressive progress in understanding and generating human poses, they often support only a single modality of control signals and operate in isolation, limiting their application in real-world scenarios. This paper presents UniPose, a framework employing Large Language Models (LLMs) to comprehend, generate, and edit human poses across various modalities, including images, text, and 3D SMPL poses. Specifically, we apply a pose tokenizer to convert 3D poses into discrete pose tokens, enabling seamless integration into the LLM within a unified vocabulary. To further enhance the fine-grained pose perception capabilities, we facilitate UniPose with a mixture of visual encoders, among them a pose-specific visual encoder. Benefiting from a unified learning strategy, UniPose effectively transfers knowledge across different pose-relevant tasks, adapts to unseen tasks, and exhibits extended capabilities. This work serves as the first attempt at building a general-purpose framework for pose comprehension, generation, and editing. Extensive experiments highlight UniPose's competitive and even superior performance across various pose-relevant tasks.",
            "score": 4,
            "issue_id": 830,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "015e61d2dcffef60",
            "authors": [
                "Yiheng Li",
                "Ruibing Hou",
                "Hong Chang",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "affiliations": [
                "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China",
                "University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16781.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#transfer_learning",
                    "#3d",
                    "#cv"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "UniPose: универсальная система для работы с позами человека на основе LLM",
                    "desc": "UniPose - это новая система, использующая большие языковые модели (LLM) для понимания, генерации и редактирования поз человека в различных модальностях, включая изображения, текст и 3D-позы SMPL. Ключевой особенностью является применение токенизатора поз для преобразования 3D-поз в дискретные токены, что позволяет интегрировать их в LLM в рамках единого словаря. Система использует смесь визуальных энкодеров, включая специализированный энкодер для поз, что улучшает точность восприятия. UniPose демонстрирует конкурентоспособную и даже превосходящую производительность в различных задачах, связанных с позами."
                },
                "en": {
                    "title": "UniPose: Unifying Human Pose Understanding Across Modalities",
                    "desc": "This paper introduces UniPose, a novel framework that utilizes Large Language Models (LLMs) to understand, generate, and edit human poses using multiple modalities such as images, text, and 3D SMPL poses. By employing a pose tokenizer, UniPose converts 3D poses into discrete tokens, allowing for integration into the LLM with a unified vocabulary. The framework enhances pose perception through a mixture of visual encoders, including a specialized pose encoder, enabling it to adapt to various pose-related tasks. UniPose represents a significant advancement in creating a versatile system for pose manipulation, demonstrating superior performance in extensive experiments across different applications."
                },
                "zh": {
                    "title": "UniPose：多模态人类姿态理解与生成的统一框架",
                    "desc": "本文提出了UniPose框架，利用大型语言模型（LLMs）来理解、生成和编辑人类姿态。UniPose支持多种控制信号的模态，包括图像、文本和3D SMPL姿态，克服了以往方法的局限性。通过使用姿态标记器将3D姿态转换为离散的姿态标记，UniPose实现了与LLM的无缝集成。实验结果表明，UniPose在多种姿态相关任务中表现出色，展现了其通用性和适应性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18462",
            "title": "Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding",
            "url": "https://huggingface.co/papers/2411.18462",
            "abstract": "Speculative Decoding (SD) has become an important technique in accelerating the inference speed of large language models. Conventional SD methods employ a fixed draft length, which ignores the token generation difficulty across tasks. Consequently, in this paper, we address such an issue and introduce SVIP - a difficulty-aware dynamic draft length policy for speculative decoding systems. Based on a theoretical lower bound of draft token acceptance rate and its inference-time approximation, SVIP adaptively determines the lengths of draft sequences based on the entropy of each draft token distribution. Experimental results on mainstream SD benchmarks and frameworks demonstrate the superior performance of SVIP, achieving up to 20\\% walltime speedup on SpecBench over baseline SD methods and 60\\% speedup on MT-Bench for long-form generation of up to 8K tokens. Moreover, SVIP is totally training-free and compatible with any existing SD methods that generate draft tokens autoregressively. Experimental results also show that SVIP yields consistent walltime improvement on top of GliDe & CaPE and EAGLE-2.",
            "score": 4,
            "issue_id": 828,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "b9bc0d66b9a257c9",
            "authors": [
                "Ziyin Zhang",
                "Jiahao Xu",
                "Tian Liang",
                "Xingyu Chen",
                "Zhiwei He",
                "Rui Wang",
                "Zhaopeng Tu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18462.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Адаптивное спекулятивное декодирование для ускорения языковых моделей",
                    "desc": "Статья представляет SVIP - новый метод для ускорения вывода больших языковых моделей с помощью спекулятивного декодирования. SVIP адаптивно определяет длину черновых последовательностей токенов на основе энтропии распределения каждого чернового токена. Эксперименты показывают значительное ускорение по сравнению с базовыми методами спекулятивного декодирования - до 20% на SpecBench и до 60% на MT-Bench для генерации длинных текстов. SVIP не требует дополнительного обучения и совместим с существующими методами спекулятивного декодирования."
                },
                "en": {
                    "title": "Dynamic Draft Length for Faster Language Model Inference",
                    "desc": "This paper presents SVIP, a new method for speculative decoding (SD) that improves the efficiency of large language models by adapting the draft length based on task difficulty. Traditional SD approaches use a fixed draft length, which can be inefficient as it does not account for the varying complexity of token generation. SVIP utilizes the entropy of draft token distributions to dynamically adjust the length of draft sequences, leading to faster inference times. Experimental results indicate that SVIP can achieve significant speedups in processing time, making it a valuable enhancement for existing SD frameworks without requiring additional training."
                },
                "zh": {
                    "title": "动态草稿长度，提升推理速度！",
                    "desc": "本论文介绍了一种新的推测解码技术，称为SVIP，旨在提高大型语言模型的推理速度。传统的推测解码方法使用固定的草稿长度，未能考虑不同任务的生成难度。SVIP通过分析每个草稿令牌分布的熵，动态调整草稿序列的长度，从而提高效率。实验结果表明，SVIP在多个基准测试中表现优异，能够显著加快推理速度，且无需额外训练。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17991",
            "title": "VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format",
            "url": "https://huggingface.co/papers/2411.17991",
            "abstract": "Recent researches on video large language models (VideoLLM) predominantly focus on model architectures and training datasets, leaving the interaction format between the user and the model under-explored. In existing works, users often interact with VideoLLMs by using the entire video and a query as input, after which the model generates a response. This interaction format constrains the application of VideoLLMs in scenarios such as live-streaming comprehension where videos do not end and responses are required in a real-time manner, and also results in unsatisfactory performance on time-sensitive tasks that requires localizing video segments. In this paper, we focus on a video-text duet interaction format. This interaction format is characterized by the continuous playback of the video, and both the user and the model can insert their text messages at any position during the video playback. When a text message ends, the video continues to play, akin to the alternative of two performers in a duet. We construct MMDuetIT, a video-text training dataset designed to adapt VideoLLMs to video-text duet interaction format. We also introduce the Multi-Answer Grounded Video Question Answering (MAGQA) task to benchmark the real-time response ability of VideoLLMs. Trained on MMDuetIT, MMDuet demonstrates that adopting the video-text duet interaction format enables the model to achieve significant improvements in various time-sensitive tasks (76% CIDEr on YouCook2 dense video captioning, 90\\% mAP on QVHighlights highlight detection and 25% R@0.5 on Charades-STA temporal video grounding) with minimal training efforts, and also enable VideoLLMs to reply in a real-time manner as the video plays. Code, data and demo are available at: https://github.com/yellow-binary-tree/MMDuet.",
            "score": 3,
            "issue_id": 835,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "8c18e594b9993e25",
            "authors": [
                "Yueqian Wang",
                "Xiaojun Meng",
                "Yuxuan Wang",
                "Jianxin Liang",
                "Jiansheng Wei",
                "Huishuai Zhang",
                "Dongyan Zhao"
            ],
            "affiliations": [
                "Beijing Institute for General Artificial Intelligence",
                "Huawei Noahs Ark Lab",
                "National Key Laboratory of General Artificial Intelligence",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17991.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#games",
                    "#video"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Видео-текстовый дуэт: новый формат взаимодействия для VideoLLM",
                    "desc": "Эта статья представляет новый формат взаимодействия с видео-языковыми моделями (VideoLLM), названный 'видео-текстовым дуэтом'. В этом формате видео воспроизводится непрерывно, а пользователь и модель могут вставлять текстовые сообщения в любой момент во время воспроизведения. Авторы создали датасет MMDuetIT для обучения VideoLLM этому формату взаимодействия и представили задачу Multi-Answer Grounded Video Question Answering (MAGQA) для оценки способности моделей отвечать в реальном времени. Результаты показывают значительное улучшение производительности в различных задачах, чувствительных ко времени, и возможность моделей отвечать в режиме реального времени во время воспроизведения видео."
                },
                "en": {
                    "title": "Revolutionizing Video Interaction: The Duet Format for Real-Time Responses",
                    "desc": "This paper introduces a new interaction format for video large language models (VideoLLMs) called video-text duet interaction. Unlike traditional methods where users input a full video and a query, this format allows users and the model to exchange text messages while the video plays continuously. This approach is particularly beneficial for real-time applications, such as live-streaming, where immediate responses are necessary. The authors present a new dataset, MMDuetIT, and a benchmarking task, MAGQA, demonstrating that this interaction format significantly enhances the performance of VideoLLMs on time-sensitive tasks with minimal additional training."
                },
                "zh": {
                    "title": "视频文本二重奏：实时交互的新方式",
                    "desc": "本论文探讨了一种视频文本二重奏交互格式，以改善视频大型语言模型（VideoLLM）的用户交互体验。传统的交互方式要求用户在视频结束后输入查询，这限制了实时响应的能力。我们构建了MMDuetIT数据集，以适应这种新格式，并引入了多答案基础视频问答（MAGQA）任务来评估模型的实时响应能力。实验结果表明，采用视频文本二重奏交互格式后，模型在多个时间敏感任务上取得了显著提升，且能够在视频播放时实时回复。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18412",
            "title": "Adaptive Blind All-in-One Image Restoration",
            "url": "https://huggingface.co/papers/2411.18412",
            "abstract": "Blind all-in-one image restoration models aim to recover a high-quality image from an input degraded with unknown distortions. However, these models require all the possible degradation types to be defined during the training stage while showing limited generalization to unseen degradations, which limits their practical application in complex cases. In this paper, we propose a simple but effective adaptive blind all-in-one restoration (ABAIR) model, which can address multiple degradations, generalizes well to unseen degradations, and efficiently incorporate new degradations by training a small fraction of parameters. First, we train our baseline model on a large dataset of natural images with multiple synthetic degradations, augmented with a segmentation head to estimate per-pixel degradation types, resulting in a powerful backbone able to generalize to a wide range of degradations. Second, we adapt our baseline model to varying image restoration tasks using independent low-rank adapters. Third, we learn to adaptively combine adapters to versatile images via a flexible and lightweight degradation estimator. Our model is both powerful in handling specific distortions and flexible in adapting to complex tasks, it not only outperforms the state-of-the-art by a large margin on five- and three-task IR setups, but also shows improved generalization to unseen degradations and also composite distortions.",
            "score": 3,
            "issue_id": 835,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "eda508d3cf03675b",
            "authors": [
                "David Serrano-Lozano",
                "Luis Herranz",
                "Shaolin Su",
                "Javier Vazquez-Corral"
            ],
            "affiliations": [
                "Computer Vision Center",
                "Universidad Autonoma de Madrid",
                "Universitat Aut`onoma de Barcelona"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18412.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Адаптивное восстановление изображений с улучшенным обобщением",
                    "desc": "Статья представляет адаптивную модель ABAIR для всестороннего восстановления изображений. Модель способна обрабатывать множественные искажения, хорошо обобщается на новые типы деградаций и может эффективно включать новые виды искажений, обучая лишь небольшую часть параметров. ABAIR использует базовую модель, обученную на большом наборе данных с синтетическими искажениями, и адаптеры низкого ранга для настройки на конкретные задачи. Модель превосходит современные аналоги на стандартных наборах задач и демонстрирует улучшенное обобщение на новые типы искажений."
                },
                "en": {
                    "title": "Adaptive Restoration: Flexibility Meets Performance in Image Recovery",
                    "desc": "The paper introduces an Adaptive Blind All-in-One Restoration (ABAIR) model designed to improve image restoration from various unknown distortions. Unlike traditional models that require predefined degradation types, ABAIR can adapt to new and unseen distortions by training only a small number of parameters. It utilizes a segmentation head to identify per-pixel degradation types, enhancing its ability to generalize across different tasks. The model demonstrates superior performance compared to existing methods, particularly in handling complex and composite distortions."
                },
                "zh": {
                    "title": "自适应盲全能修复，灵活应对图像损伤",
                    "desc": "本文提出了一种自适应盲全能图像修复模型（ABAIR），旨在从受损图像中恢复高质量图像。与传统模型需要在训练阶段定义所有可能的损伤类型不同，ABAIR能够有效处理多种损伤，并且对未见过的损伤具有良好的泛化能力。该模型通过训练一个强大的基础模型，并使用独立的低秩适配器来适应不同的图像修复任务，从而实现灵活的损伤估计。实验结果表明，ABAIR在多个任务上显著超越了现有的最先进技术，并在处理复合损伤时表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18197",
            "title": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters",
            "url": "https://huggingface.co/papers/2411.18197",
            "abstract": "3D characters are essential to modern creative industries, but making them animatable often demands extensive manual work in tasks like rigging and skinning. Existing automatic rigging tools face several limitations, including the necessity for manual annotations, rigid skeleton topologies, and limited generalization across diverse shapes and poses. An alternative approach is to generate animatable avatars pre-bound to a rigged template mesh. However, this method often lacks flexibility and is typically limited to realistic human shapes. To address these issues, we present Make-It-Animatable, a novel data-driven method to make any 3D humanoid model ready for character animation in less than one second, regardless of its shapes and poses. Our unified framework generates high-quality blend weights, bones, and pose transformations. By incorporating a particle-based shape autoencoder, our approach supports various 3D representations, including meshes and 3D Gaussian splats. Additionally, we employ a coarse-to-fine representation and a structure-aware modeling strategy to ensure both accuracy and robustness, even for characters with non-standard skeleton structures. We conducted extensive experiments to validate our framework's effectiveness. Compared to existing methods, our approach demonstrates significant improvements in both quality and speed.",
            "score": 3,
            "issue_id": 832,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "f87c0e285ba351c9",
            "authors": [
                "Zhiyang Guo",
                "Jinxu Xiang",
                "Kai Ma",
                "Wengang Zhou",
                "Houqiang Li",
                "Ran Zhang"
            ],
            "affiliations": [
                "CAS Key Laboratory of Technology in GIPAS, EEIS Department, University of Science and Technology of China",
                "Tencent PCG"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18197.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "Мгновенная анимация для любого 3D-персонажа",
                    "desc": "Статья представляет новый метод под названием Make-It-Animatable для автоматической подготовки 3D-моделей гуманоидных персонажей к анимации. Метод использует нейронные сети для генерации весов смешивания, костей и трансформаций поз, работая с различными 3D-представлениями, включая полигональные сетки и сплаты гауссовых частиц. Подход применим к персонажам с нестандартной структурой скелета и значительно превосходит существующие методы по качеству и скорости. Make-It-Animatable позволяет подготовить модель к анимации менее чем за секунду, независимо от ее формы и позы."
                },
                "en": {
                    "title": "Instant Animation for Any 3D Character!",
                    "desc": "The paper introduces Make-It-Animatable, a new method for quickly preparing 3D humanoid models for animation. This approach overcomes limitations of existing rigging tools by eliminating the need for manual annotations and allowing for diverse shapes and poses. It utilizes a particle-based shape autoencoder to generate blend weights, bones, and pose transformations efficiently. The framework is designed to be accurate and robust, even for characters with unconventional skeletons, and shows significant improvements in quality and speed over previous methods."
                },
                "zh": {
                    "title": "快速生成可动画的3D角色模型",
                    "desc": "本论文提出了一种名为Make-It-Animatable的新方法，可以在不到一秒的时间内使任何3D人形模型准备好进行角色动画。该方法通过生成高质量的混合权重、骨骼和姿态变换，解决了现有自动绑定工具的局限性。我们的方法支持多种3D表示形式，包括网格和3D高斯点云，并采用了粗到细的表示和结构感知建模策略，以确保准确性和鲁棒性。实验结果表明，与现有方法相比，我们的方法在质量和速度上都有显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15872",
            "title": "Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024 SSA and Pediatrics",
            "url": "https://huggingface.co/papers/2411.15872",
            "abstract": "Identifying key pathological features in brain MRIs is crucial for the long-term survival of glioma patients. However, manual segmentation is time-consuming, requiring expert intervention and is susceptible to human error. Therefore, significant research has been devoted to developing machine learning methods that can accurately segment tumors in 3D multimodal brain MRI scans. Despite their progress, state-of-the-art models are often limited by the data they are trained on, raising concerns about their reliability when applied to diverse populations that may introduce distribution shifts. Such shifts can stem from lower quality MRI technology (e.g., in sub-Saharan Africa) or variations in patient demographics (e.g., children). The BraTS-2024 challenge provides a platform to address these issues. This study presents our methodology for segmenting tumors in the BraTS-2024 SSA and Pediatric Tumors tasks using MedNeXt, comprehensive model ensembling, and thorough postprocessing. Our approach demonstrated strong performance on the unseen validation set, achieving an average Dice Similarity Coefficient (DSC) of 0.896 on the BraTS-2024 SSA dataset and an average DSC of 0.830 on the BraTS Pediatric Tumor dataset. Additionally, our method achieved an average Hausdorff Distance (HD95) of 14.682 on the BraTS-2024 SSA dataset and an average HD95 of 37.508 on the BraTS Pediatric dataset. Our GitHub repository can be accessed here: Project Repository : https://github.com/python-arch/BioMbz-Optimizing-Brain-Tumor-Segmentation-with-MedNeXt-BraTS-2024-SSA-and-Pediatrics",
            "score": 3,
            "issue_id": 830,
            "pub_date": "2024-11-24",
            "pub_date_card": {
                "ru": "24 ноября",
                "en": "November 24",
                "zh": "11月24日"
            },
            "hash": "64cec64b56df2942",
            "authors": [
                "Sarim Hashmi",
                "Juan Lugo",
                "Abdelrahman Elsayed",
                "Dinesh Saggurthi",
                "Mohammed Elseiagy",
                "Alikhan Nurkamal",
                "Jaskaran Walia",
                "Fadillah Adamsyah Maani",
                "Mohammad Yaqub"
            ],
            "affiliations": [
                "Mohammed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, United Arab Emirates"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15872.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#healthcare",
                    "#optimization",
                    "#cv",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MedNeXt: точная сегментация опухолей мозга для разных групп пациентов",
                    "desc": "Статья описывает метод сегментации опухолей головного мозга на МРТ-снимках с использованием глубокого обучения. Авторы применяют модель MedNeXt, ансамблирование моделей и постобработку для решения задач BraTS-2024 SSA и Pediatric Tumors. Их подход показал высокую эффективность на валидационных данных, достигнув средних значений коэффициента Дайса 0,896 и 0,830 для соответствующих наборов данных. Исследование направлено на повышение надежности моделей при применении к разнородным популяциям пациентов."
                },
                "en": {
                    "title": "Enhancing Brain Tumor Segmentation with Advanced Machine Learning",
                    "desc": "This paper focuses on improving the segmentation of glioma tumors in brain MRIs using advanced machine learning techniques. The authors highlight the challenges of manual segmentation, which is slow and prone to errors, and propose a solution through model ensembling and postprocessing methods. They tested their approach on the BraTS-2024 challenge datasets, achieving high accuracy as measured by the Dice Similarity Coefficient and Hausdorff Distance metrics. The results indicate that their method is robust and effective, even when applied to diverse populations and varying MRI quality."
                },
                "zh": {
                    "title": "提升脑肿瘤MRI分割的机器学习方法",
                    "desc": "本研究旨在提高胶质瘤患者脑部MRI图像中肿瘤的自动分割精度。我们采用了MedNeXt模型和综合模型集成的方法，针对BraTS-2024挑战中的SSA和儿童肿瘤任务进行研究。通过对未见验证集的测试，我们在BraTS-2024 SSA数据集上达到了平均Dice相似系数0.896，在儿童肿瘤数据集上达到了0.830。我们的研究表明，机器学习方法在处理不同人群和设备质量的MRI数据时具有良好的适应性和可靠性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18363",
            "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
            "url": "https://huggingface.co/papers/2411.18363",
            "abstract": "Perception and understanding are two pillars of computer vision. While multimodal large language models (MLLM) have demonstrated remarkable visual understanding capabilities, they arguably lack accurate perception abilities, e.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on the COCO dataset, limiting many tasks requiring the combination of perception and understanding. In this work, we aim to bridge this perception gap from both model designing and data development perspectives. We first introduce ChatRex, an MLLM with a decoupled perception design. Instead of having the LLM directly predict box coordinates, we feed the output boxes from a universal proposal network into the LLM, allowing it to output the corresponding box indices to represent its detection results, turning the regression task into a retrieval-based task that LLM handles more proficiently. From the data perspective, we build a fully automated data engine and construct the Rexverse-2M dataset which possesses multiple granularities to support the joint training of perception and understanding. After standard two-stage training, ChatRex demonstrates strong perception capabilities while preserving multimodal understanding performance. The combination of these two capabilities simultaneously unlocks many attractive applications, demonstrating the complementary roles of both perception and understanding in MLLM. Code is available at https://github.com/IDEA-Research/ChatRex.",
            "score": 2,
            "issue_id": 831,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "8938f77b5606e172",
            "authors": [
                "Qing Jiang",
                "Gen luo",
                "Yuqin Yang",
                "Yuda Xiong",
                "Yihao Chen",
                "Zhaoyang Zeng",
                "Tianhe Ren",
                "Lei Zhang"
            ],
            "affiliations": [
                "International Digital Economy Academy (IDEA)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18363.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#architecture",
                    "#multimodal",
                    "#dataset",
                    "#cv",
                    "#optimization",
                    "#games"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ChatRex: объединение восприятия и понимания в компьютерном зрении",
                    "desc": "Статья представляет ChatRex - новую мультимодальную большую языковую модель (MLLM) с улучшенными возможностями восприятия. Модель использует разделенный дизайн восприятия, где универсальная сеть предложений генерирует боксы объектов, а языковая модель выбирает соответствующие индексы. Авторы также создали датасет Rexverse-2M для совместного обучения восприятию и пониманию. ChatRex демонстрирует сильные способности восприятия, сохраняя при этом возможности мультимодального понимания."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Perception in Multimodal Language Models",
                    "desc": "This paper addresses the limitations of multimodal large language models (MLLM) in visual perception, particularly highlighting the low recall rates of existing models like Qwen2-VL. The authors introduce ChatRex, a new MLLM that separates perception from understanding by using a universal proposal network to generate box coordinates, which the LLM then processes as indices. Additionally, they develop the Rexverse-2M dataset to enhance training by providing diverse data for both perception and understanding tasks. The results show that ChatRex achieves improved perception capabilities while maintaining strong multimodal understanding, showcasing the importance of integrating both aspects in MLLM applications."
                },
                "zh": {
                    "title": "感知与理解的完美结合",
                    "desc": "本论文探讨了计算机视觉中的感知与理解两个重要方面。尽管多模态大型语言模型（MLLM）在视觉理解上表现出色，但在感知能力上仍显不足。我们提出了ChatRex模型，通过解耦的感知设计，将检测任务转化为检索任务，从而提高了感知能力。同时，我们构建了Rexverse-2M数据集，支持感知与理解的联合训练，最终实现了这两种能力的强大结合。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18104",
            "title": "Training and Evaluating Language Models with Template-based Data Generation",
            "url": "https://huggingface.co/papers/2411.18104",
            "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these models often struggle with tasks requiring complex reasoning, particularly in mathematical problem-solving, due in part to the scarcity of large-scale, high-quality, domain-specific datasets necessary for training sophisticated reasoning abilities. To address this limitation, we introduce Template-based Data Generation (TDG), a novel approach that leverages LLMs (GPT-4) to automatically generate parameterized meta-templates, which are then used to synthesize a vast array of high-quality problems and solutions. Leveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset comprising over 7 million synthetically generated grade school math problems--each accompanied by code-based and natural language solutions--with the potential to generate an effectively unlimited number more. This dataset alleviates the scarcity of large-scale mathematical datasets and serves as a valuable resource for pre-training, fine-tuning, and evaluating LLMs in mathematical reasoning. Our method not only enables the generation of virtually infinite data but also elevates data augmentation to a new level by using GPT-4 for meta-template generation, ensuring diverse and high-quality problem structures. The TemplateMath Part I: TemplateGSM dataset is publicly available at https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available at https://github.com/iiis-ai/TemplateMath.",
            "score": 1,
            "issue_id": 830,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "63ac26a6bf01c100",
            "authors": [
                "Yifan Zhang"
            ],
            "affiliations": [
                "IIIS, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18104.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#data",
                    "#reasoning",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Автоматическая генерация математических задач для обучения ИИ",
                    "desc": "В статье представлен новый подход к генерации данных для обучения языковых моделей (LLM) решению математических задач. Метод Template-based Data Generation (TDG) использует GPT-4 для создания параметризованных мета-шаблонов, которые затем применяются для синтеза огромного количества качественных задач с решениями. С помощью TDG авторы создали датасет TemplateMath Part I: TemplateGSM, содержащий более 7 миллионов сгенерированных математических задач школьного уровня. Этот подход позволяет генерировать практически неограниченное количество разнообразных и качественных данных для обучения и оценки LLM в области математических рассуждений."
                },
                "en": {
                    "title": "Empowering LLMs with Infinite Math Problems through Template Generation",
                    "desc": "This paper discusses the limitations of large language models (LLMs) like GPT-3 and GPT-4 in performing complex reasoning tasks, especially in mathematics. To overcome this challenge, the authors propose a new method called Template-based Data Generation (TDG), which uses LLMs to create a wide range of high-quality mathematical problems and solutions. They introduce a dataset named TemplateMath Part I: TemplateGSM, containing over 7 million synthetically generated grade school math problems, which can be expanded infinitely. This dataset aims to enhance the training and evaluation of LLMs in mathematical reasoning by providing a rich resource for pre-training and fine-tuning."
                },
                "zh": {
                    "title": "利用模板生成无限数学问题",
                    "desc": "本文介绍了一种新的数据生成方法，称为基于模板的数据生成（TDG），旨在解决大型语言模型在复杂推理任务中的不足。通过利用GPT-4，TDG能够自动生成参数化的元模板，从而合成出大量高质量的数学问题和解决方案。我们创建了TemplateMath Part I: TemplateGSM数据集，包含超过700万个合成的数学问题，极大地缓解了大规模数学数据集的稀缺问题。该数据集为大型语言模型的预训练、微调和评估提供了宝贵的资源，推动了数据增强的进步。"
                }
            }
        }
    ],
    "link_prev": "2024-11-27.html",
    "link_next": "2024-11-29.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "27.11",
        "en": "11/27",
        "zh": "11月27日"
    },
    "short_date_next": {
        "ru": "29.11",
        "en": "11/29",
        "zh": "11月29日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 3,
        "#cv": 8,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 5,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 1,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 7,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了自然语言在处理多实例的位置和属性信息时的局限性。为了解决这个问题，作者提出了一种基于区域实例控制的扩散模型。他们引入了ROI-Unpool操作，与ROI-Align结合，实现了高效和准确的区域操作。基于这一技术，他们提出了ROICtrl，一个适配器，能够精确控制区域实例。实验表明，ROICtrl在区域实例控制方面表现优异，并显著降低了计算成本。",
        "title": "ROICtrl: Boosting Instance Control for Visual Generation",
        "pinyin": "这篇文章讨论了自然语言在处理多实例的位置和属性信息时的局限性。\nZhè piān wén zhāng tǎo lùn le zì rán yǔ yán zài chǔ lǐ duō shí lì de wèi zhì hé shǔ xìng xìn xī shí de jú xiàn xìng.\n\n为了解决这个问题，作者提出了一种基于区域实例控制的扩散模型。\nWèi le jiě jué zhè gè wèn tí, zuò zhě tí chū le yī zhǒng jī yú qū yù shí lì kòng zhì de kuò sàn mó xíng.\n\n他们引入了ROI-Unpool操作，与ROI-Align结合，实现了高效和准确的区域操作。\nTā men yǐn rù le ROI-Unpool cāo zuò, yǔ ROI-Align jié hé, shí xiàn le gāo xiào hé zhǔn què de qū yù cāo zuò.\n\n基于这一技术，他们提出了ROICtrl，一个适配器，能够精确控制区域实例。\nJī yú zhè yī jì shù, tā men tí chū le ROICtrl, yī gè shì pèi qì, néng gòu jīng què kòng zhì qū yù shí lì.\n\n实验表明，ROICtrl在区域实例控制方面表现优异，并显著降低了计算成本。\nShí yàn biǎo míng, ROICtrl zài qū yù shí lì kòng zhì fāng miàn biǎo xiàn yōu yì, bìng xiǎn zhù jiàng dī le jì suàn chéng běn.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"自然语言\", \"pinyin\": \"zì rán yǔ yán\", \"trans\": \"natural language\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"process\"},\n    {\"word\": \"多实例\", \"pinyin\": \"duō shí lì\", \"trans\": \"multiple instances\"},\n    {\"word\": \"位置\", \"pinyin\": \"wèi zhì\", \"trans\": \"position\"},\n    {\"word\": \"属性\", \"pinyin\": \"shǔ xìng\", \"trans\": \"attribute\"},\n    {\"word\": \"信息\", \"pinyin\": \"xìn xī\", \"trans\": \"information\"},\n    {\"word\": \"局限性\", \"pinyin\": \"jú xiàn xìng\", \"trans\": \"limitation\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"问题\", \"pinyin\": \"wèn tí\", \"trans\": \"problem\"},\n    {\"word\": \"作者\", \"pinyin\": \"zuò zhě\", \"trans\": \"author\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"区域\", \"pinyin\": \"qū yù\", \"trans\": \"region\"},\n    {\"word\": \"实例\", \"pinyin\": \"shí lì\", \"trans\": \"instance\"},\n    {\"word\": \"控制\", \"pinyin\": \"kòng zhì\", \"trans\": \"control\"},\n    {\"word\": \"扩散\", \"pinyin\": \"kuò sàn\", \"trans\": \"diffusion\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"ROI-Unpool\", \"pinyin\": \"ROI-Unpool\", \"trans\": \"ROI-Unpool\"},\n    {\"word\": \"操作\", \"pinyin\": \"cāo zuò\", \"trans\": \"operation\"},\n    {\"word\": \"结合\", \"pinyin\": \"jié hé\", \"trans\": \"combine\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāo xiào\", \"trans\": \"efficient\"},\n    {\"word\": \"准确\", \"pinyin\": \"zhǔn què\", \"trans\": \"accurate\"},\n    {\"word\": \"适配器\", \"pinyin\": \"shì pèi qì\", \"trans\": \"adapter\"},\n    {\"word\": \"精确\", \"pinyin\": \"jīng què\", \"trans\": \"precise\"},\n    {\"word\": \"表明\", \"pinyin\": \"biǎo míng\", \"trans\": \"indicate\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"优异\", \"pinyin\": \"yōu yì\", \"trans\": \"excellent\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"降低\", \"pinyin\": \"jiàng dī\", \"trans\": \"reduce\"},\n    {\"word\": \"计算\", \"pinyin\": \"jì suàn\", \"trans\": \"computation\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"}\n]",
        "trans": "This article discusses the limitations of natural language in handling multi-instance positional and attribute information. To address this issue, the authors propose a diffusion model based on region instance control. They introduce the ROI-Unpool operation, which, when combined with ROI-Align, achieves efficient and accurate regional operations. Based on this technology, they present ROICtrl, an adapter that can precisely control regional instances. Experiments show that ROICtrl performs exceptionally well in regional instance control and significantly reduces computational costs.",
        "update_ts": "2024-11-28 09:11"
    }
}