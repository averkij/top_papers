{
    "date": {
        "ru": "10 января",
        "en": "January 10",
        "zh": "1月10日"
    },
    "time_utc": "2025-01-10 20:11",
    "weekday": 4,
    "issue_id": 1612,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.05441",
            "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
            "url": "https://huggingface.co/papers/2501.05441",
            "abstract": "There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, our new loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline -- R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.",
            "score": 22,
            "issue_id": 1596,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 января",
                "en": "January 9",
                "zh": "1月9日"
            },
            "hash": "eb1cd90c4d5cb0ef",
            "authors": [
                "Yiwen Huang",
                "Aaron Gokaslan",
                "Volodymyr Kuleshov",
                "James Tompkin"
            ],
            "affiliations": [
                "Brown University",
                "Cornell University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05441.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Упрощение и модернизация GAN: новый взгляд на обучение генеративных моделей",
                    "desc": "Исследователи опровергают распространенное мнение о сложности обучения генеративно-состязательных сетей (GAN). Они разработали новый регуляризованный релятивистский GAN-лосс, который решает проблемы потери мод и отсутствия сходимости. Авторы математически доказывают, что их лосс обеспечивает локальную сходимость, в отличие от большинства существующих релятивистских лоссов. На основе этого подхода они создали минималистичную базовую модель R3GAN, которая превосходит StyleGAN2 и другие современные GAN на нескольких наборах данных."
                },
                "en": {
                    "title": "Simplifying GAN Training with R3GAN: A New Era of Efficiency",
                    "desc": "This paper challenges the common belief that Generative Adversarial Networks (GANs) are inherently difficult to train. It introduces a new GAN loss function called the regularized relativistic GAN loss, which effectively addresses issues like mode dropping and non-convergence without relying on numerous empirical tricks. The authors provide mathematical analysis showing that their loss function guarantees local convergence, which is a significant improvement over existing methods. By applying this new loss to modern architectures like StyleGAN2, they create a simplified and efficient GAN model named R3GAN, which outperforms previous models on several benchmark datasets."
                },
                "zh": {
                    "title": "简化GAN训练，超越传统架构",
                    "desc": "这篇论文探讨了生成对抗网络（GAN）训练的难点，并提出了一种新的方法来简化这一过程。作者提出了一种正则化的相对GAN损失函数，解决了模式丢失和非收敛的问题。通过数学分析，证明了这种损失函数具有局部收敛的保证，优于现有的相对损失函数。最终，作者展示了一个新的简约基线R3GAN，其在多个数据集上的表现超过了StyleGAN2，并与最先进的GAN和扩散模型相媲美。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05453",
            "title": "An Empirical Study of Autoregressive Pre-training from Videos",
            "url": "https://huggingface.co/papers/2501.05453",
            "abstract": "We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at https://brjathu.github.io/toto/",
            "score": 15,
            "issue_id": 1596,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 января",
                "en": "January 9",
                "zh": "1月9日"
            },
            "hash": "3846ea8507d046be",
            "authors": [
                "Jathushan Rajasegaran",
                "Ilija Radosavovic",
                "Rahul Ravishankar",
                "Yossi Gandelsman",
                "Christoph Feichtenhofer",
                "Jitendra Malik"
            ],
            "affiliations": [
                "Meta FAIR",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05453.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#architecture",
                    "#robotics",
                    "#video",
                    "#cv"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Авторегрессионное предобучение видео: путь к универсальному компьютерному зрению",
                    "desc": "В статье исследуется авторегрессионное предобучение на видеоданных с использованием модели Toto. Авторы рассматривают видео как последовательности визуальных токенов и обучают трансформеры предсказывать будущие токены. Модели предобучаются на разнообразном наборе данных из более чем триллиона визуальных токенов. Результаты показывают, что такой подход дает конкурентоспособную производительность на различных задачах компьютерного зрения."
                },
                "en": {
                    "title": "Unlocking Video Understanding with Autoregressive Models",
                    "desc": "This paper investigates the use of autoregressive pre-training for video data through a series of models named Toto. The authors treat videos as sequences of visual tokens and employ transformer architectures to predict future tokens in these sequences. They pre-train their models on a massive dataset containing over 1 trillion visual tokens, exploring various design choices in architecture and training. The results show that these autoregressive models achieve strong performance on tasks like image recognition and video classification, indicating that scaling video models can yield similar benefits as seen in language models."
                },
                "zh": {
                    "title": "自回归预训练：视频模型的新突破",
                    "desc": "本文研究了视频的自回归预训练。我们构建了一系列名为Toto的自回归视频模型，将视频视为视觉标记的序列，并训练变换器模型以自回归方式预测未来的标记。我们的模型在一个包含超过1万亿视觉标记的多样化视频和图像数据集上进行预训练，并在多个下游任务上评估学习到的视觉表示。结果表明，尽管诱导偏差较小，自回归预训练在所有基准测试中表现出竞争力的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04003",
            "title": "Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives",
            "url": "https://huggingface.co/papers/2501.04003",
            "abstract": "Recent advancements in Vision-Language Models (VLMs) have sparked interest in their use for autonomous driving, particularly in generating interpretable driving decisions through natural language. However, the assumption that VLMs inherently provide visually grounded, reliable, and interpretable explanations for driving remains largely unexamined. To address this gap, we introduce DriveBench, a benchmark dataset designed to evaluate VLM reliability across 17 settings (clean, corrupted, and text-only inputs), encompassing 19,200 frames, 20,498 question-answer pairs, three question types, four mainstream driving tasks, and a total of 12 popular VLMs. Our findings reveal that VLMs often generate plausible responses derived from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs. This behavior, concealed by dataset imbalances and insufficient evaluation metrics, poses significant risks in safety-critical scenarios like autonomous driving. We further observe that VLMs struggle with multi-modal reasoning and display heightened sensitivity to input corruptions, leading to inconsistencies in performance. To address these challenges, we propose refined evaluation metrics that prioritize robust visual grounding and multi-modal understanding. Additionally, we highlight the potential of leveraging VLMs' awareness of corruptions to enhance their reliability, offering a roadmap for developing more trustworthy and interpretable decision-making systems in real-world autonomous driving contexts. The benchmark toolkit is publicly accessible.",
            "score": 8,
            "issue_id": 1599,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 января",
                "en": "January 7",
                "zh": "1月7日"
            },
            "hash": "720b493a608f478a",
            "authors": [
                "Shaoyuan Xie",
                "Lingdong Kong",
                "Yuhao Dong",
                "Chonghao Sima",
                "Wenwei Zhang",
                "Qi Alfred Chen",
                "Ziwei Liu",
                "Liang Pan"
            ],
            "affiliations": [
                "National University of Singapore",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory",
                "The University of Hong Kong",
                "University of California, Irvine"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04003.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#interpretability",
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Проверка надёжности VLM для безопасного автономного вождения",
                    "desc": "Статья представляет DriveBench - набор данных для оценки надёжности мультимодальных языковых моделей (VLM) в контексте автономного вождения. Исследование выявило, что VLM часто генерируют правдоподобные ответы на основе общих знаний, а не визуальной информации, что опасно в критически важных сценариях. Авторы предлагают усовершенствованные метрики оценки, ориентированные на надёжную визуальную привязку и мультимодальное понимание. Также отмечается потенциал использования осведомленности VLM о искажениях для повышения их надёжности."
                },
                "en": {
                    "title": "Enhancing Trust in Vision-Language Models for Safer Autonomous Driving",
                    "desc": "This paper discusses the limitations of Vision-Language Models (VLMs) in the context of autonomous driving, particularly their ability to provide reliable and interpretable driving decisions. The authors introduce DriveBench, a comprehensive benchmark dataset that tests VLM performance across various conditions, including clean and corrupted inputs. Their research shows that VLMs often rely on general knowledge rather than true visual understanding, especially when visual data is compromised. To improve VLM reliability, the paper suggests new evaluation metrics focused on visual grounding and multi-modal reasoning, aiming to enhance the safety of autonomous driving systems."
                },
                "zh": {
                    "title": "提升自动驾驶决策的可靠性与可解释性",
                    "desc": "本文介绍了DriveBench，一个用于评估视觉语言模型（VLMs）在自动驾驶中可靠性的基准数据集。该数据集包含19200帧图像和20498个问答对，涵盖了多种驾驶任务和输入类型。研究发现，VLMs在处理受损或缺失的视觉输入时，往往依赖于一般知识而非真实的视觉信息，导致安全隐患。为了解决这些问题，本文提出了改进的评估指标，强调视觉基础和多模态理解的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04377",
            "title": "On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis",
            "url": "https://huggingface.co/papers/2501.04377",
            "abstract": "Recently, Visual Autoregressive (VAR) Models introduced a groundbreaking advancement in the field of image generation, offering a scalable approach through a coarse-to-fine \"next-scale prediction\" paradigm. However, the state-of-the-art algorithm of VAR models in [Tian, Jiang, Yuan, Peng and Wang, NeurIPS 2024] takes O(n^4) time, which is computationally inefficient. In this work, we analyze the computational limits and efficiency criteria of VAR Models through a fine-grained complexity lens. Our key contribution is identifying the conditions under which VAR computations can achieve sub-quadratic time complexity. Specifically, we establish a critical threshold for the norm of input matrices used in VAR attention mechanisms. Above this threshold, assuming the Strong Exponential Time Hypothesis (SETH) from fine-grained complexity theory, a sub-quartic time algorithm for VAR models is impossible. To substantiate our theoretical findings, we present efficient constructions leveraging low-rank approximations that align with the derived criteria. This work initiates the study of the computational efficiency of the VAR model from a theoretical perspective. Our technique will shed light on advancing scalable and efficient image generation in VAR frameworks.",
            "score": 5,
            "issue_id": 1597,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 января",
                "en": "January 8",
                "zh": "1月8日"
            },
            "hash": "be8a0f20db676680",
            "authors": [
                "Yekun Ke",
                "Xiaoyu Li",
                "Yingyu Liang",
                "Zhizhou Sha",
                "Zhenmei Shi",
                "Zhao Song"
            ],
            "affiliations": [
                "The Simons Institute for the Theory of Computing at UC Berkeley",
                "The University of Hong Kong",
                "Tsinghua University",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04377.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Преодоление вычислительных барьеров в VAR моделях",
                    "desc": "Статья исследует вычислительные ограничения и критерии эффективности Визуальных Авторегрессионных (VAR) моделей с точки зрения тонкой теории сложности. Авторы определяют условия, при которых вычисления VAR могут достичь субквадратичной временной сложности. Они устанавливают критический порог для нормы входных матриц, используемых в механизмах внимания VAR, выше которого невозможен субкварцевый алгоритм времени для моделей VAR. Представлены эффективные конструкции, использующие аппроксимации низкого ранга, которые соответствуют выведенным критериям."
                },
                "en": {
                    "title": "Unlocking Efficiency in Image Generation with VAR Models",
                    "desc": "This paper explores the computational efficiency of Visual Autoregressive (VAR) Models, which are used for generating images. The authors identify that the current state-of-the-art VAR algorithm is computationally expensive, operating in O(n^4) time complexity. They establish conditions under which VAR computations can be optimized to achieve sub-quadratic time complexity, particularly focusing on the input matrix norms in the attention mechanisms. By applying low-rank approximations, the authors provide practical constructions that meet their theoretical criteria, paving the way for more efficient image generation techniques in VAR frameworks."
                },
                "zh": {
                    "title": "提升VAR模型的计算效率",
                    "desc": "最近，视觉自回归（VAR）模型在图像生成领域取得了突破性进展，采用粗到细的“下一个尺度预测”范式。然而，VAR模型的最新算法在计算上效率低下，时间复杂度为O(n^4)。本文通过细粒度复杂性分析，探讨了VAR模型的计算限制和效率标准。我们确定了VAR计算可以实现亚二次时间复杂度的条件，并提出了利用低秩近似的高效构造，以支持我们的理论发现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05122",
            "title": "Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model",
            "url": "https://huggingface.co/papers/2501.05122",
            "abstract": "Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50\\% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train Centurio, a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages.",
            "score": 4,
            "issue_id": 1604,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 января",
                "en": "January 9",
                "zh": "1月9日"
            },
            "hash": "92d74f3bbeb4a400",
            "authors": [
                "Gregor Geigle",
                "Florian Schneider",
                "Carolin Holtermann",
                "Chris Biemann",
                "Radu Timofte",
                "Anne Lauscher",
                "Goran Glavaš"
            ],
            "affiliations": [
                "Data Science Group, University of Hamburg",
                "Language Technology Group",
                "WüNLP, Computer Vision Lab, CAIDAS, University of Würzburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05122.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#multilingual",
                    "#benchmark",
                    "#low_resource"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Centurio: Прорыв в многоязычном визуально-языковом ИИ",
                    "desc": "В статье описывается исследование стратегий обучения многоязычных крупномасштабных визуально-языковых моделей (LVLMs). Авторы проводят эксперименты на 13 задачах и 43 языках, изучая оптимальное распределение языков в данных для предобучения и инструктивной настройки. Они обнаруживают, что можно включить до 100 языков обучения, используя всего 25-50% неанглийских данных, значительно улучшив многоязычную производительность при сохранении высокой эффективности на английском. На основе полученных результатов авторы обучают Centurio - 100-язычную LVLM, демонстрирующую передовые результаты на 14 задачах и 56 языках."
                },
                "en": {
                    "title": "Unlocking Multilingual Mastery in Vision-Language Models",
                    "desc": "This paper investigates how to effectively train Large Vision-Language Models (LVLMs) on multiple languages, particularly focusing on improving their performance in non-English languages. The authors conduct experiments across various tasks and languages to determine the best strategies for including multilingual data without harming English performance. They discover that including up to 100 languages and using a smaller proportion of non-English data can enhance multilingual capabilities while maintaining strong English results. Additionally, they emphasize the importance of incorporating non-English OCR data to boost understanding of text within images, culminating in the development of Centurio, a 100-language LVLM with state-of-the-art performance."
                },
                "zh": {
                    "title": "提升多语言理解，Centurio引领新潮流",
                    "desc": "本文研究了大规模多语言视觉-语言模型（LVLM）的训练策略，特别关注如何提高模型对非英语输入的理解和输出能力。我们通过多阶段实验，分析了包含多种语言的训练数据对英语性能的影响，并探索了最佳的语言分布策略。研究发现，最多可以同时包含100种语言的训练数据，并且只需25-50%的非英语数据即可显著提升多语言性能。最后，我们结合所有发现，训练了Centurio，一个支持100种语言的LVLM，在14个任务和56种语言的评估中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03489",
            "title": "Entropy-Guided Attention for Private LLMs",
            "url": "https://huggingface.co/papers/2501.03489",
            "abstract": "The pervasiveness of proprietary language models has raised critical privacy concerns, necessitating advancements in private inference (PI), where computations are performed directly on encrypted data without revealing users' sensitive information. While PI offers a promising solution, its practical deployment is hindered by substantial communication and latency overheads, primarily stemming from nonlinear operations. To address this, we introduce an information-theoretic framework to characterize the role of nonlinearities in decoder-only language models, laying a principled foundation for optimizing transformer-architectures tailored to the demands of PI.   By leveraging Shannon's entropy as a quantitative measure, we uncover the previously unexplored dual significance of nonlinearities: beyond ensuring training stability, they are crucial for maintaining attention head diversity. Specifically, we find that their removal triggers two critical failure modes: {\\em entropy collapse} in deeper layers that destabilizes training, and {\\em entropic overload} in earlier layers that leads to under-utilization of Multi-Head Attention's (MHA) representational capacity.   We propose an entropy-guided attention mechanism paired with a novel entropy regularization technique to mitigate entropic overload. Additionally, we explore PI-friendly alternatives to layer normalization for preventing entropy collapse and stabilizing the training of LLMs with reduced-nonlinearities. Our study bridges the gap between information theory and architectural design, establishing entropy dynamics as a principled guide for developing efficient PI architectures. The code and implementation are available at https://github.com/Nandan91/entropy-guided-attention-llm{entropy-guided-llm}.",
            "score": 4,
            "issue_id": 1597,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 января",
                "en": "January 7",
                "zh": "1月7日"
            },
            "hash": "18abcfb3fe1b209b",
            "authors": [
                "Nandan Kumar Jha",
                "Brandon Reagen"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03489.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#inference",
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "Энтропия как ключ к конфиденциальным языковым моделям",
                    "desc": "Статья рассматривает проблему конфиденциальности при использовании языковых моделей и предлагает решение через частное вычисление (PI). Авторы представляют информационно-теоретическую основу для оптимизации архитектур трансформеров под задачи PI, используя энтропию Шеннона как количественную меру. Исследование выявляет двойную роль нелинейностей в моделях: обеспечение стабильности обучения и поддержание разнообразия в механизме внимания. Предложен энтропийно-управляемый механизм внимания и новая техника регуляризации энтропии для улучшения эффективности PI-архитектур."
                },
                "en": {
                    "title": "Optimizing Language Models for Privacy with Entropy Dynamics",
                    "desc": "This paper addresses privacy concerns related to proprietary language models by focusing on private inference (PI), which allows computations on encrypted data. The authors introduce an information-theoretic framework to analyze the impact of nonlinearities in decoder-only language models, which are essential for optimizing transformer architectures for PI. They identify two critical issues caused by the removal of nonlinearities: entropy collapse in deeper layers and entropic overload in earlier layers, both of which affect training stability and attention mechanisms. To resolve these issues, the paper proposes an entropy-guided attention mechanism and explores alternatives to layer normalization, aiming to enhance the efficiency of PI architectures while maintaining model performance."
                },
                "zh": {
                    "title": "优化私密推理架构的熵动态",
                    "desc": "本论文探讨了在加密数据上进行私密推理（PI）时，非线性操作对解码器语言模型的影响。我们提出了一种信息论框架，帮助优化适合PI需求的变换器架构。研究发现，非线性不仅确保了训练的稳定性，还对注意力头的多样性至关重要。为了解决熵崩溃和熵过载问题，我们提出了一种基于熵的注意力机制和新的熵正则化技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05032",
            "title": "Enhancing Human-Like Responses in Large Language Models",
            "url": "https://huggingface.co/papers/2501.05032",
            "abstract": "This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes.",
            "score": 3,
            "issue_id": 1609,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 января",
                "en": "January 9",
                "zh": "1月9日"
            },
            "hash": "64e14687fd1e5dab",
            "authors": [
                "Ethem Yağız Çalık",
                "Talha Rüzgar Akkuş"
            ],
            "affiliations": [
                "Hugging Face"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05032.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#ethics",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Путь к человекоподобному ИИ: улучшение больших языковых моделей",
                    "desc": "Статья исследует методы повышения человекоподобности больших языковых моделей (LLM). Авторы рассматривают техники улучшения понимания естественного языка, связности диалогов и эмоционального интеллекта в системах искусственного интеллекта. Исследование оценивает различные подходы, включая дообучение на разнообразных датасетах, внедрение психологических принципов и разработку моделей, лучше имитирующих человеческие паттерны мышления. Результаты показывают, что эти улучшения не только совершенствуют взаимодействие с пользователем, но и открывают новые возможности для применения ИИ в различных областях."
                },
                "en": {
                    "title": "Enhancing AI: Making Language Models More Human-Like",
                    "desc": "This paper investigates how to make large language models (LLMs) behave more like humans. It emphasizes improving natural language understanding, making conversations more coherent, and increasing emotional intelligence in AI. The research assesses methods such as fine-tuning models with varied datasets and applying psychological principles to enhance human-like reasoning. The results show that these improvements lead to better user experiences and expand the potential uses of AI, while also highlighting the need to consider ethical issues and biases that may arise."
                },
                "zh": {
                    "title": "让人工智能更像人类的未来",
                    "desc": "本文探讨了使大型语言模型（LLMs）更具人性化的进展。我们重点关注增强自然语言理解、对话连贯性和情感智能的技术。研究评估了多种方法，包括使用多样化数据集进行微调、融入心理学原理，以及设计更好模拟人类推理模式的模型。我们的发现表明，这些增强不仅改善了用户互动，还为不同领域的人工智能应用开辟了新可能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.04828",
            "title": "Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models",
            "url": "https://huggingface.co/papers/2501.04828",
            "abstract": "This paper introduces foundational resources and models for natural language processing (NLP) of historical Turkish, a domain that has remained underexplored in computational linguistics. We present the first named entity recognition (NER) dataset, HisTR and the first Universal Dependencies treebank, OTA-BOUN for a historical form of the Turkish language along with transformer-based models trained using these datasets for named entity recognition, dependency parsing, and part-of-speech tagging tasks. Additionally, we introduce Ottoman Text Corpus (OTC), a clean corpus of transliterated historical Turkish texts that spans a wide range of historical periods. Our experimental results show significant improvements in the computational analysis of historical Turkish, achieving promising results in tasks that require understanding of historical linguistic structures. They also highlight existing challenges, such as domain adaptation and language variations across time periods. All of the presented resources and models are made available at https://huggingface.co/bucolin to serve as a benchmark for future progress in historical Turkish NLP.",
            "score": 3,
            "issue_id": 1603,
            "pub_date": "2025-01-08",
            "pub_date_card": {
                "ru": "8 января",
                "en": "January 8",
                "zh": "1月8日"
            },
            "hash": "40fe69c40d907fc4",
            "authors": [
                "Şaziye Betül Özateş",
                "Tarık Emre Tıraş",
                "Ece Elif Adak",
                "Berat Doğan",
                "Fatih Burak Karagöz",
                "Efe Eren Genç",
                "Esma F. Bilgin Taşdemir"
            ],
            "affiliations": [
                "Bogaziçi University",
                "Medeniyet University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.04828.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#low_resource",
                    "#science",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "🏛️",
                "ru": {
                    "title": "Прорыв в NLP для исторического турецкого языка",
                    "desc": "Статья представляет первые ресурсы и модели для обработки естественного языка (NLP) исторического турецкого языка. Авторы создали первый датасет для распознавания именованных сущностей (NER) HisTR и первый Universal Dependencies тривбанк OTA-BOUN для исторической формы турецкого языка. Также были разработаны трансформерные модели для задач NER, синтаксического анализа и морфологической разметки. Дополнительно представлен Османский текстовый корпус (OTC) - очищенный корпус транслитерированных исторических турецких текстов разных периодов."
                },
                "en": {
                    "title": "Unlocking Historical Turkish: New Resources for NLP",
                    "desc": "This paper provides essential resources and models for processing historical Turkish language using natural language processing (NLP) techniques. It introduces the first named entity recognition (NER) dataset, HisTR, and the first Universal Dependencies treebank, OTA-BOUN, specifically for historical Turkish. The authors also present the Ottoman Text Corpus (OTC), a comprehensive collection of transliterated texts from various historical periods. The results demonstrate advancements in analyzing historical Turkish, while also addressing challenges like domain adaptation and linguistic variations over time."
                },
                "zh": {
                    "title": "推动历史土耳其语NLP的进步",
                    "desc": "本文介绍了历史土耳其语自然语言处理（NLP）的基础资源和模型，这是一个在计算语言学中尚未深入研究的领域。我们首次发布了命名实体识别（NER）数据集HisTR和历史土耳其语的Universal Dependencies树库OTA-BOUN，并基于这些数据集训练了用于命名实体识别、依存句法分析和词性标注任务的变换器模型。此外，我们还推出了奥斯曼文本语料库（OTC），这是一个涵盖多个历史时期的清晰转写历史土耳其语文本的语料库。实验结果显示，在历史土耳其语的计算分析中取得了显著进展，但也突显了领域适应和语言随时间变化等挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05040",
            "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution",
            "url": "https://huggingface.co/papers/2501.05040",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source LLM designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other LLM model to generate patches for the identified files. Then, to mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches, and train the two modules of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving state-of-the-art performance among open-source models with scores of 23.3% and 30.2%, respectively. These outcomes highlight the efficacy of our approach. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.",
            "score": 2,
            "issue_id": 1608,
            "pub_date": "2025-01-09",
            "pub_date_card": {
                "ru": "9 января",
                "en": "January 9",
                "zh": "1月9日"
            },
            "hash": "54d8f8a0fe5436c6",
            "authors": [
                "Chengxing Xie",
                "Bowen Li",
                "Chang Gao",
                "He Du",
                "Wai Lam",
                "Difan Zou",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "Xidian University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05040.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#training",
                    "#science"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "Открытая языковая модель для эффективного решения проблем на GitHub",
                    "desc": "SWE-Fixer - это новая модель с открытым исходным кодом для решения проблем на GitHub. Она состоит из модуля поиска файлов кода и модуля редактирования кода, использующих легковесные языковые модели. Авторы создали обширный датасет из 110 тысяч GitHub-issues с патчами для обучения модели. SWE-Fixer достигла лучших результатов среди моделей с открытым кодом на бенчмарках SWE-Bench Lite и Verified."
                },
                "en": {
                    "title": "SWE-Fixer: Open-Source Solutions for GitHub Issues",
                    "desc": "This paper presents SWE-Fixer, an open-source Large Language Model (LLM) specifically designed to address software engineering challenges on GitHub. It features two main components: a code file retrieval module that uses BM25 and a lightweight LLM for efficient file identification, and a code editing module that generates code patches using another LLM. The authors also created a comprehensive dataset of 110,000 GitHub issues and their corresponding patches to train the model effectively. SWE-Fixer achieves state-of-the-art performance on benchmark tests, demonstrating its potential to enhance accessibility and transparency in software engineering solutions."
                },
                "zh": {
                    "title": "开源LLM助力软件工程问题解决",
                    "desc": "大型语言模型（LLMs）在处理复杂任务方面表现出色，尤其是在软件工程领域。本文介绍了一种新颖的开源LLM，名为SWE-Fixer，旨在有效解决GitHub上的问题。SWE-Fixer包含两个主要模块：代码文件检索模块和代码编辑模块，前者使用BM25和轻量级LLM进行文件检索，后者生成代码补丁。通过构建包含11万个GitHub问题及其补丁的数据集，SWE-Fixer在开源模型中实现了领先的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-01-09.html",
    "link_next": "2025-01-13.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "09.01",
        "en": "01/09",
        "zh": "1月9日"
    },
    "short_date_next": {
        "ru": "13.01",
        "en": "01/13",
        "zh": "1月13日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 2,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 2
    },
    "zh": {
        "text": "我们通过实验研究了从视频中进行自回归预训练的方法。为了进行研究，我们构建了一系列自回归视频模型，称为Toto。我们将视频视为视觉标记的序列，并训练变压器模型自回归地预测未来的标记。我们的模型在包含超过1万亿视觉标记的多样化视频和图像数据集上进行了预训练。我们探索了不同的架构、训练和推理设计选择。我们在包括图像识别、视频分类、目标跟踪和机器人技术在内的多个下游任务上评估了学习到的视觉表示。我们的结果表明，尽管存在最小的归纳偏差，自回归预训练在所有基准测试中都表现出竞争力。最后，我们发现扩展我们的视频模型会产生类似于语言模型的扩展曲线，只是速率不同。更多细节请访问 https://brjathu.github.io/toto/。",
        "title": "An Empirical Study of Autoregressive Pre-training from Videos",
        "pinyin": "Wǒmen tōngguò shíyàn yánjiūle cóng shìpín zhōng jìnxíng zìhuíguī yùxùnliàn de fāngfǎ. Wèile jìnxíng yánjiū, wǒmen gòujiànle yī xìliè zìhuíguī shìpín móxíng, chēngwéi Toto. Wǒmen jiāng shìpín shìwéi shìjué biāojì de xùliè, bìng xùnliàn biànshùqì móxíng zìhuíguī de yùcè wèilái de biāojì. Wǒmen de móxíng zài bāohán chāoguò 1 wàn yì shìjué biāojì de duōyànghuà shìpín hé túxiàng shùjùjí shàng jìnxíngle yùxùnliàn. Wǒmen tuànsuǒle bùtóng de jiàgòu, xùnliàn hé tuīlǐ shèjì xuǎnzé. Wǒmen zài bāokuò túxiàng shíbié, shìpín fēnlèi, mùbiāo gēnzōng hé jīqìrén jìshù zài nèi de duōgè xiàyóu rènwù shàng pínggūle xuéxí dào de shìjué biǎoshì. Wǒmen de jiéguǒ biǎomíng, jīnshǐ yǒu zuìshǎo de guīnà piānchá, zìhuíguī yùxùnliàn zài suǒyǒu jīzhǔn cèshì zhōng dōu biǎoxiàn chū jìngzhēnglì. Zuìhòu, wǒmen fāxiàn kuòzhǎn wǒmen de shìpín móxíng huì chǎnshēng xiàng yǔyán móxíng de kuòzhǎn qǔxiàn, zhǐshì sùlǜ bùtóng. Gèng duō xìjiè qǐng fǎngwèn https://brjathu.github.io/toto/。",
        "vocab": "[{'word': '自回归', 'pinyin': 'zì huí guī', 'trans': 'autoregressive'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pretraining'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'token'}, {'word': '变压器', 'pinyin': 'biàn yā qì', 'trans': 'transformer'}, {'word': '多样化', 'pinyin': 'duō yàng huà', 'trans': 'diversified'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '归纳', 'pinyin': 'guī nà', 'trans': 'inductive'}, {'word': '偏差', 'pinyin': 'piān chā', 'trans': 'bias'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '竞争力', 'pinyin': 'jìng zhēng lì', 'trans': 'competitive'}, {'word': '扩展', 'pinyin': 'kuò zhǎn', 'trans': 'scaling'}, {'word': '曲线', 'pinyin': 'qǔ xiàn', 'trans': 'curve'}, {'word': '速率', 'pinyin': 'sù lǜ', 'trans': 'rate'}]",
        "trans": "We investigated methods for autoregressive pretraining from videos through experimental research. To conduct the study, we constructed a series of autoregressive video models called Toto. We treated videos as sequences of visual tokens and trained transformer models to autoregressively predict future tokens. Our models were pretrained on diverse video and image datasets containing over 1 trillion visual tokens. We explored various architectural, training, and inference design choices. We evaluated the learned visual representations on multiple downstream tasks, including image recognition, video classification, object tracking, and robotics. Our results indicate that, despite minimal inductive bias, autoregressive pretraining performs competitively on all benchmarks. Finally, we found that scaling our video models produces scaling curves similar to those of language models, albeit at different rates. For more details, please visit https://brjathu.github.io/toto/.",
        "update_ts": "2025-01-10 09:11"
    }
}