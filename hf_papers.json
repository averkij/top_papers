{
    "date": {
        "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 21",
        "zh": "1æœˆ21æ—¥"
    },
    "time_utc": "2026-01-21 04:46",
    "weekday": 2,
    "issue_id": 684,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.11522",
            "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
            "url": "https://huggingface.co/papers/2601.11522",
            "abstract": "UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.",
            "score": 14,
            "issue_id": 683,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "2e8309152ea38796",
            "authors": [
                "Ruiheng Zhang",
                "Jingfeng Yao",
                "Huangxuan Zhao",
                "Hao Yan",
                "Xiao He",
                "Lei Chen",
                "Zhou Wei",
                "Yong Luo",
                "Zengmao Wang",
                "Lefei Zhang",
                "Dacheng Tao",
                "Bo Du"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Nanyang Technological University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11522.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#data",
                    "#architecture",
                    "#open_source",
                    "#multimodal",
                    "#healthcare"
                ],
                "emoji": "ğŸ«€",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ: Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                    "desc": "UniX â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»Ñ‘Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ pipeline Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ² ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹. UniX Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 46,1% Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° 24,2% Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸."
                },
                "en": {
                    "title": "UniX: Bridging Understanding and Generation in Medical Imaging",
                    "desc": "UniX is a new medical foundation model designed to improve how machines understand and generate medical images, specifically chest X-rays. It separates the tasks of visual understanding and image generation into two different branches: one that uses autoregressive methods for understanding and another that employs diffusion techniques for generating high-quality images. This model introduces a cross-modal self-attention mechanism that helps the generation process by incorporating features from the understanding branch, enhancing overall performance. As a result, UniX shows significant improvements in both understanding and generation tasks while using fewer parameters than previous models, making it a scalable solution for medical image analysis."
                },
                "zh": {
                    "title": "UniXï¼šåŒ»å­¦å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ååŒæ–°èŒƒå¼",
                    "desc": "UniXæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŒ»å­¦åŸºç¡€æ¨¡å‹ï¼Œå®ƒå°†è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡åˆ†å¼€å¤„ç†ï¼Œä½¿ç”¨ä¸åŒçš„è‡ªå›å½’å’Œæ‰©æ•£åˆ†æ”¯ï¼Œå¹¶ç»“åˆè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜æ€§èƒ½ã€‚ä¼ ç»Ÿçš„åŒ»å­¦æ¨¡å‹åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬çš„ç›®æ ‡ç›¸äº’çŸ›ç›¾ï¼šè¯­ä¹‰æŠ½è±¡ä¸åƒç´ çº§é‡å»ºã€‚UniXé€šè¿‡è‡ªå›å½’åˆ†æ”¯è¿›è¡Œç†è§£ï¼Œé€šè¿‡æ‰©æ•£åˆ†æ”¯è¿›è¡Œé«˜ä¿çœŸç”Ÿæˆï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚ç»è¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…ç†å’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒUniXåœ¨ç†è§£æ€§èƒ½å’Œç”Ÿæˆè´¨é‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†åŒ»å­¦å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ååŒå·¥ä½œæ–°èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13247",
            "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
            "url": "https://huggingface.co/papers/2601.13247",
            "abstract": "WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.",
            "score": 8,
            "issue_id": 683,
            "pub_date": "2026-01-19",
            "pub_date_card": {
                "ru": "19 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 19",
                "zh": "1æœˆ19æ—¥"
            },
            "hash": "d94ae6ed0aceb888",
            "authors": [
                "Baochang Ren",
                "Yunzhi Yao",
                "Rui Sun",
                "Shuofei Qiao",
                "Ningyu Zhang",
                "Huajun Chen"
            ],
            "affiliations": [
                "University of California, Los Angeles",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.13247.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#alignment",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“Ñ€ounded Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ LLM: Ğ¾Ñ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ°Ğ¼",
                    "desc": "WorldMind Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ°: Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚ Ñ†ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, WorldMind Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… EB-ALFRED Ğ¸ EB-Habitat Ñ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing LLMs with World Knowledge",
                    "desc": "WorldMind is a framework designed to improve Large Language Models (LLMs) by creating a symbolic repository of world knowledge. This repository helps LLMs understand and respect the physical laws of the world, reducing the occurrence of unrealistic plans or 'physical hallucinations'. Instead of relying on static model parameters that require expensive retraining, WorldMind uses experience-based learning to adaptively synthesize feedback from the environment. Experiments show that WorldMind significantly outperforms existing models in various tasks, demonstrating its ability to transfer knowledge across different models and environments."
                },
                "zh": {
                    "title": "WorldMindï¼šæ‰“ç ´æ¨¡æ€è„±èŠ‚çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "WorldMind è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ¨¡æ€è„±èŠ‚é—®é¢˜ï¼Œé€šè¿‡è‡ªä¸»æ„å»ºç¬¦å·ä¸–ç•ŒçŸ¥è¯†åº“ï¼Œå¢å¼ºäº†ç‰©ç†å¯è¡Œæ€§å’Œä»»åŠ¡æœ€ä¼˜æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆæˆç¯å¢ƒåé¦ˆï¼Œç»Ÿä¸€äº†è¿‡ç¨‹ç»éªŒå’Œç›®æ ‡ç»éªŒï¼Œä»¥ç¡®ä¿ç‰©ç†å¯è¡Œæ€§å’Œä»»åŠ¡å¯¼å‘ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–èµ„æºå¯†é›†å‹è®­ç»ƒçš„å¯¹é½ç­–ç•¥ç›¸æ¯”ï¼ŒWorldMind èƒ½å¤Ÿæ›´çµæ´»åœ°é€‚åº”ç‰©ç†åŠ¨æ€çš„å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWorldMind åœ¨ EB-ALFRED å’Œ EB-Habitat ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå…·æœ‰æ˜¾è‘—çš„è·¨æ¨¡å‹å’Œè·¨ç¯å¢ƒçš„è¿ç§»èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11969",
            "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
            "url": "https://huggingface.co/papers/2601.11969",
            "abstract": "A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.",
            "score": 7,
            "issue_id": 683,
            "pub_date": "2026-01-17",
            "pub_date_card": {
                "ru": "17 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 17",
                "zh": "1æœˆ17æ—¥"
            },
            "hash": "3d51cd5cc576b6cc",
            "authors": [
                "Zecheng Tang",
                "Baibei Ji",
                "Ruoxi Sun",
                "Haitian Wang",
                "WangJie You",
                "Zhang Yijun",
                "Wenpeng Zhu",
                "Ji Qi",
                "Juntao Li",
                "Min Zhang"
            ],
            "affiliations": [
                "China Mobile (Suzhou)",
                "LCM Laboratory",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11969.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#survey",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MemoryRewardBench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚ 8K Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ÑÑ‚ÑÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºĞ°Ğº Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Evaluating Memory Management in Language Models with MemoryRewardBench",
                    "desc": "The paper introduces MemoryRewardBench, a benchmark designed to evaluate how well reward models (RMs) assess long-term memory management in large language models (LLMs). It focuses on the ability of these models to handle long contexts and manage memory effectively across various tasks. The benchmark includes 10 different settings with context lengths ranging from 8K to 128K tokens, allowing for a comprehensive analysis of memory management patterns. Results show that newer RMs outperform older ones, highlighting both the strengths and limitations of current models in evaluating memory quality."
                },
                "zh": {
                    "title": "è¯„ä¼°é•¿æœŸè®°å¿†ç®¡ç†çš„åˆ›æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMemoryRewardBenchçš„åŸºå‡†ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¥–åŠ±æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç®¡ç†é•¿æœŸè®°å¿†çš„èƒ½åŠ›ã€‚éšç€å¯¹è®°å¿†ä¸­å¿ƒæœºåˆ¶çš„é‡‡ç”¨ï¼Œå¦‚ä½•æœ‰æ•ˆç®¡ç†è®°å¿†æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¼ æ’­ä¿¡æ¯çš„å…³é”®èƒ½åŠ›ã€‚MemoryRewardBenchæ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿç ”ç©¶å¥–åŠ±æ¨¡å‹è¯„ä¼°é•¿æœŸè®°å¿†ç®¡ç†è¿‡ç¨‹çš„åŸºå‡†ï¼Œæ¶µç›–äº†é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œé•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡å¯¹13ä¸ªå‰æ²¿å¥–åŠ±æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°å¼€æºæ¨¡å‹ä¸ä¸“æœ‰æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·é€æ¸ç¼©å°ï¼Œæ–°ä¸€ä»£æ¨¡å‹åœ¨å„ä¸ªå‚æ•°æ•°é‡ä¸‹å‡ä¼˜äºå‰ä»£æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.12294",
            "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
            "url": "https://huggingface.co/papers/2601.12294",
            "abstract": "ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.",
            "score": 6,
            "issue_id": 684,
            "pub_date": "2026-01-18",
            "pub_date_card": {
                "ru": "18 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 18",
                "zh": "1æœˆ18æ—¥"
            },
            "hash": "4ace6c0ff6a7867b",
            "authors": [
                "Dawei Li",
                "Yuguang Yao",
                "Zhen Tan",
                "Huan Liu",
                "Ruocheng Guo"
            ],
            "affiliations": [
                "Arizona State University",
                "Intuit AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.12294.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#dataset",
                    "#rl"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ToolPRMBench â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² (process reward models) Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "ToolPRMBench: Elevating Evaluation of Process Reward Models for Tool-Using Agents",
                    "desc": "This paper presents ToolPRMBench, a comprehensive benchmark designed to evaluate process reward models (PRMs) specifically for tool-using agents. It addresses the need for systematic evaluation by converting agent trajectories into detailed step-level test cases, which include correct and plausible incorrect actions. The benchmark employs both offline and online sampling methods to identify single-step and multi-step errors, ensuring a thorough assessment of PRM performance. The findings demonstrate significant variations in the effectiveness of different PRMs, emphasizing the advantages of specialized models for enhancing tool-using capabilities."
                },
                "zh": {
                    "title": "ToolPRMBenchï¼šè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ToolPRMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„è§„æ¨¡åŒ–åŸºå‡†ã€‚è¯¥åŸºå‡†é€šè¿‡å°†ä»£ç†è½¨è¿¹è½¬æ¢ä¸ºé€æ­¥æµ‹è¯•æ¡ˆä¾‹ï¼Œæä¾›äº†æ›´ç»†è‡´çš„ç›‘æ§å’Œè¯„ä¼°ã€‚æˆ‘ä»¬é‡‡ç”¨ç¦»çº¿é‡‡æ ·å’Œåœ¨çº¿é‡‡æ ·çš„æ–¹æ³•ï¼Œåˆ†åˆ«æ•æ‰å±€éƒ¨å•æ­¥é”™è¯¯å’ŒçœŸå®çš„å¤šæ­¥å¤±è´¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸“é—¨åŒ–çš„PRMåœ¨å·¥å…·ä½¿ç”¨ä¸­çš„æœ‰æ•ˆæ€§æ˜æ˜¾ä¼˜äºé€šç”¨PRMï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11655",
            "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey",
            "url": "https://huggingface.co/papers/2601.11655",
            "abstract": "Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.",
            "score": 6,
            "issue_id": 683,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "3bba28a3c4ff036b",
            "authors": [
                "Caihua Li",
                "Lianghong Guo",
                "Yanlin Wang",
                "Daya Guo",
                "Wei Tao",
                "Zhenyu Shan",
                "Mingwei Liu",
                "Jiachi Chen",
                "Haoyu Song",
                "Duyu Tang",
                "Hongyu Zhang",
                "Zibin Zheng"
            ],
            "affiliations": [
                "Chongqing University",
                "Hangzhou Normal University",
                "Huawei Technologies Co, Ltd",
                "Independent Researcher",
                "Sun Yat-sen University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11655.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#data",
                    "#training",
                    "#open_source",
                    "#plp",
                    "#survey"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering Autonomous Coding Agents for Software Issue Resolution",
                    "desc": "This paper explores the challenges that large language models face in resolving software issues, which is a crucial task in software engineering. It highlights the development of autonomous coding agents through both training-free and training-based methods. The authors provide a systematic survey of data construction techniques and analyze various methodologies, including supervised fine-tuning and reinforcement learning. Additionally, the paper discusses data quality, agent behavior, and future research directions, while maintaining an open-source repository for ongoing contributions in this area."
                },
                "zh": {
                    "title": "è‡ªä¸»ç¼–ç ä»£ç†ï¼šè§£å†³è½¯ä»¶é—®é¢˜çš„æ–°æ–¹å‘",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶é—®é¢˜è§£å†³æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› æ­¤å¼€å‘äº†è‡ªä¸»ç¼–ç ä»£ç†ï¼Œé‡‡ç”¨äº†å¤šç§æ— è®­ç»ƒå’ŒåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚è½¯ä»¶é—®é¢˜è§£å†³æ˜¯ä¸€ä¸ªå¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œå¯¹äººå·¥æ™ºèƒ½æ¥è¯´æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥äº†è¿™ä¸€æ–°å…´é¢†åŸŸï¼Œåˆ†æäº†æ•°æ®æ„å»ºæµç¨‹å’Œå„ç§æ–¹æ³•è®ºï¼ŒåŒ…æ‹¬æ— è®­ç»ƒæ¡†æ¶å’ŒåŸºäºè®­ç»ƒçš„æŠ€æœ¯ã€‚æœ€åï¼Œè®¨è®ºäº†æ•°æ®è´¨é‡ã€ä»£ç†è¡Œä¸ºçš„å…³é”®åˆ†æä»¥åŠå®é™…åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„ä¸»è¦æŒ‘æˆ˜å’Œæ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.12993",
            "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
            "url": "https://huggingface.co/papers/2601.12993",
            "abstract": "Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.",
            "score": 3,
            "issue_id": 684,
            "pub_date": "2026-01-19",
            "pub_date_card": {
                "ru": "19 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 19",
                "zh": "1æœˆ19æ—¥"
            },
            "hash": "ccaa8b7bf86cfa7f",
            "authors": [
                "Hao Luo",
                "Ye Wang",
                "Wanpeng Zhang",
                "Sipeng Zheng",
                "Ziheng Xi",
                "Chaoyi Xu",
                "Haiweng Xu",
                "Haoqi Yuan",
                "Chi Zhang",
                "Yiqing Wang",
                "Yicheng Feng",
                "Zongqing Lu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.12993.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training",
                    "#multimodal",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: universal VLA Ñ ĞºÑ€Ğ¾ÑÑĞ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Being-H0.5 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action (VLA), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ UniHand-2.0 â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 35 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ 30 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Mixture-of-Transformers Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Mixture-of-Flow (MoF) Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LIBERO (98.9%) Ğ¸ RoboCasa (53.9%), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Robots with Human-Centric Learning for Cross-Embodiment Mastery",
                    "desc": "Being-H0.5 is a Vision-Language-Action model that enhances the ability of robots to generalize across different physical forms. It employs a human-centric learning approach, using human interaction data as a foundational guide for robotic actions. The model is built on a Mixture-of-Transformers architecture, which allows it to effectively manage various robotic embodiments and their unique control requirements. With its innovative techniques like Manifold-Preserving Gating and Universal Async Chunking, Being-H0.5 demonstrates impressive performance on multiple robotic platforms, achieving state-of-the-art results in simulated environments."
                },
                "zh": {
                    "title": "è·¨ä½“ç°æ³›åŒ–çš„å¼ºå¤§æ¨¡å‹",
                    "desc": "Being-H0.5æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä»¥äººä¸ºä¸­å¿ƒçš„å­¦ä¹ å’Œæ··åˆå˜æ¢å™¨æ¶æ„å®ç°å¼ºå¤§çš„è·¨ä½“ç°æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åˆ©ç”¨äººç±»äº¤äº’è½¨è¿¹ä½œä¸ºç‰©ç†äº¤äº’çš„é€šç”¨â€œæ¯è¯­â€ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å½¢æ€å¼‚è´¨æ€§å’Œæ•°æ®ç¨€ç¼ºæ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„UniHand-2.0æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å…·èº«é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡35,000å°æ—¶çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ¶µç›–30ç§ä¸åŒçš„æœºå™¨äººä½“ç°ã€‚é€šè¿‡ç»Ÿä¸€çš„åŠ¨ä½œç©ºé—´å’Œå¤šä»»åŠ¡é¢„è®­ç»ƒèŒƒå¼ï¼ŒBeing-H0.5èƒ½å¤Ÿåœ¨ä¸åŒçš„æœºå™¨äººå¹³å°ä¸Šå®ç°ç¨³å®šçš„è·¨ä½“ç°ç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13288",
            "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
            "url": "https://huggingface.co/papers/2601.13288",
            "abstract": "Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.",
            "score": 1,
            "issue_id": 683,
            "pub_date": "2026-01-19",
            "pub_date_card": {
                "ru": "19 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 19",
                "zh": "1æœˆ19æ—¥"
            },
            "hash": "16b301dd1efbd006",
            "authors": [
                "Gonzalo Ariel Meyoyan",
                "Luciano Del Corro"
            ],
            "affiliations": [
                "Departamento de ComputaciÃ³n, FCEyN Universidad de Buenos Aires",
                "ELIAS Lab, Departamento de IngenierÃ­a Universidad de San AndrÃ©s"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.13288.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#small_models"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ›Ñ‘Ğ³ĞºĞ¸Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸: Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ğ±ĞµĞ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ² (probes) Ğ½Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ LLM, Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ²ÑĞµĞ³Ğ¾ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²ÑĞµÑ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ², Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Classification with Lightweight Probes on LLMs",
                    "desc": "This paper presents a method for improving classification tasks using lightweight probes that are trained on the hidden states of large language models (LLMs). By reusing the computation from the LLM during its generation process, the approach reduces latency and memory usage compared to traditional methods that require separate models for classification. The authors propose a two-stage aggregator that summarizes information from multiple layers of the LLM to create a single representation for classification tasks. The results show that these probes enhance performance in safety and sentiment analysis while maintaining efficiency, making them competitive with larger, task-specific models."
                },
                "zh": {
                    "title": "åˆ©ç”¨è½»é‡çº§æ¢é’ˆæå‡åˆ†ç±»æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§æ¢é’ˆï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éšè—çŠ¶æ€è¿›è¡Œé«˜æ•ˆåˆ†ç±»ä»»åŠ¡ï¼Œé¿å…äº†é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œä»è€Œæå‡äº†å®‰å…¨æ€§å’Œæƒ…æ„Ÿåˆ†æçš„æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é‡ç”¨LLMçš„è®¡ç®—ï¼Œè®­ç»ƒè¿™äº›æ¢é’ˆæ¥é¢„æµ‹æ ‡ç­¾ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå•ç‹¬çš„æ¨¡å‹ã€‚åˆ†ç±»è¢«è§†ä¸ºåœ¨å®Œæ•´çš„éšè—çŠ¶æ€å¼ é‡ä¸Šè¿›è¡Œè¡¨ç¤ºé€‰æ‹©ï¼Œè€Œä¸æ˜¯å›ºå®šäºæŸä¸ªç‰¹å®šçš„tokenæˆ–å±‚ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µèšåˆå™¨ï¼Œé¦–å…ˆåœ¨æ¯ä¸€å±‚å†…æ€»ç»“tokenï¼Œç„¶åè·¨å±‚æ±‡æ€»å½¢æˆå•ä¸€çš„åˆ†ç±»è¡¨ç¤ºã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-20.html",
    "link_next": "2026-01-22.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "20.01",
        "en": "01/20",
        "zh": "1æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "22.01",
        "en": "01/22",
        "zh": "1æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}