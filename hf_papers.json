{
    "date": {
        "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 7",
        "zh": "1æœˆ7æ—¥"
    },
    "time_utc": "2025-01-07 03:17",
    "weekday": 1,
    "issue_id": 1527,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.02976",
            "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution",
            "url": "https://huggingface.co/papers/2501.02976",
            "abstract": "Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce~\\name (Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate~\\name~outperforms state-of-the-art methods on both synthetic and real-world datasets.",
            "score": 1,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "13ac412646c508f5",
            "authors": [
                "Rui Xie",
                "Yinhong Liu",
                "Penghao Zhou",
                "Chen Zhao",
                "Jun Zhou",
                "Kai Zhang",
                "Zhenyu Zhang",
                "Jian Yang",
                "Zhenheng Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "ByteDance",
                "Nanjing University",
                "Southwest University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02976.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ T2V Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° STAR Ğ´Ğ»Ñ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-video. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ LIEM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Dynamic Frequency Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ STAR Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Video Quality with T2V Models for Real-World Super-Resolution",
                    "desc": "This paper presents a new method called Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution, which aims to improve video quality by addressing issues of over-smoothing and temporal consistency. Traditional image diffusion models struggle with video because they are designed for static images, leading to challenges in capturing motion dynamics. The proposed approach incorporates a Local Information Enhancement Module to enhance local details and reduce artifacts, along with a Dynamic Frequency Loss to maintain fidelity across different frequency components. Experimental results show that this method outperforms existing techniques in both synthetic and real-world scenarios, providing better spatial and temporal quality in restored videos."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘è¶…åˆ†è¾¨ç‡çš„æ—¶ç©ºä¸€è‡´æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåä¸º~\\name~ï¼Œç”¨äºæé«˜çœŸå®ä¸–ç•Œè§†é¢‘è¶…åˆ†è¾¨ç‡çš„æ—¶ç©ºè´¨é‡ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹ï¼Œä»¥è§£å†³ä¼ ç»Ÿç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ–¹æ³•ä¸­çš„è¿‡å¹³æ»‘é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å±€éƒ¨ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼ˆLIEMï¼‰å’ŒåŠ¨æ€é¢‘ç‡æŸå¤±ï¼ˆDF Lossï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„è§†é¢‘çš„å±€éƒ¨ç»†èŠ‚å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ~\\name~åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02157",
            "title": "Personalized Graph-Based Retrieval for Large Language Models",
            "url": "https://huggingface.co/papers/2501.02157",
            "abstract": "As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.",
            "score": 1,
            "issue_id": 1527,
            "pub_date": "2025-01-04",
            "pub_date_card": {
                "ru": "4 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 4",
                "zh": "1æœˆ4æ—¥"
            },
            "hash": "65e3736cfc1e3295",
            "authors": [
                "Steven Au",
                "Cameron J. Dimacali",
                "Ojasmitha Pedirappagari",
                "Namyong Park",
                "Franck Dernoncourt",
                "Yu Wang",
                "Nikos Kanakaris",
                "Hanieh Deilamsalehy",
                "Ryan A. Rossi",
                "Nesreen K. Ahmed"
            ],
            "affiliations": [
                "Adobe Research",
                "Cisco AI Research",
                "Meta AI",
                "University of California Santa Cruz",
                "University of Oregon",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02157.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#optimization",
                    "#graphs",
                    "#multimodal",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PGraphRAG. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, PGraphRAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PGraphRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Personalization with Graph-based Retrieval",
                    "desc": "This paper introduces a new framework called Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) that enhances the personalization of large language models (LLMs). Unlike traditional methods that depend only on user history, PGraphRAG utilizes user-centric knowledge graphs to provide richer context for generating responses. By integrating structured user information into the retrieval process, it improves the model's understanding and the quality of its outputs, especially in situations where user data is limited. The authors also present a benchmark for evaluating personalized text generation, showing that PGraphRAG outperforms existing methods in various tasks."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å›¾è°±æå‡ç”Ÿæˆè´¨é‡",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œå®ƒä»¬åœ¨æä¾›ä¸ªæ€§åŒ–å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•é€šå¸¸ä»…ä¾èµ–ç”¨æˆ·å†å²æ•°æ®æ¥å¢å¼ºæç¤ºï¼Œè¿™åœ¨æ•°æ®ç¨€ç–çš„å†·å¯åŠ¨åœºæ™¯ä¸­æ•ˆæœæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆPGraphRAGï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„çŸ¥è¯†å›¾è°±æ¥ä¸°å¯Œä¸ªæ€§åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPGraphRAGåœ¨å¤šç§ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œå±•ç¤ºäº†åŸºäºå›¾è°±çš„æ£€ç´¢åœ¨ä¸ªæ€§åŒ–ä¸­çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03006",
            "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
            "url": "https://huggingface.co/papers/2501.03006",
            "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
            "score": 1,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "e85e5fa9a03d5d04",
            "authors": [
                "Luozhou Wang",
                "Yijun Li",
                "Zhifei Chen",
                "Jui-Hsien Wang",
                "Zhifei Zhang",
                "He Zhang",
                "Zhe Lin",
                "Yingcong Chen"
            ],
            "affiliations": [
                "Adobe Research",
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03006.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "TransPixar: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGBA-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "TransPixar - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGBA-Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° (DiT) Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ°, Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGB Ğ¸ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ RGB-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. TransPixar ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ RGBA-Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "TransPixar: Bridging RGB and Alpha for Enhanced Video Generation",
                    "desc": "This paper presents TransPixar, a novel method for generating RGBA videos, which include transparency information crucial for visual effects. The challenge lies in the limited datasets and the need to adapt existing models to handle alpha channels effectively. TransPixar utilizes a diffusion transformer architecture and incorporates alpha-specific tokens, allowing it to generate both RGB and alpha channels simultaneously. By optimizing attention mechanisms and employing LoRA-based fine-tuning, TransPixar achieves high consistency between RGB and alpha outputs, enhancing the quality of video generation for applications in VFX and interactive media."
                },
                "zh": {
                    "title": "TransPixarï¼šç”Ÿæˆé«˜è´¨é‡RGBAè§†é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTransPixarçš„æ–¹æ³•ï¼Œæ—¨åœ¨ç”ŸæˆåŒ…å«é€æ˜é€šé“çš„RGBAè§†é¢‘ã€‚ä¼ ç»Ÿçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†é€æ˜æ•ˆæœæ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒTransPixaré€šè¿‡æ‰©å±•é¢„è®­ç»ƒæ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œç»“åˆç‰¹å®šçš„é€æ˜é€šé“æ ‡è®°ï¼Œå¹¶é€šè¿‡LoRAå¾®è°ƒå®ç°RGBå’Œé€æ˜é€šé“çš„é«˜ä¸€è‡´æ€§ç”Ÿæˆã€‚æœ€ç»ˆï¼ŒTransPixaråœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šä¼˜åŒ–äº†æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæˆåŠŸç”Ÿæˆå¤šæ ·ä¸”ä¸€è‡´çš„RGBAè§†é¢‘ï¼Œæ¨åŠ¨äº†è§†è§‰ç‰¹æ•ˆå’Œäº’åŠ¨å†…å®¹åˆ›ä½œçš„å¯èƒ½æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-06.html",
    "link_next": "2025-01-08.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "08.01",
        "en": "01/08",
        "zh": "1æœˆ8æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "This article introduces EnerVerse, a framework for generating future space in robotic tasks. It uses attention mechanisms for consistent space modeling and a memory context for long sequence generation. The FAV space enhances robot observation and adaptability. A data engine with 4DGS improves data quality and diversity. Experiments show it boosts performance in long-range robotic tasks.",
        "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nZhÃ¨ wÃ©nzhÄng jiÃ¨shÃ o EnerVerse, yÄ«gÃ¨ kuÃ ngjiÃ  yÇnqÇ wÃ¨ilÃ¡i kÅngjiÄn zÃ i jÄ«qirÃ©n rÃ©nwÃ¹ zhÅng. TÄ shÇyÃ²ng zhÃ¹yÃ¬ jÄ«zhÃ¬ wÃ¨i yuÃ¡nchÃ¡ng kÅngjiÄn mÃ³xÃ­ng hÃ© yÄ«gÃ¨ jÃ¬yÃ¬ qÅ«jiÃ n wÃ¨i chÃ¡ng xÃ¹liÃ¨ shÄ“ngchÃ©ng. FAV kÅngjiÄn zÄ“ngqiÃ¡ng jÄ«qirÃ©n guÄnchÃ¡ hÃ© shÃ¬yÃ¬ngxÃ¬ng. YÄ«gÃ¨ shÃ¹jÃ¹ yÇnqÃ­ng yÇ’u 4DGS gÇishÃ n shÃ¹jÃ¹ zhÃ¬liÃ ng hÃ© duÅyÃ ngxÃ¬ng. ShÃ­yÃ n xiÇnshÃ¬ tÄ zÄ“ngqiÃ¡ng xiÃ oguÇ’ zÃ i chÃ¡ngqÄ« jÄ«qirÃ©n rÃ©nwÃ¹ zhÅng.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific terminology used.",
        "vocab": "[\n    {\"word\": \"framework\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"æ¡†æ¶\"},\n    {\"word\": \"generating\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"ç”Ÿæˆ\"},\n    {\"word\": \"future\", \"pinyin\": \"wÃ¨ilÃ¡i\", \"trans\": \"æœªæ¥\"},\n    {\"word\": \"space\", \"pinyin\": \"kÅngjiÄn\", \"trans\": \"ç©ºé—´\"},\n    {\"word\": \"robotic\", \"pinyin\": \"jÄ«qirÃ©n\", \"trans\": \"æœºå™¨äºº\"},\n    {\"word\": \"tasks\", \"pinyin\": \"rÃ¨nwÃ¹\", \"trans\": \"ä»»åŠ¡\"},\n    {\"word\": \"attention\", \"pinyin\": \"zhÃ¹yÃ¬\", \"trans\": \"æ³¨æ„\"},\n    {\"word\": \"mechanisms\", \"pinyin\": \"jÄ«zhÃ¬\", \"trans\": \"æœºåˆ¶\"},\n    {\"word\": \"consistent\", \"pinyin\": \"wÃºguÇ’\", \"trans\": \"ä¸€è‡´\"},\n    {\"word\": \"modeling\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"å»ºæ¨¡\"},\n    {\"word\": \"memory\", \"pinyin\": \"jÃ¬yÃ¬\", \"trans\": \"è®°å¿†\"},\n    {\"word\": \"context\", \"pinyin\": \"qÇ”wÃ©n\", \"trans\": \"ä¸Šä¸‹æ–‡\"},\n    {\"word\": \"sequence\", \"pinyin\": \"xÃ¹liÃ¨\", \"trans\": \"åºåˆ—\"},\n    {\"word\": \"generation\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"ç”Ÿæˆ\"},\n    {\"word\": \"FAV\", \"pinyin\": \"FÄ“i-Ä’i-WÄ“i\", \"trans\": \"FAV\"},\n    {\"word\": \"enhances\", \"pinyin\": \"zÄ“ngqiÃ¡ng\", \"trans\": \"å¢å¼º\"},\n    {\"word\": \"observation\", \"pinyin\": \"guÄnchÃ¡\", \"trans\": \"è§‚å¯Ÿ\"},\n    {\"word\": \"adaptability\", \"pinyin\": \"shÃ¬yÃ¬ngxÃ¬ng\", \"trans\": \"é€‚åº”æ€§\"},\n    {\"word\": \"engine\", \"pinyin\": \"yÇnqÃ­ng\", \"trans\": \"å¼•æ“\"},\n    {\"word\": \"4DGS\", \"pinyin\": \"SÃ¬-DÄ«-JÄ«-Ä’s\", \"trans\": \"4DGS\"},\n    {\"word\": \"improves\", \"pinyin\": \"gÇishÃ n\", \"trans\": \"æ”¹å–„\"},\n    {\"word\": \"quality\", \"pinyin\": \"zhÃ¬liÃ ng\", \"trans\": \"è´¨é‡\"},\n    {\"word\": \"diversity\", \"pinyin\": \"duÅyÃ ngxÃ¬ng\", \"trans\": \"å¤šæ ·æ€§\"},\n    {\"word\": \"experiments\", \"pinyin\": \"shÃ­yÃ n\", \"trans\": \"å®éªŒ\"},\n    {\"word\": \"show\", \"pinyin\": \"xiÇnshÃ¬\", \"trans\": \"æ˜¾ç¤º\"},\n    {\"word\": \"boosts\", \"pinyin\": \"zÄ“ngqiÃ¡ng\", \"trans\": \"å¢å¼º\"},\n    {\"word\": \"performance\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"è¡¨ç°\"},\n    {\"word\": \"long-range\", \"pinyin\": \"chÃ¡ngyuÇn\", \"trans\": \"é•¿è¿œ\"}\n]",
        "trans": "This article introduces EnerVerse, a framework designed to generate future space in robotic tasks. It employs attention mechanisms to ensure consistent space modeling and utilizes a memory context for generating long sequences. The FAV space enhances robot observation capabilities and adaptability. Additionally, a data engine equipped with 4DGS improves the quality and diversity of data. Experiments demonstrate that it significantly boosts performance in long-range robotic tasks.",
        "update_ts": "2025-01-06 09:11"
    }
}