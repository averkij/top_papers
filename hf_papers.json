{
    "date": {
        "ru": "13 января",
        "en": "January 13",
        "zh": "1月13日"
    },
    "time_utc": "2025-01-13 05:11",
    "weekday": 0,
    "issue_id": 1628,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.05874",
            "title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus",
            "url": "https://huggingface.co/papers/2501.05874",
            "abstract": "Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily focused on textual information, with some recent advancements beginning to consider images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing events, processes, and contextual details more effectively than any other modality. While a few recent studies explore the integration of videos in the response generation process, they either predefine query-associated videos without retrieving them according to queries, or convert videos into the textual descriptions without harnessing their multimodal richness. To tackle these, we introduce VideoRAG, a novel framework that not only dynamically retrieves relevant videos based on their relevance with queries but also utilizes both visual and textual information of videos in the output generation. Further, to operationalize this, our method revolves around the recent advance of Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and seamless integration of the retrieved videos jointly with queries. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines.",
            "score": 17,
            "issue_id": 1626,
            "pub_date": "2025-01-10",
            "pub_date_card": {
                "ru": "10 января",
                "en": "January 10",
                "zh": "1月10日"
            },
            "hash": "a6a86d4d49a42b4d",
            "authors": [
                "Soyeong Jeong",
                "Kangsan Kim",
                "Jinheon Baek",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05874.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rag",
                    "#interpretability",
                    "#hallucinations",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VideoRAG: Обогащение генерации ответов с помощью видеоконтента",
                    "desc": "VideoRAG - это новая система для улучшения генерации ответов с использованием видеоконтента. В отличие от существующих подходов, она динамически извлекает релевантные видео и использует как визуальную, так и текстовую информацию из них. VideoRAG основан на Больших Видеоязыковых Моделях (LVLM), которые позволяют напрямую обрабатывать видеоконтент. Экспериментальные результаты показывают превосходство VideoRAG над существующими методами."
                },
                "en": {
                    "title": "Enhancing Generation with Dynamic Video Retrieval",
                    "desc": "This paper presents VideoRAG, a new framework that enhances the Retrieval-Augmented Generation (RAG) approach by incorporating video content into the generation process. Unlike previous methods that primarily focused on text or predefined videos, VideoRAG dynamically retrieves relevant videos based on the user's query. It leverages both visual and textual information from the videos, allowing for a richer and more accurate output generation. The framework utilizes Large Video Language Models (LVLMs) to effectively process and integrate video content, demonstrating superior performance compared to existing methods."
                },
                "zh": {
                    "title": "视频检索增强生成：提升多模态知识的利用",
                    "desc": "检索增强生成（RAG）是一种强大的策略，用于解决基础模型生成事实不准确输出的问题。现有的RAG方法主要集中在文本信息上，最近的一些进展开始考虑图像，但大多数忽视了视频这一丰富的多模态知识源。我们提出了VideoRAG框架，它不仅根据查询动态检索相关视频，还利用视频的视觉和文本信息进行输出生成。实验结果验证了VideoRAG的有效性，显示其优于相关基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.05727",
            "title": "Enabling Scalable Oversight via Self-Evolving Critic",
            "url": "https://huggingface.co/papers/2501.05727",
            "abstract": "Despite their remarkable performance, the development of Large Language Models (LLMs) faces a critical challenge in scalable oversight: providing effective feedback for tasks where human evaluation is difficult or where LLMs outperform humans. While there is growing interest in using LLMs for critique, current approaches still rely on human annotations or more powerful models, leaving the issue of enhancing critique capabilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that enables genuine self-evolution of critique abilities. Technically, SCRIT self-improves by training on synthetic data, generated by a contrastive-based self-critic that uses reference solutions for step-by-step critique, and a self-validation mechanism that ensures critique quality through correction outcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs, SCRIT achieves up to a 10.3\\% improvement on critique-correction and error identification benchmarks. Our analysis reveals that SCRIT's performance scales positively with data and model size, outperforms alternative approaches, and benefits critically from its self-validation component.",
            "score": 12,
            "issue_id": 1626,
            "pub_date": "2025-01-10",
            "pub_date_card": {
                "ru": "10 января",
                "en": "January 10",
                "zh": "1月10日"
            },
            "hash": "5a9e3b95b6aa1312",
            "authors": [
                "Zhengyang Tang",
                "Ziniu Li",
                "Zhenyang Xiao",
                "Tian Ding",
                "Ruoyu Sun",
                "Benyou Wang",
                "Dayiheng Liu",
                "Fei Huang",
                "Tianyu Liu",
                "Bowen Yu",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Inc., Beijing, China",
                "Shenzhen Research Institute of Big Data, Shenzhen, China",
                "The Chinese University of Hong Kong, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.05727.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#rlhf",
                    "#synthetic"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "SCRIT: Самосовершенствующийся критик для LLM",
                    "desc": "SCRIT - это новая система для улучшения способностей больших языковых моделей (LLM) к самокритике без внешнего надзора. Она использует синтетические данные, созданные с помощью самокритика на основе контрастного обучения и механизма самопроверки. Реализованная на базе Qwen2.5-72B-Instruct, SCRIT демонстрирует значительное улучшение в задачах критики-коррекции и идентификации ошибок. Анализ показывает, что производительность SCRIT растет с увеличением объема данных и размера модели."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Evolving Critique",
                    "desc": "This paper addresses the challenge of providing effective feedback for Large Language Models (LLMs) in tasks where human evaluation is difficult. It introduces SCRIT (Self-evolving CRITic), a framework that enhances the critique capabilities of LLMs without relying on external supervision. SCRIT utilizes synthetic data generated by a contrastive-based self-critic and incorporates a self-validation mechanism to ensure the quality of critiques. The results show that SCRIT significantly improves critique-correction and error identification benchmarks, demonstrating its effectiveness as LLMs scale in size and data."
                },
                "zh": {
                    "title": "自我进化，提升批评能力！",
                    "desc": "尽管大型语言模型（LLMs）表现出色，但在可扩展监督方面面临挑战，特别是在难以进行人类评估的任务中。本文提出了SCRIT（自我进化批评者）框架，旨在提升模型的自我批评能力。SCRIT通过对比自我批评生成合成数据，并利用自我验证机制确保批评质量，从而实现自我改进。实验结果表明，SCRIT在批评纠正和错误识别基准上提高了10.3%的性能，且其表现随着数据和模型规模的增加而提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03841",
            "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
            "url": "https://huggingface.co/papers/2501.03841",
            "abstract": "The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation. Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints. In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM fine-tuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation.",
            "score": 5,
            "issue_id": 1628,
            "pub_date": "2025-01-07",
            "pub_date_card": {
                "ru": "7 января",
                "en": "January 7",
                "zh": "1月7日"
            },
            "hash": "c2dc8cc20b9b990a",
            "authors": [
                "Mingjie Pan",
                "Jiyao Zhang",
                "Tianshu Wu",
                "Yinghao Zhao",
                "Wenlong Gao",
                "Hao Dong"
            ],
            "affiliations": [
                "AgiBot",
                "CFCS, School of CS, Peking University",
                "PKU-AgiBot Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03841.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#robotics",
                    "#3d",
                    "#transfer_learning",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Объектно-ориентированный подход к роботизированной манипуляции с использованием VLM",
                    "desc": "Статья представляет новый подход к робототехнике, объединяющий возможности моделей визуального языка (VLM) с точным 3D-пониманием, необходимым для манипуляций. Авторы предлагают объектно-ориентированное представление, использующее каноническое пространство объекта для описания примитивов взаимодействия. Система включает два цикла: планирование высокого уровня с использованием VLM и низкоуровневое выполнение с отслеживанием 6D-позы. Эксперименты показывают сильную обобщающую способность в различных задачах робототехнической манипуляции."
                },
                "en": {
                    "title": "Bridging High-Level Reasoning and Low-Level Manipulation in Robotics",
                    "desc": "This paper addresses the challenge of enabling robots to manipulate objects in unpredictable environments by enhancing Vision-Language Models (VLM) with a new approach. The authors propose a Vision-Language-Action Model (VLA) that utilizes an object-centric representation, focusing on an object's canonical space defined by its functional affordances. This representation helps translate high-level reasoning from VLM into specific 3D spatial actions needed for manipulation tasks. The proposed dual closed-loop system allows for effective planning and execution without the need for extensive fine-tuning, demonstrating strong performance in various robotic tasks."
                },
                "zh": {
                    "title": "打破高层推理与低层操作的壁垒",
                    "desc": "本论文探讨了在非结构化环境中操作的通用机器人系统的开发挑战。虽然视觉-语言模型（VLM）在高层次的常识推理方面表现出色，但缺乏精细的三维空间理解能力。我们提出了一种新颖的以对象为中心的表示方法，旨在弥合VLM的高层推理与操作所需的低层精度之间的差距。通过引入双闭环、开放词汇的机器人操作系统，我们实现了高效的实时控制，且无需对VLM进行微调。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.06186",
            "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2501.06186",
            "abstract": "Reasoning is a fundamental capability for solving complex multi-step problems, particularly in visual contexts where sequential step-wise understanding is essential. Existing approaches lack a comprehensive framework for evaluating visual reasoning and do not emphasize step-wise problem-solving. To this end, we propose a comprehensive framework for advancing step-by-step visual reasoning in large language models (LMMs) through three key contributions. First, we introduce a visual reasoning benchmark specifically designed to evaluate multi-step reasoning tasks. The benchmark presents a diverse set of challenges with eight different categories ranging from complex visual perception to scientific reasoning with over 4k reasoning steps in total, enabling robust evaluation of LLMs' abilities to perform accurate and interpretable visual reasoning across multiple steps. Second, we propose a novel metric that assesses visual reasoning quality at the granularity of individual steps, emphasizing both correctness and logical coherence. The proposed metric offers deeper insights into reasoning performance compared to traditional end-task accuracy metrics. Third, we present a new multimodal visual reasoning model, named LlamaV-o1, trained using a multi-step curriculum learning approach, where tasks are progressively organized to facilitate incremental skill acquisition and problem-solving. The proposed LlamaV-o1 is designed for multi-step reasoning and learns step-by-step through a structured training paradigm. Extensive experiments show that our LlamaV-o1 outperforms existing open-source models and performs favorably against close-source proprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an average score of 67.3 with an absolute gain of 3.8\\% across six benchmarks while being 5 times faster during inference scaling. Our benchmark, model, and code are publicly available.",
            "score": 3,
            "issue_id": 1626,
            "pub_date": "2025-01-10",
            "pub_date_card": {
                "ru": "10 января",
                "en": "January 10",
                "zh": "1月10日"
            },
            "hash": "40e1a0d2c562cda5",
            "authors": [
                "Omkar Thawakar",
                "Dinura Dissanayake",
                "Ketan More",
                "Ritesh Thawkar",
                "Ahmed Heakl",
                "Noor Ahsan",
                "Yuhao Li",
                "Mohammed Zumri",
                "Jean Lahoud",
                "Rao Muhammad Anwer",
                "Hisham Cholakkal",
                "Ivan Laptev",
                "Mubarak Shah",
                "Fahad Shahbaz Khan",
                "Salman Khan"
            ],
            "affiliations": [
                "Australian National University",
                "Linköping University",
                "Mohamed bin Zayed University of AI",
                "University of Central Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.06186.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Шаг за шагом к совершенному визуальному рассуждению",
                    "desc": "Статья представляет комплексный подход к улучшению пошагового визуального рассуждения в больших языковых моделях (LLM). Авторы вводят новый бенчмарк для оценки многошаговых задач визуального рассуждения и метрику для оценки качества рассуждения на уровне отдельных шагов. Они также предлагают новую мультимодальную модель визуального рассуждения LlamaV-o1, обученную с использованием подхода многоступенчатого куррикулярного обучения. Эксперименты показывают, что LlamaV-o1 превосходит существующие модели с открытым исходным кодом и демонстрирует хорошие результаты по сравнению с проприетарными моделями."
                },
                "en": {
                    "title": "Advancing Step-by-Step Visual Reasoning in LLMs",
                    "desc": "This paper introduces a new framework to enhance visual reasoning in large language models (LLMs) by focusing on step-by-step problem-solving. It presents a visual reasoning benchmark with over 4,000 reasoning steps across eight categories, allowing for thorough evaluation of LLMs' multi-step reasoning capabilities. Additionally, a novel metric is proposed to assess the quality of visual reasoning at each step, providing insights beyond traditional accuracy measures. The authors also introduce LlamaV-o1, a multimodal model trained with a curriculum learning approach, which shows significant performance improvements over existing models."
                },
                "zh": {
                    "title": "提升视觉推理能力的全新框架",
                    "desc": "本论文提出了一种新的框架，旨在提升大型语言模型（LLMs）在视觉推理中的逐步推理能力。我们设计了一个视觉推理基准，包含多达4000个推理步骤，涵盖复杂的视觉感知和科学推理等八个类别，以便全面评估模型的推理能力。我们还提出了一种新颖的度量标准，专注于逐步推理的正确性和逻辑一致性，提供比传统的任务准确率更深入的洞察。最后，我们介绍了名为LlamaV-o1的多模态视觉推理模型，通过逐步课程学习的方法进行训练，显著提升了推理性能。"
                }
            }
        }
    ],
    "link_prev": "2025-01-10.html",
    "link_next": "2025-01-14.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "10.01",
        "en": "01/10",
        "zh": "1月10日"
    },
    "short_date_next": {
        "ru": "14.01",
        "en": "01/14",
        "zh": "1月14日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章反驳了“GAN难以训练”的说法。作者提出了一种新的、规范的GAN损失函数，解决了之前通过临时技巧处理的模式丢失和不收敛问题。作者证明了这个损失函数的局部收敛性，并用它替换了常见GAN中的过时架构。通过StyleGAN2的例子，作者展示了简化和现代化的路线图，得到了一个新的最小化基准R3GAN。尽管简单，但这种方法在多个数据集上超越了StyleGAN2，并与最先进的GAN和扩散模型相媲美。",
        "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
        "pinyin": "这篇文章反驳了“GAN难以训练”的说法。\nZhè piān wénzhāng fǎnbó le “GAN nányǐ xùnliàn” de shuōfǎ.\n\n作者提出了一种新的、规范的GAN损失函数，解决了之前通过临时技巧处理的模式丢失和不收敛问题。\nZuòzhě tíchū le yī zhǒng xīn de, guīfàn de GAN sǔnshī hánshù, jiějué le zhīqián tōngguò línshí jìqiǎo chǔlǐ de móshì diūshī hé bù shōuliǎn wèntí.\n\n作者证明了这个损失函数的局部收敛性，并用它替换了常见GAN中的过时架构。\nZuòzhě zhèngmíng le zhè ge sǔnshī hánshù de júbù shōuliǎn xìng, bìng yòng tā tìhuàn le chángjiàn GAN zhōng de guòshí jiàgòu.\n\n通过StyleGAN2的例子，作者展示了简化和现代化的路线图，得到了一个新的最小化基准R3GAN。\nTōngguò StyleGAN2 de lìzi, zuòzhě zhǎnshì le jiǎnhuà hé xiàndàihuà de lùxiàn tú, dédào le yī ge xīn de zuìxiǎohuà jīzhǔn R3GAN.\n\n尽管简单，但这种方法在多个数据集上超越了StyleGAN2，并与最先进的GAN和扩散模型相媲美。\nJǐnguǎn jiǎndān, dàn zhè zhǒng fāngfǎ zài duō ge shùjùjí shàng chāoyuè le StyleGAN2, bìng yǔ zuì xiānjìn de GAN hé kuòsàn móxíng xiāng bǐměi.",
        "vocab": "[{'word': '反驳', 'pinyin': 'fǎn bó', 'trans': 'refute'}, {'word': 'GAN', 'pinyin': 'GAN', 'trans': 'Generative Adversarial Network'}, {'word': '难以', 'pinyin': 'nán yǐ', 'trans': 'difficult to'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'}, {'word': '说法', 'pinyin': 'shuō fǎ', 'trans': 'saying'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '规范', 'pinyin': 'guī fàn', 'trans': 'standard'}, {'word': '损失函数', 'pinyin': 'sǔn shī hán shù', 'trans': 'loss function'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '之前', 'pinyin': 'zhī qián', 'trans': 'before'}, {'word': '临时', 'pinyin': 'lín shí', 'trans': 'temporary'}, {'word': '技巧', 'pinyin': 'jì qiǎo', 'trans': 'skill'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'handle'}, {'word': '模式', 'pinyin': 'mó shì', 'trans': 'pattern'}, {'word': '丢失', 'pinyin': 'diū shī', 'trans': 'lose'}, {'word': '不收敛', 'pinyin': 'bù shōu liǎn', 'trans': 'not converge'}, {'word': '问题', 'pinyin': 'wèn tí', 'trans': 'problem'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'}, {'word': '收敛性', 'pinyin': 'shōu liǎn xìng', 'trans': 'convergence'}, {'word': '替换', 'pinyin': 'tì huàn', 'trans': 'replace'}, {'word': '常见', 'pinyin': 'cháng jiàn', 'trans': 'common'}, {'word': '过时', 'pinyin': 'guò shí', 'trans': 'outdated'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '通过', 'pinyin': 'tōng guò', 'trans': 'through'}, {'word': 'StyleGAN2', 'pinyin': 'StyleGAN2', 'trans': 'StyleGAN2'}, {'word': '例子', 'pinyin': 'lì zi', 'trans': 'example'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'display'}, {'word': '简化', 'pinyin': 'jiǎn huà', 'trans': 'simplify'}, {'word': '现代化', 'pinyin': 'xiàn dài huà', 'trans': 'modernize'}, {'word': '路线图', 'pinyin': 'lù xiàn tú', 'trans': 'roadmap'}, {'word': '得到', 'pinyin': 'dé dào', 'trans': 'obtain'}, {'word': '最小化', 'pinyin': 'zuì xiǎo huà', 'trans': 'minimize'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': 'R3GAN', 'pinyin': 'R3GAN', 'trans': 'R3GAN'}, {'word': '尽管', 'pinyin': 'jìn guǎn', 'trans': 'although'}, {'word': '但', 'pinyin': 'dàn', 'trans': 'but'}, {'word': '这种', 'pinyin': 'zhè zhǒng', 'trans': 'this'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '多个', 'pinyin': 'duō gè', 'trans': 'multiple'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '超越', 'pinyin': 'chāo yuè', 'trans': 'surpass'}, {'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '相媲美', 'pinyin': 'xiāng pì měi', 'trans': 'compare favorably'}]",
        "trans": "This article refutes the claim that \"GANs are difficult to train.\" The author proposes a new, regularized GAN loss function that addresses the issues of mode collapse and non-convergence, which were previously handled through ad-hoc tricks. The author demonstrates the local convergence of this loss function and uses it to replace outdated architectures in common GANs. Through the example of StyleGAN2, the author presents a simplified and modernized roadmap, resulting in a new minimized benchmark, R3GAN. Despite its simplicity, this method outperforms StyleGAN2 on multiple datasets and is comparable to state-of-the-art GANs and diffusion models.",
        "update_ts": "2025-01-12 12:39"
    }
}