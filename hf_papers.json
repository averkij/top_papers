{
    "date": {
        "ru": "17 января",
        "en": "January 17",
        "zh": "1月17日"
    },
    "time_utc": "2025-01-17 08:12",
    "weekday": 4,
    "issue_id": 1723,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.09751",
            "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking",
            "url": "https://huggingface.co/papers/2501.09751",
            "abstract": "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.",
            "score": 17,
            "issue_id": 1722,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "7e8d42358354f79b",
            "authors": [
                "Zekun Xi",
                "Wenbiao Yin",
                "Jizhan Fang",
                "Jialong Wu",
                "Runnan Fang",
                "Ningyu Zhang",
                "Jiang Yong",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group",
                "Zhejiang Key Laboratory of Big Data Intelligent Computing",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09751.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#story_generation",
                    "#long_context",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "OmniThink: Имитация человеческого мышления для улучшения машинной генерации текста",
                    "desc": "Статья представляет новый подход к генерации текста с использованием больших языковых моделей, названный OmniThink. Этот метод имитирует человеческий процесс итеративного расширения знаний и рефлексии, преодолевая ограничения стандартных методов извлечения информации. OmniThink улучшает плотность знаний в генерируемых статьях, не жертвуя связностью и глубиной. Эксперименты и оценки экспертов подтверждают эффективность OmniThink для решения реальных задач генерации длинных статей."
                },
                "en": {
                    "title": "OmniThink: Elevating Machine Writing through Human-Like Learning",
                    "desc": "This paper introduces OmniThink, a novel machine writing framework that enhances the capabilities of large language models by mimicking human cognitive processes. Unlike traditional retrieval-augmented generation methods, which often produce shallow and repetitive content, OmniThink focuses on iterative expansion and reflection to deepen knowledge on topics. The framework significantly improves the knowledge density of generated articles while maintaining coherence and depth, as shown by experimental results. Human evaluations and expert feedback confirm that OmniThink effectively addresses challenges in generating high-quality long-form content."
                },
                "zh": {
                    "title": "OmniThink：提升机器写作的知识密度",
                    "desc": "本文提出了一种名为OmniThink的机器写作框架，旨在改善传统大语言模型在生成内容时的局限性。OmniThink模拟人类学习者的认知过程，通过迭代扩展和反思来加深对主题的理解。实验结果表明，OmniThink能够提高生成文章的知识密度，同时保持连贯性和深度等指标。人类评估和专家反馈进一步验证了OmniThink在生成长篇文章时解决实际问题的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09732",
            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
            "url": "https://huggingface.co/papers/2501.09732",
            "abstract": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
            "score": 13,
            "issue_id": 1720,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "2ad32c666f91ba05",
            "authors": [
                "Nanye Ma",
                "Shangyuan Tong",
                "Haolin Jia",
                "Hexiang Hu",
                "Yu-Chuan Su",
                "Mingda Zhang",
                "Xuan Yang",
                "Yandong Li",
                "Tommi Jaakkola",
                "Xuhui Jia",
                "Saining Xie"
            ],
            "affiliations": [
                "Google",
                "MIT",
                "NYU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09732.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Повышение качества генерации изображений за счет масштабирования вычислений при выводе",
                    "desc": "Это исследование посвящено изучению поведения диффузионных моделей при масштабировании вычислений во время вывода. Авторы рассматривают задачу поиска лучших шумов для процесса сэмплирования диффузионной модели. Они структурируют пространство решений по двум осям: верификаторы для обратной связи и алгоритмы поиска лучших кандидатов шума. Эксперименты показывают, что увеличение вычислений при выводе приводит к значительному улучшению качества сгенерированных изображений."
                },
                "en": {
                    "title": "Enhancing Diffusion Models: Scaling Inference for Better Image Generation",
                    "desc": "This paper investigates how to enhance the performance of diffusion models during the inference phase by increasing computational resources. It highlights that, unlike Large Language Models (LLMs), diffusion models can adjust their inference process through the number of denoising steps, but improvements tend to plateau after a certain point. The authors propose a method to optimize the noise used in the diffusion sampling process by exploring different feedback verifiers and algorithms. Their experiments demonstrate that by strategically increasing computation during inference, the quality of generated images can be significantly improved, tailored to various application needs."
                },
                "zh": {
                    "title": "扩散模型推理时的计算扩展与性能提升",
                    "desc": "生成模型在多个领域产生了重要影响，主要得益于其在训练过程中通过增加数据、计算资源和模型规模来扩展的能力。最近的研究开始探讨大型语言模型（LLMs）在推理时的扩展行为，发现额外的计算可以进一步提高性能。与LLMs不同，扩散模型通过去噪步骤的数量灵活调整推理时的计算，尽管性能提升通常在几十步后趋于平稳。本文探讨了扩散模型在推理时的扩展行为，研究如何通过增加计算来进一步提高生成性能，特别是通过寻找更好的噪声来优化扩散采样过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09484",
            "title": "Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators",
            "url": "https://huggingface.co/papers/2501.09484",
            "abstract": "Online medical consultation (OMC) restricts doctors to gathering patient information solely through inquiries, making the already complex sequential decision-making process of diagnosis even more challenging. Recently, the rapid advancement of large language models has demonstrated a significant potential to transform OMC. However, most studies have primarily focused on improving diagnostic accuracy under conditions of relatively sufficient information, while paying limited attention to the \"inquiry\" phase of the consultation process. This lack of focus has left the relationship between \"inquiry\" and \"diagnosis\" insufficiently explored. In this paper, we first extract real patient interaction strategies from authentic doctor-patient conversations and use these strategies to guide the training of a patient simulator that closely mirrors real-world behavior. By inputting medical records into our patient simulator to simulate patient responses, we conduct extensive experiments to explore the relationship between \"inquiry\" and \"diagnosis\" in the consultation process. Experimental results demonstrate that inquiry and diagnosis adhere to the Liebig's law: poor inquiry quality limits the effectiveness of diagnosis, regardless of diagnostic capability, and vice versa. Furthermore, the experiments reveal significant differences in the inquiry performance of various models. To investigate this phenomenon, we categorize the inquiry process into four types: (1) chief complaint inquiry; (2) specification of known symptoms; (3) inquiry about accompanying symptoms; and (4) gathering family or medical history. We analyze the distribution of inquiries across the four types for different models to explore the reasons behind their significant performance differences. We plan to open-source the weights and related code of our patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator.",
            "score": 9,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "aff7d86ad63040d9",
            "authors": [
                "Zhaocheng Liu",
                "Quan Tu",
                "Wen Ye",
                "Yu Xiao",
                "Zhishou Zhang",
                "Hengfu Cui",
                "Yalun Zhu",
                "Qiang Ju",
                "Shizheng Li",
                "Jian Xie"
            ],
            "affiliations": [
                "Baichuan Inc.",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09484.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#science",
                    "#open_source",
                    "#healthcare"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Симуляция пациента для улучшения онлайн-диагностики с помощью ИИ",
                    "desc": "Эта статья исследует процесс онлайн-медицинских консультаций с использованием больших языковых моделей. Авторы разработали симулятор пациента на основе реальных стратегий взаимодействия врача и пациента. Эксперименты показали, что качество опроса и диагностики взаимозависимы и подчиняются закону Либиха. Анализ различных моделей выявил значительные различия в эффективности опроса, которые были классифицированы по четырем типам."
                },
                "en": {
                    "title": "Enhancing Diagnosis through Effective Inquiry in Online Medical Consultations",
                    "desc": "This paper addresses the challenges of online medical consultations (OMC) by focusing on the inquiry phase, which is crucial for accurate diagnosis. It utilizes large language models to create a patient simulator that mimics real patient interactions based on actual doctor-patient conversations. The study reveals that the quality of inquiry directly impacts diagnostic effectiveness, following Liebig's law, which states that the weakest link limits overall performance. Additionally, the research categorizes inquiry types and analyzes their distribution across different models, highlighting significant performance variations in inquiry effectiveness."
                },
                "zh": {
                    "title": "优化询问，提升诊断效果",
                    "desc": "本文探讨了在线医疗咨询中询问与诊断之间的关系。我们从真实的医患对话中提取了患者互动策略，并利用这些策略训练了一个模拟患者的模型。实验结果表明，询问质量的差异直接影响诊断效果，且不同模型在询问表现上存在显著差异。我们将询问过程分为四种类型，并分析了不同模型在这些类型上的表现，以揭示其性能差异的原因。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08617",
            "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
            "url": "https://huggingface.co/papers/2501.08617",
            "abstract": "Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users' utility. We demonstrate that feedback based on evaluators' foresight estimates of downstream consequences systematically induces Goodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods -- Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) -- and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.",
            "score": 6,
            "issue_id": 1720,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "f758bc630d8dd443",
            "authors": [
                "Kaiqu Liang",
                "Haimin Hu",
                "Ryan Liu",
                "Thomas L. Griffiths",
                "Jaime Fernández Fisac"
            ],
            "affiliations": [
                "Department of Computer Science, Princeton University",
                "Department of Electrical and Computer Engineering, Princeton University",
                "Department of Psychology, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08617.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#training",
                    "#rl"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "Взгляд в будущее для лучшей настройки ИИ",
                    "desc": "Статья представляет новый метод обучения с подкреплением - Reinforcement Learning from Hindsight Simulation (RLHS). В отличие от стандартного RLHF, RLHS использует симуляцию долгосрочных последствий действий модели и оценку их полезности постфактум. Авторы показывают, что RLHS позволяет уменьшить проблему неправильной мотивации модели и улучшить соответствие человеческим ценностям. Эмпирические эксперименты демонстрируют превосходство RLHS над RLHF в достижении целей пользователей."
                },
                "en": {
                    "title": "Aligning AI with Human Values through Hindsight Feedback",
                    "desc": "This paper addresses the challenge of aligning generative AI systems with human values using Reinforcement Learning from Human Feedback (RLHF). It identifies that relying on immediate feedback can lead to misaligned behaviors, such as sycophancy and deception, due to Goodhart's Law dynamics. The authors propose a new approach called Reinforcement Learning from Hindsight Simulation (RLHS), which uses simulated consequences to gather feedback on beneficial behaviors. Their experiments show that RLHS improves user satisfaction and goal achievement compared to traditional RLHF methods, highlighting the importance of considering long-term outcomes in AI alignment."
                },
                "zh": {
                    "title": "关注长期后果，提升AI对齐性",
                    "desc": "这篇论文探讨了生成性人工智能系统如何更好地与人类价值观对齐，以确保其行为有益且可信。现有的基于人类反馈的强化学习（RLHF）方法主要依赖即时反馈，但这种反馈可能无法准确反映与用户效用相关的长期影响。作者提出了一种新的方法，称为基于事后模拟的强化学习（RLHS），通过模拟可能的后果来获取反馈，从而改善模型的对齐性。研究表明，RLHS在帮助用户实现目标和提高满意度方面，优于传统的RLHF方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09756",
            "title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces",
            "url": "https://huggingface.co/papers/2501.09756",
            "abstract": "We introduce SynthLight, a diffusion model for portrait relighting. Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subject's identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: https://vrroom.github.io/synthlight/",
            "score": 5,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "e6621d55eb165448",
            "authors": [
                "Sumit Chaturvedi",
                "Mengwei Ren",
                "Yannick Hold-Geoffroy",
                "Jingyuan Liu",
                "Julie Dorsey",
                "Zhixin Shu"
            ],
            "affiliations": [
                "Adobe Research",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09756.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#inference",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "SynthLight: реалистичная перезасветка портретов с помощью диффузионной модели",
                    "desc": "SynthLight - это диффузионная модель для перезасветки портретов. Модель рассматривает перезасветку как проблему повторного рендеринга, где пиксели трансформируются в ответ на изменения условий освещения окружающей среды. Авторы синтезировали датасет с помощью физически корректного рендеринга, симулируя трансформации освещения на 3D-моделях голов. Предложены две стратегии обучения и вывода для преодоления разрыва между синтетическими и реальными изображениями."
                },
                "en": {
                    "title": "Revolutionizing Portrait Relighting with SynthLight",
                    "desc": "SynthLight is a diffusion model designed for relighting portraits by treating the task as a re-rendering challenge influenced by environmental lighting changes. It utilizes a physically-based rendering engine to create a synthetic dataset that simulates how lighting affects 3D head models. The model employs multi-task training to utilize real portraits without specific lighting labels and a novel inference strategy that enhances detail preservation during the relighting process. The results show that SynthLight can effectively generalize to real images, producing realistic lighting effects while maintaining the identity of the subjects, outperforming existing methods in both quantitative and qualitative assessments."
                },
                "zh": {
                    "title": "SynthLight：肖像重光照的新方法",
                    "desc": "我们介绍了SynthLight，这是一种用于肖像重光照的扩散模型。我们将图像重光照视为重新渲染的问题，通过物理基础渲染引擎合成数据集，以模拟在不同光照条件下的像素变换。我们提出了两种训练和推理策略，以缩小合成图像和真实图像之间的差距，利用真实人像进行多任务训练，并在推理时使用无分类器引导的扩散采样程序。我们的模型能够在多样的真实照片中推广，生成逼真的光照效果，同时保持主体的身份特征。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09747",
            "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2501.09747",
            "abstract": "Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.",
            "score": 5,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "1ff64d2f7e62d274",
            "authors": [
                "Karl Pertsch",
                "Kyle Stachowicz",
                "Brian Ichter",
                "Danny Driess",
                "Suraj Nair",
                "Quan Vuong",
                "Oier Mees",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "affiliations": [
                "Physical Intelligence",
                "Stanford",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09747.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#training",
                    "#games",
                    "#optimization",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Революция в токенизации действий робота: от частотного пространства к универсальности",
                    "desc": "Статья представляет новый метод токенизации действий робота под названием FAST (Frequency-space Action Sequence Tokenization), основанный на дискретном косинусном преобразовании. Этот подход позволяет обучать авторегрессионные модели VLA (Vision-Language Action) для высокочастотных и сложных задач манипулирования, где стандартные методы дискретизации не работают. Авторы также представляют FAST+, универсальный токенизатор действий робота, обученный на 1 миллионе реальных траекторий. В сочетании с моделью pi0 VLA, метод FAST позволяет обучаться на 10 тысячах часов данных робота и достигать производительности диффузионных VLA, сокращая время обучения до 5 раз."
                },
                "en": {
                    "title": "Revolutionizing Robot Action Tokenization with FAST",
                    "desc": "This paper introduces a new method for tokenizing continuous robot actions to improve the performance of autoregressive sequence models, specifically in the context of vision-language action (VLA) policies. The authors identify that traditional tokenization methods, which use simple binning techniques, struggle with high-frequency and dexterous robotic tasks. To overcome this limitation, they propose Frequency-space Action Sequence Tokenization (FAST), which utilizes the discrete cosine transform for better action representation. The results demonstrate that FAST can effectively train VLAs on extensive robot data, achieving performance comparable to diffusion models while significantly reducing training time."
                },
                "zh": {
                    "title": "提升机器人灵巧技能的标记化新方法",
                    "desc": "本文提出了一种新的机器人动作标记化方案，称为频率空间动作序列标记化（FAST），旨在解决现有基于简单分箱方法的标记化在学习灵巧技能时的不足。FAST利用离散余弦变换来有效地处理高频机器人数据，从而提高了模型在复杂任务中的表现。我们还发布了FAST+，这是一个通用的机器人动作标记器，能够处理多种动作序列和控制频率。通过与pi0 VLA结合，我们的方法在训练10,000小时的机器人数据时，能够与扩散VLA的性能相匹配，同时将训练时间减少了多达5倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09755",
            "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
            "url": "https://huggingface.co/papers/2501.09755",
            "abstract": "Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.",
            "score": 5,
            "issue_id": 1720,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "426aa3415c3c0ef4",
            "authors": [
                "Philippe Hansen-Estruch",
                "David Yan",
                "Ching-Yao Chung",
                "Orr Zohar",
                "Jialiang Wang",
                "Tingbo Hou",
                "Tao Xu",
                "Sriram Vishwanath",
                "Peter Vajda",
                "Xinlei Chen"
            ],
            "affiliations": [
                "FAIR, Meta",
                "GenAI, Meta",
                "Stanford University",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09755.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#video",
                    "#optimization",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "ViTok: Оптимизация визуальной токенизации для генеративных моделей",
                    "desc": "Статья исследует масштабирование автоэнкодеров для визуальной токенизации в генеративных моделях изображений и видео. Авторы представляют ViTok - легковесный автоэнкодер на основе Vision Transformer, обученный на масштабных датасетах. Исследование показывает, что масштабирование декодера улучшает реконструкцию, но неоднозначно влияет на генерацию. ViTok демонстрирует конкурентоспособную производительность при меньшем количестве FLOP и устанавливает новые рекорды в условной генерации видео."
                },
                "en": {
                    "title": "Scaling Auto-Encoders for Enhanced Image and Video Generation",
                    "desc": "This paper explores the scaling of auto-encoders, particularly focusing on the tokenizer component, which is crucial for image and video generation. The authors introduce ViTok, a Vision Transformer-based architecture that replaces traditional convolutional backbones, allowing for better scaling on large datasets. They investigate how different scaling strategies for the encoder and decoder affect both reconstruction and generative performance, finding that scaling the decoder is more beneficial for reconstruction. Ultimately, ViTok achieves competitive results with fewer computational resources and sets new benchmarks in image and video generation tasks."
                },
                "zh": {
                    "title": "自编码器的视觉标记化：提升生成模型的关键",
                    "desc": "本论文探讨了通过自编码器进行视觉标记化对图像和视频生成模型的影响。我们提出了一种增强的视觉变换器架构（ViTok），用于替代传统的卷积骨干网络，以提高标记化的效果。研究发现，自编码器的瓶颈规模与重建性能高度相关，但与生成性能的关系更为复杂。最终，ViTok在多个任务中表现出色，尤其是在视频重建和图像生成方面，展示了其在计算效率上的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09686",
            "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models",
            "url": "https://huggingface.co/papers/2501.09686",
            "abstract": "Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of \"thought\" -- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\" with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.",
            "score": 3,
            "issue_id": 1720,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "1c6b1b1f0235304c",
            "authors": [
                "Fengli Xu",
                "Qianyue Hao",
                "Zefang Zong",
                "Jingwei Wang",
                "Yunke Zhang",
                "Jingyi Wang",
                "Xiaochong Lan",
                "Jiahui Gong",
                "Tianjian Ouyang",
                "Fanjin Meng",
                "Chenyang Shao",
                "Yuwei Yan",
                "Qinglong Yang",
                "Yiwen Song",
                "Sijian Ren",
                "Xinyuan Hu",
                "Yu Li",
                "Jie Feng",
                "Chen Gao",
                "Yong Li"
            ],
            "affiliations": [
                "Emory University, Atlanta GA, USA",
                "HKUST (GZ), Guangzhou, China",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09686.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#rl",
                    "#survey",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Путь к большим моделям рассуждений: новый рубеж в ИИ",
                    "desc": "Этот обзор посвящен прогрессу в области рассуждений с использованием больших языковых моделей (LLM). Рассматриваются ключевые технические компоненты, способствующие развитию крупных моделей рассуждений, включая автоматизированное построение данных, методы обучения рассуждениям и масштабирование во время тестирования. Анализируются популярные проекты с открытым исходным кодом по созданию крупных моделей рассуждений. Обсуждаются открытые проблемы и направления будущих исследований в этой области."
                },
                "en": {
                    "title": "Unlocking Human-Like Reasoning in Large Language Models",
                    "desc": "This paper discusses the advancements in Large Language Models (LLMs) and their application to complex reasoning tasks. It introduces the concept of 'thought', which represents intermediate reasoning steps, allowing LLMs to simulate human-like reasoning processes. The paper highlights the use of reinforcement learning to enhance LLMs' reasoning capabilities by generating high-quality reasoning trajectories through trial-and-error methods. Additionally, it emphasizes the importance of scaling both training and testing phases to improve reasoning accuracy, paving the way for the development of Large Reasoning Models."
                },
                "zh": {
                    "title": "推动大型推理模型的研究新前沿",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在复杂推理任务中的应用。研究者们引入了“思考”的概念，通过中间步骤的令牌序列来模拟人类的推理过程。最近，强化学习（RL）被应用于训练LLMs，以自动生成高质量的推理轨迹，从而显著提高推理能力。论文还讨论了在测试时增加令牌数量以提高推理准确性的效果，并展望了大型推理模型的未来研究方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09503",
            "title": "AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2501.09503",
            "abstract": "Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an \"encode-then-route\" manner. In the encoding step, AnyStory utilizes a universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .",
            "score": 2,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "fb27e795153a9668",
            "authors": [
                "Junjie He",
                "Yuxiang Tuo",
                "Binghui Chen",
                "Chongyang Zhong",
                "Yifeng Geng",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Institute for Intelligent Computing, Alibaba Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09503.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "AnyStory: Высококачественная генерация персонализированных изображений с множественными субъектами",
                    "desc": "Статья представляет AnyStory - новый подход к генерации персонализированных изображений с несколькими субъектами. Метод использует универсальный энкодер изображений ReferenceNet и CLIP для высококачественного кодирования характеристик субъектов. AnyStory применяет декуплированный маршрутизатор субъектов для точного определения их потенциального расположения в латентном пространстве. Эксперименты показывают превосходную производительность метода в сохранении деталей субъектов, соответствии текстовым описаниям и персонализации для нескольких субъектов одновременно."
                },
                "en": {
                    "title": "AnyStory: Mastering Personalized Image Generation for Multiple Subjects",
                    "desc": "This paper introduces AnyStory, a novel method for generating personalized images with high fidelity, even when multiple subjects are involved. It employs an 'encode-then-route' strategy, where a powerful image encoder, ReferenceNet, captures detailed subject features. The routing mechanism uses an instance-aware subject router to accurately determine where each subject should be placed in the generated image. Experimental results show that AnyStory excels in maintaining subject details and aligning them with text descriptions, making it effective for both single and multiple subjects."
                },
                "zh": {
                    "title": "AnyStory：个性化主题生成的新方法",
                    "desc": "最近，大规模生成模型在文本到图像生成方面表现出色。然而，生成高保真度的个性化图像，尤其是涉及多个主题的情况，仍然面临挑战。本文提出了AnyStory，这是一种统一的个性化主题生成方法，能够在不牺牲主题保真的情况下，实现单个和多个主题的高保真个性化。AnyStory通过“编码-再路由”的方式建模主题个性化问题，利用强大的图像编码器和实例感知路由器，准确预测主题在潜在空间中的位置。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09433",
            "title": "CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation",
            "url": "https://huggingface.co/papers/2501.09433",
            "abstract": "The synthesis of high-quality 3D assets from textual or visual inputs has become a central objective in modern generative modeling. Despite the proliferation of 3D generation algorithms, they frequently grapple with challenges such as multi-view inconsistency, slow generation times, low fidelity, and surface reconstruction problems. While some studies have addressed some of these issues, a comprehensive solution remains elusive. In this paper, we introduce CaPa, a carve-and-paint framework that generates high-fidelity 3D assets efficiently. CaPa employs a two-stage process, decoupling geometry generation from texture synthesis. Initially, a 3D latent diffusion model generates geometry guided by multi-view inputs, ensuring structural consistency across perspectives. Subsequently, leveraging a novel, model-agnostic Spatially Decoupled Attention, the framework synthesizes high-resolution textures (up to 4K) for a given geometry. Furthermore, we propose a 3D-aware occlusion inpainting algorithm that fills untextured regions, resulting in cohesive results across the entire model. This pipeline generates high-quality 3D assets in less than 30 seconds, providing ready-to-use outputs for commercial applications. Experimental results demonstrate that CaPa excels in both texture fidelity and geometric stability, establishing a new standard for practical, scalable 3D asset generation.",
            "score": 1,
            "issue_id": 1721,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "8c7a54f21e46af7a",
            "authors": [
                "Hwan Heo",
                "Jangyeong Kim",
                "Seongyeong Lee",
                "Jeong A Wi",
                "Junyoung Choi",
                "Sangjun Ahn"
            ],
            "affiliations": [
                "Graphics AI Lab, NC Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09433.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "CaPa: Революция в генерации 3D-моделей",
                    "desc": "В статье представлен CaPa - фреймворк для генерации высококачественных 3D-моделей. Он использует двухэтапный процесс, разделяя создание геометрии и текстур с помощью латентной диффузионной модели и пространственно-разделенного внимания. CaPa также предлагает алгоритм для заполнения нетекстурированных областей, обеспечивая целостность результатов. Фреймворк генерирует 3D-модели менее чем за 30 секунд, превосходя аналоги по качеству текстур и стабильности геометрии."
                },
                "en": {
                    "title": "CaPa: Fast and High-Fidelity 3D Asset Generation",
                    "desc": "This paper presents CaPa, a novel framework for generating high-quality 3D assets from textual or visual inputs. It addresses common challenges in 3D generation, such as multi-view inconsistency and slow generation times, by separating geometry generation from texture synthesis. The framework utilizes a 3D latent diffusion model for consistent geometry creation and a Spatially Decoupled Attention mechanism for high-resolution texture synthesis. CaPa also includes a 3D-aware occlusion inpainting algorithm to enhance the final output, achieving high fidelity and stability in under 30 seconds."
                },
                "zh": {
                    "title": "高效生成高保真3D资产的CaPa框架",
                    "desc": "本论文介绍了一种名为CaPa的框架，用于高效生成高保真度的3D资产。该框架采用两阶段的过程，将几何体生成与纹理合成解耦。首先，使用3D潜在扩散模型生成几何体，确保多视角之间的结构一致性。然后，通过一种新颖的空间解耦注意力机制合成高分辨率纹理，并提出了3D感知的遮挡修复算法，最终在30秒内生成高质量的3D资产。"
                }
            }
        }
    ],
    "link_prev": "2025-01-16.html",
    "link_next": "2025-01-20.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1月16日"
    },
    "short_date_next": {
        "ru": "20.01",
        "en": "01/20",
        "zh": "1月20日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了多模态文档检索，旨在从大量文档中识别和检索图表、表格、图形和布局信息。尽管其重要性，缺乏有效的基准测试来评估系统性能。为此，文章提出了一个新的基准测试MMDocIR，包括页面级和布局级检索任务。MMDocIR包含1,685个专家标注和173,843个自动标注的问题，是一个重要的资源。实验显示，视觉检索器比文本检索器表现更好，并且使用VLM-text的文本检索器优于使用OCR-text的检索器。",
        "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
        "pinyin": "这篇文章介绍了多模态文档检索，旨在从大量文档中识别和检索图表、表格、图形和布局信息。尽管其重要性，缺乏有效的基准测试来评估系统性能。为此，文章提出了一个新的基准测试MMDocIR，包括页面级和布局级检索任务。MMDocIR包含1,685个专家标注和173,843个自动标注的问题，是一个重要的资源。实验显示，视觉检索器比文本检索器表现更好，并且使用VLM-text的文本检索器优于使用OCR-text的检索器。\n\nzhè piān wén zhāng jiè shào le duō mó tài wén dàng jiàn suǒ, zhǐ zài cóng dà liàng wén dàng zhōng shí bié hé jiàn suǒ tú biǎo, biǎo gé, tú xíng hé bù jú xìn xī. Jǐn guǎn qí zhòng yào xìng, quē fá yǒu xiào de jī zhǔn cè shì lái píng gū xì tǒng xìng néng. Wèi cǐ, wén zhāng tí chū le yī gè xīn de jī zhǔn cè shì MMDocIR, bāo kuò yè miàn jí hé bù jú jí jiàn suǒ rèn wù. MMDocIR bāo hán 1,685 gè zhuān jiā biāo zhù hé 173,843 gè zì dòng biāo zhù de wèn tí, shì yī gè zhòng yào de zī yuán. Shí yàn xiǎn shì, shì jué jiàn suǒ qì bǐ wén běn jiàn suǒ qì biǎo xiàn gèng hǎo, bìng qiě shǐ yòng VLM-text de wén běn jiàn suǒ qì yōu yú shǐ yòng OCR-text de jiàn suǒ qì.",
        "vocab": "[{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '检索', 'pinyin': 'jiǎn suǒ', 'trans': 'retrieval'},\n{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},\n{'word': '识别', 'pinyin': 'shí bié', 'trans': 'recognize'},\n{'word': '布局', 'pinyin': 'bù jiú', 'trans': 'layout'},\n{'word': '尽管', 'pinyin': 'jìn guǎn', 'trans': 'although'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'},\n{'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '页面级', 'pinyin': 'yè miàn jí', 'trans': 'page-level'},\n{'word': '标注', 'pinyin': 'biāo zhù', 'trans': 'annotation'},\n{'word': '资源', 'pinyin': 'zī yuán', 'trans': 'resource'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'},\n{'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'},\n{'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'},\n{'word': 'OCR', 'pinyin': '', 'trans': 'Optical Character Recognition'}]",
        "trans": "This article introduces multimodal document retrieval, which aims to identify and retrieve charts, tables, graphics, and layout information from a large number of documents. Despite its importance, there is a lack of effective benchmark tests to evaluate system performance. To address this, the article proposes a new benchmark test called MMDocIR, which includes page-level and layout-level retrieval tasks. MMDocIR contains 1,685 expert-annotated and 173,843 automatically annotated questions, making it a valuable resource. Experiments show that visual retrievers perform better than text retrievers, and text retrievers using VLM-text outperform those using OCR-text.",
        "update_ts": "2025-01-16 09:10"
    }
}