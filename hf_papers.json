{
    "date": {
        "ru": "30 июня",
        "en": "June 30",
        "zh": "6月30日"
    },
    "time_utc": "2025-06-30 05:15",
    "weekday": 0,
    "issue_id": 4550,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.17450",
            "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
            "url": "https://huggingface.co/papers/2506.17450",
            "abstract": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  \t\t\t\t\tAI-generated summary \t\t\t\t We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows a layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into a coherent scene using a generative compositor (compositing). Our generative compositor extends a pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks.",
            "score": 14,
            "issue_id": 4550,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 июня",
                "en": "June 20",
                "zh": "6月20日"
            },
            "hash": "b5bb4470d500be10",
            "authors": [
                "Jiacheng Chen",
                "Ramin Mehran",
                "Xuhui Jia",
                "Saining Xie",
                "Sanghyun Woo"
            ],
            "affiliations": [
                "Google DeepMind",
                "New York University",
                "Simon Fraser University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17450.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#3d",
                    "#training"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Генеративная композиция сцен с 3D-контролем",
                    "desc": "BlenderFusion - это генеративная система визуальной композиции, использующая диффузионную модель для редактирования и составления сцен. Она работает по принципу разделения на слои, редактирования и композиции, преобразуя визуальные входные данные в редактируемые 3D-объекты. Система использует предобученную диффузионную модель, дообученную на видеокадрах с применением маскирования исходного изображения и симуляции дрожания объектов. BlenderFusion значительно превосходит существующие методы в сложных задачах композиционного редактирования сцен."
                },
                "en": {
                    "title": "Revolutionizing Scene Editing with BlenderFusion",
                    "desc": "BlenderFusion is a framework that allows users to create new scenes by rearranging objects, backgrounds, and camera angles. It uses a three-step process: first, it segments visual inputs into 3D elements, then it allows for editing these elements in Blender, and finally, it combines them into a complete scene using a generative compositor. The compositor is based on a diffusion model that processes both the original and edited scenes simultaneously, enhancing the editing process. Key techniques like source masking and simulated object jittering improve flexibility and control in scene composition, leading to better results than previous methods."
                },
                "zh": {
                    "title": "生成视觉合成的新方法",
                    "desc": "BlenderFusion是一个生成视觉合成框架，能够通过重新组合对象、相机和背景来合成新场景。它采用分层-编辑-合成的流程，首先将视觉输入分割并转换为可编辑的3D实体，然后在Blender中进行3D控制的编辑，最后使用生成合成器将它们融合成一个连贯的场景。该生成合成器扩展了预训练的扩散模型，能够并行处理原始场景和编辑后的场景。BlenderFusion在复杂的合成场景编辑任务中显著优于之前的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21862",
            "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
            "url": "https://huggingface.co/papers/2506.21862",
            "abstract": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
            "score": 6,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 июня",
                "en": "June 27",
                "zh": "6月27日"
            },
            "hash": "b9ad171aa3fb5bbf",
            "authors": [
                "Boyuan Sun",
                "Jiaxing Zhao",
                "Xihan Wei",
                "Qibin Hou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group",
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21862.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#multimodal",
                    "#long_context",
                    "#dataset",
                    "#video"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Умное сжатие для умных видеомоделей",
                    "desc": "LLaVA-Scissor - это стратегия сжатия токенов для видео мультимодальных больших языковых моделей. Она использует метод Семантически Связанных Компонентов (SCC) для эффективного сжатия токенов, сохраняя при этом семантическое покрытие. LLaVA-Scissor применяет двухэтапный подход к пространственно-временному сжатию токенов, используя SCC как в пространственной, так и во временной областях. Экспериментальные результаты показывают, что LLaVA-Scissor превосходит другие методы сжатия токенов в различных задачах понимания видео."
                },
                "en": {
                    "title": "Efficient Video Understanding with Semantic Token Compression",
                    "desc": "LLaVA-Scissor is a novel token compression strategy specifically designed for video multimodal large language models. It utilizes Semantic Connected Components (SCC) to effectively group tokens into distinct semantic regions, which helps in reducing redundancy and maintaining semantic integrity. Unlike previous methods that rely on attention scores, LLaVA-Scissor compresses tokens in both spatial and temporal dimensions, ensuring comprehensive coverage of the video's content. Extensive evaluations demonstrate that this approach significantly outperforms existing token compression techniques, especially when retaining fewer tokens during video understanding tasks."
                },
                "zh": {
                    "title": "LLaVA-Scissor：高效的视频令牌压缩策略",
                    "desc": "LLaVA-Scissor是一种针对视频多模态大语言模型的令牌压缩策略。它利用语义连通组件（SCC）方法，有效地将令牌分配到不同的语义区域，从而确保全面的语义覆盖。与以往基于注意力分数的压缩方法不同，LLaVA-Scissor能够减少令牌冗余，并在空间和时间域中进行两步压缩。实验结果表明，该方法在视频理解基准测试中表现优异，尤其是在低令牌保留比率下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22434",
            "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
            "url": "https://huggingface.co/papers/2506.22434",
            "abstract": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  \t\t\t\t\tAI-generated summary \t\t\t\t This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.",
            "score": 5,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 июня",
                "en": "June 27",
                "zh": "6月27日"
            },
            "hash": "d7e89f248d4c331e",
            "authors": [
                "Xi Chen",
                "Mingkang Zhu",
                "Shaoteng Liu",
                "Xiaoyang Wu",
                "Xiaogang Xu",
                "Yu Liu",
                "Xiang Bai",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "CUHK",
                "HKU",
                "HUST",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22434.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Самообучение ИИ визуальным рассуждениям без участия человека",
                    "desc": "Статья представляет метод самообучения моделей компьютерного зрения и обработки естественного языка (VLM) для улучшения их способности рассуждать о нескольких изображениях. Авторы используют триплеты изображений и обучение с подкреплением, чтобы научить модель сравнивать тонкие визуальные детали. Этот подход не требует размеченных человеком пар вопрос-ответ и позволяет генерировать цепочки рассуждений. Эксперименты показывают, что полученные навыки обобщаются на широкий спектр задач визуального анализа."
                },
                "en": {
                    "title": "Empowering VLMs with Self-Supervised Image Triplet Learning",
                    "desc": "This paper presents a method for enhancing the reasoning capabilities of Vision-Language Models (VLMs) using self-supervised learning with image triplets. The approach involves creating pairs of augmented images along with a distinct image, allowing the model to learn to compare and reason about visual differences. By training the model to determine whether images are the same or different, it develops a reasoning process that generalizes to various tasks without needing human-annotated data. The results demonstrate that this method significantly improves performance on multi-image reasoning benchmarks and general vision tasks."
                },
                "zh": {
                    "title": "自监督学习提升视觉语言模型推理能力",
                    "desc": "这篇论文探讨了如何通过使用图像三元组的自监督学习来增强视觉语言模型（VLM）在多图像任务上的推理能力，而无需人工标注的问题-答案对。研究者们构建了由同一图像的两个增强视图和一个相似但不同的图像组成的图像三元组。在训练过程中，模型被要求生成推理过程，以比较这些图像（即判断相同或不同）。实验表明，尽管模型仅在视觉比较任务上训练，但其学习到的推理能力能够有效地推广到各种问题上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21656",
            "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
            "url": "https://huggingface.co/papers/2506.21656",
            "abstract": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.",
            "score": 3,
            "issue_id": 4548,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 июня",
                "en": "June 26",
                "zh": "6月26日"
            },
            "hash": "8d063b13fc555964",
            "authors": [
                "Yifan Shen",
                "Yuanzhe Liu",
                "Jingyuan Zhu",
                "Xu Cao",
                "Xiaofeng Zhang",
                "Yixiao He",
                "Wenming Ye",
                "James Matthew Rehg",
                "Ismini Lourentzou"
            ],
            "affiliations": [
                "Google",
                "Shanghai Jiao Tong University",
                "University of Illinois Urbana-Champaign",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21656.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#multimodal",
                    "#rlhf",
                    "#cv",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Пространственное мышление ИИ выходит на новый уровень",
                    "desc": "SpatialReasoner-R1 - это новая модель зрительно-языкового рассуждения, которая улучшает пространственное мышление с помощью мультимодельного поиска Монте-Карло по дереву и оптимизации прямых предпочтений. Модель генерирует длинные цепочки рассуждений и использует сегментированную оптимизацию предпочтений для улучшения визуальной и логической согласованности. SpatialReasoner-R1 достигает нового уровня производительности на бенчмарке SPATIALRGPT-Bench, превосходя базовые модели на 9.8% по средней точности. При этом модель сохраняет конкурентоспособность в общих задачах компьютерного зрения и обработки естественного языка."
                },
                "en": {
                    "title": "Elevating Spatial Reasoning with SpatialReasoner-R1",
                    "desc": "SpatialReasoner-R1 is a vision-language reasoning model that enhances spatial reasoning capabilities in AI. It employs Multi-Model Monte Carlo Tree Search (M3CTS) to create diverse reasoning paths that are logically consistent, improving the model's ability to handle complex spatial tasks. Additionally, it introduces fine-grained Direct Preference Optimization (fDPO), which refines the model's decision-making by focusing on specific segments of reasoning and using a spatial reward mechanism. The model achieves state-of-the-art performance on the SPATIALRGPT-Bench, significantly outperforming previous models in both spatial quality and quantity tasks."
                },
                "zh": {
                    "title": "空间推理的新突破",
                    "desc": "SpatialReasoner-R1是一种视觉-语言推理模型，旨在解决当前视觉-语言模型在细粒度空间推理方面的不足。该模型采用多模型蒙特卡洛树搜索（M3CTS）方法，生成多样且逻辑一致的长链思维推理轨迹，以构建高质量的空间推理监督。除此之外，SpatialReasoner-R1还引入了细粒度直接偏好优化（fDPO），通过空间奖励机制对候选响应进行评估，从而提高描述性基础和逻辑推理的准确性。实验结果表明，SpatialReasoner-R1在SPATIALRGPT-Bench上设定了新的最先进水平，平均准确率比最强基线提高了9.8%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20279",
            "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
            "url": "https://huggingface.co/papers/2506.20279",
            "abstract": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise annotated label for an input image. Despite advances in this field, existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data. To systematically study this problem, we first introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction tasks that correspond to urgent real-world applications, featuring unified evaluation across tasks. Then, we propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters. Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization. In contrast, DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment. Our data, and checkpoints and codes are available at https://xcltql666.github.io/DenseDiTProj",
            "score": 2,
            "issue_id": 4550,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 июня",
                "en": "June 25",
                "zh": "6月25日"
            },
            "hash": "8382f71877fe1997",
            "authors": [
                "Changliang Xia",
                "Chengyou Jia",
                "Zhuohang Dang",
                "Minnan Luo"
            ],
            "affiliations": [
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20279.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#cv",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективное плотное предсказание с минимумом данных",
                    "desc": "DenseDiT - это подход к генеративным моделям для задач плотного предсказания в компьютерном зрении. Он достигает превосходных результатов на реальных данных, используя минимальное количество обучающих примеров. DenseDiT максимально использует визуальные прайоры генеративных моделей и включает механизм повторного использования параметров. Модель превосходит существующие методы, используя менее 0,01% обучающих данных по сравнению с базовыми моделями."
                },
                "en": {
                    "title": "DenseDiT: Revolutionizing Dense Prediction with Minimal Data",
                    "desc": "DenseDiT is a generative model that excels in dense prediction tasks, which involve assigning labels to each pixel in an image. It addresses the challenge of limited training data by leveraging visual priors from generative models, allowing it to perform well in real-world scenarios. The model introduces DenseWorld, a benchmark for evaluating various dense prediction tasks, highlighting the shortcomings of existing methods in real-world applications. DenseDiT's innovative design, which includes a parameter-reuse mechanism and multi-scale context integration, enables it to achieve superior performance with significantly less training data compared to traditional approaches."
                },
                "zh": {
                    "title": "DenseDiT：用最少数据实现密集预测的突破",
                    "desc": "DenseDiT是一种基于生成模型的方法，能够在真实世界的密集预测任务中以最少的训练数据实现优越的性能。密集预测任务在计算机视觉中非常重要，旨在为输入图像学习逐像素的标注标签。现有方法主要集中在理想条件下，缺乏对真实场景的广泛适应性，且面临真实数据稀缺的挑战。DenseDiT通过最大限度地利用生成模型的视觉先验，结合参数重用机制和轻量级分支，能够在多种真实世界的密集预测任务中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19741",
            "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
            "url": "https://huggingface.co/papers/2506.19741",
            "abstract": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT",
            "score": 2,
            "issue_id": 4548,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 июня",
                "en": "June 24",
                "zh": "6月24日"
            },
            "hash": "288a2c7ef1ba6865",
            "authors": [
                "Yihong Luo",
                "Shuchen Xue",
                "Tianyang Hu",
                "Jing Tang"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "NUS",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19741.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "Эффективная адаптация генеративных моделей без переобучения",
                    "desc": "Статья представляет новый метод Noise Consistency Training (NCT) для интеграции новых сигналов управления в предобученные одношаговые генераторы без необходимости переобучения. NCT использует адаптерный модуль и функцию потерь согласованности шума в пространстве шума генератора. Этот подход позволяет эффективно адаптировать модели к новым условиям, таким как структурные ограничения или семантические указания. Эксперименты показывают, что NCT превосходит существующие методы по качеству генерации и вычислительной эффективности."
                },
                "en": {
                    "title": "Efficient Control in AI Content Generation with Noise Consistency Training",
                    "desc": "This paper presents a new method called Noise Consistency Training (NCT) that enhances pre-trained one-step generators for content generation without the need for retraining. NCT efficiently integrates new control signals, such as structural or semantic guidelines, into the generator by using an adapter module and a noise consistency loss. This approach allows the generator to produce high-quality outputs while maintaining computational efficiency, outperforming traditional methods that require extensive modifications. The results show that NCT achieves superior controllable generation in a single forward pass, making it a significant advancement in the field of artificial intelligence-generated content."
                },
                "zh": {
                    "title": "噪声一致性训练：高效可控生成的新方法",
                    "desc": "本文提出了一种新颖的噪声一致性训练（NCT）方法，能够高效地将新的控制信号整合到预训练的一步生成器中，而无需重新训练。传统方法通常需要对基础模型进行昂贵的修改，而NCT通过引入适配模块和噪声一致性损失，在生成器的噪声空间中直接进行调整。该方法在生成质量和计算效率上超越了现有的多步和蒸馏方法，展示了其在可控生成方面的优越性。NCT的模块化设计使其在数据使用上更加高效，易于部署。"
                }
            }
        }
    ],
    "link_prev": "2025-06-27.html",
    "link_next": "2025-07-01.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "27.06",
        "en": "06/27",
        "zh": "6月27日"
    },
    "short_date_next": {
        "ru": "01.07",
        "en": "07/01",
        "zh": "7月1日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}