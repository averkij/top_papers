{
    "date": {
        "ru": "10 апреля",
        "en": "April 10",
        "zh": "4月10日"
    },
    "time_utc": "2025-04-10 02:21",
    "weekday": 3,
    "issue_id": 3159,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.05741",
            "title": "DDT: Decoupled Diffusion Transformer",
            "url": "https://huggingface.co/papers/2504.05741",
            "abstract": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\color{ddtD}ecoupled \\color{ddtD}iffusion \\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.",
            "score": 4,
            "issue_id": 3159,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "2f4cd9583b2418f3",
            "authors": [
                "Shuai Wang",
                "Zhi Tian",
                "Weilin Huang",
                "Limin Wang"
            ],
            "affiliations": [
                "ByteDance Seed Vision",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05741.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DDT: Разделяй и властвуй в мире диффузионных трансформеров",
                    "desc": "Статья представляет новый подход к архитектуре диффузионных трансформеров, называемый Decoupled Diffusion Transformer (DDT). DDT разделяет процессы кодирования семантики и декодирования высокочастотных компонентов, что позволяет разрешить проблему оптимизации, присущую стандартным диффузионным трансформерам. Эксперименты показывают, что DDT достигает нового уровня производительности на наборе данных ImageNet, значительно ускоряя обучение и улучшая качество генерации изображений. Кроме того, предложенная архитектура позволяет оптимизировать процесс вывода путем разделения условий между соседними шагами денойзинга."
                },
                "en": {
                    "title": "Decoupling for Faster and Better Image Generation",
                    "desc": "This paper introduces a new model called the Decoupled Diffusion Transformer (DDT), which addresses the challenges faced by traditional diffusion transformers in generating high-quality outputs. The DDT separates the tasks of semantic encoding and high-frequency decoding, allowing for better optimization and improved performance. Experiments show that as the model size increases, a more powerful encoder leads to significant enhancements in generation quality, achieving state-of-the-art results on ImageNet datasets. Additionally, the decoupled design improves inference speed by optimizing the sharing of self-conditions between denoising steps, while a novel dynamic programming method helps maintain performance."
                },
                "zh": {
                    "title": "解耦扩散变换器：提升生成质量与推理速度的创新方案",
                    "desc": "扩散变换器在生成质量上表现出色，但训练迭代时间较长且推理步骤较多。每个去噪步骤中，扩散变换器对噪声输入进行编码，以提取低频语义成分，然后用相同的模块解码高频成分。这种方案导致了一个固有的优化困境：编码低频语义需要减少高频成分，从而在语义编码和高频解码之间产生紧张关系。为了解决这个问题，我们提出了一种新的解耦扩散变换器（DDT），它采用专门的条件编码器进行语义提取，并配备专门的速度解码器。"
                }
            }
        }
    ],
    "link_prev": "2025-04-09.html",
    "link_next": "2025-04-11.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "09.04",
        "en": "04/09",
        "zh": "4月9日"
    },
    "short_date_next": {
        "ru": "11.04",
        "en": "04/11",
        "zh": "4月11日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了 Skywork R1V，一种多模态推理模型。它通过高效的多模态转移方法，将 R1-series 大语言模型扩展到视觉模态。Skywork R1V 使用轻量级的视觉投影器，实现无需重新训练语言模型或视觉编码器的多模态适应。文章还提出了一种混合优化策略和自适应长度的思维链提炼方法，提高推理效率。实验结果显示，Skywork R1V 在多个基准测试中表现出色，并且模型权重已公开。",
        "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
        "pinyin": "这篇文章介绍了 Skywork R1V，一种多模态推理模型。它通过高效的多模态转移方法，将 R1-series 大语言模型扩展到视觉模态。Skywork R1V 使用轻量级的视觉投影器，实现无需重新训练语言模型或视觉编码器的多模态适应。文章还提出了一种混合优化策略和自适应长度的思维链提炼方法，提高推理效率。实验结果显示，Skywork R1V 在多个基准测试中表现出色，并且模型权重已公开。\n\nZhè piān wénzhāng jièshào le Skywork R1V, yīzhǒng duō móshuài tuīlǐ móxíng. Tā tōngguò gāoxiào de duō móshuài zhuǎnwéi fāngfǎ, jiāng R1-series dà yǔyán móxíng kuòzhǎn dào shìjué móshuài. Skywork R1V shǐyòng qīngliàngjí de shìjué tóujīngqì, shíxiàn wúxū chóngxīn xùnliàn yǔyán móxíng huò shìjué biānmǎqì de duō móshuài shìyìng. Wénzhāng hái tíchū le yīzhǒng hùnhé yōuhuà cèlüè hé zì shìyìng chángdù de sīwéi liàn tíxiàng fāngfǎ, tígāo tuīlǐ xiàoyìng. Shíyàn jiéguǒ xiǎnshì, Skywork R1V zài duō gè jīzhǔn cèshì zhōng biǎoxiàn chūsè, bìngqiě móxíng quánzhòng yǐ gōngkāi.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"转移\", \"pinyin\": \"zhuǎn yí\", \"trans\": \"transfer\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuò zhǎn\", \"trans\": \"extend\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"投影器\", \"pinyin\": \"tóu yǐng qì\", \"trans\": \"projector\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"重新\", \"pinyin\": \"chóng xīn\", \"trans\": \"re-\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"train\"},\n    {\"word\": \"编码器\", \"pinyin\": \"biān mǎ qì\", \"trans\": \"encoder\"},\n    {\"word\": \"适应\", \"pinyin\": \"shì yìng\", \"trans\": \"adapt\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùn hé\", \"trans\": \"hybrid\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"自适应\", \"pinyin\": \"zì shì yìng\", \"trans\": \"adaptive\"},\n    {\"word\": \"长度\", \"pinyin\": \"cháng dù\", \"trans\": \"length\"},\n    {\"word\": \"思维链\", \"pinyin\": \"sī wéi lián\", \"trans\": \"chain of thought\"},\n    {\"word\": \"提炼\", \"pinyin\": \"tí liàn\", \"trans\": \"extract\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiào lǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎn shì\", \"trans\": \"show\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"测试\", \"pinyin\": \"cè shì\", \"trans\": \"test\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"权重\", \"pinyin\": \"quán zhòng\", \"trans\": \"weights\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōng kāi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces Skywork R1V, a multimodal reasoning model. It extends the R1-series large language model to the visual modality through an efficient multimodal transfer method. Skywork R1V employs a lightweight visual projector to achieve multimodal adaptation without the need to retrain the language model or visual encoder. The article also proposes a hybrid optimization strategy and an adaptive-length chain-of-thought extraction method to enhance reasoning efficiency. Experimental results demonstrate that Skywork R1V performs excellently on multiple benchmark tests, and the model weights have been made publicly available.",
        "update_ts": "2025-04-09 09:12"
    }
}