{
    "date": {
        "ru": "14 февраля",
        "en": "February 14",
        "zh": "2月14日"
    },
    "time_utc": "2025-02-14 10:10",
    "weekday": 4,
    "issue_id": 2216,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.08910",
            "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU",
            "url": "https://huggingface.co/papers/2502.08910",
            "abstract": "In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations.",
            "score": 43,
            "issue_id": 2210,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "eed812d17aeec57e",
            "authors": [
                "Heejun Lee",
                "Geon Park",
                "Jaduk Suh",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai, Seoul, Korea",
                "Graduate School of AI, KAIST, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08910.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#long_context",
                    "#inference"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Преодоление барьера длинного контекста в LLM",
                    "desc": "InfiniteHiP - это новая система для обработки больших языковых моделей (LLM) с длинным контекстом. Она ускоряет обработку, удаляя неважные токены контекста с помощью иерархического алгоритма. Система позволяет обобщать на более длинные последовательности, применяя методы настройки RoPE. InfiniteHiP достигает 18.95-кратного ускорения в декодировании внимания для контекста в 1 миллион токенов без дополнительного обучения."
                },
                "en": {
                    "title": "InfiniteHiP: Unlocking Long Contexts in Language Models Efficiently",
                    "desc": "This paper presents InfiniteHiP, a new framework designed to improve the efficiency of large language models (LLMs) when processing very long contexts. It addresses the issues of slow inference speeds and high memory costs by using a hierarchical token pruning algorithm to remove irrelevant tokens dynamically. Additionally, InfiniteHiP enhances the model's ability to generalize to longer sequences by adjusting the relative positional encodings based on attention patterns. The framework allows for processing up to 3 million tokens on a single GPU while achieving significant speed improvements without the need for extra training."
                },
                "zh": {
                    "title": "InfiniteHiP：高效处理超长上下文的LLM框架",
                    "desc": "在现代大型语言模型（LLMs）中，处理非常长的上下文长度面临显著挑战，导致推理速度变慢和内存成本增加。现有的大多数预训练LLMs无法超出其原始训练序列长度进行泛化。为了解决这一问题，我们提出了InfiniteHiP，这是一种新颖且实用的LLM推理框架，通过模块化的分层令牌修剪算法动态消除无关的上下文令牌，从而加速处理。我们的框架能够在不需要额外训练的情况下，实现对长达300万令牌的处理，并在1百万令牌上下文中实现18.95倍的注意力解码加速。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08690",
            "title": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2502.08690",
            "abstract": "Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.",
            "score": 24,
            "issue_id": 2210,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "c7734d31994dcb35",
            "authors": [
                "Hoigi Seo",
                "Wongi Jeong",
                "Jae-sun Seo",
                "Se Young Chun"
            ],
            "affiliations": [
                "Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea",
                "INMC & IPAI, Seoul National University, Republic of Korea",
                "School of Electrical and Computer Engineering, Cornell Tech, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08690.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Эффективное сжатие текстовых энкодеров для генерации изображений без потери качества",
                    "desc": "Статья представляет новый метод оптимизации текстовых энкодеров в моделях диффузии текст-в-изображение (T2I). Авторы предлагают стратегию прунинга под названием Skip and Re-use layers (Skrr), которая позволяет значительно сократить потребление памяти без ущерба для качества генерируемых изображений. Метод Skrr избирательно пропускает или повторно использует определенные слои в трансформерных блоках, учитывая специфику задач T2I. Эксперименты показывают, что Skrr превосходит существующие методы поблочного прунинга и достигает наилучших показателей эффективности использования памяти при сохранении производительности по различным метрикам оценки."
                },
                "en": {
                    "title": "Efficient Memory Use in Text-to-Image Models with Skrr",
                    "desc": "This paper introduces a new method called Skip and Re-use layers (Skrr) to improve the efficiency of text encoders in text-to-image diffusion models. Text encoders are crucial for generating images from text but use a lot of memory compared to denoising modules. Skrr reduces memory usage by selectively skipping or reusing layers in the transformer architecture, which helps maintain performance while lowering resource demands. The results show that Skrr achieves high image quality and outperforms other pruning methods, making it a significant advancement in memory efficiency for T2I tasks."
                },
                "zh": {
                    "title": "提升文本编码器的内存效率",
                    "desc": "本文提出了一种名为Skip and Re-use layers（Skrr）的新策略，旨在提高文本到图像扩散模型中文本编码器的内存效率。尽管文本编码器在推理时间和浮点运算方面的贡献较小，但它们的内存需求却高达去噪模块的八倍。Skrr通过选择性跳过或重用某些变换器层，利用了变换器块中的冗余性，从而在不影响性能的情况下减少内存消耗。实验结果表明，Skrr在高稀疏度下仍能保持与原始模型相当的图像质量，并在多个评估指标上实现了最先进的内存效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09604",
            "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models",
            "url": "https://huggingface.co/papers/2502.09604",
            "abstract": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.",
            "score": 19,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "7aa5ce3731848736",
            "authors": [
                "Yung-Sung Chuang",
                "Benjamin Cohen-Wang",
                "Shannon Zejiang Shen",
                "Zhaofeng Wu",
                "Hu Xu",
                "Xi Victoria Lin",
                "James Glass",
                "Shang-Wen Li",
                "Wen-tau Yih"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Meta FAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09604.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "SelfCite: Самообучение ИИ искусству цитирования",
                    "desc": "SelfCite - это новый подход к самообучению больших языковых моделей (LLM) для генерации качественных цитат на уровне предложений. Метод использует сигнал награды, предоставляемый самой моделью через абляцию контекста, что позволяет избежать дорогостоящей ручной разметки. SelfCite применяет стратегию выборки best-of-N во время вывода и оптимизацию предпочтений для точной настройки моделей. Эффективность подхода подтверждается увеличением F1-меры цитирования до 5.3 пунктов на бенчмарке LongBench-Cite."
                },
                "en": {
                    "title": "Enhancing Citation Quality with SelfCite",
                    "desc": "SelfCite is a self-supervised method designed to enhance the citation quality in responses generated by large language models (LLMs). It uses a unique reward signal derived from the LLM itself, which assesses the necessity of citations by analyzing the impact of removing or retaining cited text. This approach not only improves the citation generation process during inference but also allows for fine-tuning the models to produce better citations through preference optimization. The results show a significant increase in citation accuracy, as evidenced by a 5.3 point improvement in citation F1 scores on the LongBench-Cite benchmark."
                },
                "zh": {
                    "title": "自监督引用生成的创新方法",
                    "desc": "SelfCite是一种新颖的自监督方法，旨在使大型语言模型（LLM）生成高质量、细粒度的句子级引用。该方法通过上下文消融提供的奖励信号，减少对昂贵和劳动密集型注释的依赖。具体来说，如果引用是必要的，移除被引用文本应防止相同的响应；如果足够，保留被引用文本应保持相同的响应。SelfCite在LongBench-Cite基准测试中显示出有效性，使引用的F1分数提高了5.3个百分点。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09620",
            "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
            "url": "https://huggingface.co/papers/2502.09620",
            "abstract": "Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL",
            "score": 17,
            "issue_id": 2214,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "2e519b64f13f6506",
            "authors": [
                "Yiwen Tang",
                "Zoey Guo",
                "Zhuhao Wang",
                "Ray Zhang",
                "Qizhi Chen",
                "Junli Liu",
                "Delin Qu",
                "Zhigang Wang",
                "Dong Wang",
                "Xuelong Li",
                "Bin Zhao"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09620.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Революция в 3D-понимании: архитектуры без энкодера покоряют новые вершины",
                    "desc": "Статья представляет первое всестороннее исследование потенциала архитектур без энкодера для 3D-понимания. Авторы предлагают стратегию LLM-embedded Semantic Encoding для предобучения и Hierarchical Geometry Aggregation для инструктивной настройки. Разработанная модель ENEL с 7 миллиардами параметров показывает результаты, сопоставимые с современными моделями, имеющими 13 миллиардов параметров. Исследование демонстрирует перспективность архитектур без энкодера в области 3D-понимания."
                },
                "en": {
                    "title": "Revolutionizing 3D Understanding with Encoder-Free Architectures",
                    "desc": "This paper explores the use of encoder-free architectures for 3D understanding, addressing limitations of traditional encoder-based models. It introduces the LLM-embedded Semantic Encoding strategy during pre-training to enhance point cloud representation and proposes a Hybrid Semantic Loss for better semantic extraction. Additionally, the Hierarchical Geometry Aggregation strategy is introduced in the instruction tuning phase to improve local detail focus in point clouds. The proposed model, ENEL, demonstrates competitive performance against existing models, indicating the potential of encoder-free approaches in 3D Large Multimodal Models."
                },
                "zh": {
                    "title": "无编码器架构：3D理解的新突破",
                    "desc": "本论文探讨了无编码器架构在3D理解中的应用潜力，首次对其进行全面研究。我们提出了一种新的策略，使大型语言模型（LLM）能够替代传统的3D编码器，解决了点云分辨率变化和特征语义需求不匹配的问题。通过引入嵌入式语义编码和分层几何聚合策略，我们的模型在分类、描述和视觉问答任务上表现出色。最终，我们的7B模型ENEL在性能上与当前最先进的模型ShapeLLM-13B相媲美，显示出无编码器架构在3D理解领域的巨大潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09082",
            "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles",
            "url": "https://huggingface.co/papers/2502.09082",
            "abstract": "Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively.",
            "score": 15,
            "issue_id": 2214,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "3d13dc492344cf84",
            "authors": [
                "Xintao Wang",
                "Heng Wang",
                "Yifei Zhang",
                "Xinfeng Yuan",
                "Rui Xu",
                "Jen-tse Huang",
                "Siyu Yuan",
                "Haoran Guo",
                "Jiangjie Chen",
                "Wei Wang",
                "Yanghua Xiao",
                "Shuchang Zhou"
            ],
            "affiliations": [
                "Fudan University",
                "Johns Hopkins University",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09082.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#story_generation",
                    "#benchmark",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "CoSER: революция в ролевом воспроизведении персонажей с помощью ИИ",
                    "desc": "Статья представляет CoSER - набор данных, открытые модели и протокол оценки для эффективного ролевого воспроизведения персонажей с помощью больших языковых моделей (LLM). Набор данных CoSER содержит 17 966 персонажей из 771 известной книги, включая аутентичные диалоги и разнообразные типы данных. Авторы вводят понятие 'актерской игры в заданных обстоятельствах' для обучения и оценки ролевых LLM. Разработанные модели CoSER 8B и CoSER 70B демонстрируют высокую эффективность, причем CoSER 70B превосходит или соответствует GPT-4 по нескольким критериям оценки."
                },
                "en": {
                    "title": "Empowering Role-Playing Agents with CoSER: A New Dataset and Methodology",
                    "desc": "This paper introduces CoSER, a comprehensive dataset designed to enhance role-playing language agents (RPLAs) using large language models (LLMs). It includes 17,966 characters from 771 well-known books, providing authentic dialogues and various data types that reflect real-world interactions. The authors propose a novel training and evaluation method based on acting techniques, allowing LLMs to portray multiple characters in specific scenes. The results show that the CoSER 70B model outperforms existing models like GPT-4o in several benchmarks, demonstrating the effectiveness of the CoSER dataset for training and evaluating RPLAs."
                },
                "zh": {
                    "title": "提升角色扮演语言代理的有效性",
                    "desc": "角色扮演语言代理（RPLA）是大型语言模型（LLM）的新兴应用，但模拟已建立角色的任务具有挑战性，因为缺乏真实的角色数据集和细致的评估方法。本文介绍了CoSER，一个高质量的数据集、开放模型和评估协议，旨在有效支持已建立角色的RPLA。CoSER数据集涵盖了771本著名书籍中的17,966个角色，提供了真实对话和多样化的数据类型。通过引入给定情境表演的方法，我们训练和评估角色扮演的LLM，实验结果表明CoSER数据集在RPLA训练、评估和检索中的重要价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09056",
            "title": "An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging",
            "url": "https://huggingface.co/papers/2502.09056",
            "abstract": "This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks.",
            "score": 15,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "36c3c29072ae279d",
            "authors": [
                "Kunat Pipatanakul",
                "Pittawat Taveekitworachai",
                "Potsawee Manakul",
                "Kasima Tharnpipitchai"
            ],
            "affiliations": [
                "SCB 10X R&D SCBX Group Bangkok, Thailand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09056.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#low_resource",
                    "#multilingual",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление логики локальных языковых моделей",
                    "desc": "Данное исследование посвящено методам улучшения способностей к рассуждению у языково-специфичных больших языковых моделей (LLM), в частности для тайского языка. Авторы предлагают подходы к отбору данных и слиянию моделей, чтобы перенести продвинутые навыки рассуждения из модели DeepSeek R1 в локальные LLM. Цель состоит в том, чтобы усилить логические возможности языково-специфичных моделей, сохраняя при этом их способности в целевом языке. Исследователи показывают, что даже с ограниченным бюджетом и общедоступными данными можно значительно улучшить рассуждения локальных LLM до уровня DeepSeek R1."
                },
                "en": {
                    "title": "Empowering Thai LLMs with Enhanced Reasoning Capabilities",
                    "desc": "This paper explores methods for selecting data and merging models to improve reasoning abilities in language-specific large language models (LLMs), particularly for the Thai language. The authors aim to enhance these models' reasoning skills while ensuring they remain effective in their target languages. They highlight the challenges faced by low-resource languages, which often lack the extensive training data available for high-resource languages like English. The study demonstrates that it is feasible to boost the reasoning capabilities of these LLMs using only publicly available datasets and a modest budget, achieving results comparable to advanced models like DeepSeek R1."
                },
                "zh": {
                    "title": "提升低资源语言LLM的推理能力",
                    "desc": "本文研究了数据选择和模型合并的方法，旨在将先进的推理能力（如DeepSeek R1）融入特定语言的大型语言模型（LLMs），特别关注泰语LLM。我们的目标是增强特定语言LLMs的推理能力，同时保持其目标语言的能力。DeepSeek R1在推理方面表现出色，但主要受益于高资源语言，如英语和中文，而低资源语言则受到忽视。我们展示了仅使用公开数据集和120美元的计算预算，就可以提升特定语言LLMs的推理能力，使其达到DeepSeek R1的水平，而不影响其在目标语言任务上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06608",
            "title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models",
            "url": "https://huggingface.co/papers/2502.06608",
            "abstract": "Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.",
            "score": 11,
            "issue_id": 2210,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 февраля",
                "en": "February 10",
                "zh": "2月10日"
            },
            "hash": "0602cc4a46e4c69c",
            "authors": [
                "Yangguang Li",
                "Zi-Xin Zou",
                "Zexiang Liu",
                "Dehu Wang",
                "Yuan Liang",
                "Zhipeng Yu",
                "Xingchao Liu",
                "Yuan-Chen Guo",
                "Ding Liang",
                "Wanli Ouyang",
                "Yan-Pei Cao"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "The University of Texas at Austin",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06608.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "TripoSG: Революция в генерации трехмерных форм",
                    "desc": "TripoSG - это новая парадигма генерации трехмерных форм, способная создавать высококачественные 3D-модели с точным соответствием входным изображениям. Она использует масштабный трансформер с выпрямленным потоком для генерации 3D-форм, обученный на обширных высококачественных данных. TripoSG применяет гибридную стратегию обучения, сочетающую потери SDF, нормалей и эйконала для 3D VAE. Благодаря интеграции этих компонентов, TripoSG достигает передового уровня в генерации 3D-форм с улучшенной детализацией и верностью входным изображениям."
                },
                "en": {
                    "title": "TripoSG: Revolutionizing 3D Shape Generation with Diffusion Techniques",
                    "desc": "This paper introduces TripoSG, a new method for generating high-quality 3D shapes using diffusion techniques. It addresses the challenges of 3D shape generation by employing a large-scale rectified flow transformer and a hybrid supervised training strategy that enhances reconstruction performance. The model is trained on a vast dataset, producing 2 million high-quality 3D samples, which improves the fidelity and detail of the generated shapes. TripoSG not only achieves state-of-the-art results but also shows strong generalization across various input images, making it a significant advancement in 3D generative models."
                },
                "zh": {
                    "title": "TripoSG：高保真3D形状生成的新范式",
                    "desc": "本论文介绍了一种新的3D形状生成方法TripoSG，旨在提高3D网格的生成质量。我们提出了一种大规模的流动变换器，能够在大量高质量数据上进行训练，从而实现高保真度的3D形状生成。此外，我们采用了一种混合监督训练策略，结合了SDF、法线和Eikonal损失，以提高3D重建性能。通过全面的实验验证，我们的框架在3D形状生成方面达到了最先进的性能，展现了对输入图像的高保真度和多样化的生成能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09100",
            "title": "Logical Reasoning in Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2502.09100",
            "abstract": "With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.",
            "score": 11,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "72b32d40c559c7e4",
            "authors": [
                "Hanmeng Liu",
                "Zhizhang Fu",
                "Mengru Ding",
                "Ruoxi Ning",
                "Chaoli Zhang",
                "Xiaozhang Liu",
                "Yue Zhang"
            ],
            "affiliations": [
                "Hainan University",
                "Westlake University",
                "Zhejiang Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09100.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#survey",
                    "#reasoning",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Логическое мышление в больших языковых моделях: прогресс и перспективы",
                    "desc": "Этот обзор посвящен последним достижениям в области логического рассуждения в больших языковых моделях (LLM). В нем рассматриваются теоретические основы и методы оценки способностей LLM к логическому мышлению. Авторы анализируют существующие возможности в различных парадигмах рассуждений, включая дедуктивное, индуктивное, абдуктивное и аналогическое. Также обсуждаются стратегии улучшения производительности рассуждений, такие как настройка данных, обучение с подкреплением и нейросимволические подходы."
                },
                "en": {
                    "title": "Enhancing Logical Reasoning in Large Language Models",
                    "desc": "This paper reviews the progress made in enhancing logical reasoning capabilities of large language models (LLMs) like OpenAI o3 and DeepSeek-R1. It discusses various reasoning paradigms such as deductive, inductive, abductive, and analogical reasoning, highlighting their theoretical foundations and evaluation benchmarks. The authors analyze strategies to improve reasoning performance, including data-centric tuning and neuro-symbolic approaches. The paper concludes by suggesting future research directions to further develop logical reasoning in AI systems."
                },
                "zh": {
                    "title": "提升AI系统逻辑推理能力的探索",
                    "desc": "随着OpenAI o3和DeepSeek-R1等先进推理模型的出现，大型语言模型（LLMs）展现了出色的推理能力。然而，它们在进行严格逻辑推理方面的能力仍然是一个未解之谜。本文综述了LLMs中逻辑推理的最新进展，探讨了逻辑推理的范围、理论基础以及评估推理能力的基准。我们分析了不同推理范式（如演绎、归纳、溯因和类比）的现有能力，并评估了增强推理性能的策略，包括数据中心调优、强化学习、解码策略和神经符号方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09621",
            "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",
            "url": "https://huggingface.co/papers/2502.09621",
            "abstract": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/",
            "score": 10,
            "issue_id": 2213,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "ac755f4f5584a58c",
            "authors": [
                "Dongzhi Jiang",
                "Renrui Zhang",
                "Ziyu Guo",
                "Yanwei Li",
                "Yu Qi",
                "Xinyan Chen",
                "Liuhui Wang",
                "Jianhan Jin",
                "Claire Guo",
                "Shen Yan",
                "Bo Zhang",
                "Chaoyou Fu",
                "Peng Gao",
                "Hongsheng Li"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.09621.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оценка мультимодальных рассуждений: новый взгляд на Chain-of-Thought в LMM",
                    "desc": "Статья представляет MME-CoT - специализированный бенчмарк для оценки производительности цепочки рассуждений (Chain-of-Thought) в мультимодальных языковых моделях (LMM). Исследование охватывает шесть областей и предлагает три новых метрики для оценки качества, надежности и эффективности рассуждений. Анализ показывает, что модели с механизмом рефлексии демонстрируют превосходное качество CoT, при этом Kimi k1.5 превосходит GPT-4o. Однако выявлено, что CoT может ухудшать производительность LMM в задачах, требующих восприятия, а модели с рефлексией демонстрируют значительную неэффективность."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning with MME-CoT",
                    "desc": "This paper presents MME-CoT, a benchmark designed to evaluate the Chain-of-Thought (CoT) reasoning capabilities of Large Multimodal Models (LMMs) across various domains. It introduces three new metrics to assess reasoning quality, robustness, and efficiency in a detailed manner. The study reveals that models utilizing a reflection mechanism, like Kimi k1.5, outperform others such as GPT-4o in CoT quality, but may struggle with perception-heavy tasks due to overthinking. Overall, MME-CoT aims to enhance the understanding and development of multimodal reasoning in LMMs."
                },
                "zh": {
                    "title": "MME-CoT：提升多模态模型推理能力的基准",
                    "desc": "本文介绍了MME-CoT，这是一个专门评估大型多模态模型（LMMs）在链式思维（CoT）推理性能的基准，涵盖数学、科学、光学字符识别、逻辑、时空和一般场景六个领域。我们提出了一套全面的评估工具，包含三种新颖的指标，细致评估推理质量、鲁棒性和效率。研究发现，具有反思机制的模型在CoT质量上表现优越，而在感知密集型任务中，CoT提示可能会降低LMM的性能。尽管CoT质量较高，但具有反思的LMM在正常响应和自我修正阶段表现出显著的低效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09042",
            "title": "Typhoon T1: An Open Thai Reasoning Model",
            "url": "https://huggingface.co/papers/2502.09042",
            "abstract": "This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.",
            "score": 10,
            "issue_id": 2213,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "5cb078b546437366",
            "authors": [
                "Pittawat Taveekitworachai",
                "Potsawee Manakul",
                "Kasima Tharnpipitchai",
                "Kunat Pipatanakul"
            ],
            "affiliations": [
                "SCB 10X R&D SCBX Group Bangkok, Thailand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09042.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#multilingual",
                    "#dataset",
                    "#open_source",
                    "#low_resource",
                    "#data",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Открытая модель рассуждений для тайского языка",
                    "desc": "Статья представляет Typhoon T1 - открытую модель рассуждений на тайском языке. Модель рассуждений - это новый тип генеративной модели, построенной на основе больших языковых моделей (LLM). Она генерирует длинную цепочку мыслей перед выдачей окончательного ответа, что улучшает производительность на сложных задачах. Авторы делятся деталями разработки такой модели с использованием обучения с учителем на открытых датасетах вместо обучения с подкреплением."
                },
                "en": {
                    "title": "Typhoon T1: Advancing Thai Reasoning Models for Complex Tasks",
                    "desc": "This paper presents Typhoon T1, an initiative to create an open reasoning model specifically for the Thai language. Reasoning models are advanced generative models that produce a sequence of thoughts leading to a conclusion, enhancing performance on intricate tasks. The authors focus on developing this model using supervised fine-tuning with open datasets, rather than relying on reinforcement learning, making it more accessible and cost-effective. The paper details the synthetic data generation process, training methods, and shares the model weights, aiming to support future research in low-resource language reasoning models."
                },
                "zh": {
                    "title": "开放泰语推理模型的创新之路",
                    "desc": "本文介绍了Typhoon T1，这是一个开发开放泰语推理模型的项目。推理模型是一种新型的生成模型，基于大型语言模型（LLMs）构建，能够在得出最终答案之前生成长链思维，从而提高复杂任务的表现。Typhoon T1通过利用监督微调和开放数据集，以更具成本效益的方式深入探讨推理模型的开发，避免了强化学习的复杂性。我们希望这个开放项目为该领域的进一步研究奠定基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09560",
            "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
            "url": "https://huggingface.co/papers/2502.09560",
            "abstract": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.",
            "score": 9,
            "issue_id": 2211,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "019b4d19788a85cc",
            "authors": [
                "Rui Yang",
                "Hanyang Chen",
                "Junyu Zhang",
                "Mark Zhao",
                "Cheng Qian",
                "Kangrui Wang",
                "Qineng Wang",
                "Teja Venkat Koripella",
                "Marziyeh Movahedi",
                "Manling Li",
                "Heng Ji",
                "Huan Zhang",
                "Tong Zhang"
            ],
            "affiliations": [
                "Northwestern University",
                "Toyota Technological Institute at Chicago",
                "University of Illinois Urbana-Champaign",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09560.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#games",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "EmbodiedBench: новый стандарт оценки воплощенного ИИ",
                    "desc": "В статье представлен EmbodiedBench - комплексный инструмент для оценки мультимодальных языковых моделей (MLLM) в задачах воплощенного искусственного интеллекта. Бенчмарк включает 1128 тестовых заданий в четырех средах, охватывающих как высокоуровневые семантические задачи, так и низкоуровневые действия. Эксперименты с 13 ведущими MLLM показали, что модели успешно справляются с высокоуровневыми задачами, но испытывают трудности с низкоуровневыми манипуляциями. EmbodiedBench предоставляет стандартизированную платформу для оценки и развития воплощенных агентов на основе MLLM."
                },
                "en": {
                    "title": "Empowering Embodied Agents with Comprehensive Evaluation",
                    "desc": "This paper discusses the development of EmbodiedBench, a benchmark for evaluating multi-modal large language models (MLLMs) in the context of embodied agents. It highlights the gap in existing evaluation frameworks for MLLM-based agents, which are crucial for performing real-world tasks. The benchmark includes a wide range of tasks that assess various capabilities such as commonsense reasoning and spatial awareness. The results indicate that while MLLMs perform well on high-level tasks, they face significant challenges with low-level manipulation tasks."
                },
                "zh": {
                    "title": "多模态大型语言模型助力具身智能体的评估与发展",
                    "desc": "本论文探讨了多模态大型语言模型（MLLMs）在创建具身智能体方面的应用，旨在解决现实世界任务。我们提出了EmbodiedBench，这是一个全面的基准测试框架，用于评估视觉驱动的具身智能体。EmbodiedBench包含1288个测试任务，涵盖高层语义任务和低层原子动作任务，并评估智能体的常识推理、复杂指令理解、空间意识、视觉感知和长期规划等能力。实验结果表明，尽管MLLMs在高层任务中表现优异，但在低层操作任务上仍然存在挑战，最好的模型GPT-4o的平均得分仅为28.9%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09601",
            "title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
            "url": "https://huggingface.co/papers/2502.09601",
            "abstract": "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.",
            "score": 8,
            "issue_id": 2212,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "4b0518724dea8b37",
            "authors": [
                "Xinyin Ma",
                "Guangnian Wan",
                "Runpeng Yu",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09601.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эластичное управление длиной цепочки рассуждений в моделях машинного обучения",
                    "desc": "Статья представляет новую стратегию обучения и вывода под названием CoT-Valve, которая позволяет моделям генерировать цепочки рассуждений различной длины. Авторы предлагают идентифицировать направление в пространстве параметров, манипулируя которым, можно эффективно контролировать длину генерируемой цепочки рассуждений. Они разрабатывают методы точного обучения сжимаемых цепочек рассуждений и прогрессивного сжатия длины цепочки. Эксперименты показывают, что CoT-Valve успешно обеспечивает управляемость и сжимаемость цепочки, превосходя контроль на основе промптов."
                },
                "en": {
                    "title": "Dynamic Reasoning Control with CoT-Valve",
                    "desc": "This paper presents CoT-Valve, a new strategy for controlling the length of reasoning chains in machine learning models, particularly in Chain-of-Thought (CoT) reasoning. The authors observe that while longer reasoning paths improve performance, they also increase inference costs, especially on harder tasks. CoT-Valve allows a single model to dynamically adjust the length of its reasoning based on task difficulty, thus optimizing efficiency. The experiments demonstrate that this method not only compresses reasoning chains significantly but also maintains high performance compared to traditional prompt-based controls."
                },
                "zh": {
                    "title": "动态控制推理链长度的创新策略",
                    "desc": "本文提出了一种名为CoT-Valve的新策略，用于动态控制推理链的长度，以降低推理模型的计算开销。研究发现，推理路径在简单任务中容易压缩，但在困难任务中则较为困难，因此我们探索了如何在单一模型中实现这一目标。通过调整参数空间中的方向，CoT-Valve能够有效控制生成的推理链长度，并且在压缩推理链方面表现出色。实验结果表明，CoT-Valve在控制和压缩推理链方面优于基于提示的控制方法，且在性能上仅有轻微下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09390",
            "title": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models",
            "url": "https://huggingface.co/papers/2502.09390",
            "abstract": "In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.",
            "score": 6,
            "issue_id": 2214,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "4caf9cec56011350",
            "authors": [
                "Daniel Fleischer",
                "Moshe Berchansky",
                "Gad Markovits",
                "Moshe Wasserblat"
            ],
            "affiliations": [
                "IntelLabs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09390.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SQuARE: самоопрос для улучшения рассуждений ИИ",
                    "desc": "В статье представлен новый метод промптинга под названием SQuARE, который улучшает способности больших языковых моделей к рассуждению. SQuARE побуждает модели генерировать и решать вспомогательные вопросы перед ответом на основной запрос, что способствует более глубокому исследованию темы. Эксперименты с моделями Llama 3 и GPT-4 показали, что SQuARE значительно превосходит традиционные методы цепочки размышлений и существующие методы перефразирования. Предложенный подход систематически декомпозирует запросы, повышая эффективность языковых моделей в задачах, требующих рассуждений."
                },
                "en": {
                    "title": "Unlocking Deeper Reasoning with SQuARE",
                    "desc": "This paper presents SQuARE, a new prompting technique aimed at enhancing the reasoning abilities of Large Language Models (LLMs) in Natural Language Processing. Unlike traditional methods that may not fully utilize a model's reasoning potential, SQuARE employs a self-interrogation approach, encouraging models to generate and answer multiple auxiliary questions before addressing the main question. This method builds on existing chain-of-thought frameworks, allowing for a deeper exploration of topics. Evaluations with Llama 3 and GPT-4o show that SQuARE outperforms conventional prompting techniques, leading to improved reasoning in question-answering tasks."
                },
                "zh": {
                    "title": "SQuARE：提升推理能力的新方法",
                    "desc": "在自然语言处理领域，大型语言模型（LLMs）面临越来越复杂的推理挑战。传统的链式思维提示方法虽然有一定效果，但往往无法充分发挥模型的推理能力。本文提出了一种新颖的提示技术SQuARE（顺序问答推理引擎），通过自我质询的方式来改善推理过程。SQuARE在多个问答数据集上的评估结果显示，其在推理任务中的表现显著优于传统的链式思维提示和现有的重述-回应方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09619",
            "title": "Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights",
            "url": "https://huggingface.co/papers/2502.09619",
            "abstract": "With the increasing numbers of publicly available models, there are probably pretrained, online models for most tasks users require. However, current model search methods are rudimentary, essentially a text-based search in the documentation, thus users cannot find the relevant models. This paper presents ProbeLog, a method for retrieving classification models that can recognize a target concept, such as \"Dog\", without access to model metadata or training data. Differently from previous probing methods, ProbeLog computes a descriptor for each output dimension (logit) of each model, by observing its responses on a fixed set of inputs (probes). Our method supports both logit-based retrieval (\"find more logits like this\") and zero-shot, text-based retrieval (\"find all logits corresponding to dogs\"). As probing-based representations require multiple costly feedforward passes through the model, we develop a method, based on collaborative filtering, that reduces the cost of encoding repositories by 3x. We demonstrate that ProbeLog achieves high retrieval accuracy, both in real-world and fine-grained search tasks and is scalable to full-size repositories.",
            "score": 4,
            "issue_id": 2214,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "2222d8b83a19a957",
            "authors": [
                "Jonathan Kahana",
                "Or Nathan",
                "Eliahu Horwitz",
                "Yedid Hoshen"
            ],
            "affiliations": [
                "School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09619.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ProbeLog: Умный поиск нужных моделей машинного обучения",
                    "desc": "ProbeLog - это метод поиска классификационных моделей без доступа к метаданным или обучающим данным. Он вычисляет дескриптор для каждого выходного измерения модели, наблюдая ее ответы на фиксированном наборе входных данных. ProbeLog поддерживает как поиск на основе логитов, так и текстовый поиск с нулевым обучением. Метод использует совместную фильтрацию для снижения вычислительных затрат и демонстрирует высокую точность поиска в реальных задачах."
                },
                "en": {
                    "title": "Efficient Model Retrieval with ProbeLog",
                    "desc": "This paper introduces ProbeLog, a novel method for retrieving classification models that can identify specific concepts without needing detailed model information. It generates a descriptor for each model's output dimension by analyzing its responses to a set of predefined inputs, known as probes. ProbeLog allows users to perform both logit-based and zero-shot retrieval, making it easier to find models relevant to their needs. Additionally, it employs collaborative filtering to significantly reduce the computational cost of processing large model repositories, achieving high accuracy in model retrieval tasks."
                },
                "zh": {
                    "title": "高效模型检索，轻松找到所需！",
                    "desc": "随着公开模型数量的增加，用户所需的任务几乎都有预训练的在线模型。然而，目前的模型搜索方法相对简单，主要依赖文档中的文本搜索，导致用户无法找到相关模型。本文提出了ProbeLog，一种检索分类模型的方法，可以识别目标概念，如“狗”，而无需访问模型元数据或训练数据。与之前的探测方法不同，ProbeLog通过观察模型在固定输入集上的响应，为每个模型的每个输出维度（logit）计算描述符，从而实现高效的模型检索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08946",
            "title": "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding",
            "url": "https://huggingface.co/papers/2502.08946",
            "abstract": "In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.",
            "score": 4,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "daecc7f38306f7b8",
            "authors": [
                "Mo Yu",
                "Lemao Liu",
                "Junjie Wu",
                "Tsz Ting Chung",
                "Shunchi Zhang",
                "Jiangnan Li",
                "Dit-Yan Yeung",
                "Jie Zhou"
            ],
            "affiliations": [
                "HKUST",
                "JHU",
                "WeChat AI, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08946.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "🦜",
                "ru": {
                    "title": "Языковые модели: понимание или имитация?",
                    "desc": "Исследователи изучают вопрос о действительном понимании языковыми моделями (LLM) того, что они говорят. Они разработали задачу PhysiCo для оценки понимания физических концепций, используя абстрактные сетки вместо естественного языка. Результаты показывают, что современные LLM, включая GPT-4 и Gemini 2.0, отстают от людей примерно на 40% в этой задаче. Исследование подтверждает феномен 'стохастического попугая', так как модели не справляются с сеточной задачей, но хорошо описывают те же концепции на естественном языке."
                },
                "en": {
                    "title": "Unveiling the Limits of LLM Understanding",
                    "desc": "This paper investigates whether large language models (LLMs) truly understand the content they generate, addressing the concept of the 'Stochastic Parrot'. The authors introduce a new assessment task called PhysiCo, which uses grid-format inputs to evaluate understanding of physical concepts. Their findings reveal that top-performing LLMs, like GPT-4o and Gemini 2.0, significantly underperform compared to humans, indicating a gap in comprehension. Additionally, the study shows that LLMs struggle with the task due to inherent challenges in understanding rather than just the format of the input data."
                },
                "zh": {
                    "title": "探究大型语言模型的理解能力",
                    "desc": "本研究系统性地探讨了一个常见问题：大型语言模型（LLMs）是否真正理解它们所说的内容。我们提出了一种名为PhysiCo的评估任务，旨在通过网格格式的输入来减轻记忆问题，这些输入抽象地描述了物理现象。研究表明，当前最先进的LLMs在理解能力上落后于人类约40%，并且在网格任务中表现不佳，显示出随机鹦鹉现象的存在。我们的任务挑战了LLMs，主要是由于内在的困难，而非网格格式的不熟悉。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08468",
            "title": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data",
            "url": "https://huggingface.co/papers/2502.08468",
            "abstract": "Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.",
            "score": 3,
            "issue_id": 2211,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "ac7814f15d1e9616",
            "authors": [
                "Haonan Chen",
                "Liang Wang",
                "Nan Yang",
                "Yutao Zhu",
                "Ziliang Zhao",
                "Furu Wei",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Microsoft Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08468.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#dataset",
                    "#training",
                    "#data",
                    "#multilingual"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Синтетические данные открывают новые горизонты для мультимодальных эмбеддингов",
                    "desc": "В статье представлена новая модель мультимодальных эмбеддингов mmE5, обученная на синтетических данных высокого качества. Авторы определили три критерия для создания таких данных: широкий охват задач и модальностей, надежное кросс-модальное выравнивание и высокая достоверность. Используя эти принципы, они синтезировали наборы данных с помощью мультимодальной большой языковой модели. Эксперименты показали, что mmE5 достигает передовых результатов на бенчмарках MMEB и XTD."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with High-Quality Synthetic Data",
                    "desc": "This paper discusses the development of multimodal embedding models that integrate different types of data, like text and images, into a single representation. It highlights the challenge of limited labeled multimodal data and proposes a solution through high-quality synthetic data generation. The authors establish three key criteria for effective synthetic data: broad scope, robust cross-modal alignment, and high fidelity. By adhering to these principles, they create a multimodal multilingual E5 model, mmE5, which demonstrates exceptional performance on various benchmarks."
                },
                "zh": {
                    "title": "高质量合成数据助力多模态模型",
                    "desc": "多模态嵌入模型能够将文本和图像等不同模态的数据映射到统一的表示空间，但有限的标注多模态数据常常影响嵌入性能。本文提出了高质量合成多模态数据的三个标准：广泛的范围、稳健的跨模态对齐和高保真度。通过这些标准，我们合成了覆盖多种任务和模态组合的数据集，并利用多模态大语言模型进行生成。最终，我们训练的多模态多语言E5模型mmE5在MMEB基准测试中表现出色，并在XTD基准测试中展现了卓越的多语言性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09614",
            "title": "DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References",
            "url": "https://huggingface.co/papers/2502.09614",
            "abstract": "We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness. Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models. We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller. Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations. We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments. At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method. The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity. We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world. Our method achieves over a 10% improvement in success rates compared to leading baselines. The project website with animated results is available at https://meowuu7.github.io/DexTrack/.",
            "score": 1,
            "issue_id": 2216,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "f83cdb806eef0812",
            "authors": [
                "Xueyi Liu",
                "Jianibieke Adalibieke",
                "Qianwei Han",
                "Yuzhe Qin",
                "Li Yi"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Qi Zhi Institute",
                "Tsinghua University",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09614.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#training",
                    "#optimization",
                    "#rl",
                    "#games",
                    "#agents"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "Нейронный контроллер для ловких манипуляций роботизированной руки",
                    "desc": "Статья описывает разработку нейронного контроллера для отслеживания движений при манипуляциях с объектами роботизированной рукой. Авторы предлагают подход, основанный на обучении на большом наборе демонстраций успешного отслеживания движений человека. Используется итеративное улучшение контроллера с помощью комбинации обучения с подкреплением и имитационного обучения. Метод включает оптимизацию отдельных траекторий с использованием гомотопической оптимизации для повышения разнообразия демонстраций."
                },
                "en": {
                    "title": "Empowering Robots with Human-Like Dexterity through Neural Tracking!",
                    "desc": "This paper presents a novel neural tracking controller designed for dexterous manipulation of objects by a robot hand, using human references as a guide. The authors tackle the complexities of contact dynamics and the need for the controller to be adaptable and robust across various tasks. They propose a method that combines reinforcement learning and imitation learning, leveraging a large dataset of successful robot tracking demonstrations to improve performance iteratively. The results show a significant improvement in success rates, demonstrating the effectiveness of their approach in both simulated and real-world environments."
                },
                "zh": {
                    "title": "通用神经控制器：提升灵巧操作的成功率",
                    "desc": "本文提出了一种通用的神经跟踪控制器，用于从人类参考中进行灵巧操作。该控制器旨在管理灵巧机器人手，以操控多种物体，适应不同的人机交互需求。我们的方法通过大规模成功的机器人跟踪示例来训练控制器，并结合强化学习和模仿学习来提升其在动态环境中的表现。最终，我们在仿真和现实世界中评估了该控制器，成功率比现有方法提高了10%以上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05761",
            "title": "3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly",
            "url": "https://huggingface.co/papers/2502.05761",
            "abstract": "Industrial anomaly detection achieves progress thanks to datasets such as MVTec-AD and VisA. However, they suf- fer from limitations in terms of the number of defect sam- ples, types of defects, and availability of real-world scenes. These constraints inhibit researchers from further exploring the performance of industrial detection with higher accuracy. To this end, we propose a new large-scale anomaly detection dataset called 3CAD, which is derived from real 3C produc- tion lines. Specifically, the proposed 3CAD includes eight different types of manufactured parts, totaling 27,039 high- resolution images labeled with pixel-level anomalies. The key features of 3CAD are that it covers anomalous regions of different sizes, multiple anomaly types, and the possibility of multiple anomalous regions and multiple anomaly types per anomaly image. This is the largest and first anomaly de- tection dataset dedicated to 3C product quality control for community exploration and development. Meanwhile, we in- troduce a simple yet effective framework for unsupervised anomaly detection: a Coarse-to-Fine detection paradigm with Recovery Guidance (CFRG). To detect small defect anoma- lies, the proposed CFRG utilizes a coarse-to-fine detection paradigm. Specifically, we utilize a heterogeneous distilla- tion model for coarse localization and then fine localiza- tion through a segmentation model. In addition, to better capture normal patterns, we introduce recovery features as guidance. Finally, we report the results of our CFRG frame- work and popular anomaly detection methods on the 3CAD dataset, demonstrating strong competitiveness and providing a highly challenging benchmark to promote the development of the anomaly detection field. Data and code are available: https://github.com/EnquanYang2022/3CAD.",
            "score": 1,
            "issue_id": 2215,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 февраля",
                "en": "February 9",
                "zh": "2月9日"
            },
            "hash": "f7f244334d53bca7",
            "authors": [
                "Enquan Yang",
                "Peng Xing",
                "Hanyang Sun",
                "Wenbo Guo",
                "Yuanwei Ma",
                "Zechao Li",
                "Dan Zeng"
            ],
            "affiliations": [
                "Changzhou Microintelligence Corporation",
                "Nanjing University of Science and Technology",
                "School of Communication and Information Engineering, Shanghai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05761.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "3CAD: Крупномасштабный датасет для обнаружения промышленных аномалий",
                    "desc": "Авторы представляют новый крупномасштабный набор данных для обнаружения аномалий под названием 3CAD, полученный с реальных производственных линий 3C. Этот датасет включает 27,039 изображений высокого разрешения с пиксельной разметкой аномалий для восьми различных типов производственных деталей. 3CAD отличается разнообразием размеров аномальных областей, множеством типов аномалий и возможностью наличия нескольких аномальных регионов и типов аномалий на одном изображении. Также авторы предлагают эффективный фреймворк для обнаружения аномалий без учителя, названный CFRG, который использует парадигму обнаружения от грубого к точному с руководством по восстановлению."
                },
                "en": {
                    "title": "3CAD: A New Benchmark for Anomaly Detection in 3C Products",
                    "desc": "This paper introduces a new dataset called 3CAD for industrial anomaly detection, specifically focusing on 3C product quality control. The 3CAD dataset contains 27,039 high-resolution images with pixel-level annotations for eight types of manufactured parts, addressing the limitations of existing datasets like MVTec-AD and VisA. To enhance anomaly detection, the authors propose a Coarse-to-Fine detection framework with Recovery Guidance (CFRG), which improves the localization of small defects by first identifying coarse regions and then refining the detection through segmentation. The results show that the CFRG framework is competitive with existing methods, providing a valuable resource for advancing research in anomaly detection."
                },
                "zh": {
                    "title": "3CAD：推动3C产品异常检测的新数据集",
                    "desc": "本论文提出了一个新的大规模异常检测数据集，名为3CAD，专注于3C产品的质量控制。该数据集包含27,039张高分辨率图像，标注了不同类型和大小的像素级异常。为了提高异常检测的准确性，论文还介绍了一种简单有效的无监督异常检测框架，称为CFRG，采用粗到细的检测范式。通过该框架，研究人员可以更好地捕捉正常模式并定位小缺陷，推动异常检测领域的发展。"
                }
            }
        }
    ],
    "link_prev": "2025-02-13.html",
    "link_next": "2025-02-17.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "13.02",
        "en": "02/13",
        "zh": "2月13日"
    },
    "short_date_next": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2月17日"
    },
    "categories": {
        "#dataset": 10,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 3,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 8,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 2
    },
    "zh": {
        "text": "这篇文章讨论了现代大语言模型（LLMs）处理非常长的上下文时面临的挑战，包括推理速度变慢和内存成本增加。大多数预训练的LLMs无法推广到其原始训练序列长度之外。为了解决这个问题，作者提出了InfiniteHiP，一种新的LLM推理框架，通过动态消除不相关的上下文标记来加速处理。该方法还允许通过选择性应用各种RoPE调整方法来推广到更长的序列。此外，作者在推理过程中将键值缓存卸载到主机内存，显著减少了GPU内存压力。结果是，InfiniteHiP能够在单个L40s 48GB GPU上处理多达300万个标记，而不会永久丢失上下文信息，并且在100万个标记的上下文中实现了18.95倍的注意力解码加速。",
        "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU",
        "pinyin": "这篇文章讨论了现代大语言模型（LLMs）处理非常长的上下文时面临的挑战，包括推理速度变慢和内存成本增加。大多数预训练的LLMs无法推广到其原始训练序列长度之外。为了解决这个问题，作者提出了InfiniteHiP，一种新的LLM推理框架，通过动态消除不相关的上下文标记来加速处理。该方法还允许通过选择性应用各种RoPE调整方法来推广到更长的序列。此外，作者在推理过程中将键值缓存卸载到主机内存，显著减少了GPU内存压力。结果是，InfiniteHiP能够在单个L40s 48GB GPU上处理多达300万个标记，而不会永久丢失上下文信息，并且在100万个标记的上下文中实现了18.95倍的注意力解码加速。\n\nzhè piān wén zhāng tǎo lùn le xiàn dài dà yǔ yán mó xíng (LLMs) chǔ lǐ fēi cháng cháng de shàng xià wén shí miàn lín de tiǎo zhàn, bāo kuò tuī lǐ sù dù biàn màn hé nèi cún chéng bèn zēng jiā. dà duō shù yù xùn liàn de LLMs wú fǎ tuī guǎng dào qí yuán shǐ xùn liàn xù liè cháng dù zhī wài. wèi le jiě jué zhè ge wèn tí, zuò zhě tí chū le InfiniteHiP, yī zhǒng xīn de LLM tuī lǐ kuàng jià, tōng guò dòng tài xiāo chú bù xiāng guān de shàng xià wén biāo jì lái jiā sù chǔ lǐ. gǎi fǎng huò yǔn xù tōng guò xuǎn zé xìng yìng yòng gè zhǒng RoPE tiáo zhěng fǎng fǎ lái tuī guǎng dào gèng cháng de xù liè. cǐ wài, zuò zhě zài tuī lǐ guò chéng zhōng jiāng jiàn zhí bèi huàn cún dào zhǔ jī nèi cún, xiǎn zhù jiǎn shǎo le GPU nèi cún yā lì. jié guǒ shì, InfiniteHiP néng gòu zài dān gè L40s 48GB GPU shàng chǔ lǐ duō dà 300 wàn gè biāo jì, ér bù huì yǒng jiǔ diū shī shàng xià wén xìn xī, bìng qiě zài 100 wàn gè biāo jì de shàng xià wén zhōng shí xiàn le 18.95 bèi de zhù yì jiě mǎ jiā sù.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"现代\", \"pinyin\": \"xiàn dài\", \"trans\": \"modern\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"process\"},\n    {\"word\": \"非常\", \"pinyin\": \"fēi cháng\", \"trans\": \"very\"},\n    {\"word\": \"上下文\", \"pinyin\": \"shàng xià wén\", \"trans\": \"context\"},\n    {\"word\": \"面临\", \"pinyin\": \"miàn lín\", \"trans\": \"face\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"速度\", \"pinyin\": \"sù dù\", \"trans\": \"speed\"},\n    {\"word\": \"变慢\", \"pinyin\": \"biàn màn\", \"trans\": \"slow down\"},\n    {\"word\": \"内存\", \"pinyin\": \"nèi cún\", \"trans\": \"memory\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"增加\", \"pinyin\": \"zēng jiā\", \"trans\": \"increase\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-trained\"},\n    {\"word\": \"无法\", \"pinyin\": \"wú fǎ\", \"trans\": \"unable\"},\n    {\"word\": \"推广\", \"pinyin\": \"tuī guǎng\", \"trans\": \"extend\"},\n    {\"word\": \"原始\", \"pinyin\": \"yuán shǐ\", \"trans\": \"original\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"training\"},\n    {\"word\": \"序列\", \"pinyin\": \"xù liè\", \"trans\": \"sequence\"},\n    {\"word\": \"长度\", \"pinyin\": \"cháng dù\", \"trans\": \"length\"},\n    {\"word\": \"之外\", \"pinyin\": \"zhī wài\", \"trans\": \"beyond\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"动态\", \"pinyin\": \"dòng tài\", \"trans\": \"dynamic\"},\n    {\"word\": \"消除\", \"pinyin\": \"xiāo chú\", \"trans\": \"eliminate\"},\n    {\"word\": \"不相关\", \"pinyin\": \"bù xiāng guān\", \"trans\": \"irrelevant\"},\n    {\"word\": \"标记\", \"pinyin\": \"biāo jì\", \"trans\": \"token\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"accelerate\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"选择性\", \"pinyin\": \"xuǎn zé xìng\", \"trans\": \"selective\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"apply\"},\n    {\"word\": \"各种\", \"pinyin\": \"gè zhǒng\", \"trans\": \"various\"},\n    {\"word\": \"RoPE\", \"pinyin\": \"RoPE\", \"trans\": \"RoPE\"},\n    {\"word\": \"调整\", \"pinyin\": \"tiáo zhěng\", \"trans\": \"adjust\"},\n    {\"word\": \"键值\", \"pinyin\": \"jiàn zhí\", \"trans\": \"key-value\"},\n    {\"word\": \"缓存\", \"pinyin\": \"huǎn cún\", \"trans\": \"cache\"},\n    {\"word\": \"卸载\", \"pinyin\": \"xiè zài\", \"trans\": \"offload\"},\n    {\"word\": \"主机\", \"pinyin\": \"zhǔ jī\", \"trans\": \"host\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"减少\", \"pinyin\": \"jiǎn shǎo\", \"trans\": \"reduce\"},\n    {\"word\": \"压力\", \"pinyin\": \"yā lì\", \"trans\": \"pressure\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"能够\", \"pinyin\": \"néng gòu\", \"trans\": \"be able to\"},\n    {\"word\": \"单个\", \"pinyin\": \"dān gè\", \"trans\": \"single\"},\n    {\"word\": \"L40s\", \"pinyin\": \"L40s\", \"trans\": \"L40s\"},\n    {\"word\": \"48GB\", \"pinyin\": \"48GB\", \"trans\": \"48GB\"},\n    {\"word\": \"GPU\", \"pinyin\": \"GPU\", \"trans\": \"GPU\"},\n    {\"word\": \"永久\", \"pinyin\": \"yǒng jiǔ\", \"trans\": \"permanent\"},\n    {\"word\": \"丢失\", \"pinyin\": \"diū shī\", \"trans\": \"lose\"},\n    {\"word\": \"信息\", \"pinyin\": \"xìn xī\", \"trans\": \"information\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"注意力\", \"pinyin\": \"zhù yì lì\", \"trans\": \"attention\"},\n    {\"word\": \"解码\", \"pinyin\": \"jiě mǎ\", \"trans\": \"decode\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"accelerate\"}\n]",
        "trans": "This article discusses the challenges faced by modern large language models (LLMs) when handling very long contexts, including slower inference speeds and increased memory costs. Most pre-trained LLMs cannot generalize beyond their original training sequence lengths. To address this issue, the authors propose InfiniteHiP, a new LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens. This method also allows for generalization to longer sequences by selectively applying various RoPE adjustment methods. Additionally, the authors offload key-value caches to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP can process up to 3 million tokens on a single L40s 48GB GPU without permanently losing context information and achieves an 18.95-fold speedup in attention decoding within a context of 1 million tokens.",
        "update_ts": "2025-02-14 09:10"
    }
}