{
    "date": {
        "ru": "9 –∞–ø—Ä–µ–ª—è",
        "en": "April 9",
        "zh": "4Êúà9Êó•"
    },
    "time_utc": "2025-04-09 15:14",
    "weekday": 2,
    "issue_id": 3150,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.06263",
            "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
            "url": "https://huggingface.co/papers/2504.06263",
            "abstract": "Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.",
            "score": 68,
            "issue_id": 3138,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 –∞–ø—Ä–µ–ª—è",
                "en": "April 8",
                "zh": "4Êúà8Êó•"
            },
            "hash": "3b3365aa60717b2a",
            "authors": [
                "Yiying Yang",
                "Wei Cheng",
                "Sijin Chen",
                "Xianfang Zeng",
                "Jiaxu Zhang",
                "Liao Wang",
                "Gang Yu",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.06263.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "OmniSVG: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò",
                    "desc": "OmniSVG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ SVG —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑—É–µ—Ç –∫–æ–º–∞–Ω–¥—ã –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã SVG –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, —Ä–∞–∑–¥–µ–ª—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –ª–æ–≥–∏–∫—É –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—é –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MMSVG-2M —Å 2 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö SVG-—Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ OmniSVG –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∏ –∏–º–µ–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –¥–∏–∑–∞–π–Ω–∞ SVG."
                },
                "en": {
                    "title": "OmniSVG: Revolutionizing SVG Generation with Vision-Language Models",
                    "desc": "This paper presents OmniSVG, a novel framework for generating high-quality Scalable Vector Graphics (SVG) using pre-trained Vision-Language Models (VLMs). It addresses the limitations of existing methods by producing structured outputs efficiently, avoiding the high computational costs and oversimplification seen in previous approaches. OmniSVG achieves this by converting SVG commands and coordinates into discrete tokens, allowing for a clear separation of structural logic from geometric details. Additionally, the introduction of the MMSVG-2M dataset, containing two million annotated SVG assets, supports the framework's training and evaluation, showcasing its superiority over current SVG generation techniques."
                },
                "zh": {
                    "title": "OmniSVGÔºöÈ´òÊïàÁîüÊàêÂ§çÊùÇSVGÁöÑÁªü‰∏ÄÊ°ÜÊû∂",
                    "desc": "Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫OmniSVGÁöÑÁªü‰∏ÄÊ°ÜÊû∂ÔºåÁî®‰∫éÁîüÊàêÈ´òË¥®ÈáèÂíåÂ§çÊùÇÁöÑÂèØÁº©ÊîæÁü¢ÈáèÂõæÂΩ¢ÔºàSVGÔºâ„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÔºåÈÄöËøáÂ∞ÜSVGÂëΩ‰ª§ÂíåÂùêÊ†áÂèÇÊï∞Âåñ‰∏∫Á¶ªÊï£Ê†áËÆ∞ÔºåÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑÁ´ØÂà∞Á´ØÂ§öÊ®°ÊÄÅSVGÁîüÊàê„ÄÇOmniSVGÂ∞ÜÁªìÊûÑÈÄªËæë‰∏é‰ΩéÁ∫ßÂá†‰ΩïËß£ËÄ¶Ôºå‰ªéËÄåÂú®‰øùÊåÅÂ§çÊùÇSVGÁªìÊûÑË°®Áé∞ÂäõÁöÑÂêåÊó∂ÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜMMSVG-2MÊï∞ÊçÆÈõÜÔºåÂåÖÂê´‰∏§Áôæ‰∏á‰∏™‰∏∞ÂØåÊ≥®ÈáäÁöÑSVGËµÑ‰∫ßÔºå‰ª•Êé®Âä®SVGÂêàÊàêÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05599",
            "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
            "url": "https://huggingface.co/papers/2504.05599",
            "abstract": "We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retraining of either the foundational language model or the vision encoder. To strengthen visual-text alignment, we propose a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), significantly enhancing cross-modal integration efficiency. Additionally, we introduce an adaptive-length Chain-of-Thought distillation approach for reasoning data generation. This approach dynamically optimizes reasoning chain lengths, thereby enhancing inference efficiency and preventing excessive reasoning overthinking. Empirical evaluations demonstrate that Skywork R1V, with only 38B parameters, delivers competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. Meanwhile, it maintains robust textual reasoning performance, evidenced by impressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model weights have been publicly released to promote openness and reproducibility.",
            "score": 49,
            "issue_id": 3142,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 –∞–ø—Ä–µ–ª—è",
                "en": "April 8",
                "zh": "4Êúà8Êó•"
            },
            "hash": "b963d5098e669229",
            "authors": [
                "Yi Peng",
                "Chris",
                "Xiaokun Wang",
                "Yichen Wei",
                "Jiangbo Pei",
                "Weijie Qiu",
                "Ai Jian",
                "Yunzhuo Hao",
                "Jiachun Pan",
                "Tianyidan Xie",
                "Li Ge",
                "Rongxian Zhuang",
                "Xuchen Song",
                "Yang Liu",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05599.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#transfer_learning",
                    "#benchmark",
                    "#inference",
                    "#architecture",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Skywork R1V - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Ä–∞—Å—à–∏—Ä—è—é—â—É—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–µ—Ä–∏–∏ R1 –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–æ—Ä –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –∑–∞–¥–∞—á–∞–º –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, —Å–æ—á–µ—Ç–∞—é—â—É—é –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –≥—Ä—É–ø–ø–æ–≤—É—é –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏, –∞ —Ç–∞–∫–∂–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Skywork R1V —Å 38 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö."
                },
                "en": {
                    "title": "Seamless Multimodal Reasoning with Skywork R1V",
                    "desc": "Skywork R1V is a new multimodal reasoning model that enhances the capabilities of existing R1-series large language models by integrating visual data. It uses a lightweight visual projector to adapt to visual inputs without needing to retrain the foundational language model or the vision encoder. The model employs a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning and Group Relative Policy Optimization to improve the alignment between text and visual information. Additionally, it features an adaptive-length Chain-of-Thought distillation method that optimizes reasoning processes, leading to efficient inference and strong performance on various benchmarks."
                },
                "zh": {
                    "title": "Skywork R1VÔºöÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°Âûã",
                    "desc": "Skywork R1VÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°ÂûãÔºåÂÆÉÈÄöËøáÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅËΩ¨ÁßªÊñπÊ≥ïÊâ©Â±ï‰∫ÜR1Á≥ªÂàóÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂà∞ËßÜËßâÊ®°ÊÄÅ„ÄÇËØ•Ê®°ÂûãÂà©Áî®ËΩªÈáèÁ∫ßËßÜËßâÊäïÂΩ±Âô®ÔºåÂÆûÁé∞‰∫ÜÊó†È°ªÈáçÊñ∞ËÆ≠ÁªÉÂü∫Á°ÄËØ≠Ë®ÄÊ®°ÂûãÊàñËßÜËßâÁºñÁ†ÅÂô®ÁöÑÊó†ÁºùÂ§öÊ®°ÊÄÅÈÄÇÂ∫î„ÄÇ‰∏∫‰∫ÜÂ¢ûÂº∫ËßÜËßâ‰∏éÊñáÊú¨ÁöÑÂØπÈΩêÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑Âêà‰ºòÂåñÁ≠ñÁï•ÔºåÁªìÂêà‰∫ÜËø≠‰ª£ÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÁªÑÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâÔºåÊòæËëóÊèêÈ´ò‰∫ÜË∑®Ê®°ÊÄÅÈõÜÊàêÁöÑÊïàÁéá„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÈïøÂ∫¶ÁöÑÊÄùÁª¥ÈìæËí∏È¶èÊñπÊ≥ïÔºåÂä®ÊÄÅ‰ºòÂåñÊé®ÁêÜÈìæÁöÑÈïøÂ∫¶Ôºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÊïàÁéáÔºåÈÅøÂÖçËøáÂ∫¶ÊÄùËÄÉ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06261",
            "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
            "url": "https://huggingface.co/papers/2504.06261",
            "abstract": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
            "score": 48,
            "issue_id": 3141,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 –∞–ø—Ä–µ–ª—è",
                "en": "April 8",
                "zh": "4Êúà8Êó•"
            },
            "hash": "354599744af26b06",
            "authors": [
                "Gleb Rodionov",
                "Roman Garipov",
                "Alina Shutova",
                "George Yakushev",
                "Vage Egiazarian",
                "Anton Sinitsin",
                "Denis Kuznedelev",
                "Dan Alistarh"
            ],
            "affiliations": [
                "HSE University",
                "IST Austria",
                "Yandex"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06261.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#long_context",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–°–∞–º–æ–æ—Ä–≥–∞–Ω–∏–∑—É—é—â–µ–µ—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∑–∞–¥–∞—á –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–∏—Å—Ç–µ–º—É Hogwild! Inference, –≥–¥–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –æ–¥–Ω–æ–π LLM —Ä–∞–±–æ—Ç–∞—é—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—â–∏–π –∫—ç—à –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–µ—à–∞—è, –∫–∞–∫ –ª—É—á—à–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—Ç—å. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏, –∏–º–µ—è –¥–æ—Å—Ç—É–ø –∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –¥—Ä—É–≥ –¥—Ä—É–≥–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Rotary Position Embeddings –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤."
                },
                "en": {
                    "title": "Collaborative Parallelism for Enhanced LLM Efficiency",
                    "desc": "This paper explores a new method for improving the efficiency of Large Language Models (LLMs) during complex tasks by enabling them to work in parallel. The authors introduce a system called Hogwild! Inference, where multiple LLM instances share an attention cache, allowing them to see each other's progress and collaborate effectively. This approach leverages Rotary Position Embeddings (RoPE) to enhance performance without the need for extra fine-tuning. The findings suggest that LLMs can autonomously develop collaboration strategies, leading to faster and more efficient problem-solving."
                },
                "zh": {
                    "title": "Âπ∂Ë°åÂêà‰ΩúÔºåÊèêÂçáLLMÊé®ÁêÜÊïàÁéá",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÉΩÂ§üÈÄöËøáÈ´òÁ∫ßÊé®ÁêÜ„ÄÅÈïøÁØáÂÜÖÂÆπÁîüÊàêÂíåÂ∑•ÂÖ∑‰ΩøÁî®Êù•Â§ÑÁêÜË∂äÊù•Ë∂äÂ§çÊùÇÁöÑ‰ªªÂä°„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õ‰ªªÂä°ÔºåÁ†îÁ©∂Ë°®ÊòéLLMsÂèØ‰ª•ÈÄöËøáÂÆûÁé∞ÊòéÁ°ÆÁöÑÂêà‰ΩúÊ°ÜÊû∂Êù•Âπ∂Ë°åÊìç‰ΩúÔºå‰æãÂ¶ÇÊäïÁ•®Êú∫Âà∂ÊàñÁã¨Á´ãÂ≠ê‰ªªÂä°ÁöÑÂàõÂª∫„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆæËÆ°ÊñπÊ≥ïÔºöËÆ©LLM‚ÄúÂ∑•‰ΩúËÄÖ‚ÄùÂπ∂Ë°åËøêË°åÔºåÈÄöËøáÂêåÊó∂Êõ¥Êñ∞ÁöÑÊ≥®ÊÑèÂäõÁºìÂ≠òËøõË°åÂêåÊ≠•ÔºåÂπ∂ÂÜ≥ÂÆöÊúÄ‰Ω≥ÁöÑÂêà‰ΩúÊñπÂºè„ÄÇÊàë‰ª¨ÂÆûÁé∞‰∫ÜHogwild!Êé®ÁêÜÔºåËøôÊòØ‰∏ÄÁßçÂπ∂Ë°åLLMÊé®ÁêÜÂºïÊìéÔºåÂ§ö‰∏™Áõ∏ÂêåÁöÑLLMÂÆû‰æãÂú®ÂÖ±‰∫´ÁöÑÊ≥®ÊÑèÂäõÁºìÂ≠ò‰∏≠Âπ∂Ë°åËøêË°åÔºåËÉΩÂ§ü‚ÄúÂç≥Êó∂‚ÄùËÆøÈóÆÂΩºÊ≠§ÁîüÊàêÁöÑÊ†áËÆ∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05979",
            "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
            "url": "https://huggingface.co/papers/2504.05979",
            "abstract": "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.",
            "score": 43,
            "issue_id": 3137,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 –∞–ø—Ä–µ–ª—è",
                "en": "April 8",
                "zh": "4Êúà8Êó•"
            },
            "hash": "f1195a87ec5b86f1",
            "authors": [
                "Sixiang Chen",
                "Jinbin Bai",
                "Zhuoran Zhao",
                "Tian Ye",
                "Qingyu Shi",
                "Donghao Zhou",
                "Wenhao Chai",
                "Xin Lin",
                "Jianzong Wu",
                "Chao Tang",
                "Shilin Xu",
                "Tao Zhang",
                "Haobo Yuan",
                "Yikang Zhou",
                "Wei Chow",
                "Linfeng Li",
                "Xiangtai Li",
                "Lei Zhu",
                "Lu Qi"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology (GZ)",
                "University of Washington",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05979.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#architecture",
                    "#multimodal",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "GPT-4o: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–¥–µ–ª—å—é GPT-4o. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ GPT-4o —Å –≤–µ–¥—É—â–∏–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –±–æ–ª–µ–µ —á–µ–º 20 –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –û—Ü–µ–Ω–∫–∞ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–≤-3D –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–≤-X. –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤—ã—è–≤–ª—è—é—Ç—Å—è —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã GPT-4o –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –∞ —Ç–∞–∫–∂–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Unifying Image and Text Generation with GPT-4o",
                    "desc": "This paper explores the advancements in image generation, focusing on the capabilities of the GPT-4o model. It conducts a thorough evaluation of GPT-4o's performance in various generative tasks, including text-to-image and image-to-3D generation. The study benchmarks GPT-4o against other leading models, revealing its strengths and weaknesses in multimodal generation. The findings suggest future directions for improving unified generative architectures, particularly in terms of design and data utilization."
                },
                "zh": {
                    "title": "Êé¢Á¥¢Áªü‰∏ÄÁîüÊàêÊ®°ÂûãÁöÑÊú™Êù•ÊñπÂêë",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂõæÂÉèÁîüÊàêÈ¢ÜÂüüÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÁâπÂà´ÊòØGPT-4oÊ®°ÂûãÂú®ÂõæÂÉèÁîüÊàêÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÂØπÂÖ∂ËøõË°å‰∫ÜÂÆûËØÅÁ†îÁ©∂ÔºåÂπ∂‰∏éÈ¢ÜÂÖàÁöÑÂºÄÊ∫êÂíåÂïÜ‰∏öÊ®°ÂûãËøõË°å‰∫ÜÂü∫ÂáÜÊµãËØïÔºåÊ∂µÁõñ‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉè„ÄÅÂõæÂÉèÂà∞ÂõæÂÉè„ÄÅÂõæÂÉèÂà∞3DÂíåÂõæÂÉèÂà∞XÁîüÊàêÁ≠âÂõõ‰∏™‰∏ªË¶ÅÁ±ªÂà´„ÄÇÂàÜÊûêÁªìÊûúÊè≠Á§∫‰∫ÜGPT-4oÂú®‰∏çÂêåËÆæÁΩÆ‰∏ãÁöÑ‰ºòÁº∫ÁÇπÔºåÂπ∂Â∞ÜÂÖ∂ÁΩÆ‰∫éÁîüÊàêÂª∫Ê®°ÁöÑÊõ¥ÂπøÊ≥õÊºîÂèò‰∏≠„ÄÇÈÄöËøáËøôÈ°πÁ†îÁ©∂ÔºåÊàë‰ª¨ËØÜÂà´Âá∫Êú™Êù•Áªü‰∏ÄÁîüÊàêÊ®°ÂûãÁöÑÊúâÂ∏åÊúõÁöÑÊñπÂêëÔºåÂº∫Ë∞É‰∫ÜÊû∂ÊûÑËÆæËÆ°ÂíåÊï∞ÊçÆÊâ©Â±ïÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05535",
            "title": "COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for\n  Alignment with Human Values",
            "url": "https://huggingface.co/papers/2504.05535",
            "abstract": "Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and response labeling significantly constrains the scalability of human preference datasets. To address these challenges, we design an LLM-based Chinese preference dataset annotation pipeline with no human intervention. Specifically, we crawled and carefully filtered 92k high-quality Chinese queries and employed 15 mainstream LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce COIG-P (Chinese Open Instruction Generalist - Preference), a high-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese preference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel, and Role. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously constructed a Chinese Reward Benchmark (CRBench). Evaluation results based on AlignBench liu2024alignbenchbenchmarkingchinesealignment show that that COIG-P significantly outperforms other Chinese preference datasets, and it brings significant performance improvements ranging from 2% to 12% for the Qwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results on CRBench demonstrate that our CRM has a strong and robust scoring ability. We apply it to filter chosen-rejected response pairs in a test split of COIG-P, and our experiments show that it is comparable to GPT-4o in identifying low-quality samples while maintaining efficiency and cost-effectiveness. Our codes and data are released in https://github.com/multimodal-art-projection/COIG-P.",
            "score": 30,
            "issue_id": 3145,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 –∞–ø—Ä–µ–ª—è",
                "en": "April 7",
                "zh": "4Êúà7Êó•"
            },
            "hash": "420fafe139f00d54",
            "authors": [
                "M-A-P Team",
                "Siwei Wu",
                "Jincheng Ren",
                "Xinrun Du",
                "Shuyue Guo",
                "Xingwei Qu",
                "Yiming Liang",
                "Jie Liu",
                "Yunwen Li",
                "Tianyu Zheng",
                "Boyu Feng",
                "Huaqing Yuan",
                "Zenith Wang",
                "Jiaheng Liu",
                "Wenhao Huang",
                "Chenglin Cai",
                "Haoran Que",
                "Jian Yang",
                "Yuelin Bai",
                "Zekun Moore Wang",
                "Zhouliang Yu",
                "Qunshu Lin",
                "Ding Pan",
                "Yuchen Jiang",
                "Tiannan Wang",
                "Wangchunshu Zhou",
                "Shenzhi Wang",
                "Xingyuan Bu",
                "Minghao Liu",
                "Guoyin Wang",
                "Ge Zhang",
                "Chenghua Lin"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.05535.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#data",
                    "#rlhf",
                    "#open_source"
                ],
                "emoji": "üá®üá≥",
                "ru": {
                    "title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è LLM",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç COIG-P - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –∫–∏—Ç–∞–π—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM, —Å–æ–±—Ä–∞–≤ –±–æ–ª–µ–µ –º–∏–ª–ª–∏–æ–Ω–∞ –ø–∞—Ä –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤ 6 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö. –ù–∞ –æ—Å–Ω–æ–≤–µ COIG-P –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (CRM) –∏ —Å–æ–∑–¥–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫ CRBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ COIG-P –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –∫–∏—Ç–∞–π—Å–∫–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ 2-12%."
                },
                "en": {
                    "title": "Revolutionizing Chinese Preference Datasets with LLMs",
                    "desc": "This paper presents a novel approach to creating a large-scale Chinese preference dataset, COIG-P, which addresses the limitations of existing datasets. By utilizing 15 mainstream large language models (LLMs) to generate and score response pairs, the authors eliminate the need for human annotators, enhancing scalability. The dataset includes 1,009k preference pairs across six diverse domains, significantly improving performance metrics for various models. Additionally, a Chinese Reward Model (CRM) is developed to efficiently score responses, demonstrating strong performance in identifying low-quality samples compared to existing benchmarks."
                },
                "zh": {
                    "title": "ÊûÑÂª∫È´òË¥®Èáè‰∏≠ÊñáÂÅèÂ•ΩÊï∞ÊçÆÈõÜÁöÑÂàõÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†‰∫∫Â∑•Âπ≤È¢ÑÁöÑ‰∏≠ÊñáÂÅèÂ•ΩÊï∞ÊçÆÈõÜÊ≥®ÈáäÁÆ°ÈÅìÔºå‰ª•Ëß£ÂÜ≥Áé∞Êúâ‰∏≠ÊñáÂÅèÂ•ΩÊï∞ÊçÆÈõÜËßÑÊ®°Â∞è„ÄÅÈ¢ÜÂüüÁã≠Á™ÑÂíåÁº∫‰πè‰∏•Ê†ºÈ™åËØÅÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨Êî∂ÈõÜÂπ∂Á≠õÈÄâ‰∫Ü92,000‰∏™È´òË¥®Èáè‰∏≠ÊñáÊü•ËØ¢ÔºåÂπ∂Âà©Áî®15‰∏™‰∏ªÊµÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂíåËØÑÂàÜÈÄâÊã©-ÊãíÁªùÁöÑÂìçÂ∫îÂØπ„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜCOIG-PÊï∞ÊçÆÈõÜÔºåÂåÖÂê´1,009,000‰∏™‰∏≠ÊñáÂÅèÂ•ΩÂØπÔºåË¶ÜÁõñËÅäÂ§©„ÄÅ‰ª£Á†Å„ÄÅÊï∞Â≠¶„ÄÅÈÄªËæë„ÄÅÂ∞èËØ¥ÂíåËßíËâ≤Á≠âÂÖ≠‰∏™Â§öÊ†∑ÂåñÈ¢ÜÂüü„ÄÇÈÄöËøáËÆ≠ÁªÉ‰∏Ä‰∏™8BËßÑÊ®°ÁöÑ‰∏≠ÊñáÂ•ñÂä±Ê®°ÂûãÔºàCRMÔºâÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏≠ÊñáÂ•ñÂä±Âü∫ÂáÜÔºàCRBenchÔºâÔºåÂπ∂Âú®ËØÑ‰º∞‰∏≠ÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02160",
            "title": "Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation",
            "url": "https://huggingface.co/papers/2504.02160",
            "abstract": "Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.",
            "score": 22,
            "issue_id": 3139,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 –∞–ø—Ä–µ–ª—è",
                "en": "April 2",
                "zh": "4Êúà2Êó•"
            },
            "hash": "511e3ea71050e14e",
            "authors": [
                "Shaojin Wu",
                "Mengqi Huang",
                "Wenxu Wu",
                "Yufeng Cheng",
                "Fei Ding",
                "Qian He"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02160.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#synthetic",
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –û–Ω–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –º–æ–¥–µ–ª—å UNO —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç—å –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –æ–¥–Ω–∏–º –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏."
                },
                "en": {
                    "title": "Enhancing Multi-Subject Image Generation with Consistent Data Synthesis",
                    "desc": "This paper addresses the challenges of generating images with multiple subjects by proposing a new data synthesis pipeline. The authors utilize diffusion transformers to create high-consistency paired data for both single and multi-subject scenarios. They introduce a novel model called UNO, which incorporates cross-modal alignment and rotary position embedding to enhance the generation process. Experimental results demonstrate that their approach maintains high consistency and controllability in image generation tasks."
                },
                "zh": {
                    "title": "È´ò‰∏ÄËá¥ÊÄßÂ§ö‰∏ªÈ¢òÁîüÊàêÁöÑÂàõÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂú®ÂõæÂÉèÁîüÊàê‰∏≠ÔºåÂ¶Ç‰ΩïËß£ÂÜ≥‰ª•‰∏ªÈ¢ò‰∏∫È©±Âä®ÁöÑÁîüÊàêÈù¢‰∏¥ÁöÑÊï∞ÊçÆÂèØÊâ©Â±ïÊÄßÂíå‰∏ªÈ¢òÊâ©Â±ïÊÄßÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ´ò‰∏ÄËá¥ÊÄßÁöÑÊï∞ÊçÆÂêàÊàêÁÆ°ÈÅìÔºåÂà©Áî®Êâ©Êï£ÂèòÊç¢Âô®ÁöÑÂÜÖÂú®ÁîüÊàêËÉΩÂäõÔºåÁîüÊàêÈ´ò‰∏ÄËá¥ÊÄßÁöÑÂ§ö‰∏ªÈ¢òÈÖçÂØπÊï∞ÊçÆ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜUNOÊ®°ÂûãÔºåÁªìÂêà‰∫ÜÊ∏êËøõÁöÑË∑®Ê®°ÊÄÅÂØπÈΩêÂíåÈÄöÁî®ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºåËÉΩÂ§üÂú®Âçï‰∏ªÈ¢òÂíåÂ§ö‰∏ªÈ¢òÁîüÊàê‰∏≠‰øùÊåÅÈ´ò‰∏ÄËá¥ÊÄßÂíåÂèØÊéßÊÄß„ÄÇÈÄöËøáÂ§ßÈáèÂÆûÈ™åÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÁîüÊàêË¥®ÈáèÂíåÊéßÂà∂ËÉΩÂäõ‰∏äÂùáË°®Áé∞Âá∫Ëâ≤„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02810",
            "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
            "url": "https://huggingface.co/papers/2504.02810",
            "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.",
            "score": 10,
            "issue_id": 3138,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 –∞–ø—Ä–µ–ª—è",
                "en": "April 3",
                "zh": "4Êúà3Êó•"
            },
            "hash": "5a9b5f817a1c09b6",
            "authors": [
                "Haowei Lin",
                "Xiangyu Wang",
                "Ruilin Yan",
                "Baizhou Huang",
                "Haotian Ye",
                "Jianhua Zhu",
                "Zihao Wang",
                "James Zou",
                "Jianzhu Ma",
                "Yitao Liang"
            ],
            "affiliations": [
                "Computer Science Department, Stanford University, California, United States",
                "Department of Electronic Engineering, Tsinghua University, Beijing, China",
                "Institute for AI Industry Research, Tsinghua University, Beijing, China",
                "Institute for Artificial Intelligence, Peking University, Beijing, China",
                "Wangxuan institute of computer technology, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02810.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "KUMO: –Ω–æ–≤—ã–π —Å–ø–æ—Å–æ–± –æ—Ü–µ–Ω–∏—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ KUMO - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. KUMO –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –∫–æ–º–±–∏–Ω–∏—Ä—É—è LLM —Å —Å–∏–º–≤–æ–ª—å–Ω—ã–º–∏ –¥–≤–∏–∂–∫–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∑–∞—Å—Ç–∞–≤–ª—è—è –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ, –∞ –Ω–µ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 23 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM –Ω–∞ 5000 –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –º–Ω–æ–≥–∏–µ –∏–∑ –Ω–∏—Ö –ø—Ä–µ–≤–∑–æ—à–ª–∏ —É—Ä–æ–≤–µ–Ω—å —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –≤ –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ."
                },
                "en": {
                    "title": "KUMO: Unveiling True Reasoning in LLMs",
                    "desc": "This paper introduces KUMO, a new framework for evaluating the reasoning abilities of large language models (LLMs). KUMO generates diverse reasoning tasks that require models to demonstrate true understanding rather than simple recall from their training data. By combining LLMs with symbolic engines, it creates adjustable tasks that challenge models across various domains. The evaluation shows that many LLMs can outperform university students on easier tasks and achieve comparable performance on more complex reasoning challenges, highlighting KUMO's effectiveness as a reliable assessment tool."
                },
                "zh": {
                    "title": "KUMOÔºöËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Â∑•ÂÖ∑",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊòØÂê¶ÁúüÊ≠£ÂÖ∑Â§áÊé®ÁêÜËÉΩÂäõÔºåËøòÊòØ‰ªÖ‰ªÖ‰ªéÂÖ∂Â∫ûÂ§ßÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏≠ÂõûÂøÜÁ≠îÊ°à„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜKUMOÔºå‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞LLMsÊé®ÁêÜËÉΩÂäõÁöÑÁîüÊàêËØÑ‰º∞Ê°ÜÊû∂„ÄÇKUMOÁªìÂêà‰∫ÜLLMsÂíåÁ¨¶Âè∑ÂºïÊìéÔºåÂä®ÊÄÅÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊé®ÁêÜ‰ªªÂä°Ôºå‰øÉËøõÊ®°ÂûãÂ±ïÁ§∫ÁúüÊ≠£ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÂØπ23‰∏™ÊúÄÂÖàËøõÁöÑLLMsËøõË°åËØÑ‰º∞ÔºåÁªìÊûúÊòæÁ§∫ËÆ∏Â§öÊ®°ÂûãÂú®ÁÆÄÂçïÊé®ÁêÜ‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜÂ§ßÂ≠¶ÁîüÁöÑË°®Áé∞ÔºåËÄåÂú®Â§çÊùÇÊé®ÁêÜÊåëÊàò‰∏≠‰πüËææÂà∞‰∫ÜÂ§ßÂ≠¶Ê∞¥Âπ≥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05594",
            "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model",
            "url": "https://huggingface.co/papers/2504.05594",
            "abstract": "Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.",
            "score": 9,
            "issue_id": 3138,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 –∞–ø—Ä–µ–ª—è",
                "en": "April 8",
                "zh": "4Êúà8Êó•"
            },
            "hash": "7da2f86ad5e0bfc2",
            "authors": [
                "Qi Mao",
                "Lan Chen",
                "Yuchao Gu",
                "Mike Zheng Shou",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore",
                "State Key Laboratory of Media Convergence and Communication, Communication University of China",
                "University of California at Merced",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05594.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "‚öñÔ∏è",
                "ru": {
                    "title": "–ë–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UnifyEdit - –º–µ—Ç–æ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–µ–∂–¥—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–≤–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–Ω–∏–º–∞–Ω–∏—è: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤–≤–µ–¥–µ–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–µ–≥—É–ª–∏—Ä—É—é—â–∏–π –≤–ª–∏—è–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –≤ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Achieving Perfect Balance in Text-Based Image Editing with UnifyEdit",
                    "desc": "This paper presents UnifyEdit, a novel method for text-based image editing that aims to balance fidelity and editability. Traditional approaches often struggle with over- or under-editing due to conflicting constraints in attention mechanisms. UnifyEdit introduces a unified framework that employs self-attention and cross-attention constraints to maintain structural fidelity and enhance text alignment, respectively. An adaptive time-step scheduler is also proposed to dynamically manage the influence of these constraints, ensuring optimal performance in various editing tasks."
                },
                "zh": {
                    "title": "Âπ≥Ë°°‰øùÁúüÂ∫¶‰∏éÂèØÁºñËæëÊÄßÁöÑÂàõÊñ∞ÊñπÊ≥ï",
                    "desc": "Âú®Âü∫‰∫éÊñáÊú¨ÁöÑÂõæÂÉèÁºñËæë‰∏≠ÔºåÂπ≥Ë°°‰øùÁúüÂ∫¶ÂíåÂèØÁºñËæëÊÄßÈùûÂ∏∏ÈáçË¶Å„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•‰øùÊåÅÁªìÊûÑÔºå‰ΩÜÁº∫‰πèÁªü‰∏ÄÁöÑÊú∫Âà∂Êù•Âπ≥Ë°°Ëøô‰∏§‰∏™ÁõÆÊ†á„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜUnifyEditÔºåËøôÊòØ‰∏ÄÁßçÊó†Ë∞É‰ºòÁöÑÊñπÊ≥ïÔºåÈÄöËøáÊâ©Êï£ÊΩúÂú®‰ºòÂåñÂÆûÁé∞‰øùÁúüÂ∫¶ÂíåÂèØÁºñËæëÊÄßÁöÑÂπ≥Ë°°„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫ÜËá™Ê≥®ÊÑèÂäõÂíå‰∫§ÂèâÊ≥®ÊÑèÂäõÁ∫¶ÊùüÔºåÂπ∂ÂºïÂÖ•Ëá™ÈÄÇÂ∫îÊó∂Èó¥Ê≠•Ë∞ÉÂ∫¶Âô®Ôºå‰ª•Âä®ÊÄÅË∞ÉÊï¥Ëøô‰∫õÁ∫¶ÊùüÁöÑÂΩ±ÂìçÔºå‰ªéËÄå‰ºòÂåñÁºñËæëÊïàÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06148",
            "title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric\n  Capabilities in Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2504.06148",
            "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.",
            "score": 7,
            "issue_id": 3144,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 –∞–ø—Ä–µ–ª—è",
                "en": "April 8",
                "zh": "4Êúà8Êó•"
            },
            "hash": "7d7e4a2197e26482",
            "authors": [
                "Xiangxi Zheng",
                "Linjie Li",
                "Zhengyuan Yang",
                "Ping Yu",
                "Alex Jinpeng Wang",
                "Rui Yan",
                "Yuan Yao",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Central South University, China",
                "Microsoft Corporation, USA",
                "Nanjing University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06148.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#multimodal",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "üéÆ",
                "ru": {
                    "title": "V-MAGE: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò —á–µ—Ä–µ–∑ –∏–≥—Ä–æ–≤—ã–µ –∑–∞–¥–∞—á–∏",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º V-MAGE. –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –ø—è—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏–≥—Ä —Å –±–æ–ª–µ–µ —á–µ–º 30 —É—Ä–æ–≤–Ω—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç–∏—Ä—É—é—Ç —Ç–∞–∫–∏–µ –Ω–∞–≤—ã–∫–∏, –∫–∞–∫ –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —Ç–∞–π–º–∏–Ω–≥ –∏ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –≤–µ–¥—É—â–∏—Ö MLLM –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤–æ –≤—Å–µ—Ö –∏–≥—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning in MLLMs with V-MAGE",
                    "desc": "This paper presents a new evaluation framework called Visual-centric Multiple Abilities Game Evaluation (V-MAGE) for assessing the visual reasoning skills of Multimodal Large Language Models (MLLMs). Unlike traditional benchmarks, V-MAGE includes dynamic, game-based tasks that require models to demonstrate core visual skills and higher-level reasoning abilities. The framework consists of five games with over 30 levels, focusing on tasks like positioning and visual memory. The evaluation reveals significant performance gaps between MLLMs and humans, highlighting the models' perceptual errors and suggesting areas for improvement in their reasoning strategies."
                },
                "zh": {
                    "title": "ËßÜËßâÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞ËØÑ‰º∞Ê°ÜÊû∂",
                    "desc": "ÊúÄËøëÔºåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§ö‰∏™Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÈöèÁùÄËØÑ‰º∞‰ªéÈùôÊÄÅÊï∞ÊçÆÈõÜËΩ¨ÂêëÂºÄÊîæ‰∏ñÁïåÁöÑÂä®ÊÄÅÁéØÂ¢ÉÔºåÁé∞ÊúâÁöÑÂü∫‰∫éÊ∏∏ÊàèÁöÑÂü∫ÂáÜÊµãËØïÊòæÂæó‰∏çË∂≥ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Áº∫‰πèËßÜËßâ‰∏≠ÂøÉÁöÑ‰ªªÂä°ÔºåÊó†Ê≥ïËØÑ‰º∞ÁúüÂÆû‰∏ñÁïåÂÜ≥Á≠ñÊâÄÈúÄÁöÑÂ§öÊ†∑Êé®ÁêÜËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËßÜËßâ‰∏≠ÂøÉÂ§öËÉΩÂäõÊ∏∏ÊàèËØÑ‰º∞ÔºàV-MAGEÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®ËØÑ‰º∞MLLMsËßÜËßâÊé®ÁêÜËÉΩÂäõÁöÑÊ∏∏ÊàèËØÑ‰º∞Ê°ÜÊû∂„ÄÇV-MAGEÂåÖÂê´‰∫î‰∏™Â§öÊ†∑ÂåñÁöÑÊ∏∏ÊàèÂíå30Â§ö‰∏™ÊâãÂ∑•ËÆæËÆ°ÁöÑÂÖ≥Âç°ÔºåÊµãËØïÊ®°ÂûãÂú®ÂÆö‰Ωç„ÄÅËΩ®ËøπË∑üË∏™„ÄÅÊó∂Êú∫ÊääÊè°ÂíåËßÜËßâËÆ∞ÂøÜÁ≠âÊ†∏ÂøÉËßÜËßâÊäÄËÉΩ‰∏äÁöÑË°®Áé∞Ôºå‰ª•ÂèäÈïøÊúüËßÑÂàíÂíåÊ∑±ÊÄùÁÜüËôëÁ≠âÊõ¥È´òÂ±ÇÊ¨°ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06232",
            "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned\n  Guidance",
            "url": "https://huggingface.co/papers/2504.06232",
            "abstract": "Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's superiority in achieving superior high-resolution image quality over current state-of-the-art methods.",
            "score": 6,
            "issue_id": 3145,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 –∞–ø—Ä–µ–ª—è",
                "en": "April 8",
                "zh": "4Êúà8Êó•"
            },
            "hash": "7def3ddddf039685",
            "authors": [
                "Jiazi Bu",
                "Pengyang Ling",
                "Yujie Zhou",
                "Pan Zhang",
                "Tong Wu",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06232.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "HiFlow: —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "HiFlow - —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ—Ç–æ–∫–æ–≤. –ú–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–µ—Ç –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –ø–æ—Ç–æ–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π capture —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ—Ç–æ–∫–µ –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. HiFlow –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —É—Å–∫–æ—Ä–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ HiFlow –Ω–∞–¥ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Unlocking High-Resolution Image Synthesis with HiFlow",
                    "desc": "This paper introduces HiFlow, a novel framework designed to enhance the capabilities of text-to-image (T2I) diffusion and flow models for generating high-resolution images. HiFlow operates without the need for additional training and is compatible with existing pre-trained flow models. It utilizes a virtual reference flow to align low-resolution and high-resolution data, ensuring consistency in low-frequency details, structural integrity, and fine details. The results show that HiFlow significantly improves the quality of high-resolution images compared to current leading methods, demonstrating its effectiveness and adaptability across various T2I models."
                },
                "zh": {
                    "title": "HiFlowÔºöÊèêÂçáÈ´òÂàÜËæ®ÁéáÂõæÂÉèÂêàÊàêÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫HiFlowÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥È´òÂàÜËæ®ÁéáÂõæÂÉèÂêàÊàê‰∏≠ÁöÑÊåëÊàò„ÄÇHiFlow‰∏çÈúÄË¶ÅËÆ≠ÁªÉÔºåÂπ∂‰∏î‰∏éÊ®°ÂûãÊó†ÂÖ≥ÔºåÂèØ‰ª•ÊúâÊïàÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÊµÅÊ®°Âûã„ÄÇÂÆÉÈÄöËøáÂª∫Á´ãËôöÊãüÂèÇËÄÉÊµÅÔºåÊçïÊçâ‰ΩéÂàÜËæ®ÁéáÊµÅ‰ø°ÊÅØÁöÑÁâπÂæÅÔºå‰ªéËÄåÊåáÂØºÈ´òÂàÜËæ®ÁéáÁîüÊàê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHiFlowÂú®È´òÂàÜËæ®ÁéáÂõæÂÉèË¥®Èáè‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00043",
            "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation",
            "url": "https://huggingface.co/papers/2504.00043",
            "abstract": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.",
            "score": 6,
            "issue_id": 3137,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 –º–∞—Ä—Ç–∞",
                "en": "March 30",
                "zh": "3Êúà30Êó•"
            },
            "hash": "2b2bfdd590c5394d",
            "authors": [
                "Jixuan Leng",
                "Chengsong Huang",
                "Langlin Huang",
                "Bill Yuchen Lin",
                "William W. Cohen",
                "Haohan Wang",
                "Jiaxin Huang"
            ],
            "affiliations": [
                "CMU",
                "UIUC",
                "UW",
                "WUSTL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00043.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "üß©",
                "ru": {
                    "title": "–ö—Ä–æ—Å—Å–≤–æ—Ä–¥—ã –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –æ—Ü–µ–Ω–∫–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
                    "desc": "CrossWordBench - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—Ä–æ—Å—Å–≤–æ—Ä–¥—ã –∫–∞–∫ –∑–∞–¥–∞—á—É, —Ç—Ä–µ–±—É—é—â—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Å–æ–±–ª—é–¥–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∏ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä —Å–µ—Ç–∫–∏. –û—Ü–µ–Ω–∫–∞ –±–æ–ª–µ–µ 20 –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ LLM —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ–ª–∏ –±–µ–∑ —Ç–∞–∫–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ LVLM —Å —ç—Ç–æ–π –∑–∞–¥–∞—á–µ–π, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é —Ä–µ—à–∞—Ç—å –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ä–∞–∑–±–æ—Ä–∞ —Å–µ—Ç–∫–∏."
                },
                "en": {
                    "title": "CrossWordBench: Evaluating Reasoning in LLMs and LVLMs with Crossword Puzzles",
                    "desc": "This paper presents CrossWordBench, a new benchmark for evaluating the reasoning abilities of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) using crossword puzzles. The benchmark focuses on the interaction between text-based clues and visual grid structures, requiring models to adhere to both semantic and intersectional constraints. The study shows that reasoning LLMs significantly outperform non-reasoning models by effectively utilizing crossing-letter constraints, while LVLMs face challenges linked to their grid-parsing accuracy. Overall, the findings highlight the limitations of current models in reasoning tasks and suggest a novel approach for multimodal evaluation."
                },
                "zh": {
                    "title": "Ë∑®Ê®°ÊÄÅÊé®ÁêÜÁöÑÊñ∞Âü∫ÂáÜÔºöCrossWordBench",
                    "desc": "Áé∞ÊúâÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÁöÑÊé®ÁêÜËØÑ‰º∞Ê°ÜÊû∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÊñáÊú¨Êé®ÁêÜÊàñËßÜËßâËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõ‰∏äÔºåÁº∫‰πèÊñáÊú¨‰∏éËßÜËßâ‰πãÈó¥ÁöÑÂä®ÊÄÅ‰∫íÂä®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜCrossWordBenchÔºåËøôÊòØ‰∏Ä‰∏™ÈÄöËøáÂ°´Â≠óÊ∏∏ÊàèËØÑ‰º∞LLMsÂíåLVLMsÊé®ÁêÜËÉΩÂäõÁöÑÂü∫ÂáÜÔºåË¶ÅÊ±ÇÂú®ÊñáÊú¨Á∫øÁ¥¢ÂíåËßÜËßâÁΩëÊ†ºÁªìÊûÑÁöÑËØ≠‰πâÁ∫¶Êùü‰∏ãËøõË°åÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÇCrossWordBenchÂà©Áî®ÂèØÊéßÁöÑÊãºÂõæÁîüÊàêÊ°ÜÊû∂ÔºåÁîüÊàêÂ§öÁßçÊ†ºÂºèÁöÑÊãºÂõæÔºåÂπ∂Êèê‰æõ‰ªéÁõ¥Êé•Ëß£Ë∞úÂà∞‰∫íÂä®Ê®°ÂºèÁöÑ‰∏çÂêåËØÑ‰º∞Á≠ñÁï•„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÊé®ÁêÜËÉΩÂäõÂº∫ÁöÑLLMsÂú®Âà©Áî®‰∫§ÂèâÂ≠óÊØçÁ∫¶ÊùüÊñπÈù¢ÊòæËëó‰ºò‰∫éÈùûÊé®ÁêÜÊ®°ÂûãÔºåËÄåLVLMsÂú®Ê≠§‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂÖ∂Ëß£Ë∞úË°®Áé∞‰∏éÁΩëÊ†ºËß£ÊûêÂáÜÁ°ÆÊÄß‰πãÈó¥Â≠òÂú®Âº∫Áõ∏ÂÖ≥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20533",
            "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One\n  Sequence",
            "url": "https://huggingface.co/papers/2503.20533",
            "abstract": "Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.",
            "score": 6,
            "issue_id": 3144,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 –º–∞—Ä—Ç–∞",
                "en": "March 26",
                "zh": "3Êúà26Êó•"
            },
            "hash": "3b237389882b1344",
            "authors": [
                "Yijiong Yu"
            ],
            "affiliations": [
                "OpenCSG",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20533.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#math"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–µ—Ç–≤–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ —à–∞–≥ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —á–µ–º –≤–¥–≤–æ–µ —É—Å–∫–æ—Ä–∏—Ç—å –≤—Ä–µ–º—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤."
                },
                "en": {
                    "title": "Accelerating Reasoning with Parallel Token Decoding",
                    "desc": "This paper presents a novel approach to enhance the efficiency of reasoning models in machine learning, particularly for complex tasks like mathematical reasoning. The authors focus on reducing the computational cost associated with generating lengthy reasoning sequences by utilizing parallel processing techniques. By implementing a specialized attention mask, they enable the model to decode multiple tokens simultaneously, which significantly speeds up the reasoning process without increasing memory usage. Experimental results indicate that this method can achieve more than 100% improvement in decoding speed while preserving the quality of the answers."
                },
                "zh": {
                    "title": "Âä†ÈÄüÊé®ÁêÜËøáÁ®ãÔºåÊèêÂçáÊïàÁéá‰∏éË¥®Èáè",
                    "desc": "ÊúÄËøëÁöÑÊé®ÁêÜÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°ÔºàÂ¶ÇÊï∞Â≠¶Êé®ÁêÜÔºâ‰∏≠ÊòæÁ§∫Âá∫ÊòæËëóÁöÑÂáÜÁ°ÆÊÄßÊèêÂçáÔºå‰∏ªË¶ÅÂæóÁõä‰∫éËØ¶ÁªÜÂíåÂÖ®Èù¢ÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÁÑ∂ËÄåÔºåÁîüÊàêËøô‰∫õÂÜóÈïøÁöÑÊé®ÁêÜÂ∫èÂàóÂú®ËÆ°ÁÆó‰∏äÊòØÊòÇË¥µ‰∏îËÄóÊó∂ÁöÑ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏Ä‰ΩéÊïàÈóÆÈ¢òÔºåÊàë‰ª¨Âà©Áî®Êüê‰∫õ‰ªªÂä°ÁöÑÂõ∫ÊúâÂπ∂Ë°åÊÄßÊù•Âä†ÈÄüÊé®ÁêÜËøáÁ®ã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂΩìÂ≠òÂú®Â§ö‰∏™Âπ∂Ë°åÊé®ÁêÜÂàÜÊîØÊó∂ÔºåÊàë‰ª¨‰ΩøÁî®‰∏ìÈó®ÁöÑÊ≥®ÊÑèÂäõÊé©Á†ÅÂú®ÊØè‰∏ÄÊ≠•Ëß£Á†ÅÂ§ö‰∏™Ê†áËÆ∞Ôºå‰ªéËÄåÂú®Âçï‰∏™Â∫èÂàó‰∏≠Â§ÑÁêÜÂÆÉ‰ª¨ÔºåÈÅøÂÖç‰∫ÜÈ¢ùÂ§ñÁöÑÂÜÖÂ≠ò‰ΩøÁî®„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05520",
            "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning",
            "url": "https://huggingface.co/papers/2504.05520",
            "abstract": "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.",
            "score": 4,
            "issue_id": 3150,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 –∞–ø—Ä–µ–ª—è",
                "en": "April 7",
                "zh": "4Êúà7Êó•"
            },
            "hash": "474ad1b587ab6240",
            "authors": [
                "Taiwei Shi",
                "Yiyang Wu",
                "Linxin Song",
                "Tianyi Zhou",
                "Jieyu Zhao"
            ],
            "affiliations": [
                "University of Maryland, College Park",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05520.jpg",
            "data": {
                "error": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06122",
            "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
            "url": "https://huggingface.co/papers/2504.06122",
            "abstract": "Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.",
            "score": 3,
            "issue_id": 3147,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 –∞–ø—Ä–µ–ª—è",
                "en": "April 8",
                "zh": "4Êúà8Êó•"
            },
            "hash": "a180ec0989dfd0c8",
            "authors": [
                "Jingyuan Zhang",
                "Qi Wang",
                "Xingguang Ji",
                "Yahui Liu",
                "Yang Yue",
                "Fuzheng Zhang",
                "Di Zhang",
                "Guorui Zhou",
                "Kun Gai"
            ],
            "affiliations": [
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06122.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ —Ç–µ–æ—Ä–µ–º: –æ—Ç —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–µ",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º (–ê–î–¢) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º –ø–∞—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ-–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –û–Ω–∏ —Ç–∞–∫–∂–µ –∏—Å—Å–ª–µ–¥—É—é—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –æ—Ç –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–∞ Lean 4. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∏–º —É–¥–∞–ª–æ—Å—å —É–ª—É—á—à–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–∏—Å—Ç–µ–º—ã –ê–î–¢, –¥–æ—Å—Ç–∏–≥–Ω—É–≤ –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤."
                },
                "en": {
                    "title": "Revolutionizing Automated Theorem Proving with Human-like Reasoning",
                    "desc": "This paper explores the enhancement of automated theorem proving (ATP) using large language models (LLMs) and Lean 4 code. The authors propose a continual training approach that combines a hybrid dataset of statement-proof pairs with cognitive behavior data to mimic human reasoning. They also implement reinforcement learning, utilizing feedback from the Lean 4 compiler to refine the models further. As a result, they report significant improvements in existing ATP systems, achieving a notable 59.8% pass rate on the MiniF2F benchmark."
                },
                "zh": {
                    "title": "ÊèêÂçáËá™Âä®ÂÆöÁêÜËØÅÊòéÁöÑÊô∫ËÉΩÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÈÄöËøáÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊèêÂçáËá™Âä®ÂÆöÁêÜËØÅÊòéÔºàATPÔºâÁöÑÊΩúÂäõÔºåÁâπÂà´ÊòØ‰∏éLean 4‰ª£Á†ÅÁöÑÂΩ¢ÂºèÊé®ÁêÜÁõ∏ÂÖ≥ÁöÑËøõÂ±ï„ÄÇÊàë‰ª¨ÈááÁî®Ê∑∑ÂêàÊï∞ÊçÆÈõÜÂØπÁé∞ÊúâATPÊ®°ÂûãËøõË°åÊåÅÁª≠ËÆ≠ÁªÉÔºåÊï∞ÊçÆÈõÜ‰∏≠ÂåÖÂê´Â§ßÈáèÁöÑÂëΩÈ¢ò-ËØÅÊòéÂØπÔºåÂπ∂Âä†ÂÖ•Ê®°Êãü‰∫∫Á±ªÊé®ÁêÜÂíåÂÅáËÆæ‰øÆÊ≠£ÁöÑËÆ§Áü•Ë°å‰∏∫Êï∞ÊçÆ„ÄÇÊé•ÁùÄÔºåÊàë‰ª¨Âà©Áî®Lean 4ÁºñËØëÂô®ËøîÂõûÁöÑÁªìÊûúÂ•ñÂä±ËøõË°åÂº∫ÂåñÂ≠¶‰π†Ôºå‰ª•Ëøõ‰∏ÄÊ≠•‰ºòÂåñÊ®°ÂûãÊÄßËÉΩ„ÄÇÈÄöËøáËøô‰∫õÊñπÊ≥ïÔºåÊàë‰ª¨ÊàêÂäüÊèêÂçá‰∫ÜÁé∞ÊúâÁöÑÂΩ¢ÂºèËØÅÊòéÂô®ÔºåÂ¶ÇDeepSeek-Prover-v1.5ÂíåGoedel-ProverÔºåÂú®Êï¥‰ΩìËØÅÊòéÁîüÊàêÈ¢ÜÂüüËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑË°®Áé∞„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-04-08.html",
    "link_next": "2025-04-10.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "08.04",
        "en": "04/08",
        "zh": "4Êúà8Êó•"
    },
    "short_date_next": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4Êúà10Êó•"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 8,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü Skywork R1VÔºå‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°Âûã„ÄÇÂÆÉÈÄöËøáÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅËΩ¨ÁßªÊñπÊ≥ïÔºåÂ∞Ü R1-series Â§ßËØ≠Ë®ÄÊ®°ÂûãÊâ©Â±ïÂà∞ËßÜËßâÊ®°ÊÄÅ„ÄÇSkywork R1V ‰ΩøÁî®ËΩªÈáèÁ∫ßÁöÑËßÜËßâÊäïÂΩ±Âô®ÔºåÂÆûÁé∞Êó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÊàñËßÜËßâÁºñÁ†ÅÂô®ÁöÑÂ§öÊ®°ÊÄÅÈÄÇÂ∫î„ÄÇÊñáÁ´†ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑Âêà‰ºòÂåñÁ≠ñÁï•ÂíåËá™ÈÄÇÂ∫îÈïøÂ∫¶ÁöÑÊÄùÁª¥ÈìæÊèêÁÇºÊñπÊ≥ïÔºåÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSkywork R1V Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂‰∏îÊ®°ÂûãÊùÉÈáçÂ∑≤ÂÖ¨ÂºÄ„ÄÇ",
        "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
        "pinyin": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü Skywork R1VÔºå‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°Âûã„ÄÇÂÆÉÈÄöËøáÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅËΩ¨ÁßªÊñπÊ≥ïÔºåÂ∞Ü R1-series Â§ßËØ≠Ë®ÄÊ®°ÂûãÊâ©Â±ïÂà∞ËßÜËßâÊ®°ÊÄÅ„ÄÇSkywork R1V ‰ΩøÁî®ËΩªÈáèÁ∫ßÁöÑËßÜËßâÊäïÂΩ±Âô®ÔºåÂÆûÁé∞Êó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÊàñËßÜËßâÁºñÁ†ÅÂô®ÁöÑÂ§öÊ®°ÊÄÅÈÄÇÂ∫î„ÄÇÊñáÁ´†ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑Âêà‰ºòÂåñÁ≠ñÁï•ÂíåËá™ÈÄÇÂ∫îÈïøÂ∫¶ÁöÑÊÄùÁª¥ÈìæÊèêÁÇºÊñπÊ≥ïÔºåÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSkywork R1V Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂‰∏îÊ®°ÂûãÊùÉÈáçÂ∑≤ÂÖ¨ÂºÄ„ÄÇ\n\nZh√® piƒÅn w√©nzhƒÅng ji√®sh√†o le Skywork R1V, yƒ´zh«íng du≈ç m√≥shu√†i tuƒ´l«ê m√≥x√≠ng. TƒÅ t≈çnggu√≤ gƒÅoxi√†o de du≈ç m√≥shu√†i zhu«énw√©i fƒÅngf«é, jiƒÅng R1-series d√† y«îy√°n m√≥x√≠ng ku√≤zh«én d√†o sh√¨ju√© m√≥shu√†i. Skywork R1V sh«êy√≤ng qƒ´ngli√†ngj√≠ de sh√¨ju√© t√≥ujƒ´ngq√¨, sh√≠xi√†n w√∫x≈´ ch√≥ngxƒ´n x√πnli√†n y«îy√°n m√≥x√≠ng hu√≤ sh√¨ju√© biƒÅnm«éq√¨ de du≈ç m√≥shu√†i sh√¨y√¨ng. W√©nzhƒÅng h√°i t√≠ch≈´ le yƒ´zh«íng h√πnh√© y≈çuhu√† c√®l√º√® h√© z√¨ sh√¨y√¨ng ch√°ngd√π de sƒ´w√©i li√†n t√≠xi√†ng fƒÅngf«é, t√≠gƒÅo tuƒ´l«ê xi√†oy√¨ng. Sh√≠y√†n ji√©gu«í xi«énsh√¨, Skywork R1V z√†i du≈ç g√® jƒ´zh«în c√®sh√¨ zh≈çng bi«éoxi√†n ch≈´s√®, b√¨ngqiƒõ m√≥x√≠ng qu√°nzh√≤ng y«ê g≈çngkƒÅi.",
        "vocab": "[\n    {\"word\": \"Â§öÊ®°ÊÄÅ\", \"pinyin\": \"du≈ç m√≥ t√†i\", \"trans\": \"multimodal\"},\n    {\"word\": \"Êé®ÁêÜ\", \"pinyin\": \"tuƒ´ l«ê\", \"trans\": \"inference\"},\n    {\"word\": \"Ê®°Âûã\", \"pinyin\": \"m√≥ x√≠ng\", \"trans\": \"model\"},\n    {\"word\": \"ËΩ¨Áßª\", \"pinyin\": \"zhu«én y√≠\", \"trans\": \"transfer\"},\n    {\"word\": \"ÊñπÊ≥ï\", \"pinyin\": \"fƒÅng f«é\", \"trans\": \"method\"},\n    {\"word\": \"Êâ©Â±ï\", \"pinyin\": \"ku√≤ zh«én\", \"trans\": \"extend\"},\n    {\"word\": \"ËßÜËßâ\", \"pinyin\": \"sh√¨ ju√©\", \"trans\": \"visual\"},\n    {\"word\": \"ÊäïÂΩ±Âô®\", \"pinyin\": \"t√≥u y«êng q√¨\", \"trans\": \"projector\"},\n    {\"word\": \"ÂÆûÁé∞\", \"pinyin\": \"sh√≠ xi√†n\", \"trans\": \"achieve\"},\n    {\"word\": \"ÈáçÊñ∞\", \"pinyin\": \"ch√≥ng xƒ´n\", \"trans\": \"re-\"},\n    {\"word\": \"ËÆ≠ÁªÉ\", \"pinyin\": \"x√πn li√†n\", \"trans\": \"train\"},\n    {\"word\": \"ÁºñÁ†ÅÂô®\", \"pinyin\": \"biƒÅn m«é q√¨\", \"trans\": \"encoder\"},\n    {\"word\": \"ÈÄÇÂ∫î\", \"pinyin\": \"sh√¨ y√¨ng\", \"trans\": \"adapt\"},\n    {\"word\": \"Ê∑∑Âêà\", \"pinyin\": \"h√πn h√©\", \"trans\": \"hybrid\"},\n    {\"word\": \"‰ºòÂåñ\", \"pinyin\": \"y≈çu hu√†\", \"trans\": \"optimization\"},\n    {\"word\": \"Á≠ñÁï•\", \"pinyin\": \"c√® l√º√®\", \"trans\": \"strategy\"},\n    {\"word\": \"Ëá™ÈÄÇÂ∫î\", \"pinyin\": \"z√¨ sh√¨ y√¨ng\", \"trans\": \"adaptive\"},\n    {\"word\": \"ÈïøÂ∫¶\", \"pinyin\": \"ch√°ng d√π\", \"trans\": \"length\"},\n    {\"word\": \"ÊÄùÁª¥Èìæ\", \"pinyin\": \"sƒ´ w√©i li√°n\", \"trans\": \"chain of thought\"},\n    {\"word\": \"ÊèêÁÇº\", \"pinyin\": \"t√≠ li√†n\", \"trans\": \"extract\"},\n    {\"word\": \"ÊïàÁéá\", \"pinyin\": \"xi√†o l«ú\", \"trans\": \"efficiency\"},\n    {\"word\": \"ÂÆûÈ™å\", \"pinyin\": \"sh√≠ y√†n\", \"trans\": \"experiment\"},\n    {\"word\": \"ÁªìÊûú\", \"pinyin\": \"ji√© gu«í\", \"trans\": \"result\"},\n    {\"word\": \"ÊòæÁ§∫\", \"pinyin\": \"xi«én sh√¨\", \"trans\": \"show\"},\n    {\"word\": \"Âü∫ÂáÜ\", \"pinyin\": \"jƒ´ zh«în\", \"trans\": \"benchmark\"},\n    {\"word\": \"ÊµãËØï\", \"pinyin\": \"c√® sh√¨\", \"trans\": \"test\"},\n    {\"word\": \"Ë°®Áé∞\", \"pinyin\": \"bi«éo xi√†n\", \"trans\": \"performance\"},\n    {\"word\": \"Âá∫Ëâ≤\", \"pinyin\": \"ch≈´ s√®\", \"trans\": \"outstanding\"},\n    {\"word\": \"ÊùÉÈáç\", \"pinyin\": \"qu√°n zh√≤ng\", \"trans\": \"weights\"},\n    {\"word\": \"ÂÖ¨ÂºÄ\", \"pinyin\": \"g≈çng kƒÅi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces Skywork R1V, a multimodal reasoning model. It extends the R1-series large language model to the visual modality through an efficient multimodal transfer method. Skywork R1V employs a lightweight visual projector to achieve multimodal adaptation without the need to retrain the language model or visual encoder. The article also proposes a hybrid optimization strategy and an adaptive-length chain-of-thought extraction method to enhance reasoning efficiency. Experimental results demonstrate that Skywork R1V performs excellently on multiple benchmark tests, and the model weights have been made publicly available.",
        "update_ts": "2025-04-09 09:12"
    }
}