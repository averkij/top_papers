{
    "date": {
        "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 29",
        "zh": "10æœˆ29æ—¥"
    },
    "time_utc": "2025-10-29 02:31",
    "weekday": 2,
    "issue_id": 6667,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.24320",
            "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.24320",
            "abstract": "Critique-RL is an online reinforcement learning approach for developing critiquing language models without strong supervision, using a two-stage optimization strategy to improve both the critic's discriminability and helpfulness.  \t\t\t\t\tAI-generated summary \t\t\t\t Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
            "score": 5,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "0c26fc5b04f76bff",
            "authors": [
                "Zhiheng Xi",
                "Jixuan Huang",
                "Xin Guo",
                "Boyang Hong",
                "Dingwen Yang",
                "Xiaoran Fan",
                "Shuo Li",
                "Zehui Chen",
                "Junjie Ye",
                "Siyu Yuan",
                "Zhengyin Du",
                "Xuesong Yao",
                "Yufei Xu",
                "Jiecao Chen",
                "Rui Zheng",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24320.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Critique-RL: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°",
                    "desc": "Critique-RL â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµĞ· ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ğº ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ’ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ²Ğ¾ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Critique-RL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Language Models with Self-Supervised Critique",
                    "desc": "Critique-RL is a novel online reinforcement learning method designed to enhance critiquing language models without the need for strong supervision. It employs a two-stage optimization strategy where an actor generates responses and a critic evaluates them, allowing for iterative refinement. The first stage focuses on improving the critic's ability to distinguish high-quality responses using direct reward signals, while the second stage enhances the critic's helpfulness through indirect rewards based on the actor's improvements. Experimental results demonstrate that Critique-RL significantly boosts performance, achieving notable gains in both in-domain and out-of-domain tasks."
                },
                "zh": {
                    "title": "Critique-RLï¼šæ— å¼ºç›‘ç£çš„è¯„ä¼°è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ–¹æ³•",
                    "desc": "Critique-RLæ˜¯ä¸€ç§åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåœ¨æ²¡æœ‰å¼ºç›‘ç£çš„æƒ…å†µä¸‹å¼€å‘è¯„ä¼°è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜è¯„ä¼°è€…çš„åŒºåˆ†èƒ½åŠ›å’Œæœ‰ç”¨æ€§ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡ç›´æ¥çš„åŸºäºè§„åˆ™çš„å¥–åŠ±ä¿¡å·æ¥å¢å¼ºè¯„ä¼°è€…çš„åŒºåˆ†èƒ½åŠ›ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒåŸºäºç”Ÿæˆè€…çš„åé¦ˆå¼•å…¥é—´æ¥å¥–åŠ±ï¼Œä»¥æé«˜è¯„ä¼°è€…çš„æœ‰ç”¨æ€§ï¼ŒåŒæ—¶é€šè¿‡é€‚å½“çš„æ­£åˆ™åŒ–ä¿æŒå…¶åŒºåˆ†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCritique-RLåœ¨å¤šä¸ªä»»åŠ¡å’Œæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24701",
            "title": "Tongyi DeepResearch Technical Report",
            "url": "https://huggingface.co/papers/2510.24701",
            "abstract": "Tongyi DeepResearch, a large language model with agentic capabilities, achieves top performance in various deep research tasks through an end-to-end training framework and automated data synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",
            "score": 4,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "a6f467c756fc29b7",
            "authors": [
                "Tongyi DeepResearch Team",
                "Baixuan Li",
                "Bo Zhang",
                "Dingchu Zhang",
                "Fei Huang",
                "Guangyu Li",
                "Guoxin Chen",
                "Huifeng Yin",
                "Jialong Wu",
                "Jingren Zhou",
                "Kuan Li",
                "Liangcai Su",
                "Litu Ou",
                "Liwen Zhang",
                "Pengjun Xie",
                "Rui Ye",
                "Wenbiao Yin",
                "Xinmiao Yu",
                "Xinyu Wang",
                "Xixi Wu",
                "Xuanzhong Chen",
                "Yida Zhao",
                "Zhen Zhang",
                "Zhengwei Tao",
                "Zhongwang Zhang",
                "Zile Qiao",
                "Chenxi Wang",
                "Donglei Yu",
                "Gang Fu",
                "Haiyang Shen",
                "Jiayin Yang",
                "Jun Lin",
                "Junkai Zhang",
                "Kui Zeng",
                "Li Yang",
                "Hailong Yin",
                "Maojia Song",
                "Ming Yan",
                "Peng Xia",
                "Qian Xiao",
                "Rui Min",
                "Ruixue Ding",
                "Runnan Fang",
                "Shaowei Chen",
                "Shen Huang",
                "Shihang Wang",
                "Shihao Cai",
                "Weizhou Shen",
                "Xiaobin Wang",
                "Xin Guan",
                "Xinyu Geng",
                "Yingcheng Shi",
                "Yuning Wu",
                "Zhuo Chen",
                "Zijian Li",
                "Yong Jiang"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24701.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#synthetic",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#open_source",
                    "#agi",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Tongyi DeepResearch - ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ Ğ½Ğ° Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· end-to-end Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ mid-training Ğ¸ post-training, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞŸÑ€Ğ¸ 30.5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 3.3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering Autonomous Deep Research with Tongyi DeepResearch",
                    "desc": "Tongyi DeepResearch is a large language model designed for complex research tasks that require deep information-seeking capabilities. It utilizes an end-to-end training framework that includes both mid-training and post-training phases to enhance its autonomous research abilities. The model features a fully automated data synthesis pipeline, eliminating the need for expensive human annotations, which allows for scalable training across various tasks. With 30.5 billion parameters, Tongyi DeepResearch achieves top performance on multiple benchmarks, demonstrating its effectiveness in deep research applications."
                },
                "zh": {
                    "title": "è‡ªä¸»æ·±åº¦ç ”ç©¶çš„æœªæ¥",
                    "desc": "Tongyi DeepResearch æ˜¯ä¸€ç§å…·æœ‰è‡ªä¸»èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºé•¿æ—¶é—´çš„ä¿¡æ¯æ£€ç´¢ç ”ç©¶ä»»åŠ¡ã€‚å®ƒé€šè¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒæ¡†æ¶å’Œè‡ªåŠ¨åŒ–çš„æ•°æ®åˆæˆï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­å®ç°å¯æ‰©å±•çš„æ¨ç†å’Œä¿¡æ¯è·å–ã€‚è¯¥æ¨¡å‹å…·æœ‰ 305 äº¿ä¸ªå‚æ•°ï¼Œä¸”æ¯ä¸ªä»¤ç‰Œä»…æ¿€æ´» 33 äº¿ä¸ªå‚æ•°ï¼Œè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†å¤šé¡¹æ·±åº¦ç ”ç©¶åŸºå‡†çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æˆ‘ä»¬å°†æ¨¡å‹ã€æ¡†æ¶å’Œå®Œæ•´è§£å†³æ–¹æ¡ˆå¼€æºï¼Œä»¥æ”¯æŒç¤¾åŒºçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22099",
            "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
            "url": "https://huggingface.co/papers/2510.22099",
            "abstract": "A framework using the Information Bottleneck principle and Dynamic Mode Steering algorithm improves the reliability of Large Language Models by balancing generalization and memorization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.",
            "score": 1,
            "issue_id": 6667,
            "pub_date": "2025-10-25",
            "pub_date_card": {
                "ru": "25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 25",
                "zh": "10æœˆ25æ—¥"
            },
            "hash": "7a4136db7043277f",
            "authors": [
                "Xuanming Zhang"
            ],
            "affiliations": [
                "Department of Computer Science, University of Wisconsin-Madison, Madison, USA",
                "Stanford University, Palo Alto, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22099.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ LLM Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ, Ğ° Ğ½Ğµ Ğ·Ğ°ÑƒÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾ Ğ±Ğ»ĞµÑÑ‚ÑÑ‰Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ·Ñ‹ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Information Bottleneck Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ³Ğ´Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ â€” ĞºĞ°Ğº Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Dynamic Mode Steering Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞµÑ‘ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM."
                },
                "en": {
                    "title": "Balancing Generalization and Memorization for Reliable LLMs",
                    "desc": "This paper presents a framework that enhances the reliability of Large Language Models (LLMs) by addressing their tendency to both generalize and memorize training data. It introduces the Information Bottleneck principle to differentiate between effective generalization, which compresses information, and problematic memorization, which retains too much detail. The Dynamic Mode Steering algorithm is developed to dynamically adjust the model's focus during inference, promoting generalization while minimizing reliance on memorization. Experimental results show that this approach improves logical consistency and factual accuracy in LLM outputs, making them more reliable for critical applications."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹å¯é æ€§çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä¿¡æ¯ç“¶é¢ˆåŸç†å’ŒåŠ¨æ€æ¨¡å¼å¼•å¯¼ç®—æ³•æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é æ€§ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å¹³è¡¡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè®°å¿†èƒ½åŠ›ï¼Œè§£å†³æ¨¡å‹åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„ä¸å¯é æ€§é—®é¢˜ã€‚é€šè¿‡ç†è®ºæ¨¡å‹ï¼Œæ³›åŒ–è¢«å®šä¹‰ä¸ºå­¦ä¹ å‹ç¼©çš„ã€ä¸ä»»åŠ¡ç›¸å…³çš„è¡¨ç¤ºï¼Œè€Œè®°å¿†åˆ™è¢«è§†ä¸ºæœªèƒ½è¿›è¡Œå‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€æ¨¡å¼å¼•å¯¼ç®—æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é€»è¾‘ä¸€è‡´æ€§å’Œäº‹å®å‡†ç¡®æ€§ï¼Œæä¾›äº†ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹å¯é æ€§çš„åŸåˆ™æ€§æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21323",
            "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set",
            "url": "https://huggingface.co/papers/2510.21323",
            "abstract": "VL-SAE, a sparse autoencoder, enhances vision-language alignment by correlating neurons to unified concepts, improving interpretability and performance in tasks like zero-shot image classification and hallucination elimination.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of vision-language representations endows current Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities. However, the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multi-modal representations into a unified concept set. To address this problem, we propose VL-SAE, a sparse autoencoder that encodes vision-language representations into its hidden activations. Each neuron in its hidden layer correlates to a concept represented by semantically similar images and texts, thereby interpreting these representations with a unified concept set. To establish the neuron-concept correlation, we encourage semantically similar representations to exhibit consistent neuron activations during self-supervised training. First, to measure the semantic similarity of multi-modal representations, we perform their alignment in an explicit form based on cosine similarity. Second, we construct the VL-SAE with a distance-based encoder and two modality-specific decoders to ensure the activation consistency of semantically similar representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA) demonstrate the superior capability of VL-SAE in interpreting and enhancing the vision-language alignment. For interpretation, the alignment between vision and language representations can be understood by comparing their semantics with concepts. For enhancement, the alignment can be strengthened by aligning vision-language representations at the concept level, contributing to performance improvements in downstream tasks, including zero-shot image classification and hallucination elimination. Codes are available at https://github.com/ssfgunner/VL-SAE.",
            "score": 1,
            "issue_id": 6667,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "41279ff25d4d0ecb",
            "authors": [
                "Shufan Shen",
                "Junshu Sun",
                "Qingming Huang",
                "Shuhui Wang"
            ],
            "affiliations": [
                "Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21323.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#multimodal",
                    "#interpretability",
                    "#cv",
                    "#hallucinations",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼",
                    "desc": "VL-SAE â€” ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² VLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ñ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ². ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… zero-shot ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ğ¸Ğ¿Ğ° CLIP Ğ¸ LLaVA."
                },
                "en": {
                    "title": "Enhancing Vision-Language Alignment with VL-SAE",
                    "desc": "The paper introduces VL-SAE, a sparse autoencoder designed to improve the alignment between vision and language representations in multi-modal models. By correlating neurons in its hidden layer to unified concepts derived from semantically similar images and texts, VL-SAE enhances both interpretability and performance. The model employs self-supervised training to ensure consistent neuron activations for similar representations, using cosine similarity for semantic alignment. Experiments show that VL-SAE significantly boosts the effectiveness of vision-language models in tasks like zero-shot image classification and reducing hallucinations."
                },
                "zh": {
                    "title": "VL-SAEï¼šæå‡è§†è§‰-è¯­è¨€å¯¹é½çš„ç¨€ç–è‡ªç¼–ç å™¨",
                    "desc": "VL-SAEæ˜¯ä¸€ç§ç¨€ç–è‡ªç¼–ç å™¨ï¼Œé€šè¿‡å°†è§†è§‰å’Œè¯­è¨€è¡¨ç¤ºå…³è”åˆ°ç»Ÿä¸€çš„æ¦‚å¿µï¼Œå¢å¼ºäº†è§†è§‰-è¯­è¨€å¯¹é½çš„èƒ½åŠ›ã€‚å®ƒçš„éšè—å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒä¸è¯­ä¹‰ç›¸ä¼¼çš„å›¾åƒå’Œæ–‡æœ¬æ‰€ä»£è¡¨çš„æ¦‚å¿µç›¸å…³è”ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚é€šè¿‡è‡ªç›‘ç£è®­ç»ƒï¼ŒVL-SAEé¼“åŠ±è¯­ä¹‰ç›¸ä¼¼çš„è¡¨ç¤ºåœ¨ç¥ç»å…ƒæ¿€æ´»ä¸Šä¿æŒä¸€è‡´ã€‚å®éªŒè¡¨æ˜ï¼ŒVL-SAEåœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ¶ˆé™¤å¹»è§‰ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24591",
            "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
            "url": "https://huggingface.co/papers/2510.24591",
            "abstract": "ReplicationBench evaluates AI agents' ability to replicate astrophysics research papers, providing insights into their faithfulness and correctness in scientific research tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.",
            "score": 0,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "d655b0884e7b15c0",
            "authors": [
                "Christine Ye",
                "Sihan Yuan",
                "Suchetha Cooray",
                "Steven Dillmann",
                "Ian L. V. Roque",
                "Dalya Baron",
                "Philipp Frank",
                "Sergio Martin-Alvarez",
                "Nolan Koblischke",
                "Frank J Qu",
                "Diyi Yang",
                "Risa Wechsler",
                "Ioana Ciuca"
            ],
            "affiliations": [
                "Stanford University",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24591.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#agents"
                ],
                "emoji": "ğŸ”­",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ReplicationBench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²ÑĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹. Ğ”Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğ¸Ğ¶Ğµ 20%, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ½Ğ°ÑƒĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Evaluating AI Agents in Astrophysics Research Replication",
                    "desc": "ReplicationBench is a framework designed to evaluate AI agents' ability to replicate research papers in astrophysics, focusing on their faithfulness and correctness. It breaks down each paper into specific tasks that require the agents to reproduce key contributions, such as experimental setups and data analyses, in collaboration with the original authors. The framework reveals that even advanced language models struggle with these tasks, scoring below 20%, highlighting various failure modes in their performance. This benchmark not only assesses AI reliability in astrophysics but also offers insights applicable to other scientific fields."
                },
                "zh": {
                    "title": "è¯„ä¼° AI ä»£ç†åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„å¯é æ€§",
                    "desc": "ReplicationBench æ˜¯ä¸€ä¸ªè¯„ä¼° AI ä»£ç†åœ¨å¤åˆ¶å¤©ä½“ç‰©ç†å­¦ç ”ç©¶è®ºæ–‡èƒ½åŠ›çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡å°†è®ºæ–‡åˆ†è§£ä¸ºå¤šä¸ªä»»åŠ¡ï¼Œæµ‹è¯•ä»£ç†æ˜¯å¦èƒ½å¤Ÿå‡†ç¡®å¤ç°è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼ŒåŒ…æ‹¬å®éªŒè®¾ç½®ã€æ¨å¯¼ã€æ•°æ®åˆ†æå’Œä»£ç åº“ã€‚è¯¥æ¡†æ¶ä¸åŸè®ºæ–‡ä½œè€…å…±åŒå¼€å‘ï¼Œç¡®ä¿è¯„ä¼°çš„å®¢è§‚æ€§ï¼Œå…³æ³¨ä»£ç†çš„å¿ å®æ€§å’Œæ­£ç¡®æ€§ã€‚å°½ç®¡å½“å‰çš„å‰æ²¿è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼ŒReplicationBench ä»ä¸ºç§‘å­¦ç ”ç©¶ä¸­çš„ AI ä»£ç†æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„å¯é æ€§æµ‹é‡æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24563",
            "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
            "url": "https://huggingface.co/papers/2510.24563",
            "abstract": "OSWorld-MCP is a benchmark that evaluates multimodal agents' tool invocation, GUI operation, and decision-making abilities, highlighting the importance of assessing tool usage in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.",
            "score": 0,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "31a5246f81b64759",
            "authors": [
                "Hongrui Jia",
                "Jitong Liao",
                "Xi Zhang",
                "Haiyang Xu",
                "Tianbao Xie",
                "Chaoya Jiang",
                "Ming Yan",
                "Si Liu",
                "Wei Ye",
                "Fei Huang"
            ],
            "affiliations": [
                "Beijing Zhongguancun Academy",
                "Peking University",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24563.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ GUI, Ğ½Ğ¾ Ğ¸ ÑƒĞ¼ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "OSWorld-MCP â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ 158 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ 7 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MCP-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ñ 8.3% Ğ´Ğ¾ 20.4% Ğ´Ğ»Ñ OpenAI o3. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² 36.3% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Agent Evaluation with OSWorld-MCP",
                    "desc": "OSWorld-MCP is a new benchmark designed to evaluate multimodal agents on their ability to invoke tools, operate GUIs, and make decisions in real-world scenarios. It addresses the gap in previous assessments that primarily focused on GUI interactions, providing a fair comparison by including tool invocation capabilities. The benchmark features a unique automated code-generation pipeline that creates and validates 158 high-quality tools for common applications. Results show that while integrating MCP tools significantly improves task success rates, there is still a need for enhancement in tool invocation rates among leading models, emphasizing the benchmark's importance in advancing multimodal agent evaluation."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„æ–°æ ‡å‡†",
                    "desc": "OSWorld-MCPæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨å·¥å…·è°ƒç”¨ã€å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ“ä½œå’Œå†³ç­–èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨çœŸå®åœºæ™¯ä¸­è¯„ä¼°å·¥å…·ä½¿ç”¨çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å®ç°çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆç®¡é“ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†158ä¸ªé«˜è´¨é‡å·¥å…·ï¼Œå¹¶å¯¹å…¶åŠŸèƒ½å’Œé€‚ç”¨æ€§è¿›è¡Œäº†ä¸¥æ ¼éªŒè¯ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MCPå·¥å…·çš„æ™ºèƒ½ä½“åœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œæ˜¾ç¤ºå‡ºå·¥å…·è°ƒç”¨èƒ½åŠ›çš„å…³é”®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24448",
            "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
            "url": "https://huggingface.co/papers/2510.24448",
            "abstract": "Video Diffusion Models (VDMs) show higher data efficiency than large language models across various visual tasks, suggesting video pretraining can enhance visual foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.",
            "score": 0,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "cb963e7271205da4",
            "authors": [
                "Pablo Acuaviva",
                "Aram Davtyan",
                "Mariam Hassan",
                "Sebastian Stapf",
                "Ahmad Rahimi",
                "Alexandre Alahi",
                "Paolo Favaro"
            ],
            "affiliations": [
                "Computer Vision Group University of Bern Bern, Switzerland",
                "VITA Lab, EPFL Lausanne, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24448.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#transfer_learning",
                    "#games",
                    "#diffusion",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Video Diffusion Models (VDMs) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. VDMs, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ARC-AGI, ConceptARC, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… foundation models."
                },
                "en": {
                    "title": "Unlocking Visual Potential with Video Diffusion Models",
                    "desc": "This paper explores the effectiveness of Video Diffusion Models (VDMs) in improving visual tasks through video pretraining. Unlike large language models (LLMs), which excel in language tasks, VDMs leverage spatiotemporal data to enhance their understanding of structure and dynamics. The authors conducted experiments comparing pretrained VDMs and LLMs, finding that VDMs showed superior data efficiency across various benchmarks. The results suggest that video pretraining can significantly advance the development of visual foundation models."
                },
                "zh": {
                    "title": "è§†é¢‘é¢„è®­ç»ƒæå‡è§†è§‰æ¨¡å‹æ•ˆç‡",
                    "desc": "è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ¯”å¤§å‹è¯­è¨€æ¨¡å‹æ›´é«˜çš„æ•°æ®æ•ˆç‡ï¼Œè¡¨æ˜è§†é¢‘é¢„è®­ç»ƒå¯ä»¥å¢å¼ºè§†è§‰åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€é¢†åŸŸçš„é¢„è®­ç»ƒå–å¾—äº†æˆåŠŸï¼Œä½†åœ¨è§†è§‰é¢†åŸŸï¼Œæ¨¡å‹ä»ç„¶é¢ä¸´ç»„åˆç†è§£å’Œæ ·æœ¬æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç ”ç©¶VDMsä½œä¸ºå¼¥åˆè¿™ä¸€å·®è·çš„æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œè®¤ä¸ºå…¶åœ¨æ—¶ç©ºæ•°æ®ä¸Šçš„é¢„è®­ç»ƒèµ‹äºˆäº†æ¨¡å‹å¼ºå¤§çš„ç»“æ„å’ŒåŠ¨æ€çš„å½’çº³åç½®ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVDMsåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ¯”è¯­è¨€æ¨¡å‹æ›´é«˜çš„æ•°æ®æ•ˆç‡ï¼Œæ”¯æŒäº†è§†é¢‘é¢„è®­ç»ƒå¯¹è§†è§‰åŸºç¡€æ¨¡å‹çš„è¿›å±•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-28.html",
    "link_next": "2025-10-30.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "28.10",
        "en": "10/28",
        "zh": "10æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "30.10",
        "en": "10/30",
        "zh": "10æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}