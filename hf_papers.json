{
    "date": {
        "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 6",
        "zh": "1æœˆ6æ—¥"
    },
    "time_utc": "2026-01-06 17:24",
    "weekday": 1,
    "issue_id": 440,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.01739",
            "title": "K-EXAONE Technical Report",
            "url": "https://huggingface.co/papers/2601.01739",
            "abstract": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.  \t\t\t\t\tAI-generated summary \t\t\t\t This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
            "score": 42,
            "issue_id": 426,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "ab3744d785e77871",
            "authors": [
                "Eunbi Choi",
                "Kibong Choi",
                "Seokhee Hong",
                "Junwon Hwang",
                "Hyojin Jeon",
                "Hyunjik Jo",
                "Joonkee Kim",
                "Seonghwan Kim",
                "Soyeon Kim",
                "Sunkyoung Kim",
                "Yireun Kim",
                "Yongil Kim",
                "Haeju Lee",
                "Jinsik Lee",
                "Kyungmin Lee",
                "Sangha Park",
                "Heuiyeen Yeen",
                "Hwan Chang",
                "Stanley Jungkyu Choi",
                "Yejin Choi",
                "Jiwon Ham",
                "Kijeong Jeon",
                "Geunyeong Jeong",
                "Gerrard Jeongwon Jo",
                "Yonghwan Jo",
                "Jiyeon Jung",
                "Naeun Kang",
                "Dohoon Kim",
                "Euisoon Kim",
                "Hayeon Kim",
                "Hyosang Kim",
                "Hyunseo Kim",
                "Jieun Kim",
                "Minu Kim",
                "Myoungshin Kim",
                "Unsol Kim",
                "Youchul Kim",
                "YoungJin Kim",
                "Chaeeun Lee",
                "Chaeyoon Lee",
                "Changhun Lee",
                "Dahm Lee",
                "Edward Hwayoung Lee",
                "Honglak Lee",
                "Jinsang Lee",
                "Jiyoung Lee",
                "Sangeun Lee",
                "Seungwon Lim",
                "Solji Lim",
                "Woohyung Lim",
                "Chanwoo Moon",
                "Jaewoo Park",
                "Jinho Park",
                "Yongmin Park",
                "Hyerin Seo",
                "Wooseok Seo",
                "Yongwoo Song",
                "Sejong Yang",
                "Sihoon Yang",
                "Chang En Yea",
                "Sihyuk Yi",
                "Chansik Yoon",
                "Dongkeun Yoon",
                "Sangyeon Yoon",
                "Hyeongu Yun"
            ],
            "affiliations": [
                "LG AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.01739.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual",
                    "#reasoning",
                    "#agents",
                    "#architecture",
                    "#long_context"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²",
                    "desc": "K-EXAONE - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸ĞµĞ¹ LG AI Research. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 236 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… 23 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. K-EXAONE Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ² 256K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ…: ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼, Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼, Ğ¸ÑĞ¿Ğ°Ğ½ÑĞºĞ¾Ğ¼, Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ¼, ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¼ Ğ¸ Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼. ĞĞ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°."
                },
                "en": {
                    "title": "K-EXAONE: Powering Multilingual AI with Efficiency",
                    "desc": "K-EXAONE is a state-of-the-art multilingual language model that utilizes a Mixture-of-Experts architecture, allowing it to efficiently manage a large number of parameters. With a total of 236 billion parameters, it activates only a subset of 23 billion during inference, optimizing performance while maintaining resource efficiency. The model supports a long context window of 256,000 tokens and is capable of understanding and generating text in six different languages. Evaluations show that K-EXAONE performs competitively against other large models, making it a valuable tool for various applications in both industry and research."
                },
                "zh": {
                    "title": "K-EXAONEï¼šå¤šè¯­è¨€çš„å¼ºå¤§AIåŸºç¡€æ¨¡å‹",
                    "desc": "K-EXAONEæ˜¯ä¸€ç§å¤šè¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼Œå…·æœ‰2360äº¿ä¸ªå‚æ•°ï¼Œåœ¨æ¨ç†æ—¶æ¿€æ´»230äº¿ä¸ªå‚æ•°ã€‚å®ƒæ”¯æŒ256Kçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶è¦†ç›–å…­ç§è¯­è¨€ï¼ŒåŒ…æ‹¬éŸ©è¯­ã€è‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­ã€æ—¥è¯­å’Œè¶Šå—è¯­ã€‚é€šè¿‡å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼ŒK-EXAONEåœ¨æ¨ç†ã€ä»£ç†ã€é€šç”¨ã€éŸ©è¯­å’Œå¤šè¯­è¨€èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½ä¸ç±»ä¼¼è§„æ¨¡çš„å¼€æ”¾æƒé‡æ¨¡å‹ç›¸å½“ã€‚è¯¥æ¨¡å‹æ—¨åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œä»¥æ”¹å–„äººç±»ç”Ÿæ´»ï¼Œé€‚ç”¨äºå¹¿æ³›çš„å·¥ä¸šå’Œç ”ç©¶åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02204",
            "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2601.02204",
            "abstract": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
            "score": 39,
            "issue_id": 426,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "0489432e14ac471c",
            "authors": [
                "Huichao Zhang",
                "Liao Qu",
                "Yiheng Liu",
                "Hang Chen",
                "Yangyang Song",
                "Yongsheng Dong",
                "Shikun Sun",
                "Xian Li",
                "Xu Wang",
                "Yi Jiang",
                "Hu Ye",
                "Bo Chen",
                "Yiming Gao",
                "Peng Liu",
                "Akide Liu",
                "Zhipeng Yang",
                "Qili Deng",
                "Linjie Xing",
                "Jiyang Liu",
                "Zhao Wang",
                "Yang Zhou",
                "Mingcong Liu",
                "Yi Zhang",
                "Qian He",
                "Xiwei Hu",
                "Zhongqi Qi",
                "Jie Shao",
                "Zhiye Fu",
                "Shuai Wang",
                "Fangmin Chen",
                "Xuezhi Chai",
                "Zhihua Wu",
                "Yitong Wang",
                "Zehuan Yuan",
                "Daniel K. Du",
                "Xinglong Wu"
            ],
            "affiliations": [
                "ByteDance",
                "Monash University",
                "TsingHua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02204.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#rlhf",
                    "#architecture",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ ÑĞ²ĞµÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "NextFlow â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ½Ğ° 6 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ â€” Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (1024x1024) Ğ·Ğ° 5 ÑĞµĞºÑƒĞ½Ğ´, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "NextFlow: Fast and Unified Multimodal Generation",
                    "desc": "NextFlow is a powerful autoregressive transformer designed to handle both text and images simultaneously by processing interleaved tokens. It uses a unique approach called next-scale prediction for images, which allows it to generate high-quality visuals much faster than traditional methods. By training on a massive dataset of 6 trillion tokens, NextFlow enhances its ability to understand and create multimodal content, such as videos and edited images. The model's innovative training techniques and prefix-tuning strategy help it achieve top performance in generating images and text, competing with specialized models in visual quality."
                },
                "zh": {
                    "title": "NextFlowï¼šå¿«é€Ÿå¤šæ¨¡æ€ç”Ÿæˆçš„å˜é©è€…",
                    "desc": "NextFlowæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§£ç å™¨è‡ªå›å½’å˜æ¢å™¨ï¼Œèƒ½å¤Ÿå¤„ç†äº¤é”™çš„æ–‡æœ¬å’Œå›¾åƒæ ‡è®°ï¼Œä»è€Œå®ç°å¿«é€Ÿçš„å¤šæ¨¡æ€ç”Ÿæˆã€‚å®ƒé€šè¿‡è®­ç»ƒ6ä¸‡äº¿ä¸ªäº¤é”™çš„æ–‡æœ¬-å›¾åƒç¦»æ•£æ ‡è®°ï¼Œæ¿€æ´»äº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ï¼Œæ”¯æŒå›¾åƒç¼–è¾‘å’Œè§†é¢‘ç”Ÿæˆã€‚NextFlowé‡‡ç”¨äº†æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œè§†è§‰ç”Ÿæˆçš„ä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼Œèƒ½å¤Ÿåœ¨5ç§’å†…ç”Ÿæˆ1024x1024çš„å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNextFlowåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œè§†è§‰è´¨é‡ä¸ä¸“é—¨çš„æ‰©æ•£æ¨¡å‹ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.01425",
            "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
            "url": "https://huggingface.co/papers/2601.01425",
            "abstract": "A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.",
            "score": 33,
            "issue_id": 426,
            "pub_date": "2026-01-04",
            "pub_date_card": {
                "ru": "4 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 4",
                "zh": "1æœˆ4æ—¥"
            },
            "hash": "86e81641764103d2",
            "authors": [
                "Xu Guo",
                "Fulong Ye",
                "Xinghui Li",
                "Pengqi Tu",
                "Pengze Zhang",
                "Qichao Sun",
                "Songtao Zhao",
                "Xiangwang Hou",
                "Qian He"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.01425.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#synthetic",
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ† Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ† Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¾Ğ¼ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ pipeline SyncID-Pipe Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DreamID-V Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·-Ğ²-Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IDBench-V Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ†."
                },
                "en": {
                    "title": "Seamless Video Face Swapping with Identity Preservation",
                    "desc": "This paper presents a new framework for video face swapping that integrates image face swapping techniques with advanced machine learning methods like diffusion transformers and curriculum learning. The framework, named DreamID-V, focuses on maintaining identity preservation and visual realism while ensuring temporal consistency in videos. It introduces a novel data pipeline called SyncID-Pipe, which helps in training a video synthesizer that effectively manages identity information. Additionally, the paper proposes a new benchmark, IDBench-V, to evaluate the performance of video face swapping methods across various scenarios, demonstrating that DreamID-V outperforms existing techniques."
                },
                "zh": {
                    "title": "è§†é¢‘æ¢è„¸çš„æ–°çªç ´ï¼šçœŸå®ä¸èº«ä»½çš„å®Œç¾ç»“åˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘æ¢è„¸æ¡†æ¶ï¼Œç»“åˆäº†å›¾åƒæ¢è„¸æŠ€æœ¯ã€æ‰©æ•£å˜æ¢å™¨å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œä»¥å®ç°æ›´å¥½çš„èº«ä»½ä¿ç•™å’Œè§†è§‰çœŸå®æ„Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥SyncID-Pipeæ•°æ®ç®¡é“ï¼Œé¢„è®­ç»ƒèº«ä»½é”šå®šè§†é¢‘åˆæˆå™¨ï¼Œå¹¶ä¸å›¾åƒæ¢è„¸æ¨¡å‹ç»“åˆï¼Œæ„å»ºåŒå‘èº«ä»½å››å…ƒç»„è¿›è¡Œæ˜¾å¼ç›‘ç£ã€‚è®ºæ–‡ä¸­è¿˜æå‡ºäº†åŸºäºæ‰©æ•£å˜æ¢å™¨çš„DreamID-Væ¡†æ¶ï¼Œåˆ©ç”¨æ¨¡æ€æ„ŸçŸ¥æ¡ä»¶æ¨¡å—æ¥æœ‰æ•ˆæ³¨å…¥å¤šæ¨¡æ€æ¡ä»¶ã€‚é€šè¿‡åˆæˆåˆ°çœŸå®çš„è¯¾ç¨‹æœºåˆ¶å’Œèº«ä»½ä¸€è‡´æ€§å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œå¢å¼ºäº†åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è§†è§‰çœŸå®æ„Ÿå’Œèº«ä»½ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02256",
            "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
            "url": "https://huggingface.co/papers/2601.02256",
            "abstract": "Visual autoregressive models face training instability due to asynchronous policy conflicts, which are addressed through a novel framework enhancing group relative policy optimization with intermediate rewards, dynamic time-step reweighting, and mask propagation algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.",
            "score": 27,
            "issue_id": 426,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "b51d8fc1590931c7",
            "authors": [
                "Shikun Sun",
                "Liao Qu",
                "Huichao Zhang",
                "Yiheng Liu",
                "Yangyang Song",
                "Xian Li",
                "Xu Wang",
                "Yi Jiang",
                "Daniel K. Du",
                "Xinglong Wu",
                "Jia Jia"
            ],
            "affiliations": [
                "ByteDance",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02256.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#rl",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Group Relative Policy Optimization, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞºĞ»Ğ°Ğ´Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Reward Feedback Learning Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ VAR Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Stabilizing Visual Generation with Enhanced Policy Optimization",
                    "desc": "This paper addresses the training instability in Visual AutoRegressive (VAR) models caused by asynchronous policy conflicts. It introduces a new framework that enhances Group Relative Policy Optimization (GRPO) by incorporating intermediate rewards, dynamic time-step reweighting, and mask propagation algorithms. These components work together to stabilize training and improve the alignment of generated outputs. The proposed method shows significant improvements in sample quality compared to traditional GRPO methods, making VAR models more robust and effective in visual generation tasks."
                },
                "zh": {
                    "title": "è§£å†³è§†è§‰è‡ªå›å½’æ¨¡å‹è®­ç»ƒä¸ç¨³å®šæ€§çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è‡ªå›å½’æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”±äºå¼‚æ­¥ç­–ç•¥å†²çªè€Œå¯¼è‡´çš„ä¸ç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå¼•å…¥äº†ä¸­é—´å¥–åŠ±ã€åŠ¨æ€æ—¶é—´æ­¥é‡åŠ æƒå’Œæ©ç ä¼ æ’­ç®—æ³•ç­‰ä¸‰ä¸ªååŒç»„ä»¶ã€‚ä¸­é—´å¥–åŠ±ç”¨äºæŒ‡å¯¼æ—©æœŸç”Ÿæˆï¼ŒåŠ¨æ€æ—¶é—´æ­¥é‡åŠ æƒåˆ™å¸®åŠ©ç²¾ç¡®åˆ†é…ä¿¡ç”¨ï¼Œè€Œæ©ç ä¼ æ’­ç®—æ³•åˆ™ä»å¥–åŠ±åé¦ˆå­¦ä¹ çš„åŸåˆ™å‡ºå‘ï¼Œæ—¨åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šéš”ç¦»ä¼˜åŒ–æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ·æœ¬è´¨é‡å’Œç›®æ ‡å¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„GRPOåŸºçº¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¼˜åŒ–è§†è§‰è‡ªå›å½’æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.24138",
            "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
            "url": "https://huggingface.co/papers/2512.24138",
            "abstract": "Online reinforcement learning for diffusion model fine-tuning suffers from reward hacking due to proxy reward mismatches, which GARDO addresses through selective regularization, adaptive reference updates, and diversity-aware reward amplification.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.",
            "score": 23,
            "issue_id": 428,
            "pub_date": "2026-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "824aa5f73ac6084b",
            "authors": [
                "Haoran He",
                "Yuxiao Ye",
                "Jie Liu",
                "Jiajun Liang",
                "Zhiyong Wang",
                "Ziyang Yuan",
                "Xintao Wang",
                "Hangyu Mao",
                "Pengfei Wan",
                "Ling Pan"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Hong Kong University of Science and Technology",
                "Kuaishou Technology",
                "The University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.24138.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#optimization",
                    "#diffusion",
                    "#rlhf",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ reward hacking Ğ¿Ñ€Ğ¸ fine-tuning Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ GARDO â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GARDO ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ reward hacking, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ sample efficiency Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Diffusion Models with GARDO: Tackling Reward Hacking and Boosting Diversity",
                    "desc": "This paper presents GARDO, a framework designed to improve online reinforcement learning for fine-tuning diffusion models. It addresses the problem of reward hacking, which occurs when models optimize for proxy rewards that do not accurately reflect the desired outcomes. GARDO employs selective regularization to penalize only uncertain samples, adaptive reference updates to keep the model aligned with the current policy, and diversity-aware reward amplification to encourage varied outputs. The results demonstrate that GARDO effectively enhances image quality and diversity while maintaining efficient exploration in the learning process."
                },
                "zh": {
                    "title": "è§£å†³å¥–åŠ±é»‘å®¢ï¼Œæå‡ç”Ÿæˆå¤šæ ·æ€§ï¼",
                    "desc": "åœ¨çº¿å¼ºåŒ–å­¦ä¹ åœ¨æ‰©æ•£æ¨¡å‹å¾®è°ƒä¸­é¢ä¸´å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºä»£ç†å¥–åŠ±ä¸çœŸå®ç›®æ ‡ä¸åŒ¹é…ã€‚GARDOé€šè¿‡é€‰æ‹©æ€§æ­£åˆ™åŒ–ã€è‡ªé€‚åº”å‚è€ƒæ›´æ–°å’Œå¤šæ ·æ€§å¥–åŠ±æ”¾å¤§æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†å¥–åŠ±é»‘å®¢ç°è±¡ï¼ŒåŒæ—¶æé«˜äº†ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œè€Œä¸ç‰ºç‰²æ ·æœ¬æ•ˆç‡ã€‚GARDOçš„å…³é”®åœ¨äºé€‰æ‹©æ€§åœ°å¯¹é«˜ä¸ç¡®å®šæ€§çš„æ ·æœ¬è¿›è¡Œæƒ©ç½šï¼Œå¹¶å®šæœŸæ›´æ–°å‚è€ƒæ¨¡å‹ï¼Œä»¥ç¡®ä¿æ­£åˆ™åŒ–ç›®æ ‡çš„ç›¸å…³æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02358",
            "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
            "url": "https://huggingface.co/papers/2601.02358",
            "abstract": "VINO is a unified visual generator that uses a shared diffusion backbone with multimodal inputs to perform image and video generation and editing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
            "score": 21,
            "issue_id": 426,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "921cf734749276d1",
            "authors": [
                "Junyi Chen",
                "Tong He",
                "Zhoujie Fu",
                "Pengfei Wan",
                "Kun Gai",
                "Weicai Ye"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Nanyang Technology University",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02358.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "VINO â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ vision-language model Ñ Multimodal Diffusion Transformer, Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡. VINO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "VINO: One Model for All Visual Creations",
                    "desc": "VINO is a cutting-edge visual generator that integrates image and video generation and editing into a single framework using a shared diffusion backbone. It leverages a vision-language model combined with a Multimodal Diffusion Transformer to process various inputs like text, images, and videos, allowing for versatile visual tasks. The model employs interleaved conditioning tokens to guide the diffusion process, ensuring coherent outputs and effective instruction adherence. VINO's innovative multi-stage training pipeline enhances its capabilities, demonstrating superior visual quality and control in generating and editing both static and dynamic content."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è§†è§‰ç”Ÿæˆçš„æœªæ¥",
                    "desc": "VINOæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¡†æ¶å†…æ‰§è¡Œå›¾åƒå’Œè§†é¢‘çš„ç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡ã€‚å®ƒä½¿ç”¨å…±äº«çš„æ‰©æ•£éª¨å¹²ç½‘ç»œï¼Œç»“åˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘çš„å¤šæ¨¡æ€è¾“å…¥ï¼Œé¿å…äº†ä¾èµ–ç‰¹å®šä»»åŠ¡æ¨¡å‹çš„å¤æ‚æ€§ã€‚VINOå°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ç»“åˆï¼Œé€šè¿‡äº¤é”™çš„æ¡ä»¶æ ‡è®°å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œå®ç°å¤šå‚è€ƒåŸºç¡€ã€é•¿å½¢å¼æŒ‡ä»¤è·Ÿéšå’Œä¸€è‡´çš„èº«ä»½ä¿ç•™ã€‚è¯¥ç³»ç»Ÿçš„å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ä½¿å…¶èƒ½å¤Ÿé€æ­¥æ‰©å±•ä¸ºä¸€ä¸ªå¤šä»»åŠ¡ç”Ÿæˆå™¨ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„è§†è§‰è´¨é‡å’Œå¯æ§çš„å¤šèº«ä»½ç¼–è¾‘èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02281",
            "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
            "url": "https://huggingface.co/papers/2601.02281",
            "abstract": "InfiniteVGGT enables continuous 3D visual geometry understanding through a causal transformer with adaptive memory management, outperforming existing streaming methods in long-term stability while introducing a new benchmark for extended evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
            "score": 20,
            "issue_id": 432,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "6ad1993c1280bb64",
            "authors": [
                "Shuai Yuan",
                "Yantai Yang",
                "Xiaotian Yang",
                "Xupeng Zhang",
                "Zhonghao Zhao",
                "Lingming Zhang",
                "Zhipeng Zhang"
            ],
            "affiliations": [
                "AutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02281.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "â™¾ï¸",
                "ru": {
                    "title": "Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ: Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ²ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸",
                    "desc": "InfiniteVGGT Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ KV-ĞºĞµÑˆĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ĞµÑĞ¿Ñ€Ğ¸ÑÑ‚Ñ€Ğ°ÑÑ‚Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Long3D Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¸Ğ· 10,000 ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "InfiniteVGGT: Revolutionizing 3D Geometry Understanding with Adaptive Memory",
                    "desc": "InfiniteVGGT is a novel approach that enhances 3D visual geometry understanding by utilizing a causal transformer with an adaptive memory system. This method addresses the challenges of scalability and long-term stability that have hindered previous streaming architectures. By implementing a rolling memory mechanism and a pruning strategy, InfiniteVGGT effectively manages information over extended sequences without losing performance. Additionally, it introduces the Long3D benchmark, allowing for rigorous evaluation of continuous 3D geometry estimation across thousands of frames, setting a new standard for future research."
                },
                "zh": {
                    "title": "æ— é™è§†é‡ï¼Œç¨³å®šå‡ ä½•ç†è§£",
                    "desc": "InfiniteVGGT æ˜¯ä¸€ç§å› æœè§†è§‰å‡ ä½•å˜æ¢å™¨ï¼Œèƒ½å¤Ÿå®ç°æŒç»­çš„ 3D è§†è§‰å‡ ä½•ç†è§£ã€‚å®ƒé€šè¿‡è‡ªé€‚åº”çš„è®°å¿†ç®¡ç†ï¼Œå…‹æœäº†ç°æœ‰æµå¼æ–¹æ³•åœ¨é•¿æœŸç¨³å®šæ€§æ–¹é¢çš„ä¸è¶³ã€‚ä¸ä¼ ç»Ÿçš„æ‰¹å¤„ç†æ¨¡å‹ä¸åŒï¼ŒInfiniteVGGT é‡‡ç”¨äº†æ»šåŠ¨è®°å¿†çš„æ¦‚å¿µï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ— é™æ—¶åŸŸçš„è¾“å…¥ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº† Long3D åŸºå‡†ï¼Œé¦–æ¬¡æä¾›äº†å¯¹è¿ç»­ 3D å‡ ä½•ä¼°è®¡çš„ä¸¥æ ¼è¯„ä¼°å¹³å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20578",
            "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
            "url": "https://huggingface.co/papers/2512.20578",
            "abstract": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",
            "score": 19,
            "issue_id": 426,
            "pub_date": "2026-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "044a6c076f36aa3e",
            "authors": [
                "Amirhosein Ghasemabadi",
                "Di Niu"
            ],
            "affiliations": [
                "University of Alberta, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20578.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#hallucinations",
                    "#reasoning",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Gnosis â€” Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ ÑĞµÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¶Ğ¸Ğ¼Ğ°Ñ Ğ¸Ñ… Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ ~5 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¿Ñ€ĞµĞ½ĞµĞ±Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾ Ğ¼Ğ°Ğ»Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑÑƒĞ´ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Gnosis Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Awareness for Error Prediction",
                    "desc": "This paper presents Gnosis, a novel self-awareness mechanism for large language models (LLMs) that allows them to predict their own errors during inference. Instead of relying on external judges or additional computational resources, Gnosis analyzes internal states and attention patterns to assess correctness. It compresses these internal signals into manageable descriptors, enabling LLMs to perform self-verification with minimal added parameters and computational cost. The results demonstrate that Gnosis enhances accuracy and reliability in various tasks, showing that LLMs can effectively monitor their own performance without external assistance."
                },
                "zh": {
                    "title": "Gnosisï¼šè®©æ¨¡å‹è‡ªæˆ‘è¯†åˆ«é”™è¯¯çš„åˆ›æ–°æœºåˆ¶",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆæµç•…ä¸”å¤æ‚çš„è¾“å‡ºï¼Œä½†å¸¸å¸¸æ— æ³•è¯†åˆ«è‡ªèº«çš„é”™è¯¯å’Œå¹»è§‰ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–å¤–éƒ¨è¯„ä¼°è€…ã€å¤šæ ·æœ¬ä¸€è‡´æ€§æˆ–åŸºäºæ–‡æœ¬çš„è‡ªæˆ‘æ‰¹è¯„ï¼Œè¿™äº›æ–¹æ³•ä¼šå¢åŠ è®¡ç®—æˆæœ¬æˆ–ä¸çœŸå®æ­£ç¡®æ€§ç›¸å…³æ€§è¾ƒå¼±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGnosisçš„è½»é‡çº§è‡ªæˆ‘æ„è¯†æœºåˆ¶ï¼Œä½¿å¾—å†»ç»“çš„LLMsèƒ½å¤Ÿé€šè¿‡è§£ç éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æ¨¡å¼ä¸­çš„ä¿¡å·è¿›è¡Œå†…åœ¨è‡ªæˆ‘éªŒè¯ã€‚Gnosisèƒ½å¤Ÿè¢«åŠ¨è§‚å¯Ÿå†…éƒ¨ç—•è¿¹ï¼Œå°†å…¶å‹ç¼©ä¸ºå›ºå®šé¢„ç®—çš„æè¿°ç¬¦ï¼Œå¹¶ä»¥æå°çš„æ¨ç†æˆæœ¬é¢„æµ‹æ­£ç¡®æ€§ï¼Œä¸”ä»…å¢åŠ çº¦5Må‚æ•°ï¼Œç‹¬ç«‹äºåºåˆ—é•¿åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02346",
            "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
            "url": "https://huggingface.co/papers/2601.02346",
            "abstract": "Falcon-H1R is a 7B-parameter language model that achieves competitive reasoning performance through efficient training strategies and architectural design, enabling scalable reasoning capabilities in compact models.  \t\t\t\t\tAI-generated summary \t\t\t\t This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
            "score": 12,
            "issue_id": 427,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "be0a839ff0b87c9a",
            "authors": [
                "Falcon LLM Team",
                "Iheb Chaabane",
                "Puneesh Khanna",
                "Suhail Mohmad",
                "Slim Frikha",
                "Shi Hu",
                "Abdalgader Abubaker",
                "Reda Alami",
                "Mikhail Lubinets",
                "Mohamed El Amine Seddik",
                "Hakim Hacid"
            ],
            "affiliations": [
                "Falcon LLM Team",
                "Technology Innovation Institute (TIIUAE)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02346.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#small_models"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞœĞ¾Ñ‰Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Falcon-H1R â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰ÑƒÑÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 2-7 Ñ€Ğ°Ğ· Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (SFT Ğ¸ RL) Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾-Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ SLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Compact Power: Reasoning Efficiency with Falcon-H1R",
                    "desc": "Falcon-H1R is a compact 7B-parameter language model designed to excel in reasoning tasks while maintaining efficiency. It achieves competitive performance by utilizing advanced training strategies and a hybrid-parallel architecture, allowing it to outperform larger models in various benchmarks. The model emphasizes the significance of data curation and targeted training methods, such as efficient supervised fine-tuning (SFT) and reinforcement learning (RL) scaling. Overall, Falcon-H1R showcases that smaller models can achieve high reasoning capabilities without the need for increased size, making them practical for complex reasoning applications."
                },
                "zh": {
                    "title": "å°å‹æ¨¡å‹ä¹Ÿèƒ½å®ç°å¼ºå¤§æ¨ç†èƒ½åŠ›",
                    "desc": "Falcon-H1Ræ˜¯ä¸€ä¸ªæ‹¥æœ‰70äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ¶æ„è®¾è®¡ï¼Œå®ç°äº†ç«äº‰åŠ›çš„æ¨ç†æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°å‡ºä¸2åˆ°7å€æ›´å¤§æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å°å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚ç ”ç©¶å¼ºè°ƒäº†æ•°æ®ç²¾å¿ƒç­–åˆ’å’Œé’ˆå¯¹æ€§è®­ç»ƒç­–ç•¥çš„é‡è¦æ€§ï¼Œè¿™ä½¿å¾—åœ¨ä¸å¢åŠ æ¨¡å‹è§„æ¨¡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFalcon-H1Ré€šè¿‡æ··åˆå¹¶è¡Œæ¶æ„è®¾è®¡ï¼Œç»“åˆå¿«é€Ÿæ¨ç†å’Œé«˜å‡†ç¡®æ€§ï¼Œæ¨åŠ¨äº†æ¨ç†æ•ˆç‡çš„æé™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02356",
            "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
            "url": "https://huggingface.co/papers/2601.02356",
            "abstract": "Talk2Move presents a reinforcement learning-based diffusion framework that enables precise, semantically faithful spatial transformations of objects in scenes using natural language instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
            "score": 11,
            "issue_id": 426,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "a0b15d4316aee180",
            "authors": [
                "Jing Tan",
                "Zhaoyang Zhang",
                "Yantao Shen",
                "Jiarui Cai",
                "Shuo Yang",
                "Jiajun Wu",
                "Wei Xia",
                "Zhuowen Tu",
                "Stefano Soatto"
            ],
            "affiliations": [
                "AWS Agentic AI",
                "Amazon Robotics",
                "Amazon Web Services",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02356.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº",
                    "desc": "Talk2Move â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² ÑÑ†ĞµĞ½Ğµ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹."
                },
                "en": {
                    "title": "Transforming Scenes with Words: Talk2Move's Reinforcement Learning Revolution",
                    "desc": "Talk2Move is a novel framework that uses reinforcement learning to enable spatial transformations of objects in scenes based on natural language instructions. It addresses the challenge of performing geometric transformations like translation, rotation, and resizing, which are difficult for existing methods due to limited data and optimization constraints. By utilizing Group Relative Policy Optimization (GRPO), Talk2Move generates diverse rollouts from images and text variations, eliminating the need for expensive paired datasets. The framework incorporates spatial rewards that directly assess geometric changes, resulting in accurate and coherent transformations that align well with the provided linguistic descriptions."
                },
                "zh": {
                    "title": "Talk2Moveï¼šè‡ªç„¶è¯­è¨€é©±åŠ¨çš„ç‰©ä½“ç©ºé—´å˜æ¢",
                    "desc": "Talk2Moveæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç²¾ç¡®åœ°å¯¹åœºæ™¯ä¸­çš„ç‰©ä½“è¿›è¡Œç©ºé—´å˜æ¢ã€‚è¯¥æ–¹æ³•è§£å†³äº†å¤šæ¨¡æ€ç”Ÿæˆç³»ç»Ÿåœ¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼ä¸‹è¿›è¡Œç‰©ä½“å‡ ä½•å˜æ¢çš„æŒ‘æˆ˜ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨é…å¯¹ç›‘ç£å’Œåƒç´ çº§ä¼˜åŒ–æ–¹é¢çš„å±€é™ã€‚é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒTalk2Moveèƒ½å¤Ÿåœ¨ä¸éœ€è¦æ˜‚è´µé…å¯¹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ¢ç´¢å‡ ä½•åŠ¨ä½œå¹¶ç”Ÿæˆå¤šæ ·çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTalk2Moveåœ¨ç©ºé—´å‡†ç¡®æ€§å’Œåœºæ™¯ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æ–‡æœ¬å¼•å¯¼ç¼–è¾‘æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.24601",
            "title": "Recursive Language Models",
            "url": "https://huggingface.co/papers/2512.24601",
            "abstract": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.",
            "score": 7,
            "issue_id": 430,
            "pub_date": "2026-12-31",
            "pub_date_card": {
                "ru": "31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 31",
                "zh": "12æœˆ31æ—¥"
            },
            "hash": "55f7266c613668ec",
            "authors": [
                "Alex L. Zhang",
                "Tim Kraska",
                "Omar Khattab"
            ],
            "affiliations": [
                "MIT CSAIL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.24601.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Recursive Language Models (RLM) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ñƒ ÑĞµĞ±Ñ, Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ²Ğ° Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unlocking Infinite Context with Recursive Language Models",
                    "desc": "This paper introduces Recursive Language Models (RLMs), a novel approach for large language models (LLMs) to manage long prompts effectively. RLMs treat lengthy inputs as an external environment, enabling the model to break down the prompt and recursively analyze its components. The study demonstrates that RLMs can process inputs significantly larger than the typical context window of LLMs, achieving superior performance on various long-context tasks. Additionally, RLMs maintain a cost-effective solution, providing high-quality outputs without increasing the expense per query."
                },
                "zh": {
                    "title": "é€’å½’è¯­è¨€æ¨¡å‹ï¼šè¶…è¶Šä¸Šä¸‹æ–‡é™åˆ¶çš„åˆ›æ–°",
                    "desc": "æˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•è®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†ä»»æ„é•¿åº¦çš„æç¤ºï¼Œé‡‡ç”¨æ¨ç†æ—¶é—´æ‰©å±•çš„è§†è§’ã€‚æˆ‘ä»¬æå‡ºäº†é€’å½’è¯­è¨€æ¨¡å‹ï¼ˆRLMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ¨ç†ç­–ç•¥ï¼Œå°†é•¿æç¤ºè§†ä¸ºå¤–éƒ¨ç¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼Œå…è®¸LLMä»¥ç¼–ç¨‹æ–¹å¼æ£€æŸ¥ã€åˆ†è§£å¹¶é€’å½’è°ƒç”¨è‡ªèº«å¤„ç†æç¤ºç‰‡æ®µã€‚ç ”ç©¶å‘ç°ï¼ŒRLMsèƒ½å¤ŸæˆåŠŸå¤„ç†è¾“å…¥é•¿åº¦è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ä¸¤ä¸ªæ•°é‡çº§çš„æƒ…å†µï¼Œå¹¶ä¸”åœ¨å¤„ç†è¾ƒçŸ­æç¤ºæ—¶ï¼Œå…¶è´¨é‡æ˜¾è‘—ä¼˜äºåŸºç¡€LLMså’Œå¸¸è§çš„é•¿ä¸Šä¸‹æ–‡æ¡†æ¶ï¼Œä¸”æ¯æ¬¡æŸ¥è¯¢çš„æˆæœ¬ç›¸å½“æˆ–æ›´ä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.01046",
            "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
            "url": "https://huggingface.co/papers/2601.01046",
            "abstract": "KV-Embedding enables training-free representation learning from frozen LLMs by utilizing key-value states for enhanced context access and automated layer selection.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.",
            "score": 5,
            "issue_id": 426,
            "pub_date": "2026-01-03",
            "pub_date_card": {
                "ru": "3 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 3",
                "zh": "1æœˆ3æ—¥"
            },
            "hash": "0d6e03d3443f8363",
            "authors": [
                "Yixuan Tang",
                "Yi Yang"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.01046.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ”‘",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ KV-Embedding Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ-ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ ĞºĞ°Ğº ÑĞ¶Ğ°Ñ‚Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 10% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Context with KV-Embedding in Frozen LLMs",
                    "desc": "The paper introduces KV-Embedding, a novel framework that enhances representation learning from frozen large language models (LLMs) without the need for additional training. It addresses two main challenges: the limitations of causal attention that prevent early tokens from accessing later context, and the bias of next-token prediction that favors generation over meaningful representation. By utilizing key-value states from the final token of each layer, KV-Embedding allows all tokens to access comprehensive sequence-level context in a single forward pass. The method shows significant improvements in performance on various benchmarks, demonstrating the potential of manipulating internal states for better representation learning."
                },
                "zh": {
                    "title": "KV-Embeddingï¼šæ— è®­ç»ƒè¡¨ç¤ºå­¦ä¹ çš„æ–°æ–¹æ³•",
                    "desc": "KV-Embeddingæ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œæ— è®­ç»ƒçš„è¡¨ç¤ºå­¦ä¹ ã€‚å®ƒé€šè¿‡åˆ©ç”¨å…³é”®-å€¼ï¼ˆKVï¼‰çŠ¶æ€æ¥å¢å¼ºä¸Šä¸‹æ–‡è®¿é—®ï¼Œå¹¶è‡ªåŠ¨é€‰æ‹©å±‚çº§ï¼Œä»è€Œè§£å†³äº†å› æœæ³¨æ„åŠ›å’Œä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡å¸¦æ¥çš„ç»“æ„æ€§æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å…è®¸æ‰€æœ‰æ ‡è®°åœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­è®¿é—®åºåˆ—çº§ä¸Šä¸‹æ–‡ï¼Œæå‡äº†è¡¨ç¤ºçš„è¯­ä¹‰å‹ç¼©èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKV-Embeddingåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æ— è®­ç»ƒåŸºçº¿ï¼Œå±•ç¤ºäº†å†…éƒ¨çŠ¶æ€æ“ä½œåœ¨è¡¨ç¤ºå­¦ä¹ ä¸­çš„é«˜æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02179",
            "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
            "url": "https://huggingface.co/papers/2601.02179",
            "abstract": "Multi-turn conversation confidence estimation lacks systematic evaluation frameworks, prompting the introduction of novel metrics and a \"Hinter-Guesser\" paradigm for controlled dataset generation to improve calibration and monotonicity.  \t\t\t\t\tAI-generated summary \t\t\t\t While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
            "score": 4,
            "issue_id": 438,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "e1689fc7ddf3b100",
            "authors": [
                "Caiqi Zhang",
                "Ruihan Yang",
                "Xiaochen Zhu",
                "Chengzu Li",
                "Tiancheng Hu",
                "Yijiang River Dong",
                "Deqing Yang",
                "Nigel Collier"
            ],
            "affiliations": [
                "Fudan University",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02179.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#hallucinations",
                    "#alignment",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞšĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ (confidence estimation) ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ³Ğ´Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑÑ…: ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ñ…Ğ¾Ğ´Ñƒ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Expected Calibration Error (InfoECE), Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° \"Hinter-Guesser\" Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ P(Sufficient) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Confidence in Multi-Turn Conversations",
                    "desc": "This paper addresses the challenge of estimating confidence in multi-turn conversations, which is crucial for improving the reliability of conversational agents. It introduces a systematic evaluation framework that focuses on two main aspects: per-turn calibration and the monotonicity of confidence as more context is provided. The authors propose new metrics, such as the length-normalized Expected Calibration Error (InfoECE), and a novel dataset generation method called the 'Hinter-Guesser' paradigm. Their findings indicate that existing confidence estimation techniques are inadequate in multi-turn settings, and they suggest a new approach, P(Sufficient), which shows improved performance but highlights that further work is needed in this area."
                },
                "zh": {
                    "title": "æå‡å¤šè½®å¯¹è¯ç½®ä¿¡åº¦çš„ç³»ç»ŸåŒ–ç ”ç©¶",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤šè½®å¯¹è¯ä¸­çš„ç½®ä¿¡åº¦ä¼°è®¡é—®é¢˜ï¼Œæå‡ºäº†ç³»ç»ŸåŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•åœ¨å¤šè½®å¯¹è¯ä¸­é¢ä¸´æ ¡å‡†å’Œå•è°ƒæ€§çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡å’Œ\"Hinter-Guesser\"èŒƒå¼ï¼Œä»¥ç”Ÿæˆå—æ§çš„æ•°æ®é›†ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘æ›´å¯é çš„å¯¹è¯ä»£ç†æä¾›äº†åŸºç¡€æ–¹æ³•è®ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.01836",
            "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
            "url": "https://huggingface.co/papers/2601.01836",
            "abstract": "COMPASS evaluates large language models' compliance with organizational policies, revealing significant gaps in enforcing prohibitions despite strong performance on legitimate requests.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
            "score": 4,
            "issue_id": 428,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "bf1e3c3f8aa12378",
            "authors": [
                "Dasol Choi",
                "DongGeon Lee",
                "Brigitta Jesica Kartono",
                "Helena Berndt",
                "Taeyoun Kwon",
                "Joonwon Jang",
                "Haon Park",
                "Hwanjo Yu",
                "Minsuk Kahng"
            ],
            "affiliations": [
                "AIM Intelligence",
                "BMW Group",
                "POSTECH",
                "Seoul National University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.01836.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#security"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğº Ğ¾Ğ±Ñ…Ğ¾Ğ´Ñƒ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ COMPASS â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 5920 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ĞµĞ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ adversarial Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞµĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ĞµĞ³Ğ¸Ñ‚Ğ¸Ğ¼Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ñ‹ÑˆĞµ 95%, Ğ½Ğ¾ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² 13-40% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Bridging the Compliance Gap in AI with COMPASS",
                    "desc": "The paper introduces COMPASS, a framework designed to assess how well large language models (LLMs) follow specific organizational policies. It highlights a critical issue where these models perform well on legitimate requests but struggle significantly with enforcing prohibitions, showing only 13-40% accuracy in denying harmful queries. By testing seven advanced models across various industry scenarios, the study reveals a concerning gap in compliance that could jeopardize safety in high-stakes applications. COMPASS aims to fill this gap by providing a systematic approach to evaluate and improve the adherence of LLMs to organizational guidelines."
                },
                "zh": {
                    "title": "ç¡®ä¿AIéµå¾ªç»„ç»‡æ”¿ç­–çš„COMPASSæ¡†æ¶",
                    "desc": "COMPASSæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦éµå¾ªç»„ç»‡æ”¿ç­–çš„ç³»ç»Ÿæ¡†æ¶ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨å¤„ç†åˆæ³•è¯·æ±‚æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ‰§è¡Œç¦æ­¢æ€§æ”¿ç­–æ–¹é¢å´å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„LLMåœ¨æ‹’ç»å¯¹æŠ—æ€§è¯·æ±‚æ—¶çš„å‡†ç¡®ç‡ä»…ä¸º13%åˆ°40%ã€‚å› æ­¤ï¼ŒCOMPASSä¸ºç¡®ä¿AIåœ¨å…³é”®é¢†åŸŸçš„å®‰å…¨æ€§æä¾›äº†å¿…è¦çš„è¯„ä¼°å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02267",
            "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
            "url": "https://huggingface.co/papers/2601.02267",
            "abstract": "DiffProxy enables human mesh recovery from multi-view images by generating multi-view consistent proxies using diffusion-based generative priors, achieving state-of-the-art performance through synthetic training and robust test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html",
            "score": 3,
            "issue_id": 439,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "f5aff3b9929ea83a",
            "authors": [
                "Renke Wang",
                "Zhenyu Zhang",
                "Ying Tai",
                "Jian Yang"
            ],
            "affiliations": [
                "Nanjing University, School of Intelligent Science and Technology",
                "PCA Lab, Nanjing University of Science and Technology, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02267.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#cv",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ§‘â€ğŸ¦¾",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ²",
                    "desc": "DiffProxy - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ mesh Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒ-Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ€ÑƒĞº Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ñ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸ĞµĞ¹ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Bridging Synthetic and Real-World Data for Superior Mesh Recovery",
                    "desc": "DiffProxy is a framework designed to improve human mesh recovery from multi-view images by creating consistent proxies using diffusion-based generative models. It addresses the issue of biased training from imperfect real-world datasets by utilizing synthetic data with accurate annotations. The framework introduces a multi-conditional mechanism for generating pixel-aligned proxies, a hand refinement module for enhancing details, and an uncertainty-aware scaling method for better performance in difficult scenarios. As a result, DiffProxy achieves state-of-the-art results on various benchmarks, showcasing its ability to generalize well to real-world challenges."
                },
                "zh": {
                    "title": "DiffProxyï¼šä»å¤šè§†è§’å›¾åƒä¸­æ¢å¤äººä½“ç½‘æ ¼çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "DiffProxy æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„äººä½“ä»£ç†æ¥å®ç°ä»å¤šè§†è§’å›¾åƒä¸­æ¢å¤äººä½“ç½‘æ ¼ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆå…ˆéªŒï¼Œè§£å†³äº†åˆæˆè®­ç»ƒä¸çœŸå®ä¸–ç•Œæ³›åŒ–ä¹‹é—´çš„å·®è·ã€‚DiffProxy çš„åˆ›æ–°åŒ…æ‹¬å¤šæ¡ä»¶æœºåˆ¶ç”Ÿæˆåƒç´ å¯¹é½çš„äººä½“ä»£ç†ã€æ‰‹éƒ¨ç»†åŒ–æ¨¡å—å¢å¼ºå±€éƒ¨ç»†èŠ‚ï¼Œä»¥åŠä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œæé«˜äº†åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„é²æ£’æ€§ã€‚é€šè¿‡å®Œå…¨åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒï¼ŒDiffProxy åœ¨äº”ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é®æŒ¡å’Œéƒ¨åˆ†è§†å›¾ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23035",
            "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
            "url": "https://huggingface.co/papers/2512.23035",
            "abstract": "A semi-supervised remote sensing image segmentation framework combines vision-language and self-supervised models to reduce pseudo-label drift through dual-student architecture and semantic co-guidance mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.",
            "score": 3,
            "issue_id": 429,
            "pub_date": "2026-12-28",
            "pub_date_card": {
                "ru": "28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 28",
                "zh": "12æœˆ28æ—¥"
            },
            "hash": "46b30a51798dbc10",
            "authors": [
                "Yi Zhou",
                "Xuechao Zou",
                "Shun Zhang",
                "Kai Li",
                "Shiying Wang",
                "Jingming Chen",
                "Congyan Lang",
                "Tengfei Cao",
                "Pin Tao",
                "Yuanchun Shi"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
                "Intelligent Computing and Application Laboratory of Qinghai Province, Qinghai University, Xining, China",
                "Key Lab of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China",
                "Key Laboratory of Pervasive Computing, Ministry of Education",
                "School of Computer Technology and Application, Qinghai University, Xining, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23035.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ´Ñ€ĞµĞ¹Ñ„Ğ°: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Co2S â€” Ğ¿Ğ¾Ğ»ÑƒÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ´Ğ²ÑƒĞ¼Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DINOv3. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾-Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Remote Sensing Segmentation with Co2S Framework",
                    "desc": "This paper presents Co2S, a semi-supervised framework for remote sensing image segmentation that addresses the issue of pseudo-label drift. By integrating vision-language models and self-supervised models, it employs a dual-student architecture to enhance stability during training. The framework utilizes a semantic co-guidance mechanism to provide both explicit and implicit guidance, improving the model's ability to maintain semantic consistency. Additionally, a collaborative fusion strategy combines global and local features to achieve high-quality segmentation results across multiple datasets."
                },
                "zh": {
                    "title": "æå‡é¥æ„Ÿå›¾åƒåˆ†å‰²ç²¾åº¦çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŠç›‘ç£é¥æ„Ÿå›¾åƒåˆ†å‰²æ¡†æ¶Co2Sï¼Œæ—¨åœ¨å‡å°‘ä¼ªæ ‡ç­¾æ¼‚ç§»çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹å’Œè‡ªç›‘ç£æ¨¡å‹ï¼Œé€šè¿‡åŒå­¦ç”Ÿæ¶æ„å’Œè¯­ä¹‰ååŒæœºåˆ¶æ¥æé«˜åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œä½¿ç”¨äº†åŸºäºViTçš„å¼‚æ„åŒå­¦ç”Ÿæ¶æ„ï¼Œåˆ†åˆ«åˆå§‹åŒ–ä¸ºé¢„è®­ç»ƒçš„CLIPå’ŒDINOv3ï¼Œä»¥å‡è½»é”™è¯¯ç´¯ç§¯ã€‚é€šè¿‡æ˜¾å¼-éšå¼è¯­ä¹‰ååŒæœºåˆ¶å’Œå…¨å±€-å±€éƒ¨ç‰¹å¾ååŒèåˆç­–ç•¥ï¼Œæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå±€éƒ¨ç»†èŠ‚ï¼Œä»è€Œå®ç°é«˜ç²¾åº¦çš„åˆ†å‰²ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.01426",
            "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
            "url": "https://huggingface.co/papers/2601.01426",
            "abstract": "SWE-Lego achieves state-of-the-art performance in software engineering task resolution through a lightweight supervised fine-tuning approach combined with a curated dataset and refined training procedures.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.",
            "score": 2,
            "issue_id": 432,
            "pub_date": "2026-01-04",
            "pub_date_card": {
                "ru": "4 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 4",
                "zh": "1æœˆ4æ—¥"
            },
            "hash": "5f09f67db35870c2",
            "authors": [
                "Chaofan Tao",
                "Jierun Chen",
                "Yuxin Jiang",
                "Kaiqi Kou",
                "Shaowei Wang",
                "Ruoyu Wang",
                "Xiaohui Li",
                "Sidi Yang",
                "Yiming Du",
                "Jianbo Dai",
                "Zhiming Mao",
                "Xinyu Wang",
                "Lifeng Shang",
                "Haoli Bai"
            ],
            "affiliations": [
                "CUHK",
                "HKU",
                "Huawei Technologies",
                "NTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.01426.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#science",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#open_source",
                    "#plp"
                ],
                "emoji": "ğŸ§±",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾: Ğ»Ñ‘Ğ³ĞºĞ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ",
                    "desc": "SWE-Lego â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ñ‚Ğ¸Ğ¿Ğ° reinforcement learning. ĞšĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 32 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ 18 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹. ĞŸÑ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ curriculum learning Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench: 42.2% Ğ´Ğ»Ñ 8B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ 52.6% Ğ´Ğ»Ñ 32B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 58.8% Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "SWE-Lego: Lightweight SFT for Superior Software Engineering Solutions",
                    "desc": "SWE-Lego is a novel approach in machine learning that focuses on software engineering task resolution using a lightweight supervised fine-tuning (SFT) method. It utilizes a specially curated dataset containing 32,000 high-quality task instances and 18,000 validated trajectories, which enhances the training process by combining real and synthetic data. The refined SFT procedure incorporates error masking and a difficulty-based curriculum to improve the quality of actions taken by the model. Empirical results demonstrate that SWE-Lego achieves state-of-the-art performance among open-source models, significantly boosting results through test-time scaling techniques."
                },
                "zh": {
                    "title": "SWE-Legoï¼šè½»é‡çº§å¾®è°ƒå®ç°è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„æœ€ä½³æ€§èƒ½",
                    "desc": "SWE-Legoæ˜¯ä¸€ç§è½»é‡çº§çš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«32,000ä¸ªé«˜è´¨é‡ä»»åŠ¡å®ä¾‹å’Œ18,000ä¸ªéªŒè¯è½¨è¿¹çš„æ•°æ®é›†ï¼Œç»“åˆçœŸå®å’Œåˆæˆæ•°æ®ä»¥æé«˜è´¨é‡å’Œæ•°é‡ã€‚è¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†æ”¹è¿›çš„å¾®è°ƒç¨‹åºï¼Œé€šè¿‡é”™è¯¯æ©è”½å’ŒåŸºäºéš¾åº¦çš„è¯¾ç¨‹æ¥æå‡è¡ŒåŠ¨è´¨é‡å’Œæ•´ä½“æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWE-Legoæ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†å¼€æºæ¨¡å‹ä¸­çš„é¢†å…ˆæ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.01576",
            "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
            "url": "https://huggingface.co/papers/2601.01576",
            "abstract": "An LLM-powered agentic system for transparent, evidence-based novelty assessment in peer review that retrieves and analyzes prior work through semantic search and hierarchical taxonomy construction.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, OpenNovelty grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.",
            "score": 1,
            "issue_id": 426,
            "pub_date": "2026-01-04",
            "pub_date_card": {
                "ru": "4 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 4",
                "zh": "1æœˆ4æ—¥"
            },
            "hash": "8eb218277ba60f81",
            "authors": [
                "Ming Zhang",
                "Kexin Tan",
                "Yueyuan Huang",
                "Yujiong Shen",
                "Chunchun Ma",
                "Li Ju",
                "Xinran Zhang",
                "Yuhui Wang",
                "Wenqing Jing",
                "Jingyi Deng",
                "Huayu Sha",
                "Binze Hu",
                "Jingqi Tong",
                "Changhao Jiang",
                "Yage Geng",
                "Yuankai Ying",
                "Yue Zhang",
                "Zhangyue Yin",
                "Zhiheng Xi",
                "Shihan Dou",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "Claremont McKenna College",
                "Fudan University",
                "WisPaper.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.01576.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#agents",
                    "#rag",
                    "#science"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸",
                    "desc": "OpenNovelty â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»Ğ°Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ’ÑĞµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ ÑĞ²Ğ½Ñ‹Ğ¼ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 500 ÑÑ‚Ğ°Ñ‚ÑŒÑÑ… ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ ICLR 2026 Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ ÑƒĞ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ."
                },
                "en": {
                    "title": "Empowering Peer Review with Evidence-Based Novelty Assessment",
                    "desc": "This paper introduces OpenNovelty, a system that uses large language models (LLMs) to improve the novelty assessment process in peer review. It operates in four phases: extracting claims, retrieving relevant literature through semantic search, constructing a hierarchical taxonomy, and synthesizing findings into a structured report. By grounding its evaluations in actual research papers, OpenNovelty ensures that its novelty assessments are transparent and verifiable. The system has been tested on over 500 submissions, demonstrating its ability to identify significant prior work that may be overlooked by authors."
                },
                "zh": {
                    "title": "æå‡åŒè¡Œè¯„å®¡çš„æ–°é¢–æ€§è¯„ä¼°å·¥å…·",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOpenNoveltyçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜åŒè¡Œè¯„å®¡ä¸­å¯¹æ–°é¢–æ€§çš„è¯„ä¼°é€æ˜åº¦å’ŒåŸºäºè¯æ®çš„åˆ†æã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å››ä¸ªé˜¶æ®µè¿›è¡Œå·¥ä½œï¼šæå–æ ¸å¿ƒä»»åŠ¡å’Œè´¡çŒ®å£°æ˜ã€åŸºäºè¯­ä¹‰æœç´¢æ£€ç´¢ç›¸å…³æ–‡çŒ®ã€æ„å»ºæ ¸å¿ƒä»»åŠ¡ç›¸å…³å·¥ä½œçš„å±‚æ¬¡åˆ†ç±»æ³•ï¼Œå¹¶è¿›è¡Œè´¡çŒ®çº§åˆ«çš„å…¨æ–‡æ¯”è¾ƒã€‚ä¸ç®€å•çš„LLMæ–¹æ³•ä¸åŒï¼ŒOpenNoveltyçš„è¯„ä¼°åŸºäºçœŸå®æ–‡çŒ®ï¼Œç¡®ä¿åˆ¤æ–­çš„å¯éªŒè¯æ€§ã€‚è¯¥ç³»ç»Ÿå·²åœ¨500å¤šç¯‡ICLR 2026çš„æäº¤ä¸­åº”ç”¨ï¼Œåˆæ­¥åˆ†æè¡¨æ˜å®ƒèƒ½å¤Ÿè¯†åˆ«ç›¸å…³çš„å…ˆå‰å·¥ä½œï¼ŒåŒ…æ‹¬ä½œè€…å¯èƒ½å¿½è§†çš„ç›¸å…³è®ºæ–‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.00863",
            "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
            "url": "https://huggingface.co/papers/2601.00863",
            "abstract": "We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
            "score": 1,
            "issue_id": 436,
            "pub_date": "2026-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "25795676ed9d7a0a",
            "authors": [
                "Markus J. Buehler"
            ],
            "affiliations": [
                "Center for Computational Science and Engineering",
                "Laboratory for Atomistic and Molecular Mechanics",
                "Massachusetts Institute of Technology",
                "Schwarzman College of Computing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.00863.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#science",
                    "#diffusion"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ±Ñ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹: Ğ¾Ñ‚ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğº Ğ¼ÑƒĞ·Ñ‹ĞºĞµ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ materiomusic â€” Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ñ‹Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ·Ğ²ÑƒĞºĞ¸ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ²ÑƒĞº Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾ÑĞ»ÑƒÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑƒÑĞ»Ñ‹ÑˆĞ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¾Ñ‚ Ñ„ĞµĞ¼Ñ‚Ğ¾ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ»ĞµÑ‚Ğ½ĞµĞ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞºĞ°Ğ» Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ¥Ğ¾Ğ»Ğ»Ğ°-ĞŸĞµÑ‚Ñ‡Ğ° Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ğ° Ğ² Ğ½Ğ°ÑƒĞºĞµ Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ¾ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ²Ğ¸Ğ±Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‰ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Harmonizing Matter and Music: A New Generative Framework",
                    "desc": "The paper presents 'materiomusic', a framework that connects the structures of matter with musical composition. It explores how principles from molecular vibrations and evolutionary patterns can be translated into musical tones and structures. By using reversible mappings, the authors demonstrate that sound can reveal scientific insights, making listening a way to understand complex systems. The study highlights the interplay between creativity and physics, suggesting that both science and art thrive under constraints, leading to innovative outcomes."
                },
                "zh": {
                    "title": "ç‰©è´¨ä¸éŸ³ä¹çš„ç”Ÿæˆæ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†materiomusicï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ç‰©è´¨çš„å±‚æ¬¡ç»“æ„ä¸éŸ³ä¹çš„ç»„æˆé€»è¾‘è”ç³»èµ·æ¥çš„ç”Ÿæˆæ¡†æ¶ã€‚é€šè¿‡å¯¹è›‹ç™½è´¨ã€èœ˜è››ç½‘å’Œç«ç„°åŠ¨åŠ›å­¦çš„ç ”ç©¶ï¼Œå‘ç°æŒ¯åŠ¨å’Œå»ºç­‘åŸåˆ™åœ¨éŸ³ä¹ä¸­è¡¨ç°ä¸ºéŸ³è°ƒå±‚æ¬¡ã€å’Œå£°è¿›ç¨‹å’Œé•¿ç¨‹éŸ³ä¹å½¢å¼ã€‚æˆ‘ä»¬ä½¿ç”¨å¯é€†æ˜ å°„ï¼Œå°†åˆ†å­å…‰è°±è½¬åŒ–ä¸ºéŸ³ä¹éŸ³è°ƒï¼Œå±•ç¤ºå£°éŸ³å¦‚ä½•ä½œä¸ºç§‘å­¦æ¢æµ‹å·¥å…·ï¼ŒéŸ³ä¹åˆ›ä½œå¦‚ä½•æˆä¸ºç‰©è´¨çš„è“å›¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå½“ç°æœ‰è‡ªç”±åº¦æ— æ³•æ»¡è¶³çº¦æŸæ—¶ï¼Œç§‘å­¦å’Œè‰ºæœ¯ä¸­çš„æ–°é¢–æ€§ä¾¿ä¼šå‡ºç°ï¼Œæ¨åŠ¨å¯è¡Œé…ç½®ç©ºé—´çš„æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21472",
            "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
            "url": "https://huggingface.co/papers/2512.21472",
            "abstract": "A large-scale public multi-annotator skin lesion segmentation dataset is introduced with extensive metadata for annotator analysis and consensus modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.",
            "score": 1,
            "issue_id": 429,
            "pub_date": "2026-12-25",
            "pub_date_card": {
                "ru": "25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 25",
                "zh": "12æœˆ25æ—¥"
            },
            "hash": "28ce6c37f8386365",
            "authors": [
                "Kumar Abhishek",
                "Jeremy Kawahara",
                "Ghassan Hamarneh"
            ],
            "affiliations": [
                "AIP Labs, Hungary",
                "Medical Image Analysis Lab, School of Computing Science, Simon Fraser University, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21472.jpg",
            "data": {
                "categories": [
                    "#open_source"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ° Ğ´Ğ»Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ°: Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´ĞµÑ€Ğ¼Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ISIC MultiAnnot++ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 17,684 Ğ¼Ğ°ÑĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµÑ€Ğ¼Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ Ğ¼ĞµÑ‚Ğ°Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ñ… ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ²Ğ°Ğ»Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´ĞµÑ€Ğ¼Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ñ€ĞµÑÑƒÑ€Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼ĞµĞ¶ÑĞºÑĞ¿ĞµÑ€Ñ‚-Ğ½ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Skin Lesion Insights with MultiAnnot++",
                    "desc": "This paper presents the ISIC MultiAnnot++ dataset, a large-scale public resource for skin lesion segmentation that includes extensive metadata for analyzing annotator performance. The dataset consists of 17,684 segmentation masks from 14,967 dermoscopic images, making it the largest publicly available multi-annotator skin lesion segmentation dataset. It allows researchers to study the variability in segmentations provided by different annotators and to model consensus among them. Additionally, the dataset includes information about the annotators' skill levels and the tools used, facilitating deeper insights into segmentation quality and preferences."
                },
                "zh": {
                    "title": "æ¨åŠ¨çš®è‚¤ç—…å˜åˆ†å‰²ç ”ç©¶çš„å¤šæ ‡æ³¨æ•°æ®é›†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å…¬å…±å¤šæ ‡æ³¨çš®è‚¤ç—…å˜åˆ†å‰²æ•°æ®é›†ï¼Œåä¸ºISIC MultiAnnot++ã€‚è¯¥æ•°æ®é›†åŒ…å«17,684ä¸ªåˆ†å‰²æ©è†œï¼Œè¦†ç›–14,967å¼ çš®è‚¤ç—…å˜çš„çš®è‚¤é•œå›¾åƒï¼Œæ˜¯ç›®å‰æœ€å¤§çš„å…¬å¼€å¯ç”¨çš„å¤šæ ‡æ³¨çš®è‚¤ç—…å˜åˆ†å‰²æ•°æ®é›†ã€‚æ•°æ®é›†ä¸­è¿˜åŒ…å«å…³äºæ ‡æ³¨è€…æŠ€èƒ½æ°´å¹³å’Œåˆ†å‰²å·¥å…·çš„å…ƒæ•°æ®ï¼Œæ”¯æŒå¯¹æ ‡æ³¨è€…åå¥½å»ºæ¨¡å’Œå…ƒæ•°æ®åˆ†æçš„ç ”ç©¶ã€‚é€šè¿‡å¯¹æ•°æ®é›†ç‰¹å¾çš„åˆ†æï¼Œæä¾›äº†ç»è¿‡æ•´ç†çš„æ•°æ®åˆ†åŒºå’Œå…±è¯†åˆ†å‰²æ©è†œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02314",
            "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
            "url": "https://huggingface.co/papers/2601.02314",
            "abstract": "Project Ariadne uses structural causal models and counterfactual logic to evaluate the causal integrity of LLM reasoning, revealing a faithfulness gap where reasoning traces are not reliable drivers of outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (Ï†) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (Ï) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
            "score": 0,
            "issue_id": 427,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "e9fe53c5bb19005f",
            "authors": [
                "Sourena Khanzadeh"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.02314.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#agents",
                    "#architecture",
                    "#benchmark",
                    "#reasoning",
                    "#interpretability",
                    "#security"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚ĞµĞ°Ñ‚Ñ€Ğ° Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¹: ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM â€” ÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°",
                    "desc": "Project Ariadne â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Â«Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸Â», Ğ³Ğ´Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° ÑĞºĞ¾Ñ€ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Â«Ğ¢ĞµĞ°Ñ‚Ñ€Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹Â» â€” ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²ÑƒÑ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° â€” Ariadne Score â€” Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Bridging the Faithfulness Gap in LLM Reasoning",
                    "desc": "Project Ariadne investigates how well Large Language Models (LLMs) explain their reasoning when making decisions. It uses Structural Causal Models and counterfactual logic to check if the reasoning provided by these models is genuinely linked to their outputs or just a facade. The study finds a significant 'Faithfulness Gap', where models often reach the same conclusions despite having conflicting internal reasoning. This indicates that the reasoning traces may not accurately reflect the decision-making process, leading to the introduction of the Ariadne Score to better assess the alignment of logic and actions in LLMs."
                },
                "zh": {
                    "title": "æ­ç¤ºæ¨ç†ä¿¡ä»»å·®è·çš„é˜¿é‡Œé˜¿å¾·é¡¹ç›®",
                    "desc": "é¡¹ç›®é˜¿é‡Œé˜¿å¾·ä½¿ç”¨ç»“æ„å› æœæ¨¡å‹å’Œåäº‹å®é€»è¾‘æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å› æœå®Œæ•´æ€§ï¼Œæ­ç¤ºäº†æ¨ç†è½¨è¿¹ä¸è¾“å‡ºä¹‹é—´å­˜åœ¨ä¿¡ä»»å·®è·ã€‚å°½ç®¡é“¾å¼æ€ç»´æç¤ºå¯ä»¥ç”Ÿæˆå¯è¯»çš„æ¨ç†è½¨è¿¹ï¼Œä½†è¿™äº›è½¨è¿¹æ˜¯å¦çœŸå®åæ˜ æ¨¡å‹è¾“å‡ºä»ä¸æ˜ç¡®ã€‚é€šè¿‡å¯¹ä¸­é—´æ¨ç†èŠ‚ç‚¹è¿›è¡Œå¹²é¢„ï¼Œé¡¹ç›®é˜¿é‡Œé˜¿å¾·æµ‹é‡äº†ç»ˆç«¯ç­”æ¡ˆçš„å› æœæ•æ„Ÿæ€§ï¼Œå‘ç°äº†å› æœè§£è€¦çš„æ™®éå¤±è´¥æ¨¡å¼ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„æ™ºèƒ½ä½“æ¶æ„å®¹æ˜“äº§ç”Ÿä¸å¯ä¿¡çš„è§£é‡Šï¼Œå¹¶æå‡ºäº†é˜¿é‡Œé˜¿å¾·è¯„åˆ†ä½œä¸ºå¯¹é½é€»è¾‘ä¸æ¨¡å‹è¡Œä¸ºçš„æ–°åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22877",
            "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
            "url": "https://huggingface.co/papers/2512.22877",
            "abstract": "A multimodal evaluation framework and robustness enhancement module are introduced to address concept erasure vulnerabilities in text-to-image diffusion models across multiple input modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.",
            "score": 0,
            "issue_id": 432,
            "pub_date": "2026-12-28",
            "pub_date_card": {
                "ru": "28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 28",
                "zh": "12æœˆ28æ—¥"
            },
            "hash": "c7c55233d17a7580",
            "authors": [
                "Ju-Hsuan Weng",
                "Jia-Wei Liao",
                "Cheng-Fu Chou",
                "Jun-Cheng Chen"
            ],
            "affiliations": [
                "National Taiwan University",
                "Research Center for Information Technology Innovation, Academia Sinica"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22877.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#diffusion"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ (M-ErasureBench) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ½Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ IRECE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ IRECE ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ‘Ñ€Ñ‚Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ° 40% Ğ² ÑĞ°Ğ¼Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Concept Erasure in Multimodal AI Models",
                    "desc": "This paper introduces a new framework called M-ErasureBench to evaluate and improve the concept erasure capabilities of text-to-image diffusion models. It highlights the issue of concept erasure vulnerabilities not just in text prompts but also in other input types like learned embeddings and inverted latents. The authors propose a robustness enhancement module named IRECE, which helps to better localize and perturb target concepts during the model's denoising process. Their findings show that while existing methods work well for text prompts, they struggle with other modalities, and IRECE significantly improves performance by reducing the Concept Reproduction Rate (CRR) in challenging scenarios."
                },
                "zh": {
                    "title": "æå‡ç”Ÿæˆæ¨¡å‹é²æ£’æ€§çš„å¤šæ¨¡æ€è¯„ä¼°ä¸æŠ¹é™¤æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶å’Œé²æ£’æ€§å¢å¼ºæ¨¡å—ï¼Œä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µæŠ¹é™¤è„†å¼±æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä»æ–‡æœ¬æç¤ºä¸­æŠ¹é™¤æ¦‚å¿µï¼Œè€Œå¿½è§†äº†åœ¨å›¾åƒç¼–è¾‘å’Œä¸ªæ€§åŒ–ç”Ÿæˆç­‰å®é™…åº”ç”¨ä¸­è¶Šæ¥è¶Šé‡è¦çš„å…¶ä»–è¾“å…¥æ¨¡æ€ã€‚æˆ‘ä»¬æå‡ºçš„M-ErasureBenchæ¡†æ¶ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸‰ç§è¾“å…¥æ¨¡æ€ä¸‹çš„æ¦‚å¿µæŠ¹é™¤æ–¹æ³•ï¼Œå¹¶å‘ç°ç°æœ‰æ–¹æ³•åœ¨æ–‡æœ¬æç¤ºä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å­¦ä¹ åµŒå…¥å’Œåè½¬æ½œå˜é‡ä¸‹æ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›è„†å¼±æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†IRECEæ¨¡å—ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å®šä½ç›®æ ‡æ¦‚å¿µï¼Œå¹¶åœ¨å»å™ªè¿‡ç¨‹ä¸­æ‰°åŠ¨ç›¸å…³æ½œå˜é‡ï¼Œä»è€Œå¢å¼ºé²æ£’æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-05.html",
    "link_next": "2026-01-07.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "05.01",
        "en": "01/05",
        "zh": "1æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "07.01",
        "en": "01/07",
        "zh": "1æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 9,
        "#agents": 4,
        "#cv": 4,
        "#rl": 5,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 8,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 3,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 7,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 3,
        "#low_resource": 0
    }
}