{
    "date": {
        "ru": "3 декабря",
        "en": "December 3",
        "zh": "12月3日"
    },
    "time_utc": "2024-12-03 05:11",
    "weekday": 1,
    "issue_id": 911,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.18671",
            "title": "TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video",
            "url": "https://huggingface.co/papers/2411.18671",
            "abstract": "In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its point tracking robustness in long videos. TAPTRv2 is a simple DETR-like framework that can accurately track any point in real-world videos without requiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in querying high quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial and temporal dimensions for more robust tracking in long videos. For better spatial feature querying, we present Context-aware Cross-Attention (CCA), which leverages surrounding spatial context to enhance the quality of attention scores when querying image features. For better temporal feature querying, we introduce Visibility-aware Long-Temporal Attention (VLTA) to conduct temporal attention to all past frames while considering their corresponding visibilities, which effectively addresses the feature drifting problem in TAPTRv2 brought by its RNN-like long-temporal modeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the challenging datasets and obtains state-of-the-art performance. Even when compared with methods trained with large-scale extra internal data, TAPTRv3 is still competitive.",
            "score": 6,
            "issue_id": 909,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "f99e1015e9222dc6",
            "authors": [
                "Jinyuan Qu",
                "Hongyang Li",
                "Shilong Liu",
                "Tianhe Ren",
                "Zhaoyang Zeng",
                "Lei Zhang"
            ],
            "affiliations": [
                "International Digital Economy Academy (IDEA)",
                "South China University of Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18671.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#video",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Контекстное внимание для надежного отслеживания точек в видео",
                    "desc": "TAPTRv3 - это улучшенная версия TAPTRv2 для более надежного отслеживания точек в длинных видео. Система использует пространственный и временной контекст для повышения качества запроса признаков. Введены два новых механизма: Context-aware Cross-Attention (CCA) для улучшения пространственного запроса и Visibility-aware Long-Temporal Attention (VLTA) для временного запроса. TAPTRv3 значительно превосходит предыдущую версию и достигает наилучших результатов на сложных наборах данных."
                },
                "en": {
                    "title": "Enhancing Video Point Tracking with TAPTRv3",
                    "desc": "TAPTRv3 is an advanced framework designed to enhance point tracking in lengthy videos, building on the previous version, TAPTRv2. It improves the ability to query high-quality features by incorporating both spatial and temporal contexts, which helps in maintaining robust tracking despite variations over time. The paper introduces Context-aware Cross-Attention (CCA) for better spatial feature querying and Visibility-aware Long-Temporal Attention (VLTA) for improved temporal feature querying, effectively addressing issues like feature drifting. TAPTRv3 demonstrates significant performance improvements over TAPTRv2 and competes well against other state-of-the-art methods, even those trained on larger datasets."
                },
                "zh": {
                    "title": "TAPTRv3：长视频点跟踪的新突破",
                    "desc": "本文介绍了TAPTRv3，这是在TAPTRv2基础上开发的，旨在提高长视频中的点跟踪鲁棒性。TAPTRv2是一个简单的类似DETR的框架，可以准确跟踪现实视频中的任意点，而无需成本体积。TAPTRv3通过利用空间和时间上下文来改善特征查询，从而在长视频中实现更稳健的跟踪。我们提出了上下文感知交叉注意力（CCA）和可见性感知长时间注意力（VLTA），显著提升了特征查询的质量，超越了TAPTRv2，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00131",
            "title": "Open-Sora Plan: Open-Source Large Video Generation Model",
            "url": "https://huggingface.co/papers/2412.00131",
            "abstract": "We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at https://github.com/PKU-YuanGroup/Open-Sora-Plan.",
            "score": 4,
            "issue_id": 911,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 ноября",
                "en": "November 28",
                "zh": "11月28日"
            },
            "hash": "fa9b0009de797ca2",
            "authors": [
                "Bin Lin",
                "Yunyang Ge",
                "Xinhua Cheng",
                "Zongjian Li",
                "Bin Zhu",
                "Shaodong Wang",
                "Xianyi He",
                "Yang Ye",
                "Shenghai Yuan",
                "Liuhan Chen",
                "Tanghui Jia",
                "Junwu Zhang",
                "Zhenyu Tang",
                "Yatian Pang",
                "Bin She",
                "Cen Yan",
                "Zhiheng Hu",
                "Xiaoyi Dong",
                "Lin Chen",
                "Zhang Pan",
                "Xing Zhou",
                "Shaoling Dong",
                "Yonghong Tian",
                "Li Yuan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.00131.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#inference",
                    "#training",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Открытая модель для генерации высококачественного видео",
                    "desc": "Проект Open-Sora Plan представляет собой открытую модель генерации видео высокого разрешения на основе различных пользовательских входных данных. Модель включает в себя вейвлет-поточный вариационный автоэнкодер, совместный денойзер изображений и видео, а также различные контроллеры условий. Разработаны стратегии для эффективного обучения и вывода, а также предложен многомерный конвейер курирования данных. Проект достиг впечатляющих результатов в генерации видео как в качественных, так и в количественных оценках."
                },
                "en": {
                    "title": "Empowering High-Resolution Video Generation with Open-Sora Plan",
                    "desc": "The Open-Sora Plan is an open-source initiative focused on creating a large-scale generative model for producing high-resolution videos that can last for extended periods, tailored to user specifications. It integrates several advanced components, including a Wavelet-Flow Variational Autoencoder and a Joint Image-Video Skiparse Denoiser, to enhance the video generation process. The project also introduces various condition controllers and efficient training strategies, along with a multi-dimensional data curation pipeline to ensure high-quality output. Overall, the Open-Sora Plan demonstrates significant advancements in video generation, aiming to inspire further research in the field."
                },
                "zh": {
                    "title": "开放源代码，生成高质量视频的未来",
                    "desc": "Open-Sora计划是一个开源项目，旨在基于用户输入生成高分辨率的长时视频。该项目包含多个组件，如小波流变分自编码器和联合图像-视频去噪器，支持整个视频生成过程。我们还设计了多种高效的训练和推理策略，并提出了多维数据策划管道，以获取高质量数据。通过这些高效的设计，Open-Sora计划在视频生成的定性和定量评估中取得了令人印象深刻的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17459",
            "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
            "url": "https://huggingface.co/papers/2411.17459",
            "abstract": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.",
            "score": 2,
            "issue_id": 909,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "9b68162718865b81",
            "authors": [
                "Zongjian Li",
                "Bin Lin",
                "Yang Ye",
                "Liuhan Chen",
                "Xinhua Cheng",
                "Shenghai Yuan",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University",
                "Peng Cheng Laboratory",
                "Rabbitpre Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17459.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#data",
                    "#video",
                    "#architecture",
                    "#open_source",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Эффективное кодирование видео с помощью вейвлетов",
                    "desc": "Статья представляет новый метод кодирования видео под названием Wavelet Flow VAE (WF-VAE). Этот автоэнкодер использует многоуровневое вейвлет-преобразование для эффективного кодирования низкочастотной информации в видео. WF-VAE решает проблему вычислительных ограничений при обработке видео высокого разрешения и большой длительности в латентных видео-диффузионных моделях (LVDM). Метод также включает технику Causal Cache для сохранения целостности латентного пространства при поблочной обработке."
                },
                "en": {
                    "title": "Efficient Video Encoding with Wavelet Flow VAE",
                    "desc": "The paper introduces Wavelet Flow VAE (WF-VAE), a novel approach to encoding videos into a low-dimensional latent space using wavelet transforms. This method addresses the computational bottleneck of traditional Video VAEs, especially when generating high-resolution and long-duration videos. By decomposing videos into frequency-domain components, WF-VAE enhances the efficiency of encoding critical information. Additionally, the proposed Causal Cache method ensures continuity in the latent space during block-wise inference, resulting in improved performance metrics compared to existing video VAEs."
                },
                "zh": {
                    "title": "小波流VAE：高效视频编码的新方法",
                    "desc": "视频变分自编码器（VAE）将视频编码为低维潜在空间，是大多数潜在视频扩散模型（LVDMs）的关键组成部分，能够降低模型训练成本。然而，随着生成视频的分辨率和时长增加，视频VAE的编码成本成为训练LVDMs的瓶颈。此外，大多数LVDMs采用的块状推理方法在处理长时长视频时可能导致潜在空间的不连续性。为了解决计算瓶颈，我们提出了小波流VAE（WF-VAE），通过多级小波变换有效编码视频的关键信息，并引入因果缓存方法以保持潜在空间的完整性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01824",
            "title": "X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models",
            "url": "https://huggingface.co/papers/2412.01824",
            "abstract": "In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.",
            "score": 1,
            "issue_id": 911,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "6b7c2d6555d8df8d",
            "authors": [
                "Zeyi Sun",
                "Ziyang Chu",
                "Pan Zhang",
                "Tong Wu",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuanjun Xiong",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "MThreads AI",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01824.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#cv",
                    "#games",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "X-Prompt: универсальная модель для генерации изображений с контекстным обучением",
                    "desc": "Статья представляет X-Prompt - авторегрессивную мультимодальную модель для генерации изображений. Модель использует контекстное обучение, что позволяет ей решать как знакомые, так и новые задачи генерации изображений. X-Prompt эффективно сжимает признаки из контекстных примеров и поддерживает длинные последовательности токенов контекста. Эксперименты подтверждают способность модели решать разнообразные задачи генерации изображений, включая ранее не встречавшиеся."
                },
                "en": {
                    "title": "X-Prompt: Unlocking Image Generation with In-Context Learning",
                    "desc": "This paper presents X-Prompt, an advanced auto-regressive vision-language model that enhances image generation through in-context learning. By utilizing a few examples as context, X-Prompt can effectively generate images for both familiar and novel tasks. The model is designed to compress important features from these examples, allowing it to manage longer sequences of context and improve generalization. Extensive testing shows that X-Prompt performs well on a variety of image generation tasks, demonstrating its ability to adapt to new challenges."
                },
                "zh": {
                    "title": "X-Prompt：提升图像生成的上下文学习能力",
                    "desc": "本文介绍了一种名为X-Prompt的自回归大规模视觉语言模型，旨在提升图像生成任务的表现。X-Prompt通过利用上下文中的示例，能够在已知和未知的图像生成任务中实现竞争力的性能。该模型采用了专门的设计，能够有效压缩上下文示例中的重要特征，从而支持更长的上下文序列并提高对未知任务的泛化能力。通过统一的训练任务，X-Prompt在文本和图像预测方面表现出色，验证了其在多样化图像生成任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00100",
            "title": "Steering Rectified Flow Models in the Vector Field for Controlled Image Generation",
            "url": "https://huggingface.co/papers/2412.00100",
            "abstract": "Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. Project Page: https://flowchef.github.io.",
            "score": 1,
            "issue_id": 911,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "5fceae277a0e3d85",
            "authors": [
                "Maitreya Patel",
                "Song Wen",
                "Dimitris N. Metaxas",
                "Yezhou Yang"
            ],
            "affiliations": [
                "Arizona State University",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00100.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "FlowChef: Управление векторным полем для эффективной генерации изображений",
                    "desc": "Статья представляет новый подход к управляемой генерации изображений под названием FlowChef. Он основан на использовании динамики векторного поля в ректифицированных потоковых моделях (RFM) для эффективного управления траекторией шумоподавления. FlowChef позволяет решать задачи управляемой генерации изображений, линейных обратных задач и редактирования изображений без дополнительного обучения или инверсии. Результаты показывают, что FlowChef значительно превосходит существующие методы по производительности, требованиям к памяти и времени вычислений."
                },
                "en": {
                    "title": "FlowChef: Revolutionizing Controlled Image Generation with Efficiency",
                    "desc": "This paper introduces FlowChef, a new framework that enhances controlled image generation using rectified flow models (RFMs). Unlike traditional diffusion models, FlowChef efficiently guides the denoising process without requiring additional training or extensive computational resources. It utilizes a unique property of RFMs to navigate the vector field in a deterministic way, allowing for effective classifier guidance and image editing. The results demonstrate that FlowChef outperforms existing methods in performance, memory usage, and processing time, setting new benchmarks in the field."
                },
                "zh": {
                    "title": "FlowChef：高效的受控图像生成新方法",
                    "desc": "扩散模型在生成真实感图像、图像编辑和解决逆问题方面表现出色，但修正流模型在这些任务中仍未得到充分探索。现有的基于扩散模型的方法通常需要额外的训练，缺乏对预训练潜在模型的泛化能力，并且在性能上表现不佳，计算资源消耗大。本文提出FlowChef，通过有效引导去噪轨迹，利用向量场的特性，实现了受控图像生成任务，且无需额外训练或复杂的反向传播。实验结果表明，FlowChef在性能、内存和时间需求上显著优于基线方法，达到了新的最先进结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00947",
            "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information",
            "url": "https://huggingface.co/papers/2412.00947",
            "abstract": "Errors in understanding visual information in images (i.e., visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual perception of LVLMs. In this work, we introduce VisOnlyQA, a new dataset designed to directly evaluate the visual perception capabilities of LVLMs on questions about geometric and numerical information in scientific figures. Our dataset enables us to analyze the visual perception of LVLMs for fine-grained visual information, independent of other capabilities such as reasoning. The evaluation set of VisOnlyQA includes 1,200 multiple-choice questions in 12 tasks on four categories of figures. We also provide synthetic training data consisting of 70k instances. Our experiments on VisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in VisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on synthetic training data demonstrates the potential for enhancing the visual perception of LVLMs, but observed improvements are limited to certain tasks and specific models. (iii) Stronger language models improve the visual perception of LVLMs. In summary, our experiments suggest that both training data and model architectures should be improved to enhance the visual perception capabilities of LVLMs. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.",
            "score": 1,
            "issue_id": 911,
            "pub_date": "2024-12-01",
            "pub_date_card": {
                "ru": "1 декабря",
                "en": "December 1",
                "zh": "12月1日"
            },
            "hash": "7135f8936b0d3ee8",
            "authors": [
                "Ryo Kamoi",
                "Yusen Zhang",
                "Sarkar Snigdha Sarathi Das",
                "Ranran Haoran Zhang",
                "Rui Zhang"
            ],
            "affiliations": [
                "Penn State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00947.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#interpretability",
                    "#cv",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "VisOnlyQA: новый путь к улучшению визуального восприятия AI",
                    "desc": "Исследователи представили новый датасет VisOnlyQA для оценки визуального восприятия больших визуально-языковых моделей (LVLM). Датасет фокусируется на геометрической и числовой информации в научных изображениях, позволяя анализировать способности моделей к тонкому визуальному восприятию. Эксперименты показали, что даже передовые LVLM, такие как GPT-4o и Gemini 1.5 Pro, плохо справляются с задачами VisOnlyQA, в то время как люди демонстрируют почти идеальные результаты. Исследование подчеркивает необходимость улучшения как обучающих данных, так и архитектур моделей для повышения качества визуального восприятия LVLM."
                },
                "en": {
                    "title": "Enhancing Visual Perception in LVLMs with VisOnlyQA",
                    "desc": "This paper addresses the issue of visual perception errors in Large Vision Language Models (LVLMs), which can lead to mistakes when interpreting images. The authors introduce a new dataset called VisOnlyQA, specifically designed to evaluate LVLMs on geometric and numerical questions related to scientific figures. The dataset includes 1,200 multiple-choice questions across various tasks and provides synthetic training data to improve model performance. The findings reveal that while LVLMs struggle with visual perception tasks, fine-tuning with synthetic data can enhance their capabilities, particularly when using stronger language models."
                },
                "zh": {
                    "title": "提升视觉感知，助力大型视觉语言模型",
                    "desc": "本研究介绍了一个新的数据集VisOnlyQA，旨在直接评估大型视觉语言模型（LVLMs）在科学图形中几何和数值信息问题上的视觉感知能力。该数据集包含1200个多项选择题，涵盖四类图形的12个任务，帮助分析LVLMs对细粒度视觉信息的理解。实验结果显示，评估的20个LVLMs在视觉感知任务上的表现较差，而人类的表现几乎完美。通过合成训练数据进行微调可以提升LVLMs的视觉感知能力，但改进效果有限，且更强的语言模型能够提高视觉感知能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01316",
            "title": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation",
            "url": "https://huggingface.co/papers/2412.01316",
            "abstract": "We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/.",
            "score": 1,
            "issue_id": 910,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "5bca597a08ec0a14",
            "authors": [
                "Xin Yan",
                "Yuxuan Cai",
                "Qiuyue Wang",
                "Yuan Zhou",
                "Wenhao Huang",
                "Huan Yang"
            ],
            "affiliations": [
                "01.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01316.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#architecture",
                    "#diffusion",
                    "#long_context"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Presto: Революция в генерации длинных видео с помощью ИИ",
                    "desc": "Presto - это новая модель диффузии видео, разработанная для генерации 15-секундных видеороликов с долгосрочной согласованностью и богатым содержанием. Модель использует стратегию Segmented Cross-Attention (SCA), которая разделяет скрытые состояния на сегменты вдоль временного измерения, позволяя каждому сегменту перекрестно обращаться к соответствующей подписи. Для обучения модели был создан датасет LongTake-HD, содержащий 261 тысячу видео с богатым содержанием и согласованностью сценариев. Эксперименты показывают, что Presto превосходит существующие методы генерации видео по показателям семантической оценки и степени динамичности."
                },
                "en": {
                    "title": "Presto: Revolutionizing Video Generation with Long-Range Coherence",
                    "desc": "Presto is a new video diffusion model that generates 15-second videos while ensuring long-range coherence and rich content. It introduces a Segmented Cross-Attention (SCA) strategy that divides hidden states into segments, allowing each segment to focus on specific sub-captions without needing extra parameters. To support this model, the LongTake-HD dataset was created, containing 261,000 videos with coherent scenarios and detailed captions. Experiments show that Presto outperforms existing methods, achieving high scores in semantic understanding and dynamic content generation."
                },
                "zh": {
                    "title": "Presto：生成长时间一致性视频的新方法",
                    "desc": "我们介绍了一种新的视频扩散模型Presto，旨在生成具有长时间一致性和丰富内容的15秒视频。为了解决在长时间内保持场景多样性的挑战，我们提出了一种分段交叉注意力(SCA)策略，该策略将隐藏状态沿时间维度分段，使每个段能够与相应的子标题进行交叉关注。SCA不需要额外的参数，可以无缝集成到现有的基于DiT的架构中。我们的实验表明，Presto在视频生成方面显著优于现有的最先进方法，提升了内容丰富性和长距离一致性。"
                }
            }
        }
    ],
    "link_prev": "2024-12-02.html",
    "link_next": "2024-12-04.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12月2日"
    },
    "short_date_next": {
        "ru": "04.12",
        "en": "12/04",
        "zh": "12月4日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "近年来，通用多模态大语言模型（MLLMs）取得了快速发展。然而，将这些通用模型应用到特定领域，如科学和工业，仍然较少探索。本文系统地研究了通过后训练进行MLLMs的领域适应，重点是数据合成、训练流水线和任务评估。我们开发了一个视觉指令合成器，能够从特定领域的图片-标题对中生成多样化的视觉指令任务。我们的合成任务在增强MLLMs的领域特定性能方面优于手动规则、GPT-4和GPT-4V生成的任务。我们还使用单阶段训练流水线来增强特定领域的任务多样性。我们在生物医学和食品领域进行了实验，并评估了不同来源和规模的MLLMs在各种特定领域任务上的性能。为支持进一步研究，我们将开源我们的实现。",
        "title": "On Domain-Specific Post-Training for Multimodal Large Language Models",
        "pinyin": "Jìn nián lái, tōngyòng duō mó tài dà yǔyán móxíng (MLLMs) qǔdé le kuàisù fāzhǎn. Rán'ér, jiāng zhèxiē tōngyòng móxíng yìngyòng dào tèdìng lǐngyù, rú kēxué hé gōngyè, réngrán jiàoshǎo tànsuǒ. Běnwén xìtǒng de yánjiū le tōngguò hòu xùnliàn jìnxíng MLLMs de lǐngyù shìyìng, zhòngdiǎn shì shùjù héchéng, xùnliàn liúshuǐxiàn hé rènwù pínggǔ. Wǒmen kāifā le yīgè shìjiào zhǐlìng héchéngqì, nénggòu cóng tèdìng lǐngyù de túpiàn-biāotí duì zhōng shēngchéng duōyànghuà de shìjiào zhǐlìng rènwù. Wǒmen de héchéng rènwù zài zēngqiáng MLLMs de lǐngyù tèdìng xìngnéng fāngmiàn yōu shǒudòng guīzé, GPT-4 hé GPT-4V shēngchéng de rènwù. Wǒmen hái shǐyòng dān jiēduàn xùnliàn liúshuǐxiàn lái zēngqiáng tèdìng lǐngyù de rènwù duōyànghuàxìng. Wǒmen zài shēngwù yīxiào hé shípǐn lǐngyù jìnxíng le shíyàn, bìng pínggǔ le bùtóng láiyuán hé guīmó de MLLMs zài gèzhǒng tèdìng lǐngyù rènwù shàng de xìngnéng. Wèi zhīchí jìnfā yánjiū, wǒmen jiāng kāiyuán wǒmen de shíxiàn.",
        "vocab": "[{'word': '近年来', 'pinyin': 'jìn nián lái', 'trans': 'in recent years'},\n{'word': '通用', 'pinyin': 'tōng yòng', 'trans': 'universal'},\n{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'},\n{'word': '快速', 'pinyin': 'kuài sù', 'trans': 'rapid'},\n{'word': '发展', 'pinyin': 'fā zhǎn', 'trans': 'development'},\n{'word': '然而', 'pinyin': 'rán ér', 'trans': 'however'},\n{'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'apply'},\n{'word': '特定', 'pinyin': 'tè dìng', 'trans': 'specific'},\n{'word': '领域', 'pinyin': 'lǐng yù', 'trans': 'field'},\n{'word': '科学', 'pinyin': 'kē xué', 'trans': 'science'},\n{'word': '工业', 'pinyin': 'gōng yè', 'trans': 'industry'},\n{'word': '探索', 'pinyin': 'tàn suǒ', 'trans': 'explore'},\n{'word': '系统地', 'pinyin': 'xì tǒng de', 'trans': 'systematically'},\n{'word': '研究', 'pinyin': 'yán jiū', 'trans': 'study'},\n{'word': '后训练', 'pinyin': 'hòu xùn liàn', 'trans': 'post-training'},\n{'word': '进行', 'pinyin': 'jìn xíng', 'trans': 'conduct'},\n{'word': '适应', 'pinyin': 'shì yìng', 'trans': 'adaptation'},\n{'word': '重点', 'pinyin': 'zhòng diǎn', 'trans': 'focus'},\n{'word': '数据', 'pinyin': 'shù jù', 'trans': 'data'},\n{'word': '合成', 'pinyin': 'hé chéng', 'trans': 'synthesis'},\n{'word': '流水线', 'pinyin': 'liú shuǐ xiàn', 'trans': 'pipeline'},\n{'word': '任务', 'pinyin': 'rèn wù', 'trans': 'task'},\n{'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluation'},\n{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'develop'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '指令', 'pinyin': 'zhǐ lìng', 'trans': 'instruction'},\n{'word': '合成器', 'pinyin': 'hé chéng qì', 'trans': 'synthesizer'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'},\n{'word': '图片', 'pinyin': 'tú piàn', 'trans': 'image'},\n{'word': '标题', 'pinyin': 'biāo tí', 'trans': 'title'},\n{'word': '对', 'pinyin': 'duì', 'trans': 'pair'},\n{'word': '多样化', 'pinyin': 'duō yàng huà', 'trans': 'diversify'},\n{'word': '增强', 'pinyin': 'zēng qiáng', 'trans': 'enhance'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'},\n{'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'},\n{'word': '手动', 'pinyin': 'shǒu dòng', 'trans': 'manual'},\n{'word': '规则', 'pinyin': 'guī zé', 'trans': 'rule'},\n{'word': '单阶段', 'pinyin': 'dān jiē duàn', 'trans': 'single-stage'},\n{'word': '生物医学', 'pinyin': 'shēng wù yī xué', 'trans': 'biomedical'},\n{'word': '食品', 'pinyin': 'shí pǐn', 'trans': 'food'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '来源', 'pinyin': 'lái yuán', 'trans': 'source'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'},\n{'word': '支持', 'pinyin': 'zhī chí', 'trans': 'support'},\n{'word': '进一步', 'pinyin': 'jìn yī bù', 'trans': 'further'},\n{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open-source'},\n{'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'implementation'}]",
        "trans": "In recent years, general-purpose multimodal large language models (MLLMs) have made rapid progress. However, the application of these general models to specific domains, such as science and industry, remains relatively unexplored. This paper systematically investigates the domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. We developed a visual instruction synthesizer capable of generating diverse visual instruction tasks from domain-specific image-caption pairs. Our synthesized tasks outperform those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. We also employed a single-stage training pipeline to enhance the diversity of domain-specific tasks. We conducted experiments in the biomedical and food domains and evaluated the performance of MLLMs from different sources and scales on various domain-specific tasks. To support further research, we will open-source our implementation.",
        "update_ts": "2024-12-02 09:12"
    }
}