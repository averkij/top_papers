{
    "date": {
        "ru": "13 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 13",
        "zh": "11æœˆ13æ—¥"
    },
    "time_utc": "2024-11-13 20:10",
    "weekday": 2,
    "issue_id": 557,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07184",
            "title": "SAMPart3D: Segment Any Part in 3D Objects",
            "url": "https://huggingface.co/papers/2411.07184",
            "abstract": "3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.",
            "score": 16,
            "issue_id": 541,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "b4e58a99e4a7e86c",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "SAMPart3D: Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SAMPart3D - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±ĞµĞ·Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SAMPart3D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with SAMPart3D",
                    "desc": "This paper presents SAMPart3D, a novel framework for zero-shot 3D part segmentation that does not depend on predefined text prompts. It utilizes text-agnostic vision foundation models to extract 3D features, enabling it to scale effectively to large unlabeled datasets. The framework also incorporates scale-conditioned part-aware features, allowing for segmentation at various levels of detail. SAMPart3D outperforms existing methods and introduces a new benchmark to enhance the diversity and complexity of 3D part segmentation tasks."
                },
                "zh": {
                    "title": "SAMPart3Dï¼šæ— æ–‡æœ¬æç¤ºçš„3Déƒ¨ä»¶åˆ†å‰²æ–°æ¡†æ¶",
                    "desc": "3Déƒ¨ä»¶åˆ†å‰²æ˜¯3Dæ„ŸçŸ¥ä¸­çš„ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºæœºå™¨äººæŠ€æœ¯ã€3Dç”Ÿæˆå’Œ3Dç¼–è¾‘ç­‰é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†SAMPart3Dæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ä¾èµ–é¢„å®šä¹‰æ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹ï¼Œå¯¹ä»»æ„3Då¯¹è±¡è¿›è¡Œå¤šç²’åº¦çš„è¯­ä¹‰éƒ¨ä»¶åˆ†å‰²ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ— æ–‡æœ¬ä¾èµ–çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œä»å¤§è§„æ¨¡æœªæ ‡è®°çš„3Dæ•°æ®é›†ä¸­æå–ä¸°å¯Œçš„3Dç‰¹å¾ï¼Œå¹¶é€šè¿‡æ¡ä»¶åŒ–çš„éƒ¨ä»¶æ„ŸçŸ¥ç‰¹å¾å®ç°çµæ´»çš„åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAMPart3Dåœ¨å¤„ç†å¤æ‚å¯¹è±¡æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²æ–¹æ³•ï¼Œå¹¶èƒ½æ”¯æŒå¤šç§åº”ç”¨ï¼Œå¦‚éƒ¨ä»¶çº§ç¼–è¾‘å’Œäº¤äº’å¼åˆ†å‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07133",
            "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
            "url": "https://huggingface.co/papers/2411.07133",
            "abstract": "Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.",
            "score": 11,
            "issue_id": 546,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "be2fc1cdad8aa9f3",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#synthetic",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLMs) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ, Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Compatibility-Adjusted Reward (CAR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ CAR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²ÑĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Rethinking Instruction Tuning: Size Isn't Everything!",
                    "desc": "This paper investigates the effectiveness of instruction tuning in large language models (LLMs) and challenges the assumption that larger models are better teachers for smaller models. The authors introduce the concept of the Larger Models' Paradox, showing that bigger models do not always enhance the instruction-following capabilities of smaller models. They highlight the limitations of existing metrics in evaluating response generators, which fail to consider the compatibility between the teacher and the base model. To address this, they propose a new metric called Compatibility-Adjusted Reward (CAR), which shows improved performance in assessing the effectiveness of response generators across various models."
                },
                "zh": {
                    "title": "å¤§æ¨¡å‹ä¸ä¸€å®šæ˜¯å¥½æ•™å¸ˆï¼",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æŒ‡ä»¤è°ƒä¼˜åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†æŒ‡ä»¤æ•°æ®é›†å¯¹æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¾ƒå¤§æˆ–è¾ƒå¼ºçš„æ¨¡å‹å¹¶ä¸ä¸€å®šæ˜¯è¾ƒå°æ¨¡å‹çš„æ›´å¥½æ•™å¸ˆï¼Œè¿™ä¸€ç°è±¡è¢«ç§°ä¸ºâ€œå¤§æ¨¡å‹æ‚–è®ºâ€ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡æ— æ³•å‡†ç¡®é¢„æµ‹å“åº”ç”Ÿæˆå™¨çš„æœ‰æ•ˆæ€§ï¼Œå› ä¸ºå®ƒä»¬å¿½ç•¥äº†æ•™å¸ˆæ¨¡å‹ä¸è¢«è°ƒä¼˜åŸºç¡€æ¨¡å‹ä¹‹é—´çš„å…¼å®¹æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”å…¼å®¹æ€§è°ƒæ•´å¥–åŠ±ï¼ˆCARï¼‰ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07975",
            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2411.07975",
            "abstract": "We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.",
            "score": 11,
            "issue_id": 544,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 12",
                "zh": "11æœˆ12æ—¥"
            },
            "hash": "294dc65a01cd1218",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#architecture",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "JanusFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ rectified flow Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ rectified flow Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "JanusFlow: Unifying Image Understanding and Generation Efficiently",
                    "desc": "JanusFlow is a novel framework that combines image understanding and generation into one cohesive model. It utilizes a simple architecture that merges autoregressive language models with rectified flow, enhancing generative modeling capabilities. The study reveals that rectified flow can be effectively trained within the large language model context without needing complex changes to the architecture. By decoupling the encoders for understanding and generation and aligning their representations during training, JanusFlow demonstrates superior performance compared to specialized models and existing unified approaches."
                },
                "zh": {
                    "title": "JanusFlowï¼šå›¾åƒç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "JanusFlowæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œå°†å›¾åƒç†è§£å’Œç”Ÿæˆç»Ÿä¸€åœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ã€‚å®ƒé‡‡ç”¨äº†ç®€çº¦çš„æ¶æ„ï¼Œå°†è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸ä¿®æ­£æµç»“åˆï¼Œè¿™æ˜¯ç”Ÿæˆå»ºæ¨¡ä¸­çš„ä¸€ç§å…ˆè¿›æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œä¿®æ­£æµå¯ä»¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶å†…è½»æ¾è®­ç»ƒï¼Œæ— éœ€å¤æ‚çš„æ¶æ„ä¿®æ”¹ã€‚é€šè¿‡è§£è€¦ç†è§£å’Œç”Ÿæˆç¼–ç å™¨ä»¥åŠåœ¨ç»Ÿä¸€è®­ç»ƒä¸­å¯¹é½å®ƒä»¬çš„è¡¨ç¤ºï¼ŒJanusFlowåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç»Ÿä¸€æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07461",
            "title": "BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions",
            "url": "https://huggingface.co/papers/2411.07461",
            "abstract": "We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that bridges the gap between descriptive synthetic captions and factual web-scale alt-text. KALE augments synthetic dense image captions with web-scale alt-text to generate factually grounded image captions. Our two-stage approach leverages large vision-language models and language models to create knowledge-augmented captions, which are then used to train a specialized VLM for scaling up the dataset. We train vision-language models on KALE and demonstrate improvements on vision-language tasks. Our experiments show the utility of KALE for training more capable and knowledgeable multimodal models. We release the KALE dataset at https://huggingface.co/datasets/Salesforce/blip3-kale",
            "score": 5,
            "issue_id": 552,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 12",
                "zh": "11æœˆ12æ—¥"
            },
            "hash": "08fb959148999629",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#synthetic",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "KALE: ĞĞ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… BLIP3-KALE, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 218 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ğ²ĞµĞ±-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° KALE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Bridging Descriptive and Factual Captions with KALE",
                    "desc": "The paper presents BLIP3-KALE, a new dataset containing 218 million image-text pairs that enhances the quality of image captions by combining synthetic captions with factual web-based alt-text. This dataset is created using a two-stage method that employs large vision-language models and language models to produce captions that are both descriptive and factually accurate. By training specialized vision-language models on the KALE dataset, the authors demonstrate significant improvements in various vision-language tasks. The findings highlight the effectiveness of KALE in developing more advanced multimodal models that can better understand and generate content across different modalities."
                },
                "zh": {
                    "title": "çŸ¥è¯†å¢å¼ºçš„å›¾åƒæ ‡é¢˜ç”Ÿæˆ",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†BLIP3-KALEï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2.18äº¿å¯¹å›¾åƒ-æ–‡æœ¬çš„æ•°æ®é›†ï¼Œæ—¨åœ¨å¼¥åˆæè¿°æ€§åˆæˆæ ‡é¢˜ä¸äº‹å®æ€§ç½‘ç»œè§„æ¨¡æ›¿ä»£æ–‡æœ¬ä¹‹é—´çš„å·®è·ã€‚KALEé€šè¿‡å°†åˆæˆçš„å¯†é›†å›¾åƒæ ‡é¢˜ä¸ç½‘ç»œè§„æ¨¡çš„æ›¿ä»£æ–‡æœ¬ç›¸ç»“åˆï¼Œç”ŸæˆåŸºäºäº‹å®çš„å›¾åƒæ ‡é¢˜ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹åˆ›å»ºçŸ¥è¯†å¢å¼ºçš„æ ‡é¢˜ï¼Œå¹¶ç”¨è¿™äº›æ ‡é¢˜è®­ç»ƒä¸“é—¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»¥æ‰©å¤§æ•°æ®é›†è§„æ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKALEåœ¨è®­ç»ƒæ›´å¼ºå¤§å’Œæ›´å…·çŸ¥è¯†æ€§çš„å¤šæ¨¡æ€æ¨¡å‹æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08017",
            "title": "Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings",
            "url": "https://huggingface.co/papers/2411.08017",
            "abstract": "Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively. To address this, we introduce a novel approach called Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based, compact latent encodings. Specifically, we compress a 256^3 signed distance field into a 12^3 times 4 latent grid, achieving an impressive 2427x compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing the inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at 256^3 resolution. Moreover, WaLa offers rapid inference, producing shapes within two to four seconds depending on the condition, despite the model's scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. We open-source our code and, to the best of our knowledge, release the largest pretrained 3D generative models across different modalities.",
            "score": 4,
            "issue_id": 547,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 12",
                "zh": "11æœˆ12æ—¥"
            },
            "hash": "0af1f4c0dc38cc5b",
            "data": {
                "categories": [
                    "#inference",
                    "#open_source",
                    "#3d",
                    "#dataset",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-ÑĞ¶Ğ°Ñ‚Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Wavelet Latent Diffusion (WaLa). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ 3D-Ñ„Ğ¾Ñ€Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 2427x. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. WaLa Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient 3D Shape Generation with Wavelet Latent Diffusion",
                    "desc": "This paper presents a new method called Wavelet Latent Diffusion (WaLa) for generating high-quality 3D shapes efficiently. It addresses the limitations of existing 3D generative models by using wavelet-based latent encodings, which compress 3D shapes significantly while preserving detail. The method achieves a remarkable 2427x compression ratio, allowing for the training of large-scale generative networks without increasing inference time. WaLa demonstrates state-of-the-art performance in generating diverse and high-quality 3D shapes, and the authors have made their code and pretrained models available to the public."
                },
                "zh": {
                    "title": "é«˜æ•ˆå‹ç¼©ï¼Œå¿«é€Ÿç”Ÿæˆ3Då½¢çŠ¶çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºWavelet Latent Diffusionï¼ˆWaLaï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡3Dç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡ã€‚é€šè¿‡å°†3Då½¢çŠ¶ç¼–ç ä¸ºåŸºäºå°æ³¢çš„ç´§å‡‘æ½œåœ¨ç¼–ç ï¼ŒWaLaå®ç°äº†é«˜è¾¾2427å€çš„å‹ç¼©æ¯”ï¼ŒåŒæ—¶ä¿æŒäº†ç»†èŠ‚çš„å®Œæ•´æ€§ã€‚è¯¥æ–¹æ³•ä½¿å¾—è®­ç»ƒå¤§å‹ç”Ÿæˆç½‘ç»œå˜å¾—æ›´åŠ é«˜æ•ˆï¼Œå¹¶ä¸”åœ¨æ¨ç†æ—¶ä¸ä¼šæ˜¾è‘—å¢åŠ æ—¶é—´ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆé«˜è´¨é‡çš„3Då½¢çŠ¶ï¼Œå¹¶ä¸”å¼€æºäº†ä»£ç ï¼Œæ¨åŠ¨äº†3Dç”Ÿæˆæ¨¡å‹çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08034",
            "title": "Scaling Properties of Diffusion Models for Perceptual Tasks",
            "url": "https://huggingface.co/papers/2411.08034",
            "abstract": "In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and segmentation under image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perception tasks. Through a careful analysis of these scaling behaviors, we present various techniques to efficiently train diffusion models for visual perception tasks. Our models achieve improved or comparable performance to state-of-the-art methods using significantly less data and compute. To use our code and models, see https://scaling-diffusion-perception.github.io .",
            "score": 3,
            "issue_id": 557,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 12",
                "zh": "11æœˆ12æ—¥"
            },
            "hash": "4be3b9af89108627",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Scaling Diffusion Models for Enhanced Visual Perception",
                    "desc": "This paper explores how diffusion models can be effectively used for both generating images and performing visual perception tasks like depth estimation and segmentation. It presents a unified approach that treats these tasks as image-to-image translation problems, highlighting the advantages of scaling in training and testing. The authors analyze the scaling behaviors of diffusion models and propose techniques to enhance their efficiency in visual perception tasks. As a result, their models demonstrate competitive performance compared to leading methods while requiring less data and computational resources."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹ï¼šè§†è§‰æ„ŸçŸ¥çš„æ–°åŠ›é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå’Œè§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚æˆ‘ä»¬å°†æ·±åº¦ä¼°è®¡ã€å…‰æµå’Œåˆ†å‰²ç­‰ä»»åŠ¡ç»Ÿä¸€ä¸ºå›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ï¼Œå¹¶å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨è¿™äº›æ„ŸçŸ¥ä»»åŠ¡ä¸­å¦‚ä½•é€šè¿‡æ‰©å±•è®­ç»ƒå’Œæµ‹è¯•è®¡ç®—æ¥è·ç›Šã€‚é€šè¿‡å¯¹è¿™äº›æ‰©å±•è¡Œä¸ºçš„ä»”ç»†åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç§é«˜æ•ˆè®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä½¿ç”¨æ˜¾è‘—æ›´å°‘çš„æ•°æ®å’Œè®¡ç®—çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05197",
            "title": "Hardware and Software Platform Inference",
            "url": "https://huggingface.co/papers/2411.05197",
            "abstract": "It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. In this paper we introduce \\textbf{hardware and software platform inference (HSPI)} -- a method for identifying the underlying  architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. Our method leverages the inherent differences of various  architectures and compilers to distinguish between different  types and software stacks. By analyzing the numerical patterns in the model's outputs, we propose a classification framework capable of accurately identifying the  used for model inference as well as the underlying software configuration. Our findings demonstrate the feasibility of inferring  type from black-box models. We evaluate HSPI against models served on different real hardware and find that in a white-box setting we can distinguish between different s with between 83.9% and 100% accuracy. Even in a black-box setting we are able to achieve results that are up to three times higher than random guess accuracy.",
            "score": 3,
            "issue_id": 550,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "7685c8e74f6dbc6b",
            "data": {
                "categories": [
                    "#leakage",
                    "#security",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ¾Ğ±Ğ»Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ°: ĞºĞ°Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ HSPI (Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ÑÑ‚ĞµĞº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµĞµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğµ-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ° Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ 100% Ğ² Ğ±ĞµĞ»Ğ¾Ğ¼ ÑÑ‰Ğ¸ĞºĞµ Ğ¸ Ğ´Ğ¾ Ñ‚Ñ€ĞµÑ… Ñ€Ğ°Ğ· Ğ²Ñ‹ÑˆĞµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‡ĞµÑ€Ğ½Ğ¾Ğ¼ ÑÑ‰Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Verify Your Model: Uncovering the Truth Behind LLM Inference",
                    "desc": "This paper addresses the challenge of verifying the authenticity of large language model (LLM) services purchased by businesses. It introduces a novel method called hardware and software platform inference (HSPI), which analyzes the input-output behavior of machine learning models to identify their underlying hardware and software configurations. By examining the numerical patterns in the outputs, HSPI can distinguish between different architectures and software stacks, even in a black-box scenario. The results show that HSPI can achieve high accuracy in identifying the model's hardware and software, significantly improving upon random guessing."
                },
                "zh": {
                    "title": "éªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹çš„çœŸå®æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºç¡¬ä»¶å’Œè½¯ä»¶å¹³å°æ¨ç†ï¼ˆHSPIï¼‰çš„æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«æœºå™¨å­¦ä¹ æ¨¡å‹çš„åº•å±‚æ¶æ„å’Œè½¯ä»¶å †æ ˆã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†ææ¨¡å‹çš„è¾“å…¥è¾“å‡ºè¡Œä¸ºï¼Œåˆ©ç”¨ä¸åŒæ¶æ„å’Œç¼–è¯‘å™¨çš„å›ºæœ‰å·®å¼‚æ¥åŒºåˆ†ä¸åŒç±»å‹çš„ç¡¬ä»¶å’Œè½¯ä»¶é…ç½®ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç™½ç›’ç¯å¢ƒä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥83.9%åˆ°100%çš„å‡†ç¡®ç‡åŒºåˆ†ä¸åŒçš„ç¡¬ä»¶ï¼Œè€Œåœ¨é»‘ç›’ç¯å¢ƒä¸‹ï¼Œå‡†ç¡®ç‡ä¹Ÿèƒ½è¾¾åˆ°éšæœºçŒœæµ‹çš„ä¸‰å€ä»¥ä¸Šã€‚æ­¤æ–¹æ³•ä¸ºéªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹çš„çœŸå®æ€§æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ‰‹æ®µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06307",
            "title": "Acoustic Volume Rendering for Neural Impulse Response Fields",
            "url": "https://huggingface.co/papers/2411.06307",
            "abstract": "Realistic audio synthesis that captures accurate acoustic phenomena is essential for creating immersive experiences in virtual and augmented reality. Synthesizing the sound received at any position relies on the estimation of impulse response (IR), which characterizes how sound propagates in one scene along different paths before arriving at the listener's position. In this paper, we present Acoustic Volume Rendering (AVR), a novel approach that adapts volume rendering techniques to model acoustic impulse responses. While volume rendering has been successful in modeling radiance fields for images and neural scene representations, IRs present unique challenges as time-series signals. To address these challenges, we introduce frequency-domain volume rendering and use spherical integration to fit the IR measurements. Our method constructs an impulse response field that inherently encodes wave propagation principles and achieves state-of-the-art performance in synthesizing impulse responses for novel poses. Experiments show that AVR surpasses current leading methods by a substantial margin. Additionally, we develop an acoustic simulation platform, AcoustiX, which provides more accurate and realistic IR simulations than existing simulators. Code for AVR and AcoustiX are available at https://zitonglan.github.io/avr.",
            "score": 2,
            "issue_id": 554,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "f6935a265562d416",
            "data": {
                "categories": [
                    "#audio"
                ],
                "emoji": "ğŸ”Š",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Acoustic Volume Rendering (AVR). AVR Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AVR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Sound: Acoustic Volume Rendering for Immersive Audio Experiences",
                    "desc": "This paper introduces Acoustic Volume Rendering (AVR), a new method for synthesizing realistic audio in virtual and augmented reality. AVR uses volume rendering techniques to model acoustic impulse responses (IRs), which describe how sound travels in a scene. The authors tackle the unique challenges of IRs as time-series signals by employing frequency-domain volume rendering and spherical integration. Their approach not only improves the accuracy of sound synthesis but also outperforms existing methods, demonstrating significant advancements in acoustic simulation with their platform, AcoustiX."
                },
                "zh": {
                    "title": "å£°å­¦ä½“ç§¯æ¸²æŸ“ï¼šæå‡è™šæ‹Ÿç°å®ä¸­çš„éŸ³é¢‘ä½“éªŒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å£°å­¦ä½“ç§¯æ¸²æŸ“ï¼ˆAVRï¼‰æ–¹æ³•ï¼Œç”¨äºåˆæˆçœŸå®çš„å£°å­¦è„‰å†²å“åº”ï¼ˆIRï¼‰ï¼Œä»¥å¢å¼ºè™šæ‹Ÿå’Œå¢å¼ºç°å®ä¸­çš„æ²‰æµ¸ä½“éªŒã€‚AVRé€šè¿‡é€‚åº”ä½“ç§¯æ¸²æŸ“æŠ€æœ¯ï¼Œè§£å†³äº†å£°æ³¢ä¼ æ’­çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å°†æ—¶é—´åºåˆ—ä¿¡å·å»ºæ¨¡ä¸ºè„‰å†²å“åº”åœºã€‚æˆ‘ä»¬å¼•å…¥äº†é¢‘åŸŸä½“ç§¯æ¸²æŸ“å’Œçƒé¢ç§¯åˆ†æŠ€æœ¯ï¼Œä»¥æ›´å‡†ç¡®åœ°æ‹ŸåˆIRæµ‹é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAVRåœ¨åˆæˆæ–°å§¿æ€çš„è„‰å†²å“åº”æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„é¢†å…ˆæ–¹æ³•ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå£°å­¦æ¨¡æ‹Ÿå¹³å°AcoustiXï¼Œæä¾›æ›´å‡†ç¡®çš„IRæ¨¡æ‹Ÿã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-12.html",
    "link_next": "2024-11-14.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "14.11",
        "en": "11/14",
        "zh": "11æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†3Déƒ¨ä»¶åˆ†å‰²çš„é‡è¦æ€§åŠå…¶åœ¨æœºå™¨äººã€3Dç”Ÿæˆå’Œ3Dç¼–è¾‘ä¸­çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œ2Dåˆ°3Dçš„çŸ¥è¯†è’¸é¦ï¼Œå®ç°é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²ï¼Œä½†ä¾èµ–æ–‡æœ¬æç¤ºï¼Œé™åˆ¶äº†æ‰©å±•æ€§å’Œå¤„ç†éƒ¨ä»¶æ­§ä¹‰çš„çµæ´»æ€§ã€‚æ–‡ç« ä»‹ç»äº†SAMPart3Dï¼Œä¸€ä¸ªå¯æ‰©å±•çš„é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²æ¡†æ¶ï¼Œä¸éœ€è¦é¢„å®šä¹‰çš„éƒ¨ä»¶æ ‡ç­¾é›†ã€‚å®ƒä½¿ç”¨ä¸æ–‡æœ¬æ— å…³çš„è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œè’¸é¦ï¼Œå¹¶æå–å¤šå°ºåº¦çš„éƒ¨ä»¶æ„ŸçŸ¥3Dç‰¹å¾ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSAMPart3Dåœ¨å¤§è§„æ¨¡3Dæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚",
        "title": "SAMPart3D: Segment Any Part in 3D Objects",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†3Déƒ¨ä»¶åˆ†å‰²çš„é‡è¦æ€§åŠå…¶åœ¨æœºå™¨äººã€3Dç”Ÿæˆå’Œ3Dç¼–è¾‘ä¸­çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œ2Dåˆ°3Dçš„çŸ¥è¯†è’¸é¦ï¼Œå®ç°é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²ï¼Œä½†ä¾èµ–æ–‡æœ¬æç¤ºï¼Œé™åˆ¶äº†æ‰©å±•æ€§å’Œå¤„ç†éƒ¨ä»¶æ­§ä¹‰çš„çµæ´»æ€§ã€‚æ–‡ç« ä»‹ç»äº†SAMPart3Dï¼Œä¸€ä¸ªå¯æ‰©å±•çš„é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²æ¡†æ¶ï¼Œä¸éœ€è¦é¢„å®šä¹‰çš„éƒ¨ä»¶æ ‡ç­¾é›†ã€‚å®ƒä½¿ç”¨ä¸æ–‡æœ¬æ— å…³çš„è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œè’¸é¦ï¼Œå¹¶æå–å¤šå°ºåº¦çš„éƒ¨ä»¶æ„ŸçŸ¥3Dç‰¹å¾ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSAMPart3Dåœ¨å¤§è§„æ¨¡3Dæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚\n\nPinyin transcription:\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le 3D bÃ¹jiÃ n fÄ“ngÃ© de zhÃ²ngyÃ oxÃ¬ng jÃ­ qÃ­ zÃ i jÄ«qÃ¬rÃ©n, 3D shÄ“ngchÃ©ng hÃ© 3D biÄnjÃ­ zhÅng de yÃ¬ngyÃ²ng. XiÃ nyÇ’u fÄngfÇ lÃ¬yÃ²ng shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng jÃ¬nxÃ­ng 2D dÃ o 3D de zhÄ«shi zhÄ“ngliÃº, shÃ­xiÃ n lÃ­ng yÃ ngbÄ›n 3D bÃ¹jiÃ n fÄ“ngÃ©, dÃ n yÄ«lÃ i wÃ©nbÄ›n tÃ­shÃ¬, xiÃ nzhÃ¬ le kuÃ²zhÇnxÃ¬ng hÃ© chÇ”lÇ bÃ¹jiÃ n jÃ­yÃ¬ de lÃ­nghuÃ³xÃ¬ng. WÃ©nzhÄng jiÃ¨shÃ o le SAMPart3D, yÄ«gÃ¨ kÄ› kuÃ²zhÇn de lÃ­ng yÃ ngbÄ›n 3D bÃ¹jiÃ n fÄ“ngÃ© kuÃ ngjiÃ , bÃ¹ xÅ«yÃ o yÃ¹dÃ¬ngyÃ¬ de bÃ¹jiÃ n biÇoqiÃ¡n jÃ­. TÄ shÇyÃ²ng yÇ” wÃ©nbÄ›n wÃºguÄn de shÃ¬juÃ© jÄ«chÇ” mÃ³xÃ­ng jÃ¬nxÃ­ng zhÄ“ngliÃº, bÃ¬ng tÄ«quÇn duÅ chÇdÃ¹ de bÃ¹jiÃ n gÇnjuÃ© 3D tÃ¨zhÄ“ng. ShÃ­yÃ n xiÇnshÃ¬, SAMPart3D zÃ i dÃ  guÄ«mÃ³ 3D shÃ¹jÃ¹jÃ­ shÃ ng biÇoxiÃ n yÅuyuÃ¨, bÃ¬ng chÄoyuÃ© xiÃ nyÇ’u fÄngfÇ.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"3Déƒ¨ä»¶åˆ†å‰²\", \"pinyin\": \"3D bÃ¹ jiÃ n fÄ“n gÄ“\", \"trans\": \"3D part segmentation\"},\n    {\"word\": \"é‡è¦æ€§\", \"pinyin\": \"zhÃ²ng yÃ o xÃ¬ng\", \"trans\": \"importance\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"ç°æœ‰æ–¹æ³•\", \"pinyin\": \"xiÃ n yÇ’u fÄng fÇ\", \"trans\": \"existing methods\"},\n    {\"word\": \"è§†è§‰è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"çŸ¥è¯†è’¸é¦\", \"pinyin\": \"zhÄ« shi zhÄ“ng liÃº\", \"trans\": \"knowledge distillation\"},\n    {\"word\": \"é›¶æ ·æœ¬\", \"pinyin\": \"lÃ­ng yÃ ng bÄ›n\", \"trans\": \"zero-shot\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ« lÃ i\", \"trans\": \"rely on\"},\n    {\"word\": \"æ–‡æœ¬æç¤º\", \"pinyin\": \"wÃ©n bÄ›n tÃ­ shÃ¬\", \"trans\": \"textual prompts\"},\n    {\"word\": \"é™åˆ¶\", \"pinyin\": \"xiÃ n zhÃ¬\", \"trans\": \"limit\"},\n    {\"word\": \"æ‰©å±•æ€§\", \"pinyin\": \"kuÃ² zhÇn xÃ¬ng\", \"trans\": \"scalability\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"handle\"},\n    {\"word\": \"éƒ¨ä»¶æ­§ä¹‰\", \"pinyin\": \"bÃ¹ jiÃ n qÃ­ yÃ¬\", \"trans\": \"part ambiguity\"},\n    {\"word\": \"çµæ´»æ€§\", \"pinyin\": \"lÃ­ng huÃ³ xÃ¬ng\", \"trans\": \"flexibility\"},\n    {\"word\": \"SAMPart3D\", \"pinyin\": \"SAMPart3D\", \"trans\": \"SAMPart3D\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"é¢„å®šä¹‰\", \"pinyin\": \"yÃ¹ dÃ¬ng yÃ¬\", \"trans\": \"predefined\"},\n    {\"word\": \"æ ‡ç­¾é›†\", \"pinyin\": \"biÄo qiÄn jÃ­\", \"trans\": \"label set\"},\n    {\"word\": \"è§†è§‰åŸºç¡€æ¨¡å‹\", \"pinyin\": \"shÃ¬ juÃ© jÄ« chÇ” mÃ³ xÃ­ng\", \"trans\": \"vision foundation model\"},\n    {\"word\": \"å¤šå°ºåº¦\", \"pinyin\": \"duÅ chÇ dÃ¹\", \"trans\": \"multi-scale\"},\n    {\"word\": \"éƒ¨ä»¶æ„ŸçŸ¥\", \"pinyin\": \"bÃ¹ jiÃ n gÇn zhÄ«\", \"trans\": \"part-aware\"},\n    {\"word\": \"3Dç‰¹å¾\", \"pinyin\": \"3D tÃ¨ zhÄ“ng\", \"trans\": \"3D features\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"å¤§è§„æ¨¡\", \"pinyin\": \"dÃ  guÄ« mÃ³\", \"trans\": \"large-scale\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"ä¼˜å¼‚\", \"pinyin\": \"yÅu yÃ¬\", \"trans\": \"excellent\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"}\n]",
        "trans": "This article discusses the importance of 3D part segmentation and its applications in robotics, 3D generation, and 3D editing. Existing methods utilize vision-language models for knowledge distillation from 2D to 3D, achieving zero-shot 3D part segmentation, but they rely on textual prompts, which limits their scalability and flexibility in handling part ambiguities. The article introduces SAMPart3D, a scalable zero-shot 3D part segmentation framework that does not require predefined part labels. It uses a text-agnostic visual foundation model for distillation and extracts multi-scale part-aware 3D features. Experiments show that SAMPart3D performs excellently on large-scale 3D datasets and outperforms existing methods.",
        "update_ts": "2024-11-13 09:10"
    }
}