{
    "date": {
        "ru": "18 июля",
        "en": "July 18",
        "zh": "7月18日"
    },
    "time_utc": "2025-07-18 10:13",
    "weekday": 4,
    "issue_id": 4891,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.13334",
            "title": "A Survey of Context Engineering for Large Language Models",
            "url": "https://huggingface.co/papers/2507.13334",
            "abstract": "Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.",
            "score": 65,
            "issue_id": 4883,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "e0191e89e0360224",
            "authors": [
                "Lingrui Mei",
                "Jiayu Yao",
                "Yuyao Ge",
                "Yiwei Wang",
                "Baolong Bi",
                "Yujun Cai",
                "Jiazhi Liu",
                "Mingyu Li",
                "Zhong-Zhi Li",
                "Duzhen Zhang",
                "Chenlin Zhou",
                "Jiayi Mao",
                "Tianze Xia",
                "Jiafeng Guo",
                "Shenghua Liu"
            ],
            "affiliations": [
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "Peking University",
                "The University of Queensland",
                "Tsinghua University",
                "University of California, Merced",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13334.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#survey",
                    "#multimodal",
                    "#long_context",
                    "#architecture",
                    "#rag"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Контекстная инженерия: оптимизация входных данных для раскрытия потенциала LLM",
                    "desc": "Эта статья представляет собой обзор области контекстной инженерии для больших языковых моделей (LLM). Авторы систематизируют компоненты и методы оптимизации контекстной информации, подаваемой в LLM во время вывода. Рассматриваются такие темы как извлечение и генерация контекста, обработка контекста, системы памяти и мультиагентные системы. Выявлен существенный разрыв между способностью моделей понимать сложный контекст и генерировать столь же сложные выходные данные."
                },
                "en": {
                    "title": "Optimizing Context for Superior AI Outputs",
                    "desc": "This paper introduces Context Engineering, a new approach to optimize the information provided to Large Language Models (LLMs) for better performance. It breaks down the process into key components like context retrieval, processing, and management, and shows how these can be combined in advanced systems like retrieval-augmented generation and multi-agent systems. The authors analyze over 1300 research papers to highlight a significant gap: while LLMs can understand complex contexts well, they struggle to generate sophisticated long-form content. The paper aims to provide a roadmap for future research to enhance the capabilities of LLMs in generating high-quality outputs."
                },
                "zh": {
                    "title": "优化上下文，提升语言模型能力",
                    "desc": "本文介绍了上下文工程（Context Engineering），这是一个系统优化大型语言模型（LLMs）信息负载的正式学科。研究表明，LLMs的性能主要取决于推理过程中提供的上下文信息。我们对上下文工程进行了全面的分类，分析了其基础组件及其在智能系统中的复杂实现。通过对1300多篇研究论文的系统分析，本文揭示了当前模型在生成复杂长文本输出方面的显著局限性，并强调了未来研究的优先方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.13348",
            "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2507.13348",
            "abstract": "VisionThink dynamically adjusts image resolution and visual token processing for efficient and effective vision-language tasks, improving performance on OCR tasks while reducing token usage in simpler tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.",
            "score": 38,
            "issue_id": 4884,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "1f1a89020e55859c",
            "authors": [
                "Senqiao Yang",
                "Junyi Li",
                "Xin Lai",
                "Bei Yu",
                "Hengshuang Zhao",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKU",
                "HKUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13348.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#training",
                    "#rl",
                    "#games"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Умное зрение: эффективность через адаптивное разрешение",
                    "desc": "VisionThink - это новый подход к обработке визуальных токенов в задачах компьютерного зрения и обработки естественного языка. Он динамически регулирует разрешение изображений и обработку визуальных токенов для повышения эффективности. Метод улучшает производительность на задачах оптического распознавания символов (OCR), одновременно уменьшая использование токенов в более простых задачах. VisionThink использует обучение с подкреплением и стратегию 'LLM-as-Judge' для успешного применения в задачах визуального вопросно-ответного анализа (VQA)."
                },
                "en": {
                    "title": "Dynamic Resolution for Efficient Vision-Language Processing",
                    "desc": "VisionThink is a novel approach that optimizes image resolution and visual token processing for vision-language tasks. It intelligently adjusts the resolution of images based on the complexity of the task, allowing for efficient processing by reducing unnecessary visual tokens in simpler tasks. The model uses reinforcement learning to determine when to request higher-resolution images, enhancing performance on OCR tasks while maintaining accuracy in general visual question answering (VQA) tasks. This dynamic token compression strategy leads to improved efficiency and effectiveness in handling various vision-language challenges."
                },
                "zh": {
                    "title": "动态调整，提升视觉语言任务效率",
                    "desc": "VisionThink 是一种动态调整图像分辨率和视觉标记处理的方法，旨在提高视觉语言任务的效率和效果。该方法在光学字符识别（OCR）任务中表现出色，同时在简单任务中减少了视觉标记的使用。通过智能判断图像分辨率是否足够，VisionThink 可以在需要时请求更高分辨率的图像。与传统的固定压缩方法不同，VisionThink 根据具体情况自主决定是否压缩标记，从而在保持性能的同时节省计算资源。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.13347",
            "title": "π^3: Scalable Permutation-Equivariant Visual Geometry Learning",
            "url": "https://huggingface.co/papers/2507.13347",
            "abstract": "A permutation-equivariant neural network, $\\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce pi^3, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, pi^3 employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available.",
            "score": 30,
            "issue_id": 4883,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "006f7b52edb3a67a",
            "authors": [
                "Yifan Wang",
                "Jianjun Zhou",
                "Haoyi Zhu",
                "Wenzheng Chang",
                "Yang Zhou",
                "Zizun Li",
                "Junyi Chen",
                "Jiangmiao Pang",
                "Chunhua Shen",
                "Tong He"
            ],
            "affiliations": [
                "SII",
                "Shanghai AI Lab",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13347.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#cv",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Революция в реконструкции 3D сцен без опорных кадров",
                    "desc": "Статья представляет новую нейронную сеть π^3 для реконструкции визуальной геометрии без фиксированного опорного вида. В отличие от предыдущих методов, π^3 использует полностью перестановочно-эквивариантную архитектуру для предсказания аффинно-инвариантных поз камеры и масштабно-инвариантных локальных карт точек. Такой подход делает модель устойчивой к порядку входных данных и хорошо масштабируемой. π^3 достигает лучших результатов в оценке положения камеры, глубины и реконструкции плотной карты точек."
                },
                "en": {
                    "title": "Revolutionizing Visual Geometry with Permutation-Equivariance",
                    "desc": "The paper presents pi^3, a novel permutation-equivariant neural network designed for visual geometry reconstruction without relying on a fixed reference view. Traditional methods often depend on a specific viewpoint, which can introduce biases and lead to inaccuracies. In contrast, pi^3 utilizes a fully permutation-equivariant architecture to predict camera poses and point maps that are invariant to scale and reference frames. This innovative approach enhances robustness and scalability, allowing pi^3 to achieve state-of-the-art results in tasks like camera pose estimation and depth estimation."
                },
                "zh": {
                    "title": "无参考视角的视觉几何重建新方法",
                    "desc": "本文介绍了一种名为pi^3的神经网络，它在视觉几何重建中不依赖于固定的参考视角。传统方法通常将重建锚定在特定的视点，这种偏置可能导致不稳定和失败。与此不同，pi^3采用完全的置换等变架构，能够在没有参考框架的情况下预测仿射不变的相机姿态和尺度不变的局部点图。该模型的设计使其对输入顺序具有内在的鲁棒性，并且具有很高的可扩展性，从而在相机姿态估计、单目/视频深度估计和密集点图重建等任务中实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.13332",
            "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner",
            "url": "https://huggingface.co/papers/2507.13332",
            "abstract": "TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.",
            "score": 30,
            "issue_id": 4883,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "d03adfbd7cec2623",
            "authors": [
                "Zhouqi Hua",
                "Wenwei Zhang",
                "Chengqi Lyu",
                "Yuzhe Gu",
                "Songyang Gao",
                "Kuikun Liu",
                "Kai Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13332.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#synthetic",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Имитация машины Тьюринга для улучшения обобщающей способности языковых моделей",
                    "desc": "Метод TAIL имитирует процессы выполнения машины Тьюринга для улучшения способности больших языковых моделей (LLM) к обобщению на более длинные последовательности. Он синтезирует данные цепочки рассуждений, которые имитируют процесс выполнения машины Тьюринга, линейно расширяя шаги рассуждений до атомарных состояний. TAIL значительно улучшает способность к обобщению по длине и производительность модели Qwen2.5-7B на различных задачах, используя только синтетические данные. Эксперименты показывают, что ключевые концепции машины Тьюринга необходимы для TAIL для обобщения по длине."
                },
                "en": {
                    "title": "Enhancing LLMs with Turing Machine Imitation for Better Generalization",
                    "desc": "The paper introduces TAIL, a novel method that enhances the performance of large language models (LLMs) by mimicking the execution processes of Turing Machines. It addresses the challenge of length generalization, enabling LLMs to solve longer sequences than those seen during training. TAIL synthesizes chain-of-thought data to improve reasoning capabilities and reduce shortcut learning, which often leads to poor generalization. The method demonstrates significant improvements in performance across various tasks using synthetic data, highlighting the importance of Turing Machine concepts in LLM reasoning."
                },
                "zh": {
                    "title": "提升LLM长度泛化能力的图灵机模仿学习",
                    "desc": "本文提出了一种名为TAIL的方法，旨在提高大型语言模型（LLM）的长度泛化能力。TAIL通过模拟图灵机的执行过程，合成链式思维数据，从而减少了快捷学习现象。该方法通过线性扩展推理步骤，改善了动态和长距离数据访问的难度。实验结果表明，TAIL在多个任务上显著提升了模型的性能，展示了图灵机的关键概念在长度泛化中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12841",
            "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning",
            "url": "https://huggingface.co/papers/2507.12841",
            "abstract": "The AnyCap Project introduces a framework, dataset, and evaluation protocol to enhance controllability and reliability in multimodal captioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores by 45\\% and style scores by 12\\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench.",
            "score": 27,
            "issue_id": 4884,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "02e4f51787ec491d",
            "authors": [
                "Yiming Ren",
                "Zhiqiang Lin",
                "Yu Li",
                "Gao Meng",
                "Weiyun Wang",
                "Junjie Wang",
                "Zicheng Lin",
                "Jifeng Dai",
                "Yujiu Yang",
                "Wenhai Wang",
                "Ruihang Chu"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12841.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#benchmark",
                    "#games",
                    "#dataset"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "Универсальный контроль над мультимодальными подписями",
                    "desc": "Проект AnyCap представляет комплексное решение для улучшения контролируемости и надежности мультимодального создания подписей. Он включает в себя модель AnyCapModel (ACM), которая повышает контролируемость существующих фундаментальных моделей без их переобучения. Также создан набор данных AnyCapDataset (ACD), охватывающий три модальности и 28 типов пользовательских инструкций. Предложен новый бенчмарк AnyCapEval для более надежной оценки контролируемого создания подписей."
                },
                "en": {
                    "title": "Enhancing Multimodal Captioning Control and Reliability",
                    "desc": "The AnyCap Project presents a comprehensive framework aimed at improving controllability and reliability in multimodal captioning tasks. It introduces the AnyCapModel (ACM), which enhances existing models' capabilities without the need for retraining, allowing for better alignment with user instructions and modality features. To support this, the AnyCapDataset (ACD) is created, featuring a diverse set of 300,000 high-quality entries across three modalities and 28 types of user instructions. Additionally, the AnyCapEval benchmark offers improved evaluation metrics that separate content accuracy from stylistic fidelity, demonstrating significant performance improvements in caption quality across various models."
                },
                "zh": {
                    "title": "提升多模态字幕生成的可控性与可靠性",
                    "desc": "AnyCap项目提出了一个框架、数据集和评估协议，以增强多模态字幕生成的可控性和可靠性。该项目的核心是AnyCapModel（ACM），它是一个轻量级的插件框架，可以在不重新训练基础模型的情况下，提高现有模型的可控性。为了应对可控多模态字幕生成中的数据稀缺问题，我们构建了AnyCapDataset（ACD），涵盖三种模态、28种用户指令类型和30万个高质量数据条目。此外，我们还提出了AnyCapEval，一个新的基准，提供更可靠的评估指标，通过解耦内容准确性和风格保真度来评估可控字幕生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.13344",
            "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models",
            "url": "https://huggingface.co/papers/2507.13344",
            "abstract": "A sliding iterative denoising process is proposed to enhance spatio-temporal consistency in 4D diffusion models for high-fidelity view synthesis from sparse-view videos.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/ .",
            "score": 18,
            "issue_id": 4889,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "02785244ea887bec",
            "authors": [
                "Yudong Jin",
                "Sida Peng",
                "Xuan Wang",
                "Tao Xie",
                "Zhen Xu",
                "Yifan Yang",
                "Yujun Shen",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "affiliations": [
                "Ant Research",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13344.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшение 4D-согласованности в синтезе видов с помощью скользящего шумоподавления",
                    "desc": "В статье предлагается новый метод улучшения пространственно-временной согласованности в 4D-диффузионных моделях для высококачественного синтеза видов из видео с редкими ракурсами. Авторы вводят скользящий итеративный процесс шумоподавления в скрытой сетке, кодирующей изображение, позу камеры и человека. Этот подход позволяет диффузионной модели получить большое рецептивное поле и улучшить 4D-согласованность выходных данных при доступном потреблении памяти GPU. Эксперименты показывают, что метод значительно превосходит существующие подходы в синтезе качественных и согласованных видео с новых ракурсов."
                },
                "en": {
                    "title": "Enhancing Video Synthesis with Sliding Iterative Denoising",
                    "desc": "This paper presents a new method to improve the quality of video synthesis from sparse-view inputs using 4D diffusion models. The authors introduce a sliding iterative denoising process that enhances spatio-temporal consistency, which is crucial for generating realistic videos. By organizing the data into a latent grid that captures image, camera, and human poses, the method allows for effective denoising across both spatial and temporal dimensions. Experiments show that this approach significantly outperforms previous techniques, producing high-fidelity and consistent videos."
                },
                "zh": {
                    "title": "增强4D一致性，合成高保真视图",
                    "desc": "本文提出了一种滑动迭代去噪过程，以增强4D扩散模型在稀疏视角视频中的时空一致性，从而实现高保真度的视图合成。以往的方法通过利用4D扩散模型生成新视角的视频，但生成的视频往往缺乏时空一致性，影响了合成质量。我们的方法通过定义一个潜在网格，交替沿空间和时间维度进行去噪，最终从去噪后的潜在表示中解码出目标视角的视频。实验结果表明，我们的方法在DNA-Rendering和ActorsHQ数据集上能够合成高质量且一致的新视角视频，显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12508",
            "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
            "url": "https://huggingface.co/papers/2507.12508",
            "abstract": "MindJourney enhances vision-language models with 3D reasoning by coupling them with a video diffusion-based world model, achieving improved performance on spatial reasoning tasks without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.",
            "score": 10,
            "issue_id": 4883,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 июля",
                "en": "July 16",
                "zh": "7月16日"
            },
            "hash": "1a89f50f8edd267e",
            "authors": [
                "Yuncong Yang",
                "Jiageng Liu",
                "Zheyuan Zhang",
                "Siyuan Zhou",
                "Reuben Tan",
                "Jianwei Yang",
                "Yilun Du",
                "Chuang Gan"
            ],
            "affiliations": [
                "HKUST",
                "Harvard",
                "JHU",
                "Microsoft Research",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12508.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#benchmark",
                    "#diffusion",
                    "#rl",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MindJourney: 3D-рассуждения для моделей машинного зрения без дообучения",
                    "desc": "MindJourney - это фреймворк, который улучшает способности моделей машинного зрения и обработки естественного языка (VLM) к пространственному рассуждению в 3D-пространстве. Он сочетает VLM с управляемой моделью мира, основанной на видеодиффузии. Этот подход позволяет VLM создавать траекторию камеры, а модель мира генерирует соответствующие виды на каждом шаге. MindJourney достигает улучшения производительности на 8% в задачах пространственного рассуждения без дополнительного обучения."
                },
                "en": {
                    "title": "MindJourney: Enhancing 3D Reasoning in Vision-Language Models",
                    "desc": "MindJourney is a novel framework that enhances vision-language models (VLMs) by integrating them with a video diffusion-based world model, enabling better spatial reasoning in 3D environments. This approach allows VLMs to generate a camera trajectory and synthesize views dynamically, facilitating improved understanding of 3D dynamics without the need for fine-tuning. By leveraging multi-view evidence during interactive exploration, MindJourney achieves an average performance boost of over 8% on spatial reasoning tasks. This method demonstrates the effectiveness of coupling VLMs with world models for robust 3D reasoning, offering a straightforward solution for enhancing model capabilities at test time."
                },
                "zh": {
                    "title": "MindJourney：提升视觉-语言模型的3D推理能力",
                    "desc": "MindJourney 是一种增强视觉-语言模型的框架，通过与基于视频扩散的世界模型结合，实现了3D推理能力的提升。该方法在不进行微调的情况下，显著提高了空间推理任务的表现，尤其是在SAT基准测试中平均提升了8%。传统的视觉-语言模型在处理3D动态时常常表现不佳，而MindJourney通过迭代生成相机轨迹并合成多视图证据，帮助模型更好地理解空间关系。此方法展示了将世界模型与视觉-语言模型结合的潜力，为3D推理提供了一种简单有效的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.13300",
            "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research",
            "url": "https://huggingface.co/papers/2507.13300",
            "abstract": "AbGen evaluates LLMs in designing ablation studies for scientific research, revealing performance gaps compared to human experts and highlighting the unreliability of current automated evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.",
            "score": 9,
            "issue_id": 4884,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "4836fc7ccb11ed5e",
            "authors": [
                "Yilun Zhao",
                "Weiyuan Chen",
                "Zhijian Xu",
                "Manasi Patwardhan",
                "Yixin Liu",
                "Chengye Wang",
                "Lovekesh Vig",
                "Arman Cohan"
            ],
            "affiliations": [
                "TCS Research",
                "Yale NLP Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13300.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#science",
                    "#interpretability"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "AbGen: Выявление пробелов в способностях ИИ проектировать научные эксперименты",
                    "desc": "AbGen - это первый бенчмарк для оценки способностей языковых моделей в разработке аблационных исследований для научных работ. Он состоит из 1500 примеров, аннотированных экспертами, на основе 807 статей по обработке естественного языка. Оценка ведущих языковых моделей показала значительный разрыв в производительности между ними и экспертами-людьми. Исследование также выявило ненадежность существующих автоматизированных методов оценки для этой задачи."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating LLMs in Scientific Ablation Studies",
                    "desc": "AbGen is a benchmark created to assess how well large language models (LLMs) can design ablation studies in scientific research. It includes 1,500 examples from 807 NLP papers, where LLMs must generate detailed study designs based on specific research contexts. The evaluation shows that LLMs like DeepSeek-R1-0528 and o4-mini fall short compared to human experts in key areas such as importance and soundness of the designs. Additionally, the study reveals that current automated evaluation methods are unreliable, prompting the development of AbGen-Eval to better assess LLM performance in this context."
                },
                "zh": {
                    "title": "评估LLMs在科学研究中的消融研究设计能力",
                    "desc": "AbGen是一个新基准，用于评估大型语言模型（LLMs）在设计科学研究的消融研究中的能力。该基准包含1500个专家注释的示例，来源于807篇自然语言处理（NLP）论文。我们的评估显示，领先的LLMs在消融研究设计的重要性、真实性和合理性方面，与人类专家存在显著的性能差距。此外，我们还发现当前的自动评估方法在这一任务中并不可靠，存在与人类评估结果的显著差异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12956",
            "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers",
            "url": "https://huggingface.co/papers/2507.12956",
            "abstract": "FantasyPortrait, a diffusion transformer framework, generates high-fidelity and emotion-rich facial animations for single and multi-character scenarios using implicit representations and a masked cross-attention mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is https://fantasy-amap.github.io/fantasy-portrait/.",
            "score": 9,
            "issue_id": 4887,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "4e0dcacd5b147ff7",
            "authors": [
                "Qiang Wang",
                "Mengchao Wang",
                "Fan Jiang",
                "Yaqi Fan",
                "Yonggang Qi",
                "Mu Xu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Beijing University of Posts and Telecommunications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12956.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#diffusion",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Оживление портретов: от статики к эмоциональной динамике",
                    "desc": "FantasyPortrait - это фреймворк на основе диффузионного трансформера для генерации высококачественных и эмоционально богатых анимаций лиц. Он использует неявные представления и механизм маскированного кросс-внимания для создания анимаций как одного, так и нескольких персонажей. Метод вводит стратегию обучения с усилением выражений для захвата независимой от личности динамики лица. Для управления несколькими персонажами разработан механизм маскированного кросс-внимания, обеспечивающий независимую, но скоординированную генерацию выражений."
                },
                "en": {
                    "title": "Transforming Static Images into Emotion-Rich Animations with FantasyPortrait",
                    "desc": "FantasyPortrait is a novel framework that utilizes diffusion transformers to create high-quality facial animations from static images, focusing on both single and multi-character scenarios. It overcomes limitations of previous methods that relied on explicit geometric features, which often led to artifacts and failed to capture subtle emotional expressions. The framework employs an expression-augmented learning strategy with implicit representations to enhance the rendering of fine-grained emotions. Additionally, a masked cross-attention mechanism allows for independent expression generation in multi-character settings, minimizing interference between different characters' features."
                },
                "zh": {
                    "title": "高保真情感面部动画生成的创新框架",
                    "desc": "FantasyPortrait是一种基于扩散变换器的框架，能够为单个和多个角色场景生成高保真且富有情感的面部动画。该方法通过隐式表示和掩蔽交叉注意机制，克服了传统方法在面部重现中的伪影问题，并能够捕捉细腻的情感变化。为了支持多角色动画，FantasyPortrait设计了一个掩蔽交叉注意机制，确保独立而协调的表情生成，避免特征干扰。我们的实验表明，FantasyPortrait在定量指标和定性评估中显著优于现有的最先进方法，尤其在复杂的跨重现和多角色场景中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12990",
            "title": "Teach Old SAEs New Domain Tricks with Boosting",
            "url": "https://huggingface.co/papers/2507.12990",
            "abstract": "A residual learning approach enhances Sparse Autoencoders to capture domain-specific features without retraining, improving interpretability and performance on specialized domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.",
            "score": 4,
            "issue_id": 4889,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "15daa9824ba8ff47",
            "authors": [
                "Nikita Koriagin",
                "Yaroslav Aksenov",
                "Daniil Laptev",
                "Gleb Gerasimov",
                "Nikita Balagansky",
                "Daniil Gavrilov"
            ],
            "affiliations": [
                "HSE University",
                "Moscow Institute of Physics and Technology",
                "T-Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12990.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#data",
                    "#interpretability",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Повышение интерпретируемости языковых моделей с помощью остаточного обучения автоэнкодеров",
                    "desc": "Статья представляет новый подход к улучшению разреженных автоэнкодеров (Sparse Autoencoders) для интерпретации внутренних представлений больших языковых моделей (LLM). Предложенный метод использует остаточное обучение (residual learning) для захвата специфических особенностей предметной области без полного переобучения модели. Вторичный автоэнкодер обучается моделировать ошибку реконструкции предварительно обученного автоэнкодера на текстах определенной области. Эксперименты показывают значительное улучшение метрик кросс-энтропии и объясненной дисперсии для LLM в специализированных доменах."
                },
                "en": {
                    "title": "Enhancing Sparse Autoencoders with Residual Learning for Domain-Specific Insights",
                    "desc": "This paper presents a novel method that enhances Sparse Autoencoders (SAEs) using a residual learning approach to better capture domain-specific features. By training a secondary SAE to model the reconstruction error of a pretrained SAE, the method allows for the integration of new domain knowledge without the need for complete retraining. The results show significant improvements in performance metrics, such as cross-entropy and explained variance, across various specialized domains. This approach not only improves the interpretability of SAEs but also maintains their effectiveness on general tasks, paving the way for targeted mechanistic interpretability in large language models."
                },
                "zh": {
                    "title": "残差学习提升稀疏自编码器的领域特征捕捉能力",
                    "desc": "本文提出了一种残差学习方法，增强了稀疏自编码器（SAE），使其能够捕捉特定领域的特征，而无需重新训练。通过训练一个次级SAE来建模预训练SAE在特定领域文本上的重构误差，有效地捕捉到主模型遗漏的特征。我们在推理过程中将两个模型的输出相加，显著提高了多个专业领域的LLM交叉熵和解释方差指标。该方法能够高效地将新的领域知识融入现有的SAE，同时保持其在一般任务上的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12720",
            "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models",
            "url": "https://huggingface.co/papers/2507.12720",
            "abstract": "FLEXITOKENS, a byte-level language model with a learnable tokenizer, reduces token over-fragmentation and improves performance across multilingual and morphologically diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10\\% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens",
            "score": 4,
            "issue_id": 4883,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "195b799c7dd66533",
            "authors": [
                "Abraham Toluase Owodunni",
                "Orevaoghene Ahia",
                "Sachin Kumar"
            ],
            "affiliations": [
                "The Ohio State University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12720.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#optimization",
                    "#multilingual"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Гибкая токенизация для улучшения языковых моделей",
                    "desc": "FLEXITOKENS - это байтовая языковая модель с обучаемым токенизатором, которая снижает чрезмерную фрагментацию токенов и улучшает производительность в многоязычных задачах и задачах с разнообразной морфологией. Модель включает подмодуль, который учится предсказывать границы между байтовыми последовательностями, кодируя их в сегменты переменной длины. В отличие от существующих методов без токенизаторов, FLEXITOKENS использует упрощенную целевую функцию обучения, обеспечивающую большую гибкость при адаптации. Эксперименты показывают, что FLEXITOKENS последовательно снижает чрезмерную фрагментацию токенов и достигает до 10% улучшения производительности в целевых задачах по сравнению с подсловными и другими токенизаторами на основе градиентов."
                },
                "en": {
                    "title": "FLEXITOKENS: Adaptive Tokenization for Enhanced Language Model Performance",
                    "desc": "FLEXITOKENS is a novel byte-level language model that introduces a learnable tokenizer to enhance adaptability in tokenization. Traditional subword tokenizers are rigid and often lead to over-fragmentation, especially in multilingual and morphologically diverse contexts. By allowing the model to learn token boundaries dynamically, FLEXITOKENS reduces inefficiencies and improves performance on various tasks. The results show up to a 10% increase in performance compared to existing tokenization methods, demonstrating its effectiveness in handling diverse data distributions."
                },
                "zh": {
                    "title": "FLEXITOKENS：灵活的字节级语言模型",
                    "desc": "FLEXITOKENS是一种字节级语言模型，具有可学习的分词器，旨在减少分词过度碎片化的问题。传统的子词分词器在适应新数据时往往不够灵活，导致在处理不同语言或脚本时效率低下。通过引入可学习的分词器，FLEXITOKENS能够根据输入字节序列自适应地预测分界，从而生成可变长度的分段。实验结果表明，FLEXITOKENS在多语言基准测试和形态多样性任务中表现优异，性能提升可达10%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04984",
            "title": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame\n  Interpolation",
            "url": "https://huggingface.co/papers/2507.04984",
            "abstract": "Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) improves video frame interpolation by efficiently extracting temporal information, reducing parameters, and requiring less training data compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Frame Interpolation (VFI) aims to predict the intermediate frame I_n (we use n to denote time in videos to avoid notation overload with the timestep t in diffusion models) based on two consecutive neighboring frames I_0 and I_1. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: https://zonglinl.github.io/tlbvfi_page.",
            "score": 4,
            "issue_id": 4884,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "b8c67ea4defb3288",
            "authors": [
                "Zonglin Lyu",
                "Chen Chen"
            ],
            "affiliations": [
                "Center for Research in Computer Vision, University of Central Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04984.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#data",
                    "#diffusion"
                ],
                "emoji": "🎞️",
                "ru": {
                    "title": "Эффективная интерполяция видеокадров с помощью временно-осведомленной диффузионной модели",
                    "desc": "Статья представляет новый метод интерполяции видеокадров под названием TLB-VFI, основанный на диффузионных моделях. Данный подход эффективно извлекает временную информацию из видео с помощью 3D-вейвлет гейтинга и автоэнкодера с учетом времени. TLB-VFI превосходит существующие методы, улучшая показатель FID на 20% на сложных датасетах, при этом используя в 3 раза меньше параметров. Метод также требует в 9000 раз меньше данных для обучения по сравнению с видео-ориентированными диффузионными моделями."
                },
                "en": {
                    "title": "Efficient Video Frame Interpolation with Temporal Awareness",
                    "desc": "Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) enhances the process of predicting intermediate video frames by effectively utilizing temporal information. It introduces a novel 3D-wavelet gating and a temporal-aware autoencoder to improve efficiency and reduce the number of parameters needed for training. Compared to existing methods, TLB-VFI achieves a significant performance boost while requiring much less training data and computational resources. This approach not only accelerates the inference process but also maintains high-quality output, making it a promising advancement in video frame interpolation."
                },
                "zh": {
                    "title": "高效视频帧插值的新方法",
                    "desc": "本文提出了一种名为时间感知潜在布朗桥扩散（TLB-VFI）的视频帧插值方法。该方法通过提取丰富的时间信息，显著提高了视频帧插值的效率。与现有方法相比，TLB-VFI在参数数量上减少了三倍，并且训练数据需求降低了9000倍。实验结果表明，该方法在最具挑战性的数据集上相较于最新的图像扩散模型，FID指标提高了20%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11589",
            "title": "Einstein Fields: A Neural Perspective To Computational General\n  Relativity",
            "url": "https://huggingface.co/papers/2507.11589",
            "abstract": "Einstein Fields, a neural tensor field representation, compresses four-dimensional numerical relativity simulations into neural network weights, enabling automatic differentiation and natural emergence of dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Einstein Fields, a neural representation that is designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the metric, which is the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. However, unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields are Neural Tensor Fields with the key difference that when encoding the spacetime geometry of general relativity into neural field representations, dynamics emerge naturally as a byproduct. Einstein Fields show remarkable potential, including continuum modeling of 4D spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. We address these challenges across several canonical test beds of general relativity and release an open source JAX-based library, paving the way for more scalable and expressive approaches to numerical relativity. Code is made available at https://github.com/AndreiB137/EinFields",
            "score": 0,
            "issue_id": 4891,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 июля",
                "en": "July 15",
                "zh": "7月15日"
            },
            "hash": "3d0f8c09fa171915",
            "authors": [
                "Sandeep Suresh Cranganore",
                "Andrei Bodnar",
                "Arturs Berzins",
                "Johannes Brandstetter"
            ],
            "affiliations": [
                "Emmi AI GmbH, Linz, Austria",
                "LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria",
                "University of Manchester, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11589.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🌌",
                "ru": {
                    "title": "Einstein Fields: Нейронное сжатие пространства-времени",
                    "desc": "Статья представляет Einstein Fields - нейронное представление для сжатия четырехмерных симуляций численной теории относительности в компактные веса нейронной сети. Эта технология позволяет моделировать метрику пространства-времени и выводить физические величины с помощью автоматического дифференцирования. В отличие от обычных нейронных полей, в Einstein Fields динамика возникает естественным образом. Метод демонстрирует многообещающий потенциал для численной теории относительности, включая непрерывное моделирование 4D пространства-времени и эффективное хранение данных."
                },
                "en": {
                    "title": "Revolutionizing Numerical Relativity with Einstein Fields",
                    "desc": "Einstein Fields is a novel neural tensor field representation that compresses complex four-dimensional numerical relativity simulations into compact neural network weights. This approach allows for automatic differentiation, enabling the extraction of physical quantities directly from the model. Unlike traditional neural fields, Einstein Fields naturally incorporate dynamics when encoding the spacetime geometry of general relativity. The framework demonstrates advantages such as efficient storage, high accuracy in derivatives, and versatility across different mesh types, making it a significant advancement in the field of numerical relativity."
                },
                "zh": {
                    "title": "爱因斯坦场：压缩四维数值相对论的神经网络新方法",
                    "desc": "爱因斯坦场是一种神经张量场表示，旨在将计算密集型的四维数值相对论模拟压缩为紧凑的隐式神经网络权重。这种方法通过建模度量，能够利用自动微分推导物理量。与传统的神经场不同，爱因斯坦场在编码广义相对论的时空几何时，自然地产生动态效果。该方法展示了在四维时空建模、存储效率和易用性等方面的显著潜力，并提供了一个开源的JAX库，促进了数值相对论的可扩展性和表现力。"
                }
            }
        }
    ],
    "link_prev": "2025-07-17.html",
    "link_next": "2025-07-21.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "17.07",
        "en": "07/17",
        "zh": "7月17日"
    },
    "short_date_next": {
        "ru": "21.07",
        "en": "07/21",
        "zh": "7月21日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 4,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 5,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    }
}