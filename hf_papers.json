{
    "date": {
        "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 3",
        "zh": "2æœˆ3æ—¥"
    },
    "time_utc": "2025-02-03 04:12",
    "weekday": 0,
    "issue_id": 1995,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.19393",
            "title": "s1: Simple test-time scaling",
            "url": "https://huggingface.co/papers/2501.19393",
            "abstract": "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending \"Wait\" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.",
            "score": 5,
            "issue_id": 1994,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "8fcf84a9effc288f",
            "authors": [
                "Niklas Muennighoff",
                "Zitong Yang",
                "Weijia Shi",
                "Xiang Lisa Li",
                "Li Fei-Fei",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Percy Liang",
                "Emmanuel CandÃ¨s",
                "Tatsunori Hashimoto"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Contextual AI",
                "Stanford University",
                "University of Washington, Seattle"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19393.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ s1, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Qwen2.5-32B-Instruct, Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… s1K Ğ¸Ğ· 1000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ s1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ o1-preview Ğ¾Ñ‚ OpenAI Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Language Models with Test-Time Scaling",
                    "desc": "This paper introduces a method called test-time scaling for enhancing language model performance during evaluation. The authors create a dataset of 1,000 questions with reasoning traces to train their model, focusing on difficulty, diversity, and quality. They implement a technique called budget forcing, which manipulates the model's response time to encourage deeper reasoning and correct errors. After fine-tuning their model, they demonstrate significant improvements in solving math competition questions compared to previous models, showcasing the effectiveness of their approach."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶é—´æ‰©å±•ï¼šæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•â€”â€”æµ‹è¯•æ—¶é—´æ‰©å±•ï¼Œæ—¨åœ¨é€šè¿‡å¢åŠ æµ‹è¯•æ—¶é—´è®¡ç®—æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶è€…ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«1000ä¸ªé—®é¢˜åŠå…¶æ¨ç†è¿‡ç¨‹çš„å°æ•°æ®é›†s1Kï¼Œå¹¶é€šè¿‡éš¾åº¦ã€å¤šæ ·æ€§å’Œè´¨é‡ä¸‰ä¸ªæ ‡å‡†è¿›è¡ŒéªŒè¯ã€‚ä¸ºäº†æ§åˆ¶æµ‹è¯•æ—¶é—´è®¡ç®—ï¼Œæå‡ºäº†é¢„ç®—å¼ºåˆ¶çš„æ–¹æ³•ï¼Œé€šè¿‡å¼ºåˆ¶ç»ˆæ­¢æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹æˆ–åœ¨æ¨¡å‹ç”Ÿæˆæ—¶å¤šæ¬¡æ·»åŠ â€œç­‰å¾…â€æ¥å»¶é•¿æ€è€ƒæ—¶é—´ï¼Œä»è€Œä¿ƒä½¿æ¨¡å‹æ£€æŸ¥ç­”æ¡ˆã€‚ç»è¿‡ç›‘ç£å¾®è°ƒåï¼Œæ¨¡å‹s1åœ¨æ•°å­¦ç«èµ›é—®é¢˜ä¸Šè¶…è¶Šäº†OpenAIçš„o1æ¨¡å‹ï¼Œè¡¨ç°æå‡è¾¾27%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.19324",
            "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
            "url": "https://huggingface.co/papers/2501.19324",
            "abstract": "",
            "score": 1,
            "issue_id": 1995,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "ce2d414eedfb7a1e",
            "authors": [
                "Baohao Liao",
                "Yuhui Xu",
                "Hanze Dong",
                "Junnan Li",
                "Christof Monz",
                "Silvio Savarese",
                "Doyen Sahoo",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Language Technology Lab, University of Amsterdam",
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19324.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ AI Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Hybrid Networks: Bridging Spatial and Temporal Learning",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ•°æ®å¤„ç†ï¼Œæå‡æœºå™¨å­¦ä¹ æ€§èƒ½",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾é€‰æ‹©æ¥å¢å¼ºå­¦ä¹ æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œæ”¹è¿›çš„æ•°æ®å¤„ç†æµç¨‹å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18841",
            "title": "Trading Inference-Time Compute for Adversarial Robustness",
            "url": "https://huggingface.co/papers/2501.18841",
            "abstract": "We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.",
            "score": 0,
            "issue_id": 1994,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "f1e75e6b24f3e044",
            "authors": [
                "Wojciech Zaremba",
                "Evgenia Nitishinskaya",
                "Boaz Barak",
                "Stephanie Lin",
                "Sam Toyer",
                "Yaodong Yu",
                "Rachel Dias",
                "Eric Wallace",
                "Kai Xiao",
                "Johannes Heidecke",
                "Amelia Glaese"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.18841.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ - Ğ²Ñ‹ÑˆĞµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ°: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼. Ğ’ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ´Ğ¾Ğ»Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº ÑÑ‚Ñ€ĞµĞ¼Ğ¸Ñ‚ÑÑ Ğº Ğ½ÑƒĞ»Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ¾ÑÑ‚Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Boosting Robustness: More Compute, Less Vulnerability",
                    "desc": "This paper investigates how increasing the amount of compute used during inference can enhance the robustness of reasoning models, specifically OpenAI's o1-preview and o1-mini, against adversarial attacks. The authors find that as the compute increases, the success rate of these attacks generally decreases, indicating improved model resilience. Notably, this improvement occurs without any adversarial training, simply by allowing the models to utilize more resources for reasoning tasks. The study also examines new types of attacks and scenarios where increased compute does not lead to better reliability, providing insights into potential solutions."
                },
                "zh": {
                    "title": "å¢åŠ æ¨ç†è®¡ç®—ï¼Œæå‡æ¨¡å‹é²æ£’æ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æ¨ç†æ¨¡å‹ä¸­å¢åŠ æ¨ç†æ—¶é—´è®¡ç®—å¯¹å…¶æŠµå¾¡å¯¹æŠ—æ”»å‡»çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œéšç€æ¨ç†æ—¶é—´è®¡ç®—çš„å¢åŠ ï¼Œæ¨¡å‹çš„é²æ£’æ€§å¾—åˆ°äº†æå‡ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ”»å‡»æˆåŠŸçš„æ¨¡å‹æ ·æœ¬æ¯”ä¾‹éšç€æµ‹è¯•æ—¶é—´è®¡ç®—çš„å¢åŠ è€Œè¶‹è¿‘äºé›¶ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨ç†æ—¶é—´è®¡ç®—æœ‰æ½œåŠ›æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-31.html",
    "link_next": "2025-02-04.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "31.01",
        "en": "01/31",
        "zh": "1æœˆ31æ—¥"
    },
    "short_date_next": {
        "ru": "04.02",
        "en": "02/04",
        "zh": "2æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¦‚ä½•ç¡®ä¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å®‰å…¨æ€§ã€‚ä½œè€…æå‡ºäº†GuardReasonerï¼Œä¸€ç§æ–°çš„ä¿æŠ¤æœºåˆ¶ï¼Œé€šè¿‡å¼•å¯¼ä¿æŠ¤æ¨¡å‹å­¦ä¼šæ¨ç†æ¥æé«˜æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«127Kæ ·æœ¬å’Œ460Kè¯¦ç»†æ¨ç†æ­¥éª¤çš„æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†æ¨ç†SFTå’Œéš¾æ ·æœ¬DPOæ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGuardReasoneråœ¨13ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚",
        "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¦‚ä½•ç¡®ä¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å®‰å…¨æ€§ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le rÃºhÃ© quÃ¨bÇo dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i ÄnquÃ¡n guÇnjiÃ n yÃ¬ngyÃ²ng zhÅng de ÄnquÃ¡nxÃ¬ng.\n\nä½œè€…æå‡ºäº†GuardReasonerï¼Œä¸€ç§æ–°çš„ä¿æŠ¤æœºåˆ¶ï¼Œé€šè¿‡å¼•å¯¼ä¿æŠ¤æ¨¡å‹å­¦ä¼šæ¨ç†æ¥æé«˜æ€§èƒ½ã€‚\nZuÃ²zhÄ› tÃ­chÅ« le GuardReasoner, yÄ«zhÇ’ng xÄ«n de bÇohÃ¹ jÄ«zhÃ¬, tÅngguÃ² yÇndÇo bÇohÃ¹ mÃ³xÃ­ng xuÃ©huÃ¬ tuÄ«lÇ lÃ¡i tÃ­gÄo xÃ¬ngnÃ©ng.\n\nç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«127Kæ ·æœ¬å’Œ460Kè¯¦ç»†æ¨ç†æ­¥éª¤çš„æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†æ¨ç†SFTå’Œéš¾æ ·æœ¬DPOæ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚\nYÃ¡njiÅ« tuÃ¡nduÃ¬ chuÃ ngjiÃ n le yÄ«gÃ¨ bÄohÃ¡n 127K yÃ ngbÄ›n hÃ© 460K xiÃ¡ngxÃ¬ tuÄ«lÇ bÃ¹zhÃ²u de shÃ¹jÃ¹jÃ­, bÃ¬ng yÇnrÃ¹ le tuÄ«lÇ SFT hÃ© nÃ¡n yÃ ngbÄ›n DPO lÃ¡i zÄ“ngqiÃ¡ng tuÄ«lÇ nÃ©nglÃ¬.\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼ŒGuardReasoneråœ¨13ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚\nShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, GuardReasoner zÃ i 13 gÃ¨ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng biÇoxiÃ n yÅuyÃ¬, chÄoyuÃ¨ le qÃ­tÄ mÃ³xÃ­ng.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"ç¡®ä¿\", \"pinyin\": \"quÃ¨ bÇo\", \"trans\": \"ensure\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"å®‰å…¨\", \"pinyin\": \"Än quÃ¡n\", \"trans\": \"security\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇn jiÃ n\", \"trans\": \"critical\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"GuardReasoner\", \"pinyin\": \"N/A\", \"trans\": \"GuardReasoner\"},\n    {\"word\": \"ä¿æŠ¤\", \"pinyin\": \"bÇo hÃ¹\", \"trans\": \"protect\"},\n    {\"word\": \"æœºåˆ¶\", \"pinyin\": \"jÄ« zhÃ¬\", \"trans\": \"mechanism\"},\n    {\"word\": \"å¼•å¯¼\", \"pinyin\": \"yÇn dÇo\", \"trans\": \"guide\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reason\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"å›¢é˜Ÿ\", \"pinyin\": \"tuÃ¡n duÃ¬\", \"trans\": \"team\"},\n    {\"word\": \"åˆ›å»º\", \"pinyin\": \"chuÃ ng jiÃ n\", \"trans\": \"create\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"æ ·æœ¬\", \"pinyin\": \"yÃ ng bÄ›n\", \"trans\": \"sample\"},\n    {\"word\": \"è¯¦ç»†\", \"pinyin\": \"xiÃ¡ng xÃ¬\", \"trans\": \"detailed\"},\n    {\"word\": \"æ­¥éª¤\", \"pinyin\": \"bÃ¹ zhÃ²u\", \"trans\": \"step\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ng qiÃ¡ng\", \"trans\": \"enhance\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇn shÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"æµ‹è¯•\", \"pinyin\": \"cÃ¨ shÃ¬\", \"trans\": \"test\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"ä¼˜å¼‚\", \"pinyin\": \"yÅu yÃ¬\", \"trans\": \"excellent\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"}\n]",
        "trans": "This article discusses how to ensure the safety of large language models (LLMs) in security-critical applications. The authors propose GuardReasoner, a new protective mechanism that enhances performance by guiding the protective model to learn reasoning. The research team created a dataset containing 127K samples and 460K detailed reasoning steps and introduced reasoning SFT and hard sample DPO to enhance reasoning capabilities. Experimental results show that GuardReasoner performs excellently on 13 benchmark tests, outperforming other models.",
        "update_ts": "2025-02-02 12:36"
    }
}