{
    "date": {
        "ru": "25 ноября",
        "en": "November 25",
        "zh": "11月25日"
    },
    "time_utc": "2024-11-25 03:26",
    "weekday": 0,
    "issue_id": 752,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.14793",
            "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
            "url": "https://huggingface.co/papers/2411.14793",
            "abstract": "Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new \"style templates\", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation.",
            "score": 10,
            "issue_id": 752,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "03859b57f29683ab",
            "authors": [
                "Jooyoung Choi",
                "Chaehun Shin",
                "Yeongtak Oh",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14793.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#multimodal",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшение стилизации изображений с помощью оптимизации шума в диффузионных моделях",
                    "desc": "Статья представляет новый метод улучшения генерации изображений в определенном стиле с помощью диффузионных моделей. Авторы предлагают использовать Style-friendly SNR sampler, который смещает распределение соотношения сигнал-шум в сторону более высоких уровней шума при дообучении модели. Это позволяет лучше захватывать уникальные стилистические особенности и генерировать изображения с более высоким соответствием заданному стилю. Метод демонстрирует способность генерировать различные стили, включая акварельные рисунки, минималистичные мультфильмы, 3D-рендеры и мемы с текстом."
                },
                "en": {
                    "title": "Unlocking Unique Artistic Styles with Style-friendly SNR Sampler",
                    "desc": "This paper addresses the challenge of adapting large-scale diffusion models to generate personalized artistic styles. The authors introduce the Style-friendly SNR sampler, which modifies the signal-to-noise ratio (SNR) during fine-tuning to emphasize higher noise levels where stylistic features are more prominent. By doing so, the model improves its ability to capture unique styles, resulting in images that align better with the desired artistic expression. The proposed method expands the creative possibilities for generating diverse styles, including watercolor paintings and cartoons, thus enhancing personalized content creation."
                },
                "zh": {
                    "title": "提升个性化艺术风格生成的信噪比方法",
                    "desc": "最近的大规模扩散模型能够生成高质量的图像，但在学习新的个性化艺术风格方面存在困难，这限制了独特风格模板的创建。微调参考图像是最有前景的方法，但通常盲目使用预训练时的目标和噪声水平分布，导致风格对齐不理想。我们提出了风格友好的信噪比（SNR）采样器，在微调过程中积极将信噪比分布向更高的噪声水平转移，以专注于风格特征出现的噪声水平。这使得模型能够更好地捕捉独特风格，并生成具有更高风格对齐的图像。"
                }
            }
        }
    ],
    "link_prev": "2024-11-22.html",
    "link_next": "2024-11-26.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "22.11",
        "en": "11/22",
        "zh": "11月22日"
    },
    "short_date_next": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11月26日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了现有开源多模态大语言模型（MLLMs）的训练过程及其在分布偏移上的局限性。为了提升多模态推理能力，作者引入了偏好优化（PO）过程。他们创建了一个高质量的多模态推理偏好数据集MMPR，并开发了一种混合偏好优化（MPO）方法。结果显示，这种方法在多个基准测试中表现出色，特别是在多模态推理任务中。作者希望这项研究能激发更多进展。代码、数据和模型将公开发布。",
        "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization",
        "pinyin": "这篇文章讨论了现有开源多模态大语言模型（MLLMs）的训练过程及其在分布偏移上的局限性。\nZhè piān wénzhāng tǎolùn le xiànyǒu kāiyuán duō móshì dà yǔyán móxíng (MLLMs) de xùnliàn guòchéng jí qí zài fēnbù piānyí shàng de júxìanxìng.\n\n为了提升多模态推理能力，作者引入了偏好优化（PO）过程。\nWèile tíshēng duō móshì tuīlǐ nénglì, zuòzhě yǐnrù le piānhào yōuhuà (PO) guòchéng.\n\n他们创建了一个高质量的多模态推理偏好数据集MMPR，并开发了一种混合偏好优化（MPO）方法。\nTāmen chuàngjiàn le yīgè gāo zhìliàng de duō móshì tuīlǐ piānhào shùjùjí MMPR, bìng kāifā le yīzhǒng hùnhé piānhào yōuhuà (MPO) fāngfǎ.\n\n结果显示，这种方法在多个基准测试中表现出色，特别是在多模态推理任务中。\nJiégǔo xiǎnshì, zhè zhǒng fāngfǎ zài duō gè jīzhǔn cèshì zhōng biǎoxiàn chūsè, tèbié shì zài duō móshì tuīlǐ rènwù zhōng.\n\n作者希望这项研究能激发更多进展。\nZuòzhě xīwàng zhè xiàng yánjiū néng jīfā gèng duō jìnzhǎn.\n\n代码、数据和模型将公开发布。\nDàimǎ, shùjù hé móxíng jiāng gōngkāi fābù.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'},\n{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open-source'},\n{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'},\n{'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'},\n{'word': '局限性', 'pinyin': 'jú xiàn xìng', 'trans': 'limitations'},\n{'word': '提升', 'pinyin': 'tí shēng', 'trans': 'enhance'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'},\n{'word': '偏好', 'pinyin': 'piān hào', 'trans': 'preference'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'},\n{'word': '创建', 'pinyin': 'chuàng jiàn', 'trans': 'create'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high-quality'},\n{'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'},\n{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'develop'},\n{'word': '混合', 'pinyin': 'hùn hé', 'trans': 'hybrid'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '特别', 'pinyin': 'tè bié', 'trans': 'particularly'},\n{'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'},\n{'word': '激发', 'pinyin': 'jī fā', 'trans': 'inspire'},\n{'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'}]",
        "trans": "This article discusses the training process of existing open-source multimodal large language models (MLLMs) and their limitations in distribution shifts. To enhance multimodal reasoning capabilities, the authors introduce a preference optimization (PO) process. They created a high-quality multimodal reasoning preference dataset called MMPR and developed a hybrid preference optimization (MPO) method. The results show that this method performs excellently on multiple benchmark tests, particularly in multimodal reasoning tasks. The authors hope that this research will inspire further advancements. The code, data, and models will be publicly released.",
        "update_ts": "2024-11-24 09:32"
    }
}