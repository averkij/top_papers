{
    "date": {
        "ru": "25 ноября",
        "en": "November 25",
        "zh": "11月25日"
    },
    "time_utc": "2024-11-25 05:11",
    "weekday": 0,
    "issue_id": 754,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.14793",
            "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
            "url": "https://huggingface.co/papers/2411.14793",
            "abstract": "Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new \"style templates\", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation.",
            "score": 14,
            "issue_id": 752,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "03859b57f29683ab",
            "authors": [
                "Jooyoung Choi",
                "Chaehun Shin",
                "Yeongtak Oh",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14793.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#multimodal",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшение стилизации изображений с помощью оптимизации шума в диффузионных моделях",
                    "desc": "Статья представляет новый метод улучшения генерации изображений в определенном стиле с помощью диффузионных моделей. Авторы предлагают использовать Style-friendly SNR sampler, который смещает распределение соотношения сигнал-шум в сторону более высоких уровней шума при дообучении модели. Это позволяет лучше захватывать уникальные стилистические особенности и генерировать изображения с более высоким соответствием заданному стилю. Метод демонстрирует способность генерировать различные стили, включая акварельные рисунки, минималистичные мультфильмы, 3D-рендеры и мемы с текстом."
                },
                "en": {
                    "title": "Unlocking Unique Artistic Styles with Style-friendly SNR Sampler",
                    "desc": "This paper addresses the challenge of adapting large-scale diffusion models to generate personalized artistic styles. The authors introduce the Style-friendly SNR sampler, which modifies the signal-to-noise ratio (SNR) during fine-tuning to emphasize higher noise levels where stylistic features are more prominent. By doing so, the model improves its ability to capture unique styles, resulting in images that align better with the desired artistic expression. The proposed method expands the creative possibilities for generating diverse styles, including watercolor paintings and cartoons, thus enhancing personalized content creation."
                },
                "zh": {
                    "title": "提升个性化艺术风格生成的信噪比方法",
                    "desc": "最近的大规模扩散模型能够生成高质量的图像，但在学习新的个性化艺术风格方面存在困难，这限制了独特风格模板的创建。微调参考图像是最有前景的方法，但通常盲目使用预训练时的目标和噪声水平分布，导致风格对齐不理想。我们提出了风格友好的信噪比（SNR）采样器，在微调过程中积极将信噪比分布向更高的噪声水平转移，以专注于风格特征出现的噪声水平。这使得模型能够更好地捕捉独特风格，并生成具有更高风格对齐的图像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15098",
            "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
            "url": "https://huggingface.co/papers/2411.15098",
            "abstract": "In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only ~0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation.",
            "score": 2,
            "issue_id": 754,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "9cd668db99ed0902",
            "authors": [
                "Zhenxiong Tan",
                "Songhua Liu",
                "Xingyi Yang",
                "Qiaochu Xue",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15098.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#synthetic",
                    "#data",
                    "#diffusion",
                    "#dataset",
                    "#architecture",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "OminiControl: Универсальное управление генерацией изображений с минимальными затратами",
                    "desc": "OminiControl - это новая эффективная система для интеграции изображений в предобученные модели Diffusion Transformer (DiT). Она использует механизм повторного использования параметров, позволяя DiT кодировать условия изображений с помощью собственной архитектуры. OminiControl требует всего 0,1% дополнительных параметров и может решать широкий спектр задач условной генерации изображений. Система превосходит существующие модели на основе UNet и адаптированные DiT как в генерации, управляемой субъектом, так и в пространственно-согласованной условной генерации."
                },
                "en": {
                    "title": "Efficient Image Conditioning with OminiControl",
                    "desc": "OminiControl is a new framework that enhances pre-trained Diffusion Transformer (DiT) models by integrating image conditions efficiently. It uses a parameter reuse mechanism, allowing the DiT to process image conditions with minimal additional parameters, specifically around 0.1%. This framework can handle various image conditioning tasks, such as generating images based on specific subjects or aligning them with spatial features like edges and depth. OminiControl has shown superior performance compared to traditional UNet-based models and other DiT adaptations, and it comes with a large dataset, Subjects200K, to support further research."
                },
                "zh": {
                    "title": "OminiControl：高效整合图像条件的创新框架",
                    "desc": "本文介绍了OminiControl，这是一个高度灵活且参数高效的框架，能够将图像条件集成到预训练的扩散变换器（DiT）模型中。OminiControl利用参数重用机制，使DiT能够使用自身作为强大的基础，编码图像条件，并通过灵活的多模态注意力处理器进行处理。与现有方法不同，OminiControl仅需约0.1%的额外参数，就能有效地整合注入的图像条件，并以统一的方式处理多种图像条件任务。通过在DiT自身生成的图像上进行训练，OminiControl在主题驱动生成和空间对齐条件生成方面的表现优于现有的UNet和DiT适应模型。"
                }
            }
        }
    ],
    "link_prev": "2024-11-22.html",
    "link_next": "2024-11-26.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "22.11",
        "en": "11/22",
        "zh": "11月22日"
    },
    "short_date_next": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11月26日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了现有开源多模态大语言模型（MLLMs）的训练过程及其在分布偏移上的局限性。为了提升多模态推理能力，作者引入了偏好优化（PO）过程。他们创建了一个高质量的多模态推理偏好数据集MMPR，并开发了一种混合偏好优化（MPO）方法。结果显示，这种方法在多个基准测试中表现出色，特别是在多模态推理任务中。作者希望这项研究能激发更多进展。代码、数据和模型将公开发布。",
        "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization",
        "pinyin": "这篇文章讨论了现有开源多模态大语言模型（MLLMs）的训练过程及其在分布偏移上的局限性。\nZhè piān wénzhāng tǎolùn le xiànyǒu kāiyuán duō móshì dà yǔyán móxíng (MLLMs) de xùnliàn guòchéng jí qí zài fēnbù piānyí shàng de júxìanxìng.\n\n为了提升多模态推理能力，作者引入了偏好优化（PO）过程。\nWèile tíshēng duō móshì tuīlǐ nénglì, zuòzhě yǐnrù le piānhào yōuhuà (PO) guòchéng.\n\n他们创建了一个高质量的多模态推理偏好数据集MMPR，并开发了一种混合偏好优化（MPO）方法。\nTāmen chuàngjiàn le yīgè gāo zhìliàng de duō móshì tuīlǐ piānhào shùjùjí MMPR, bìng kāifā le yīzhǒng hùnhé piānhào yōuhuà (MPO) fāngfǎ.\n\n结果显示，这种方法在多个基准测试中表现出色，特别是在多模态推理任务中。\nJiégǔo xiǎnshì, zhè zhǒng fāngfǎ zài duō gè jīzhǔn cèshì zhōng biǎoxiàn chūsè, tèbié shì zài duō móshì tuīlǐ rènwù zhōng.\n\n作者希望这项研究能激发更多进展。\nZuòzhě xīwàng zhè xiàng yánjiū néng jīfā gèng duō jìnzhǎn.\n\n代码、数据和模型将公开发布。\nDàimǎ, shùjù hé móxíng jiāng gōngkāi fābù.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'},\n{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open-source'},\n{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'},\n{'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'},\n{'word': '局限性', 'pinyin': 'jú xiàn xìng', 'trans': 'limitations'},\n{'word': '提升', 'pinyin': 'tí shēng', 'trans': 'enhance'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'},\n{'word': '偏好', 'pinyin': 'piān hào', 'trans': 'preference'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'},\n{'word': '创建', 'pinyin': 'chuàng jiàn', 'trans': 'create'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high-quality'},\n{'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'},\n{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'develop'},\n{'word': '混合', 'pinyin': 'hùn hé', 'trans': 'hybrid'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '特别', 'pinyin': 'tè bié', 'trans': 'particularly'},\n{'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'},\n{'word': '激发', 'pinyin': 'jī fā', 'trans': 'inspire'},\n{'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'}]",
        "trans": "This article discusses the training process of existing open-source multimodal large language models (MLLMs) and their limitations in distribution shifts. To enhance multimodal reasoning capabilities, the authors introduce a preference optimization (PO) process. They created a high-quality multimodal reasoning preference dataset called MMPR and developed a hybrid preference optimization (MPO) method. The results show that this method performs excellently on multiple benchmark tests, particularly in multimodal reasoning tasks. The authors hope that this research will inspire further advancements. The code, data, and models will be publicly released.",
        "update_ts": "2024-11-24 09:32"
    }
}