{
    "date": {
        "ru": "6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 6",
        "zh": "8æœˆ6æ—¥"
    },
    "time_utc": "2025-08-06 20:14",
    "weekday": 2,
    "issue_id": 5215,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.02193",
            "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference",
            "url": "https://huggingface.co/papers/2508.02193",
            "abstract": "Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.",
            "score": 57,
            "issue_id": 5199,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "bec183ec45598da2",
            "authors": [
                "Yuxuan Song",
                "Zheng Zhang",
                "Cheng Luo",
                "Pengyang Gao",
                "Fan Xia",
                "Hao Luo",
                "Zheng Li",
                "Yuehang Yang",
                "Hongli Yu",
                "Xingwei Qu",
                "Yuwei Fu",
                "Jing Su",
                "Ge Zhang",
                "Wenhao Huang",
                "Mingxuan Wang",
                "Lin Yan",
                "Xiaoying Jia",
                "Jingjing Liu",
                "Wei-Ying Ma",
                "Ya-Qin Zhang",
                "Yonghui Wu",
                "Hao Zhou"
            ],
            "affiliations": [
                "ByteDance",
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "SIA-Lab of Tsinghua AIR and ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02193.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#benchmark",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Seed Diffusion Preview - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ½ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° 2146 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ½Ğ° GPU H20, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Mercury Ğ¸ Gemini Diffusion. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Seed Diffusion Preview ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Speed Meets Quality: The Future of Code Generation",
                    "desc": "Seed Diffusion Preview is a novel discrete-state diffusion language model that enhances inference speed through parallel generation techniques. By utilizing non-sequential decoding, it significantly reduces the latency typically associated with traditional token-by-token generation methods. This model achieves an impressive speed of 2,146 tokens per second on H20 GPUs while still delivering competitive performance on standard code evaluation benchmarks. As a result, Seed Diffusion Preview sets a new standard in the speed-quality trade-off for code generation models, outperforming existing models like Mercury and Gemini Diffusion."
                },
                "zh": {
                    "title": "ç§å­æ‰©æ•£é¢„è§ˆï¼šé€Ÿåº¦ä¸è´¨é‡çš„æ–°æ ‡æ†",
                    "desc": "Seed Diffusion Previewæ˜¯ä¸€ç§åŸºäºç¦»æ•£çŠ¶æ€æ‰©æ•£çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰æå¿«çš„æ¨ç†é€Ÿåº¦ã€‚é€šè¿‡éé¡ºåºçš„å¹¶è¡Œç”Ÿæˆï¼Œç¦»æ•£æ‰©æ•£æ¨¡å‹æ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œå‡å°‘äº†é€ä¸ªè§£ç çš„å»¶è¿Ÿã€‚è¯¥æ¨¡å‹åœ¨H20 GPUä¸Šå®ç°äº†æ¯ç§’2,146ä¸ªtokençš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶åœ¨æ ‡å‡†ä»£ç è¯„ä¼°åŸºå‡†ä¸Šä¿æŒäº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ä¸å½“å‰çš„Mercuryå’ŒGemini Diffusionç›¸æ¯”ï¼ŒSeed Diffusion Previewåœ¨é€Ÿåº¦å’Œè´¨é‡ä¸Šéƒ½è®¾ç«‹äº†æ–°çš„æ ‡æ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03320",
            "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation",
            "url": "https://huggingface.co/papers/2508.03320",
            "abstract": "Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
            "score": 40,
            "issue_id": 5199,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "71dc78f7c773cefd",
            "authors": [
                "Peiyu Wang",
                "Yi Peng",
                "Yimeng Gan",
                "Liang Hu",
                "Tianyidan Xie",
                "Xiaokun Wang",
                "Yichen Wei",
                "Chuanxin Tang",
                "Bo Zhu",
                "Changshi Li",
                "Hongyang Wei",
                "Eric Li",
                "Xuchen Song",
                "Yang Liu",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Multimodality Team, Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03320.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#multimodal",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Skywork UniPic - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GenEval, DPG-Bench Ğ¸ GEditBench-EN. Skywork UniPic Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ° Ğ±ĞµĞ· Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "Unifying Multimodal AI: Efficiency Meets Performance with Skywork UniPic",
                    "desc": "Skywork UniPic is a powerful 1.5 billion-parameter autoregressive model that combines image understanding, text-to-image generation, and image editing into one system. It eliminates the need for separate components for different tasks, allowing for efficient performance on standard hardware. The model achieves impressive scores on various benchmarks, showcasing its capabilities in generating and editing high-quality images. By using innovative training strategies and large datasets, Skywork UniPic demonstrates that advanced multimodal AI can be accessible without requiring excessive computational resources."
                },
                "zh": {
                    "title": "Skywork UniPicï¼šç»Ÿä¸€å¤šæ¨¡æ€AIçš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆ",
                    "desc": "Skywork UniPicæ˜¯ä¸€ä¸ªæ‹¥æœ‰15äº¿å‚æ•°çš„è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿç»Ÿä¸€å›¾åƒç†è§£ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸€ä¸ªå•ä¸€æ¶æ„æ¶ˆé™¤äº†å¯¹ç‰¹å®šä»»åŠ¡é€‚é…å™¨æˆ–æ¨¡å—è¿æ¥å™¨çš„éœ€æ±‚ï¼Œå±•ç¤ºäº†ç´§å‡‘çš„å¤šæ¨¡æ€ç³»ç»Ÿåœ¨æ™®é€šç¡¬ä»¶ä¸Šä¹Ÿèƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚Skywork UniPicåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ•°æ®é›†è®¾è®¡ã€‚è¯¥æ¨¡å‹ä¸ºé«˜ä¿çœŸå¤šæ¨¡æ€AIçš„å®é™…åº”ç”¨æä¾›äº†æ–°çš„èŒƒå¼ï¼Œä¸”ä»£ç å’Œæƒé‡å·²å…¬å¼€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03694",
            "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation",
            "url": "https://huggingface.co/papers/2508.03694",
            "abstract": "LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.",
            "score": 31,
            "issue_id": 5198,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "8c05bd06521b3fb7",
            "authors": [
                "Jianxiong Gao",
                "Zhaoxi Chen",
                "Xian Liu",
                "Jianfeng Feng",
                "Chenyang Si",
                "Yanwei Fu",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "NVIDIA",
                "Nanjing University",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03694.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#multimodal",
                    "#long_context",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "LongVie: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "LongVie - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€Ğ°Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². LongVie Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "LongVie: Mastering Ultra-Long Video Generation with Consistency and Quality",
                    "desc": "LongVie is an innovative autoregressive framework designed for generating ultra-long videos while maintaining visual quality and temporal consistency. It addresses common challenges in video generation, such as noise initialization and control signal normalization, by implementing a unified approach that ensures consistent output across clips. The framework also incorporates multi-modal control, allowing it to utilize various types of guidance signals, and employs a degradation-aware training method to enhance visual fidelity over time. Through extensive testing, LongVie demonstrates superior performance in generating long videos that are both controllable and visually appealing."
                },
                "zh": {
                    "title": "è¶…é•¿è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šLongVie",
                    "desc": "LongVieæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¶…é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰é€€åŒ–é—®é¢˜ã€‚å®ƒé€šè¿‡ç»Ÿä¸€çš„å™ªå£°åˆå§‹åŒ–ã€å…¨å±€æ§åˆ¶ä¿¡å·å½’ä¸€åŒ–ã€å¤šæ¨¡æ€æ§åˆ¶å’Œé€€åŒ–æ„ŸçŸ¥è®­ç»ƒæ¥å®ç°è¿™äº›ç›®æ ‡ã€‚LongVieçš„æ ¸å¿ƒè®¾è®¡ç¡®ä¿äº†æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æ§åˆ¶æ¡†æ¶æ¥å‡è½»è§†è§‰é€€åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongVieåœ¨é•¿æ—¶é—´å¯æ§æ€§ã€ä¸€è‡´æ€§å’Œè´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03686",
            "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
            "url": "https://huggingface.co/papers/2508.03686",
            "abstract": "CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.",
            "score": 21,
            "issue_id": 5201,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "dddc5da46c921b94",
            "authors": [
                "Shudong Liu",
                "Hongwei Liu",
                "Junnan Liu",
                "Linchen Xiao",
                "Songyang Gao",
                "Chengqi Lyu",
                "Yuzhe Gu",
                "Wenwei Zhang",
                "Derek F. Wong",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "NLP2CT Lab",
                "Shanghai AI Laboratory",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03686.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#benchmark",
                    "#dataset",
                    "#interpretability",
                    "#optimization"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "CompassVerifier: ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…",
                    "desc": "CompassVerifier - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ VerifierBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. CompassVerifier Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing LLM Output Verification with CompassVerifier",
                    "desc": "CompassVerifier is a new model designed to verify the outputs of large language models (LLMs) across different subjects. It addresses the limitations of existing verification methods by providing a robust and lightweight solution that can handle complex answer types and identify invalid responses. The model is supported by VerifierBench, a benchmark dataset that helps evaluate the verification capabilities of various LLMs. This work aims to improve answer verification processes and enhance reinforcement learning research by offering a comprehensive tool for evaluating AI-generated responses."
                },
                "zh": {
                    "title": "CompassVerifierï¼šå¤šé¢†åŸŸç­”æ¡ˆéªŒè¯çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆ",
                    "desc": "CompassVerifier æ˜¯ä¸€ç§è½»é‡çº§ä¸”ç¨³å¥çš„æ¨¡å‹ï¼Œç”¨äºéªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒé¢†åŸŸçš„è¾“å‡ºã€‚å®ƒé€šè¿‡ VerifierBench è¿™ä¸€å…¨é¢çš„åŸºå‡†æ•°æ®é›†æ¥æ”¯æŒéªŒè¯è¿‡ç¨‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§ç±»å‹çš„ç­”æ¡ˆï¼ŒåŒ…æ‹¬å¤šå­é—®é¢˜ã€å…¬å¼å’Œåºåˆ—ç­”æ¡ˆï¼Œå¹¶æœ‰æ•ˆè¯†åˆ«å¼‚å¸¸æˆ–æ— æ•ˆçš„å“åº”ã€‚æˆ‘ä»¬å¸Œæœ› CompassVerifier å’Œ VerifierBench èƒ½å¤Ÿä¿ƒè¿›ç­”æ¡ˆéªŒè¯ã€è¯„ä¼°åè®®å’Œå¼ºåŒ–å­¦ä¹ ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03012",
            "title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
            "url": "https://huggingface.co/papers/2508.03012",
            "abstract": "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue localization, the process of identifying code locations that need modification to resolve software issues, is a critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLM-based agents attempt to address this by integrating repository retrieval tools. However, this transforms issue localization into a demanding task we call Repo Deep Search, which requires the LLM to effectively utilize various repository retrieval tools throughout a multi-step reasoning and navigation process. To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs' ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is a viable and effective strategy for improving automated software development.",
            "score": 8,
            "issue_id": 5199,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "4ab74a355fed1d76",
            "authors": [
                "Zexiong Ma",
                "Chao Peng",
                "Qunhong Zeng",
                "Pengfei Gao",
                "Yanzhen Zou",
                "Bing Xie"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "ByteDance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03012.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ToolTrain: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² ĞºĞ¾Ğ´Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ToolTrain - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ToolTrain, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "ToolTrain: Enhancing LLMs for Superior Issue Localization",
                    "desc": "This paper introduces ToolTrain, a novel two-stage training framework designed to improve large language models (LLMs) for the task of issue localization in software development. It combines supervised fine-tuning with reinforcement learning to enhance the models' ability to utilize repository retrieval tools effectively. The framework addresses the challenges posed by the semantic gap between natural language descriptions of issues and the corresponding faulty code, requiring complex reasoning through code dependencies. Experimental results indicate that models trained with ToolTrain achieve state-of-the-art performance, significantly improving both localization and overall issue resolution in automated software development."
                },
                "zh": {
                    "title": "ToolTrainï¼šæå‡é—®é¢˜å®šä½çš„æ™ºèƒ½å·¥å…·è®­ç»ƒæ¡†æ¶",
                    "desc": "ToolTrainæ˜¯ä¸€ç§ä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®é¢˜å®šä½æ–¹é¢çš„èƒ½åŠ›ã€‚é—®é¢˜å®šä½æ˜¯è¯†åˆ«éœ€è¦ä¿®æ”¹çš„ä»£ç ä½ç½®ä»¥è§£å†³è½¯ä»¶é—®é¢˜çš„è¿‡ç¨‹ï¼Œä½†ç”±äºè‡ªç„¶è¯­è¨€æè¿°ä¸æ•…éšœä»£ç ä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼Œè¿™ä¸€ä»»åŠ¡éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ToolTrainé€šè¿‡æ•´åˆä»£ç åº“æ£€ç´¢å·¥å…·ï¼Œå¸®åŠ©LLMsåœ¨å¤šæ­¥éª¤æ¨ç†å’Œå¯¼èˆªè¿‡ç¨‹ä¸­æœ‰æ•ˆåˆ©ç”¨è¿™äº›å·¥å…·ï¼Œä»è€Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒToolTrainè®­ç»ƒçš„æ¨¡å‹åœ¨åŠŸèƒ½çº§å®šä½ä¸Šè¶…è¶Šäº†Claude-3.7ï¼Œè¯æ˜äº†é’ˆå¯¹é—®é¢˜å®šä½çš„è®­ç»ƒç­–ç•¥åœ¨è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ä¸­æ˜¯æœ‰æ•ˆçš„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02091",
            "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search",
            "url": "https://huggingface.co/papers/2508.02091",
            "abstract": "CRINN, a reinforcement learning-based approach, optimizes approximate nearest-neighbor search algorithms for speed while maintaining accuracy, outperforming state-of-the-art methods on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
            "score": 7,
            "issue_id": 5201,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "10eb53caada711ad",
            "authors": [
                "Xiaoya Li",
                "Xiaofei Sun",
                "Albert Wang",
                "Chris Shum",
                "Jiwei Li"
            ],
            "affiliations": [
                "DeepReinforce Team",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02091.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#rag",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "CRINN: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "CRINN - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹ (ANNS), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ANNS, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. CRINN Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GIST-960-Euclidean Ğ¸ MNIST-784-Euclidean. Ğ£ÑĞ¿ĞµÑ… CRINN Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "CRINN: Speeding Up Nearest-Neighbor Search with Reinforcement Learning",
                    "desc": "CRINN is a novel approach that uses reinforcement learning to enhance approximate nearest-neighbor search (ANNS) algorithms, focusing on improving their speed while ensuring accuracy. By framing the optimization of ANNS as a reinforcement learning problem, CRINN uses execution speed as a reward signal to automatically generate faster implementations. The results show that CRINN outperforms existing state-of-the-art ANNS methods on multiple benchmark datasets, achieving top performance in several cases. This work highlights the potential of combining reinforcement learning with large language models (LLMs) for automating complex algorithmic optimizations."
                },
                "zh": {
                    "title": "CRINNï¼šç”¨å¼ºåŒ–å­¦ä¹ åŠ é€Ÿè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢",
                    "desc": "CRINNæ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ç®—æ³•çš„é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å°†è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢çš„ä¼˜åŒ–è§†ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œä»¥æ‰§è¡Œé€Ÿåº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCRINNèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé€æ¸æ›´å¿«çš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢å®ç°ï¼Œå¹¶æ»¡è¶³å‡†ç¡®æ€§çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCRINNåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00367",
            "title": "Representation Shift: Unifying Token Compression with FlashAttention",
            "url": "https://huggingface.co/papers/2508.00367",
            "abstract": "Representation Shift is a training-free, model-agnostic metric that integrates token compression with FlashAttention, enabling significant speedups in video-text retrieval and video QA.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers have demonstrated remarkable success across vision, language, and video. Yet, increasing task complexity has led to larger models and more tokens, raising the quadratic cost of self-attention and the overhead of GPU memory access. To reduce the computation cost of self-attention, prior work has proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. This, however, makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance. Here, we propose Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation. This seamlessly integrates token compression with FlashAttention, without attention maps or retraining. Our method further generalizes beyond Transformers to CNNs and state space models. Extensive experiments show that Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA, respectively. Code is available at https://github.com/mlvlab/Representation-Shift.",
            "score": 7,
            "issue_id": 5209,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 1",
                "zh": "8æœˆ1æ—¥"
            },
            "hash": "5a4ad3025ab24bd1",
            "authors": [
                "Joonmyung Choi",
                "Sanghyeok Lee",
                "Byungoh Ko",
                "Eunseo Kim",
                "Jihyung Kil",
                "Hyunwoo J. Kim"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST",
                "Korea University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00367.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#data",
                    "#optimization",
                    "#video"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Representation Shift, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ FlashAttention, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Representation Shift Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼, Ğ½Ğ¾ Ğ¸ Ğº CNN Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Boosting Video Retrieval Efficiency with Representation Shift",
                    "desc": "This paper introduces Representation Shift, a new metric that helps improve the efficiency of video-text retrieval and video question answering without needing to retrain models. It combines token compression techniques with FlashAttention, which reduces memory usage and speeds up processing. By measuring how much each token's representation changes, it allows for effective token selection without relying on attention maps. The method works not only with Transformers but also with other model types like CNNs, achieving notable performance improvements."
                },
                "zh": {
                    "title": "Representation Shiftï¼šåŠ é€Ÿè§†é¢‘æ£€ç´¢ä¸é—®ç­”çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "Representation Shiftæ˜¯ä¸€ç§æ— è®­ç»ƒã€æ¨¡å‹æ— å…³çš„åº¦é‡æ–¹æ³•ï¼Œå®ƒå°†ä»¤ç‰Œå‹ç¼©ä¸FlashAttentionç»“åˆï¼Œæ˜¾è‘—åŠ å¿«è§†é¢‘-æ–‡æœ¬æ£€ç´¢å’Œè§†é¢‘é—®ç­”çš„é€Ÿåº¦ã€‚éšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼Œæ¨¡å‹å’Œä»¤ç‰Œçš„è§„æ¨¡ä¹Ÿåœ¨æ‰©å¤§ï¼Œå¯¼è‡´è‡ªæ³¨æ„åŠ›çš„è®¡ç®—æˆæœ¬å‘ˆå¹³æ–¹å¢é•¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æµ‹é‡æ¯ä¸ªä»¤ç‰Œè¡¨ç¤ºçš„å˜åŒ–ç¨‹åº¦ï¼Œæ¥å®ç°æœ‰æ•ˆçš„ä»¤ç‰Œå‹ç¼©ï¼Œè€Œæ— éœ€æ„å»ºæ³¨æ„åŠ›å›¾æˆ–é‡æ–°è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepresentation Shiftåœ¨è§†é¢‘-æ–‡æœ¬æ£€ç´¢å’Œè§†é¢‘é—®ç­”ä¸­åˆ†åˆ«å®ç°äº†é«˜è¾¾5.5%å’Œ4.4%çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03050",
            "title": "Multi-human Interactive Talking Dataset",
            "url": "https://huggingface.co/papers/2508.03050",
            "abstract": "MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.",
            "score": 6,
            "issue_id": 5200,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "01ba126a166568d6",
            "authors": [
                "Zeyu Zhu",
                "Weijia Wu",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03050.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ»ÑĞ´ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MIT - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ»ÑĞ´ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ CovOG - Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¿Ğ¾Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ñ€Ğ°Ğ¹Ğ²ĞµÑ€. MIT ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 12 Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ 2-4 Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ· Ñ‚ĞµĞ»Ğ° Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. CovOG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¿Ğ¾Ğ· Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾."
                },
                "en": {
                    "title": "MIT: Pioneering Multi-Human Talking Video Generation",
                    "desc": "The paper introduces MIT, a large-scale dataset aimed at generating multi-human talking videos, which includes detailed annotations for body poses and speech interactions. This dataset addresses the limitations of previous studies that focused mainly on single-person videos, making it more applicable to real-life conversations. To utilize this dataset, the authors propose CovOG, a baseline model that combines a Multi-Human Pose Encoder to manage multiple speakers and an Interactive Audio Driver to synchronize head movements with audio cues. This work not only demonstrates the potential of generating realistic multi-human interactions but also sets a new benchmark for future research in this area."
                },
                "zh": {
                    "title": "MITï¼šå¤šäººäººå¯¹è¯è§†é¢‘ç”Ÿæˆçš„æ–°åŸºå‡†",
                    "desc": "MITæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºå¤šäººçš„å¯¹è¯è§†é¢‘ç”Ÿæˆï¼ŒåŒ…å«ç»†è‡´çš„æ³¨é‡Šä¿¡æ¯ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•äººç‹¬ç™½æˆ–å­¤ç«‹çš„é¢éƒ¨åŠ¨ç”»ä¸Šï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®å¤šäººçš„äº’åŠ¨ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æµç¨‹ï¼Œæ”¶é›†å’Œæ³¨é‡Šå¤šäººçš„å¯¹è¯è§†é¢‘ï¼Œæ•°æ®é›†åŒ…å«12å°æ—¶çš„é«˜åˆ†è¾¨ç‡è§†é¢‘ï¼Œå±•ç¤ºäº†è‡ªç„¶çš„å¯¹è¯åŠ¨æ€ã€‚ä¸ºäº†å±•ç¤ºMITçš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†CovOGæ¨¡å‹ï¼Œç»“åˆäº†å¤šäººçš„å§¿æ€ç¼–ç å™¨å’Œäº’åŠ¨éŸ³é¢‘é©±åŠ¨å™¨ï¼Œå±•ç¤ºäº†ç”ŸæˆçœŸå®å¤šäººçš„å¯¹è¯è§†é¢‘çš„å¯è¡Œæ€§å’ŒæŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01119",
            "title": "The Promise of RL for Autoregressive Image Editing",
            "url": "https://huggingface.co/papers/2508.01119",
            "abstract": "Reinforcement learning combined with a large multimodal language model verifier enhances image editing performance in an autoregressive multimodal framework.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner. We find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies. As a result, we release EARL: Editing with Autoregression and RL, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL.",
            "score": 6,
            "issue_id": 5215,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 1",
                "zh": "8æœˆ1æ—¥"
            },
            "hash": "b7e0974935b60296",
            "authors": [
                "Saba Ahmadi",
                "Rabiul Awal",
                "Ankur Sikarwar",
                "Amirhossein Kazemnejad",
                "Ge Ya Luo",
                "Juan A. Rodriguez",
                "Sai Rajeswar",
                "Siva Reddy",
                "Christopher Pal",
                "Benno Krojer",
                "Aishwarya Agrawal"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "McGill University",
                "Mila Quebec AI Institute",
                "Polytechnique MontrÃ©al",
                "ServiceNow",
                "UniversitÃ© de MontrÃ©al",
                "Ã‰cole de Technologie SupÃ©rieure (ETS)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01119.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#multimodal",
                    "#open_source",
                    "#training",
                    "#games"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. ĞĞ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EARL, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Reinforcement Learning Meets Multimodal Image Editing",
                    "desc": "This paper presents a novel approach to image editing by integrating reinforcement learning (RL) with a large multimodal language model (LLM) verifier within an autoregressive framework. The authors investigate three strategies: supervised fine-tuning, RL, and Chain-of-Thought reasoning, to improve image editing tasks. They demonstrate that the combination of RL and a multimodal LLM verifier significantly enhances performance, leading to the development of EARL, a robust RL-based image editing model. EARL achieves competitive results on various editing tasks while requiring less training data compared to existing methods, marking a significant advancement in autoregressive multimodal models for image editing."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ ä¸å¤šæ¨¡æ€æ¨¡å‹ç»“åˆï¼Œæå‡å›¾åƒç¼–è¾‘æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹éªŒè¯å™¨æ¥æå‡å›¾åƒç¼–è¾‘æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§ç­–ç•¥ï¼šç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œé“¾å¼æ€ç»´æ¨ç†ï¼Œå¹¶åœ¨ä¸€ä¸ªè‡ªå›å½’å¤šæ¨¡æ€æ¡†æ¶ä¸­è¿›è¡Œç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ä¸å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹éªŒè¯å™¨çš„ç»“åˆæ˜¯æœ€æœ‰æ•ˆçš„ç­–ç•¥ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å‘å¸ƒäº†EARLæ¨¡å‹ï¼Œå®ƒåœ¨å¤šç§å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”è®­ç»ƒæ•°æ®éœ€æ±‚è¾ƒå°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03613",
            "title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction",
            "url": "https://huggingface.co/papers/2508.03613",
            "abstract": "Goedel-Prover-V2, a series of open-source language models, achieves state-of-the-art performance in automated theorem proving through scaffolded data synthesis, verifier-guided self-correction, and model averaging.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.",
            "score": 5,
            "issue_id": 5204,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "50044cd9b7eb1802",
            "authors": [
                "Yong Lin",
                "Shange Tang",
                "Bohan Lyu",
                "Ziran Yang",
                "Jui-Hui Chung",
                "Haoyu Zhao",
                "Lai Jiang",
                "Yihan Geng",
                "Jiawei Ge",
                "Jingruo Sun",
                "Jiayun Wu",
                "Jiri Gesi",
                "Ximing Lu",
                "David Acuna",
                "Kaiyu Yang",
                "Hongzhou Lin",
                "Yejin Choi",
                "Danqi Chen",
                "Sanjeev Arora",
                "Chi Jin"
            ],
            "affiliations": [
                "Amazon",
                "Meta FAIR",
                "NVIDIA",
                "Peking University",
                "Princeton Language and Intelligence, Princeton University",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03613.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#synthetic",
                    "#reasoning",
                    "#small_models",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°: Goedel-Prover-V2 Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¸Ñ€ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼",
                    "desc": "Goedel-Prover-V2 - ÑÑ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Goedel-Prover-V2-8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MiniF2F Ğ¸ PutnamBench. ĞĞ° Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ° Goedel-Prover-V2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼."
                },
                "en": {
                    "title": "Revolutionizing Theorem Proving with Goedel-Prover-V2",
                    "desc": "Goedel-Prover-V2 is a series of open-source language models that excel in automated theorem proving. It introduces innovative techniques such as scaffolded data synthesis for training on progressively complex tasks, verifier-guided self-correction for iterative proof refinement, and model averaging to enhance output diversity. The models achieve impressive performance metrics, with the smaller Goedel-Prover-V2-8B outperforming larger models like DeepSeek-Prover-V2-671B. Overall, Goedel-Prover-V2 sets a new benchmark in the field, demonstrating superior capabilities in solving mathematical problems with a reduced computational footprint."
                },
                "zh": {
                    "title": "Goedel-Prover-V2ï¼šè‡ªåŠ¨å®šç†è¯æ˜çš„æ–°æ ‡æ†",
                    "desc": "Goedel-Prover-V2æ˜¯ä¸€ç³»åˆ—å¼€æºè¯­è¨€æ¨¡å‹ï¼Œåœ¨è‡ªåŠ¨å®šç†è¯æ˜é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯å®ç°äº†è¿™ä¸€ç›®æ ‡ï¼šé¦–å…ˆï¼Œä½¿ç”¨æ”¯æ¶æ•°æ®åˆæˆç”Ÿæˆé€æ¸å¢åŠ éš¾åº¦çš„åˆæˆä»»åŠ¡ï¼Œä»¥å¸®åŠ©æ¨¡å‹æŒæ¡å¤æ‚çš„å®šç†ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨éªŒè¯å™¨å¼•å¯¼çš„è‡ªæˆ‘ä¿®æ­£ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®Leanç¼–è¯‘å™¨çš„åé¦ˆè¿­ä»£ä¿®æ­£å…¶è¯æ˜ï¼›æœ€åï¼Œé€šè¿‡æ¨¡å‹å¹³å‡æŠ€æœ¯åˆå¹¶æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä»¥å‡å°‘è®­ç»ƒåæœŸæ¨¡å‹è¾“å‡ºå¤šæ ·æ€§çš„ä¸‹é™ã€‚Goedel-Prover-V2-32Bæ¨¡å‹åœ¨MiniF2Fä¸Šè¾¾åˆ°äº†88.1%çš„é€šè¿‡ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01780",
            "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
            "url": "https://huggingface.co/papers/2508.01780",
            "abstract": "LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.",
            "score": 5,
            "issue_id": 5199,
            "pub_date": "2025-08-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 3",
                "zh": "8æœˆ3æ—¥"
            },
            "hash": "b9c09b0ce4e2dad3",
            "authors": [
                "Guozhao Mo",
                "Wenliang Zhong",
                "Jiawei Chen",
                "Xuanang Chen",
                "Yaojie Lu",
                "Hongyu Lin",
                "Ben He",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01780.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#survey",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LiveMCPBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… MCP-ÑÑ€ĞµĞ´Ğ°Ñ…",
                    "desc": "LiveMCPBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ MCP. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 95 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ ÑÑƒĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ LiveMCPTool - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 70 MCP-ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ² Ğ¸ 527 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ LiveMCPEval - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 10 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Claude-Sonnet-4) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 78.95% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Revolutionizing LLM Evaluation in Dynamic MCP Environments",
                    "desc": "LiveMCPBench is a new benchmark designed to evaluate large language model (LLM) agents in real-world tasks within the Model Context Protocol (MCP) ecosystem. It addresses the limitations of existing benchmarks that only test single-server settings by providing a scalable evaluation framework with 95 diverse tasks and a collection of 70 MCP servers and 527 tools. The benchmark includes an innovative LLM-as-a-Judge system for automated evaluation, achieving high agreement with human reviewers. Results show significant performance differences among models, highlighting the challenges LLMs face in complex, tool-rich environments."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°LLMä»£ç†çš„åŸºå‡†æµ‹è¯•å¹³å°",
                    "desc": "LiveMCPBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®ƒè§£å†³äº†ç°æœ‰åŸºå‡†æµ‹è¯•ä»…é™äºå•ä¸€æœåŠ¡å™¨è®¾ç½®çš„é—®é¢˜ï¼Œæä¾›äº†95ä¸ªåŸºäºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ç”Ÿæ€ç³»ç»Ÿçš„çœŸå®ä»»åŠ¡ã€‚é€šè¿‡LiveMCPToolï¼Œç ”ç©¶äººå‘˜å¯ä»¥ä½¿ç”¨70ä¸ªMCPæœåŠ¡å™¨å’Œ527ä¸ªå·¥å…·ï¼Œæ”¯æŒå¯æ‰©å±•å’Œå¯é‡å¤çš„è¯„ä¼°æµç¨‹ã€‚æ­¤å¤–ï¼ŒLiveMCPEvalæ¡†æ¶å®ç°äº†è‡ªåŠ¨åŒ–å’Œè‡ªé€‚åº”è¯„ä¼°ï¼Œç¡®ä¿åœ¨åŠ¨æ€ä»»åŠ¡ç¯å¢ƒä¸­ä¸äººç±»è¯„å®¡è€…çš„é«˜ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00477",
            "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer",
            "url": "https://huggingface.co/papers/2508.00477",
            "abstract": "LAMIC, a Layout-Aware Multi-Image Composition framework, extends single-reference diffusion models to multi-reference scenarios using attention mechanisms, achieving state-of-the-art performance in controllable image synthesis without training.  \t\t\t\t\tAI-generated summary \t\t\t\t In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.",
            "score": 4,
            "issue_id": 5202,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 1",
                "zh": "8æœˆ1æ—¥"
            },
            "hash": "94d96ba2b9f92b31",
            "authors": [
                "Yuzhuo Chen",
                "Zehua Ma",
                "Jianhua Wang",
                "Kai Kang",
                "Shunyu Yao",
                "Weiming Zhang"
            ],
            "affiliations": [
                "East China Normal University",
                "Onestory Team",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00477.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#optimization",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "LAMIC: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°Ğ¼Ğ¸",
                    "desc": "LAMIC - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼ Ğ½Ğ° ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°Ğ¼Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Group Isolation Attention Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Region-Modulated Attention Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°. LAMIC Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ„Ğ¾Ğ½Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğµ Ğ¼Ğ°ĞºĞµÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "LAMIC: Revolutionizing Multi-Image Synthesis Without Training",
                    "desc": "LAMIC is a new framework designed for creating images from multiple references while considering their layout. It builds on existing single-reference diffusion models and introduces attention mechanisms to improve how different elements in the images are handled. This framework allows for high-quality image synthesis without the need for additional training, demonstrating strong performance in maintaining identity, background consistency, and layout control. LAMIC sets a new standard in controllable image composition by effectively managing multiple images in a training-free manner."
                },
                "zh": {
                    "title": "LAMICï¼šæ— è®­ç»ƒçš„å¤šå›¾åƒåˆæˆæ–°èŒƒå¼",
                    "desc": "LAMICæ˜¯ä¸€ä¸ªå¸ƒå±€æ„ŸçŸ¥çš„å¤šå›¾åƒåˆæˆæ¡†æ¶ï¼Œå®ƒå°†å•å‚è€ƒæ‰©æ•£æ¨¡å‹æ‰©å±•åˆ°å¤šå‚è€ƒåœºæ™¯ï¼Œä¸”æ— éœ€è®­ç»ƒã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶ï¼šç¾¤ä½“éš”ç¦»æ³¨æ„åŠ›ï¼ˆGIAï¼‰å’ŒåŒºåŸŸè°ƒåˆ¶æ³¨æ„åŠ›ï¼ˆRMAï¼‰ï¼Œä»¥å¢å¼ºå®ä½“åˆ†ç¦»å’Œå¸ƒå±€æ„ŸçŸ¥ç”Ÿæˆã€‚é€šè¿‡å¼•å…¥ä¸‰ç§è¯„ä¼°æŒ‡æ ‡ï¼ŒLAMICåœ¨èº«ä»½ä¿æŒã€èƒŒæ™¯ä¸€è‡´æ€§å’Œå¸ƒå±€æ§åˆ¶ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šå‚è€ƒåŸºçº¿ã€‚LAMICå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¯æ§çš„å¤šå›¾åƒåˆæˆå»ºç«‹äº†æ–°çš„æ— è®­ç»ƒèŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03164",
            "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning",
            "url": "https://huggingface.co/papers/2508.03164",
            "abstract": "ChartCap, a large-scale dataset with dense, type-specific captions for real-world charts, improves caption accuracy and reduces hallucinations in vision language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.",
            "score": 3,
            "issue_id": 5204,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 5",
                "zh": "8æœˆ5æ—¥"
            },
            "hash": "aec1f860dbfa8231",
            "authors": [
                "Junyoung Lim",
                "Jaewoo Ahn",
                "Gunhee Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03164.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#hallucinations",
                    "#dataset",
                    "#data",
                    "#open_source"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ChartCap: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼ Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "ChartCap - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 565 Ñ‚Ñ‹ÑÑÑ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ChartCap Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ - Visual Consistency Score, Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ¿Ğ¾Ñ€Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸."
                },
                "en": {
                    "title": "ChartCap: Elevating Chart Captioning Accuracy and Reducing Hallucinations",
                    "desc": "ChartCap is a new dataset designed to enhance the performance of vision language models in generating captions for charts. It contains 565,000 real-world chart images with detailed, type-specific captions that focus on essential structural elements and insights, avoiding irrelevant information. The dataset is created through a four-stage process that ensures high-quality captions, verified by a cycle consistency method for efficient quality control. Experiments show that models trained on ChartCap produce more accurate and informative captions, with fewer hallucinations compared to existing models and even human-generated captions."
                },
                "zh": {
                    "title": "ChartCapï¼šæå‡å›¾è¡¨è¯´æ˜å‡†ç¡®æ€§çš„å…³é”®æ•°æ®é›†",
                    "desc": "ChartCapæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼ŒåŒ…å«565Kä¸ªçœŸå®ä¸–ç•Œå›¾è¡¨å›¾åƒåŠå…¶ç‰¹å®šç±»å‹çš„è¯¦ç»†è¯´æ˜ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯´æ˜å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘è™šå‡ä¿¡æ¯çš„ç”Ÿæˆã€‚é€šè¿‡è®¾è®¡å››é˜¶æ®µçš„ç”Ÿæˆç®¡é“ï¼ŒChartCapç¡®ä¿è¯´æ˜ä»…åŸºäºå›¾è¡¨ä¸­å¯è¾¨åˆ«çš„æ•°æ®ï¼Œå¹¶é€šè¿‡å¾ªç¯ä¸€è‡´æ€§çš„äººç±»éªŒè¯åŠ é€Ÿè´¨é‡æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºChartCapå¾®è°ƒçš„æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®å’Œä¿¡æ¯ä¸°å¯Œçš„è¯´æ˜æ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œç”šè‡³è¶…è¿‡äººç±»æ ‡æ³¨çš„è¯´æ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02629",
            "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents",
            "url": "https://huggingface.co/papers/2508.02629",
            "abstract": "HyCodePolicy integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair to enhance the robustness and efficiency of embodied agent policies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.",
            "score": 3,
            "issue_id": 5212,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "5b4c4a212eae58d6",
            "authors": [
                "Yibin Liu",
                "Zhixuan Liang",
                "Zanxin Chen",
                "Tianxing Chen",
                "Mengkang Hu",
                "Wanxi Dong",
                "Congsheng Xu",
                "Zhaoming Han",
                "Yusen Qin",
                "Yao Mu"
            ],
            "affiliations": [
                "D-Robotics",
                "HKU MMLab",
                "NEU",
                "SJTU ScaleLab",
                "SUSTech",
                "SZU",
                "Shanghai AI Lab",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02629.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#optimization",
                    "#agents",
                    "#reasoning",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "HyCodePolicy - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ĞµĞµ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. HyCodePolicy ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Robots with Self-Correcting Code Synthesis",
                    "desc": "HyCodePolicy is a novel framework that enhances the performance of embodied agents by integrating several advanced techniques. It combines code synthesis, geometric grounding, perceptual monitoring, and iterative repair to create a robust programming cycle. The system translates natural language instructions into executable programs, monitors their execution, and identifies failures using a vision-language model. This approach allows for self-correcting capabilities in program synthesis, improving both the efficiency and reliability of robotic manipulation tasks."
                },
                "zh": {
                    "title": "è‡ªæˆ‘ä¿®æ­£çš„æ™ºèƒ½ä½“ç¼–ç¨‹ç­–ç•¥",
                    "desc": "HyCodePolicy æ˜¯ä¸€ç§æ··åˆè¯­è¨€æ§åˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå…·èº«æ™ºèƒ½ä½“ç­–ç•¥çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†ä»£ç åˆæˆã€å‡ ä½•åŸºç¡€ã€æ„ŸçŸ¥ç›‘æ§å’Œè¿­ä»£ä¿®å¤æ•´åˆåˆ°ä¸€ä¸ªé—­ç¯ç¼–ç¨‹å‘¨æœŸä¸­ï¼Œæ¥å®ç°è‡ªæˆ‘ä¿®æ­£çš„ç¨‹åºåˆæˆã€‚å®ƒé¦–å…ˆå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œå¹¶ç”ŸæˆåŸºäºå¯¹è±¡ä¸­å¿ƒå‡ ä½•åŸè¯­çš„å¯æ‰§è¡Œç¨‹åºã€‚é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç›‘æ§æ‰§è¡Œè¿‡ç¨‹ï¼ŒHyCodePolicy èƒ½å¤Ÿæ£€æµ‹æ‰§è¡Œå¤±è´¥å¹¶è¿›è¡Œä¿®å¤ï¼Œä»è€Œæé«˜æœºå™¨äººæ“ä½œç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02079",
            "title": "AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization",
            "url": "https://huggingface.co/papers/2508.02079",
            "abstract": "AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models (LLMs). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian overlap -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.",
            "score": 2,
            "issue_id": 5198,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "25a555fc0f91f562",
            "authors": [
                "Amitava Das",
                "Abhilekh Borah",
                "Vinija Jain",
                "Aman Chadha"
            ],
            "affiliations": [
                "Amazon AI, USA",
                "BITS Goa, India",
                "Manipal University, India",
                "Meta AI, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02079.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "AlignGuard-LoRA (AGL) - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. AGL Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¤Ğ¸ÑˆĞµÑ€Ğ° Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ»Ğ¸Ğ·Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AGL ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´Ñ€ĞµĞ¹Ñ„ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ 50% Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Preserving Alignment in Fine-Tuning with AGL",
                    "desc": "AlignGuard-LoRA (AGL) is a new framework designed to maintain the alignment of large language models during the fine-tuning process. It introduces regularization techniques that help prevent alignment drift, which can occur even with small updates in low-rank adaptation (LoRA). AGL employs a primary task loss for supervision and uses the Fisher Information Matrix to limit updates in sensitive areas, ensuring that safety and behavioral constraints remain intact. Additionally, it features a diagnostic benchmark called DriftCaps to measure alignment drift and safety, demonstrating significant improvements in maintaining alignment without sacrificing performance on other tasks."
                },
                "zh": {
                    "title": "AlignGuard-LoRAï¼šä¿æŒå¯¹é½ï¼Œç¡®ä¿å®‰å…¨",
                    "desc": "AlignGuard-LoRA (AGL) æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥æ­£åˆ™åŒ–æŠ€æœ¯å’Œè¯Šæ–­åŸºå‡†æ¥ä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„å¯¹é½æ€§ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åœ¨æ›´æ–°è¿‡ç¨‹ä¸­å¯èƒ½å¯¼è‡´çš„å¯¹é½æ¼‚ç§»é—®é¢˜ï¼Œä»è€Œå¢å¼ºå®‰å…¨æ€§å’Œè¡Œä¸ºçº¦æŸã€‚AGL é‡‡ç”¨äº†å¤šç§å…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬åŸºäºè´¹èˆå°”ä¿¡æ¯çŸ©é˜µçš„æ­£åˆ™åŒ–å’Œä»»åŠ¡ç‰¹å®šçš„æ­£åˆ™åŒ–ï¼Œä»¥ç¨³å®šæ–°çŸ¥è¯†çš„æ•´åˆã€‚å®éªŒè¯æ˜ï¼ŒAGL èƒ½å¤Ÿåœ¨ä¸é™ä½ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå°†å¯¹é½æ¼‚ç§»å‡å°‘å¤šè¾¾ 50%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02630",
            "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce",
            "url": "https://huggingface.co/papers/2508.02630",
            "abstract": "ACES, a sandbox environment, studies AI agents' shopping behavior in a mock marketplace, revealing position effects, sensitivity to sponsored tags, endorsements, prices, ratings, and reviews, and highlighting implications for seller strategies and platform design.  \t\t\t\t\tAI-generated summary \t\t\t\t Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, vision-language-model (VLM) agents can parse webpages, evaluate products, and transact. This raises a fundamental question: what do AI agents buy, and why? We develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent with a fully programmable mock marketplace to study this question. We first conduct basic rationality checks in the context of simple tasks, and then, by randomizing product positions, prices, ratings, reviews, sponsored tags, and platform endorsements, we obtain causal estimates of how frontier VLMs actually shop. Models show strong but heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal \"top\" rank. They penalize sponsored tags and reward endorsements. Sensitivities to price, ratings, and reviews are directionally human-like but vary sharply in magnitude across models. Motivated by scenarios where sellers use AI agents to optimize product listings, we show that a seller-side agent that makes minor tweaks to product descriptions, targeting AI buyer preferences, can deliver substantial market-share gains if AI-mediated shopping dominates. We also find that modal product choices can differ across models and, in some cases, demand may concentrate on a few select products, raising competition questions. Together, our results illuminate how AI agents may behave in e-commerce settings and surface concrete seller strategy, platform design, and regulatory questions in an AI-mediated ecosystem.",
            "score": 1,
            "issue_id": 5215,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "2860dd8bc6a41f92",
            "authors": [
                "Amine Allouah",
                "Omar Besbes",
                "JosuÃ© D Figueroa",
                "Yash Kanoria",
                "Akshit Kumar"
            ],
            "affiliations": [
                "Columbia University, Graduate School of Business",
                "MyCustomAI",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02630.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#ethics",
                    "#games",
                    "#alignment"
                ],
                "emoji": "ğŸ›’",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ¸Ğ´ĞµÑ‚ Ğ·Ğ° Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ°Ğ¼Ğ¸: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ACES Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ĞºÑƒĞ¿Ğ¾Ğº Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ², ÑĞ¿Ğ¾Ğ½ÑĞ¾Ñ€ÑĞºĞ¸Ñ… Ñ‚ĞµĞ³Ğ¾Ğ², Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ñ†ĞµĞ½, Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ˜Ğ˜-Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº ÑÑ‚Ğ¸Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑÑ… Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ñ†Ğ¾Ğ², Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ, Ğ³Ğ´Ğµ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Understanding AI Agents in E-Commerce: Insights from ACES",
                    "desc": "The paper introduces ACES, a sandbox environment designed to analyze the shopping behavior of AI agents in a simulated marketplace. It investigates how these agents respond to various factors such as product position, pricing, ratings, and endorsements, revealing that their preferences can differ significantly from human shoppers. The study finds that while AI agents generally prefer higher-ranked products, their specific choices can vary based on the model used, challenging the idea of a single 'top' product. The findings suggest that sellers can optimize their listings to cater to AI preferences, potentially reshaping strategies in e-commerce as AI-driven shopping becomes more prevalent."
                },
                "zh": {
                    "title": "æ¢ç´¢AIä»£ç†åœ¨ç”µå•†ä¸­çš„è´­ç‰©è¡Œä¸º",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†äººå·¥æ™ºèƒ½ä»£ç†åœ¨æ¨¡æ‹Ÿå¸‚åœºä¸­çš„è´­ç‰©è¡Œä¸ºï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºACESçš„æ²™ç›’ç¯å¢ƒã€‚é€šè¿‡éšæœºåŒ–äº§å“ä½ç½®ã€ä»·æ ¼ã€è¯„åˆ†ã€è¯„è®ºå’ŒèµåŠ©æ ‡ç­¾ï¼Œç ”ç©¶äº†ä¸åŒæ¨¡å‹åœ¨è´­ç‰©æ—¶çš„å› æœå…³ç³»å’Œåå¥½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒAIä»£ç†å¯¹äº§å“ä½ç½®æœ‰æ˜æ˜¾çš„åå¥½ï¼Œä½†ä¸åŒæ¨¡å‹çš„é€‰æ‹©å·®å¼‚å¾ˆå¤§ï¼Œä¸”å¯¹ä»·æ ¼å’Œè¯„åˆ†çš„æ•æ„Ÿåº¦ä¸äººç±»ç›¸ä¼¼ä½†å¹…åº¦ä¸åŒã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå–æ–¹å¯ä»¥é€šè¿‡ä¼˜åŒ–äº§å“æè¿°æ¥å¸å¼•AIä¹°å®¶çš„åå¥½ï¼Œä»è€Œåœ¨å¸‚åœºä¸­è·å¾—æ˜¾è‘—çš„ä»½é¢æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02455",
            "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs",
            "url": "https://huggingface.co/papers/2508.02455",
            "abstract": "A new scoring approach using language models ranks static code completions in IDEs by organizing them into a prefix tree and performing a single greedy decoding pass.  \t\t\t\t\tAI-generated summary \t\t\t\t Token-level code completion is one of the most critical features in modern Integrated Development Environments (IDEs). It assists developers by suggesting relevant identifiers and APIs during coding. While completions are typically derived from static analysis, their usefulness depends heavily on how they are ranked, as correct predictions buried deep in the list are rarely seen by users. Most current systems rely on hand-crafted heuristics or lightweight machine learning models trained on user logs, which can be further improved to capture context information and generalize across projects and coding styles. In this work, we propose a new scoring approach to ranking static completions using language models in a lightweight and model-agnostic way. Our method organizes all valid completions into a prefix tree and performs a single greedy decoding pass to collect token-level scores across the tree. This enables a precise token-aware ranking without needing beam search, prompt engineering, or model adaptations. The approach is fast, architecture-agnostic, and compatible with already deployed models for code completion. These findings highlight a practical and effective pathway for integrating language models into already existing tools within IDEs, and ultimately providing smarter and more responsive developer assistance.",
            "score": 1,
            "issue_id": 5215,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "70799804b6937703",
            "authors": [
                "Daniele Cipollone",
                "Egor Bogomolov",
                "Arie van Deursen",
                "Maliheh Izadi"
            ],
            "affiliations": [
                "Delft University of Technology, Delft, Netherlands",
                "JetBrains, Amsterdam, Netherlands"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02455.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ´Ğ° Ğ² IDE Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ½Ğ¾Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¶Ğ°Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ»ÑƒÑ‡ĞµĞ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹, Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ ÑƒĞ¶Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Smart Code Completion with Language Models",
                    "desc": "This paper introduces a novel method for ranking code completions in Integrated Development Environments (IDEs) using language models. The approach organizes potential code completions into a prefix tree, allowing for efficient scoring of completions through a single greedy decoding pass. This method enhances the ranking of static code completions by providing a token-aware scoring system without the need for complex techniques like beam search or prompt engineering. The proposed solution is lightweight, model-agnostic, and can be easily integrated into existing IDEs, improving developer assistance significantly."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä»£ç è¡¥å…¨çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„åˆ†æ–¹æ³•ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹å¯¹é™æ€ä»£ç è¡¥å…¨è¿›è¡Œæ’åã€‚è¯¥æ–¹æ³•å°†æ‰€æœ‰æœ‰æ•ˆçš„è¡¥å…¨ç»„ç»‡æˆå‰ç¼€æ ‘ï¼Œå¹¶é€šè¿‡å•æ¬¡è´ªå©ªè§£ç æ¥æ”¶é›†æ¯ä¸ªæ ‡è®°çš„åˆ†æ•°ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ‰‹å·¥å¯å‘å¼æˆ–è½»é‡çº§æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒé¡¹ç›®å’Œç¼–ç é£æ ¼ä¸­è¿›è¡Œæ³›åŒ–ã€‚æœ€ç»ˆï¼Œè¿™ç§å¿«é€Ÿä¸”ä¸æ¨¡å‹æ— å…³çš„æ–¹æ³•ä¸ºé›†æˆè¯­è¨€æ¨¡å‹åˆ°ç°æœ‰IDEå·¥å…·ä¸­æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02063",
            "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs",
            "url": "https://huggingface.co/papers/2508.02063",
            "abstract": "TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) fine-tuned to align with human values often exhibit alignment drift, producing unsafe or policy-violating completions when exposed to adversarial prompts, decoding perturbations, or paraphrased jailbreaks. While prior work has behaviorally characterized alignment failure, little is known about the training-time belief sources underlying these failures. We introduce TraceAlign, a unified framework for tracing unsafe completions back to their root causes in the model's training corpus. Central to our approach is the Belief Conflict Index (BCI), which quantifies semantic inconsistency between generated spans and aligned policies, based on retrieved training documents using suffix-array matching. We propose three complementary interventions: (i) TraceShield, an inference-time safety filter that refuses completions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a contrastive fine-tuning objective penalizing high-BCI continuations during DPO, and (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam expansions predicted to yield high-BCI spans. Together, these defenses reduce alignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB) while preserving utility on standard tasks, with delta less than 0.2 and improved refusal quality. We further derive a theoretical upper bound on drift likelihood via suffix-array span statistics, linking memorization frequency and length to adversarial reactivation risk. TraceAlign thus provides the first scalable, traceable, and grounded toolkit for understanding and mitigating alignment failures at source. To encourage further exploration and development, we open-source our implementation at: https://anonymous.4open.science/r/tracealign-2DA7",
            "score": 1,
            "issue_id": 5198,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "0b136776fbb19b26",
            "authors": [
                "Amitava Das",
                "Vinija Jain",
                "Aman Chadha"
            ],
            "affiliations": [
                "Amazon GenAI",
                "BITS Pilani Goa",
                "Meta AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02063.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#rlhf",
                    "#alignment",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "TraceAlign: Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "TraceAlign - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ˜Ğ½Ğ´ĞµĞºÑ ĞšĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ£Ğ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (BCI), ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹: Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ TraceShield, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Prov-Decode."
                },
                "en": {
                    "title": "TraceAlign: Bridging the Gap in LLM Alignment",
                    "desc": "TraceAlign is a novel framework designed to address alignment drift in Large Language Models (LLMs), which occurs when these models generate unsafe outputs that deviate from human values. It traces these unsafe completions back to their origins in the training data, allowing for a better understanding of the underlying causes of alignment failures. The framework introduces the Belief Conflict Index (BCI) to measure inconsistencies between generated outputs and aligned policies, and proposes three interventions to mitigate drift. By implementing these strategies, TraceAlign significantly reduces alignment drift while maintaining the model's performance on standard tasks."
                },
                "zh": {
                    "title": "TraceAlignï¼šå‡è½»å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ¼‚ç§»çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "TraceAlignæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè¯†åˆ«å’Œå‡è½»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¯¹é½æ¼‚ç§»ã€‚å®ƒé€šè¿‡è¿½è¸ªä¸å®‰å…¨çš„ç”Ÿæˆç»“æœåˆ°å…¶è®­ç»ƒæ¥æºï¼Œå¹¶åº”ç”¨å¹²é¢„æªæ–½æ¥å‡å°‘æ¼‚ç§»ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¿¡å¿µå†²çªæŒ‡æ•°ï¼ˆBCIï¼‰ï¼Œé‡åŒ–ç”Ÿæˆå†…å®¹ä¸å¯¹é½æ”¿ç­–ä¹‹é—´çš„è¯­ä¹‰ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡ä¸‰ç§äº’è¡¥çš„å¹²é¢„æªæ–½ï¼ŒTraceAlignèƒ½å¤Ÿåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½å¯¹é½æ¼‚ç§»çš„å‘ç”Ÿç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01126",
            "title": "UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,\n  Forecasting, and Generation",
            "url": "https://huggingface.co/papers/2508.01126",
            "abstract": "A unified conditional motion diffusion model, UniEgoMotion, is introduced for egocentric motion generation and forecasting using first-person images, achieving state-of-the-art performance and generating motion from a single image.  \t\t\t\t\tAI-generated summary \t\t\t\t Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.",
            "score": 0,
            "issue_id": 5212,
            "pub_date": "2025-08-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 2",
                "zh": "8æœˆ2æ—¥"
            },
            "hash": "8f30093d889bde3a",
            "authors": [
                "Chaitanya Patel",
                "Hiroki Nakamura",
                "Yuta Kyuragi",
                "Kazuki Kozuka",
                "Juan Carlos Niebles",
                "Ehsan Adeli"
            ],
            "affiliations": [
                "Panasonic Holdings Corporation",
                "Panasonic R&D Company of America",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01126.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#benchmark",
                    "#video",
                    "#diffusion",
                    "#multimodal",
                    "#cv",
                    "#healthcare"
                ],
                "emoji": "ğŸ•¶ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ UniEgoMotion Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. UniEgoMotion Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… EE4D-Motion Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ EgoExo4D Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Egocentric Motion with UniEgoMotion",
                    "desc": "The paper presents UniEgoMotion, a unified conditional motion diffusion model designed for generating and forecasting egocentric motion from first-person images. This model addresses the limitations of existing methods that focus on third-person perspectives and structured 3D scenes, which are not effective in real-world scenarios with dynamic cameras and occlusions. UniEgoMotion utilizes a novel head-centric motion representation and effectively extracts scene context from images to predict plausible 3D motion. The introduction of the EE4D-Motion dataset, which includes pseudo-ground-truth 3D motion annotations, supports the training of this model, achieving state-of-the-art results in egocentric motion tasks."
                },
                "zh": {
                    "title": "è‡ªæˆ‘ä¸­å¿ƒè¿åŠ¨ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»Ÿä¸€çš„æ¡ä»¶è¿åŠ¨æ‰©æ•£æ¨¡å‹UniEgoMotionï¼Œç”¨äºä»ç¬¬ä¸€äººç§°å›¾åƒç”Ÿæˆå’Œé¢„æµ‹è‡ªæˆ‘ä¸­å¿ƒçš„è¿åŠ¨ã€‚è¯¥æ¨¡å‹åœ¨è‡ªæˆ‘ä¸­å¿ƒè¿åŠ¨é‡å»ºå’Œé¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿä»…ä»å•å¼ å›¾åƒç”Ÿæˆè¿åŠ¨ã€‚UniEgoMotioné‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„å¤´éƒ¨ä¸­å¿ƒè¿åŠ¨è¡¨ç¤ºï¼Œæ”¯æŒåœ¨æ²¡æœ‰æ˜ç¡®3Dåœºæ™¯çš„æƒ…å†µä¸‹è¿›è¡Œåœºæ™¯æ„ŸçŸ¥çš„è¿åŠ¨åˆæˆã€‚é€šè¿‡å¼•å…¥EE4D-Motionæ•°æ®é›†ï¼Œæœ¬æ–‡ä¸ºè®­ç»ƒæä¾›äº†ä¸°å¯Œçš„ä¼ªçœŸå®3Dè¿åŠ¨æ³¨é‡Šï¼Œæ¨åŠ¨äº†è‡ªæˆ‘ä¸­å¿ƒè¿åŠ¨å»ºæ¨¡çš„æ–°æ ‡å‡†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-05.html",
    "link_next": "2025-08-07.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "05.08",
        "en": "08/05",
        "zh": "8æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "07.08",
        "en": "08/07",
        "zh": "8æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 10,
        "#agents": 4,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 1,
        "#agi": 0,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}