{
    "date": {
        "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 16",
        "zh": "9æœˆ16æ—¥"
    },
    "time_utc": "2025-09-16 04:13",
    "weekday": 1,
    "issue_id": 5908,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.11543",
            "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.11543",
            "abstract": "Semi-online Reinforcement Learning addresses the limitations of offline and online RL by simulating online RL on offline trajectories, achieving state-of-the-art performance in dynamic benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours Semi-online RL achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.",
            "score": 23,
            "issue_id": 5907,
            "pub_date": "2025-09-15",
            "pub_date_card": {
                "ru": "15 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 15",
                "zh": "9æœˆ15æ—¥"
            },
            "hash": "dcbb6b99868e170d",
            "authors": [
                "Zhengxi Lu",
                "Jiabo Ye",
                "Fei Tang",
                "Yongliang Shen",
                "Haiyang Xu",
                "Ziwei Zheng",
                "Weiming Lu",
                "Ming Yan",
                "Fei Huang",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.11543.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#games",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸĞ¾Ğ»ÑƒĞ¾Ğ½Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ - Ğ¿Ğ¾Ğ»ÑƒĞ¾Ğ½Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (Semi-online Reinforcement Learning). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½- Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Patch Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Bridging Offline and Online RL for Superior Performance",
                    "desc": "Semi-online Reinforcement Learning (RL) is a new approach that combines the strengths of offline and online RL to improve performance in dynamic environments. It simulates online RL using pre-collected offline trajectories, allowing for stable training while addressing the challenges of multi-step task execution. The method incorporates a Patch Module to align the model's outputs with expert trajectories and uses discounted future returns to enhance reward computation. Experiments show that this approach significantly outperforms existing models, achieving state-of-the-art results in various benchmarks."
                },
                "zh": {
                    "title": "åŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼šè¿æ¥ç¦»çº¿æ•ˆç‡ä¸åœ¨çº¿æ¨ç†çš„æ¡¥æ¢",
                    "desc": "åŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆSemi-online Reinforcement Learningï¼‰è§£å†³äº†ç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„å±€é™æ€§ï¼Œé€šè¿‡åœ¨ç¦»çº¿è½¨è¿¹ä¸Šæ¨¡æ‹Ÿåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œè¾¾åˆ°äº†åŠ¨æ€åŸºå‡†æµ‹è¯•ä¸­çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨æ¯æ¬¡å›åˆè¿‡ç¨‹ä¸­ä¿ç•™äº†å¤šè½®å¯¹è¯ä¸­çš„åŸå§‹æ¨¡å‹è¾“å‡ºï¼Œå¹¶é€šè¿‡è¡¥ä¸æ¨¡å—è‡ªé€‚åº”åœ°æ¢å¤å›åˆä¸ä¸“å®¶è½¨è¿¹ä¹‹é—´çš„å·®å¼‚ã€‚ä¸ºäº†æ•æ‰é•¿æœŸè®­ç»ƒä¿¡å·ï¼ŒåŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ åœ¨å¥–åŠ±è®¡ç®—ä¸­å¼•å…¥äº†æŠ˜æ‰£æœªæ¥æ”¶ç›Šï¼Œå¹¶ä½¿ç”¨åŠ æƒçš„æ­¥éª¤çº§å’Œå›åˆçº§ä¼˜åŠ¿æ¥ä¼˜åŒ–ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ åœ¨å››ä¸ªåŠ¨æ€åŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼ŒæˆåŠŸç¼©å°äº†ç¦»çº¿è®­ç»ƒæ•ˆç‡ä¸åœ¨çº¿å¤šè½®æ¨ç†ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.12201",
            "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
            "url": "https://huggingface.co/papers/2509.12201",
            "abstract": "OmniWorld, a large-scale, multi-domain, multi-modal dataset, addresses the limitations of existing 4D world modeling datasets and benchmarks, enabling significant performance improvements in 4D reconstruction and video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.",
            "score": 9,
            "issue_id": 5907,
            "pub_date": "2025-09-15",
            "pub_date_card": {
                "ru": "15 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 15",
                "zh": "9æœˆ15æ—¥"
            },
            "hash": "6236e5a1b21f6911",
            "authors": [
                "Yang Zhou",
                "Yifan Wang",
                "Jianjun Zhou",
                "Wenzheng Chang",
                "Haoyu Guo",
                "Zizun Li",
                "Kaijing Ma",
                "Xinyue Li",
                "Yating Wang",
                "Haoyi Zhu",
                "Mingyu Liu",
                "Dingning Liu",
                "Jiange Yang",
                "Zhoujie Fu",
                "Junyi Chen",
                "Chunhua Shen",
                "Jiangmiao Pang",
                "Kaipeng Zhang",
                "Tong He"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.12201.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#multimodal",
                    "#dataset",
                    "#video",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "OmniWorld: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ 4D-Ğ¼Ğ¸Ñ€Ğ¾Ğ²",
                    "desc": "OmniWorld - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 4D-Ğ¼Ğ¸Ñ€Ğ¾Ğ². ĞĞ½ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. OmniWorld Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… OmniWorld-Game Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ OmniWorld Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "OmniWorld: Revolutionizing 4D World Modeling with Rich Data",
                    "desc": "OmniWorld is a comprehensive dataset designed to enhance 4D world modeling, which combines spatial and temporal data. It addresses the shortcomings of current datasets by providing diverse, high-quality data that includes dynamic interactions and multi-domain scenarios. The dataset supports critical tasks like 4D geometric reconstruction and video generation, allowing for better training of machine learning models. By benchmarking existing state-of-the-art methods on OmniWorld, significant performance improvements are observed, showcasing its potential to advance the field of 4D modeling."
                },
                "zh": {
                    "title": "OmniWorldï¼šæ¨åŠ¨4Dä¸–ç•Œå»ºæ¨¡çš„æ–°åŠ¨åŠ›",
                    "desc": "OmniWorldæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šé¢†åŸŸå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰4Dä¸–ç•Œå»ºæ¨¡æ•°æ®é›†çš„å±€é™æ€§ã€‚å®ƒæ”¯æŒ4Då‡ ä½•é‡å»ºå’Œè§†é¢‘ç”Ÿæˆç­‰å…³é”®ä»»åŠ¡ï¼Œæä¾›äº†ä¸°å¯Œçš„åŠ¨æ€å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡å¼•å…¥OmniWorld-Gameæ•°æ®é›†ï¼ŒOmniWorldåœ¨æ¨¡æ€è¦†ç›–ã€è§„æ¨¡å’ŒåŠ¨æ€äº¤äº’æ–¹é¢ä¼˜äºç°æœ‰åˆæˆæ•°æ®é›†ã€‚æˆ‘ä»¬å¸Œæœ›OmniWorldèƒ½åŠ é€Ÿé€šç”¨4Dä¸–ç•Œæ¨¡å‹çš„å‘å±•ï¼Œæå‡æœºå™¨å¯¹ç‰©ç†ä¸–ç•Œçš„æ•´ä½“ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.12203",
            "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion\n  Transformers via Explicit Correspondence",
            "url": "https://huggingface.co/papers/2509.12203",
            "abstract": "LazyDrag, a drag-based image editing method for Multi-Modal Diffusion Transformers, eliminates implicit point matching, enabling precise geometric control and text guidance without test-time optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.",
            "score": 4,
            "issue_id": 5907,
            "pub_date": "2025-09-15",
            "pub_date_card": {
                "ru": "15 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 15",
                "zh": "9æœˆ15æ—¥"
            },
            "hash": "5c030994bd1fdc14",
            "authors": [
                "Zixin Yin",
                "Xili Dai",
                "Duomin Wang",
                "Xianfang Zeng",
                "Lionel M. Ni",
                "Gang Yu",
                "Heung-Yeung Shum"
            ],
            "affiliations": [
                "StepFun",
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.12203.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "LazyDrag: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "LazyDrag - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. LazyDrag Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾Ğ¿Ğ¾Ñ€Ñƒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "LazyDrag: Revolutionizing Image Editing with Precision and Control",
                    "desc": "LazyDrag is a novel drag-based image editing technique designed for Multi-Modal Diffusion Transformers that removes the need for implicit point matching, which has been a major limitation in previous methods. By generating an explicit correspondence map from user inputs, LazyDrag enhances attention control and allows for a more robust inversion process without the need for test-time optimization. This advancement enables users to perform complex edits, such as inpainting and generating new objects, with greater precision and flexibility. The method has been shown to outperform existing techniques in terms of drag accuracy and perceptual quality, setting a new standard in the field of image editing."
                },
                "zh": {
                    "title": "LazyDragï¼šç²¾ç¡®æ§åˆ¶ä¸æ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "LazyDragæ˜¯ä¸€ç§åŸºäºæ‹–åŠ¨çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œä¸“ä¸ºå¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨è®¾è®¡ã€‚å®ƒæ¶ˆé™¤äº†å¯¹éšå¼ç‚¹åŒ¹é…çš„ä¾èµ–ï¼Œä»è€Œå®ç°äº†ç²¾ç¡®çš„å‡ ä½•æ§åˆ¶å’Œæ–‡æœ¬æŒ‡å¯¼ï¼Œè€Œæ— éœ€åœ¨æµ‹è¯•æ—¶è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡ç”Ÿæˆç”¨æˆ·æ‹–åŠ¨è¾“å…¥çš„æ˜¾å¼å¯¹åº”å›¾ï¼ŒLazyDragæé«˜äº†æ³¨æ„åŠ›æ§åˆ¶çš„å¯é æ€§ï¼Œæ”¯æŒå¤æ‚çš„ç¼–è¾‘ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åœ¨DragBenchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLazyDragåœ¨æ‹–åŠ¨ç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œç¡®ç«‹äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10884",
            "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes",
            "url": "https://huggingface.co/papers/2509.10884",
            "abstract": "Nav-R1, an embodied foundation model, enhances navigation by integrating structured reasoning and decoupled control mechanisms, outperforming existing approaches on benchmarks and in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1.",
            "score": 3,
            "issue_id": 5907,
            "pub_date": "2025-09-13",
            "pub_date_card": {
                "ru": "13 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 13",
                "zh": "9æœˆ13æ—¥"
            },
            "hash": "c1cc764cd16892f2",
            "authors": [
                "Qingxiang Liu",
                "Ting Huang",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai University of Engineering Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10884.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#rl",
                    "#agents",
                    "#reasoning",
                    "#dataset",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Nav-R1: Ğ˜Ğ˜-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Nav-R1 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. Nav-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Navigation with Structured Reasoning and Decoupled Control",
                    "desc": "Nav-R1 is an advanced embodied foundation model designed to improve navigation in complex 3D environments by combining structured reasoning with decoupled control mechanisms. It addresses common issues in existing navigation systems, such as unstable reasoning and the challenge of balancing long-term planning with quick responses. The model utilizes a large dataset called Nav-CoT-110K, which provides step-by-step reasoning examples for better initialization and learning. By implementing a unique Fast-in-Slow reasoning approach and a GRPO-based reinforcement learning framework, Nav-R1 achieves significant performance improvements in both simulated benchmarks and real-world applications."
                },
                "zh": {
                    "title": "Nav-R1ï¼šæ™ºèƒ½å¯¼èˆªçš„æ–°çºªå…ƒ",
                    "desc": "Nav-R1æ˜¯ä¸€ç§å…·èº«åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡æ•´åˆç»“æ„åŒ–æ¨ç†å’Œè§£è€¦æ§åˆ¶æœºåˆ¶æ¥å¢å¼ºå¯¼èˆªèƒ½åŠ›ã€‚å®ƒè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤æ‚3Dç¯å¢ƒä¸­æ¨ç†ä¸è¿è´¯å’Œä¸ç¨³å®šçš„é—®é¢˜ï¼Œæå‡äº†åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†Nav-CoT-110Kæ•°æ®é›†ï¼Œæ”¯æŒåŸºäºæ­¥éª¤çš„æ¨ç†ï¼Œå¹¶é‡‡ç”¨äº†åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†æ ¼å¼ã€ç†è§£å’Œå¯¼èˆªä¸‰ç§å¥–åŠ±æœºåˆ¶ã€‚é€šè¿‡å¿«é€Ÿä¸æ…¢é€Ÿæ¨ç†çš„è§£è€¦ï¼ŒNav-R1å®ç°äº†é«˜æ•ˆä¸”è¿è´¯çš„å¯¼èˆªï¼Œåœ¨åŸºå‡†æµ‹è¯•å’Œå®é™…åº”ç”¨ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09672",
            "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
            "url": "https://huggingface.co/papers/2509.09672",
            "abstract": "Research shows that locality in deep diffusion models is a statistical property of image datasets rather than an inductive bias of convolutional neural networks, leading to the development of a more accurate analytical denoiser.  \t\t\t\t\tAI-generated summary \t\t\t\t Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.",
            "score": 3,
            "issue_id": 5908,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "80381c11a7c56ebb",
            "authors": [
                "Artem Lukoianov",
                "Chenyang Yuan",
                "Justin Solomon",
                "Vincent Sitzmann"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Toyota Research Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09672.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#cv",
                    "#diffusion",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ½Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·ĞµÑ€ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·ĞµÑ€Ğ°Ğ¼Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."
                },
                "en": {
                    "title": "Locality in Deep Diffusion: A Dataset Property, Not a Network Bias!",
                    "desc": "This paper investigates the relationship between locality in deep diffusion models and the statistical properties of image datasets. It argues that the observed locality is not a result of the inductive biases inherent in convolutional neural networks, but rather a characteristic of the datasets themselves. The authors demonstrate that an optimal linear denoiser can replicate the locality properties found in deep neural denoisers, suggesting that pixel correlations in natural images drive this phenomenon. By leveraging these findings, they develop a new analytical denoiser that outperforms previous models in aligning with the scores from deep diffusion models."
                },
                "zh": {
                    "title": "æ­ç¤ºæ·±åº¦æ‰©æ•£æ¨¡å‹çš„å±€éƒ¨æ€§æœ¬è´¨",
                    "desc": "æœ¬ç ”ç©¶è¡¨æ˜ï¼Œæ·±åº¦æ‰©æ•£æ¨¡å‹ä¸­çš„å±€éƒ¨æ€§æ˜¯å›¾åƒæ•°æ®é›†çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œè€Œä¸æ˜¯å·ç§¯ç¥ç»ç½‘ç»œçš„å½’çº³åç½®ã€‚è¿™ä¸€å‘ç°ä¿ƒä½¿æˆ‘ä»¬å¼€å‘å‡ºä¸€ç§æ›´å‡†ç¡®çš„åˆ†æå»å™ªå™¨ã€‚æˆ‘ä»¬è¯æ˜äº†æœ€ä¼˜å‚æ•°çº¿æ€§å»å™ªå™¨ä¸æ·±åº¦ç¥ç»å»å™ªå™¨å…·æœ‰ç›¸ä¼¼çš„å±€éƒ¨æ€§ç‰¹å¾ï¼Œå¹¶ä¸”è¿™ç§å±€éƒ¨æ€§ç›´æ¥æºäºè‡ªç„¶å›¾åƒæ•°æ®é›†ä¸­çš„åƒç´ ç›¸å…³æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™äº›è§è§£è®¾è®¡äº†ä¸€ç§åˆ†æå»å™ªå™¨ï¼Œå…¶æ€§èƒ½ä¼˜äºä¹‹å‰çš„ä¸“å®¶è®¾è®¡æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.11444",
            "title": "CognitiveSky: Scalable Sentiment and Narrative Analysis for\n  Decentralized Social Media",
            "url": "https://huggingface.co/papers/2509.11444",
            "abstract": "CognitiveSky, a transformer-based framework, analyzes sentiment, emotion, and narratives on Bluesky, providing insights through a dynamic dashboard and supporting various applications in computational social science.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or X.com alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.",
            "score": 2,
            "issue_id": 5906,
            "pub_date": "2025-09-14",
            "pub_date_card": {
                "ru": "14 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 14",
                "zh": "9æœˆ14æ—¥"
            },
            "hash": "a5f0549d84adb9a5",
            "authors": [
                "Gaurab Chhetri",
                "Anandi Dutta",
                "Subasish Das"
            ],
            "affiliations": [
                "Department of Computer Science Texas State University San Marcos, Texas, USA",
                "Ingram School of Engineering Texas State University San Marcos, Texas, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.11444.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#healthcare",
                    "#open_source",
                    "#dataset",
                    "#multimodal",
                    "#science"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ñ†ÑĞµÑ‚ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "CognitiveSky - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹, ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Bluesky. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ API Bluesky Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. CognitiveSky Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ğ½ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ñ†Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸."
                },
                "en": {
                    "title": "CognitiveSky: Transforming Social Media Insights with AI",
                    "desc": "CognitiveSky is a transformer-based framework that analyzes sentiment, emotion, and narratives on the decentralized social media platform Bluesky. It utilizes Bluesky's API to gather user-generated content and applies advanced machine learning models to extract meaningful insights. The framework features a dynamic dashboard that visualizes trends in public discourse, making it useful for various applications in computational social science. Its open-source nature and low operational costs enhance accessibility for researchers and practitioners in fields like mental health monitoring and disinformation detection."
                },
                "zh": {
                    "title": "CognitiveSkyï¼šè§£è¯»Blueskyçš„æƒ…æ„Ÿä¸å™äº‹",
                    "desc": "CognitiveSkyæ˜¯ä¸€ä¸ªåŸºäºå˜æ¢å™¨çš„æ¡†æ¶ï¼Œä¸“æ³¨äºåˆ†æBlueskyå¹³å°ä¸Šçš„æƒ…æ„Ÿã€æƒ…ç»ªå’Œå™äº‹ã€‚å®ƒé€šè¿‡Blueskyçš„APIè·å–æ•°æ®ï¼Œåˆ©ç”¨å˜æ¢å™¨æ¨¡å‹å¯¹ç”¨æˆ·ç”Ÿæˆçš„å†…å®¹è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„å¯åˆ†æè¾“å‡ºã€‚è¯¥æ¡†æ¶æä¾›ä¸€ä¸ªåŠ¨æ€ä»ªè¡¨æ¿ï¼Œå®æ—¶å¯è§†åŒ–æƒ…æ„Ÿã€æ´»åŠ¨å’Œè¯é¢˜çš„å˜åŒ–æ¨¡å¼ã€‚CognitiveSkyçš„æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶èƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºè™šå‡ä¿¡æ¯æ£€æµ‹ã€å±æœºå“åº”å’Œå…¬æ°‘æƒ…æ„Ÿåˆ†æç­‰é¢†åŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10813",
            "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with\n  Realistic Layouts",
            "url": "https://huggingface.co/papers/2509.10813",
            "abstract": "InternScenes is a large-scale, diverse, and realistic indoor scene dataset that addresses limitations in existing datasets, enabling better scene layout generation and point-goal navigation.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce InternScenes, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community.",
            "score": 2,
            "issue_id": 5907,
            "pub_date": "2025-09-13",
            "pub_date_card": {
                "ru": "13 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 13",
                "zh": "9æœˆ13æ—¥"
            },
            "hash": "b7614d46b6b62960",
            "authors": [
                "Weipeng Zhong",
                "Peizhou Cao",
                "Yichen Jin",
                "Li Luo",
                "Wenzhe Cai",
                "Jingli Lin",
                "Hanqing Wang",
                "Zhaoyang Lyu",
                "Tai Wang",
                "Bo Dai",
                "Xudong Xu",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Beihang University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10813.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#data",
                    "#games",
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Embodied AI",
                    "desc": "InternScenes - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 40 000 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºĞ°Ğ½Ñ‹, Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ°Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 1,96 Ğ¼Ğ»Ğ½ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ĞµĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¼Ğ°ĞºĞµÑ‚Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "InternScenes: A Game-Changer for Indoor Scene Understanding",
                    "desc": "InternScenes is a new dataset designed to improve the training of AI in understanding and navigating indoor environments. It includes around 40,000 diverse scenes with realistic layouts, featuring a wide variety of small objects to enhance complexity. The dataset addresses common issues in existing datasets, such as lack of diversity and object collisions, by using a combination of real-world scans, procedural generation, and designer input. By providing a rich and interactive environment, InternScenes supports advancements in scene layout generation and point-goal navigation tasks for Embodied AI."
                },
                "zh": {
                    "title": "InternScenesï¼šæ¨åŠ¨å®¤å†…åœºæ™¯ç†è§£çš„æœªæ¥",
                    "desc": "InternScenesæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–ä¸”çœŸå®çš„å®¤å†…åœºæ™¯æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†çš„å±€é™æ€§ï¼Œä»è€Œæ”¹å–„åœºæ™¯å¸ƒå±€ç”Ÿæˆå’Œç›®æ ‡å¯¼èˆªã€‚è¯¥æ•°æ®é›†åŒ…å«çº¦40,000ä¸ªå¤šæ ·åŒ–åœºæ™¯ï¼Œæ•´åˆäº†çœŸå®ä¸–ç•Œæ‰«æã€ç¨‹åºç”Ÿæˆåœºæ™¯å’Œè®¾è®¡å¸ˆåˆ›å»ºçš„åœºæ™¯ï¼Œæ¶µç›–äº†1.96ç™¾ä¸‡ä¸ª3Dç‰©ä½“å’Œ15ç§å¸¸è§åœºæ™¯ç±»å‹ã€‚æˆ‘ä»¬ç‰¹åˆ«ä¿ç•™äº†å¤§é‡å°ç‰©å“ï¼Œä½¿å¾—åœºæ™¯å¸ƒå±€æ›´åŠ çœŸå®å’Œå¤æ‚ï¼Œå¹³å‡æ¯ä¸ªåŒºåŸŸæœ‰41.5ä¸ªç‰©ä½“ã€‚InternScenesä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†æ–°çš„æŒ‘æˆ˜ï¼Œå¹¶æ‰¿è¯ºå¼€æºæ•°æ®ã€æ¨¡å‹å’ŒåŸºå‡†ï¼Œä»¥é€ ç¦æ•´ä¸ªç¤¾åŒºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.11648",
            "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI",
            "url": "https://huggingface.co/papers/2509.11648",
            "abstract": "EthicsMH is a dataset of 125 scenarios designed to evaluate AI systems' ethical reasoning in mental health contexts, focusing on decision accuracy, explanation quality, and alignment with professional norms.  \t\t\t\t\tAI-generated summary \t\t\t\t The deployment of large language models (LLMs) in mental health and other sensitive domains raises urgent questions about ethical reasoning, fairness, and responsible alignment. Yet, existing benchmarks for moral and clinical decision-making do not adequately capture the unique ethical dilemmas encountered in mental health practice, where confidentiality, autonomy, beneficence, and bias frequently intersect. To address this gap, we introduce Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios designed to evaluate how AI systems navigate ethically charged situations in therapeutic and psychiatric contexts. Each scenario is enriched with structured fields, including multiple decision options, expert-aligned reasoning, expected model behavior, real-world impact, and multi-stakeholder viewpoints. This structure enables evaluation not only of decision accuracy but also of explanation quality and alignment with professional norms. Although modest in scale and developed with model-assisted generation, EthicsMH establishes a task framework that bridges AI ethics and mental health decision-making. By releasing this dataset, we aim to provide a seed resource that can be expanded through community and expert contributions, fostering the development of AI systems capable of responsibly handling some of society's most delicate decisions.",
            "score": 1,
            "issue_id": 5906,
            "pub_date": "2025-09-15",
            "pub_date_card": {
                "ru": "15 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 15",
                "zh": "9æœˆ15æ—¥"
            },
            "hash": "ab3984446a10aa88",
            "authors": [
                "Sai Kartheek Reddy Kasu"
            ],
            "affiliations": [
                "IIIT Dharwad, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.11648.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#ethics",
                    "#healthcare",
                    "#alignment",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ² Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒĞµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸",
                    "desc": "Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ EthicsMH, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 125 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼. EthicsMH Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ… Ğ² Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¿ÑĞ¸Ñ…Ğ¸Ğ°Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ñ€ĞµĞ¼ÑÑ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´ĞµĞ»Ğ¸ĞºĞ°Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Evaluating AI Ethics in Mental Health with EthicsMH",
                    "desc": "EthicsMH is a dataset consisting of 125 scenarios aimed at assessing the ethical reasoning capabilities of AI systems in mental health settings. It focuses on key aspects such as decision accuracy, explanation quality, and adherence to professional ethical standards. The dataset includes structured elements like decision options and expert reasoning to facilitate comprehensive evaluation. By addressing the unique ethical challenges in mental health, EthicsMH serves as a foundational resource for developing AI systems that can make responsible decisions in sensitive contexts."
                },
                "zh": {
                    "title": "æ¨åŠ¨AIåœ¨å¿ƒç†å¥åº·é¢†åŸŸçš„ä¼¦ç†å†³ç­–èƒ½åŠ›",
                    "desc": "EthicsMHæ˜¯ä¸€ä¸ªåŒ…å«125ä¸ªåœºæ™¯çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨å¿ƒç†å¥åº·é¢†åŸŸçš„ä¼¦ç†æ¨ç†èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†å…³æ³¨å†³ç­–å‡†ç¡®æ€§ã€è§£é‡Šè´¨é‡å’Œä¸ä¸“ä¸šè§„èŒƒçš„ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠä¿å¯†æ€§ã€è‡ªä¸»æ€§å’Œåè§ç­‰ä¼¦ç†å›°å¢ƒæ—¶ã€‚æ¯ä¸ªåœºæ™¯éƒ½åŒ…å«å¤šä¸ªå†³ç­–é€‰é¡¹ã€ä¸“å®¶å¯¹é½çš„æ¨ç†ã€é¢„æœŸæ¨¡å‹è¡Œä¸ºå’Œå¤šæ–¹åˆ©ç›Šç›¸å…³è€…çš„è§‚ç‚¹ã€‚è¿™ä¸€æ•°æ®é›†ä¸ºAIä¼¦ç†ä¸å¿ƒç†å¥åº·å†³ç­–æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨ä¿ƒè¿›AIç³»ç»Ÿåœ¨å¤„ç†ç¤¾ä¼šæ•æ„Ÿå†³ç­–æ—¶çš„è´£ä»»æ„Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.11452",
            "title": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward\n  Weighting",
            "url": "https://huggingface.co/papers/2509.11452",
            "abstract": "Dynamic reward weighting in multi-objective reinforcement learning adaptively adjusts weights during training to explore Pareto fronts effectively, outperforming fixed-weight scalarization methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Prior works in multi-objective reinforcement learning typically use linear reward scalarization with fixed weights, which provably fail to capture non-convex Pareto fronts and thus yield suboptimal results. This limitation becomes especially critical in online preference alignment for large language models. Here, stochastic trajectories generated by parameterized policies create highly non-linear and non-convex mappings from parameters to objectives that no single static weighting scheme can find optimal trade-offs. We address this limitation by introducing dynamic reward weighting, which adaptively adjusts reward weights during the online reinforcement learning process. Unlike existing approaches that rely on fixed-weight interpolation, our dynamic weighting continuously balances and prioritizes objectives in training, facilitating effective exploration of Pareto fronts in objective space. We introduce two approaches of increasing sophistication and generalizability: (1) hypervolume-guided weight adaptation and (2) gradient-based weight optimization, offering a versatile toolkit for online multi-objective alignment. Our extensive experiments demonstrate their compatibility with commonly used online reinforcement learning algorithms (including GRPO, REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning datasets, and applicability to different model families, consistently achieving Pareto dominant solutions with fewer training steps than fixed-weight linear scalarization baselines.",
            "score": 1,
            "issue_id": 5906,
            "pub_date": "2025-09-14",
            "pub_date_card": {
                "ru": "14 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 14",
                "zh": "9æœˆ14æ—¥"
            },
            "hash": "5e0e1351e67bf729",
            "authors": [
                "Yining Lu",
                "Zilong Wang",
                "Shiyang Li",
                "Xin Liu",
                "Changlong Yu",
                "Qingyu Yin",
                "Zhan Shi",
                "Zixuan Zhang",
                "Meng Jiang"
            ],
            "affiliations": [
                "Amazon",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.11452.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ñ†ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ - Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸, ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ñ€Ğ¾Ğ½Ñ‚ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Dynamic Reward Weighting: Optimizing Multi-Objective Learning",
                    "desc": "This paper presents a method called dynamic reward weighting for multi-objective reinforcement learning, which adjusts the importance of different objectives during training. Traditional methods use fixed weights, which can lead to poor performance when dealing with complex, non-linear relationships between objectives. The proposed approach allows for better exploration of the Pareto front, leading to more optimal solutions. The authors demonstrate that their method outperforms existing techniques across various datasets and reinforcement learning algorithms, achieving better results with fewer training steps."
                },
                "zh": {
                    "title": "åŠ¨æ€å¥–åŠ±åŠ æƒï¼šä¼˜åŒ–å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ çš„åˆ©å™¨",
                    "desc": "åœ¨å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒåŠ¨æ€å¥–åŠ±åŠ æƒé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”è°ƒæ•´æƒé‡ï¼Œæœ‰æ•ˆæ¢ç´¢å¸•ç´¯æ‰˜å‰æ²¿ï¼Œä¼˜äºå›ºå®šæƒé‡çš„æ ‡é‡åŒ–æ–¹æ³•ã€‚ä»¥å¾€çš„ç ”ç©¶é€šå¸¸ä½¿ç”¨å›ºå®šæƒé‡çš„çº¿æ€§å¥–åŠ±æ ‡é‡åŒ–ï¼Œè¿™æ— æ³•æ•æ‰éå‡¸çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œå¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚æˆ‘ä»¬æå‡ºçš„åŠ¨æ€å¥–åŠ±åŠ æƒæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­æŒç»­å¹³è¡¡å’Œä¼˜å…ˆè€ƒè™‘ç›®æ ‡ï¼Œä»è€Œæ›´å¥½åœ°æ¢ç´¢ç›®æ ‡ç©ºé—´ä¸­çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸å¸¸ç”¨çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•å…¼å®¹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä»¥æ›´å°‘çš„è®­ç»ƒæ­¥éª¤å®ç°å¸•ç´¯æ‰˜ä¸»å¯¼è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.11362",
            "title": "PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits",
            "url": "https://huggingface.co/papers/2509.11362",
            "abstract": "PersonaX, a multimodal dataset, combines behavioral traits, facial imagery, and biographical information to enable comprehensive analysis and causal reasoning using large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding human behavior traits is central to applications in human-computer interaction, computational social science, and personalized AI systems. Such understanding often requires integrating multiple modalities to capture nuanced patterns and relationships. However, existing resources rarely provide datasets that combine behavioral descriptors with complementary modalities such as facial attributes and biographical information. To address this gap, we present PersonaX, a curated collection of multimodal datasets designed to enable comprehensive analysis of public traits across modalities. PersonaX consists of (1) CelebPersona, featuring 9444 public figures from diverse occupations, and (2) AthlePersona, covering 4181 professional athletes across 7 major sports leagues. Each dataset includes behavioral trait assessments inferred by three high-performing large language models, alongside facial imagery and structured biographical features. We analyze PersonaX at two complementary levels. First, we abstract high-level trait scores from text descriptions and apply five statistical independence tests to examine their relationships with other modalities. Second, we introduce a novel causal representation learning (CRL) framework tailored to multimodal and multi-measurement data, providing theoretical identifiability guarantees. Experiments on both synthetic and real-world data demonstrate the effectiveness of our approach. By unifying structured and unstructured analysis, PersonaX establishes a foundation for studying LLM-inferred behavioral traits in conjunction with visual and biographical attributes, advancing multimodal trait analysis and causal reasoning.",
            "score": 1,
            "issue_id": 5907,
            "pub_date": "2025-09-14",
            "pub_date_card": {
                "ru": "14 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 14",
                "zh": "9æœˆ14æ—¥"
            },
            "hash": "90082309548dd76f",
            "authors": [
                "Loka Li",
                "Wong Yu Kang",
                "Minghao Fu",
                "Guangyi Chen",
                "Zhenhao Chen",
                "Gongxu Luo",
                "Yuewen Sun",
                "Salman Khan",
                "Peter Spirtes",
                "Kun Zhang"
            ],
            "affiliations": [
                "Australian National University",
                "Carnegie Mellon University",
                "Mohamed bin Zayed University of Artificial Intelligence",
                "University of California San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.11362.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#multimodal",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "PersonaX: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "PersonaX - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‡ĞµÑ€Ñ‚Ñ‹, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ¸ Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹: CelebPersona Ñ 9444 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ AthlePersona Ñ 4181 Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¿Ğ¾Ñ€Ñ‚ÑĞ¼ĞµĞ½Ğ¾Ğ¼. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‡ĞµÑ€Ñ‚, ÑĞ´ĞµĞ»Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ¼Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Human Behavior Through Multimodal Analysis with PersonaX",
                    "desc": "PersonaX is a new multimodal dataset that combines behavioral traits, facial images, and biographical data to enhance the analysis of human behavior using large language models. It includes two main components: CelebPersona, which features public figures, and AthlePersona, which focuses on professional athletes, both enriched with behavioral assessments from advanced language models. The dataset allows researchers to explore relationships between different types of data through statistical tests and introduces a causal representation learning framework for better understanding these connections. By integrating various modalities, PersonaX aims to improve the study of behavioral traits and their implications in AI systems and human-computer interaction."
                },
                "zh": {
                    "title": "PersonaXï¼šå¤šæ¨¡æ€ç‰¹å¾åˆ†æçš„æ–°åŸºç¡€",
                    "desc": "PersonaXæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ•°æ®é›†ï¼Œç»“åˆäº†è¡Œä¸ºç‰¹å¾ã€é¢éƒ¨å›¾åƒå’Œä¼ è®°ä¿¡æ¯ï¼Œä»¥ä¾¿ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨é¢åˆ†æå’Œå› æœæ¨ç†ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬CelebPersonaå’ŒAthlePersonaï¼Œæ¶µç›–äº†æ¥è‡ªä¸åŒèŒä¸šçš„å…¬å…±äººç‰©å’Œä¸“ä¸šè¿åŠ¨å‘˜ã€‚æ¯ä¸ªæ•°æ®é›†éƒ½åŒ…å«ç”±é«˜æ€§èƒ½å¤§å‹è¯­è¨€æ¨¡å‹æ¨æ–­çš„è¡Œä¸ºç‰¹å¾è¯„ä¼°ï¼Œä»¥åŠé¢éƒ¨å›¾åƒå’Œç»“æ„åŒ–çš„ä¼ è®°ç‰¹å¾ã€‚é€šè¿‡å¼•å…¥æ–°çš„å› æœè¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼ŒPersonaXä¸ºå¤šæ¨¡æ€å’Œå¤šæµ‹é‡æ•°æ®çš„åˆ†æå¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€ç‰¹å¾åˆ†æå’Œå› æœæ¨ç†çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09658",
            "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2509.09658",
            "abstract": "HumbleBench evaluates multimodal large language models' ability to reject incorrect answers, addressing the issue of hallucinations in visual question answering and decision-making.  \t\t\t\t\tAI-generated summary \t\t\t\t Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a \"None of the above\" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench.",
            "score": 1,
            "issue_id": 5907,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "f1250c153f3e9659",
            "authors": [
                "Bingkui Tong",
                "Jiaer Xia",
                "Sifeng Shang",
                "Kaiyang Zhou"
            ],
            "affiliations": [
                "Hong Kong Baptist University, Hong Kong",
                "Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09658.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#multimodal",
                    "#dataset",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞºÑ€Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM",
                    "desc": "HumbleBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¾Ñ‚Ğ²ĞµÑ€Ğ³Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ñ‹Ñ…, Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ¿Ñ†Ğ¸Ñ 'ĞĞ¸Ñ‡ĞµĞ³Ğ¾ Ğ¸Ğ· Ğ²Ñ‹ÑˆĞµĞ¿ĞµÑ€ĞµÑ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾'. HumbleBench Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½Ğ¸ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²ĞµÑ€Ğ½Ñ‹Ğ¼."
                },
                "en": {
                    "title": "HumbleBench: Evaluating AI's Ability to Say 'None of the Above'",
                    "desc": "HumbleBench is a new benchmark designed to assess the ability of multimodal large language models (MLLMs) to reject incorrect answers, addressing the problem of hallucinations in visual question answering. Hallucinations occur when models generate responses that do not align with the input image, which can lead to misinformation and unsafe decisions. Unlike existing benchmarks that focus solely on recognizing correct answers, HumbleBench evaluates models on their capacity to identify when none of the provided options are correct, promoting epistemic humility. The benchmark includes a variety of question types and is built from a detailed scene graph dataset, allowing for a comprehensive evaluation of MLLMs in real-world applications."
                },
                "zh": {
                    "title": "HumbleBenchï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„å¯é æ€§",
                    "desc": "HumbleBench æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ‹’ç»é”™è¯¯ç­”æ¡ˆçš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å…³æ³¨æ¨¡å‹åœ¨è§†è§‰é—®ç­”å’Œå†³ç­–è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å¹»è§‰é—®é¢˜ï¼Œå³ç”Ÿæˆä¸è¾“å…¥å›¾åƒä¸ä¸€è‡´çš„å†…å®¹ã€‚HumbleBench é€šè¿‡ä¸‰ç§å¹»è§‰ç±»å‹ï¼ˆå¯¹è±¡ã€å…³ç³»å’Œå±æ€§ï¼‰æ¥æµ‹è¯•æ¨¡å‹çš„èƒ½åŠ›ï¼Œç¡®ä¿æ¨¡å‹ä¸ä»…èƒ½è¯†åˆ«æ­£ç¡®çš„ä¿¡æ¯ï¼Œè¿˜èƒ½åˆ¤æ–­æä¾›çš„é€‰é¡¹ä¸­æ²¡æœ‰æœ‰æ•ˆç­”æ¡ˆã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºå½“å‰è¯„ä¼°å·¥å…·å¡«è¡¥äº†å…³é”®ç©ºç™½ï¼Œæä¾›äº†æ›´çœŸå®çš„ MLLM å¯é æ€§æµ‹é‡ï¼Œå°¤å…¶åœ¨å®‰å…¨å…³é”®çš„åº”ç”¨åœºæ™¯ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.11986",
            "title": "Lost in Embeddings: Information Loss in Vision-Language Models",
            "url": "https://huggingface.co/papers/2509.11986",
            "abstract": "Two approaches are introduced to analyze and quantify information loss in vision-language models during the projection of visual inputs into the language model's embedding space, revealing significant distortions and their impact on model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision--language models (VLMs) often process visual inputs through a pretrained vision encoder, followed by a projection into the language model's embedding space via a connector component. While crucial for modality fusion, the potential information loss induced by this projection step and its direct impact on model capabilities remain understudied. We introduce two complementary approaches to examine and quantify this loss by analyzing the latent representation space. First, we evaluate semantic information preservation by analyzing changes in k-nearest neighbor relationships between image representations, before and after projection. Second, we directly measure information loss by reconstructing visual embeddings from the projected representation, localizing loss at an image patch level. Experiments reveal that connectors substantially distort the local geometry of visual representations, with k-nearest neighbors diverging by 40--60\\% post-projection, correlating with degradation in retrieval performance. The patch-level embedding reconstruction provides interpretable insights for model behavior on visually grounded question-answering tasks, finding that areas of high information loss reliably predict instances where models struggle.",
            "score": 0,
            "issue_id": 5907,
            "pub_date": "2025-09-15",
            "pub_date_card": {
                "ru": "15 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 15",
                "zh": "9æœˆ15æ—¥"
            },
            "hash": "51ff80bb55b3f8d8",
            "authors": [
                "Wenyan Li",
                "Raphael Tang",
                "Chengzu Li",
                "Caiqi Zhang",
                "Ivan VuliÄ‡",
                "Anders SÃ¸gaard"
            ],
            "affiliations": [
                "Microsoft",
                "University of Cambridge",
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.11986.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#cv",
                    "#games"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… k-Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ· ÑĞ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Quantifying Information Loss in Vision-Language Models",
                    "desc": "This paper investigates how vision-language models (VLMs) lose information when converting visual inputs into a language model's embedding space. It introduces two methods to analyze this information loss: one examines how well the semantic relationships between images are preserved after projection, and the other reconstructs visual embeddings to identify loss at a detailed level. The findings show that the projection process significantly distorts the spatial relationships of visual data, leading to a 40-60% divergence in k-nearest neighbor relationships, which negatively affects model performance. Additionally, the study highlights that areas with high information loss can predict where models may struggle in tasks like visually grounded question-answering."
                },
                "zh": {
                    "title": "æ­ç¤ºè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„ä¿¡æ¯æŸå¤±",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸¤ç§æ–¹æ³•æ¥åˆ†æå’Œé‡åŒ–è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å°†è§†è§‰è¾“å…¥æŠ•å½±åˆ°è¯­è¨€æ¨¡å‹åµŒå…¥ç©ºé—´æ—¶çš„ä¿¡æ¯æŸå¤±ã€‚è¿™ç§æŠ•å½±æ­¥éª¤å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„å¤±çœŸï¼Œå¹¶ç›´æ¥å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡åˆ†ææ½œåœ¨è¡¨ç¤ºç©ºé—´æ¥è¯„ä¼°è¯­ä¹‰ä¿¡æ¯çš„ä¿ç•™æƒ…å†µï¼Œå¹¶æµ‹é‡ä¿¡æ¯æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿æ¥å™¨æ˜¾è‘—æ‰­æ›²äº†è§†è§‰è¡¨ç¤ºçš„å±€éƒ¨å‡ ä½•ç»“æ„ï¼Œå¯¼è‡´kè¿‘é‚»å…³ç³»åœ¨æŠ•å½±åå‘ç”Ÿ40-60%çš„åç¦»ï¼Œä¸”ä¸æ£€ç´¢æ€§èƒ½çš„ä¸‹é™ç›¸å…³è”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10844",
            "title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings",
            "url": "https://huggingface.co/papers/2509.10844",
            "abstract": "GAPrune, a pruning framework that considers domain importance and general linguistic foundation, effectively compresses models while maintaining and enhancing domain-specific performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Domain-specific embedding models have shown promise for applications that require specialized semantic understanding, such as coding agents and financial retrieval systems, often achieving higher performance gains than general models. However, state-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Model compression through pruning offers a promising solution, but existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions. Thus, we propose GAPrune, a pruning framework that addresses this challenge by considering both domain importance and preserving general linguistic foundation. Our method uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on two domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our findings demonstrate that principled pruning strategies can achieve model compression and enhanced domain specialization, providing the research community with a new approach for development.",
            "score": 0,
            "issue_id": 5907,
            "pub_date": "2025-09-13",
            "pub_date_card": {
                "ru": "13 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 13",
                "zh": "9æœˆ13æ—¥"
            },
            "hash": "018cc042df405787",
            "authors": [
                "Yixuan Tang",
                "Yi Yang"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10844.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğµ, Ğ¾Ñ‚ÑĞµĞºĞ°ĞµĞ¼ Ğ»Ğ¸ÑˆĞ½ĞµĞµ",
                    "desc": "GAPrune - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹. ĞĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. GAPrune Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¤Ğ¸ÑˆĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GAPrune Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸."
                },
                "en": {
                    "title": "GAPrune: Smart Pruning for Enhanced Domain Performance",
                    "desc": "GAPrune is a novel pruning framework designed to compress machine learning models while enhancing their performance in specific domains. It distinguishes between general semantic representations and domain-specific patterns, allowing for more effective pruning decisions. By utilizing Fisher Information and Domain Alignment Importance (DAI) scoring, GAPrune identifies which parameters are crucial for domain tasks and which can be pruned without losing performance. Experiments show that GAPrune not only maintains high accuracy but also improves domain-specific capabilities after retraining, making it a valuable tool for deploying models in resource-limited environments."
                },
                "zh": {
                    "title": "GAPruneï¼šæ™ºèƒ½å‰ªæï¼Œæå‡é¢†åŸŸæ€§èƒ½",
                    "desc": "GAPruneæ˜¯ä¸€ç§å‰ªææ¡†æ¶ï¼Œè€ƒè™‘äº†é¢†åŸŸé‡è¦æ€§å’Œé€šç”¨è¯­è¨€åŸºç¡€ï¼Œæœ‰æ•ˆåœ°å‹ç¼©æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒå’Œå¢å¼ºé¢†åŸŸç‰¹å®šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨Fisherä¿¡æ¯æ¥è¡¡é‡å‚æ•°çš„é‡è¦æ€§ï¼Œå¹¶ç»“åˆé€šç”¨é¢†åŸŸæ¢¯åº¦å¯¹é½æ¥è¯„ä¼°å‚æ•°è¡Œä¸ºï¼Œä»è€Œå®ç°æ›´ä¼˜çš„å‰ªæå†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGAPruneåœ¨FinMTEBå’ŒChemTEBä¸¤ä¸ªé¢†åŸŸåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨50%ç¨€ç–åº¦ä¸‹ä¿æŒä¸å¯†é›†æ¨¡å‹ç›¸è¿‘çš„æ€§èƒ½ï¼Œå¹¶åœ¨é‡æ–°è®­ç»ƒåè¿›ä¸€æ­¥æå‡é¢†åŸŸç‰¹å®šèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆç†çš„å‰ªæç­–ç•¥ä¸ä»…å¯ä»¥å®ç°æ¨¡å‹å‹ç¼©ï¼Œè¿˜èƒ½å¢å¼ºé¢†åŸŸä¸“ä¸šåŒ–ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-15.html",
    "link_next": "2025-09-17.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "15.09",
        "en": "09/15",
        "zh": "9æœˆ15æ—¥"
    },
    "short_date_next": {
        "ru": "17.09",
        "en": "09/17",
        "zh": "9æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 3,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 3,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 2,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 4,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}